{"questions":[{"id":"q-1020","question":"You’re evaluating a beginner feature: a daily market digest in a Robinhood-like app. Do a practical cost‑benefit analysis: state assumptions, estimate data/API costs, storage and engineering time, quantify benefits (retention lift, ARPU), and compute break-even time. Provide a rough calculation and rationale?","answer":"Assume 50k MAU, 4% retention lift, and $0.001 data cost per digest. Costs: data ~$120/mo, storage ~$20, engineering ~16h at $75/h = $1,200. Benefit: 7,500 users x $0.08/mo ARPU uplift for 6 months ≈ $","explanation":"## Why This Is Asked\n\nGauges ability to justify product bets with a pragmatic, numbers-driven approach and to handle uncertainty in inputs.\n\n## Key Concepts\n\n- Cost estimation\n- Benefit estimation\n- Break-even analysis\n- Reasonable assumptions\n\n## Code Example\n\n```javascript\n// Simple CBA calculator\nfunction cba(params) {\n  const {maU, lift, arpu, months, cost} = params;\n  const annualizedBenefit = maU * lift * arpu * months;\n  return {annualizedBenefit, cost, breakevenMonths: cost / (annualizedBenefit || 1)};\n}\n```\n\n## Follow-up Questions\n\n- How would you adjust if MAU grows or adoption changes?\n- What if ARPU uplift is not uniform across user segments?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T19:33:39.493Z","createdAt":"2026-01-12T19:33:39.493Z"},{"id":"q-1052","question":"Scenario: Real-time ingestion service receives JSON events from edge devices. Guarantee per-user throughput while allowing bursts in a distributed cluster. Design and implement a practical throttling mechanism, specify data structures, atomicity (e.g., Redis Lua script), failure modes, testing strategy, and observability?","answer":"Implement a per-user token bucket in Redis. Refill at 500 tokens/sec with a burst capacity of 1000 tokens (2s burst). On each event, atomically call a Lua script to consume 1 token; if available, forw","explanation":"## Why This Is Asked\n\nThis question tests practical rate-limiting in distributed systems, emphasizing per-user fairness, op readiness, and recovery from clock skew.\n\n## Key Concepts\n\n- Token bucket\n- Redis Lua scripts for atomic ops\n- Burst handling and backpressure\n- Observability and testing\n\n## Code Example\n\n```javascript\n// Redis Lua script (conceptual)\nlocal bucket = KEYS[1]\nlocal tokens = tonumber(redis.call('GET', bucket) or '0')\nlocal refill = tonumber(ARGV[1])\nlocal cap = tonumber(ARGV[2])\nlocal new_tokens = math.min(cap, tokens + refill)\nif new_tokens >= 1 then\n  redis.call('SET', bucket, new_tokens - 1)\n  return 1\nelse\n  return 0\nend\n```\n\n## Follow-up Questions\n\n- How would you test for clock skew and drift?\n- How would you adapt for multi-tenant fairness?\n","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","OpenAI","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T20:36:38.142Z","createdAt":"2026-01-12T20:36:38.142Z"},{"id":"q-1057","question":"In a real-time feed system using a contextual bandit with attention weighting (CBA), design a policy that balances short-term CTR and long-term engagement. Explain your reward decomposition, exploration strategy, and handling of non-stationarity. How would you validate offline with CPE and ramp online safely? Provide a concise update rule?","answer":"Implement a contextual bandit with attention weights. Reward = alpha * CTR + beta * retention, with alphas learned from data. Use Thompson Sampling or a neural head to estimate rewards. Update theta v","explanation":"## Why This Is Asked\nTests a candidate's ability to design production-ready policies balancing immediate metrics with long-term user value, handling non-stationarity, and validating safely.\n\n## Key Concepts\n- Contextual bandits and attention weighting\n- Reward decomposition for short-term vs long-term goals\n- Exploration strategies (Thompson Sampling, posterior updates)\n- Non-stationarity handling and evaluation (offline CPE, staged online ramp)\n\n## Code Example\n```python\n# Pseudo update step for theta\nimport numpy as np\n\ndef update_theta(theta, phi, reward, lr):\n    pred = np.dot(theta, phi)\n    grad = (reward - pred) * phi\n    theta += lr * grad\n    return theta\n```\n\n## Follow-up Questions\n- How would you handle feature drift in attention weights?\n- What offline metrics would you trust for CPE in this setup?","diagram":"flowchart TD\n  Context[Context] --> Features[Compute features with attention]\n  Features --> Decide[Policy selects action]\n  Decide --> Reward[Observe reward]\n  Reward --> Update[Update model]\n  Update --> Evaluation[Online/Offline evaluation]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Microsoft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T21:18:55.668Z","createdAt":"2026-01-12T21:18:55.668Z"},{"id":"q-1084","question":"Given a large social network planning to adopt a real-time feature flag evaluation service that runs on a hybrid stream/batch pipeline. Current pipeline: 1.2M events/sec, median latency 50 ms, 5% tail. New service promises 20–30% latency reduction and 25% cost increase, plus migration risk. Perform a cost-benefit analysis: quantify costs, benefits, risks, horizon (12 months), and decision rule with sensitivity ranges?","answer":"Quantify TCO over 12 months: infra/licensing, migration, and ops for both pipelines; quantify benefits from latency/throughput gains (fewer SLA penalties, higher retention, lower cost per event); incl","explanation":"## Why This Is Asked\n\nTests ability to perform a structured CBA for a real-time system, balancing latency impact, migration risk, and total cost of ownership. It also probes framing the horizon, risk quantification, and decision criteria.\n\n## Key Concepts\n\n- TCO, NPV, IRR\n- Latency vs cost trade-offs\n- Migration risk, rollback plans, and governance\n\n## Code Example\n\n```javascript\n// Pseudocode: simple NPV calculation\nfunction npv(r, cashFlows) {\n  return cashFlows.reduce((acc, cf, i) => acc + cf / Math.pow(1+r, i+1), 0)\n}\n```\n\n## Follow-up Questions\n\n- How would you model outsized risk of outages?\n- How would you present to executives under uncertainty?\n","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T22:18:59.423Z","createdAt":"2026-01-12T22:18:59.423Z"},{"id":"q-1114","question":"You’re migrating a real-time feature flag engine (multi-tenant SaaS) to reduce tail latency and cost. Propose a three-phase migration: centralized eval, regional edge gateway, then hybrid routing with per-tenant caches. Specify success metrics, rollback criteria, and a 12-month plan?","answer":"Propose a phased migration: 1) centralized eval gate, 2) regional edge gateways, 3) hybrid routing with per-tenant caches and rule-level debouncing. Metrics: P99 latency <15 ms, 99.9th <40 ms, cost pe","explanation":"## Why This Is Asked\n\nAssesses ability to design staged rollouts with measurable SLAs, cost models, and risk controls; emphasizes regional latency considerations and guardrails.\n\n## Key Concepts\n\n- phased migration strategy\n- tail latency and cost modeling\n- rollback criteria and canary policies\n\n## Code Example\n\n```javascript\nfunction evaluateFlag(flag, ctx) {\n  // simplified evaluation path\n  const gate = flag.gate || 'centralized';\n  const rules = flag.rules || [];\n  // in a real system this would consult caches and regional data\n  return rules.some(r => ctx.userRoles.includes(r.requiredRole));\n}\n```\n\n## Follow-up Questions\n\n- How would you monitor drift in evaluation accuracy across regions?\n- What metrics would you use to trigger a rollback and how would you implement it?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Hashicorp","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T23:25:36.721Z","createdAt":"2026-01-12T23:25:36.721Z"},{"id":"q-1151","question":"In a real-time analytics system for engagement on a large social app, design a privacy-preserving cohort analytics pipeline that ingests ~3M events/sec with sub-100 ms latency per region. Requirements: data residency, differential privacy for cohort counts, delta- or exact-once streaming state, drift detection, and cost-conscious multi-region deployment. Outline architecture, data contracts, and trade-offs?","answer":"Propose a privacy-preserving real-time cohort analytics pipeline using region-local streaming (e.g., Kafka + Flink), per-region state, and differential privacy (epsilon ~1.0) for cohort counts. Use wi","explanation":"## Why This Is Asked\nTests ability to design a privacy-conscious, low-latency analytics pipeline under multi-region constraints, with DP guarantees and drift detection.\n\n## Key Concepts\n- Real-time streaming with Kafka + Flink\n- Differential privacy (epsilon ~1.0)\n- Region-local state and windowed aggregations (<100 ms)\n- Drift detection and observability\n- Data residency and governance\n\n## Code Example\n```javascript\nfunction addDPNoise(count, epsilon) {\n  const sigma = Math.sqrt(2 / epsilon);\n  const noise = randn_bm() * sigma;\n  return Math.max(0, Math.round(count + noise));\n}\nfunction randn_bm() {\n  let u = 0, v = 0;\n  while (u === 0) u = Math.random();\n  while (v === 0) v = Math.random();\n  return Math.sqrt(-2.0 * Math.log(u)) * Math.cos(2 * Math.PI * v);\n}\n```\n\n## Follow-up Questions\n- How would you validate privacy guarantees in production?\n- What data contracts would you enforce with product teams to prevent over-collection?","diagram":"flowchart TD\n  Ingest[Ingest events] --> Stream[Stream processing]\n  Stream --> Cohort[Cohort analytics]\n  Cohort --> Privacy[DP noise]\n  Cohort --> Drift[Drift detection]\n  Drift --> Observ[Observability]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Meta","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:35:47.807Z","createdAt":"2026-01-13T01:35:47.807Z"},{"id":"q-1209","question":"You’re building an offline-first version of a daily digest app for low-connectivity users. Outline a minimal data model, eviction policy, and a practical plan to quantify cost savings from reduced network usage versus increased storage and complexity; provide a concrete example with rough numbers?","answer":"Use an offline cache storing up to M articles per user with fields: id, title, summary, url, publishedAt, hash. Evict by LRU or per-topic quotas. Background sync uses conditional GET/ETag to update ca","explanation":"## Why This Is Asked\nTests ability to design offline-first cache, quantify trade-offs, and produce concrete numbers relevant to mobile apps.\n\n## Key Concepts\n- Offline-first design\n- Data modeling for cached content\n- Cache eviction strategies (LRU, quota-based)\n- Cost-benefit estimation (bandwidth vs storage)\n\n## Code Example\n```javascript\n// Data model sketch\ntype Article = {\n  id: string;\n  title: string;\n  summary: string;\n  url: string;\n  publishedAt: string;\n  hash: string;\n};\n\nconst CACHE_LIMIT = 50; // per user\nfunction evictLRU(cache: Article[]) {\n  // simplistic placeholder\n}\n```\n\n## Follow-up Questions\n- How would you test offline scenarios across flaky networks?\n- How would you measure real-world bandwidth savings after rollout?","diagram":"flowchart TD\n  A[User opens digest] --> B{Online?}\n  B -- Yes --> C[Fetch latest and update cache]\n  B -- No --> D[Show offline cache]\n  C --> D","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","DoorDash","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T05:24:07.770Z","createdAt":"2026-01-13T05:24:07.770Z"},{"id":"q-1238","question":"You're evaluating a beginner feature: a daily 'Portfolio Health Snap' panel that assigns a risk score to each user based on volatility of top holdings. Do a practical cost-benefit analysis: state assumptions, data/API costs, storage, and engineering time; quantify benefits (retention lift, ARPU) and compute break-even time. Provide rough calculations and rationale?","answer":"Assumptions: 5,000 DAU, one volatility check per user per day; API/data costs $0.0008/call; 10 MB storage; backend effort ~1.5 weeks. Benefits: +3% DAU, +$0.20 ARPU, improved retention by 2%. Break-ev","explanation":"## Why This Is Asked\nTests ability to plan ROI for a data-driven feature, balancing cost and user value, with beginner-friendly data and timing considerations.\n\n## Key Concepts\n- ROI modeling and payback\n- Data sourcing costs and API limits\n- Storage and compute budgeting\n- Validation and lightweight experimentation\n- Privacy and governance\n\n## Code Example\n```javascript\n// Simple break-even calculator\nfunction breakEvenDays(cost, upliftPerUserPerDay, dailyActiveUsers) {\n  const dailyGain = upliftPerUserPerDay * dailyActiveUsers;\n  return dailyGain > 0 ? cost / dailyGain : Infinity;\n}\n```\n\n## Follow-up Questions\n- How would you estimate the uplift values without a full launch?\n- What would be your MVP scope and validation plan?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Square","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T06:34:22.701Z","createdAt":"2026-01-13T06:34:22.701Z"},{"id":"q-1274","question":"You're assessing migrating a high-volume telemetry pipeline from a centralized data warehouse to a lakehouse with streaming ingestion and on-demand materialized views. Current throughput 5M events/sec, latency 5–7 min; target latency 2–3 min, 25% cost increase. Build a 12-month cost-benefit model: incremental storage/compute, streaming infra, data egress, drift/rollback costs; specify decision rules and sensitivity ranges?","answer":"12-month TCO model comparing a centralized data warehouse to a lakehouse with streaming ingestion and on-demand materialized views. Quantify incremental storage/compute, streaming infra (Flink/Spark),","explanation":"## Why This Is Asked\n\nThis question probes ability to design and justify moves to a lakehouse architecture, balancing cost, latency, data freshness, and risk in a measurable way.\n\n## Key Concepts\n\n- TCO modeling across storage, compute, and data transfer\n- Data freshness vs cost and drift/rollback risk\n- Sensitivity analysis and decision rules\n\n## Code Example\n\n```javascript\nfunction npv(cashFlows, rate) {\n  return cashFlows.reduce((acc, v, i) => acc + v / Math.pow(1+rate, i+1), 0);\n}\n```\n\n## Follow-up Questions\n\n- How would you measure and model data drift in this context?\n- What rollout milestones and metrics would trigger a rollback or full migration?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Citadel","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:39:19.754Z","createdAt":"2026-01-13T07:39:19.754Z"},{"id":"q-843","question":"Given a CSV file with columns user_id, action, timestamp, write a Python function using only the standard library that returns the most recent action per user by deduplicating on user_id and keeping the latest timestamp; describe time complexity and edge cases?","answer":"Parse the CSV with the csv module, iterate rows, parse timestamp with datetime.fromisoformat, and keep a dict mapping user_id to (timestamp, action). If a row has a newer timestamp, replace the entry.","explanation":"## Why This Is Asked\nTests ability to implement a practical data-processing task: deduplicate by key using timestamps, relies only on standard library, and reveals awareness of edge cases like ties, malformed timestamps, and missing fields.\n\n## Key Concepts\n- CSV parsing with csv module\n- Deduplication by key using a dictionary\n- Timestamp parsing with datetime (ISO 8601)\n- Edge cases: missing fields, invalid timestamps, tie-breaking\n\n## Code Example\n```python\nimport csv\nfrom datetime import datetime\n\ndef latest_actions(csv_path):\n    best = {}\n    with open(csv_path, newline='') as f:\n        rdr = csv.DictReader(f)\n        for row in rdr:\n            uid = row['user_id']\n            ts = row['timestamp']\n            act = row['action']\n            try:\n                t = datetime.fromisoformat(ts)\n            except Exception:\n                continue\n            if uid not in best or t > best[uid][0]:\n                best[uid] = (t, act)\n    return [(uid, act, t.isoformat()) for uid, (t, act) in best.items()]\n```\n\n## Follow-up Questions\n- How would you adapt this for streaming data or files larger than memory?\n- If multiple actions share the same timestamp for a user, how would you break ties and what metadata would you add?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","PayPal","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:27:05.585Z","createdAt":"2026-01-12T13:27:05.585Z"},{"id":"q-872","question":"You are evaluating two deployment options for a new AI inference service under budget and latency constraints. Outline a practical, end-to-end cost-benefit analysis framework to decide which to deploy in production. Include: data you would collect (traffic, latency, SLA penalties, accuracy), metrics (NPV, ROI, payback), horizons, discount rate, handling uncertainty (scenarios), and a concrete calculation workflow you would run?","answer":"I would model total cost as compute, storage, and ops, plus latency penalties; benefits as incremental revenue or user value from faster or more reliable services. I would calculate NPV over 12-24 mon","explanation":"## Why This Is Asked\nTests the ability to structure a pragmatic CBA for ML deployment, balancing cost with user value, handling uncertainty, and communicating the result.\n\n## Key Concepts\n- Cost and benefits modeling\n- Time horizon and discounting\n- Scenario analysis (base/best/worst)\n- Metrics: NPV, ROI, risk-adjusted EV\n\n## Code Example\n```javascript\nfunction npv(cashFlows, rate) {\n  return cashFlows.reduce((acc, cf, i) => acc + cf / Math.pow(1 + rate, i+1), 0);\n}\n```\n\n## Follow-up Questions\n- How would you handle non-linear cost factors (e.g., burst traffic) in your model?\n- How would you present results to non-technical stakeholders?","diagram":"flowchart TD\n  DataInputs[Data Inputs] --> Option[Model Option]\n  Option --> Decision{Decision}\n  Decision --> Deploy[Deploy to Prod]\n  Decision --> Iterate[Iterate Optimization]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:54:31.848Z","createdAt":"2026-01-12T13:54:31.848Z"}],"subChannels":["general"],"companies":["Adobe","Bloomberg","Citadel","Coinbase","DoorDash","Goldman Sachs","Hashicorp","Hugging Face","LinkedIn","Meta","Microsoft","MongoDB","NVIDIA","OpenAI","PayPal","Robinhood","Scale Ai","Slack","Snap","Snowflake","Square","Tesla","Twitter","Two Sigma","Uber"],"stats":{"total":11,"beginner":4,"intermediate":4,"advanced":3,"newThisWeek":11}}