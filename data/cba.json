{"questions":[{"id":"q-1020","question":"You’re evaluating a beginner feature: a daily market digest in a Robinhood-like app. Do a practical cost‑benefit analysis: state assumptions, estimate data/API costs, storage and engineering time, quantify benefits (retention lift, ARPU), and compute break-even time. Provide a rough calculation and rationale?","answer":"Assume 50k MAU, 4% retention lift, and $0.001 data cost per digest. Costs: data ~$120/mo, storage ~$20, engineering ~16h at $75/h = $1,200. Benefit: 7,500 users x $0.08/mo ARPU uplift for 6 months ≈ $","explanation":"## Why This Is Asked\n\nGauges ability to justify product bets with a pragmatic, numbers-driven approach and to handle uncertainty in inputs.\n\n## Key Concepts\n\n- Cost estimation\n- Benefit estimation\n- Break-even analysis\n- Reasonable assumptions\n\n## Code Example\n\n```javascript\n// Simple CBA calculator\nfunction cba(params) {\n  const {maU, lift, arpu, months, cost} = params;\n  const annualizedBenefit = maU * lift * arpu * months;\n  return {annualizedBenefit, cost, breakevenMonths: cost / (annualizedBenefit || 1)};\n}\n```\n\n## Follow-up Questions\n\n- How would you adjust if MAU grows or adoption changes?\n- What if ARPU uplift is not uniform across user segments?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T19:33:39.493Z","createdAt":"2026-01-12T19:33:39.493Z"},{"id":"q-1052","question":"Scenario: Real-time ingestion service receives JSON events from edge devices. Guarantee per-user throughput while allowing bursts in a distributed cluster. Design and implement a practical throttling mechanism, specify data structures, atomicity (e.g., Redis Lua script), failure modes, testing strategy, and observability?","answer":"Implement a per-user token bucket in Redis. Refill at 500 tokens/sec with a burst capacity of 1000 tokens (2s burst). On each event, atomically call a Lua script to consume 1 token; if available, forw","explanation":"## Why This Is Asked\n\nThis question tests practical rate-limiting in distributed systems, emphasizing per-user fairness, op readiness, and recovery from clock skew.\n\n## Key Concepts\n\n- Token bucket\n- Redis Lua scripts for atomic ops\n- Burst handling and backpressure\n- Observability and testing\n\n## Code Example\n\n```javascript\n// Redis Lua script (conceptual)\nlocal bucket = KEYS[1]\nlocal tokens = tonumber(redis.call('GET', bucket) or '0')\nlocal refill = tonumber(ARGV[1])\nlocal cap = tonumber(ARGV[2])\nlocal new_tokens = math.min(cap, tokens + refill)\nif new_tokens >= 1 then\n  redis.call('SET', bucket, new_tokens - 1)\n  return 1\nelse\n  return 0\nend\n```\n\n## Follow-up Questions\n\n- How would you test for clock skew and drift?\n- How would you adapt for multi-tenant fairness?\n","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","OpenAI","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T20:36:38.142Z","createdAt":"2026-01-12T20:36:38.142Z"},{"id":"q-1057","question":"In a real-time feed system using a contextual bandit with attention weighting (CBA), design a policy that balances short-term CTR and long-term engagement. Explain your reward decomposition, exploration strategy, and handling of non-stationarity. How would you validate offline with CPE and ramp online safely? Provide a concise update rule?","answer":"Implement a contextual bandit with attention weights. Reward = alpha * CTR + beta * retention, with alphas learned from data. Use Thompson Sampling or a neural head to estimate rewards. Update theta v","explanation":"## Why This Is Asked\nTests a candidate's ability to design production-ready policies balancing immediate metrics with long-term user value, handling non-stationarity, and validating safely.\n\n## Key Concepts\n- Contextual bandits and attention weighting\n- Reward decomposition for short-term vs long-term goals\n- Exploration strategies (Thompson Sampling, posterior updates)\n- Non-stationarity handling and evaluation (offline CPE, staged online ramp)\n\n## Code Example\n```python\n# Pseudo update step for theta\nimport numpy as np\n\ndef update_theta(theta, phi, reward, lr):\n    pred = np.dot(theta, phi)\n    grad = (reward - pred) * phi\n    theta += lr * grad\n    return theta\n```\n\n## Follow-up Questions\n- How would you handle feature drift in attention weights?\n- What offline metrics would you trust for CPE in this setup?","diagram":"flowchart TD\n  Context[Context] --> Features[Compute features with attention]\n  Features --> Decide[Policy selects action]\n  Decide --> Reward[Observe reward]\n  Reward --> Update[Update model]\n  Update --> Evaluation[Online/Offline evaluation]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Microsoft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T21:18:55.668Z","createdAt":"2026-01-12T21:18:55.668Z"},{"id":"q-1084","question":"Given a large social network planning to adopt a real-time feature flag evaluation service that runs on a hybrid stream/batch pipeline. Current pipeline: 1.2M events/sec, median latency 50 ms, 5% tail. New service promises 20–30% latency reduction and 25% cost increase, plus migration risk. Perform a cost-benefit analysis: quantify costs, benefits, risks, horizon (12 months), and decision rule with sensitivity ranges?","answer":"Quantify TCO over 12 months: infra/licensing, migration, and ops for both pipelines; quantify benefits from latency/throughput gains (fewer SLA penalties, higher retention, lower cost per event); incl","explanation":"## Why This Is Asked\n\nTests ability to perform a structured CBA for a real-time system, balancing latency impact, migration risk, and total cost of ownership. It also probes framing the horizon, risk quantification, and decision criteria.\n\n## Key Concepts\n\n- TCO, NPV, IRR\n- Latency vs cost trade-offs\n- Migration risk, rollback plans, and governance\n\n## Code Example\n\n```javascript\n// Pseudocode: simple NPV calculation\nfunction npv(r, cashFlows) {\n  return cashFlows.reduce((acc, cf, i) => acc + cf / Math.pow(1+r, i+1), 0)\n}\n```\n\n## Follow-up Questions\n\n- How would you model outsized risk of outages?\n- How would you present to executives under uncertainty?\n","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T22:18:59.423Z","createdAt":"2026-01-12T22:18:59.423Z"},{"id":"q-1114","question":"You’re migrating a real-time feature flag engine (multi-tenant SaaS) to reduce tail latency and cost. Propose a three-phase migration: centralized eval, regional edge gateway, then hybrid routing with per-tenant caches. Specify success metrics, rollback criteria, and a 12-month plan?","answer":"Propose a phased migration: 1) centralized eval gate, 2) regional edge gateways, 3) hybrid routing with per-tenant caches and rule-level debouncing. Metrics: P99 latency <15 ms, 99.9th <40 ms, cost pe","explanation":"## Why This Is Asked\n\nAssesses ability to design staged rollouts with measurable SLAs, cost models, and risk controls; emphasizes regional latency considerations and guardrails.\n\n## Key Concepts\n\n- phased migration strategy\n- tail latency and cost modeling\n- rollback criteria and canary policies\n\n## Code Example\n\n```javascript\nfunction evaluateFlag(flag, ctx) {\n  // simplified evaluation path\n  const gate = flag.gate || 'centralized';\n  const rules = flag.rules || [];\n  // in a real system this would consult caches and regional data\n  return rules.some(r => ctx.userRoles.includes(r.requiredRole));\n}\n```\n\n## Follow-up Questions\n\n- How would you monitor drift in evaluation accuracy across regions?\n- What metrics would you use to trigger a rollback and how would you implement it?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Hashicorp","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T23:25:36.721Z","createdAt":"2026-01-12T23:25:36.721Z"},{"id":"q-1151","question":"In a real-time analytics system for engagement on a large social app, design a privacy-preserving cohort analytics pipeline that ingests ~3M events/sec with sub-100 ms latency per region. Requirements: data residency, differential privacy for cohort counts, delta- or exact-once streaming state, drift detection, and cost-conscious multi-region deployment. Outline architecture, data contracts, and trade-offs?","answer":"Propose a privacy-preserving real-time cohort analytics pipeline using region-local streaming (e.g., Kafka + Flink), per-region state, and differential privacy (epsilon ~1.0) for cohort counts. Use wi","explanation":"## Why This Is Asked\nTests ability to design a privacy-conscious, low-latency analytics pipeline under multi-region constraints, with DP guarantees and drift detection.\n\n## Key Concepts\n- Real-time streaming with Kafka + Flink\n- Differential privacy (epsilon ~1.0)\n- Region-local state and windowed aggregations (<100 ms)\n- Drift detection and observability\n- Data residency and governance\n\n## Code Example\n```javascript\nfunction addDPNoise(count, epsilon) {\n  const sigma = Math.sqrt(2 / epsilon);\n  const noise = randn_bm() * sigma;\n  return Math.max(0, Math.round(count + noise));\n}\nfunction randn_bm() {\n  let u = 0, v = 0;\n  while (u === 0) u = Math.random();\n  while (v === 0) v = Math.random();\n  return Math.sqrt(-2.0 * Math.log(u)) * Math.cos(2 * Math.PI * v);\n}\n```\n\n## Follow-up Questions\n- How would you validate privacy guarantees in production?\n- What data contracts would you enforce with product teams to prevent over-collection?","diagram":"flowchart TD\n  Ingest[Ingest events] --> Stream[Stream processing]\n  Stream --> Cohort[Cohort analytics]\n  Cohort --> Privacy[DP noise]\n  Cohort --> Drift[Drift detection]\n  Drift --> Observ[Observability]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Meta","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:35:47.807Z","createdAt":"2026-01-13T01:35:47.807Z"},{"id":"q-1209","question":"You’re building an offline-first version of a daily digest app for low-connectivity users. Outline a minimal data model, eviction policy, and a practical plan to quantify cost savings from reduced network usage versus increased storage and complexity; provide a concrete example with rough numbers?","answer":"Use an offline cache storing up to M articles per user with fields: id, title, summary, url, publishedAt, hash. Evict by LRU or per-topic quotas. Background sync uses conditional GET/ETag to update ca","explanation":"## Why This Is Asked\nTests ability to design offline-first cache, quantify trade-offs, and produce concrete numbers relevant to mobile apps.\n\n## Key Concepts\n- Offline-first design\n- Data modeling for cached content\n- Cache eviction strategies (LRU, quota-based)\n- Cost-benefit estimation (bandwidth vs storage)\n\n## Code Example\n```javascript\n// Data model sketch\ntype Article = {\n  id: string;\n  title: string;\n  summary: string;\n  url: string;\n  publishedAt: string;\n  hash: string;\n};\n\nconst CACHE_LIMIT = 50; // per user\nfunction evictLRU(cache: Article[]) {\n  // simplistic placeholder\n}\n```\n\n## Follow-up Questions\n- How would you test offline scenarios across flaky networks?\n- How would you measure real-world bandwidth savings after rollout?","diagram":"flowchart TD\n  A[User opens digest] --> B{Online?}\n  B -- Yes --> C[Fetch latest and update cache]\n  B -- No --> D[Show offline cache]\n  C --> D","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","DoorDash","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T05:24:07.770Z","createdAt":"2026-01-13T05:24:07.770Z"},{"id":"q-1238","question":"You're evaluating a beginner feature: a daily 'Portfolio Health Snap' panel that assigns a risk score to each user based on volatility of top holdings. Do a practical cost-benefit analysis: state assumptions, data/API costs, storage, and engineering time; quantify benefits (retention lift, ARPU) and compute break-even time. Provide rough calculations and rationale?","answer":"Assumptions: 5,000 DAU, one volatility check per user per day; API/data costs $0.0008/call; 10 MB storage; backend effort ~1.5 weeks. Benefits: +3% DAU, +$0.20 ARPU, improved retention by 2%. Break-ev","explanation":"## Why This Is Asked\nTests ability to plan ROI for a data-driven feature, balancing cost and user value, with beginner-friendly data and timing considerations.\n\n## Key Concepts\n- ROI modeling and payback\n- Data sourcing costs and API limits\n- Storage and compute budgeting\n- Validation and lightweight experimentation\n- Privacy and governance\n\n## Code Example\n```javascript\n// Simple break-even calculator\nfunction breakEvenDays(cost, upliftPerUserPerDay, dailyActiveUsers) {\n  const dailyGain = upliftPerUserPerDay * dailyActiveUsers;\n  return dailyGain > 0 ? cost / dailyGain : Infinity;\n}\n```\n\n## Follow-up Questions\n- How would you estimate the uplift values without a full launch?\n- What would be your MVP scope and validation plan?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Square","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T06:34:22.701Z","createdAt":"2026-01-13T06:34:22.701Z"},{"id":"q-1274","question":"You're assessing migrating a high-volume telemetry pipeline from a centralized data warehouse to a lakehouse with streaming ingestion and on-demand materialized views. Current throughput 5M events/sec, latency 5–7 min; target latency 2–3 min, 25% cost increase. Build a 12-month cost-benefit model: incremental storage/compute, streaming infra, data egress, drift/rollback costs; specify decision rules and sensitivity ranges?","answer":"12-month TCO model comparing a centralized data warehouse to a lakehouse with streaming ingestion and on-demand materialized views. Quantify incremental storage/compute, streaming infra (Flink/Spark),","explanation":"## Why This Is Asked\n\nThis question probes ability to design and justify moves to a lakehouse architecture, balancing cost, latency, data freshness, and risk in a measurable way.\n\n## Key Concepts\n\n- TCO modeling across storage, compute, and data transfer\n- Data freshness vs cost and drift/rollback risk\n- Sensitivity analysis and decision rules\n\n## Code Example\n\n```javascript\nfunction npv(cashFlows, rate) {\n  return cashFlows.reduce((acc, v, i) => acc + v / Math.pow(1+rate, i+1), 0);\n}\n```\n\n## Follow-up Questions\n\n- How would you measure and model data drift in this context?\n- What rollout milestones and metrics would trigger a rollback or full migration?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Citadel","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:39:19.754Z","createdAt":"2026-01-13T07:39:19.754Z"},{"id":"q-1403","question":"Design a real-time, fault-tolerant order processing pipeline for a high-traffic ecommerce system. Partition Kafka by user_id, use transactional producers for exactly-once ingestion, and idempotent consumers keyed by event_id. Maintain a durable log, emit to a warehouse via an idempotent sink, and support replay via event sourcing and schema evolution. Include chaos and backpressure tests?","answer":"Design a real-time, fault-tolerant order processing pipeline for a high-traffic ecommerce system. Partition Kafka by user_id, use transactional producers for exactly-once ingestion, and idempotent con","explanation":"## Why This Is Asked\n\nTests ability to design end-to-end, scalable data pipelines with strong correctness guarantees in production.\n\n## Key Concepts\n\n- Exactly-once semantics in Kafka with transactions\n- Per-user ordering via partitioning\n- Idempotent processing and event deduplication\n- Durable logs and write-ahead logging\n- Replayability and schema evolution in sinks\n\n## Code Example\n\n```python\n# simplified idempotent processor\ndef process(event, state, seen):\n    if event.id in seen:\n        return state\n    state = update(state, event)\n    seen.add(event.id)\n    return state\n```\n\n## Follow-up Questions\n\n- How would you test failure modes (partial commits, replay)?\n- How would you monitor lag and backpressure?\n","diagram":"flowchart TD\n  A[UserOrderEvent] --> B[Kafka: orders]\n  B --> C[Processor: per-user state]\n  C --> D[Sink: warehouse]\n  D --> E[Replay/Audit Log]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Instacart","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T15:44:36.305Z","createdAt":"2026-01-13T15:44:36.306Z"},{"id":"q-1472","question":"Design and implement a cost-benefit analysis module for a feature-flag rollout in a global streaming platform. Given 1000 edge regions, 1M users, latency budget 50ms, and known incremental costs, specify a data model, metrics to collect, uplift estimation, and a core function that returns whether to roll out. Include rollout policy and plan for validation via A/B tests in production?","answer":"Net value = (uplift_fraction × ARPU × users) − incremental_cost. If net value ≥ threshold, roll out; else pause. Model uplift with CI and regional variance for staged rollout. Provide a compact functi","explanation":"## Why This Is Asked\nAssess ability to translate business impact into technical rollout decisions, including statistical thinking and cost models.\n\n## Key Concepts\n- Cost-benefit modeling\n- A/B testing & uplift estimation\n- Edge compute cost vs latency trade-offs\n- Rollout policy and monitoring\n\n## Code Example\n```javascript\nfunction shouldRollout(uplift, arpu, users, cost, threshold){\n  const revenue = uplift * arpu * users;\n  const net = revenue - cost;\n  return net >= threshold;\n}\n```\n\n## Follow-up Questions\n- How would you handle uplift uncertainty (CI) in decision?\n- How would you budget rollouts across regions with variable costs?","diagram":"flowchart TD\n  A[Gather metrics] --> B[Compute uplift]\n  B --> C[Compute net value]\n  C --> D[Rollout decision]\n  D --> E[Monitor]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T18:47:18.476Z","createdAt":"2026-01-13T18:47:18.476Z"},{"id":"q-1508","question":"You're deploying a real-time personalization pipeline where a new ML model runs behind a feature flag with a 1% canary. If live drift is detected against offline benchmarks and latency deviates beyond a threshold, outline a concrete plan for rollout control, rollback, and data integrity checks that minimizes user impact while preserving auditability?","answer":"Propose a canary rollout for the new model with 1% traffic and a feature-flag toggle for immediate disable. Implement drift/QA guardrails: offline-live distribution comparison, latency/throughput chec","explanation":"## Why This Is Asked\nTests the ability to manage risk in live ML-style deployments, ensuring data integrity and latency goals while using feature flags and controlled rollouts.\n\n## Key Concepts\n- Canary deployments and feature flags\n- Drift detection and rollback strategies\n- Observability, alerts, and guardrails\n- Auditability and versioning of models/artifacts\n\n## Code Example\n```javascript\n// Pseudo drift check sketch\nfunction driftOk(liveDist, offlineDist, thresh){\n  const kl = klDiv(liveDist, offlineDist);\n  return kl < thresh;\n}\n```\n\n## Follow-up Questions\n- How would you choose drift thresholds? Why different for users vs. admins?\n- How would you structure pre-release validation and rollback testing before full reintroduction?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Apple","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:45:38.432Z","createdAt":"2026-01-13T19:45:38.432Z"},{"id":"q-1531","question":"Context: A streaming feature store for real-time personalization must decide between two deployment models under GDPR/CCPA constraints: (A) cloud-region-centric with EU data replicated to a central US region for global analytics, delivering tail latency 30–70 ms; (B) EU-first on-prem data plane with a lightweight cloud cache for global lookup, targeting 50–120 ms tail latency. Provide a 12–18 month cost model including data residency compliance, data transfer, storage, compute, tooling, outage risk, and a decision rule with sensitivity ranges?","answer":"Compute the 18-month Total Cost of Ownership (TCO) for both deployment models: (A) EU-regional cloud deployment with cross-region replication and analytics; (B) EU-first on-premises data plane with lightweight cloud cache. Decision rule: select the option with lower TCO that maintains 95th percentile latency under 100ms while ensuring full GDPR/CCPA compliance.","explanation":"## Why This Is Asked\n\nTests ability to reason about regulatory constraints, cross-region data flows, and real-time latency versus cost trade-offs in hybrid cloud environments.\n\n## Key Concepts\n\n- Total Cost of Ownership in hybrid cloud architectures\n- Data residency and compliance costs under GDPR/CCPA\n- Tail latency risk management and decision thresholds\n- Cross-region data transfer economics\n\n## Code Example\n\n```javascript\n// Pseudocode for TCO comparison\nfunction tcoA(params) { /* EU region + cross-region egress */ }\nfunction tcoB(params) { /* EU on-prem + cloud cache */ }\n```\n\n## Follow-up Questions","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:35:03.193Z","createdAt":"2026-01-13T20:48:17.164Z"},{"id":"q-1724","question":"You operate a real-time feature store for a global e-commerce platform. Ingested events arrive from multiple regions with clock skew and occasional late arrivals. You must deliver online feature values with tail latency under 100 ms while ensuring correctness when late data retroactively updates aggregates (e.g., cart_value_last_24h). Propose the architecture, covering data versioning, watermarking strategy, exactly-once processing, per-tenant isolation, and testing approach. Include a concrete late-event example and the system’s expected behavior?","answer":"I’d use a stream engine (e.g., Flink) with event-time processing, per-tenant keys, and exactly-once guarantees. Features live in a KV store with versioned rows and TTL, plus a delta store for late dat","explanation":"## Why This Is Asked\nThis assesses ability to design robust real-time feature stores with late data handling, multi-region constraints, and data governance. It also tests correctness guarantees and testability.\n\n## Key Concepts\n- Event-time processing with watermarks\n- Exactly-once semantics and state management\n- Versioned feature rows and delta handling\n- Per-tenant isolation and data residency\n- Backfill and auditability\n\n## Code Example\n```java\n// Flink pseudo-code: assign timestamps and watermarks, handle late data\nDataStream<Event> s = ...;\ns.assignTimestampsAndWatermarks(WatermarkStrategy.forMonotonousTimestamps());\n```\n\n## Follow-up Questions\n- How would you test watermark correctness in CI?\n- How would you validate cross-region consistency without leaking data?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","IBM","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T08:41:49.147Z","createdAt":"2026-01-14T08:41:49.147Z"},{"id":"q-843","question":"Given a CSV file with columns user_id, action, timestamp, write a Python function using only the standard library that returns the most recent action per user by deduplicating on user_id and keeping the latest timestamp; describe time complexity and edge cases?","answer":"Parse the CSV with the csv module, iterate rows, parse timestamp with datetime.fromisoformat, and keep a dict mapping user_id to (timestamp, action). If a row has a newer timestamp, replace the entry.","explanation":"## Why This Is Asked\nTests ability to implement a practical data-processing task: deduplicate by key using timestamps, relies only on standard library, and reveals awareness of edge cases like ties, malformed timestamps, and missing fields.\n\n## Key Concepts\n- CSV parsing with csv module\n- Deduplication by key using a dictionary\n- Timestamp parsing with datetime (ISO 8601)\n- Edge cases: missing fields, invalid timestamps, tie-breaking\n\n## Code Example\n```python\nimport csv\nfrom datetime import datetime\n\ndef latest_actions(csv_path):\n    best = {}\n    with open(csv_path, newline='') as f:\n        rdr = csv.DictReader(f)\n        for row in rdr:\n            uid = row['user_id']\n            ts = row['timestamp']\n            act = row['action']\n            try:\n                t = datetime.fromisoformat(ts)\n            except Exception:\n                continue\n            if uid not in best or t > best[uid][0]:\n                best[uid] = (t, act)\n    return [(uid, act, t.isoformat()) for uid, (t, act) in best.items()]\n```\n\n## Follow-up Questions\n- How would you adapt this for streaming data or files larger than memory?\n- If multiple actions share the same timestamp for a user, how would you break ties and what metadata would you add?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","PayPal","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:27:05.585Z","createdAt":"2026-01-12T13:27:05.585Z"},{"id":"q-872","question":"You are evaluating two deployment options for a new AI inference service under budget and latency constraints. Outline a practical, end-to-end cost-benefit analysis framework to decide which to deploy in production. Include: data you would collect (traffic, latency, SLA penalties, accuracy), metrics (NPV, ROI, payback), horizons, discount rate, handling uncertainty (scenarios), and a concrete calculation workflow you would run?","answer":"I would model total cost as compute, storage, and ops, plus latency penalties; benefits as incremental revenue or user value from faster or more reliable services. I would calculate NPV over 12-24 mon","explanation":"## Why This Is Asked\nTests the ability to structure a pragmatic CBA for ML deployment, balancing cost with user value, handling uncertainty, and communicating the result.\n\n## Key Concepts\n- Cost and benefits modeling\n- Time horizon and discounting\n- Scenario analysis (base/best/worst)\n- Metrics: NPV, ROI, risk-adjusted EV\n\n## Code Example\n```javascript\nfunction npv(cashFlows, rate) {\n  return cashFlows.reduce((acc, cf, i) => acc + cf / Math.pow(1 + rate, i+1), 0);\n}\n```\n\n## Follow-up Questions\n- How would you handle non-linear cost factors (e.g., burst traffic) in your model?\n- How would you present results to non-technical stakeholders?","diagram":"flowchart TD\n  DataInputs[Data Inputs] --> Option[Model Option]\n  Option --> Decision{Decision}\n  Decision --> Deploy[Deploy to Prod]\n  Decision --> Iterate[Iterate Optimization]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:54:31.848Z","createdAt":"2026-01-12T13:54:31.848Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Apple","Bloomberg","Citadel","Coinbase","Databricks","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","PayPal","Robinhood","Scale Ai","Slack","Snap","Snowflake","Square","Tesla","Twitter","Two Sigma","Uber"],"stats":{"total":16,"beginner":4,"intermediate":7,"advanced":5,"newThisWeek":16}}