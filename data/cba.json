{"questions":[{"id":"q-1020","question":"You’re evaluating a beginner feature: a daily market digest in a Robinhood-like app. Do a practical cost‑benefit analysis: state assumptions, estimate data/API costs, storage and engineering time, quantify benefits (retention lift, ARPU), and compute break-even time. Provide a rough calculation and rationale?","answer":"Assume 50k MAU, 4% retention lift, and $0.001 data cost per digest. Costs: data ~$120/mo, storage ~$20, engineering ~16h at $75/h = $1,200. Benefit: 7,500 users x $0.08/mo ARPU uplift for 6 months ≈ $","explanation":"## Why This Is Asked\n\nGauges ability to justify product bets with a pragmatic, numbers-driven approach and to handle uncertainty in inputs.\n\n## Key Concepts\n\n- Cost estimation\n- Benefit estimation\n- Break-even analysis\n- Reasonable assumptions\n\n## Code Example\n\n```javascript\n// Simple CBA calculator\nfunction cba(params) {\n  const {maU, lift, arpu, months, cost} = params;\n  const annualizedBenefit = maU * lift * arpu * months;\n  return {annualizedBenefit, cost, breakevenMonths: cost / (annualizedBenefit || 1)};\n}\n```\n\n## Follow-up Questions\n\n- How would you adjust if MAU grows or adoption changes?\n- What if ARPU uplift is not uniform across user segments?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:33:39.493Z","createdAt":"2026-01-12T19:33:39.493Z"},{"id":"q-1052","question":"Scenario: Real-time ingestion service receives JSON events from edge devices. Guarantee per-user throughput while allowing bursts in a distributed cluster. Design and implement a practical throttling mechanism, specify data structures, atomicity (e.g., Redis Lua script), failure modes, testing strategy, and observability?","answer":"Implement a per-user token bucket in Redis. Refill at 500 tokens/sec with a burst capacity of 1000 tokens (2s burst). On each event, atomically call a Lua script to consume 1 token; if available, forw","explanation":"## Why This Is Asked\n\nThis question tests practical rate-limiting in distributed systems, emphasizing per-user fairness, op readiness, and recovery from clock skew.\n\n## Key Concepts\n\n- Token bucket\n- Redis Lua scripts for atomic ops\n- Burst handling and backpressure\n- Observability and testing\n\n## Code Example\n\n```javascript\n// Redis Lua script (conceptual)\nlocal bucket = KEYS[1]\nlocal tokens = tonumber(redis.call('GET', bucket) or '0')\nlocal refill = tonumber(ARGV[1])\nlocal cap = tonumber(ARGV[2])\nlocal new_tokens = math.min(cap, tokens + refill)\nif new_tokens >= 1 then\n  redis.call('SET', bucket, new_tokens - 1)\n  return 1\nelse\n  return 0\nend\n```\n\n## Follow-up Questions\n\n- How would you test for clock skew and drift?\n- How would you adapt for multi-tenant fairness?\n","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","OpenAI","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:36:38.142Z","createdAt":"2026-01-12T20:36:38.142Z"},{"id":"q-1057","question":"In a real-time feed system using a contextual bandit with attention weighting (CBA), design a policy that balances short-term CTR and long-term engagement. Explain your reward decomposition, exploration strategy, and handling of non-stationarity. How would you validate offline with CPE and ramp online safely? Provide a concise update rule?","answer":"Implement a contextual bandit with attention weights. Reward = alpha * CTR + beta * retention, with alphas learned from data. Use Thompson Sampling or a neural head to estimate rewards. Update theta v","explanation":"## Why This Is Asked\nTests a candidate's ability to design production-ready policies balancing immediate metrics with long-term user value, handling non-stationarity, and validating safely.\n\n## Key Concepts\n- Contextual bandits and attention weighting\n- Reward decomposition for short-term vs long-term goals\n- Exploration strategies (Thompson Sampling, posterior updates)\n- Non-stationarity handling and evaluation (offline CPE, staged online ramp)\n\n## Code Example\n```python\n# Pseudo update step for theta\nimport numpy as np\n\ndef update_theta(theta, phi, reward, lr):\n    pred = np.dot(theta, phi)\n    grad = (reward - pred) * phi\n    theta += lr * grad\n    return theta\n```\n\n## Follow-up Questions\n- How would you handle feature drift in attention weights?\n- What offline metrics would you trust for CPE in this setup?","diagram":"flowchart TD\n  Context[Context] --> Features[Compute features with attention]\n  Features --> Decide[Policy selects action]\n  Decide --> Reward[Observe reward]\n  Reward --> Update[Update model]\n  Update --> Evaluation[Online/Offline evaluation]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Microsoft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:18:55.668Z","createdAt":"2026-01-12T21:18:55.668Z"},{"id":"q-1084","question":"Given a large social network planning to adopt a real-time feature flag evaluation service that runs on a hybrid stream/batch pipeline. Current pipeline: 1.2M events/sec, median latency 50 ms, 5% tail. New service promises 20–30% latency reduction and 25% cost increase, plus migration risk. Perform a cost-benefit analysis: quantify costs, benefits, risks, horizon (12 months), and decision rule with sensitivity ranges?","answer":"Quantify TCO over 12 months: infra/licensing, migration, and ops for both pipelines; quantify benefits from latency/throughput gains (fewer SLA penalties, higher retention, lower cost per event); incl","explanation":"## Why This Is Asked\n\nTests ability to perform a structured CBA for a real-time system, balancing latency impact, migration risk, and total cost of ownership. It also probes framing the horizon, risk quantification, and decision criteria.\n\n## Key Concepts\n\n- TCO, NPV, IRR\n- Latency vs cost trade-offs\n- Migration risk, rollback plans, and governance\n\n## Code Example\n\n```javascript\n// Pseudocode: simple NPV calculation\nfunction npv(r, cashFlows) {\n  return cashFlows.reduce((acc, cf, i) => acc + cf / Math.pow(1+r, i+1), 0)\n}\n```\n\n## Follow-up Questions\n\n- How would you model outsized risk of outages?\n- How would you present to executives under uncertainty?\n","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:18:59.423Z","createdAt":"2026-01-12T22:18:59.423Z"},{"id":"q-1114","question":"You’re migrating a real-time feature flag engine (multi-tenant SaaS) to reduce tail latency and cost. Propose a three-phase migration: centralized eval, regional edge gateway, then hybrid routing with per-tenant caches. Specify success metrics, rollback criteria, and a 12-month plan?","answer":"Propose a phased migration: 1) centralized eval gate, 2) regional edge gateways, 3) hybrid routing with per-tenant caches and rule-level debouncing. Metrics: P99 latency <15 ms, 99.9th <40 ms, cost pe","explanation":"## Why This Is Asked\n\nAssesses ability to design staged rollouts with measurable SLAs, cost models, and risk controls; emphasizes regional latency considerations and guardrails.\n\n## Key Concepts\n\n- phased migration strategy\n- tail latency and cost modeling\n- rollback criteria and canary policies\n\n## Code Example\n\n```javascript\nfunction evaluateFlag(flag, ctx) {\n  // simplified evaluation path\n  const gate = flag.gate || 'centralized';\n  const rules = flag.rules || [];\n  // in a real system this would consult caches and regional data\n  return rules.some(r => ctx.userRoles.includes(r.requiredRole));\n}\n```\n\n## Follow-up Questions\n\n- How would you monitor drift in evaluation accuracy across regions?\n- What metrics would you use to trigger a rollback and how would you implement it?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Hashicorp","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:25:36.721Z","createdAt":"2026-01-12T23:25:36.721Z"},{"id":"q-1151","question":"In a real-time analytics system for engagement on a large social app, design a privacy-preserving cohort analytics pipeline that ingests ~3M events/sec with sub-100 ms latency per region. Requirements: data residency, differential privacy for cohort counts, delta- or exact-once streaming state, drift detection, and cost-conscious multi-region deployment. Outline architecture, data contracts, and trade-offs?","answer":"Propose a privacy-preserving real-time cohort analytics pipeline using region-local streaming (e.g., Kafka + Flink), per-region state, and differential privacy (epsilon ~1.0) for cohort counts. Use wi","explanation":"## Why This Is Asked\nTests ability to design a privacy-conscious, low-latency analytics pipeline under multi-region constraints, with DP guarantees and drift detection.\n\n## Key Concepts\n- Real-time streaming with Kafka + Flink\n- Differential privacy (epsilon ~1.0)\n- Region-local state and windowed aggregations (<100 ms)\n- Drift detection and observability\n- Data residency and governance\n\n## Code Example\n```javascript\nfunction addDPNoise(count, epsilon) {\n  const sigma = Math.sqrt(2 / epsilon);\n  const noise = randn_bm() * sigma;\n  return Math.max(0, Math.round(count + noise));\n}\nfunction randn_bm() {\n  let u = 0, v = 0;\n  while (u === 0) u = Math.random();\n  while (v === 0) v = Math.random();\n  return Math.sqrt(-2.0 * Math.log(u)) * Math.cos(2 * Math.PI * v);\n}\n```\n\n## Follow-up Questions\n- How would you validate privacy guarantees in production?\n- What data contracts would you enforce with product teams to prevent over-collection?","diagram":"flowchart TD\n  Ingest[Ingest events] --> Stream[Stream processing]\n  Stream --> Cohort[Cohort analytics]\n  Cohort --> Privacy[DP noise]\n  Cohort --> Drift[Drift detection]\n  Drift --> Observ[Observability]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Meta","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:35:47.807Z","createdAt":"2026-01-13T01:35:47.807Z"},{"id":"q-1209","question":"You’re building an offline-first version of a daily digest app for low-connectivity users. Outline a minimal data model, eviction policy, and a practical plan to quantify cost savings from reduced network usage versus increased storage and complexity; provide a concrete example with rough numbers?","answer":"Use an offline cache storing up to M articles per user with fields: id, title, summary, url, publishedAt, hash. Evict by LRU or per-topic quotas. Background sync uses conditional GET/ETag to update ca","explanation":"## Why This Is Asked\nTests ability to design offline-first cache, quantify trade-offs, and produce concrete numbers relevant to mobile apps.\n\n## Key Concepts\n- Offline-first design\n- Data modeling for cached content\n- Cache eviction strategies (LRU, quota-based)\n- Cost-benefit estimation (bandwidth vs storage)\n\n## Code Example\n```javascript\n// Data model sketch\ntype Article = {\n  id: string;\n  title: string;\n  summary: string;\n  url: string;\n  publishedAt: string;\n  hash: string;\n};\n\nconst CACHE_LIMIT = 50; // per user\nfunction evictLRU(cache: Article[]) {\n  // simplistic placeholder\n}\n```\n\n## Follow-up Questions\n- How would you test offline scenarios across flaky networks?\n- How would you measure real-world bandwidth savings after rollout?","diagram":"flowchart TD\n  A[User opens digest] --> B{Online?}\n  B -- Yes --> C[Fetch latest and update cache]\n  B -- No --> D[Show offline cache]\n  C --> D","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","DoorDash","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:24:07.770Z","createdAt":"2026-01-13T05:24:07.770Z"},{"id":"q-1238","question":"You're evaluating a beginner feature: a daily 'Portfolio Health Snap' panel that assigns a risk score to each user based on volatility of top holdings. Do a practical cost-benefit analysis: state assumptions, data/API costs, storage, and engineering time; quantify benefits (retention lift, ARPU) and compute break-even time. Provide rough calculations and rationale?","answer":"Assumptions: 5,000 DAU, one volatility check per user per day; API/data costs $0.0008/call; 10 MB storage; backend effort ~1.5 weeks. Benefits: +3% DAU, +$0.20 ARPU, improved retention by 2%. Break-ev","explanation":"## Why This Is Asked\nTests ability to plan ROI for a data-driven feature, balancing cost and user value, with beginner-friendly data and timing considerations.\n\n## Key Concepts\n- ROI modeling and payback\n- Data sourcing costs and API limits\n- Storage and compute budgeting\n- Validation and lightweight experimentation\n- Privacy and governance\n\n## Code Example\n```javascript\n// Simple break-even calculator\nfunction breakEvenDays(cost, upliftPerUserPerDay, dailyActiveUsers) {\n  const dailyGain = upliftPerUserPerDay * dailyActiveUsers;\n  return dailyGain > 0 ? cost / dailyGain : Infinity;\n}\n```\n\n## Follow-up Questions\n- How would you estimate the uplift values without a full launch?\n- What would be your MVP scope and validation plan?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Square","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:34:22.701Z","createdAt":"2026-01-13T06:34:22.701Z"},{"id":"q-1274","question":"You're assessing migrating a high-volume telemetry pipeline from a centralized data warehouse to a lakehouse with streaming ingestion and on-demand materialized views. Current throughput 5M events/sec, latency 5–7 min; target latency 2–3 min, 25% cost increase. Build a 12-month cost-benefit model: incremental storage/compute, streaming infra, data egress, drift/rollback costs; specify decision rules and sensitivity ranges?","answer":"12-month TCO model comparing a centralized data warehouse to a lakehouse with streaming ingestion and on-demand materialized views. Quantify incremental storage/compute, streaming infra (Flink/Spark),","explanation":"## Why This Is Asked\n\nThis question probes ability to design and justify moves to a lakehouse architecture, balancing cost, latency, data freshness, and risk in a measurable way.\n\n## Key Concepts\n\n- TCO modeling across storage, compute, and data transfer\n- Data freshness vs cost and drift/rollback risk\n- Sensitivity analysis and decision rules\n\n## Code Example\n\n```javascript\nfunction npv(cashFlows, rate) {\n  return cashFlows.reduce((acc, v, i) => acc + v / Math.pow(1+rate, i+1), 0);\n}\n```\n\n## Follow-up Questions\n\n- How would you measure and model data drift in this context?\n- What rollout milestones and metrics would trigger a rollback or full migration?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Citadel","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:39:19.754Z","createdAt":"2026-01-13T07:39:19.754Z"},{"id":"q-1403","question":"Design a real-time, fault-tolerant order processing pipeline for a high-traffic ecommerce system. Partition Kafka by user_id, use transactional producers for exactly-once ingestion, and idempotent consumers keyed by event_id. Maintain a durable log, emit to a warehouse via an idempotent sink, and support replay via event sourcing and schema evolution. Include chaos and backpressure tests?","answer":"Design a real-time, fault-tolerant order processing pipeline for a high-traffic ecommerce system. Partition Kafka by user_id, use transactional producers for exactly-once ingestion, and idempotent con","explanation":"## Why This Is Asked\n\nTests ability to design end-to-end, scalable data pipelines with strong correctness guarantees in production.\n\n## Key Concepts\n\n- Exactly-once semantics in Kafka with transactions\n- Per-user ordering via partitioning\n- Idempotent processing and event deduplication\n- Durable logs and write-ahead logging\n- Replayability and schema evolution in sinks\n\n## Code Example\n\n```python\n# simplified idempotent processor\ndef process(event, state, seen):\n    if event.id in seen:\n        return state\n    state = update(state, event)\n    seen.add(event.id)\n    return state\n```\n\n## Follow-up Questions\n\n- How would you test failure modes (partial commits, replay)?\n- How would you monitor lag and backpressure?\n","diagram":"flowchart TD\n  A[UserOrderEvent] --> B[Kafka: orders]\n  B --> C[Processor: per-user state]\n  C --> D[Sink: warehouse]\n  D --> E[Replay/Audit Log]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Instacart","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T15:44:36.305Z","createdAt":"2026-01-13T15:44:36.306Z"},{"id":"q-1472","question":"Design and implement a cost-benefit analysis module for a feature-flag rollout in a global streaming platform. Given 1000 edge regions, 1M users, latency budget 50ms, and known incremental costs, specify a data model, metrics to collect, uplift estimation, and a core function that returns whether to roll out. Include rollout policy and plan for validation via A/B tests in production?","answer":"Net value = (uplift_fraction × ARPU × users) − incremental_cost. If net value ≥ threshold, roll out; else pause. Model uplift with CI and regional variance for staged rollout. Provide a compact functi","explanation":"## Why This Is Asked\nAssess ability to translate business impact into technical rollout decisions, including statistical thinking and cost models.\n\n## Key Concepts\n- Cost-benefit modeling\n- A/B testing & uplift estimation\n- Edge compute cost vs latency trade-offs\n- Rollout policy and monitoring\n\n## Code Example\n```javascript\nfunction shouldRollout(uplift, arpu, users, cost, threshold){\n  const revenue = uplift * arpu * users;\n  const net = revenue - cost;\n  return net >= threshold;\n}\n```\n\n## Follow-up Questions\n- How would you handle uplift uncertainty (CI) in decision?\n- How would you budget rollouts across regions with variable costs?","diagram":"flowchart TD\n  A[Gather metrics] --> B[Compute uplift]\n  B --> C[Compute net value]\n  C --> D[Rollout decision]\n  D --> E[Monitor]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T18:47:18.476Z","createdAt":"2026-01-13T18:47:18.476Z"},{"id":"q-1508","question":"You're deploying a real-time personalization pipeline where a new ML model runs behind a feature flag with a 1% canary. If live drift is detected against offline benchmarks and latency deviates beyond a threshold, outline a concrete plan for rollout control, rollback, and data integrity checks that minimizes user impact while preserving auditability?","answer":"Propose a canary rollout for the new model with 1% traffic and a feature-flag toggle for immediate disable. Implement drift/QA guardrails: offline-live distribution comparison, latency/throughput chec","explanation":"## Why This Is Asked\nTests the ability to manage risk in live ML-style deployments, ensuring data integrity and latency goals while using feature flags and controlled rollouts.\n\n## Key Concepts\n- Canary deployments and feature flags\n- Drift detection and rollback strategies\n- Observability, alerts, and guardrails\n- Auditability and versioning of models/artifacts\n\n## Code Example\n```javascript\n// Pseudo drift check sketch\nfunction driftOk(liveDist, offlineDist, thresh){\n  const kl = klDiv(liveDist, offlineDist);\n  return kl < thresh;\n}\n```\n\n## Follow-up Questions\n- How would you choose drift thresholds? Why different for users vs. admins?\n- How would you structure pre-release validation and rollback testing before full reintroduction?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Apple","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T19:45:38.432Z","createdAt":"2026-01-13T19:45:38.432Z"},{"id":"q-1531","question":"Context: A streaming feature store for real-time personalization must decide between two deployment models under GDPR/CCPA constraints: (A) cloud-region-centric with EU data replicated to a central US region for global analytics, delivering tail latency 30–70 ms; (B) EU-first on-prem data plane with a lightweight cloud cache for global lookup, targeting 50–120 ms tail latency. Provide a 12–18 month cost model including data residency compliance, data transfer, storage, compute, tooling, outage risk, and a decision rule with sensitivity ranges?","answer":"Compute the 18-month Total Cost of Ownership (TCO) for both deployment models: (A) EU-regional cloud deployment with cross-region replication and analytics; (B) EU-first on-premises data plane with lightweight cloud cache. Decision rule: select the option with lower TCO that maintains 95th percentile latency under 100ms while ensuring full GDPR/CCPA compliance.","explanation":"## Why This Is Asked\n\nTests ability to reason about regulatory constraints, cross-region data flows, and real-time latency versus cost trade-offs in hybrid cloud environments.\n\n## Key Concepts\n\n- Total Cost of Ownership in hybrid cloud architectures\n- Data residency and compliance costs under GDPR/CCPA\n- Tail latency risk management and decision thresholds\n- Cross-region data transfer economics\n\n## Code Example\n\n```javascript\n// Pseudocode for TCO comparison\nfunction tcoA(params) { /* EU region + cross-region egress */ }\nfunction tcoB(params) { /* EU on-prem + cloud cache */ }\n```\n\n## Follow-up Questions","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:35:03.193Z","createdAt":"2026-01-13T20:48:17.164Z"},{"id":"q-1724","question":"You operate a real-time feature store for a global e-commerce platform. Ingested events arrive from multiple regions with clock skew and occasional late arrivals. You must deliver online feature values with tail latency under 100 ms while ensuring correctness when late data retroactively updates aggregates (e.g., cart_value_last_24h). Propose the architecture, covering data versioning, watermarking strategy, exactly-once processing, per-tenant isolation, and testing approach. Include a concrete late-event example and the system’s expected behavior?","answer":"I’d use a stream engine (e.g., Flink) with event-time processing, per-tenant keys, and exactly-once guarantees. Features live in a KV store with versioned rows and TTL, plus a delta store for late dat","explanation":"## Why This Is Asked\nThis assesses ability to design robust real-time feature stores with late data handling, multi-region constraints, and data governance. It also tests correctness guarantees and testability.\n\n## Key Concepts\n- Event-time processing with watermarks\n- Exactly-once semantics and state management\n- Versioned feature rows and delta handling\n- Per-tenant isolation and data residency\n- Backfill and auditability\n\n## Code Example\n```java\n// Flink pseudo-code: assign timestamps and watermarks, handle late data\nDataStream<Event> s = ...;\ns.assignTimestampsAndWatermarks(WatermarkStrategy.forMonotonousTimestamps());\n```\n\n## Follow-up Questions\n- How would you test watermark correctness in CI?\n- How would you validate cross-region consistency without leaking data?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","IBM","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T08:41:49.147Z","createdAt":"2026-01-14T08:41:49.147Z"},{"id":"q-1817","question":"Design and describe a scalable, fault-tolerant real-time collaboration pipeline: ingest per-document operations via a durable queue (Kafka), assign strict sequence numbers, and ensure exactly-once processing with idempotent workers. Explain data model, ordering guarantees, CRDTs vs OT, cross-region replication, failure modes, tests, and rollback strategy. How would you implement end-to-end?","answer":"Architect a scalable, fault-tolerant real-time collaboration pipeline: ingest per-document ops via a durable queue (Kafka), assign strict sequence numbers, and ensure exactly-once processing with idem","explanation":"## Why This Is Asked\n\nThis question probes the ability to design scalable, fault-tolerant real-time collaboration systems and reason about ordering, replication, and failure modes.\n\n## Key Concepts\n\n- Durable messaging and exactly-once processing\n- Temporal ordering and sequencing\n- CRDTs vs operational transformation\n- Cross-region replication and rollback\n\n## Code Example\n\n```javascript\nfunction applyOp(state, op) {\n  const key = op.docId + ':' + op.opId;\n  if (state.applied.has(key)) return state;\n  // apply op to CRDT state\n  state.applied.add(key);\n  // ... apply op\n  return state;\n}\n```\n\n## Follow-up Questions\n\n- How would you test idempotency guarantees under failure?","diagram":"flowchart TD\n  A[Client Editors] --> B[Ingestion Layer (Kafka)]\n  B --> C[Sequencer / Ordering]\n  C --> D[Event Store / CRDT State]\n  D --> E[Clients]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T11:51:17.817Z","createdAt":"2026-01-14T11:51:17.818Z"},{"id":"q-1874","question":"You’re evaluating a beginner feature: an in-app guided onboarding widget for a CRM product. Do a practical cost-benefit analysis: state assumptions, estimate data/hosting costs, content creation time, estimate uplift in activation and paid conversion, and compute break-even time. Provide a rough calculation and rationale?","answer":"Assume 5,000 monthly active users, 4% faster activation and 2% higher paid conversion due to guided tips, ARPU $25/mo. Costs: 2 engineers for 2 weeks at $70/hr ($4,480), content authoring $1,200, host","explanation":"## Why This Is Asked\nTests ability to translate a feature idea into a tangible cost-benefit plan, including rough inputs, trade-offs, and payback.\n\n## Key Concepts\n- Cost-benefit analysis\n- Activation and conversion uplift\n- ARPU and payback period\n- TCO: dev time, content, hosting, analytics\n\n## Code Example\n\n```javascript\nfunction breakEven(cost, monthlyRevenue) {\n  if (monthlyRevenue <= 0) return Infinity;\n  return cost / monthlyRevenue;\n}\n```\n\n## Follow-up Questions\n- How would you validate the uplift assumptions before full rollout?\n- What experiments would you run to de-risk the estimate (A/B tests, cohort analysis)?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T15:35:27.799Z","createdAt":"2026-01-14T15:35:27.800Z"},{"id":"q-1963","question":"How would you design a practical 1-day task to ingest daily payments from an Oracle source CSV into Snowflake, ensuring idempotent loads, proper schema mapping, and data quality checks, with a minimal Python MERGE-based load skeleton by transaction_id and a quick test plan?","answer":"Approach: stage the CSV in Snowflake, map fields to transaction_id, amount DECIMAL, status VARCHAR, tx_time TIMESTAMP. Use MERGE into analytics.transactions on transaction_id to upsert; set updated_at","explanation":"## Why This Is Asked\nAssesses practical data ingestion, idempotent upserts, and schema mapping skills suitable for junior roles.\n\n## Key Concepts\n- Data ingestion pipelines\n- Idempotent upserts with MERGE\n- Schema mapping and data quality checks\n- Uniqueness and deduplication\n\n## Code Example\n```javascript\n// Skeleton: upsert via MERGE using a Snowflake connection\nfunction upsertPayments(conn, stagingTable, targetTable) {\n  // Connect and issue MERGE ...\n}\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data or schema changes?\n- How would you test with small synthetic datasets?","diagram":"flowchart TD\n  A[Ingest daily CSV] --> B[Stage to Snowflake]\n  B --> C[MERGE into canonical.transactions by transaction_id]\n  C --> D[Update updated_at and status]\n  D --> E[Quality checks]","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Snowflake","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T18:57:29.988Z","createdAt":"2026-01-14T18:57:29.988Z"},{"id":"q-1989","question":"You're evaluating replacing a centralized analytics pipeline with a federated learning-based recommendation model deployed on-device across 3 regions for 5M MAU; build a 2-year cost-benefit model including capex, opex, data-transfer savings, regulatory risk penalties, and uplift in engagement/ARPU, and specify the break-even horizon and go/no-go criteria?","answer":"Assume 5M MAU across three regions. Federated on-device inference cuts data transfer by 60% and privacy penalties by 0.5% of annual revenue. Capex $3M; Opex $2M over 2 years. Uplift: ARPU +$0.02, rete","explanation":"## Why This Is Asked\n\nTests ability to craft a quantified cost-benefit analysis for a privacy-first architecture, balancing engineering trade-offs with business impact across regions.\n\n## Key Concepts\n\n- Federated learning and on-device inference\n- Cost modeling (capex, opex, data-transfer savings)\n- Regulatory/privacy risk and penalties\n- ROI metrics (IRR, break-even, payback)\n- Multi-region scalability and latency considerations\n\n## Code Example\n\n```javascript\n// Minimal CBA calculator skeleton\nfunction cba({cost, benefit, horizon}) {\n  const net = benefit - cost;\n  const irr = (net / cost) * 100; // simplified\n  return {net, irr};\n}\n```\n\n## Follow-up Questions\n\n- How would you adjust if MAU grows to 10M within the horizon?\n- How do regional cost differences and latency constraints alter the model?\n- What experiments would you run to validate key assumptions before committing funds?","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T19:43:51.359Z","createdAt":"2026-01-14T19:43:51.359Z"},{"id":"q-2005","question":"In a 3-site edge anomaly-detection rollout for 10k sensors per site (roughly 100 GB/day), design a 2-year cost-benefit analysis for an on-device autoencoder with federated updates to a central model. Include capex for edge hardware, opex for cloud training and data transfer, and quantified benefits from reduced downtime, maintenance savings, and MTBF uplift. Provide break-even horizon and go/no-go criteria?","answer":"Assume 3 sites, 10k sensors/site, 100 GB/day. Capex: edge devices $1.2k each + gateways $3k/site. Opex: cloud training $0.20/GB, data transfer $0.05/GB/mo, ops $40k/yr. Benefits: downtime saved $150k/","explanation":"## Why This Is Asked\n\nThis question tests the ability to translate a complex edge-ML rollout into a precise 2-year CBA, balancing hardware, cloud costs, and real-world benefits like downtime and maintenance savings.\n\n## Key Concepts\n\n- Edge computing and on-device training\n- Federated learning and privacy implications\n- CAPEX vs OPEX modeling\n- Downtime reduction and MTBF uplift as financial drivers\n- Handling non-IID data and drift in multi-site deployments\n\n## Code Example\n\n```javascript\nfunction estimateROI(capex, opexPerMonth, savingsPerMonth, horizonMonths) {\n  const totalOpex = opexPerMonth * horizonMonths;\n  const totalCost = capex + totalOpex;\n  const totalSavings = savingsPerMonth * horizonMonths;\n  const roi = (totalSavings - totalCost) / totalCost;\n  return { totalCost, totalSavings, roi };\n}\n```\n\n## Follow-up Questions\n\n- How would you monitor and mitigate model drift across sites?\n- What metrics would you track to validate the ROI assumptions and trigger governance gates?","diagram":"flowchart TD\n  A[Edge sensors (10k/site)] --> B[Edge gateway/computation]\n  B --> C[On-device autoencoder]\n  C --> D[Federated update to central model]\n  D --> E[Central model synchronization]\n  E --> F[Operational analytics & ROI tracking]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T20:42:25.190Z","createdAt":"2026-01-14T20:42:25.190Z"},{"id":"q-2057","question":"You’re evaluating a starter feature named 'Retention Policy Engine' for a Slack/HashiCorp-like chat app that auto-purges messages older than a per-channel window, with per-user holds and legal holds; design a 2-year cost-benefit model, including storage savings, indexing/compute overhead, engineering time, potential compliance penalties avoided, and a go/no-go break-even horizon?","answer":"Assume 5M MAU generating 1B messages/year (~1 TB data). Implementing per-channel retention windows of 12–24 months reduces active storage by approximately 60%, yielding ~$250K annual savings. The implementation requires ~$150K in engineering costs. Break-even occurs at ~2.5 years.","explanation":"## Why This Is Asked\nTests ability to translate retention policy decisions into a practical cost-benefit model, considering compliance, data safety, and operational impact.\n\n## Key Concepts\n- Data retention policy design for multi-tenant chat\n- Storage and compute cost modeling\n- Compliance, legal holds, and searchability trade-offs\n\n## Code Example\n\n```javascript\n// Simple retention calculation sketch\nfunction estMonthlyStorage(msgPerMonth, avgKB){\n  return (msgPerMonth * avgKB) / 1024; // GB\n}\n```\n\n## Follow-up Questions\n- How would you enforce per-channel legal holds without blocking purges?","diagram":"flowchart TD\n  A[Channel retention defined] --> B[Policy per channel stored]\n  B --> C[Scheduler triggers purge]\n  C --> D[Purge old messages, apply holds]\n  D --> E[Audit log and metrics updated]","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:51:08.299Z","createdAt":"2026-01-14T22:44:32.937Z"},{"id":"q-2086","question":"You're evaluating a privacy-preserving on-device NLP summarizer that runs on user devices with federated updates to a central model via secure aggregation. Design a 2-year cost-benefit model for deploying across Android and iOS in 4 regions with ~25M MAU. Include capex for secure enclaves and on-device storage, opex for federated aggregation and governance, update bandwidth, regulatory penalties for leakage, and uplift in engagement. Provide break-even and go/no-go criteria?","answer":"A comprehensive 2-year cost-benefit model comparing capital expenditures for secure enclaves and on-device storage against operational expenses for federated aggregation, while quantifying engagement uplift, regulatory risk mitigation, and ROI break-even analysis for cross-platform deployment.","explanation":"## Why This Is Asked\n\nAssesses the ability to construct a realistic, privacy-focused financial model for on-device machine learning at scale, balancing device-level constraints with centralized governance and regulatory compliance.\n\n## Key Concepts\n\n- On-device ML with secure federated aggregation\n- Edge infrastructure and enclave security costs\n- Federated update bandwidth and governance overhead\n- Data privacy penalties and regulatory risk exposure\n- Revenue uplift modeling and ROI break-even analysis\n\n## Code Example\n\n```javascript\n// Rough break-even calculator (simplified)\nfunction breakEvenCalculator(mau, regions, years) {\n  // Capex: Secure enclaves + on-device storage\n  const capex = {\n    enclaves: 5000000, // $5M for secure enclave infrastructure\n    storage: 2000000   // $2M for on-device storage optimization\n  };\n  \n  // Opex: Federated aggregation + governance + bandwidth\n  const opex = {\n    aggregation: 1000000, // $1M/year for federated learning\n    governance: 500000,   // $500K/year for privacy compliance\n    bandwidth: 300000     // $300K/year for update distribution\n  };\n  \n  // Revenue uplift from improved engagement\n  const engagementUplift = 0.05; // 5% increase in user engagement\n  const avgRevenuePerUser = 12; // $12/year average revenue\n  \n  // Calculate break-even point\n  const totalCapex = capex.enclaves + capex.storage;\n  const annualOpex = opex.aggregation + opex.governance + opex.bandwidth;\n  const annualRevenue = mau * avgRevenuePerUser * engagementUplift;\n  \n  const breakEvenMonths = Math.ceil(totalCapex / (annualRevenue - annualOpex) * 12);\n  \n  return {\n    totalInvestment: totalCapex + (annualOpex * years),\n    projectedRevenue: annualRevenue * years,\n    breakEvenMonths,\n    roi: ((annualRevenue * years - totalCapex - (annualOpex * years)) / \n          (totalCapex + (annualOpex * years))) * 100\n  };\n}\n```\n\n## Go/No-Go Criteria\n\n**Go Decision:**\n- Break-even < 18 months\n- ROI > 25% over 2 years\n- Regulatory risk reduction > 40%\n- Engagement uplift > 3%\n\n**No-Go Triggers:**\n- Break-even > 24 months\n- ROI < 15% over 2 years\n- Regional regulatory complexity > 3 jurisdictions\n- Device compatibility < 85% of target MAU","diagram":"flowchart TD\n  A[User devices] --> B[On-device NLP model]\n  B --> C[Secure aggregation hub]\n  C --> D[Central governance]\n  D --> E[Cost & benefit tracking]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","MongoDB","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:04:11.150Z","createdAt":"2026-01-14T23:36:57.391Z"},{"id":"q-2218","question":"You're evaluating a beginner feature: an in-app, offline-first guided onboarding using a localized FAQ (5 topics) and 3 short tutorial videos to cut first-week support tickets. Build a 2-year cost-benefit model: content creation time, local storage impact per user, update/hosting costs, uplift in activation, and reduction in first-week tickets; compute break-even horizon?","answer":"Assume 1M MAU, 2 MB local storage per user, 3 videos (0.8–1 MB each) cached offline, and 5 FAQ entries at ~1 KB. Content creation ~320 hours. Hosting/updates ~$0.01/user/month. Activation uplift 6–10%","explanation":"## Why This Is Asked\nTests ability to translate product impact into a financial model with explicit inputs, unit economics, and a concrete go/no-go horizon.\n\n## Key Concepts\n- Cost categorization: content, storage, hosting, engineering\n- Benefit metrics: activation uplift, ticket reduction\n- Timing: 24-month horizon, rollout impact\n\n## Code Example\n```javascript\nfunction breakEven(monthlyBenefit, upfrontCost) {\n  return upfrontCost / monthlyBenefit;\n}\n```\n\n## Follow-up Questions\n- How would you adjust when growth slows or uptake is uneven across regions?\n- What sensitivities would you run to validate the model?","diagram":"flowchart TD\n  A[Onboarding feature] --> B[Offline FAQ & videos cached]\n  B --> C[Reduced support tickets]\n  C --> D[Increased activation and retention]\n  D --> E[2-year cost-benefit break-even]","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Amazon","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T07:45:12.512Z","createdAt":"2026-01-15T07:45:12.512Z"},{"id":"q-2256","question":"You're evaluating a privacy-preserving on-device recommender for a mobile wallet app to surface merchant offers. Use federated learning with differential privacy across 3 regions and 5M MAU. Build a 2-year cost-benefit: edge hardware/storage, server aggregation, data transfer, content updates, regulatory penalties, activation uplift, ARPU uplift, and support costs. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","answer":"Two-year plan for a privacy-preserving on-device recommender in a payments app. 5M MAU across 3 regions; federated learning with differential privacy; estimate edge compute, server aggregation, and da","explanation":"## Why This Is Asked\nTests ability to build a complete, quantitative business case for privacy-conscious on-device ML at scale, including regulatory risk, multi-region considerations, and real-world KPIs.\n\n## Key Concepts\n- Federated learning with differential privacy\n- Edge compute vs. cloud aggregation trade-offs\n- Multi-region cost modeling (edge, bandwidth, infra)\n- Activation/ARPU uplift vs. risk penalties\n\n## Code Example\n```javascript\nfunction breakEven({activationUpliftPct, arpuLift, edgeCost, serverCost, penalty, months}) {\n  const revenue = (activationUpliftPct/100) * arpuLift * 1e6 * months;\n  const cost = (edgeCost + serverCost) * months + penalty;\n  return revenue - cost;\n}\n```\n\n## Follow-up Questions\n- How would you adjust the model if ARPU uplift is nonlinear?\n- Which metrics and signals would trigger a go/no-go decision in production?","diagram":"flowchart TD\n  A[On-device FL DP] --> B[Edge compute & storage]\n  A --> C[Server aggregation]\n  B --> D[Cost: device HW, energy]\n  C --> E[Cost: cloud infra, data transfer]\n  D --> F[Impact: activation uplift]\n  E --> F\n  F --> G[Break-even horizon]\n  G --> H[Go/No-Go]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","OpenAI","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T09:41:05.141Z","createdAt":"2026-01-15T09:41:05.141Z"},{"id":"q-2429","question":"You're deploying an on-device, privacy-preserving content moderation assistant for real-time group chats at Microsoft/Discord scale. It runs a distilled transformer on user devices to flag policy-violating messages, with periodic federated updates to keep the model aligned. Build a 2-year cost-benefit model: device storage and energy, on-device latency, update bandwidth, cloud aggregation costs, regulatory risk penalties, and expected safety/retention uplift. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","answer":"2-year TCO = device storage (2 MB/model × MAU) + on-device energy (avg ~0.2 W active) × active hours + federation updates (10 KB/week × MAU) + cloud aggregation costs. Benefit: reduced escalations, im","explanation":"## Why This Is Asked\n\nAssesss ability to translate privacy-preserving ML deployments into a lifecycle cost model and tie it to business outcomes.\n\n## Key Concepts\n- On-device ML, model distillation, federated updates\n- Cost modeling: storage, energy, bandwidth, cloud costs\n- Risk & mitigations: drift, false positives, DP\n\n## Code Example\n\n```javascript\n// Pseudocode for cost model calc\n```\n\n## Follow-up Questions\n- How would you validate model drift in production?\n- What opt-out metrics would you track and why?","diagram":"flowchart TD\n  A[Input Message] --> B[On-device Inference]\n  B --> C{Flagged?}\n  C -->|Yes| D[Mask/Flag Message]\n  C -->|No| E[Pass]\n","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T17:48:48.262Z","createdAt":"2026-01-15T17:48:48.262Z"},{"id":"q-2690","question":"You’re evaluating a new Data Quality as a Service (DQaaS) layer for a lakehouse used by Snowflake, Uber, and Databricks. The plan is to run continuous profiling, schema drift detection, and automated remediation across 4 domains (orders, users, payments, events) with 3 prod regions. Build a 12–18 month cost-benefit model detailing profiling compute, metadata storage, alerting, remediation actions, and data egress; include break-even horizon, go/no-go criteria, and 3–5 concrete quality rules with thresholds?","answer":"Propose a 12–18 month model: profiling compute (per-domain, per-region), metadata/storage for lineage, alerting, remediation automation, and cross-region data egress. 5 concrete rules: null rate >5%, ","explanation":"## Why This Is Asked\nTests ability to translate a business goal into a concrete cost-benefit plan for a lakehouse-quality feature, balancing compute/storage costs with real adoption gains and risk controls.\n\n## Key Concepts\n- Data quality rules and thresholds\n- Profiling compute and scheduling costs\n- Metadata storage and lineage impact\n- Automated remediation and guardrails\n- Break-even, ROI, and risk mitigations\n\n## Code Example\n```javascript\nfunction driftDetected(currentType, expectedType) {\n  return currentType !== expectedType;\n}\n```\n\n## Follow-up Questions\n- How would you quantify uplift in BI adoption and report accuracy?\n- How would you scale the DQaaS across more regions or tenants while preserving privacy and cost predictability?","diagram":"flowchart TD\n  A[Raw data ingested] --> B[Profiling job]\n  B --> C[Quality score]\n  C --> D[Alerts]\n  D --> E[Automated remediation]\n  E --> F[Updated lake data]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T07:01:54.140Z","createdAt":"2026-01-16T07:01:54.140Z"},{"id":"q-2717","question":"You're evaluating moving from a centralized real-time alerting system to edge-first, on-device anomaly detection for fraud in a mobile shopping app. Three regions (US, EU, APAC) total 5M MAU. Build a 2-year cost-benefit model including device storage, on-device inference costs, data transfer, cloud orchestrator costs, regulatory penalties, and uplift in fraud detection and retention. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","answer":"Propose a 2-year model: 5M MAU per region across US/EU/APAC; compare centralized real-time alerts to edge-first on-device anomaly detection for fraud in a mobile shopping app. Include device storage, ","explanation":"## Why This Is Asked\nAssess the candidate’s ability to structure a practical CBA for a distributed architecture shift, weighing latency, privacy, and cost against risk reduction and retention gains.\n\n## Key Concepts\n- Edge computing vs centralized analytics\n- Inference costs, device storage, data transfer\n- Regulatory/compliance penalties and risk\n- Uplift in fraud detection and user retention\n- Break-even analysis and sensitivity testing\n\n## Code Example\n```javascript\nfunction breakEven(initialCost, monthlySavings, monthlyCosts) {\n  let months = 0;\n  let net = -initialCost;\n  while (net < 0) {\n    net += (monthlySavings - monthlyCosts);\n    months++;\n  }\n  return months;\n}\n```\n\n## Follow-up Questions\n- How would you design the data governance model to satisfy privacy regulations across regions?\n- What mitigation plans exist if edge devices underperform (false positives, latency spikes, battery impact)?","diagram":"flowchart TD\nA[Centralized Alerts] --> B[Edge On-Device Analytics]\nB --> C[Cost Elements]\nC --> D[Storage, Inference, Data Transfer, Orchestration]\nB --> E[Benefits]\nE --> F[Fraud Uplift, Retention]\nF --> G[Break-even Horizon]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T07:49:33.320Z","createdAt":"2026-01-16T07:49:33.321Z"},{"id":"q-2735","question":"You’re evaluating a global fraud-detection feature for a ride-hailing app. Compare a hybrid deployment: (A) on-device anomaly detectors with federated updates across regions, (B) regional edge gateways with centralized analytics, under data residency constraints. Build a 2-year cost-benefit model including training, device energy, data transfer, hosting, false positives costs, and regulatory penalties. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","answer":"Hybrid model: on-device anomaly detector with federated updates plus regional edge gateway. Build a 2-year plan accounting device compute and energy, federated aggregation costs, data transfer, false-","explanation":"## Why This Is Asked\nThis question probes cost-aware architecture decisions under latency, privacy, and regulatory constraints for fraud detection. It tests modeling of on-device vs centralized trade-offs, and how to quantify benefits beyond raw accuracy.\n\n## Key Concepts\n- Edge vs on-device ML with federated updates\n- Data residency and regulatory risk\n- False positives cost and user impact\n- Opex vs Capex in hybrid deployments\n\n## Code Example\n```javascript\n// Simple break-even calculator (illustrative)\nfunction breakEven(initialCost, annualSavings) {\n  return initialCost / annualSavings;\n}\n```\n\n## Follow-up Questions\n- How would you validate production performance and drift for each model?\n- What mitigations would you implement if false positives rise and churn increases?","diagram":"flowchart TD\nA[Global Fraud Flow] --> B[On-device Federated Update]\nA --> C[Regional Edge Gateway]\nB --> D[Local Inference]\nC --> E[Central Analytics]\nD --> F[Action or Flag]\nE --> F","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","DoorDash","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T09:46:06.380Z","createdAt":"2026-01-16T09:46:06.380Z"},{"id":"q-2779","question":"Assess migrating a centralized image moderation pipeline to on-device edge moderation across 4 regions handling 2M requests/day. Build a 2-year cost-benefit analysis including edge hardware amortization, on-device model updates, quarterly bandwidth for diffs, cloud moderation backend costs, data-transfer savings, regulatory penalties, and accuracy/false-positive gains affecting retention. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","answer":"Propose a 2-year CBA comparing centralized vs on-device edge moderation. Include capex for edge devices, ongoing model training/updates, diff bandwidth costs, cloud hosting, data transfer savings, com","explanation":"## Why This Is Asked\n\nTests ability to quantify edge-versus-central trade-offs for a real-time, privacy-centric feature at scale, including regulatory and reliability risks.\n\n## Key Concepts\n\n- Hardware amortization and depreciation for edge fleets\n- Diff-based model updates vs full re-train bandwidth costs\n- Data-transfer savings and latency implications\n- Regulatory penalties and compliance costs\n- Break-even, NPV, and go/no-go criteria\n\n## Code Example\n\n```javascript\n// Pseudo-cost model sketch\nfunction model(costs) {\n  const { edgeCapex, opexCloud, bandwidth, penalties, retentionImpact } = costs;\n  // Simple ROI placeholder\n  const yearlySavings = retentionImpact * 1.2e6; // illustrative\n  return { ROI: (yearlySavings - edgeCapex - opexCloud - bandwidth - penalties) * 2, yearlySavings };\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt the model for changing user growth?\n- What data would you collect to validate the forecast after deployment?","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Cloudflare","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T11:37:18.677Z","createdAt":"2026-01-16T11:37:18.677Z"},{"id":"q-2806","question":"You're evaluating a beginner feature: on-logger data masking in analytics to redact PII before BI ingestion. Create a 12-month CBA: dev time (2 engineers for 3 weeks), hosting/storage overhead, tooling costs, privacy penalties avoided, uplift in dashboard adoption, and incident reductions; compute break-even and go/no-go criteria?","answer":"2 engineers x 3 weeks = about $26k labor; tooling $3k; storage overhead $50/mo. Privacy penalties avoided: ~$20k/yr. BI adoption uplift 8–12%, retention +2%, tickets reduced. Break-even ~9–12 months, ","explanation":"## Why This Is Asked\n\nThis question evaluates the ability to translate a compliance feature into a concrete, time-bound cost-benefit analysis suitable for a beginner. It tests how assumptions drive ROI and how to articulate go/no-go criteria.\n\n## Key Concepts\n\n- Cost estimation basics for small features\n- ROI calculation and break-even timing\n- Risk penalties avoided and adoption uplift\n- Simple, testable calculations and trade-offs\n\n## Code Example\n\n```javascript\nfunction breakEvenMonths(laborRatePerHr, weeks, engineers, monthlyOverhead, annualBenefit) {\n  const upfront = laborRatePerHr * weeks * engineers * 40;\n  const monthlyBenefit = annualBenefit / 12;\n  const netMonthlyBenefit = monthlyBenefit - monthlyOverhead;\n  if (netMonthlyBenefit <= 0) return Infinity;\n  return Math.ceil(upfront / netMonthlyBenefit);\n}\n```\n\n## Follow-up Questions\n\n- How would you adjust for ARR vs one-time benefits?\n- What metrics would you track after launch to validate forecasts?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T13:10:15.008Z","createdAt":"2026-01-16T13:10:15.009Z"},{"id":"q-2922","question":"You're evaluating a privacy-preserving fraud-detection feature for a mobile payments app. Compare (A) on-device risk scoring with secure enclaves and periodic federated updates vs (B) centralized scoring with differential privacy. Build a 2-year cost-benefit model including hardware/edge costs, data transfer, maintenance, latency, FP/FN costs, uplift in fraud detection and activation, regulatory penalties avoided, break-even horizon, and go/no-go criteria with mitigations?","answer":"Quantify total cost of ownership for both paths: edge licenses, secure enclave fees, federated update bandwidth, cloud DP charges, latency impact, and false-positive costs. Benefits to cite: 8–15% fra","explanation":"## Why This Is Asked\nTests ability to compare on-device vs centralized privacy-preserving ML in fintech, balancing privacy, latency, and regulatory risk.\n\n## Key Concepts\n- Cost modeling of edge vs cloud privacy tooling\n- Federated vs centralized DP approaches\n- Latency, FP/FN cost, and regulatory penalties\n\n## Code Example\n```javascript\n// ROI skeleton for illustration\nfunction roi(onDeviceCost, cloudCost, uplift, months){\n  return (uplift * 1000 * months) - ((onDeviceCost + cloudCost) * months);\n}\n```\n\n## Follow-up Questions\n- How would you handle model drift in both paths?\n- Which metrics would you track in production to decide go/no-go?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Coinbase","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T17:44:56.872Z","createdAt":"2026-01-16T17:44:56.872Z"},{"id":"q-2936","question":"You're evaluating a policy-driven, offline-first config-management engine for a distributed platform (like Consul/Terraform). The feature enables regional policy modules (JSON/YAML) to be evaluated locally on agents, with periodic delta updates from a central policy service when connectivity exists, and automatic conflict resolution when policies conflict. Build a 2-year cost-benefit model including authoring time, runtime overhead, update costs, data-transfer, compliance penalties avoided, uplift in deployment velocity, and break-even horizon; specify go/no-go criteria and edge-case mitigations?","answer":"Proposed model: three cost buckets (authoring/content creation, runtime overhead, update/ops) plus tangible benefits (compliance penalties avoided, faster deploys, audit readiness). Assume per-node CP","explanation":"## Why This Is Asked\nAssesses ability to translate policy-engine design into a pragmatic, multi-year business case with real-world constraints.\n\n## Key Concepts\n- Offline-first policy evaluation\n- Delta updates and conflict resolution\n- TCO vs business value, break-even analysis\n\n## Code Example\n```javascript\n// Pseudo-cost model sketch\nfunction yearlyBenefit(policies, regions, activationRate) {\n  const impact = policies.length * 1000 * activationRate;\n  return impact * regions.length;\n}\n```\n\n## Follow-up Questions\n- How would you model SLA penalties for policy drift and how would you test them?\n- What telemetry would you collect to validate the cost/benefit assumptions?","diagram":"flowchart TD\n  A[Policy Module] --> B[Local Eval]\n  B --> C[Conflict Resolve]\n  C --> D[Update Center]\n  D --> E[Policy Delta]\n  E --> F[Compliance & Audit]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T18:43:15.272Z","createdAt":"2026-01-16T18:43:15.274Z"},{"id":"q-2990","question":"You’re evaluating a beginner feature: a local-first search index for a mobile knowledge-base app used by field workers. The index caches documents and metadata on-device and syncs updates when online. Build a 12-month CBA: index size per user, CPU/memory impact, delta-sync costs, content update cadence, and projected uplift in offline task completion. Include go/no-go criteria and risk mitigations?","answer":"Index size ~4–6 MB per user; CPU ~2–6% on mid-range devices; memory ~60–120 MB; delta-sync adds 0.3–0.8 MB per update; offline lookup uplift 15–30%; dev effort ~2–3 weeks; backend data transfer ~$6k/m","explanation":"## Why This Is Asked\nThis angle tests the ability to quantify a data-heavy feature with a clear offline-first constraint, balancing device resources against business value.\n\n## Key Concepts\n- Offline-first architectures\n- Local indexing and delta synchronization\n- Resource budgeting (CPU, memory, storage)\n- Data transfer costs and break-even analysis\n\n## Code Example\n```\n// Implementation sketch: simple local search index\nclass LocalIndex {\n  constructor() { this.docs = []; }\n  build(docs) { this.docs = docs.map(d => ({ id: d.id, text: d.content })); }\n  search(q) {\n    const terms = q.toLowerCase().split(/\\s+/);\n    return this.docs.filter(d => terms.every(t => d.text.toLowerCase().includes(t)));\n  }\n  deltaUpdate(changes) {\n    // apply lightweight patches to docs\n    changes.forEach(c => {\n      const idx = this.docs.findIndex(d => d.id === c.id);\n      if (idx >= 0) this.docs[idx] = { ...this.docs[idx], ...c.patch };\n      else this.docs.push({ id: c.id, text: c.patch.content });\n    });\n  }\n}\n```\n\n## Follow-up Questions\n- How would you implement delta-sync to minimize data transfer while ensuring consistency?\n- What privacy controls would you layer to protect sensitive docs during offline usage?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T20:32:44.481Z","createdAt":"2026-01-16T20:32:44.482Z"},{"id":"q-3182","question":"You're evaluating a beginner feature: a client-side, offline-capable, autosave-enabled note editor for a CRM. Design and implement a debounced autosave to localStorage, restore on load, and handle cross-tab edits with a versioning strategy. Include a minimal code sketch and validation steps?","answer":"Implement a debounced autosave for a CRM note editor using localStorage. Save {content, lastSaved, version} with a 300ms debounce; restore on load. To handle multi-tab edits, use a version counter and","explanation":"## Why This Is Asked\\n\\nA practical UX feature tests offline resilience, local storage usage, and simple cross-tab conflict handling without server changes.\\n\\n## Key Concepts\\n\\n- Debounce autosave\\n- localStorage vs IndexedDB\\n- Versioning for conflict detection\\n- Cross-tab synchronization basics\\n\\n## Code Example\\n\\n```javascript\\n// Minimal autosave scaffold\\nlet timer;\\nfunction onChange(content){\\n  clearTimeout(timer);\\n  timer = setTimeout(()=> {\\n     const note = { content, lastSaved: Date.now(), version: (NOTE_VERSION++) };\\n     localStorage.setItem('noteDraft', JSON.stringify(note));\\n  }, 300);\\n}\\n```\\n\\n## Follow-up Questions\\n\\n- How would you test offline reload and cross-tab conflict?\\n- How would you handle storage quota and large notes?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Goldman Sachs","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T05:42:27.514Z","createdAt":"2026-01-17T05:42:27.514Z"},{"id":"q-3210","question":"You're evaluating migrating a centralized discounting and pricing engine for a marketplace app to a peer-to-peer edge-cached policy, with on-device decision rules and occasional server fallbacks due to latency and data locality. Build a 2-year cost-benefit model including compute, storage, data transfer, policy updates, fairness penalties, uplift in conversions, and risk of price wars; define break-even horizon and go/no-go criteria, plus fallback strategies?","answer":"Proposed answer outline: assume 3 regions, 5M MAU; migrate to edge-policy engine with 2 MB per user cache; reduce server pricing by 40%; content/telemetry updates cost $1.2M/year; on-device compute ne","explanation":"## Why This Is Asked\n\nThis tests ability to translate product changes into a full 2-year financial lens, including data locality, latency, and risk.\n\n## Key Concepts\n\n- Edge vs centralized cost models\n- Data locality, latency, drift, and governance\n- A/B planning, rollbacks, and risk mitigations\n\n## Code Example\n\n```javascript\nfunction breakEven(monthlyBenefit, upfront){\n  if(monthlyBenefit<=0) return Infinity\n  return Math.ceil(upfront / monthlyBenefit)\n}\n```\n\n## Follow-up Questions\n\n- How would you validate drift and fairness in production?\n- What are rollback criteria and metrics to trigger them?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Meta","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T07:00:06.310Z","createdAt":"2026-01-17T07:00:06.310Z"},{"id":"q-3263","question":"You’re evaluating a beginner feature: a privacy-preserving onboarding chatbot that operates offline with a local FAQ and optional server calls for telemetry only when the user opts in. Build a 12‑month cost-benefit model: content creation, on-device storage impact, minimal server costs, regulatory/privacy overhead, expected uplift in activation and paid conversions, and go/no-go criteria?","answer":"Outline plan: build a privacy-preserving onboarding chatbot that works offline with a local FAQ (5 topics) and optional telemetry. Content: 5 topics + 2 short videos. Dev: 1 engineer for 2 weeks. Stor","explanation":"## Why This Is Asked\nTests ability to reason about privacy-preserving onboarding, offline-first UX, and basic economics, plus handling opt-in telemetry trade-offs.\n\n## Key Concepts\n- Privacy by design and opt-in telemetry\n- On-device logic and offline UX\n- Unit economics: fixed vs variable costs, uplift estimation\n- Risk management: content drift, user confusion, opt-in bias\n\n## Code Example\n```javascript\nfunction breakEvenMonths(fixed, monthlyNet) {\n  if (monthlyNet <= 0) return Infinity;\n  return Math.ceil(fixed / monthlyNet);\n}\n```\n\n## Follow-up Questions\n- How would you validate uplift attribution given opt-in bias?\n- How would you validate offline usage metrics and ensure content stays up to date?\n","diagram":"flowchart TD\n  A[Open app] --> B[Local FAQ accessed]\n  B --> C{Telemetry opt-in?}\n  C -- Yes --> D[Send minimal telemetry]\n  C -- No --> E[No data transfer]\n  D --> F[Activation uplift]\n  F --> G[Go/No-Go decision]","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T09:29:51.701Z","createdAt":"2026-01-17T09:29:51.701Z"},{"id":"q-3401","question":"You're evaluating privacy-preserving mobile content moderation for a live video app: implement on-device real-time classification with weekly federated updates to improve policy rules. Build a 2-year cost-benefit model capturing on-device compute, memory, energy, uplink costs, server aggregation, DP noise, regulatory penalties avoided, and uplift in safe engagement; define break-even horizon and go/no-go criteria, including failure modes and mitigations?","answer":"Plan on-device inference with a lightweight classifier (0.5–1.5 ms per frame) plus weekly federated updates. Estimate costs: device compute, memory, energy, uplink; server aggregation & DP noise; cont","explanation":"## Why This Is Asked\n\nTests ability to design privacy-first moderation with on-device inference and federated learning, including lifecycle costs and risk controls.\n\n## Key Concepts\n\n- On-device ML workloads and model packaging\n- Federated learning with DP noise\n- Cost modeling for device and server infra\n- Metrics: FP/FN, latency, uplift, penalties avoided\n\n## Code Example\n\n```javascript\n// Pseudocode for per-frame inference budget\nlet t0 = performance.now();\nconst y = model.predict(frame);\nlet t1 = performance.now();\nconsole.log('inference ms', t1 - t0);\n```\n\n## Follow-up Questions\n\n- How would you handle model drift in production?\n- What testing plan ensures regulatory compliance?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","NVIDIA","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T14:39:57.347Z","createdAt":"2026-01-17T14:39:57.348Z"},{"id":"q-3477","question":"You're evaluating migrating a centralized on-call search feature for financial content to an edge-based offline index with federated updates across 3 regions. Build a 2-year cost-benefit model including edge hardware/storage per user, content update cadence, bandwidth savings, latency improvements, regulatory penalties, and user experience uplift; define break-even horizon and go/no-go criteria; include failure modes and mitigations?","answer":"Edge-based search index: hardware/storage per user 30–60 MB, quarterly content updates, 60–80% bandwidth saved. Latency drops from ~120 ms to ~40 ms; offline availability improves. Include data-locali","explanation":"## Why This Is Asked\n\nThis question probes a real-world cost-benefit scenario for edge computing in a金融 context, focusing on data locality, latency, and regulatory risk while requiring concrete numbers and go/no-go criteria.\n\n## Key Concepts\n\n- Edge vs central cost modeling\n- Data locality and regulatory penalties\n- Cadence of content updates and delta-sync strategies\n- Break-even analysis and sensitivity\n\n## Code Example\n\n```javascript\nfunction breakEven(cost, monthlySavings, months=24) {\n  let total = 0;\n  for (let m = 1; m <= months; m++) total += monthlySavings - cost;\n  return total;\n}\n```\n\n## Follow-up Questions\n\n- How would you quantify regulatory penalties for data locality breaches?\n- How would you validate latency improvements with real users and map to ARPU?","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T17:42:42.308Z","createdAt":"2026-01-17T17:42:42.308Z"},{"id":"q-3551","question":"You're evaluating a beginner feature: an adaptive animation and UI fidelity controller that reduces effects on devices with low battery or CPU to improve perceived responsiveness across iOS and Android. Build a 12-month CBA: estimate detection overhead, telemetry/data costs, maintenance, expected uplift in retention and satisfaction, and risk of degraded UX; provide go/no-go criteria and mitigations?","answer":"Present a concrete budget: detector cost per device, monthly telemetry/storage, maintenance headcount, and expected uplift in retention/time-on-app; compute break-even using ARPU and total user base, ","explanation":"## Why This Is Asked\n\nTests ability to translate a UX feature into a measurable business case, including per-device costs, data budgets, and risk management. It also assesses ability to set clear go/no-go criteria and design monitoring for a beginner-friendly feature.\n\n## Key Concepts\n\n- Cost estimation per device and per user\n- Telemetry/data budgets with privacy considerations\n- Go/No-Go criteria and risk mitigation\n\n## Code Example\n\n```javascript\n// Simple break-even helper\nfunction breakEven(uplift, yearlyCost) {\n  if (uplift <= 0) return Infinity;\n  return yearlyCost / uplift;\n}\n```\n\n## Follow-up Questions\n\n- How would you measure uplift and ensure robust A/B testing?\n- What privacy controls would you implement for telemetry data?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Apple"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T21:24:47.755Z","createdAt":"2026-01-17T21:24:47.757Z"},{"id":"q-3646","question":"You're evaluating a privacy-preserving on-device dispatcher optimization feature for a rideshare platform that runs per-driver route-choice inference inside secure enclaves, with federated model updates across 3 regions; build a 2-year cost-benefit analysis including edge hardware, secure enclave licensing, data transfer, key management, latency impact on dispatch, regulatory penalties, and uplift in on-time arrivals and rider satisfaction. Define break-even horizon and go/no-go criteria?","answer":"Develop a comprehensive 2-year cost-benefit analysis for a privacy-preserving on-device dispatcher optimization system that leverages per-driver secure enclaves with federated model updates across 3 regions. The analysis should encompass capital expenditures for Trusted Execution Environments (TEEs) and edge hardware, operational expenditures for encrypted data transfer, key management infrastructure, secure enclave licensing, and ongoing maintenance. Quantify the impact of latency improvements on dispatch operations, regulatory compliance risk mitigation, and measurable uplifts in on-time arrivals and rider satisfaction scores. Establish a clear break-even horizon and define quantitative go/no-go decision criteria based on ROI thresholds and risk-adjusted returns.","explanation":"## Why This Is Asked\nAssesses ability to model ROI for privacy-preserving distributed systems with strict latency constraints while balancing technical implementation against business outcomes.\n\n## Key Concepts\n- Trusted Execution Environments (TEEs)/secure enclaves\n- Federated learning and model updates\n- Data residency and compliance requirements\n- Latency budgets and performance metrics\n- Regulatory risk quantification\n- Hardware lifecycle management\n\n## Code Example\n```javascript\nfunction estimateROI(capex, opex, uplift, risk) {\n  // Rough calculation: annual net benefit = uplift * revenue\n  return (uplift * revenue - opex) / capex;\n}\n```","diagram":"flowchart TD\n  A[Scope] --> B[Costs]\n  B --> C[Benefits]\n  C --> D[Risks]\n  D --> E[Go/No-Go]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:01:30.536Z","createdAt":"2026-01-18T02:47:11.521Z"},{"id":"q-3837","question":"You're evaluating adding a real-time data quality monitoring microservice to a multi-region Lakehouse (Databricks-based) that tracks completeness, schema drift, and anomaly detection for streaming events (billions of events per day). Build a 2-year cost-benefit model including ingestion/processing costs, metadata storage, governance penalties avoided, uplift in downstream analytics accuracy, and operational overhead. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","answer":"Outline a 2-year model: capex for streaming infra, opex for compute/storage, drift models, and lineage; benefits from reduced data defects, faster time-to-insight, and avoided governance penalties; qu","explanation":"## Why This Is Asked\nReal-time data quality is critical in lakehouse environments serving analytics at scale; this question probes practical cost-benefit modeling, risk assessment, and trade-offs for monitoring at scale.\n\n## Key Concepts\n- Data quality monitoring for streaming data (completeness, schema drift, anomalies)\n- Multi-region cost model: capex vs opex, data ingestion/processing, metadata storage\n- Business value levers: downstream analytics accuracy, SLA penalties avoided, faster insights\n- Risk management: failure modes, mitigations, sensitivity analysis\n\n## Code Example\n```javascript\nfunction breakEven(annualBenefit, annualCost) {\n  return Math.ceil(annualCost / Math.max(annualBenefit, 1e-9));\n}\n```\n\n## Follow-up Questions\n- How would you set drift thresholds across regions and align them with SLAs?\n- What instrumentation would you add to validate that the quality service improves downstream KPIs?","diagram":"flowchart TD\n  A[Ingest Streaming Data] --> B[Run Quality Checks]\n  B --> C[Publish to Lakehouse]\n  B --> D[Quarantine & Notify]\n  D --> E[Governance & Audit]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Lyft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T11:31:26.064Z","createdAt":"2026-01-18T11:31:26.064Z"},{"id":"q-3907","question":"You're evaluating privacy-preserving on-device demand forecasting for a grocery-delivery platform. Compare a federated-learning with differential privacy setup across 4 regions (4M MAU) to a centralized cloud forecast. Build a 2-year cost-benefit model including edge hardware amortization, DP-noise overhead, orchestration and bandwidth, content updates, and regulatory penalties, plus uplift in SLA adherence and GMV. Provide break-even horizon and go/no-go criteria; specify failure modes and mitigations?","answer":"Edge CAPEX amortized over 3 years; DP noise budget per update; cloud orchestration and secure aggregation costs; inter-region bandwidth. Benefit: better forecast accuracy reduces stockouts, last-minut","explanation":"## Why This Is Asked\nTests ability to quantify cost and risk of privacy-preserving ML in a real-world, multi-region delivery scenario, including DP overhead, regulatory penalties, and business impact.\n\n## Key Concepts\n- Federated learning with differential privacy in multi-region deployments\n- Cost modeling: capex, opex, data transfer, penalties, and revenue uplift\n- Break-even analysis and risk mitigations for production ML\n\n## Code Example\n```javascript\nfunction netBenefit(savings, costs) {\n  return savings - costs;\n}\n```\n\n## Follow-up Questions\n- How would you set adaptive epsilon over time to balance privacy and utility?\n- What metrics and monitoring would trigger a rollback or cadence adjustment?","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Instacart","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T14:38:05.928Z","createdAt":"2026-01-18T14:38:05.928Z"},{"id":"q-3985","question":"Design a privacy-preserving cross-border attribution system for a delivery/booking app that measures campaign impact across 4 regions without sharing raw user signals. Use on-device buffering, secure aggregation, and periodic federated updates; enforce data residency and differential privacy on aggregates. Provide a 2-year cost model, break-even, and go/no-go criteria; include failure modes and mitigations?","answer":"Privacy-preserving cross-border attribution using on-device buffering, secure aggregation, and periodic federated updates. Data stays regional; add DP noise on aggregates. 2-year costs: edge storage/c","explanation":"## Why This Is Asked\n\nThis question probes ability to design privacy-preserving analytics at scale with regulatory constraints and a realistic cost model.\n\n## Key Concepts\n\n- Secure aggregation and on-device processing\n- Data residency and differential privacy\n- Cost modelling: edge vs cloud, data transfer, penalties\n- Reliability: event loss, retries, clock drift\n\n## Code Example\n\n```javascript\n// Pseudo on-device aggregation sketch\nfunction aggregateOnDevice(events){\n  // hash user, keep per-campaign counters in secure enclave\n  // return a masked sum for server\n  return maskedSum(events);\n}\n```\n\n## Follow-up Questions\n\n- How would you tune DP noise for campaign-level signals?\n- How would you adapt if one region imposes stricter retention limits?\n","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T18:46:33.616Z","createdAt":"2026-01-18T18:46:33.616Z"},{"id":"q-4012","question":"You're evaluating a beginner feature: an offline-first stock widget for a mobile checkout app used by field reps. The widget caches product data locally, syncs when online, and shows real-time stock/ETA. Build a 12-month CBA: per-SKU cache size, total SKUs cached, device storage constraints, delta-sync costs, ongoing maintenance, and projected uplifts in conversion and reductions in stock-out support tickets; specify break-even horizon and go/no-go criteria?","answer":"Propose a 12-month CBA for an offline-first stock widget. Assume 10k SKUs, 5 KB cache per SKU, 3 stores, 2 engineers for 4 weeks, delta-sync costs $0.002/MB, ongoing maintenance $6k/mo. Forecast 2% up","explanation":"## Why This Is Asked\nEvaluates applying CBA to a UX feature with offline data, storage, sync costs, and measurable business impact.\n\n## Key Concepts\n- Offline-first caching, data freshness, and sync costs\n- Mobile storage constraints across devices\n- Basic financial modeling: uplift, maintenance, and payback\n\n## Code Example\n```javascript\n// Pseudo: compute break-even month given uplift, cost, and tickets saved\nfunction breakEven(monthlyCosts, upliftRate, ticketsSaved) {\n  // placeholder logic\n  return Math.ceil(monthlyCosts / (upliftRate * 1000));\n}\n```\n\n## Follow-up Questions\n- How would you validate uplift estimates post-launch?\n- What metrics would you monitor for go/no-go decisions?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Scale Ai","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T19:31:36.980Z","createdAt":"2026-01-18T19:31:36.980Z"},{"id":"q-4054","question":"Evaluate a feature: an offline-first, on-device search index for a team messaging app (Slack-like) that indexes messages locally and syncs diffs when online. Build a 18–24 month CBA covering per-device storage, CPU cost, delta-sync traffic, on-device encryption overhead, regulatory/privacy risks, expected uplift in retention/usage, and a go/no-go criteria with break-even horizon?","answer":"Estimate per-user index size (10–30 MB), CPU time for indexing/search, storage costs across MAU, and delta-sync network traffic. Include encryption overhead and privacy risk exposure. Project uplift in retention/usage, calculate break-even horizon, and establish clear go/no-go criteria based on ROI thresholds.","explanation":"## Why This Is Asked\n\nTests the ability to model a feature with offline-first behavior and privacy constraints, focusing on cost drivers (storage, CPU, network) and benefits (retention, engagement) over 18–24 months, plus regulatory risk.\n\n## Key Concepts\n\n- Cost modeling\n- Data transfer and encryption overhead\n- Privacy and compliance\n- Break-even analysis\n- Trade-offs between local storage and cache invalidation\n\n## Code Example\n\n```javascript\n// Simple break-even helper (illustrative)\nfunction breakEven(revenuePerUser, costPerUser, months) {\n  const totalRevenue = revenuePerUser * months;\n```","diagram":"flowchart TD\nA[Offline on-device search] --> B[Index size per user]\nA --> C[Diff sync traffic]\nA --> D[On-device encryption]\nA --> E[Privacy/regulatory risk]\nA --> F[Business impact: uplift]\nB --> G[Costs]\nC --> G\nD --> G\nE --> G\nF --> G","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Slack","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T05:58:42.514Z","createdAt":"2026-01-18T21:38:44.841Z"},{"id":"q-4097","question":"You're evaluating a privacy-preserving on-device cohort analytics engine that uses secure aggregation across regions to generate user-segment insights without exposing raw data. Build a 2-year CBA: include per-device compute/memory, MPC costs, data transfer, content updates, regulatory penalties, uplift in decision quality, and support costs. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","answer":"Propose a comprehensive 2-year cost-benefit analysis for a privacy-preserving on-device cohort analytics engine utilizing secure aggregation across regions. Cost components include per-device CPU overhead of 2–6%, memory allocation of 20–60MB, multi-party computation orchestration and key management infrastructure, cross-region data transfer costs, content delivery system updates, regulatory compliance monitoring, and tier-3 technical support. Benefits encompass enhanced decision quality through richer user segmentation insights, reduced data privacy exposure risks, and improved regulatory alignment. Break-even is projected at 18 months with go/no-go criteria including device performance thresholds, MPC latency targets, and compliance audit outcomes. Key failure modes include device resource exhaustion, MPC node failures, and regulatory jurisdiction conflicts—mitigated through adaptive throttling mechanisms, redundant MPC infrastructure, and jurisdiction-specific data routing protocols.","explanation":"## Why This Is Asked\n\nTests the ability to quantify a privacy-preserving on-device analytics program from both cost and value perspectives, including MPC overhead, device constraints, data transfer, and legal risk.\n\n## Key Concepts\n\n- On-device compute and memory budgeting\n- Secure aggregation / MPC cost model\n- Data minimization and regulatory penalties\n- Total cost of ownership and break-even horizon\n- Operational metrics: latency, drift, audits\n\n## Code Example\n\n```javascript\n// Skeleton cost model\nfunction cba(params) {\n  const {cpuPerDevice, ramPerDevice, mpcOverhead, transferPerUser} = params;\n  // Implementation details...\n}\n```","diagram":"flowchart TD\n  A[User device] --> B[Secure aggregation]\n  B --> C[Regional MPC coordinator]\n  C --> D[Central dashboard / insights]\n  D --> E[Decision/actions]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T05:39:05.382Z","createdAt":"2026-01-18T23:39:21.960Z"},{"id":"q-4322","question":"You're evaluating a cost-aware, tiered real-time event processing pipeline for a SaaS analytics product that ingests 100M events per day across 3 regions. Propose a 2-year CBA for implementing a tiered processing model (gold/silver/bronze) with per-tenant cost attribution, dynamic sampling for bronze, and regulatory data-retention controls. Include CAPEX, OPEX, data-transfer costs, penalties, and expected uplift in retention and ARPU; specify break-even horizon and go/no-go criteria?","answer":"2-year CBA for a tiered real-time processor across 3 regions (100M events/day). CAPEX: upgraded streaming cluster + hot/cold storage tiers; OPEX: compute, storage, inter-region transfer, reprocessing.","explanation":"## Why This Is Asked\nTests ability to model multi-region, cost-aware streaming architectures with real-world data governance and business impact.\n\n## Key Concepts\n- Tiered processing, per-tenant costing, sampling, data retention policies, regulatory penalties.\n- CAPEX vs OPEX, cross-region data transfer economics, SLA risk.\n- ROI metrics: break-even horizon, NPV, ROAS, uplift in retention/ARPU.\n\n## Code Example\n```javascript\nfunction breakEven(monthlyBenefit, monthlyCost) {\n  // months to recover\n  return monthlyCost > 0 ? monthlyCost / monthlyBenefit : Infinity;\n}\n```\n\n## Follow-up Questions\n- How would you attribute costs across tenants and regions? \n- How would changes in data-regulation affect the model? \n- How would you monitor and adjust sampling to maintain SLAs?","diagram":"flowchart TD\n  A[Ingest Events] --> B[Tiered Processing: Gold/Silver/Bronze]\n  B --> C[Retention & Compliance Rules]\n  C --> D[Storage & Compute Costs]\n  D --> E[Cost Attribution & Billing]\n  E --> F[Go/No-Go Decision]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Netflix","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T13:34:07.917Z","createdAt":"2026-01-19T13:34:07.917Z"},{"id":"q-4332","question":"You're adding a lightweight, on-device offline translation feature to a mobile customer-support chat app used by field technicians. It caches phrase bundles for 20 languages, uses a small neural MT model, and syncs updates when online. Build a 12-month CBA: storage per language, total cache size, on-device latency, delta-update costs, maintenance, and projected uplift in first-contact resolution. Include go/no-go criteria?","answer":"Per-language bundle ~60KB; 20 languages → ~1.2MB per device. Target latency <180ms on mid-range CPU; memory impact ~1–2x input size. Delta sync monthly; assume 1GB/month bandwidth. Maintenance: model ","explanation":"## Why This Is Asked\n\nAssesses ability to reason about on-device ML tradeoffs, storage, latency, and multi-language data planning while producing actionable financials.\n\n## Key Concepts\n\n- On-device ML footprint\n- Cache sizing & delta updates\n- Latency, power, and UX trade-offs\n- TCO, break-even, go/no-go criteria\n\n## Code Example\n\n```javascript\nfunction estimateCachePerDevice(languages, sizePerLanguageKB){\n  return languages * sizePerLanguageKB / 1024; // MB\n}\n```\n\n## Follow-up Questions\n\n- How would you validate uplift with an A/B test?\n- How do you handle adding new languages mid-cycle without breaking existing caches?","diagram":"flowchart TD\n  A[Inputs: languages, perLanguageKB] --> B[Compute per-device cache size]\n  B --> C[Estimate latency & CPU impact]\n  C --> D[Cost for delta updates & hosting]\n  D --> E[Calculate break-even & go/no-go]","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","DoorDash","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T14:41:36.589Z","createdAt":"2026-01-19T14:41:36.589Z"},{"id":"q-4672","question":"You're evaluating a beginner feature: an offline-first customer support chat assistant that caches last 60 days of chat transcripts locally and suggests canned responses with NLP on-device. Build a 12-month CBA: per-user storage, CPU overhead, delta-sync costs, model update cadence, privacy/opt-in costs, and go/no-go criteria?","answer":"Per-user cache: 60 days of transcripts ~2–3 MB; on-device NLP model ~1–2 MB; delta-sync ~0.2–0.5 MB/mo. Costs: encryption, updates, privacy controls ~$0.03–0.07/MAU/mo. Benefits: 20–30% faster replies","explanation":"## Why This Is Asked\nAssesses ability to model real-world benefits, privacy costs, and operators for a lightweight NLP feature.\n\n## Key Concepts\n- Local caching\n- On-device ML footprint\n- Data sync costs and privacy.\n- ROI timing and go/no-go criteria\n\n## Code Example\n```javascript\nfunction cba(inputs){ /* compute CBA with storage, cpu, and benefit terms */}\n```\n\n## Follow-up Questions\n- How would you validate the privacy model and consent flow?\n- What metrics would you monitor post-launch to adjust the ROI assumptions?","diagram":"flowchart TD\n  A[Offline chat cache] --> B[Local storage]\n  A --> C[NLP on-device]\n  B --> D[Delta-sync]\n  C --> D\n  D --> E[ROI calculation]\n  E --> F{Go/No-Go}","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T07:44:23.667Z","createdAt":"2026-01-20T07:44:23.668Z"},{"id":"q-4693","question":"You're evaluating migrating a centralized analytics stack to a data mesh with domain-owned data products across 4 regions. Build a 2-year CBA for Kafka streaming, Delta Lake, and dbt-based pipelines; consider domain/platform costs, data duplication, governance, licensing/infra, regulatory penalties, uplift in decision velocity/quality, and MTTR. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","answer":"Migration plan: 2-year CBA for a data-mesh with domain-owned data products (4 regions, 200+ datasets), Kafka streams, Delta Lake, and dbt. Include domain/platform headcount, data-duplication & governa","explanation":"## Why This Is Asked\n\nAssessing a data-mesh transition tests understanding of ownership, data quality, and cross-team collaboration, plus real-world cost modeling across regions and vendors. It also probes risk budgeting for regulatory penalties and governance overhead.\n\n## Key Concepts\n\n- Data mesh vs centralized analytics\n- Domain-owned data products and governance\n- TCO: capex vs opex, data duplication, infra/licensing\n- Multi-region latency, data residency, and compliance\n- Break-even analysis and go/no-go criteria\n\n## Code Example\n\n```python\n# Simple cost model (illustrative)\ndef total_cost(domain_cost, infra_cost, dup_cost, reg_penalty):\n    return domain_cost + infra_cost + dup_cost + reg_penalty\n```\n\n## Follow-up Questions\n\n- How would you quantify data-duplication costs across regions?\n- What metrics determine break-even and when would you pause migration?","diagram":"flowchart TD\n  A[Centralized Stack] --> B[Data Mesh]\n  B --> C[Domain Teams]\n  B --> D[Platform Team]\n  C --> E[Data Products]\n  D --> F[Governance & Compliance]\n  E --> G[Business Impact]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T08:47:35.002Z","createdAt":"2026-01-20T08:47:35.002Z"},{"id":"q-4743","question":"You're evaluating an edge-first fraud-detection feature that runs entirely on mobile devices, flagging suspicious activity in real time without sending raw data to servers. Build an 18-month CBA: model size, on-device CPU and energy impact, incremental telemetry/storage, false-positive rate targets, regulatory/compliance costs, and expected business impact (revenue protection, user trust). Include go/no-go criteria and break-even horizon?","answer":"I’d forecast an 18‑month CBA: on-device model ~8 MB, +15–25% CPU cycles, negligible battery impact with batching; telemetry kept local unless consented; aim FP rate ≤1–2% to protect UX; expected uplif","explanation":"## Why This Is Asked\nThis question probes cost modeling for on-device ML with privacy constraints, trade-offs between latency, accuracy, energy, and regulatory risk.\n\n## Key Concepts\n- Edge CPU/energy impact and model size\n- FP/FN UX trade-offs and calibration\n- Privacy, telemetry, and compliance costs\n- Break-even horizon and go/no-go criteria\n- Model update cadence and maintenance\n\n## Code Example\n```javascript\n// rough CBA scaffold\nfunction cba(months, costs, savings){\n  let payback = Math.ceil(months * (costs - savings));\n  return {payback};\n}\n```\n\n## Follow-up Questions\n- How would you validate false positives in production?\n- How would regional privacy rules alter the model and costs?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","IBM","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T10:41:32.714Z","createdAt":"2026-01-20T10:41:32.714Z"},{"id":"q-4837","question":"You're evaluating a privacy-preserving on-device content moderation model for a real-time chat feature across iOS, Android, and web. Build a 24-month CBA: model size, on-device CPU cycles, energy impact, delta telemetry/storage, regulatory/compliance costs, and expected business impact (reputation, reduced moderation incidents, uplift in trusted usage); include go/no-go criteria and break-even horizon?","answer":"Propose a 24-month CBA for a privacy-preserving on-device content-moderation model in chat across iOS/Android/Web. Include: target model size (15-25 MB), on-device CPU cycles (0.5-1.5% of a mid-range ","explanation":"## Why This Is Asked\n\nTests ability to model real-world tradeoffs between privacy, latency, and cost. Requires understanding of device constraints, cross-platform deployment, and regulatory risk.\n\n## Key Concepts\n\n- On-device ML footprint and model sizing\n- Energy and CPU budgeting per user/session\n- Telemetry, local storage, and data minimization\n- Compliance costs and risk assessment\n- Business impact metrics: incidents, trust, activation, ARPU\n\n## Code Example\n\n```javascript\nfunction energyPerInference(modelSizeMB, cpuPercent, durationMs){\n  // rough proxy for energy\n  return modelSizeMB * cpuPercent * durationMs * 1e-6;\n}\n```\n\n## Follow-up Questions\n\n- How would you model break-even if moderation incidents drop by X%?\n- What telemetry would you capture and how would you protect privacy?","diagram":"flowchart TD\n  A[Input: user message] --> B[Run on-device moderation model]\n  B --> C{Pass/Flag}\n  C --> D[Action: allow or flag]\n  A --> E[Telemetry (opt-in)]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T15:44:32.981Z","createdAt":"2026-01-20T15:44:32.981Z"},{"id":"q-4937","question":"You're evaluating a beginner feature: an on-device accessibility module that live-subtitles product demo videos in offline field-sales scenarios by using a lightweight ASR model and user corrections to improve accuracy. Build a 12-month CBA: per-user storage, CPU/energy impact, cache size, model update cadence, ongoing annotation costs, and expected uplift in adoption and reduction in training/support tickets; include break-even horizon and go/no-go criteria?","answer":"12-month CBA for an on-device captioner: model ~5 MB, captions cache ~25–40 MB per user; CPU ~0.5–1.5% baseline, peak 150 mW; monthly delta updates; user corrections drive lightweight fine-tuning; exp","explanation":"## Why This Is Asked\n\nAssesses ability to quantify ROI for on-device ML features, balancing latency, storage, and energy with accessibility impact in offline contexts.\n\n## Key Concepts\n\n- On-device ML inference and model size\n- Energy and latency budgeting for mobile devices\n- Caching strategy for offline content\n- Feedback loops and lightweight fine-tuning\n- ROI, break-even, and go/no-go decision points\n\n## Code Example\n\n```javascript\n// Pseudocode: apply user corrections to lightweight fine-tuning\nfunction applyCorrections(model, corrections) {\n  // implement small adapter-tuning or gradient-free update\n}\n```\n\n## Follow-up Questions\n\n- How would you quantify latency targets (e.g., 2s caption) and accuracy impact?\n- What privacy guards and opt-in controls would you require for offline captions?","diagram":"flowchart TD\n  A[User starts video] --> B[Caption generation on-device]\n  B --> C{Caption quality acceptable?}\n  C -->|Yes| D[Cache caption locally]\n  C -->|No| E[User corrections]\n  E --> F[Lightweight fine-tuning]\n  F --> G[Delta model update]\n  G --> H[Apply update to next captions]","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Meta","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T20:28:06.494Z","createdAt":"2026-01-20T20:28:06.494Z"},{"id":"q-5041","question":"You're evaluating a privacy-preserving on-device graph-influence scoring feature to power a 'People You May Like' carousel without sharing raw user connections. Build an 18–24 month CBA around on-device embeddings, memory footprint, CPU cycles, energy use, delta telemetry, update cadence, compliance costs, and business impact (engagement lift, retention, revenue impact). Include go/no-go criteria and break-even horizon?","answer":"Propose an 18–24 month cost-benefit analysis for on-device graph-influence scoring: model size ~2.5M parameters (~10 MB), memory footprint ~20–30 MB peak, weekly update cadence; per-user CPU ~15 ms per update, energy consumption ~0.5 mWh; telemetry delta ~2 KB per user; compliance costs ~$0.5M for privacy audits; infrastructure savings ~$3M/year versus cloud processing; expected engagement lift 8–12%, retention improvement +5%, revenue impact +$4.2M/year; go/no-go criteria: ROI >1.5x within 18 months, battery impact <1% daily, memory <50 MB; break-even horizon ~14 months.","explanation":"## Why This Is Asked\nTests ability to size on-device ML systems, trade latency and energy against infrastructure savings, and quantify business impact under privacy constraints.\n\n## Key Concepts\n- On-device graph embeddings and inference\n- Privacy: no raw graph data, differential privacy options\n- CBA: capex/opex, telemetry, compliance costs, break-even\n- Performance goals: latency, battery impact, memory\n\n## Code Example\n```python\ndef cba(model_cost, server_savings, uplift, months=24):\n    return (months * server_savings) / model_cost * (1 if uplift>0 else 0)\n```\n\n## Follow-up Questions\n- How would you validate the engagement lift assumptions?\n- What privacy techniques would you implement beyond on-device processing?\n- How would you handle model versioning and rollback?","diagram":"flowchart TD\n  A[On-device graph embeddings] --> B[Local storage & RAM]\n  B --> C[Energy & CPU constraints]\n  A --> D[Telemetry reduction]\n  D --> E[Server cost savings]\n  E --> F[Break-even]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:14:56.569Z","createdAt":"2026-01-21T02:47:07.220Z"},{"id":"q-5069","question":"You're evaluating an on-device real-time speech-to-text and translation feature for a multilingual chat app that runs entirely offline on mobile devices, with on-device ASR and translation models updated via periodic federated updates. Build an 18–24 month CBA: model sizes, on-device latency, CPU cycles, energy, memory, update cadence, telemetry, translation accuracy targets, data-licensing/regulatory costs, and business impact (global accessibility, retention). Include go/no-go criteria and break-even horizon?","answer":"On-device ASR+translation for offline multilingual chat. Target per-language model 60–120 MB; latency <=150 ms; CPU ~8–12%; battery overhead ~1–2%/min; telemetry ~20 KB/user/mo; quarterly federated up","explanation":"## Why This Is Asked\nTests cost modeling for on-device ML with real-time UX, privacy, and regulatory trade-offs.\n\n## Key Concepts\n- On-device ML footprints, latency budgets, energy impact\n- Federated update cadence and telemetry strategies\n- Compliance costs vs revenue uplift and break-even\n\n## Code Example\n```javascript\n// Pseudo cost function for energy impact\nfunction energyCost(perMinMs, cpuUtil, hours) { /* ... */ return cost; }\n```\n\n## Follow-up Questions\n- How would you validate model drift and update cadence in production?\n- What are go/no-go thresholds for device categories with limited CPU?","diagram":"flowchart TD\n  A[User speaks] --> B[ASR on-device]\n  B --> C[Translate on-device]\n  C --> D[Display text]\n  D --> E[Telemetry anonymized]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","IBM","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T04:28:05.675Z","createdAt":"2026-01-21T04:28:05.675Z"},{"id":"q-5128","question":"Develop a 12-month CBA for a privacy-preserving on-device meeting summary feature in a mobile collaboration app. The app records meetings locally, caches transcripts, and syncs only anonymized metadata when online. Specify per-user storage, on-device model size, CPU/energy impact, update cadence, privacy/compliance costs, and go/no-go criteria?","answer":"Develop a 12-month CBA for a privacy-preserving on-device meeting summary feature in a mobile collaboration app. Assumptions: transcripts per user 2–5 MB/month, on-device summarizer ~2–4 MB model, CPU","explanation":"## Why This Is Asked\nTests ability to reason about edge ML trade-offs, privacy, storage, energy, and cost modeling for a real-world, privacy-sensitive feature.\n\n## Key Concepts\n- On-device ML, model size, latency, and energy impact\n- Data minimization, encryption, opt-in/privacy costs\n- Cost buckets: storage, compute, telemetry, model updates\n- Go/no-go criteria and break-even calculation\n\n## Code Example\n```javascript\nfunction cba(params){\n  // mock: totalCost = storage + compute + privacyCosts - revenue uplift\n  const total = params.storage + params.compute + params.privacy;\n  const uplift = params.uplift; // monetary benefit from retention, tickets saved\n  return {netBenefit: uplift - total, paybackMonths: total / Math.max(uplift,1)};\n}\n```\n\n## Follow-up Questions\n- How would you validate model update cadence with bandwidth limits?\n- How to quantify uplift from retention vs. activation in a CBA?","diagram":"flowchart TD\n  A[Transcript Local on-device] --> B[On-device Summarizer]\n  B --> C[Encrypted Local Storage]\n  C --> D[Metadata Sync]\n  D --> E[Business Impact: Retention/Support]\n","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Lyft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T07:37:51.541Z","createdAt":"2026-01-21T07:37:51.543Z"},{"id":"q-5229","question":"You're evaluating a privacy-preserving on-device analytics SDK that lets apps collect events, compute coarse-grained aggregates locally (e.g., histograms, counts) using CRDTs, and periodically syncs only encrypted aggregates to a central server for product insights. Build an 18-month CBA: local storage per user, CPU cycles for aggregation, energy impact, encryption/sync overhead, privacy/regulatory costs, and expected business impact (decision quality, dashboard accuracy, revenue uplift); include go/no-go criteria and break-even horizon?","answer":"Propose an 18-month CBA: quantify per-user local storage (KB), CPU cycles for aggregation (ms/event), energy delta (mWh), encryption/sync bandwidth (KB/month), privacy/regulatory costs, and projected ","explanation":"## Why This Is Asked\n\nTests ability to model privacy-preserving analytics, not ML, with real-world trade-offs across storage, compute, energy, and compliance.\n\n## Key Concepts\n- Privacy budgets & CRDTs for local aggregation\n- Edge-resource constraints and telemetry design\n- Compliance costs and regulatory penalties\n\n## Code Example\n```javascript\n// Simple CRDT-like Grow-Only Counter\nclass GCounter { constructor() { this.counters = new Map(); }\ninc(k, v=1){ this.counters.set(k, Math.max((this.counters.get(k)||0)+v,0)); }\nmerge(other){ for(const [k,v] of other.counters){ this.counters.set(k, Math.max((this.counters.get(k)||0), v)); } }\nvalue(){ let s=0; for(const v of this.counters.values()) s+=v; return s; }\n}\n```\n\n## Follow-up Questions\n- How would you handle CRDT reconciliation when offline and merging after sync?\n- What metrics would you track to detect drift in your aggregates and how would you price the analytics feature?","diagram":"flowchart TD\nA[Event captured on device] --> B[Local CRDT aggregation]\nB --> C[Encrypt & batch aggregates]\nC --> D[Periodic sync to server]\nD --> E[Central analytics & dashboards]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Meta","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T11:38:57.437Z","createdAt":"2026-01-21T11:38:57.438Z"},{"id":"q-5267","question":"You're evaluating a privacy-preserving, on-device search indexing feature that builds a per-user local index from user signals, updates via federated diffs, and serves instant results offline. Design a 24-month CBA: per-user index size, RAM/CPU impact, battery drain, diff-transfer costs, privacy/compliance costs, maintenance, and expected business impact (search conversion, retention). Include go/no-go criteria and break-even horizon?","answer":"Per-user index ~4–6 MB; peak RAM 25–40 MB; CPU impact ~0.3–0.8 CPU-hours/day; battery drain ~1–3%/day; federated diff ~0.1–0.4 MB/day; privacy/compliance costs 5–8% infra. Expected uplift: 6–12% faste","explanation":"## Why This Is Asked\n\nTests ability to model a privacy-preserving on-device feature with federated updates, balancing memory, compute, energy, and compliance costs against business impact.\n\n## Key Concepts\n\n- On-device indexing and search\n- Federated/update diffs with privacy guarantees\n- Energy and latency trade-offs\n- Compliance, data deletion, and auditability\n- Break-even and go/no-go criteria\n\n## Code Example\n\n```javascript\nfunction diffUpdate(localIndex, diff) {\n  // apply partial updates safely\n  // merge diff entries into local index\n  for (const [k, v] of Object.entries(diff)) {\n    localIndex[k] = v;\n  }\n  return localIndex;\n}\n```\n\n## Follow-up Questions\n\n- How would data deletion rights (GDPR/CCPA) be enforced on-device without leaking signals?\n- How would you validate privacy guarantees and measure energy impact in field tests?","diagram":"flowchart TD\n  A[User Signals] --> B[Local Index]\n  B --> C[Query Latency]\n  B --> D[Federated Diff]\n  D --> E[Cloud Aggregation]\n  E --> F[Device Update]\n  F --> B","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T13:32:16.862Z","createdAt":"2026-01-21T13:32:16.863Z"},{"id":"q-843","question":"Given a CSV file with columns user_id, action, timestamp, write a Python function using only the standard library that returns the most recent action per user by deduplicating on user_id and keeping the latest timestamp; describe time complexity and edge cases?","answer":"Parse the CSV with the csv module, iterate rows, parse timestamp with datetime.fromisoformat, and keep a dict mapping user_id to (timestamp, action). If a row has a newer timestamp, replace the entry.","explanation":"## Why This Is Asked\nTests ability to implement a practical data-processing task: deduplicate by key using timestamps, relies only on standard library, and reveals awareness of edge cases like ties, malformed timestamps, and missing fields.\n\n## Key Concepts\n- CSV parsing with csv module\n- Deduplication by key using a dictionary\n- Timestamp parsing with datetime (ISO 8601)\n- Edge cases: missing fields, invalid timestamps, tie-breaking\n\n## Code Example\n```python\nimport csv\nfrom datetime import datetime\n\ndef latest_actions(csv_path):\n    best = {}\n    with open(csv_path, newline='') as f:\n        rdr = csv.DictReader(f)\n        for row in rdr:\n            uid = row['user_id']\n            ts = row['timestamp']\n            act = row['action']\n            try:\n                t = datetime.fromisoformat(ts)\n            except Exception:\n                continue\n            if uid not in best or t > best[uid][0]:\n                best[uid] = (t, act)\n    return [(uid, act, t.isoformat()) for uid, (t, act) in best.items()]\n```\n\n## Follow-up Questions\n- How would you adapt this for streaming data or files larger than memory?\n- If multiple actions share the same timestamp for a user, how would you break ties and what metadata would you add?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","PayPal","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:27:05.585Z","createdAt":"2026-01-12T13:27:05.585Z"},{"id":"q-872","question":"You are evaluating two deployment options for a new AI inference service under budget and latency constraints. Outline a practical, end-to-end cost-benefit analysis framework to decide which to deploy in production. Include: data you would collect (traffic, latency, SLA penalties, accuracy), metrics (NPV, ROI, payback), horizons, discount rate, handling uncertainty (scenarios), and a concrete calculation workflow you would run?","answer":"I would model total cost as compute, storage, and ops, plus latency penalties; benefits as incremental revenue or user value from faster or more reliable services. I would calculate NPV over 12-24 mon","explanation":"## Why This Is Asked\nTests the ability to structure a pragmatic CBA for ML deployment, balancing cost with user value, handling uncertainty, and communicating the result.\n\n## Key Concepts\n- Cost and benefits modeling\n- Time horizon and discounting\n- Scenario analysis (base/best/worst)\n- Metrics: NPV, ROI, risk-adjusted EV\n\n## Code Example\n```javascript\nfunction npv(cashFlows, rate) {\n  return cashFlows.reduce((acc, cf, i) => acc + cf / Math.pow(1 + rate, i+1), 0);\n}\n```\n\n## Follow-up Questions\n- How would you handle non-linear cost factors (e.g., burst traffic) in your model?\n- How would you present results to non-technical stakeholders?","diagram":"flowchart TD\n  DataInputs[Data Inputs] --> Option[Model Option]\n  Option --> Decision{Decision}\n  Decision --> Deploy[Deploy to Prod]\n  Decision --> Iterate[Iterate Optimization]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:54:31.848Z","createdAt":"2026-01-12T13:54:31.848Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":59,"beginner":18,"intermediate":21,"advanced":20,"newThisWeek":41}}