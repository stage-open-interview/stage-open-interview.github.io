{"questions":[{"id":"q-1020","question":"You’re evaluating a beginner feature: a daily market digest in a Robinhood-like app. Do a practical cost‑benefit analysis: state assumptions, estimate data/API costs, storage and engineering time, quantify benefits (retention lift, ARPU), and compute break-even time. Provide a rough calculation and rationale?","answer":"Assume 50k MAU, 4% retention lift, and $0.001 data cost per digest. Costs: data ~$120/mo, storage ~$20, engineering ~16h at $75/h = $1,200. Benefit: 7,500 users x $0.08/mo ARPU uplift for 6 months ≈ $","explanation":"## Why This Is Asked\n\nGauges ability to justify product bets with a pragmatic, numbers-driven approach and to handle uncertainty in inputs.\n\n## Key Concepts\n\n- Cost estimation\n- Benefit estimation\n- Break-even analysis\n- Reasonable assumptions\n\n## Code Example\n\n```javascript\n// Simple CBA calculator\nfunction cba(params) {\n  const {maU, lift, arpu, months, cost} = params;\n  const annualizedBenefit = maU * lift * arpu * months;\n  return {annualizedBenefit, cost, breakevenMonths: cost / (annualizedBenefit || 1)};\n}\n```\n\n## Follow-up Questions\n\n- How would you adjust if MAU grows or adoption changes?\n- What if ARPU uplift is not uniform across user segments?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:33:39.493Z","createdAt":"2026-01-12T19:33:39.493Z"},{"id":"q-1052","question":"Scenario: Real-time ingestion service receives JSON events from edge devices. Guarantee per-user throughput while allowing bursts in a distributed cluster. Design and implement a practical throttling mechanism, specify data structures, atomicity (e.g., Redis Lua script), failure modes, testing strategy, and observability?","answer":"Implement a per-user token bucket in Redis. Refill at 500 tokens/sec with a burst capacity of 1000 tokens (2s burst). On each event, atomically call a Lua script to consume 1 token; if available, forw","explanation":"## Why This Is Asked\n\nThis question tests practical rate-limiting in distributed systems, emphasizing per-user fairness, op readiness, and recovery from clock skew.\n\n## Key Concepts\n\n- Token bucket\n- Redis Lua scripts for atomic ops\n- Burst handling and backpressure\n- Observability and testing\n\n## Code Example\n\n```javascript\n// Redis Lua script (conceptual)\nlocal bucket = KEYS[1]\nlocal tokens = tonumber(redis.call('GET', bucket) or '0')\nlocal refill = tonumber(ARGV[1])\nlocal cap = tonumber(ARGV[2])\nlocal new_tokens = math.min(cap, tokens + refill)\nif new_tokens >= 1 then\n  redis.call('SET', bucket, new_tokens - 1)\n  return 1\nelse\n  return 0\nend\n```\n\n## Follow-up Questions\n\n- How would you test for clock skew and drift?\n- How would you adapt for multi-tenant fairness?\n","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","OpenAI","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:36:38.142Z","createdAt":"2026-01-12T20:36:38.142Z"},{"id":"q-1057","question":"In a real-time feed system using a contextual bandit with attention weighting (CBA), design a policy that balances short-term CTR and long-term engagement. Explain your reward decomposition, exploration strategy, and handling of non-stationarity. How would you validate offline with CPE and ramp online safely? Provide a concise update rule?","answer":"Implement a contextual bandit with attention weights. Reward = alpha * CTR + beta * retention, with alphas learned from data. Use Thompson Sampling or a neural head to estimate rewards. Update theta v","explanation":"## Why This Is Asked\nTests a candidate's ability to design production-ready policies balancing immediate metrics with long-term user value, handling non-stationarity, and validating safely.\n\n## Key Concepts\n- Contextual bandits and attention weighting\n- Reward decomposition for short-term vs long-term goals\n- Exploration strategies (Thompson Sampling, posterior updates)\n- Non-stationarity handling and evaluation (offline CPE, staged online ramp)\n\n## Code Example\n```python\n# Pseudo update step for theta\nimport numpy as np\n\ndef update_theta(theta, phi, reward, lr):\n    pred = np.dot(theta, phi)\n    grad = (reward - pred) * phi\n    theta += lr * grad\n    return theta\n```\n\n## Follow-up Questions\n- How would you handle feature drift in attention weights?\n- What offline metrics would you trust for CPE in this setup?","diagram":"flowchart TD\n  Context[Context] --> Features[Compute features with attention]\n  Features --> Decide[Policy selects action]\n  Decide --> Reward[Observe reward]\n  Reward --> Update[Update model]\n  Update --> Evaluation[Online/Offline evaluation]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Microsoft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:18:55.668Z","createdAt":"2026-01-12T21:18:55.668Z"},{"id":"q-1084","question":"Given a large social network planning to adopt a real-time feature flag evaluation service that runs on a hybrid stream/batch pipeline. Current pipeline: 1.2M events/sec, median latency 50 ms, 5% tail. New service promises 20–30% latency reduction and 25% cost increase, plus migration risk. Perform a cost-benefit analysis: quantify costs, benefits, risks, horizon (12 months), and decision rule with sensitivity ranges?","answer":"Quantify TCO over 12 months: infra/licensing, migration, and ops for both pipelines; quantify benefits from latency/throughput gains (fewer SLA penalties, higher retention, lower cost per event); incl","explanation":"## Why This Is Asked\n\nTests ability to perform a structured CBA for a real-time system, balancing latency impact, migration risk, and total cost of ownership. It also probes framing the horizon, risk quantification, and decision criteria.\n\n## Key Concepts\n\n- TCO, NPV, IRR\n- Latency vs cost trade-offs\n- Migration risk, rollback plans, and governance\n\n## Code Example\n\n```javascript\n// Pseudocode: simple NPV calculation\nfunction npv(r, cashFlows) {\n  return cashFlows.reduce((acc, cf, i) => acc + cf / Math.pow(1+r, i+1), 0)\n}\n```\n\n## Follow-up Questions\n\n- How would you model outsized risk of outages?\n- How would you present to executives under uncertainty?\n","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:18:59.423Z","createdAt":"2026-01-12T22:18:59.423Z"},{"id":"q-1114","question":"You’re migrating a real-time feature flag engine (multi-tenant SaaS) to reduce tail latency and cost. Propose a three-phase migration: centralized eval, regional edge gateway, then hybrid routing with per-tenant caches. Specify success metrics, rollback criteria, and a 12-month plan?","answer":"Propose a phased migration: 1) centralized eval gate, 2) regional edge gateways, 3) hybrid routing with per-tenant caches and rule-level debouncing. Metrics: P99 latency <15 ms, 99.9th <40 ms, cost pe","explanation":"## Why This Is Asked\n\nAssesses ability to design staged rollouts with measurable SLAs, cost models, and risk controls; emphasizes regional latency considerations and guardrails.\n\n## Key Concepts\n\n- phased migration strategy\n- tail latency and cost modeling\n- rollback criteria and canary policies\n\n## Code Example\n\n```javascript\nfunction evaluateFlag(flag, ctx) {\n  // simplified evaluation path\n  const gate = flag.gate || 'centralized';\n  const rules = flag.rules || [];\n  // in a real system this would consult caches and regional data\n  return rules.some(r => ctx.userRoles.includes(r.requiredRole));\n}\n```\n\n## Follow-up Questions\n\n- How would you monitor drift in evaluation accuracy across regions?\n- What metrics would you use to trigger a rollback and how would you implement it?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Hashicorp","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:25:36.721Z","createdAt":"2026-01-12T23:25:36.721Z"},{"id":"q-1151","question":"In a real-time analytics system for engagement on a large social app, design a privacy-preserving cohort analytics pipeline that ingests ~3M events/sec with sub-100 ms latency per region. Requirements: data residency, differential privacy for cohort counts, delta- or exact-once streaming state, drift detection, and cost-conscious multi-region deployment. Outline architecture, data contracts, and trade-offs?","answer":"Propose a privacy-preserving real-time cohort analytics pipeline using region-local streaming (e.g., Kafka + Flink), per-region state, and differential privacy (epsilon ~1.0) for cohort counts. Use wi","explanation":"## Why This Is Asked\nTests ability to design a privacy-conscious, low-latency analytics pipeline under multi-region constraints, with DP guarantees and drift detection.\n\n## Key Concepts\n- Real-time streaming with Kafka + Flink\n- Differential privacy (epsilon ~1.0)\n- Region-local state and windowed aggregations (<100 ms)\n- Drift detection and observability\n- Data residency and governance\n\n## Code Example\n```javascript\nfunction addDPNoise(count, epsilon) {\n  const sigma = Math.sqrt(2 / epsilon);\n  const noise = randn_bm() * sigma;\n  return Math.max(0, Math.round(count + noise));\n}\nfunction randn_bm() {\n  let u = 0, v = 0;\n  while (u === 0) u = Math.random();\n  while (v === 0) v = Math.random();\n  return Math.sqrt(-2.0 * Math.log(u)) * Math.cos(2 * Math.PI * v);\n}\n```\n\n## Follow-up Questions\n- How would you validate privacy guarantees in production?\n- What data contracts would you enforce with product teams to prevent over-collection?","diagram":"flowchart TD\n  Ingest[Ingest events] --> Stream[Stream processing]\n  Stream --> Cohort[Cohort analytics]\n  Cohort --> Privacy[DP noise]\n  Cohort --> Drift[Drift detection]\n  Drift --> Observ[Observability]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Meta","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:35:47.807Z","createdAt":"2026-01-13T01:35:47.807Z"},{"id":"q-1209","question":"You’re building an offline-first version of a daily digest app for low-connectivity users. Outline a minimal data model, eviction policy, and a practical plan to quantify cost savings from reduced network usage versus increased storage and complexity; provide a concrete example with rough numbers?","answer":"Use an offline cache storing up to M articles per user with fields: id, title, summary, url, publishedAt, hash. Evict by LRU or per-topic quotas. Background sync uses conditional GET/ETag to update ca","explanation":"## Why This Is Asked\nTests ability to design offline-first cache, quantify trade-offs, and produce concrete numbers relevant to mobile apps.\n\n## Key Concepts\n- Offline-first design\n- Data modeling for cached content\n- Cache eviction strategies (LRU, quota-based)\n- Cost-benefit estimation (bandwidth vs storage)\n\n## Code Example\n```javascript\n// Data model sketch\ntype Article = {\n  id: string;\n  title: string;\n  summary: string;\n  url: string;\n  publishedAt: string;\n  hash: string;\n};\n\nconst CACHE_LIMIT = 50; // per user\nfunction evictLRU(cache: Article[]) {\n  // simplistic placeholder\n}\n```\n\n## Follow-up Questions\n- How would you test offline scenarios across flaky networks?\n- How would you measure real-world bandwidth savings after rollout?","diagram":"flowchart TD\n  A[User opens digest] --> B{Online?}\n  B -- Yes --> C[Fetch latest and update cache]\n  B -- No --> D[Show offline cache]\n  C --> D","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","DoorDash","Goldman Sachs"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:24:07.770Z","createdAt":"2026-01-13T05:24:07.770Z"},{"id":"q-1238","question":"You're evaluating a beginner feature: a daily 'Portfolio Health Snap' panel that assigns a risk score to each user based on volatility of top holdings. Do a practical cost-benefit analysis: state assumptions, data/API costs, storage, and engineering time; quantify benefits (retention lift, ARPU) and compute break-even time. Provide rough calculations and rationale?","answer":"Assumptions: 5,000 DAU, one volatility check per user per day; API/data costs $0.0008/call; 10 MB storage; backend effort ~1.5 weeks. Benefits: +3% DAU, +$0.20 ARPU, improved retention by 2%. Break-ev","explanation":"## Why This Is Asked\nTests ability to plan ROI for a data-driven feature, balancing cost and user value, with beginner-friendly data and timing considerations.\n\n## Key Concepts\n- ROI modeling and payback\n- Data sourcing costs and API limits\n- Storage and compute budgeting\n- Validation and lightweight experimentation\n- Privacy and governance\n\n## Code Example\n```javascript\n// Simple break-even calculator\nfunction breakEvenDays(cost, upliftPerUserPerDay, dailyActiveUsers) {\n  const dailyGain = upliftPerUserPerDay * dailyActiveUsers;\n  return dailyGain > 0 ? cost / dailyGain : Infinity;\n}\n```\n\n## Follow-up Questions\n- How would you estimate the uplift values without a full launch?\n- What would be your MVP scope and validation plan?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Square","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:34:22.701Z","createdAt":"2026-01-13T06:34:22.701Z"},{"id":"q-1274","question":"You're assessing migrating a high-volume telemetry pipeline from a centralized data warehouse to a lakehouse with streaming ingestion and on-demand materialized views. Current throughput 5M events/sec, latency 5–7 min; target latency 2–3 min, 25% cost increase. Build a 12-month cost-benefit model: incremental storage/compute, streaming infra, data egress, drift/rollback costs; specify decision rules and sensitivity ranges?","answer":"12-month TCO model comparing a centralized data warehouse to a lakehouse with streaming ingestion and on-demand materialized views. Quantify incremental storage/compute, streaming infra (Flink/Spark),","explanation":"## Why This Is Asked\n\nThis question probes ability to design and justify moves to a lakehouse architecture, balancing cost, latency, data freshness, and risk in a measurable way.\n\n## Key Concepts\n\n- TCO modeling across storage, compute, and data transfer\n- Data freshness vs cost and drift/rollback risk\n- Sensitivity analysis and decision rules\n\n## Code Example\n\n```javascript\nfunction npv(cashFlows, rate) {\n  return cashFlows.reduce((acc, v, i) => acc + v / Math.pow(1+rate, i+1), 0);\n}\n```\n\n## Follow-up Questions\n\n- How would you measure and model data drift in this context?\n- What rollout milestones and metrics would trigger a rollback or full migration?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Citadel","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:39:19.754Z","createdAt":"2026-01-13T07:39:19.754Z"},{"id":"q-1403","question":"Design a real-time, fault-tolerant order processing pipeline for a high-traffic ecommerce system. Partition Kafka by user_id, use transactional producers for exactly-once ingestion, and idempotent consumers keyed by event_id. Maintain a durable log, emit to a warehouse via an idempotent sink, and support replay via event sourcing and schema evolution. Include chaos and backpressure tests?","answer":"Design a real-time, fault-tolerant order processing pipeline for a high-traffic ecommerce system. Partition Kafka by user_id, use transactional producers for exactly-once ingestion, and idempotent con","explanation":"## Why This Is Asked\n\nTests ability to design end-to-end, scalable data pipelines with strong correctness guarantees in production.\n\n## Key Concepts\n\n- Exactly-once semantics in Kafka with transactions\n- Per-user ordering via partitioning\n- Idempotent processing and event deduplication\n- Durable logs and write-ahead logging\n- Replayability and schema evolution in sinks\n\n## Code Example\n\n```python\n# simplified idempotent processor\ndef process(event, state, seen):\n    if event.id in seen:\n        return state\n    state = update(state, event)\n    seen.add(event.id)\n    return state\n```\n\n## Follow-up Questions\n\n- How would you test failure modes (partial commits, replay)?\n- How would you monitor lag and backpressure?\n","diagram":"flowchart TD\n  A[UserOrderEvent] --> B[Kafka: orders]\n  B --> C[Processor: per-user state]\n  C --> D[Sink: warehouse]\n  D --> E[Replay/Audit Log]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Instacart","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T15:44:36.305Z","createdAt":"2026-01-13T15:44:36.306Z"},{"id":"q-1472","question":"Design and implement a cost-benefit analysis module for a feature-flag rollout in a global streaming platform. Given 1000 edge regions, 1M users, latency budget 50ms, and known incremental costs, specify a data model, metrics to collect, uplift estimation, and a core function that returns whether to roll out. Include rollout policy and plan for validation via A/B tests in production?","answer":"Net value = (uplift_fraction × ARPU × users) − incremental_cost. If net value ≥ threshold, roll out; else pause. Model uplift with CI and regional variance for staged rollout. Provide a compact functi","explanation":"## Why This Is Asked\nAssess ability to translate business impact into technical rollout decisions, including statistical thinking and cost models.\n\n## Key Concepts\n- Cost-benefit modeling\n- A/B testing & uplift estimation\n- Edge compute cost vs latency trade-offs\n- Rollout policy and monitoring\n\n## Code Example\n```javascript\nfunction shouldRollout(uplift, arpu, users, cost, threshold){\n  const revenue = uplift * arpu * users;\n  const net = revenue - cost;\n  return net >= threshold;\n}\n```\n\n## Follow-up Questions\n- How would you handle uplift uncertainty (CI) in decision?\n- How would you budget rollouts across regions with variable costs?","diagram":"flowchart TD\n  A[Gather metrics] --> B[Compute uplift]\n  B --> C[Compute net value]\n  C --> D[Rollout decision]\n  D --> E[Monitor]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T18:47:18.476Z","createdAt":"2026-01-13T18:47:18.476Z"},{"id":"q-1508","question":"You're deploying a real-time personalization pipeline where a new ML model runs behind a feature flag with a 1% canary. If live drift is detected against offline benchmarks and latency deviates beyond a threshold, outline a concrete plan for rollout control, rollback, and data integrity checks that minimizes user impact while preserving auditability?","answer":"Propose a canary rollout for the new model with 1% traffic and a feature-flag toggle for immediate disable. Implement drift/QA guardrails: offline-live distribution comparison, latency/throughput chec","explanation":"## Why This Is Asked\nTests the ability to manage risk in live ML-style deployments, ensuring data integrity and latency goals while using feature flags and controlled rollouts.\n\n## Key Concepts\n- Canary deployments and feature flags\n- Drift detection and rollback strategies\n- Observability, alerts, and guardrails\n- Auditability and versioning of models/artifacts\n\n## Code Example\n```javascript\n// Pseudo drift check sketch\nfunction driftOk(liveDist, offlineDist, thresh){\n  const kl = klDiv(liveDist, offlineDist);\n  return kl < thresh;\n}\n```\n\n## Follow-up Questions\n- How would you choose drift thresholds? Why different for users vs. admins?\n- How would you structure pre-release validation and rollback testing before full reintroduction?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Apple","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T19:45:38.432Z","createdAt":"2026-01-13T19:45:38.432Z"},{"id":"q-1531","question":"Context: A streaming feature store for real-time personalization must decide between two deployment models under GDPR/CCPA constraints: (A) cloud-region-centric with EU data replicated to a central US region for global analytics, delivering tail latency 30–70 ms; (B) EU-first on-prem data plane with a lightweight cloud cache for global lookup, targeting 50–120 ms tail latency. Provide a 12–18 month cost model including data residency compliance, data transfer, storage, compute, tooling, outage risk, and a decision rule with sensitivity ranges?","answer":"Compute the 18-month Total Cost of Ownership (TCO) for both deployment models: (A) EU-regional cloud deployment with cross-region replication and analytics; (B) EU-first on-premises data plane with lightweight cloud cache. Decision rule: select the option with lower TCO that maintains 95th percentile latency under 100ms while ensuring full GDPR/CCPA compliance.","explanation":"## Why This Is Asked\n\nTests ability to reason about regulatory constraints, cross-region data flows, and real-time latency versus cost trade-offs in hybrid cloud environments.\n\n## Key Concepts\n\n- Total Cost of Ownership in hybrid cloud architectures\n- Data residency and compliance costs under GDPR/CCPA\n- Tail latency risk management and decision thresholds\n- Cross-region data transfer economics\n\n## Code Example\n\n```javascript\n// Pseudocode for TCO comparison\nfunction tcoA(params) { /* EU region + cross-region egress */ }\nfunction tcoB(params) { /* EU on-prem + cloud cache */ }\n```\n\n## Follow-up Questions","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:35:03.193Z","createdAt":"2026-01-13T20:48:17.164Z"},{"id":"q-1724","question":"You operate a real-time feature store for a global e-commerce platform. Ingested events arrive from multiple regions with clock skew and occasional late arrivals. You must deliver online feature values with tail latency under 100 ms while ensuring correctness when late data retroactively updates aggregates (e.g., cart_value_last_24h). Propose the architecture, covering data versioning, watermarking strategy, exactly-once processing, per-tenant isolation, and testing approach. Include a concrete late-event example and the system’s expected behavior?","answer":"I’d use a stream engine (e.g., Flink) with event-time processing, per-tenant keys, and exactly-once guarantees. Features live in a KV store with versioned rows and TTL, plus a delta store for late dat","explanation":"## Why This Is Asked\nThis assesses ability to design robust real-time feature stores with late data handling, multi-region constraints, and data governance. It also tests correctness guarantees and testability.\n\n## Key Concepts\n- Event-time processing with watermarks\n- Exactly-once semantics and state management\n- Versioned feature rows and delta handling\n- Per-tenant isolation and data residency\n- Backfill and auditability\n\n## Code Example\n```java\n// Flink pseudo-code: assign timestamps and watermarks, handle late data\nDataStream<Event> s = ...;\ns.assignTimestampsAndWatermarks(WatermarkStrategy.forMonotonousTimestamps());\n```\n\n## Follow-up Questions\n- How would you test watermark correctness in CI?\n- How would you validate cross-region consistency without leaking data?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","IBM","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T08:41:49.147Z","createdAt":"2026-01-14T08:41:49.147Z"},{"id":"q-1817","question":"Design and describe a scalable, fault-tolerant real-time collaboration pipeline: ingest per-document operations via a durable queue (Kafka), assign strict sequence numbers, and ensure exactly-once processing with idempotent workers. Explain data model, ordering guarantees, CRDTs vs OT, cross-region replication, failure modes, tests, and rollback strategy. How would you implement end-to-end?","answer":"Architect a scalable, fault-tolerant real-time collaboration pipeline: ingest per-document ops via a durable queue (Kafka), assign strict sequence numbers, and ensure exactly-once processing with idem","explanation":"## Why This Is Asked\n\nThis question probes the ability to design scalable, fault-tolerant real-time collaboration systems and reason about ordering, replication, and failure modes.\n\n## Key Concepts\n\n- Durable messaging and exactly-once processing\n- Temporal ordering and sequencing\n- CRDTs vs operational transformation\n- Cross-region replication and rollback\n\n## Code Example\n\n```javascript\nfunction applyOp(state, op) {\n  const key = op.docId + ':' + op.opId;\n  if (state.applied.has(key)) return state;\n  // apply op to CRDT state\n  state.applied.add(key);\n  // ... apply op\n  return state;\n}\n```\n\n## Follow-up Questions\n\n- How would you test idempotency guarantees under failure?","diagram":"flowchart TD\n  A[Client Editors] --> B[Ingestion Layer (Kafka)]\n  B --> C[Sequencer / Ordering]\n  C --> D[Event Store / CRDT State]\n  D --> E[Clients]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T11:51:17.817Z","createdAt":"2026-01-14T11:51:17.818Z"},{"id":"q-1874","question":"You’re evaluating a beginner feature: an in-app guided onboarding widget for a CRM product. Do a practical cost-benefit analysis: state assumptions, estimate data/hosting costs, content creation time, estimate uplift in activation and paid conversion, and compute break-even time. Provide a rough calculation and rationale?","answer":"Assume 5,000 monthly active users, 4% faster activation and 2% higher paid conversion due to guided tips, ARPU $25/mo. Costs: 2 engineers for 2 weeks at $70/hr ($4,480), content authoring $1,200, host","explanation":"## Why This Is Asked\nTests ability to translate a feature idea into a tangible cost-benefit plan, including rough inputs, trade-offs, and payback.\n\n## Key Concepts\n- Cost-benefit analysis\n- Activation and conversion uplift\n- ARPU and payback period\n- TCO: dev time, content, hosting, analytics\n\n## Code Example\n\n```javascript\nfunction breakEven(cost, monthlyRevenue) {\n  if (monthlyRevenue <= 0) return Infinity;\n  return cost / monthlyRevenue;\n}\n```\n\n## Follow-up Questions\n- How would you validate the uplift assumptions before full rollout?\n- What experiments would you run to de-risk the estimate (A/B tests, cohort analysis)?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T15:35:27.799Z","createdAt":"2026-01-14T15:35:27.800Z"},{"id":"q-1963","question":"How would you design a practical 1-day task to ingest daily payments from an Oracle source CSV into Snowflake, ensuring idempotent loads, proper schema mapping, and data quality checks, with a minimal Python MERGE-based load skeleton by transaction_id and a quick test plan?","answer":"Approach: stage the CSV in Snowflake, map fields to transaction_id, amount DECIMAL, status VARCHAR, tx_time TIMESTAMP. Use MERGE into analytics.transactions on transaction_id to upsert; set updated_at","explanation":"## Why This Is Asked\nAssesses practical data ingestion, idempotent upserts, and schema mapping skills suitable for junior roles.\n\n## Key Concepts\n- Data ingestion pipelines\n- Idempotent upserts with MERGE\n- Schema mapping and data quality checks\n- Uniqueness and deduplication\n\n## Code Example\n```javascript\n// Skeleton: upsert via MERGE using a Snowflake connection\nfunction upsertPayments(conn, stagingTable, targetTable) {\n  // Connect and issue MERGE ...\n}\n```\n\n## Follow-up Questions\n- How would you handle late-arriving data or schema changes?\n- How would you test with small synthetic datasets?","diagram":"flowchart TD\n  A[Ingest daily CSV] --> B[Stage to Snowflake]\n  B --> C[MERGE into canonical.transactions by transaction_id]\n  C --> D[Update updated_at and status]\n  D --> E[Quality checks]","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Snowflake","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T18:57:29.988Z","createdAt":"2026-01-14T18:57:29.988Z"},{"id":"q-1989","question":"You're evaluating replacing a centralized analytics pipeline with a federated learning-based recommendation model deployed on-device across 3 regions for 5M MAU; build a 2-year cost-benefit model including capex, opex, data-transfer savings, regulatory risk penalties, and uplift in engagement/ARPU, and specify the break-even horizon and go/no-go criteria?","answer":"Assume 5M MAU across three regions. Federated on-device inference cuts data transfer by 60% and privacy penalties by 0.5% of annual revenue. Capex $3M; Opex $2M over 2 years. Uplift: ARPU +$0.02, rete","explanation":"## Why This Is Asked\n\nTests ability to craft a quantified cost-benefit analysis for a privacy-first architecture, balancing engineering trade-offs with business impact across regions.\n\n## Key Concepts\n\n- Federated learning and on-device inference\n- Cost modeling (capex, opex, data-transfer savings)\n- Regulatory/privacy risk and penalties\n- ROI metrics (IRR, break-even, payback)\n- Multi-region scalability and latency considerations\n\n## Code Example\n\n```javascript\n// Minimal CBA calculator skeleton\nfunction cba({cost, benefit, horizon}) {\n  const net = benefit - cost;\n  const irr = (net / cost) * 100; // simplified\n  return {net, irr};\n}\n```\n\n## Follow-up Questions\n\n- How would you adjust if MAU grows to 10M within the horizon?\n- How do regional cost differences and latency constraints alter the model?\n- What experiments would you run to validate key assumptions before committing funds?","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T19:43:51.359Z","createdAt":"2026-01-14T19:43:51.359Z"},{"id":"q-2005","question":"In a 3-site edge anomaly-detection rollout for 10k sensors per site (roughly 100 GB/day), design a 2-year cost-benefit analysis for an on-device autoencoder with federated updates to a central model. Include capex for edge hardware, opex for cloud training and data transfer, and quantified benefits from reduced downtime, maintenance savings, and MTBF uplift. Provide break-even horizon and go/no-go criteria?","answer":"Assume 3 sites, 10k sensors/site, 100 GB/day. Capex: edge devices $1.2k each + gateways $3k/site. Opex: cloud training $0.20/GB, data transfer $0.05/GB/mo, ops $40k/yr. Benefits: downtime saved $150k/","explanation":"## Why This Is Asked\n\nThis question tests the ability to translate a complex edge-ML rollout into a precise 2-year CBA, balancing hardware, cloud costs, and real-world benefits like downtime and maintenance savings.\n\n## Key Concepts\n\n- Edge computing and on-device training\n- Federated learning and privacy implications\n- CAPEX vs OPEX modeling\n- Downtime reduction and MTBF uplift as financial drivers\n- Handling non-IID data and drift in multi-site deployments\n\n## Code Example\n\n```javascript\nfunction estimateROI(capex, opexPerMonth, savingsPerMonth, horizonMonths) {\n  const totalOpex = opexPerMonth * horizonMonths;\n  const totalCost = capex + totalOpex;\n  const totalSavings = savingsPerMonth * horizonMonths;\n  const roi = (totalSavings - totalCost) / totalCost;\n  return { totalCost, totalSavings, roi };\n}\n```\n\n## Follow-up Questions\n\n- How would you monitor and mitigate model drift across sites?\n- What metrics would you track to validate the ROI assumptions and trigger governance gates?","diagram":"flowchart TD\n  A[Edge sensors (10k/site)] --> B[Edge gateway/computation]\n  B --> C[On-device autoencoder]\n  C --> D[Federated update to central model]\n  D --> E[Central model synchronization]\n  E --> F[Operational analytics & ROI tracking]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","PayPal","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T20:42:25.190Z","createdAt":"2026-01-14T20:42:25.190Z"},{"id":"q-2057","question":"You’re evaluating a starter feature named 'Retention Policy Engine' for a Slack/HashiCorp-like chat app that auto-purges messages older than a per-channel window, with per-user holds and legal holds; design a 2-year cost-benefit model, including storage savings, indexing/compute overhead, engineering time, potential compliance penalties avoided, and a go/no-go break-even horizon?","answer":"Assume 5M MAU generating 1B messages/year (~1 TB data). Implementing per-channel retention windows of 12–24 months reduces active storage by approximately 60%, yielding ~$250K annual savings. The implementation requires ~$150K in engineering costs. Break-even occurs at ~2.5 years.","explanation":"## Why This Is Asked\nTests ability to translate retention policy decisions into a practical cost-benefit model, considering compliance, data safety, and operational impact.\n\n## Key Concepts\n- Data retention policy design for multi-tenant chat\n- Storage and compute cost modeling\n- Compliance, legal holds, and searchability trade-offs\n\n## Code Example\n\n```javascript\n// Simple retention calculation sketch\nfunction estMonthlyStorage(msgPerMonth, avgKB){\n  return (msgPerMonth * avgKB) / 1024; // GB\n}\n```\n\n## Follow-up Questions\n- How would you enforce per-channel legal holds without blocking purges?","diagram":"flowchart TD\n  A[Channel retention defined] --> B[Policy per channel stored]\n  B --> C[Scheduler triggers purge]\n  C --> D[Purge old messages, apply holds]\n  D --> E[Audit log and metrics updated]","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:51:08.299Z","createdAt":"2026-01-14T22:44:32.937Z"},{"id":"q-2086","question":"You're evaluating a privacy-preserving on-device NLP summarizer that runs on user devices with federated updates to a central model via secure aggregation. Design a 2-year cost-benefit model for deploying across Android and iOS in 4 regions with ~25M MAU. Include capex for secure enclaves and on-device storage, opex for federated aggregation and governance, update bandwidth, regulatory penalties for leakage, and uplift in engagement. Provide break-even and go/no-go criteria?","answer":"A comprehensive 2-year cost-benefit model comparing capital expenditures for secure enclaves and on-device storage against operational expenses for federated aggregation, while quantifying engagement uplift, regulatory risk mitigation, and ROI break-even analysis for cross-platform deployment.","explanation":"## Why This Is Asked\n\nAssesses the ability to construct a realistic, privacy-focused financial model for on-device machine learning at scale, balancing device-level constraints with centralized governance and regulatory compliance.\n\n## Key Concepts\n\n- On-device ML with secure federated aggregation\n- Edge infrastructure and enclave security costs\n- Federated update bandwidth and governance overhead\n- Data privacy penalties and regulatory risk exposure\n- Revenue uplift modeling and ROI break-even analysis\n\n## Code Example\n\n```javascript\n// Rough break-even calculator (simplified)\nfunction breakEvenCalculator(mau, regions, years) {\n  // Capex: Secure enclaves + on-device storage\n  const capex = {\n    enclaves: 5000000, // $5M for secure enclave infrastructure\n    storage: 2000000   // $2M for on-device storage optimization\n  };\n  \n  // Opex: Federated aggregation + governance + bandwidth\n  const opex = {\n    aggregation: 1000000, // $1M/year for federated learning\n    governance: 500000,   // $500K/year for privacy compliance\n    bandwidth: 300000     // $300K/year for update distribution\n  };\n  \n  // Revenue uplift from improved engagement\n  const engagementUplift = 0.05; // 5% increase in user engagement\n  const avgRevenuePerUser = 12; // $12/year average revenue\n  \n  // Calculate break-even point\n  const totalCapex = capex.enclaves + capex.storage;\n  const annualOpex = opex.aggregation + opex.governance + opex.bandwidth;\n  const annualRevenue = mau * avgRevenuePerUser * engagementUplift;\n  \n  const breakEvenMonths = Math.ceil(totalCapex / (annualRevenue - annualOpex) * 12);\n  \n  return {\n    totalInvestment: totalCapex + (annualOpex * years),\n    projectedRevenue: annualRevenue * years,\n    breakEvenMonths,\n    roi: ((annualRevenue * years - totalCapex - (annualOpex * years)) / \n          (totalCapex + (annualOpex * years))) * 100\n  };\n}\n```\n\n## Go/No-Go Criteria\n\n**Go Decision:**\n- Break-even < 18 months\n- ROI > 25% over 2 years\n- Regulatory risk reduction > 40%\n- Engagement uplift > 3%\n\n**No-Go Triggers:**\n- Break-even > 24 months\n- ROI < 15% over 2 years\n- Regional regulatory complexity > 3 jurisdictions\n- Device compatibility < 85% of target MAU","diagram":"flowchart TD\n  A[User devices] --> B[On-device NLP model]\n  B --> C[Secure aggregation hub]\n  C --> D[Central governance]\n  D --> E[Cost & benefit tracking]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","MongoDB","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:04:11.150Z","createdAt":"2026-01-14T23:36:57.391Z"},{"id":"q-2218","question":"You're evaluating a beginner feature: an in-app, offline-first guided onboarding using a localized FAQ (5 topics) and 3 short tutorial videos to cut first-week support tickets. Build a 2-year cost-benefit model: content creation time, local storage impact per user, update/hosting costs, uplift in activation, and reduction in first-week tickets; compute break-even horizon?","answer":"Assume 1M MAU, 2 MB local storage per user, 3 videos (0.8–1 MB each) cached offline, and 5 FAQ entries at ~1 KB. Content creation ~320 hours. Hosting/updates ~$0.01/user/month. Activation uplift 6–10%","explanation":"## Why This Is Asked\nTests ability to translate product impact into a financial model with explicit inputs, unit economics, and a concrete go/no-go horizon.\n\n## Key Concepts\n- Cost categorization: content, storage, hosting, engineering\n- Benefit metrics: activation uplift, ticket reduction\n- Timing: 24-month horizon, rollout impact\n\n## Code Example\n```javascript\nfunction breakEven(monthlyBenefit, upfrontCost) {\n  return upfrontCost / monthlyBenefit;\n}\n```\n\n## Follow-up Questions\n- How would you adjust when growth slows or uptake is uneven across regions?\n- What sensitivities would you run to validate the model?","diagram":"flowchart TD\n  A[Onboarding feature] --> B[Offline FAQ & videos cached]\n  B --> C[Reduced support tickets]\n  C --> D[Increased activation and retention]\n  D --> E[2-year cost-benefit break-even]","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Amazon","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T07:45:12.512Z","createdAt":"2026-01-15T07:45:12.512Z"},{"id":"q-2256","question":"You're evaluating a privacy-preserving on-device recommender for a mobile wallet app to surface merchant offers. Use federated learning with differential privacy across 3 regions and 5M MAU. Build a 2-year cost-benefit: edge hardware/storage, server aggregation, data transfer, content updates, regulatory penalties, activation uplift, ARPU uplift, and support costs. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","answer":"Two-year plan for a privacy-preserving on-device recommender in a payments app. 5M MAU across 3 regions; federated learning with differential privacy; estimate edge compute, server aggregation, and da","explanation":"## Why This Is Asked\nTests ability to build a complete, quantitative business case for privacy-conscious on-device ML at scale, including regulatory risk, multi-region considerations, and real-world KPIs.\n\n## Key Concepts\n- Federated learning with differential privacy\n- Edge compute vs. cloud aggregation trade-offs\n- Multi-region cost modeling (edge, bandwidth, infra)\n- Activation/ARPU uplift vs. risk penalties\n\n## Code Example\n```javascript\nfunction breakEven({activationUpliftPct, arpuLift, edgeCost, serverCost, penalty, months}) {\n  const revenue = (activationUpliftPct/100) * arpuLift * 1e6 * months;\n  const cost = (edgeCost + serverCost) * months + penalty;\n  return revenue - cost;\n}\n```\n\n## Follow-up Questions\n- How would you adjust the model if ARPU uplift is nonlinear?\n- Which metrics and signals would trigger a go/no-go decision in production?","diagram":"flowchart TD\n  A[On-device FL DP] --> B[Edge compute & storage]\n  A --> C[Server aggregation]\n  B --> D[Cost: device HW, energy]\n  C --> E[Cost: cloud infra, data transfer]\n  D --> F[Impact: activation uplift]\n  E --> F\n  F --> G[Break-even horizon]\n  G --> H[Go/No-Go]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","OpenAI","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T09:41:05.141Z","createdAt":"2026-01-15T09:41:05.141Z"},{"id":"q-2429","question":"You're deploying an on-device, privacy-preserving content moderation assistant for real-time group chats at Microsoft/Discord scale. It runs a distilled transformer on user devices to flag policy-violating messages, with periodic federated updates to keep the model aligned. Build a 2-year cost-benefit model: device storage and energy, on-device latency, update bandwidth, cloud aggregation costs, regulatory risk penalties, and expected safety/retention uplift. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","answer":"2-year TCO = device storage (2 MB/model × MAU) + on-device energy (avg ~0.2 W active) × active hours + federation updates (10 KB/week × MAU) + cloud aggregation costs. Benefit: reduced escalations, im","explanation":"## Why This Is Asked\n\nAssesss ability to translate privacy-preserving ML deployments into a lifecycle cost model and tie it to business outcomes.\n\n## Key Concepts\n- On-device ML, model distillation, federated updates\n- Cost modeling: storage, energy, bandwidth, cloud costs\n- Risk & mitigations: drift, false positives, DP\n\n## Code Example\n\n```javascript\n// Pseudocode for cost model calc\n```\n\n## Follow-up Questions\n- How would you validate model drift in production?\n- What opt-out metrics would you track and why?","diagram":"flowchart TD\n  A[Input Message] --> B[On-device Inference]\n  B --> C{Flagged?}\n  C -->|Yes| D[Mask/Flag Message]\n  C -->|No| E[Pass]\n","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T17:48:48.262Z","createdAt":"2026-01-15T17:48:48.262Z"},{"id":"q-2690","question":"You’re evaluating a new Data Quality as a Service (DQaaS) layer for a lakehouse used by Snowflake, Uber, and Databricks. The plan is to run continuous profiling, schema drift detection, and automated remediation across 4 domains (orders, users, payments, events) with 3 prod regions. Build a 12–18 month cost-benefit model detailing profiling compute, metadata storage, alerting, remediation actions, and data egress; include break-even horizon, go/no-go criteria, and 3–5 concrete quality rules with thresholds?","answer":"Propose a 12–18 month model: profiling compute (per-domain, per-region), metadata/storage for lineage, alerting, remediation automation, and cross-region data egress. 5 concrete rules: null rate >5%, ","explanation":"## Why This Is Asked\nTests ability to translate a business goal into a concrete cost-benefit plan for a lakehouse-quality feature, balancing compute/storage costs with real adoption gains and risk controls.\n\n## Key Concepts\n- Data quality rules and thresholds\n- Profiling compute and scheduling costs\n- Metadata storage and lineage impact\n- Automated remediation and guardrails\n- Break-even, ROI, and risk mitigations\n\n## Code Example\n```javascript\nfunction driftDetected(currentType, expectedType) {\n  return currentType !== expectedType;\n}\n```\n\n## Follow-up Questions\n- How would you quantify uplift in BI adoption and report accuracy?\n- How would you scale the DQaaS across more regions or tenants while preserving privacy and cost predictability?","diagram":"flowchart TD\n  A[Raw data ingested] --> B[Profiling job]\n  B --> C[Quality score]\n  C --> D[Alerts]\n  D --> E[Automated remediation]\n  E --> F[Updated lake data]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T07:01:54.140Z","createdAt":"2026-01-16T07:01:54.140Z"},{"id":"q-2717","question":"You're evaluating moving from a centralized real-time alerting system to edge-first, on-device anomaly detection for fraud in a mobile shopping app. Three regions (US, EU, APAC) total 5M MAU. Build a 2-year cost-benefit model including device storage, on-device inference costs, data transfer, cloud orchestrator costs, regulatory penalties, and uplift in fraud detection and retention. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","answer":"Propose a 2-year model: 5M MAU per region across US/EU/APAC; compare centralized real-time alerts to edge-first on-device anomaly detection for fraud in a mobile shopping app. Include device storage, ","explanation":"## Why This Is Asked\nAssess the candidate’s ability to structure a practical CBA for a distributed architecture shift, weighing latency, privacy, and cost against risk reduction and retention gains.\n\n## Key Concepts\n- Edge computing vs centralized analytics\n- Inference costs, device storage, data transfer\n- Regulatory/compliance penalties and risk\n- Uplift in fraud detection and user retention\n- Break-even analysis and sensitivity testing\n\n## Code Example\n```javascript\nfunction breakEven(initialCost, monthlySavings, monthlyCosts) {\n  let months = 0;\n  let net = -initialCost;\n  while (net < 0) {\n    net += (monthlySavings - monthlyCosts);\n    months++;\n  }\n  return months;\n}\n```\n\n## Follow-up Questions\n- How would you design the data governance model to satisfy privacy regulations across regions?\n- What mitigation plans exist if edge devices underperform (false positives, latency spikes, battery impact)?","diagram":"flowchart TD\nA[Centralized Alerts] --> B[Edge On-Device Analytics]\nB --> C[Cost Elements]\nC --> D[Storage, Inference, Data Transfer, Orchestration]\nB --> E[Benefits]\nE --> F[Fraud Uplift, Retention]\nF --> G[Break-even Horizon]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T07:49:33.320Z","createdAt":"2026-01-16T07:49:33.321Z"},{"id":"q-2735","question":"You’re evaluating a global fraud-detection feature for a ride-hailing app. Compare a hybrid deployment: (A) on-device anomaly detectors with federated updates across regions, (B) regional edge gateways with centralized analytics, under data residency constraints. Build a 2-year cost-benefit model including training, device energy, data transfer, hosting, false positives costs, and regulatory penalties. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","answer":"Hybrid model: on-device anomaly detector with federated updates plus regional edge gateway. Build a 2-year plan accounting device compute and energy, federated aggregation costs, data transfer, false-","explanation":"## Why This Is Asked\nThis question probes cost-aware architecture decisions under latency, privacy, and regulatory constraints for fraud detection. It tests modeling of on-device vs centralized trade-offs, and how to quantify benefits beyond raw accuracy.\n\n## Key Concepts\n- Edge vs on-device ML with federated updates\n- Data residency and regulatory risk\n- False positives cost and user impact\n- Opex vs Capex in hybrid deployments\n\n## Code Example\n```javascript\n// Simple break-even calculator (illustrative)\nfunction breakEven(initialCost, annualSavings) {\n  return initialCost / annualSavings;\n}\n```\n\n## Follow-up Questions\n- How would you validate production performance and drift for each model?\n- What mitigations would you implement if false positives rise and churn increases?","diagram":"flowchart TD\nA[Global Fraud Flow] --> B[On-device Federated Update]\nA --> C[Regional Edge Gateway]\nB --> D[Local Inference]\nC --> E[Central Analytics]\nD --> F[Action or Flag]\nE --> F","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","DoorDash","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T09:46:06.380Z","createdAt":"2026-01-16T09:46:06.380Z"},{"id":"q-2779","question":"Assess migrating a centralized image moderation pipeline to on-device edge moderation across 4 regions handling 2M requests/day. Build a 2-year cost-benefit analysis including edge hardware amortization, on-device model updates, quarterly bandwidth for diffs, cloud moderation backend costs, data-transfer savings, regulatory penalties, and accuracy/false-positive gains affecting retention. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","answer":"Propose a 2-year CBA comparing centralized vs on-device edge moderation. Include capex for edge devices, ongoing model training/updates, diff bandwidth costs, cloud hosting, data transfer savings, com","explanation":"## Why This Is Asked\n\nTests ability to quantify edge-versus-central trade-offs for a real-time, privacy-centric feature at scale, including regulatory and reliability risks.\n\n## Key Concepts\n\n- Hardware amortization and depreciation for edge fleets\n- Diff-based model updates vs full re-train bandwidth costs\n- Data-transfer savings and latency implications\n- Regulatory penalties and compliance costs\n- Break-even, NPV, and go/no-go criteria\n\n## Code Example\n\n```javascript\n// Pseudo-cost model sketch\nfunction model(costs) {\n  const { edgeCapex, opexCloud, bandwidth, penalties, retentionImpact } = costs;\n  // Simple ROI placeholder\n  const yearlySavings = retentionImpact * 1.2e6; // illustrative\n  return { ROI: (yearlySavings - edgeCapex - opexCloud - bandwidth - penalties) * 2, yearlySavings };\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt the model for changing user growth?\n- What data would you collect to validate the forecast after deployment?","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Cloudflare","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T11:37:18.677Z","createdAt":"2026-01-16T11:37:18.677Z"},{"id":"q-2806","question":"You're evaluating a beginner feature: on-logger data masking in analytics to redact PII before BI ingestion. Create a 12-month CBA: dev time (2 engineers for 3 weeks), hosting/storage overhead, tooling costs, privacy penalties avoided, uplift in dashboard adoption, and incident reductions; compute break-even and go/no-go criteria?","answer":"2 engineers x 3 weeks = about $26k labor; tooling $3k; storage overhead $50/mo. Privacy penalties avoided: ~$20k/yr. BI adoption uplift 8–12%, retention +2%, tickets reduced. Break-even ~9–12 months, ","explanation":"## Why This Is Asked\n\nThis question evaluates the ability to translate a compliance feature into a concrete, time-bound cost-benefit analysis suitable for a beginner. It tests how assumptions drive ROI and how to articulate go/no-go criteria.\n\n## Key Concepts\n\n- Cost estimation basics for small features\n- ROI calculation and break-even timing\n- Risk penalties avoided and adoption uplift\n- Simple, testable calculations and trade-offs\n\n## Code Example\n\n```javascript\nfunction breakEvenMonths(laborRatePerHr, weeks, engineers, monthlyOverhead, annualBenefit) {\n  const upfront = laborRatePerHr * weeks * engineers * 40;\n  const monthlyBenefit = annualBenefit / 12;\n  const netMonthlyBenefit = monthlyBenefit - monthlyOverhead;\n  if (netMonthlyBenefit <= 0) return Infinity;\n  return Math.ceil(upfront / netMonthlyBenefit);\n}\n```\n\n## Follow-up Questions\n\n- How would you adjust for ARR vs one-time benefits?\n- What metrics would you track after launch to validate forecasts?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T13:10:15.008Z","createdAt":"2026-01-16T13:10:15.009Z"},{"id":"q-2922","question":"You're evaluating a privacy-preserving fraud-detection feature for a mobile payments app. Compare (A) on-device risk scoring with secure enclaves and periodic federated updates vs (B) centralized scoring with differential privacy. Build a 2-year cost-benefit model including hardware/edge costs, data transfer, maintenance, latency, FP/FN costs, uplift in fraud detection and activation, regulatory penalties avoided, break-even horizon, and go/no-go criteria with mitigations?","answer":"Quantify total cost of ownership for both paths: edge licenses, secure enclave fees, federated update bandwidth, cloud DP charges, latency impact, and false-positive costs. Benefits to cite: 8–15% fra","explanation":"## Why This Is Asked\nTests ability to compare on-device vs centralized privacy-preserving ML in fintech, balancing privacy, latency, and regulatory risk.\n\n## Key Concepts\n- Cost modeling of edge vs cloud privacy tooling\n- Federated vs centralized DP approaches\n- Latency, FP/FN cost, and regulatory penalties\n\n## Code Example\n```javascript\n// ROI skeleton for illustration\nfunction roi(onDeviceCost, cloudCost, uplift, months){\n  return (uplift * 1000 * months) - ((onDeviceCost + cloudCost) * months);\n}\n```\n\n## Follow-up Questions\n- How would you handle model drift in both paths?\n- Which metrics would you track in production to decide go/no-go?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Coinbase","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T17:44:56.872Z","createdAt":"2026-01-16T17:44:56.872Z"},{"id":"q-2936","question":"You're evaluating a policy-driven, offline-first config-management engine for a distributed platform (like Consul/Terraform). The feature enables regional policy modules (JSON/YAML) to be evaluated locally on agents, with periodic delta updates from a central policy service when connectivity exists, and automatic conflict resolution when policies conflict. Build a 2-year cost-benefit model including authoring time, runtime overhead, update costs, data-transfer, compliance penalties avoided, uplift in deployment velocity, and break-even horizon; specify go/no-go criteria and edge-case mitigations?","answer":"Proposed model: three cost buckets (authoring/content creation, runtime overhead, update/ops) plus tangible benefits (compliance penalties avoided, faster deploys, audit readiness). Assume per-node CP","explanation":"## Why This Is Asked\nAssesses ability to translate policy-engine design into a pragmatic, multi-year business case with real-world constraints.\n\n## Key Concepts\n- Offline-first policy evaluation\n- Delta updates and conflict resolution\n- TCO vs business value, break-even analysis\n\n## Code Example\n```javascript\n// Pseudo-cost model sketch\nfunction yearlyBenefit(policies, regions, activationRate) {\n  const impact = policies.length * 1000 * activationRate;\n  return impact * regions.length;\n}\n```\n\n## Follow-up Questions\n- How would you model SLA penalties for policy drift and how would you test them?\n- What telemetry would you collect to validate the cost/benefit assumptions?","diagram":"flowchart TD\n  A[Policy Module] --> B[Local Eval]\n  B --> C[Conflict Resolve]\n  C --> D[Update Center]\n  D --> E[Policy Delta]\n  E --> F[Compliance & Audit]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T18:43:15.272Z","createdAt":"2026-01-16T18:43:15.274Z"},{"id":"q-2990","question":"You’re evaluating a beginner feature: a local-first search index for a mobile knowledge-base app used by field workers. The index caches documents and metadata on-device and syncs updates when online. Build a 12-month CBA: index size per user, CPU/memory impact, delta-sync costs, content update cadence, and projected uplift in offline task completion. Include go/no-go criteria and risk mitigations?","answer":"Index size ~4–6 MB per user; CPU ~2–6% on mid-range devices; memory ~60–120 MB; delta-sync adds 0.3–0.8 MB per update; offline lookup uplift 15–30%; dev effort ~2–3 weeks; backend data transfer ~$6k/m","explanation":"## Why This Is Asked\nThis angle tests the ability to quantify a data-heavy feature with a clear offline-first constraint, balancing device resources against business value.\n\n## Key Concepts\n- Offline-first architectures\n- Local indexing and delta synchronization\n- Resource budgeting (CPU, memory, storage)\n- Data transfer costs and break-even analysis\n\n## Code Example\n```\n// Implementation sketch: simple local search index\nclass LocalIndex {\n  constructor() { this.docs = []; }\n  build(docs) { this.docs = docs.map(d => ({ id: d.id, text: d.content })); }\n  search(q) {\n    const terms = q.toLowerCase().split(/\\s+/);\n    return this.docs.filter(d => terms.every(t => d.text.toLowerCase().includes(t)));\n  }\n  deltaUpdate(changes) {\n    // apply lightweight patches to docs\n    changes.forEach(c => {\n      const idx = this.docs.findIndex(d => d.id === c.id);\n      if (idx >= 0) this.docs[idx] = { ...this.docs[idx], ...c.patch };\n      else this.docs.push({ id: c.id, text: c.patch.content });\n    });\n  }\n}\n```\n\n## Follow-up Questions\n- How would you implement delta-sync to minimize data transfer while ensuring consistency?\n- What privacy controls would you layer to protect sensitive docs during offline usage?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T20:32:44.481Z","createdAt":"2026-01-16T20:32:44.482Z"},{"id":"q-3182","question":"You're evaluating a beginner feature: a client-side, offline-capable, autosave-enabled note editor for a CRM. Design and implement a debounced autosave to localStorage, restore on load, and handle cross-tab edits with a versioning strategy. Include a minimal code sketch and validation steps?","answer":"Implement a debounced autosave for a CRM note editor using localStorage. Save {content, lastSaved, version} with a 300ms debounce; restore on load. To handle multi-tab edits, use a version counter and","explanation":"## Why This Is Asked\\n\\nA practical UX feature tests offline resilience, local storage usage, and simple cross-tab conflict handling without server changes.\\n\\n## Key Concepts\\n\\n- Debounce autosave\\n- localStorage vs IndexedDB\\n- Versioning for conflict detection\\n- Cross-tab synchronization basics\\n\\n## Code Example\\n\\n```javascript\\n// Minimal autosave scaffold\\nlet timer;\\nfunction onChange(content){\\n  clearTimeout(timer);\\n  timer = setTimeout(()=> {\\n     const note = { content, lastSaved: Date.now(), version: (NOTE_VERSION++) };\\n     localStorage.setItem('noteDraft', JSON.stringify(note));\\n  }, 300);\\n}\\n```\\n\\n## Follow-up Questions\\n\\n- How would you test offline reload and cross-tab conflict?\\n- How would you handle storage quota and large notes?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Goldman Sachs","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T05:42:27.514Z","createdAt":"2026-01-17T05:42:27.514Z"},{"id":"q-3210","question":"You're evaluating migrating a centralized discounting and pricing engine for a marketplace app to a peer-to-peer edge-cached policy, with on-device decision rules and occasional server fallbacks due to latency and data locality. Build a 2-year cost-benefit model including compute, storage, data transfer, policy updates, fairness penalties, uplift in conversions, and risk of price wars; define break-even horizon and go/no-go criteria, plus fallback strategies?","answer":"Proposed answer outline: assume 3 regions, 5M MAU; migrate to edge-policy engine with 2 MB per user cache; reduce server pricing by 40%; content/telemetry updates cost $1.2M/year; on-device compute ne","explanation":"## Why This Is Asked\n\nThis tests ability to translate product changes into a full 2-year financial lens, including data locality, latency, and risk.\n\n## Key Concepts\n\n- Edge vs centralized cost models\n- Data locality, latency, drift, and governance\n- A/B planning, rollbacks, and risk mitigations\n\n## Code Example\n\n```javascript\nfunction breakEven(monthlyBenefit, upfront){\n  if(monthlyBenefit<=0) return Infinity\n  return Math.ceil(upfront / monthlyBenefit)\n}\n```\n\n## Follow-up Questions\n\n- How would you validate drift and fairness in production?\n- What are rollback criteria and metrics to trigger them?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Meta","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T07:00:06.310Z","createdAt":"2026-01-17T07:00:06.310Z"},{"id":"q-3263","question":"You’re evaluating a beginner feature: a privacy-preserving onboarding chatbot that operates offline with a local FAQ and optional server calls for telemetry only when the user opts in. Build a 12‑month cost-benefit model: content creation, on-device storage impact, minimal server costs, regulatory/privacy overhead, expected uplift in activation and paid conversions, and go/no-go criteria?","answer":"Outline plan: build a privacy-preserving onboarding chatbot that works offline with a local FAQ (5 topics) and optional telemetry. Content: 5 topics + 2 short videos. Dev: 1 engineer for 2 weeks. Stor","explanation":"## Why This Is Asked\nTests ability to reason about privacy-preserving onboarding, offline-first UX, and basic economics, plus handling opt-in telemetry trade-offs.\n\n## Key Concepts\n- Privacy by design and opt-in telemetry\n- On-device logic and offline UX\n- Unit economics: fixed vs variable costs, uplift estimation\n- Risk management: content drift, user confusion, opt-in bias\n\n## Code Example\n```javascript\nfunction breakEvenMonths(fixed, monthlyNet) {\n  if (monthlyNet <= 0) return Infinity;\n  return Math.ceil(fixed / monthlyNet);\n}\n```\n\n## Follow-up Questions\n- How would you validate uplift attribution given opt-in bias?\n- How would you validate offline usage metrics and ensure content stays up to date?\n","diagram":"flowchart TD\n  A[Open app] --> B[Local FAQ accessed]\n  B --> C{Telemetry opt-in?}\n  C -- Yes --> D[Send minimal telemetry]\n  C -- No --> E[No data transfer]\n  D --> F[Activation uplift]\n  F --> G[Go/No-Go decision]","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T09:29:51.701Z","createdAt":"2026-01-17T09:29:51.701Z"},{"id":"q-3401","question":"You're evaluating privacy-preserving mobile content moderation for a live video app: implement on-device real-time classification with weekly federated updates to improve policy rules. Build a 2-year cost-benefit model capturing on-device compute, memory, energy, uplink costs, server aggregation, DP noise, regulatory penalties avoided, and uplift in safe engagement; define break-even horizon and go/no-go criteria, including failure modes and mitigations?","answer":"Plan on-device inference with a lightweight classifier (0.5–1.5 ms per frame) plus weekly federated updates. Estimate costs: device compute, memory, energy, uplink; server aggregation & DP noise; cont","explanation":"## Why This Is Asked\n\nTests ability to design privacy-first moderation with on-device inference and federated learning, including lifecycle costs and risk controls.\n\n## Key Concepts\n\n- On-device ML workloads and model packaging\n- Federated learning with DP noise\n- Cost modeling for device and server infra\n- Metrics: FP/FN, latency, uplift, penalties avoided\n\n## Code Example\n\n```javascript\n// Pseudocode for per-frame inference budget\nlet t0 = performance.now();\nconst y = model.predict(frame);\nlet t1 = performance.now();\nconsole.log('inference ms', t1 - t0);\n```\n\n## Follow-up Questions\n\n- How would you handle model drift in production?\n- What testing plan ensures regulatory compliance?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","NVIDIA","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T14:39:57.347Z","createdAt":"2026-01-17T14:39:57.348Z"},{"id":"q-3477","question":"You're evaluating migrating a centralized on-call search feature for financial content to an edge-based offline index with federated updates across 3 regions. Build a 2-year cost-benefit model including edge hardware/storage per user, content update cadence, bandwidth savings, latency improvements, regulatory penalties, and user experience uplift; define break-even horizon and go/no-go criteria; include failure modes and mitigations?","answer":"Edge-based search index: hardware/storage per user 30–60 MB, quarterly content updates, 60–80% bandwidth saved. Latency drops from ~120 ms to ~40 ms; offline availability improves. Include data-locali","explanation":"## Why This Is Asked\n\nThis question probes a real-world cost-benefit scenario for edge computing in a金融 context, focusing on data locality, latency, and regulatory risk while requiring concrete numbers and go/no-go criteria.\n\n## Key Concepts\n\n- Edge vs central cost modeling\n- Data locality and regulatory penalties\n- Cadence of content updates and delta-sync strategies\n- Break-even analysis and sensitivity\n\n## Code Example\n\n```javascript\nfunction breakEven(cost, monthlySavings, months=24) {\n  let total = 0;\n  for (let m = 1; m <= months; m++) total += monthlySavings - cost;\n  return total;\n}\n```\n\n## Follow-up Questions\n\n- How would you quantify regulatory penalties for data locality breaches?\n- How would you validate latency improvements with real users and map to ARPU?","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T17:42:42.308Z","createdAt":"2026-01-17T17:42:42.308Z"},{"id":"q-3551","question":"You're evaluating a beginner feature: an adaptive animation and UI fidelity controller that reduces effects on devices with low battery or CPU to improve perceived responsiveness across iOS and Android. Build a 12-month CBA: estimate detection overhead, telemetry/data costs, maintenance, expected uplift in retention and satisfaction, and risk of degraded UX; provide go/no-go criteria and mitigations?","answer":"Present a concrete budget: detector cost per device, monthly telemetry/storage, maintenance headcount, and expected uplift in retention/time-on-app; compute break-even using ARPU and total user base, ","explanation":"## Why This Is Asked\n\nTests ability to translate a UX feature into a measurable business case, including per-device costs, data budgets, and risk management. It also assesses ability to set clear go/no-go criteria and design monitoring for a beginner-friendly feature.\n\n## Key Concepts\n\n- Cost estimation per device and per user\n- Telemetry/data budgets with privacy considerations\n- Go/No-Go criteria and risk mitigation\n\n## Code Example\n\n```javascript\n// Simple break-even helper\nfunction breakEven(uplift, yearlyCost) {\n  if (uplift <= 0) return Infinity;\n  return yearlyCost / uplift;\n}\n```\n\n## Follow-up Questions\n\n- How would you measure uplift and ensure robust A/B testing?\n- What privacy controls would you implement for telemetry data?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Apple"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T21:24:47.755Z","createdAt":"2026-01-17T21:24:47.757Z"},{"id":"q-3646","question":"You're evaluating a privacy-preserving on-device dispatcher optimization feature for a rideshare platform that runs per-driver route-choice inference inside secure enclaves, with federated model updates across 3 regions; build a 2-year cost-benefit analysis including edge hardware, secure enclave licensing, data transfer, key management, latency impact on dispatch, regulatory penalties, and uplift in on-time arrivals and rider satisfaction. Define break-even horizon and go/no-go criteria?","answer":"Develop a comprehensive 2-year cost-benefit analysis for a privacy-preserving on-device dispatcher optimization system that leverages per-driver secure enclaves with federated model updates across 3 regions. The analysis should encompass capital expenditures for Trusted Execution Environments (TEEs) and edge hardware, operational expenditures for encrypted data transfer, key management infrastructure, secure enclave licensing, and ongoing maintenance. Quantify the impact of latency improvements on dispatch operations, regulatory compliance risk mitigation, and measurable uplifts in on-time arrivals and rider satisfaction scores. Establish a clear break-even horizon and define quantitative go/no-go decision criteria based on ROI thresholds and risk-adjusted returns.","explanation":"## Why This Is Asked\nAssesses ability to model ROI for privacy-preserving distributed systems with strict latency constraints while balancing technical implementation against business outcomes.\n\n## Key Concepts\n- Trusted Execution Environments (TEEs)/secure enclaves\n- Federated learning and model updates\n- Data residency and compliance requirements\n- Latency budgets and performance metrics\n- Regulatory risk quantification\n- Hardware lifecycle management\n\n## Code Example\n```javascript\nfunction estimateROI(capex, opex, uplift, risk) {\n  // Rough calculation: annual net benefit = uplift * revenue\n  return (uplift * revenue - opex) / capex;\n}\n```","diagram":"flowchart TD\n  A[Scope] --> B[Costs]\n  B --> C[Benefits]\n  C --> D[Risks]\n  D --> E[Go/No-Go]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:01:30.536Z","createdAt":"2026-01-18T02:47:11.521Z"},{"id":"q-3837","question":"You're evaluating adding a real-time data quality monitoring microservice to a multi-region Lakehouse (Databricks-based) that tracks completeness, schema drift, and anomaly detection for streaming events (billions of events per day). Build a 2-year cost-benefit model including ingestion/processing costs, metadata storage, governance penalties avoided, uplift in downstream analytics accuracy, and operational overhead. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","answer":"Outline a 2-year model: capex for streaming infra, opex for compute/storage, drift models, and lineage; benefits from reduced data defects, faster time-to-insight, and avoided governance penalties; qu","explanation":"## Why This Is Asked\nReal-time data quality is critical in lakehouse environments serving analytics at scale; this question probes practical cost-benefit modeling, risk assessment, and trade-offs for monitoring at scale.\n\n## Key Concepts\n- Data quality monitoring for streaming data (completeness, schema drift, anomalies)\n- Multi-region cost model: capex vs opex, data ingestion/processing, metadata storage\n- Business value levers: downstream analytics accuracy, SLA penalties avoided, faster insights\n- Risk management: failure modes, mitigations, sensitivity analysis\n\n## Code Example\n```javascript\nfunction breakEven(annualBenefit, annualCost) {\n  return Math.ceil(annualCost / Math.max(annualBenefit, 1e-9));\n}\n```\n\n## Follow-up Questions\n- How would you set drift thresholds across regions and align them with SLAs?\n- What instrumentation would you add to validate that the quality service improves downstream KPIs?","diagram":"flowchart TD\n  A[Ingest Streaming Data] --> B[Run Quality Checks]\n  B --> C[Publish to Lakehouse]\n  B --> D[Quarantine & Notify]\n  D --> E[Governance & Audit]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Lyft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T11:31:26.064Z","createdAt":"2026-01-18T11:31:26.064Z"},{"id":"q-3907","question":"You're evaluating privacy-preserving on-device demand forecasting for a grocery-delivery platform. Compare a federated-learning with differential privacy setup across 4 regions (4M MAU) to a centralized cloud forecast. Build a 2-year cost-benefit model including edge hardware amortization, DP-noise overhead, orchestration and bandwidth, content updates, and regulatory penalties, plus uplift in SLA adherence and GMV. Provide break-even horizon and go/no-go criteria; specify failure modes and mitigations?","answer":"Edge CAPEX amortized over 3 years; DP noise budget per update; cloud orchestration and secure aggregation costs; inter-region bandwidth. Benefit: better forecast accuracy reduces stockouts, last-minut","explanation":"## Why This Is Asked\nTests ability to quantify cost and risk of privacy-preserving ML in a real-world, multi-region delivery scenario, including DP overhead, regulatory penalties, and business impact.\n\n## Key Concepts\n- Federated learning with differential privacy in multi-region deployments\n- Cost modeling: capex, opex, data transfer, penalties, and revenue uplift\n- Break-even analysis and risk mitigations for production ML\n\n## Code Example\n```javascript\nfunction netBenefit(savings, costs) {\n  return savings - costs;\n}\n```\n\n## Follow-up Questions\n- How would you set adaptive epsilon over time to balance privacy and utility?\n- What metrics and monitoring would trigger a rollback or cadence adjustment?","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Instacart","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T14:38:05.928Z","createdAt":"2026-01-18T14:38:05.928Z"},{"id":"q-3985","question":"Design a privacy-preserving cross-border attribution system for a delivery/booking app that measures campaign impact across 4 regions without sharing raw user signals. Use on-device buffering, secure aggregation, and periodic federated updates; enforce data residency and differential privacy on aggregates. Provide a 2-year cost model, break-even, and go/no-go criteria; include failure modes and mitigations?","answer":"Privacy-preserving cross-border attribution using on-device buffering, secure aggregation, and periodic federated updates. Data stays regional; add DP noise on aggregates. 2-year costs: edge storage/c","explanation":"## Why This Is Asked\n\nThis question probes ability to design privacy-preserving analytics at scale with regulatory constraints and a realistic cost model.\n\n## Key Concepts\n\n- Secure aggregation and on-device processing\n- Data residency and differential privacy\n- Cost modelling: edge vs cloud, data transfer, penalties\n- Reliability: event loss, retries, clock drift\n\n## Code Example\n\n```javascript\n// Pseudo on-device aggregation sketch\nfunction aggregateOnDevice(events){\n  // hash user, keep per-campaign counters in secure enclave\n  // return a masked sum for server\n  return maskedSum(events);\n}\n```\n\n## Follow-up Questions\n\n- How would you tune DP noise for campaign-level signals?\n- How would you adapt if one region imposes stricter retention limits?\n","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T18:46:33.616Z","createdAt":"2026-01-18T18:46:33.616Z"},{"id":"q-4012","question":"You're evaluating a beginner feature: an offline-first stock widget for a mobile checkout app used by field reps. The widget caches product data locally, syncs when online, and shows real-time stock/ETA. Build a 12-month CBA: per-SKU cache size, total SKUs cached, device storage constraints, delta-sync costs, ongoing maintenance, and projected uplifts in conversion and reductions in stock-out support tickets; specify break-even horizon and go/no-go criteria?","answer":"Propose a 12-month CBA for an offline-first stock widget. Assume 10k SKUs, 5 KB cache per SKU, 3 stores, 2 engineers for 4 weeks, delta-sync costs $0.002/MB, ongoing maintenance $6k/mo. Forecast 2% up","explanation":"## Why This Is Asked\nEvaluates applying CBA to a UX feature with offline data, storage, sync costs, and measurable business impact.\n\n## Key Concepts\n- Offline-first caching, data freshness, and sync costs\n- Mobile storage constraints across devices\n- Basic financial modeling: uplift, maintenance, and payback\n\n## Code Example\n```javascript\n// Pseudo: compute break-even month given uplift, cost, and tickets saved\nfunction breakEven(monthlyCosts, upliftRate, ticketsSaved) {\n  // placeholder logic\n  return Math.ceil(monthlyCosts / (upliftRate * 1000));\n}\n```\n\n## Follow-up Questions\n- How would you validate uplift estimates post-launch?\n- What metrics would you monitor for go/no-go decisions?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Scale Ai","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T19:31:36.980Z","createdAt":"2026-01-18T19:31:36.980Z"},{"id":"q-4054","question":"Evaluate a feature: an offline-first, on-device search index for a team messaging app (Slack-like) that indexes messages locally and syncs diffs when online. Build a 18–24 month CBA covering per-device storage, CPU cost, delta-sync traffic, on-device encryption overhead, regulatory/privacy risks, expected uplift in retention/usage, and a go/no-go criteria with break-even horizon?","answer":"Estimate per-user index size (10–30 MB), CPU time for indexing/search, storage costs across MAU, and delta-sync network traffic. Include encryption overhead and privacy risk exposure. Project uplift in retention/usage, calculate break-even horizon, and establish clear go/no-go criteria based on ROI thresholds.","explanation":"## Why This Is Asked\n\nTests the ability to model a feature with offline-first behavior and privacy constraints, focusing on cost drivers (storage, CPU, network) and benefits (retention, engagement) over 18–24 months, plus regulatory risk.\n\n## Key Concepts\n\n- Cost modeling\n- Data transfer and encryption overhead\n- Privacy and compliance\n- Break-even analysis\n- Trade-offs between local storage and cache invalidation\n\n## Code Example\n\n```javascript\n// Simple break-even helper (illustrative)\nfunction breakEven(revenuePerUser, costPerUser, months) {\n  const totalRevenue = revenuePerUser * months;\n```","diagram":"flowchart TD\nA[Offline on-device search] --> B[Index size per user]\nA --> C[Diff sync traffic]\nA --> D[On-device encryption]\nA --> E[Privacy/regulatory risk]\nA --> F[Business impact: uplift]\nB --> G[Costs]\nC --> G\nD --> G\nE --> G\nF --> G","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Slack","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:58:42.514Z","createdAt":"2026-01-18T21:38:44.841Z"},{"id":"q-4097","question":"You're evaluating a privacy-preserving on-device cohort analytics engine that uses secure aggregation across regions to generate user-segment insights without exposing raw data. Build a 2-year CBA: include per-device compute/memory, MPC costs, data transfer, content updates, regulatory penalties, uplift in decision quality, and support costs. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","answer":"Propose a comprehensive 2-year cost-benefit analysis for a privacy-preserving on-device cohort analytics engine utilizing secure aggregation across regions. Cost components include per-device CPU overhead of 2–6%, memory allocation of 20–60MB, multi-party computation orchestration and key management infrastructure, cross-region data transfer costs, content delivery system updates, regulatory compliance monitoring, and tier-3 technical support. Benefits encompass enhanced decision quality through richer user segmentation insights, reduced data privacy exposure risks, and improved regulatory alignment. Break-even is projected at 18 months with go/no-go criteria including device performance thresholds, MPC latency targets, and compliance audit outcomes. Key failure modes include device resource exhaustion, MPC node failures, and regulatory jurisdiction conflicts—mitigated through adaptive throttling mechanisms, redundant MPC infrastructure, and jurisdiction-specific data routing protocols.","explanation":"## Why This Is Asked\n\nTests the ability to quantify a privacy-preserving on-device analytics program from both cost and value perspectives, including MPC overhead, device constraints, data transfer, and legal risk.\n\n## Key Concepts\n\n- On-device compute and memory budgeting\n- Secure aggregation / MPC cost model\n- Data minimization and regulatory penalties\n- Total cost of ownership and break-even horizon\n- Operational metrics: latency, drift, audits\n\n## Code Example\n\n```javascript\n// Skeleton cost model\nfunction cba(params) {\n  const {cpuPerDevice, ramPerDevice, mpcOverhead, transferPerUser} = params;\n  // Implementation details...\n}\n```","diagram":"flowchart TD\n  A[User device] --> B[Secure aggregation]\n  B --> C[Regional MPC coordinator]\n  C --> D[Central dashboard / insights]\n  D --> E[Decision/actions]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:39:05.382Z","createdAt":"2026-01-18T23:39:21.960Z"},{"id":"q-4322","question":"You're evaluating a cost-aware, tiered real-time event processing pipeline for a SaaS analytics product that ingests 100M events per day across 3 regions. Propose a 2-year CBA for implementing a tiered processing model (gold/silver/bronze) with per-tenant cost attribution, dynamic sampling for bronze, and regulatory data-retention controls. Include CAPEX, OPEX, data-transfer costs, penalties, and expected uplift in retention and ARPU; specify break-even horizon and go/no-go criteria?","answer":"2-year CBA for a tiered real-time processor across 3 regions (100M events/day). CAPEX: upgraded streaming cluster + hot/cold storage tiers; OPEX: compute, storage, inter-region transfer, reprocessing.","explanation":"## Why This Is Asked\nTests ability to model multi-region, cost-aware streaming architectures with real-world data governance and business impact.\n\n## Key Concepts\n- Tiered processing, per-tenant costing, sampling, data retention policies, regulatory penalties.\n- CAPEX vs OPEX, cross-region data transfer economics, SLA risk.\n- ROI metrics: break-even horizon, NPV, ROAS, uplift in retention/ARPU.\n\n## Code Example\n```javascript\nfunction breakEven(monthlyBenefit, monthlyCost) {\n  // months to recover\n  return monthlyCost > 0 ? monthlyCost / monthlyBenefit : Infinity;\n}\n```\n\n## Follow-up Questions\n- How would you attribute costs across tenants and regions? \n- How would changes in data-regulation affect the model? \n- How would you monitor and adjust sampling to maintain SLAs?","diagram":"flowchart TD\n  A[Ingest Events] --> B[Tiered Processing: Gold/Silver/Bronze]\n  B --> C[Retention & Compliance Rules]\n  C --> D[Storage & Compute Costs]\n  D --> E[Cost Attribution & Billing]\n  E --> F[Go/No-Go Decision]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Netflix","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T13:34:07.917Z","createdAt":"2026-01-19T13:34:07.917Z"},{"id":"q-4332","question":"You're adding a lightweight, on-device offline translation feature to a mobile customer-support chat app used by field technicians. It caches phrase bundles for 20 languages, uses a small neural MT model, and syncs updates when online. Build a 12-month CBA: storage per language, total cache size, on-device latency, delta-update costs, maintenance, and projected uplift in first-contact resolution. Include go/no-go criteria?","answer":"Per-language bundle ~60KB; 20 languages → ~1.2MB per device. Target latency <180ms on mid-range CPU; memory impact ~1–2x input size. Delta sync monthly; assume 1GB/month bandwidth. Maintenance: model ","explanation":"## Why This Is Asked\n\nAssesses ability to reason about on-device ML tradeoffs, storage, latency, and multi-language data planning while producing actionable financials.\n\n## Key Concepts\n\n- On-device ML footprint\n- Cache sizing & delta updates\n- Latency, power, and UX trade-offs\n- TCO, break-even, go/no-go criteria\n\n## Code Example\n\n```javascript\nfunction estimateCachePerDevice(languages, sizePerLanguageKB){\n  return languages * sizePerLanguageKB / 1024; // MB\n}\n```\n\n## Follow-up Questions\n\n- How would you validate uplift with an A/B test?\n- How do you handle adding new languages mid-cycle without breaking existing caches?","diagram":"flowchart TD\n  A[Inputs: languages, perLanguageKB] --> B[Compute per-device cache size]\n  B --> C[Estimate latency & CPU impact]\n  C --> D[Cost for delta updates & hosting]\n  D --> E[Calculate break-even & go/no-go]","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","DoorDash","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T14:41:36.589Z","createdAt":"2026-01-19T14:41:36.589Z"},{"id":"q-4672","question":"You're evaluating a beginner feature: an offline-first customer support chat assistant that caches last 60 days of chat transcripts locally and suggests canned responses with NLP on-device. Build a 12-month CBA: per-user storage, CPU overhead, delta-sync costs, model update cadence, privacy/opt-in costs, and go/no-go criteria?","answer":"Per-user cache: 60 days of transcripts ~2–3 MB; on-device NLP model ~1–2 MB; delta-sync ~0.2–0.5 MB/mo. Costs: encryption, updates, privacy controls ~$0.03–0.07/MAU/mo. Benefits: 20–30% faster replies","explanation":"## Why This Is Asked\nAssesses ability to model real-world benefits, privacy costs, and operators for a lightweight NLP feature.\n\n## Key Concepts\n- Local caching\n- On-device ML footprint\n- Data sync costs and privacy.\n- ROI timing and go/no-go criteria\n\n## Code Example\n```javascript\nfunction cba(inputs){ /* compute CBA with storage, cpu, and benefit terms */}\n```\n\n## Follow-up Questions\n- How would you validate the privacy model and consent flow?\n- What metrics would you monitor post-launch to adjust the ROI assumptions?","diagram":"flowchart TD\n  A[Offline chat cache] --> B[Local storage]\n  A --> C[NLP on-device]\n  B --> D[Delta-sync]\n  C --> D\n  D --> E[ROI calculation]\n  E --> F{Go/No-Go}","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T07:44:23.667Z","createdAt":"2026-01-20T07:44:23.668Z"},{"id":"q-4693","question":"You're evaluating migrating a centralized analytics stack to a data mesh with domain-owned data products across 4 regions. Build a 2-year CBA for Kafka streaming, Delta Lake, and dbt-based pipelines; consider domain/platform costs, data duplication, governance, licensing/infra, regulatory penalties, uplift in decision velocity/quality, and MTTR. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","answer":"Migration plan: 2-year CBA for a data-mesh with domain-owned data products (4 regions, 200+ datasets), Kafka streams, Delta Lake, and dbt. Include domain/platform headcount, data-duplication & governa","explanation":"## Why This Is Asked\n\nAssessing a data-mesh transition tests understanding of ownership, data quality, and cross-team collaboration, plus real-world cost modeling across regions and vendors. It also probes risk budgeting for regulatory penalties and governance overhead.\n\n## Key Concepts\n\n- Data mesh vs centralized analytics\n- Domain-owned data products and governance\n- TCO: capex vs opex, data duplication, infra/licensing\n- Multi-region latency, data residency, and compliance\n- Break-even analysis and go/no-go criteria\n\n## Code Example\n\n```python\n# Simple cost model (illustrative)\ndef total_cost(domain_cost, infra_cost, dup_cost, reg_penalty):\n    return domain_cost + infra_cost + dup_cost + reg_penalty\n```\n\n## Follow-up Questions\n\n- How would you quantify data-duplication costs across regions?\n- What metrics determine break-even and when would you pause migration?","diagram":"flowchart TD\n  A[Centralized Stack] --> B[Data Mesh]\n  B --> C[Domain Teams]\n  B --> D[Platform Team]\n  C --> E[Data Products]\n  D --> F[Governance & Compliance]\n  E --> G[Business Impact]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T08:47:35.002Z","createdAt":"2026-01-20T08:47:35.002Z"},{"id":"q-4743","question":"You're evaluating an edge-first fraud-detection feature that runs entirely on mobile devices, flagging suspicious activity in real time without sending raw data to servers. Build an 18-month CBA: model size, on-device CPU and energy impact, incremental telemetry/storage, false-positive rate targets, regulatory/compliance costs, and expected business impact (revenue protection, user trust). Include go/no-go criteria and break-even horizon?","answer":"I’d forecast an 18‑month CBA: on-device model ~8 MB, +15–25% CPU cycles, negligible battery impact with batching; telemetry kept local unless consented; aim FP rate ≤1–2% to protect UX; expected uplif","explanation":"## Why This Is Asked\nThis question probes cost modeling for on-device ML with privacy constraints, trade-offs between latency, accuracy, energy, and regulatory risk.\n\n## Key Concepts\n- Edge CPU/energy impact and model size\n- FP/FN UX trade-offs and calibration\n- Privacy, telemetry, and compliance costs\n- Break-even horizon and go/no-go criteria\n- Model update cadence and maintenance\n\n## Code Example\n```javascript\n// rough CBA scaffold\nfunction cba(months, costs, savings){\n  let payback = Math.ceil(months * (costs - savings));\n  return {payback};\n}\n```\n\n## Follow-up Questions\n- How would you validate false positives in production?\n- How would regional privacy rules alter the model and costs?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","IBM","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T10:41:32.714Z","createdAt":"2026-01-20T10:41:32.714Z"},{"id":"q-4837","question":"You're evaluating a privacy-preserving on-device content moderation model for a real-time chat feature across iOS, Android, and web. Build a 24-month CBA: model size, on-device CPU cycles, energy impact, delta telemetry/storage, regulatory/compliance costs, and expected business impact (reputation, reduced moderation incidents, uplift in trusted usage); include go/no-go criteria and break-even horizon?","answer":"Propose a 24-month CBA for a privacy-preserving on-device content-moderation model in chat across iOS/Android/Web. Include: target model size (15-25 MB), on-device CPU cycles (0.5-1.5% of a mid-range ","explanation":"## Why This Is Asked\n\nTests ability to model real-world tradeoffs between privacy, latency, and cost. Requires understanding of device constraints, cross-platform deployment, and regulatory risk.\n\n## Key Concepts\n\n- On-device ML footprint and model sizing\n- Energy and CPU budgeting per user/session\n- Telemetry, local storage, and data minimization\n- Compliance costs and risk assessment\n- Business impact metrics: incidents, trust, activation, ARPU\n\n## Code Example\n\n```javascript\nfunction energyPerInference(modelSizeMB, cpuPercent, durationMs){\n  // rough proxy for energy\n  return modelSizeMB * cpuPercent * durationMs * 1e-6;\n}\n```\n\n## Follow-up Questions\n\n- How would you model break-even if moderation incidents drop by X%?\n- What telemetry would you capture and how would you protect privacy?","diagram":"flowchart TD\n  A[Input: user message] --> B[Run on-device moderation model]\n  B --> C{Pass/Flag}\n  C --> D[Action: allow or flag]\n  A --> E[Telemetry (opt-in)]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T15:44:32.981Z","createdAt":"2026-01-20T15:44:32.981Z"},{"id":"q-4937","question":"You're evaluating a beginner feature: an on-device accessibility module that live-subtitles product demo videos in offline field-sales scenarios by using a lightweight ASR model and user corrections to improve accuracy. Build a 12-month CBA: per-user storage, CPU/energy impact, cache size, model update cadence, ongoing annotation costs, and expected uplift in adoption and reduction in training/support tickets; include break-even horizon and go/no-go criteria?","answer":"12-month CBA for an on-device captioner: model ~5 MB, captions cache ~25–40 MB per user; CPU ~0.5–1.5% baseline, peak 150 mW; monthly delta updates; user corrections drive lightweight fine-tuning; exp","explanation":"## Why This Is Asked\n\nAssesses ability to quantify ROI for on-device ML features, balancing latency, storage, and energy with accessibility impact in offline contexts.\n\n## Key Concepts\n\n- On-device ML inference and model size\n- Energy and latency budgeting for mobile devices\n- Caching strategy for offline content\n- Feedback loops and lightweight fine-tuning\n- ROI, break-even, and go/no-go decision points\n\n## Code Example\n\n```javascript\n// Pseudocode: apply user corrections to lightweight fine-tuning\nfunction applyCorrections(model, corrections) {\n  // implement small adapter-tuning or gradient-free update\n}\n```\n\n## Follow-up Questions\n\n- How would you quantify latency targets (e.g., 2s caption) and accuracy impact?\n- What privacy guards and opt-in controls would you require for offline captions?","diagram":"flowchart TD\n  A[User starts video] --> B[Caption generation on-device]\n  B --> C{Caption quality acceptable?}\n  C -->|Yes| D[Cache caption locally]\n  C -->|No| E[User corrections]\n  E --> F[Lightweight fine-tuning]\n  F --> G[Delta model update]\n  G --> H[Apply update to next captions]","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Meta","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T20:28:06.494Z","createdAt":"2026-01-20T20:28:06.494Z"},{"id":"q-5041","question":"You're evaluating a privacy-preserving on-device graph-influence scoring feature to power a 'People You May Like' carousel without sharing raw user connections. Build an 18–24 month CBA around on-device embeddings, memory footprint, CPU cycles, energy use, delta telemetry, update cadence, compliance costs, and business impact (engagement lift, retention, revenue impact). Include go/no-go criteria and break-even horizon?","answer":"Propose an 18–24 month cost-benefit analysis for on-device graph-influence scoring: model size ~2.5M parameters (~10 MB), memory footprint ~20–30 MB peak, weekly update cadence; per-user CPU ~15 ms per update, energy consumption ~0.5 mWh; telemetry delta ~2 KB per user; compliance costs ~$0.5M for privacy audits; infrastructure savings ~$3M/year versus cloud processing; expected engagement lift 8–12%, retention improvement +5%, revenue impact +$4.2M/year; go/no-go criteria: ROI >1.5x within 18 months, battery impact <1% daily, memory <50 MB; break-even horizon ~14 months.","explanation":"## Why This Is Asked\nTests ability to size on-device ML systems, trade latency and energy against infrastructure savings, and quantify business impact under privacy constraints.\n\n## Key Concepts\n- On-device graph embeddings and inference\n- Privacy: no raw graph data, differential privacy options\n- CBA: capex/opex, telemetry, compliance costs, break-even\n- Performance goals: latency, battery impact, memory\n\n## Code Example\n```python\ndef cba(model_cost, server_savings, uplift, months=24):\n    return (months * server_savings) / model_cost * (1 if uplift>0 else 0)\n```\n\n## Follow-up Questions\n- How would you validate the engagement lift assumptions?\n- What privacy techniques would you implement beyond on-device processing?\n- How would you handle model versioning and rollback?","diagram":"flowchart TD\n  A[On-device graph embeddings] --> B[Local storage & RAM]\n  B --> C[Energy & CPU constraints]\n  A --> D[Telemetry reduction]\n  D --> E[Server cost savings]\n  E --> F[Break-even]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-21T05:14:56.569Z","createdAt":"2026-01-21T02:47:07.220Z"},{"id":"q-5069","question":"You're evaluating an on-device real-time speech-to-text and translation feature for a multilingual chat app that runs entirely offline on mobile devices, with on-device ASR and translation models updated via periodic federated updates. Build an 18–24 month CBA: model sizes, on-device latency, CPU cycles, energy, memory, update cadence, telemetry, translation accuracy targets, data-licensing/regulatory costs, and business impact (global accessibility, retention). Include go/no-go criteria and break-even horizon?","answer":"On-device ASR+translation for offline multilingual chat. Target per-language model 60–120 MB; latency <=150 ms; CPU ~8–12%; battery overhead ~1–2%/min; telemetry ~20 KB/user/mo; quarterly federated up","explanation":"## Why This Is Asked\nTests cost modeling for on-device ML with real-time UX, privacy, and regulatory trade-offs.\n\n## Key Concepts\n- On-device ML footprints, latency budgets, energy impact\n- Federated update cadence and telemetry strategies\n- Compliance costs vs revenue uplift and break-even\n\n## Code Example\n```javascript\n// Pseudo cost function for energy impact\nfunction energyCost(perMinMs, cpuUtil, hours) { /* ... */ return cost; }\n```\n\n## Follow-up Questions\n- How would you validate model drift and update cadence in production?\n- What are go/no-go thresholds for device categories with limited CPU?","diagram":"flowchart TD\n  A[User speaks] --> B[ASR on-device]\n  B --> C[Translate on-device]\n  C --> D[Display text]\n  D --> E[Telemetry anonymized]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","IBM","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T04:28:05.675Z","createdAt":"2026-01-21T04:28:05.675Z"},{"id":"q-5128","question":"Develop a 12-month CBA for a privacy-preserving on-device meeting summary feature in a mobile collaboration app. The app records meetings locally, caches transcripts, and syncs only anonymized metadata when online. Specify per-user storage, on-device model size, CPU/energy impact, update cadence, privacy/compliance costs, and go/no-go criteria?","answer":"Develop a 12-month CBA for a privacy-preserving on-device meeting summary feature in a mobile collaboration app. Assumptions: transcripts per user 2–5 MB/month, on-device summarizer ~2–4 MB model, CPU","explanation":"## Why This Is Asked\nTests ability to reason about edge ML trade-offs, privacy, storage, energy, and cost modeling for a real-world, privacy-sensitive feature.\n\n## Key Concepts\n- On-device ML, model size, latency, and energy impact\n- Data minimization, encryption, opt-in/privacy costs\n- Cost buckets: storage, compute, telemetry, model updates\n- Go/no-go criteria and break-even calculation\n\n## Code Example\n```javascript\nfunction cba(params){\n  // mock: totalCost = storage + compute + privacyCosts - revenue uplift\n  const total = params.storage + params.compute + params.privacy;\n  const uplift = params.uplift; // monetary benefit from retention, tickets saved\n  return {netBenefit: uplift - total, paybackMonths: total / Math.max(uplift,1)};\n}\n```\n\n## Follow-up Questions\n- How would you validate model update cadence with bandwidth limits?\n- How to quantify uplift from retention vs. activation in a CBA?","diagram":"flowchart TD\n  A[Transcript Local on-device] --> B[On-device Summarizer]\n  B --> C[Encrypted Local Storage]\n  C --> D[Metadata Sync]\n  D --> E[Business Impact: Retention/Support]\n","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Lyft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T07:37:51.541Z","createdAt":"2026-01-21T07:37:51.543Z"},{"id":"q-5229","question":"You're evaluating a privacy-preserving on-device analytics SDK that lets apps collect events, compute coarse-grained aggregates locally (e.g., histograms, counts) using CRDTs, and periodically syncs only encrypted aggregates to a central server for product insights. Build an 18-month CBA: local storage per user, CPU cycles for aggregation, energy impact, encryption/sync overhead, privacy/regulatory costs, and expected business impact (decision quality, dashboard accuracy, revenue uplift); include go/no-go criteria and break-even horizon?","answer":"Propose an 18-month CBA: quantify per-user local storage (KB), CPU cycles for aggregation (ms/event), energy delta (mWh), encryption/sync bandwidth (KB/month), privacy/regulatory costs, and projected ","explanation":"## Why This Is Asked\n\nTests ability to model privacy-preserving analytics, not ML, with real-world trade-offs across storage, compute, energy, and compliance.\n\n## Key Concepts\n- Privacy budgets & CRDTs for local aggregation\n- Edge-resource constraints and telemetry design\n- Compliance costs and regulatory penalties\n\n## Code Example\n```javascript\n// Simple CRDT-like Grow-Only Counter\nclass GCounter { constructor() { this.counters = new Map(); }\ninc(k, v=1){ this.counters.set(k, Math.max((this.counters.get(k)||0)+v,0)); }\nmerge(other){ for(const [k,v] of other.counters){ this.counters.set(k, Math.max((this.counters.get(k)||0), v)); } }\nvalue(){ let s=0; for(const v of this.counters.values()) s+=v; return s; }\n}\n```\n\n## Follow-up Questions\n- How would you handle CRDT reconciliation when offline and merging after sync?\n- What metrics would you track to detect drift in your aggregates and how would you price the analytics feature?","diagram":"flowchart TD\nA[Event captured on device] --> B[Local CRDT aggregation]\nB --> C[Encrypt & batch aggregates]\nC --> D[Periodic sync to server]\nD --> E[Central analytics & dashboards]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Meta","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T11:38:57.437Z","createdAt":"2026-01-21T11:38:57.438Z"},{"id":"q-5267","question":"You're evaluating a privacy-preserving, on-device search indexing feature that builds a per-user local index from user signals, updates via federated diffs, and serves instant results offline. Design a 24-month CBA: per-user index size, RAM/CPU impact, battery drain, diff-transfer costs, privacy/compliance costs, maintenance, and expected business impact (search conversion, retention). Include go/no-go criteria and break-even horizon?","answer":"Per-user index ~4–6 MB; peak RAM 25–40 MB; CPU impact ~0.3–0.8 CPU-hours/day; battery drain ~1–3%/day; federated diff ~0.1–0.4 MB/day; privacy/compliance costs 5–8% infra. Expected uplift: 6–12% faste","explanation":"## Why This Is Asked\n\nTests ability to model a privacy-preserving on-device feature with federated updates, balancing memory, compute, energy, and compliance costs against business impact.\n\n## Key Concepts\n\n- On-device indexing and search\n- Federated/update diffs with privacy guarantees\n- Energy and latency trade-offs\n- Compliance, data deletion, and auditability\n- Break-even and go/no-go criteria\n\n## Code Example\n\n```javascript\nfunction diffUpdate(localIndex, diff) {\n  // apply partial updates safely\n  // merge diff entries into local index\n  for (const [k, v] of Object.entries(diff)) {\n    localIndex[k] = v;\n  }\n  return localIndex;\n}\n```\n\n## Follow-up Questions\n\n- How would data deletion rights (GDPR/CCPA) be enforced on-device without leaking signals?\n- How would you validate privacy guarantees and measure energy impact in field tests?","diagram":"flowchart TD\n  A[User Signals] --> B[Local Index]\n  B --> C[Query Latency]\n  B --> D[Federated Diff]\n  D --> E[Cloud Aggregation]\n  E --> F[Device Update]\n  F --> B","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T13:32:16.862Z","createdAt":"2026-01-21T13:32:16.863Z"},{"id":"q-5359","question":"You're evaluating a beginner feature: a privacy-preserving on-device analytics collector in a mobile app that aggregates events locally and pushes only anonymized aggregates to the backend. Build a 12-month CBA: per-user storage, energy impact, delta-sync costs, privacy/compliance costs, update cadence, and go/no-go criteria?","answer":"Propose on-device analytics: batch events locally, send only anonymized aggregates daily. Storage ~0.3–0.6 MB/user/month; energy ~0.2–0.5% battery per 1k events/day; delta-sync ~5–20 KB per sync; comp","explanation":"## Why This Is Asked\nGauges practical CBA skills for privacy-conscious features that touch user data, emphasizing cost modeling, data minimization, and risk assessment.\n\n## Key Concepts\n- On-device processing, data minimization, and opt-in consent\n- Batch sizing, compression, and secure transmission\n- Cost levers: storage, CPU energy, network, compliance, and hosting\n- Go/no-go criteria tied to break-even and risk tolerance\n\n## Code Example\n```javascript\nfunction batchEvents(events, batchSize) {\n  const batches = [];\n  for (let i = 0; i < events.length; i += batchSize) {\n    batches.push(events.slice(i, i + batchSize));\n  }\n  return batches;\n}\n```\n\n## Follow-up Questions\n- How would you validate privacy risk post-implementation (DPIA outcomes, audits)?\n- What metrics signal a need to adjust batch size or cadence?","diagram":"flowchart TD\n  A[Local event queue] --> B[Batch & compress]\n  B --> C[Encrypt]\n  C --> D[Store on device]\n  D --> E[Daily sync]\n  E --> F[Backend aggregates]","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snap","Snowflake","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T19:11:28.719Z","createdAt":"2026-01-21T19:11:28.719Z"},{"id":"q-5526","question":"You're evaluating a client-side adaptive recommender that runs on-device to re-rank video suggestions based on local interactions, never sending raw history. Build an 18-month CBA covering model size, memory footprint, CPU energy, cache strategy, update cadence, data-transfer savings, privacy/compliance costs, and go/no-go criteria with success metrics?","answer":"Design a client-side adaptive recommender that runs on-device to re-rank video suggestions based on local, anonymized signals and recent interactions, never sending raw history. Provide a 18-month CBA","explanation":"## Why This Is Asked\nProbes on-device ML design, privacy trade-offs, and business impact for streaming platforms.\n\n## Key Concepts\n- On-device inference and memory budgeting\n- Privacy by design and data minimization\n- Update cadence vs model drift and latency\n- Cost modeling: compute, storage, bandwidth, and privacy/compliance\n\n## Code Example\n```javascript\n// Pseudocode: on-device re-ranking loop\nfunction rank(items, signals){\n  const scores = items.map(i => score(i, signals));\n  return items.sort((a,b)=> scores[b]-scores[a]);\n}\n```\n\n## Follow-up Questions\n- How would you measure break-even and define success metrics across regions?\n- What rollback strategies ensure user experience isn't degraded during model updates?","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T04:26:15.438Z","createdAt":"2026-01-22T04:26:15.438Z"},{"id":"q-5542","question":"You're evaluating a beginner feature: an offline-first admin console in a SaaS mobile app that lets operators run ad-hoc SQL-like queries against a locally cached telemetry dataset and then sync anonymized summaries to the cloud. Build a 12-month CBA covering storage, CPU, network, privacy/compliance, maintenance, and go/no-go criteria?","answer":"I would estimate per-user local cache at 40-80 MB for telemetry indexes, use SQLite, and cap query latency to sub-second averages. CPU impact 5-15% during peak queries; energy impact mitigated by batc","explanation":"## Why This Is Asked\n\nThis question probes ability to translate product needs into a practical CBA for a local analytics feature that operates offline.\n\n## Key Concepts\n\n- On-device data processing and SQLite usage\n- Local cache sizing, storage budgets, and query latency\n- Delta synchronization and network costs\n- Privacy/compliance (PII stripping, aggregation)\n- Maintenance costs and go/no-go criteria\n\n## Code Example\n\n```javascript\n// Lightweight cost calculator prototype\nfunction estimateCBA(cacheMB, queryLatencyMs, dailyDeltaMB) {\n  return {\n    storageCost: cacheMB * 0.02,\n    cpuCost: queryLatencyMs * 0.001,\n    networkCost: dailyDeltaMB * 0.05\n  };\n}\n```\n\n## Follow-up Questions\n\n- How would you validate assumptions with real metrics?\n- How would you handle data deletion and retention for offline caches?","diagram":"flowchart TD\n  A[Open console] --> B[Load local telemetry cache]\n  B --> C{Query requested?}\n  C -->|Yes| D[Execute on-device SQL-like query (SQLite)]\n  D --> E[Generate anonymized summary]\n  E --> F[Sync to cloud]\n  F --> G[Cloud analytics updates]","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T05:41:41.244Z","createdAt":"2026-01-22T05:41:41.244Z"},{"id":"q-5612","question":"You're evaluating a beginner feature: a client-side data lineage auditor for a web-based admin console that caches the last 90 days of lineage events locally and periodically syncs anonymized summaries to the backend. Build a 12-month CBA: per-user storage, CPU/energy impact, delta-sync costs, governance/compliance costs, update cadence, and go/no-go criteria?","answer":"Estimated 12-month CBA: per-user storage ~10 MB for 90 days, peak ~20 MB; CPU impact ~0.5–1% on a modern workstation; delta-sync ~0.2–0.5 MB/month; governance/compliance costs include masking, access ","explanation":"## Why This Is Asked\n\nExplore data-locality, privacy, and practical CBA modeling for a lightweight client-side feature in a cloud data platform.\n\n## Key Concepts\n\n- Data locality and offline caching\n- Cost modeling: storage, compute, network\n- Privacy/compliance: anonymization, retention, audits\n- Rollout strategy and go/no-go criteria\n\n## Code Example\n\n```javascript\nfunction estimateCBA(params) {\n  // simple cost model placeholder\n  const { storagePerUserMB, months, users } = params;\n  const storageCostPerMB = 0.02; // hypothetical\n  const totalStorage = storagePerUserMB * users * months;\n  return totalStorage * storageCostPerMB;\n}\n```\n\n## Follow-up Questions\n\n- How would you adjust the model for enterprise vs SMB customers?\n- What monitoring would you set to validate the forecast post-launch?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Microsoft","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T08:44:40.287Z","createdAt":"2026-01-22T08:44:40.287Z"},{"id":"q-5845","question":"You're evaluating a privacy-preserving on-device content moderation feature: a lightweight, multi-language classifier that runs entirely on mobile to filter disallowed content before it leaves the device, with per-country policy catalogs and federated policy updates. Build an 18-month CBA including per-user RAM/CPU energy, delta-model update costs, policy-management/compliance costs, false-positive/false-negative targets, and go/no-go criteria?","answer":"On-device content moderation: quantify per-user RAM/CPU energy, model size and latency, storage for country policy catalogs, delta-update bandwidth, and costs for regulatory compliance. Include false-","explanation":"## Why This Is Asked\nThis question probes practical cost modeling for edge privacy features with multi-language policy governance and federated updates. It tests ability to quantify resource budgets, update economics, and regulatory risk in a real-world scenario.\n\n## Key Concepts\n- On-device ML budgets: RAM, CPU, energy per inference\n- Federated policy updates and security\n- Multi-language policy governance and compliance\n- Evaluation metrics: FP/FN, precision/recall, uplift in safety\n- Break-even and go/no-go criteria\n\n## Code Example\n```javascript\nfunction estimateInferenceCost(modelMB, opsPerInference, batteryWh){\n  // simplistic proxy for energy cost per inference\n  const energy = (modelMB * 0.6) + (opsPerInference * 0.0005);\n  return energy; // in mWh\n}\n```\n\n## Follow-up Questions\n- How would FP/FN targets vary across languages and policies?\n- How do you handle policy drift and secure updates?","diagram":"flowchart TD\n  A[User content] --> B[On-device moderation]\n  B --> C{Policy check}\n  C -->|Flag| D[Block/Flag content]\n  C -->|Pass| E[Upload content]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Google","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T19:07:10.787Z","createdAt":"2026-01-22T19:07:10.788Z"},{"id":"q-5880","question":"You're evaluating a beginner feature: an offline-first navigation cache in a Tesla infotainment app. Cache top regional tiles (50k), use LRU eviction, and fetch delta maps when online; collect anonymized telemetry only. Build a 12-month CBA: per-region storage, on-device CPU/energy impact, delta-sync bandwidth, privacy/compliance costs, update cadence, and go/no-go criteria?","answer":"You're evaluating a beginner feature: an offline-first navigation cache in a Tesla infotainment app. Cache top regional tiles (50k), use LRU eviction, and fetch delta maps when online; collect anonymi","explanation":"## Why This Is Asked\nTests a practical offline-first design with attention to storage, energy, privacy, and cadence signals in a real vehicle or similar app.\n\n## Key Concepts\n- Offline-first caching\n- Tile cache sizing and eviction (LRU)\n- Delta updates vs full refresh\n- On-device energy/perf budgeting\n- Privacy by design and data minimization\n\n## Code Example\n```javascript\n// Pseudo: estimate energy for rendering a tile chunk\nfunction energyPerTileKB(kb, watts, seconds) {\n  return (watts * seconds) * (kb / 1024) / 3600;\n}\n```\n\n## Follow-up Questions\n- How would you measure storage vs network cost break-even over 12 months?\n- How would you test eviction under rapid navigation changes?","diagram":"flowchart TD\n  A[User requests navigation] --> B[Check on-device cache]\n  B --> C{Cache hit}\n  C -- Yes --> D[Render tiles from cache]\n  C -- No --> E[Fetch tiles from server]\n  E --> F[Tile storage update]\n  F --> G[Render and collect anonymized telemetry]\n","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T20:44:14.129Z","createdAt":"2026-01-22T20:44:14.129Z"},{"id":"q-5940","question":"You're evaluating a privacy-preserving on-device anomaly detector for app performance that runs in the background on iOS and Android to detect CPU/memory spikes without sending raw telemetry. Build a 24-month cost-benefit analysis: on-device model size, CPU cycles, energy impact, delta telemetry/storage, privacy/compliance costs, and expected business impact (uptime, user trust, revenue). Include go/no-go criteria and break-even horizon?","answer":"Develop an on-device detector using a tiny autoencoder or isolation forest. Target specifications: model size under 3 MB, CPU overhead below 2%, energy consumption approximately 5 mW, daily aggregated telemetry of 1–3 KB with no raw data transmission. Complete Data Protection Impact Assessment (DPIA) and ensure privacy compliance.","explanation":"## Why This Is Asked\nThis question explores a privacy-preserving on-device machine learning use case with real-world constraints including energy efficiency, latency requirements, data minimization principles, and regulatory compliance. It emphasizes the importance of measurable go/no-go criteria and a concrete break-even timeline for implementation decisions.\n\n## Key Concepts\n- On-device machine learning models (autoencoder/isolation forest)\n- Energy and performance budgeting for background tasks\n- Data minimization and privacy impact assessments\n- Go/No-Go criteria and break-even horizon analysis\n\n## Code Example\n```javascript\nfunction detectAnomaly(readings, threshold){\n  const mean = readings.reduce((a,b)=>a+b,0)/readings.length;\n  const variance = readings.reduce((a,b)=>a+Math.pow(b-mean,2),0)/readings.length;\n  return Math.abs(readings[readings.length-1] - mean) > threshold * Math.sqrt(variance);\n}\n```","diagram":"flowchart TD\n  A[Input: Telemetry constraints] --> B[On-device detector] \n  B --> C{Privacy checks}\n  C --> D[Aggregated signals to server]\n  D --> E[Business impact metrics]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","LinkedIn","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T04:28:38.373Z","createdAt":"2026-01-22T22:48:56.652Z"},{"id":"q-5951","question":"You're evaluating a new on-device, privacy-preserving delivery-slot optimizer that runs a lightweight ML model on customers' phones to suggest optimal delivery windows and assigns drivers, with periodic federated updates and no raw data leaves the device. Build an 18-month CBA: per-user model size, on-device compute and energy impact, federation bandwidth, update cadence, privacy/compliance costs, server-cost offsets from reduced telemetry, and expected business impact (on-time delivery, order value, driver utilization); include break-even horizon and go/no-go criteria?","answer":"Propose a lightweight on-device recommender (≤5 MB) with 2–4 ms inference, energy budget under 2 mJ per inference, and federated updates every 14 days at ~80 KB/user/month. Reduces server telemetry by 60–80%.","explanation":"## Why This Is Asked\n\nAssesses ability to design a realistic, privacy‑first on-device ML feature with end‑to‑end cost modeling and business impact.\n\n## Key Concepts\n\n- On-device ML and federated learning cadence\n- Energy, compute, and storage budgeting per user\n- Telemetry reduction vs privacy/compliance costs\n- Break-even horizon and go/no-go criteria\n\n## Code Example\n\n```javascript\nfunction estimateEnergyPerInference(modelSizeMB, freqMs) {\n  // simplified proxy: energy scales with model size and frequency\n  const base = 2; // mJ\n  return base * (modelSizeMB / 5) * (freqMs / 14 * 24);\n}\n```","diagram":"flowchart TD\n  A[Problem] --> B[CBA Modeling]\n  B --> C[On-device ML]\n  C --> D[Federated Updates]\n  D --> E[Go/No-Go Criteria]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:29:07.450Z","createdAt":"2026-01-22T23:36:02.711Z"},{"id":"q-6104","question":"You're evaluating a privacy-preserving edge API gateway extension that runs in client environments (browsers and mobile apps) to enforce quotas locally and aggregate usage statistics via secure local aggregation before sending anonymized deltas to the backend. Build an 18-month CBA: per-user storage for usage buckets, on-device CPU/memory impact, delta-sync bandwidth, privacy/compliance costs, rollout plan, and go/no-go criteria; include concrete numbers and a staged regional rollout?","answer":"18-month CBA: per-user local storage ~20 KB for 7-day buckets; on-device CPU ~1-3% of baseline; energy ~2-6 mJ per node/event; delta-sync bandwidth ~5-15 KB/day; privacy/compliance costs (DP/DPIA) ~$1","explanation":"## Why This Is Asked\n\nTests ability to quantify an edge privacy feature with real-world constraints: device impact, data locality, and regulatory costs, plus a phased rollout plan.\n\n## Key Concepts\n\n- Edge consent, local quotas, secure aggregation\n- Local storage budgeting and battery impact\n- Delta-based telemetry and privacy penalties\n- Regional rollout and go/no-go criteria\n\n## Code Example\n\n```javascript\n// Pseudocode: compute break-even given inputs\nfunction cba(params){ /* compute TCO, revenue uplift, break-even */ }\n```\n\n## Follow-up Questions\n\n- How would you validate these assumptions in production?\n- What telemetry would you collect to monitor drift without compromising privacy?","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Google","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T08:56:55.044Z","createdAt":"2026-01-23T08:56:55.044Z"},{"id":"q-6182","question":"You're evaluating a beginner feature: a client-side A/B experimentation runner that caches experiment flags and variant mappings in local storage and applies them without server round-trips, with weekly delta config updates. Build a 12-month CBA: per-user storage for flags, update cadence, delta-costs to fetch changes, privacy/compliance considerations, maintenance, and go/no-go criteria?","answer":"Describe per-user storage impact (KB per flag), weekly delta fetch costs, and offline usability. Assess privacy/data-minimization, update cadence, and maintenance overhead. Present a break-even horizo","explanation":"## Why This Is Asked\n\nThis question probes practical cost-benefit thinking for client-side experimentation, balancing latency, data transfer, and privacy across edge and cloud contexts.\n\n## Key Concepts\n\n- Local-first feature flags and variant mapping\n- Delta configuration updates and offline resilience\n- Privacy, data minimization, and compliance\n- Cost modeling: storage, fetch bandwidth, maintenance, and break-even analysis\n\n## Code Example\n\n```javascript\n// Simple config and selection logic\nconst FLAGS_KEY = 'ab_flags'\nfunction loadFlags(){\n  const raw = localStorage.getItem(FLAGS_KEY); return raw ? JSON.parse(raw) : {};\n}\nfunction selectVariant(flagName, userHash){\n  const flags = loadFlags();\n  const flag = flags[flagName];\n  if(!flag) return null;\n  // deterministic variant from user hash\n  const idx = Math.abs(hashCode(userHash + flagName)) % flag.variants.length;\n  return flag.variants[idx];\n}\nfunction hashCode(s){ let h=0; for(let i=0;i<s.length;i++){ h = (h<<5) - h + s.charCodeAt(i); h |= 0; } return h; }\n```\n\n## Follow-up Questions\n\n- How would you detect and handle stale flags in offline mode?\n- What telemetry would you collect to validate uplift without compromising privacy?","diagram":"flowchart TD\n  A[Start] --> B[Load local flags]\n  B --> C{Delta update needed?}\n  C -->|Yes| D[Fetch delta config]\n  C -->|No| E[Apply flags & render UI]\n  D --> E","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","NVIDIA","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T12:01:06.991Z","createdAt":"2026-01-23T12:01:06.991Z"},{"id":"q-6341","question":"You're evaluating a beginner feature: an offline-first, in-IDE code search index that builds a local index of a user's repository to enable instant keyword and symbol search, with optional anonymized telemetry sent to a backend once online. Build a 12-month CBA: index size per repo, CPU energy for indexing, delta-sync costs, privacy/compliance considerations, index refresh cadence, and go/no-go criteria?","answer":"Index size: 15 MB per repo; initial indexing 3–5 minutes CPU; weekly deltas 1–2 minutes; offline search speed improvement 40–70%; telemetry data ~0.3 MB/month per user; energy impact modest (phones) w","explanation":"## Why This Is Asked\nThis question probes practical CBA for a local-first search feature, balancing storage, CPU, and privacy.\n\n## Key Concepts\n- On-device indexing\n- Delta sync costs\n- Privacy/compliance\n- Update cadence and go/no-go criteria\n\n## Code Example\n```javascript\nfunction indexRepo(files) {\n  // naive tokenization\n  const index = {};\n  for (const f of files) {\n    const tokens = f.content.split(/\\\\W+/).filter(Boolean);\n    tokens.forEach(t => (index[t] ||= []).push(f.path));\n  }\n  return index;\n}\n```\n\n## Follow-up Questions\n- How would you measure user time saved and energy impact to validate the break-even?\n- How would you design for monorepos and very large repos with incremental indexing?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T19:53:08.832Z","createdAt":"2026-01-23T19:53:08.832Z"},{"id":"q-6494","question":"You're evaluating a cross-app data sharing feature that enables end-to-end encrypted, device-scoped data channels between a mobile app and a desktop app, governed by a user-managed consent ledger with revocation. The design must support offline eligibility checks and cross-platform identity binding. Build an 18-month CBA covering hardware-backed key storage costs, crypto compute, ledger maintenance, cross-platform data transfer, privacy penalties, and expected business impact; define go/no-go criteria and break-even horizon?","answer":"Outline an 18-month CBA for cross-app, end-to-end encrypted data sharing between mobile and desktop via a consent ledger. Include hardware-backed key storage, crypto compute costs, ledger maintenance,","explanation":"## Why This Is Asked\n\nExplores cross-device, privacy-preserving data sharing with consent governance—an area not covered by existing questions. It tests ability to model costs across crypto, ledger, and data transfer, and to define actionable go/no-go criteria.\n\n## Key Concepts\n\n- End-to-end encryption with device-scoped keys\n- Consent ledger design and revocation propagation\n- Cross-platform identity binding and offline considerations\n- Cost drivers: hardware-backed storage, crypto ops, data transfer, regulatory penalties\n\n## Code Example\n\n```javascript\n// Minimal consent ledger entry model\ntype LedgerEntry = {\n  id: string;\n  userId: string;\n  deviceId: string;\n  permissions: string[];\n  revokedAt?: string;\n  createdAt: string;\n}\n```\n\n## Follow-up Questions\n\n- How would you measure revocation propagation latency across devices?\n- What mitigations exist if a device is offline during revocation?","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Coinbase","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T04:33:07.541Z","createdAt":"2026-01-24T04:33:07.541Z"},{"id":"q-6804","question":"You're evaluating a privacy-preserving cross-tenant analytics feature: a client-side differential privacy (DP) on-device layer that aggregates signals locally and streams only encrypted, anonymized aggregates to a shared data lake to power a joint recommender between partner platforms without exposing raw data. Build a 24-month CBA: device CPU and battery impact, on-device DP overhead, secure-aggregation and encryption costs, data-transfer savings, compliance penalties, governance overhead, and go/no-go criteria; include break-even horizon and worst-case privacy leakage scenarios?","answer":"Propose DP budgets and secure aggregation thresholds; estimate device overhead: 8–12% CPU, 2–4% battery; cloud costs reduced via aggregation (~30–40%); data-transfer savings; projected ARPU uplift fro","explanation":"## Why This Is Asked\nTests ability to model privacy-preserving cross-tenant data flows and quantify business impact.\n\n## Key Concepts\n- Differential privacy\n- Secure aggregation\n- Cross-tenant data sharing\n- Cost modeling and break-even analysis\n- Regulatory compliance\n\n## Code Example\n```javascript\n// Pseudo: simple DP noise budget calculator\nfunction dpBudget(epsilon, n) {\n  const noise = Math.sqrt(n) / epsilon;\n  return noise;\n}\n```\n\n## Follow-up Questions\n- How would you validate DP settings with real user data?\n- What mitigations would you add if cross-tenant latency spikes occur?","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snap","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T18:04:26.272Z","createdAt":"2026-01-24T18:04:26.272Z"},{"id":"q-6900","question":"You're evaluating a beginner feature: a mobile CRM offline vault that stores user notes locally with client-side encryption and only syncs encrypted blobs to the backend when online. Build a 12-month CBA covering storage impact, encryption CPU costs, key rotation, delta-sync bandwidth, privacy/compliance costs, and go/no-go criteria?","answer":"Use AES-256-GCM with per-note IVs and a per-user wrapping key. Assume ~50 KB notes/year per user, 5–15% mobile CPU overhead for encryption/decryption, 1–2 MB of extra local storage per user, weekly delta-sync bandwidth of 200–500 KB per user, annual compliance costs of $0.10–0.25 per user for audit trails, and go/no-go criteria based on CPU overhead <20%, storage growth <3 MB/user, and privacy audit passing.","explanation":"## Why This Is Asked\n\nTests ability to reason about client-side privacy, offline UX, and cost trade-offs without server trust. The candidate must specify concrete numbers, crypto choices, and go/no-go criteria, balancing performance, storage, and regulatory costs.\n\n## Key Concepts\n\n- Client-side encryption and key management\n- Data growth and local cache sizing\n- Delta synchronization vs full sync\n- Privacy/compliance implications and auditability\n\n## Code Example\n\n```javascript\n// Example using Web Crypto API AES-GCM\nasync function encrypt(plaintext, cryptoKey) {\n  const iv = crypto.getRandomValues(new Uint8Array(12));\n  const encrypted = await crypto.subtle.encrypt(\n    { name: 'AES-GCM', iv },\n    cryptoKey,\n    new TextEncoder().encode(plaintext)\n  );\n  return { encrypted, iv };\n}\n```","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Oracle","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:01:45.593Z","createdAt":"2026-01-24T21:55:37.026Z"},{"id":"q-6957","question":"You're evaluating a new edge-first privacy-preserving receipt verification feature for payments at the mobile app level. The verifier runs on-device and checks signed receipts against a public key, avoiding backend round-trips for valid receipts. Build an 18-month CBA: hardware, crypto library choices, per-verify CPU/memory, energy impact, local telemetry/storage, key rotation costs, regulatory/compliance, rollout plan, and expected business impact (reduced backend load, faster checkouts, trust). Include go/no-go criteria and break-even horizon?","answer":"On-device receipt verifier using Ed25519; signature verification takes 6–10 ms on typical mobile CPUs; memory footprint ~120 KB; energy impact ~0.15 mW per verification. Local telemetry ~1 KB per check with storage for last 100 receipts; 18-month CBA shows 40% reduction in backend load, 25% faster checkout completion, and 15-month break-even horizon based on infrastructure savings.","explanation":"## Why This Is Asked\nThis evaluates understanding of edge cryptography implementation, privacy-preserving design, and business impact analysis in payment systems—skills critical for roles at Cloudflare, Square, and similar fintech companies.\n\n## Key Concepts\n- On-device cryptographic verification using Ed25519 and platform crypto APIs\n- Key management lifecycle and rotation strategies\n- Mobile device constraints: latency, CPU, memory, and energy budgets\n- Local telemetry architecture and storage growth patterns\n- Regulatory compliance frameworks for payment verification\n- Financial modeling with go/no-go criteria and break-even analysis\n\n## Code Example\n```javascript\n// Pseudo-code: verify an Ed25519 signature on-device\nasync function verifyReceipt(receipt, publicKey) {\n  // receipt: { data: ArrayBuffer, signature: ArrayBuffer }\n  const isValid = await crypto.subtle.verify(\n    'Ed25519',\n    publicKey,\n    receipt.signature,\n    receipt.data\n  );\n  return isValid;\n}\n```","diagram":"flowchart TD\n  A[Edge Receipt Verifier] --> B[Key Management]\n  A --> C[Performance Goals]\n  B --> D[Public Key Rotations]\n  C --> E[Go/No-Go Criteria]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T04:23:31.795Z","createdAt":"2026-01-25T02:38:47.834Z"},{"id":"q-6993","question":"You're evaluating an on-device real-time speech-to-text and summarization feature for a video-conferencing app that processes audio locally to preserve privacy. Build an 18-month CBA: model size and compression targets, per-device energy impact, latency goals (end-to-end <300 ms per segment), offline transcript storage, update cadence, and compliance costs. Define go/no-go criteria and break-even horizon, with validation metrics?","answer":"Model size ~25 MB after quantization, deployable via NNAPI/CoreML. Target latency <300 ms per 5–8s segment; on-device energy ~6–12% CPU on mid-range devices. Transcript storage ~100–200 MB/year per us","explanation":"## Why This Is Asked\n\nAssesses ability to design a privacy-preserving, latency-sensitive on-device feature with a complete financial lens. Tests trade-offs between model compression, energy, and user experience in a realistic conferencing product.\n\n## Key Concepts\n\n- On-device ML: ASR + summarization, quantization, NNAPI/CoreML\n- Cost modeling: capex/opex, telemetry, storage, update cadence, regulatory penalties\n- Metrics: latency, energy per call, WER/accuracy, storage footprint\n\n## Code Example\n\n```javascript\n// Pseudo-cost model\nfunction breakeven(cloudCost, onDeviceCost, months) {\n  return (cloudCost - onDeviceCost) / months;\n}\n```\n\n## Follow-up Questions\n\n- How would you measure latency/accuracy trade-offs in field tests?\n- What update strategy ensures security and minimizes user disruption?","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Bloomberg","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T04:29:05.597Z","createdAt":"2026-01-25T04:29:05.597Z"},{"id":"q-7019","question":"You're evaluating a cross-tenant, privacy-preserving adaptive feature-flagging system that runs a lightweight decision engine on user devices, aggregating only anonymized, per-feature impact summaries to drive a centralized experiment controller. Build an 18-month CBA: per-user on-device compute, local state storage, delta-sync costs, privacy/compliance, rollout risk, and go/no-go criteria; include break-even horizon and sensitivity bounds?","answer":"18-month CBA for cross-tenant privacy-preserving feature-flagging: on-device eval cost ~30 ms per flag, storage ~10 KB/user for local state, delta-sync ~2 KB/user/mo; privacy/audit costs; projected up","explanation":"## Why This Is Asked\nTests ability to model cross-tenant data sharing with privacy controls and practical engineering costs.\n\n## Key Concepts\n- On-device inference cost, delta-sync, privacy/compliance, break-even analysis.\n- Cross-tenant governance, SLA, latency, false positives.\n\n## Code Example\n```javascript\n// Pseudocode: cost model inputs and outputs\n```\n\n## Follow-up Questions\n- How would you handle drift in feature impact across regions? \n- What telemetry would you collect to validate uplift without compromising privacy?","diagram":"flowchart TD\nA[Devices] --> B{Local Eval}\nB --> C[Compute Impact]\nC --> D[Anon Summary]\nD --> E[Controller]\nE --> F[Update Experiments]\nF --> G[Rollout to Devices]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Databricks","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:50:33.416Z","createdAt":"2026-01-25T05:50:33.416Z"},{"id":"q-7109","question":"You're evaluating an on-device risk-scoring model for ride/shipment assignments that runs entirely in the device OS, using only local sensor data and anonymized trip context; no raw location or credentials leave the device. Build an 18-month CBA: hardware/firmware costs, OTA update cadence, per-device CPU/memory/energy impact, telemetry/storage budget, security/compliance costs, rollout plan, and expected business impact (risk reduction, insurance savings, rider/driver trust). Include go/no-go criteria and break-even horizon?","answer":"On-device risk-scoring model runs entirely in the device, no raw data leaves; 1–2 MB footprint, 5–10 ms inference, 0.3–0.8 W extra. OTA cadence every 4 weeks; per-device telemetry 1–5 KB/month; 6–12 w","explanation":"## Why This Is Asked\nTests ability to translate a privacy-centric on-device feature into a full cost-benefit plan with rollout, compliance, and measurable business impact.\n\n## Key Concepts\n- On-device ML inference cost and model residency\n- OTA update strategies and cadence\n- Security/compliance for client-only processing\n- Telemetry budget, data minimization, and privacy risk\n\n## Code Example\n```javascript\n// Simple per-device cost model snippet (illustrative)\nfunction costPerDevice(modelSizeMB, energyW, updatesPerMonth){\n  const energyCost = energyW * 0.01;\n  const updateCost = updatesPerMonth * 0.5;\n  return modelSizeMB * 0.01 + energyCost + updateCost;\n}\n```\n\n## Follow-up Questions\n- How would you validate that the on-device model does not leak sensitive information?\n- What calibration data would you collect to ensure break-even remains realistic across regions?","diagram":null,"difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Lyft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T09:39:00.315Z","createdAt":"2026-01-25T09:39:00.315Z"},{"id":"q-7128","question":"You're evaluating a beginner feature: an in-app learning coach for a CRM mobile app that runs entirely on-device to suggest micro-tutorials based on user actions, while sending only anonymized aggregates to the cloud for improvement every quarter. Build a 12-month CBA: per-user storage for the model and prompts, CPU/energy impact, delta-sync costs, privacy/compliance, update cadence, and go/no-go criteria with measurable ROI (activation, time-to-first-sale, support tickets)?","answer":"Propose: per-user on-device model 3-5 MB plus prompts; CPU/energy impact under 1-2% of device energy in idle, peaking during coaching. Delta-sync 5-20 KB/month per user; quarterly anonymized aggregate","explanation":"## Why This Is Asked\n\nThis question tests ability to reason about on-device ML, privacy, cost modeling, and ROI in an enterprise CRM context. It requires sizing storage, energy budgets, telemetry strategy, opt-in/privacy, and defining go/no-go criteria with measurable outcomes.\n\n## Key Concepts\n\n- On-device inference and model sizing\n- Data minimization and privacy/compliance\n- Delta-sync economics and network costs\n- ROI metrics: activation, time-to-value, support tickets\n- Update cadence and governance\n\n## Code Example\n\n```javascript\nfunction estimateStorage(modelSizeMB, promptsKB) {\n  return modelSizeMB + promptsKB / 1024;\n}\n```\n\n## Follow-up Questions\n\n- How would you quantify privacy risk and set opt-in thresholds?\n- How would you validate updates to the on-device coach without leaking data?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T10:34:18.486Z","createdAt":"2026-01-25T10:34:18.486Z"},{"id":"q-7152","question":"You're evaluating a privacy-preserving on-device content moderation module for a social/e-commerce app: it runs a lightweight classifier on the device to flag policy-violating reviews before syncing anonymized metadata (category counts, timestamps) to the backend. Build an 18-month CBA: model size, on-device latency/energy, per-user storage for local queue, delta-sync costs for metadata, false positives/negatives costs, regulatory/compliance penalties, and go/no-go criteria with break-even horizon?","answer":"Architect a privacy-preserving on-device content moderation model (e.g., quantized transformer) that flags policy-violating reviews locally and only pushes anonymized metadata. Detail estimated per-us","explanation":"## Why This Is Asked\nThis angle introduces on-device content moderation for policy compliance, shifting data away from servers and raising privacy, latency, and regulatory considerations. It complements prior on-device analytics and federated learning questions by focusing on content risk and governance trade-offs.\n\n## Key Concepts\n- On-device ML for content moderation with privacy\n- CBA across energy, latency, storage, and penalties\n- Regulatory risk and go/no-go criteria\n\n## Code Example\n\n```javascript\n// Pseudocode: lightweight on-device scoring placeholder\nfunction score(text) {\n  // assume a small quantized model\n  return (text.length % 5 === 0) ? 'flag' : 'allow';\n}\n```\n\n## Follow-up Questions\n- How would you handle model drift and updates without violating privacy?\n- What metrics would you track to ensure acceptable false-positive rates over time?","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","IBM","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T11:29:56.705Z","createdAt":"2026-01-25T11:29:56.705Z"},{"id":"q-7257","question":"You're evaluating a dynamic hybrid AI assistant feature: a mobile app that runs a lightweight on-device model for offline inference and selectively offloads to a privacy-compliant cloud region to improve accuracy when confidence is low. Build an 18-month CBA covering on-device memory/CPU energy, data-transfer costs, latency constraints, consent management, data residency/regulatory penalties, rollout strategy, and go/no-go criteria with break-even targets?","answer":"Hybrid policy: run a quantized on-device model (~60–80MB) and offload to a privacy-compliant cloud region when confidence is below 0.8; encrypt all data in transit and at rest; route to EU/US regions ","explanation":"## Why This Is Asked\n\nTests the ability to design hybrid privacy-conscious ML deployment with regulatory constraints, performance, and cost.\n\n## Key Concepts\n\n- Hybrid inference, on-device vs cloud offload\n- Data residency and regulatory penalties\n- Energy, latency, data transfer economics\n- Consent management and rollback\n\n## Code Example\n\n```javascript\n// Pseudo-logic\nif (confidence(modelOutput) < 0.8) offloadToCloud(regionEU);\n```\n\n## Follow-up Questions\n\n- How would you instrument energy and latency measurement per inference?\n- How would you handle data-retention and opt-out requests in this hybrid flow?","diagram":"flowchart TD\n  A[On-device model] --> B{Confidence >= 0.8?}\n  B -->|Yes| C[Return result]\n  B -->|No| D[Offload to Cloud(region compliant)]\n  D --> E[Cloud Inference] --> F[Return result]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Microsoft","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T15:37:26.583Z","createdAt":"2026-01-25T15:37:26.583Z"},{"id":"q-7330","question":"You're evaluating a beginner feature: an on-device accessibility coach that analyzes user navigation patterns (tap heatmaps and dwell time) to suggest UI improvements. The coach runs entirely on-device, with an optional weekly anonymized telemetry packet for aggregated UX insights. Build a 12-month CBA: per-user storage, CPU energy impact, delta-sync costs, privacy/compliance costs, update cadence, and go/no-go criteria?","answer":"Propose a 12-month CBA: define data scope (tap heatmaps, dwell time), per-user storage estimates, CPU energy budget for analysis bursts, delta-sync costs for weekly anonymized telemetry, privacy/compl","explanation":"## Why This Is Asked\nThis checks the ability to translate a UX accessibility feature into a pragmatic cost-benefit, balancing on-device processing with privacy and data governance.\n\n## Key Concepts\n- On-device analytics and energy budgeting\n- Data minimization, opt-in telemetry, privacy/compliance\n- Break-even, updates cadence, risk assessment\n\n## Code Example\n```javascript\n// rough energy proxy for analytics burst\nfunction energyCost(events){ return events * 0.5; /* mJ per event */ }\n```\n\n## Follow-up Questions\n- How would you cap telemetry to meet privacy goals while keeping insights useful?\n- What would trigger a go/no-go decision besides break-even (e.g., user opt-out rate, policy changes)?","diagram":"flowchart TD\n  AUser[User engages app] --> BCoach[On-device accessibility coach runs]\n  BCoach --> C[Local heatmaps/dwell data stored]\n  C --> D{Telemetry opt-in?}\n  D -->|Yes| E[Weekly anonymous delta-sync]\n  D -->|No| F[No server data sent]\n  E --> G[UX insights feed product]\n  G --> H[Decision: continue or pivot]\n","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Google","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T18:53:17.487Z","createdAt":"2026-01-25T18:53:17.488Z"},{"id":"q-7366","question":"You're evaluating a beginner feature: an on-device A/B test runner for a mobile chat app that assigns UI variants locally to protect privacy. Design a 12-month CBA that covers per-user storage for variant state, CPU energy impact of toggling variants, delta-sync costs if any, privacy/compliance considerations, update cadence for variant definitions, and go/no-go criteria?","answer":"Design a per-user local A/B harness that stores a variant_id and cohort in a tiny SQLite table (2–6 KB). On switch, CPU energy ~0.5–2 mJ; UI latency impact negligible. Delta-sync: optional weekly anon","explanation":"## Why This Is Asked\nThis tests designing privacy-preserving experiments with strict data minimization.\n\n## Key Concepts\n- On-device experimentation and data governance\n- Energy/CPU budgeting for mobile features\n- Lightweight telemetry and delta-sync trade-offs\n- Clear go/no-go criteria tied to business impact\n\n## Code Example\n```javascript\n// Example schema (SQLite-like)\nCREATE TABLE VariantCohort (user_id TEXT PRIMARY KEY, variant_id TEXT, cohort TEXT, created_at INTEGER);\n```\n\n## Follow-up Questions\n- How would you handle drift and recomputation of results on-device? \n- What if a user revokes consent mid-test?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Coinbase","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T20:33:26.125Z","createdAt":"2026-01-25T20:33:26.125Z"},{"id":"q-7459","question":"Design an 18-month CBA for an on-device explainable AI feature that provides user-visible rationales for content recommendations running entirely offline on mobile. Include model size, on-device latency, energy, storage for explanations, telemetry, update/signing costs, privacy/regulatory costs, rollout plan, and go/no-go criteria; specify break-even horizon and success metrics?","answer":"Propose an INT8-quantized model with an integrated explainability layer targeting a total on-device footprint of 12–20 MB, latency of 0.6–1.4 seconds per inference, and energy consumption of approximately 1–3 mJ. Include explanation payloads of 20–100 characters, local telemetry for model performance monitoring, an OTA update mechanism with cryptographic signing, privacy-preserving data collection, phased rollout over 18 months, break-even horizon of 14–16 months, and success metrics including user engagement, explanation quality scores, and cost reduction targets.","explanation":"## Why This Is Asked\n\nAssesses the ability to plan edge explainability under latency, privacy, and cost constraints; evaluates trade-offs in model size, energy efficiency, and regulatory compliance.\n\n## Key Concepts\n\n- Edge explainability with attribution vs. global model approaches\n- Model quantization and on-device runtime optimization\n- OTA updates and cryptographic signing for security\n- Privacy preservation and regulatory risk management\n- Cost modeling and break-even analysis\n\n## Code Example\n\n```javascript\n// pseudo-calculation of break-even point\nfunction breakEven(yearlyBackendCost, y","diagram":"flowchart TD\n  A[User requests recommendation] --> B[On-device model]\n  B --> C[Generate explanation]\n  C --> D[Present to user]\n  D --> E[Telemetry collection]\n  E --> F[OTA updates/signing]\n  F --> G[Backend cost savings]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:42:16.011Z","createdAt":"2026-01-25T23:59:28.810Z"},{"id":"q-7474","question":"You're introducing on-device telemetry sampling to cut data transfer: sample 1 in 100 events, aggregate hourly, and send only aggregates to the backend. Do a 12-month CBA: per-user storage, CPU/battery impact, delta-synchronization costs, privacy/compliance costs, update cadence, and go/no-go criteria based on acceptable error, data usefulness, and break-even time?","answer":"I would develop a comprehensive 12-month cost-benefit analysis that models on-device telemetry sampling at a 1:100 ratio with hourly aggregation. The analysis would quantify per-user storage requirements for both raw samples and aggregated data, assess CPU and battery impact through device-level microbenchmarking, calculate delta-synchronization costs based on aggregate size and transmission frequency, and factor in privacy/compliance expenses including opt-in consent flows, data retention policies, and regulatory compliance overhead. The model would establish clear go/no-go criteria using acceptable error margins, data usefulness thresholds, and break-even timeframes.","explanation":"## Why This Is Asked\nThis question evaluates your ability to analyze a data-reduction architecture that balances cost optimization with privacy preservation—a critical consideration for large-scale SaaS analytics platforms.\n\n## Key Concepts\n- On-device sampling strategies and aggregation patterns\n- Comprehensive cost modeling: storage, compute, network, and compliance\n- Privacy-by-design implementation and consent management\n- Break-even analysis and sensitivity modeling\n- Quality metrics and error tolerance thresholds\n\n## Code Example\n```javascript\nfunction aggregateTelemetry(events) {\n  let sum = 0, count = 0;\n  for (const event of events) {\n    if (event.sampled) {\n      sum += event.value;\n      count++;\n    }\n  }\n  return { sum, count, average: count > 0 ? sum / count : 0 };\n}\n```\n\n## Follow-up Questions\n- How would you implement adaptive sampling rates based on data quality requirements?\n- What monitoring systems would you deploy to track sampling effectiveness over time?\n- How do you handle edge cases where aggregated data exceeds quality thresholds?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:31:42.162Z","createdAt":"2026-01-26T02:48:12.030Z"},{"id":"q-7593","question":"You're evaluating a client-side data-access policy enforcer for mobile and web wallets that intercepts every outgoing API request, applies DLP rules locally, and blocks leakage without server round-trips. Build an 18-month CBA: policy-compiler footprint, runtime CPU/memory, energy impact, policy update cadence, telemetry/storage, false positives/negatives, regional regulatory costs, rollout plan, and go/no-go criteria; include concrete targets and break-even horizon?","answer":"On-device DLP engine must fit 1-2 MB WASM, run with <5% extra CPU on mid-range devices, negligible battery impact. Update policy bundles weekly; FP<1%, FN<0.1%. Telemetry/storage: 1-2 KB/user/day. Inc","explanation":"## Why This Is Asked\n\nThis tests how to quantify edge security features within cost and regulatory constraints, plus rollout risk.\n\n## Key Concepts\n\n- Client-side policy engines\n- DLP trade-offs: FP/FN, latency, UX\n- Signed bundle updates and rollback\n- Regional compliance costs and penalties\n- Break-even and milestone planning\n\n## Code Example\n\n```javascript\nfunction breEstimate(revenueLeakAvoided, costs, overhead) {\n  return (revenueLeakAvoided - costs) / (overhead || 1);\n}\n```\n\n## Follow-up Questions\n\n- How would you measure FP impact in production?\n- How would you handle policy drift or stale rules during rollout?\n","diagram":"flowchart TD\n  A[Client Interceptor] --> B[Policy Engine]\n  B --> C{Action}\n  C --> D[Allow]\n  C --> E[Block]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Plaid","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T09:00:38.676Z","createdAt":"2026-01-26T09:00:38.676Z"},{"id":"q-7784","question":"You're evaluating a beginner feature: a privacy-preserving presence and activity indicator for a collaborative document editor. It shows who is currently editing and their cursor positions, but only transmits minimal deltas and uses on-device aggregation when possible to reduce data sent to the server. Build a 12-month CBA: per-user data footprint, delta update costs, latency impact, privacy/compliance costs, opt-in controls, update cadence, and go/no-go criteria?","answer":"Implement a delta-based presence protocol: on-device aggregation of cursor/selection, send compact deltas only when changes occur, batch retries, and encrypt at rest/in transit. Estimate per-user foot","explanation":"## Why This Is Asked\n\nTests understanding of data footprint, latency budgets, and privacy controls for real-time collaboration features; also assesses ability to translate qualitative benefits into concrete, time-bound financial metrics.\n\n## Key Concepts\n\n- Delta encoding for presence data (cursor positions, editors yes/no)\n- On-device aggregation to minimize network traffic\n- Privacy/compliance considerations (opt-in, data minimization, retention)\n- Latency budgets for real-time UI updates\n- Cost modeling (server/storage, bandwidth) and break-even criteria\n\n## Code Example\n\n```javascript\n// Pseudo-code: compute and emit deltapresence\nfunction toDelta(prev, curr) {\n  // compare and return only changed fields\n  const delta = {};\n  if (prev.userA !== curr.userA) delta.userA = curr.userA;\n  // ... more fields\n  return delta;\n}\n```\n\n## Follow-up Questions\n\n- How would you validate the latency targets with simulated traffic?\n- What edge cases (offline edits, rapid cursor moves) could break delta consistency?","diagram":"flowchart TD\n  A[Client] --> B[ComputeDelta]\n  B --> C[EncryptSend]\n  C --> D[ServerPresence]\n  D --> E[UIUpdate]","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Square","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T17:45:09.959Z","createdAt":"2026-01-26T17:45:09.959Z"},{"id":"q-7893","question":"You're evaluating a mobile app feature that runs an on-device voice assistant for task completion. All audio processing and NL processing happens on-device; updates arrive as small patch deltas without re-downloading the full model. Build an 18-month CBA: model size targets, per-utterance CPU/memory, end-to-end latency, energy impact, local telemetry/storage, patch update costs, data-minimization/regulatory costs, and go/no-go criteria with a break-even horizon. Include a staged regional rollout and rollback plan?","answer":"Implementing an on-device voice NLU system with patch-based updates requires a comprehensive 18-month cost-benefit analysis. Key targets include: model size optimization to 6-12 MB, end-to-end latency under 350ms per utterance, energy consumption of 1-2 Joules per interaction, local telemetry and storage footprint of 20-40 MB, biweekly patch update cadence, robust data minimization protocols, and regulatory compliance costs. The deployment strategy should feature a staged regional rollout with clearly defined rollback triggers, targeting break-even at 12-14 months. Go/no-go decisions will be based on achieving performance benchmarks and staying within cost thresholds.","explanation":"## Why This Is Asked\nThis question evaluates the ability to translate on-device ML constraints into practical business decisions, requiring technical expertise balanced with financial analysis and strategic planning.\n\n## Key Concepts\n- On-device ML optimization and resource constraints\n- Patch-based model update mechanisms\n- Performance budgets (latency, energy, memory)\n- Data minimization and privacy compliance\n- Phased deployment and risk management\n\n## Code Example\n```javascript\n// CBA calculation framework for on-device voice assistant\nfunction calculateCBA(months, implementationCost, operationalCost, userValue, complianceCost) {\n  const totalCost = implementationCost + (operationalCost * months) + (complianceCost * months);\n  const totalRevenue = userValue * months;\n  return {\n    netValue: totalRevenue - totalCost,\n    breakEvenMonth: Math.ceil(implementationCost / (userValue - operationalCost - complianceCost)),\n    roi: ((totalRevenue - totalCost) / totalCost) * 100\n  };\n}\n```\n\n## Follow-up Questions\n- How would you estimate energy impact per utterance across different device classes?\n- What metrics would you use to determine rollback triggers during regional rollout?\n- How do you balance model accuracy against size constraints for on-device deployment?\n- What compliance frameworks should be considered for voice data processing?","diagram":"flowchart TD\n  A[On-device voice pipeline] --> B[Local audio capture]\n  B --> C[On-device ASR/NLU]\n  C --> D[Action routing]\n  D --> E[Remote patch update server]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Slack","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T06:24:44.686Z","createdAt":"2026-01-26T21:51:45.333Z"},{"id":"q-8038","question":"You're evaluating an on-device causal-inference engine that suggests ride-route adjustments using only local sensor data and privacy-preserving beacons from nearby devices. Build a 24-month CBA covering per-inference energy, on-device storage, beacon traffic, privacy/compliance costs, drift-detection cadence, and go/no-go criteria with a break-even horizon?","answer":"Per-inference energy ~0.3 mJ; model on-device memory ~6 MB; beacon traffic ~12 KB/hour; privacy budget target DP epsilon 5–10; drift checks monthly; regulatory costs minor but nonzero; expected uplift","explanation":"## Why This Is Asked\n\nThe question probes a realistic CBA for on-device causal inference with privacy-beaconing, drift maintenance, and regulatory risk, not just a revenue calc.\n\n## Key Concepts\n\n- On-device causal inference and drift detection\n- Privacy-preserving beaconing and differential privacy budgets\n- Energy, CPU, and memory budgeting\n- Compliance, data-minimization, and governance\n- Break-even analysis under uncertainty\n\n## Code Example\n\n```javascript\n// Example: simple energy budget calculator for inferences per hour\nfunction maxInferencesPerHour(batterymJ, perInferenceJ) {\n  return batterymJ / (perInferenceJ * 60);\n}\n```\n\n## Follow-up Questions\n\n- How would you adjust DP budgets under strict regulatory changes?\n- How would you validate causal impact without randomized rollout?\n- What data schema and logging would you need for post-hoc audits?","diagram":"flowchart TD\n  A[Inputs: local sensors + beacons] --> B[On-device causal model]\n  B --> C[Inference results]\n  C --> D[Beacon traffic / privacy checks]\n  D --> E[CBA update & decision]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Snowflake","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T06:59:37.328Z","createdAt":"2026-01-27T06:59:37.328Z"},{"id":"q-8122","question":"You're evaluating a new on-device, privacy-preserving meeting summarization feature for an enterprise video-call client (runs entirely on client devices to avoid sending raw audio or transcripts to servers). Build an 18-month CBA covering model size, per-minute CPU/memory/energy, storage for local caches, latency impact on live calls, telemetry, local data retention policies, regulatory/compliance costs, rollout plan, and go/no-go criteria with break-even horizon?","answer":"Propose a 25-60 MB on-device summarization model; per-minute CPU ~0.5-2% on typical mobile CPU, memory ~100-200 MB; energy ~0.3-1.5 J/min; local cache 2-3 days; latency impact under 300 ms per minute ","explanation":"## Why This Is Asked\nTests ability to design on-device ML feature with privacy constraints and economic viability, including performance, rollout, and compliance.\n\n## Key Concepts\n- On-device ML model sizing and performance trade-offs\n- Energy and latency constraints on consumer devices\n- Data retention and privacy/compliance\n- Rollout strategy and phased pilots\n- Break-even and go/no-go criteria\n\n## Code Example\n```javascript\nfunction breakEven(revenuePerUser, costPerUser) {\n  return costPerUser / revenuePerUser;\n}\n```\n\n## Follow-up Questions\n- How would you handle model drift or updates on-device without network?\n- What telemetry data would you collect to validate performance without compromising privacy?","diagram":"flowchart TD\n  A[Input: meeting audio] --> B[On-device model infer]\n  B --> C[Generate summary tokens]\n  C --> D[Store locally + expose UI]\n  D --> E[Telemetry & privacy checks]","difficulty":"intermediate","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T10:38:41.227Z","createdAt":"2026-01-27T10:38:41.227Z"},{"id":"q-8183","question":"You're evaluating a beginner feature: a privacy-preserving clipboard manager for a mobile app that stores the last 10 copied items locally and optionally syncs only hashed summaries to a backend for cross-device availability. Build a 12-month CBA: per-user storage, energy impact, delta-sync costs, privacy/compliance costs, update cadence, and go/no-go criteria?","answer":"Estimate: ~6 KB/user for 10 items; delta-sync ~1–2 KB per new item; crypto hashing adds a small energy cost; privacy: opt-in, no plaintext, retention 30–90 days; update cadence near-real-time when onl","explanation":"## Why This Is Asked\nEvaluates ability to reason about data minimization, on-device storage, energy modeling, and go/no-go criteria for a lightweight feature.\n\n## Key Concepts\n- Local storage and data minimization\n- Hash-based, privacy-preserving sync\n- Delta-sync costs and energy estimation\n- Privacy/compliance considerations and opt-in flows\n- Update cadence and break-even framing\n\n## Code Example\n```javascript\nasync function hashItem(item){\n  const enc = new TextEncoder().encode(item);\n  const h = await crypto.subtle.digest('SHA-256', enc);\n  return Array.from(new Uint8Array(h)).slice(0,8).map(b=>b.toString(16).padStart(2,'0')).join('');\n}\n```\n\n## Follow-up Questions\n- How would you empirically measure energy impact during testing?\n- How would retention policy and user opt-out affect the CBA?\n- How would you validate break-even assumptions with real user data?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T13:31:50.600Z","createdAt":"2026-01-27T13:31:50.600Z"},{"id":"q-8196","question":"You're evaluating a beginner feature: an on-device, privacy-preserving predictive keyboard assistant that learns from user typing locally and periodically uploads only anonymized usage statistics. Build a 12-month CBA: per-user storage for the model and caches, CPU/energy impact during typing bursts, delta-update bandwidth, privacy/compliance costs (consent, DPIA), update cadence, and go/no-go criteria. Include break-even horizon and sensitivity to MAU?","answer":"12-month CBA: model 2–4 MB; per-user storage ~5 MB incl. caches; CPU 1–2% during typing bursts; energy 0.3–0.6 mW per burst; deltas 5–20 KB monthly; privacy: opt-in, DPIA, 30-day retention; update cad","explanation":"## Why This Is Asked\n\nAssess the candidate's ability to scope a feasible on-device ML feature with privacy constraints and quantify its business impact.\n\n## Key Concepts\n\n- On-device ML model sizing and caching\n- Energy and CPU budgeting for typing bursts\n- Delta updates and bandwidth budgeting\n- Privacy/compliance costs (consent UX, DPIA, retention)\n- Update cadence and MAU sensitivity\n- Break-even and risk assessment\n\n## Code Example\n\n```javascript\nfunction estimatePayback(monthlySavings, upfrontCost, months) {\n  const totalSavings = monthlySavings * months;\n  return totalSavings >= upfrontCost ? Math.ceil(upfrontCost / monthlySavings) : null;\n}\n```\n\n## Follow-up Questions\n\n- How would model drift and user fairness be monitored?\n- What metrics would you track in an A/B test for this feature?\n- How would you handle opt-out and data deletion requests while preserving UX?","diagram":"flowchart TD\n  A[Define feature] --> B[Estimate costs]\n  B --> C[Estimate benefits]\n  C --> D[Compute payback]\n  D --> E[Decision: go/no-go]","difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Google","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T14:40:04.162Z","createdAt":"2026-01-27T14:40:04.163Z"},{"id":"q-8257","question":"You're evaluating a privacy-preserving cross-tenant analytics capability in a cloud data platform that lets two tenants run joined analytics on combined data without sharing raw rows. Design a 18-month CBA for this feature: data-transfer costs, cryptographic compute overhead (TEEs or MPC), differential-privacy tuning, audit/compliance, governance, and SLA enforcement; include go/no-go criteria, break-even, and failure modes (data leakage, enclave/key compromise, regulatory penalties)?","answer":"Propose a cross-tenant analytics feature that allows joined queries without exposing raw data, using TEEs or MPC for computation and differential privacy for outputs. Build an 18-month CBA covering cr","explanation":"## Why This Is Asked\n\nTests ability to design privacy-preserving cross-tenant analytics at scale, balancing cryptographic overhead, compliance, and business value.\n\n## Key Concepts\n\n- Cross-tenant analytics\n- TEEs and MPC performance considerations\n- Differential privacy budgeting\n- Auditability and governance in multi-tenant clouds\n- Cost modeling and break-even analysis\n\n## Code Example\n\n```javascript\n// Pseudo config for data flow\nconst flow = {\n  sourceA: 'tenantA.table',\n  sourceB: 'tenantB.table',\n  privacy: { type: 'TEE_MPC', dp: 'epsilon-1' },\n  outcome: 'encrypted_agg'\n}\n```\n\n## Follow-up Questions\n\n- How would you calibrate privacy budget across tenants with varying data sovereignty rules?\n- What failure modes would trigger a rollback, and how would you detect them?","diagram":null,"difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Snowflake","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T17:01:59.067Z","createdAt":"2026-01-27T17:01:59.067Z"},{"id":"q-843","question":"Given a CSV file with columns user_id, action, timestamp, write a Python function using only the standard library that returns the most recent action per user by deduplicating on user_id and keeping the latest timestamp; describe time complexity and edge cases?","answer":"Parse the CSV with the csv module, iterate rows, parse timestamp with datetime.fromisoformat, and keep a dict mapping user_id to (timestamp, action). If a row has a newer timestamp, replace the entry.","explanation":"## Why This Is Asked\nTests ability to implement a practical data-processing task: deduplicate by key using timestamps, relies only on standard library, and reveals awareness of edge cases like ties, malformed timestamps, and missing fields.\n\n## Key Concepts\n- CSV parsing with csv module\n- Deduplication by key using a dictionary\n- Timestamp parsing with datetime (ISO 8601)\n- Edge cases: missing fields, invalid timestamps, tie-breaking\n\n## Code Example\n```python\nimport csv\nfrom datetime import datetime\n\ndef latest_actions(csv_path):\n    best = {}\n    with open(csv_path, newline='') as f:\n        rdr = csv.DictReader(f)\n        for row in rdr:\n            uid = row['user_id']\n            ts = row['timestamp']\n            act = row['action']\n            try:\n                t = datetime.fromisoformat(ts)\n            except Exception:\n                continue\n            if uid not in best or t > best[uid][0]:\n                best[uid] = (t, act)\n    return [(uid, act, t.isoformat()) for uid, (t, act) in best.items()]\n```\n\n## Follow-up Questions\n- How would you adapt this for streaming data or files larger than memory?\n- If multiple actions share the same timestamp for a user, how would you break ties and what metadata would you add?","diagram":null,"difficulty":"beginner","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","PayPal","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:27:05.585Z","createdAt":"2026-01-12T13:27:05.585Z"},{"id":"q-872","question":"You are evaluating two deployment options for a new AI inference service under budget and latency constraints. Outline a practical, end-to-end cost-benefit analysis framework to decide which to deploy in production. Include: data you would collect (traffic, latency, SLA penalties, accuracy), metrics (NPV, ROI, payback), horizons, discount rate, handling uncertainty (scenarios), and a concrete calculation workflow you would run?","answer":"I would model total cost as compute, storage, and ops, plus latency penalties; benefits as incremental revenue or user value from faster or more reliable services. I would calculate NPV over 12-24 mon","explanation":"## Why This Is Asked\nTests the ability to structure a pragmatic CBA for ML deployment, balancing cost with user value, handling uncertainty, and communicating the result.\n\n## Key Concepts\n- Cost and benefits modeling\n- Time horizon and discounting\n- Scenario analysis (base/best/worst)\n- Metrics: NPV, ROI, risk-adjusted EV\n\n## Code Example\n```javascript\nfunction npv(cashFlows, rate) {\n  return cashFlows.reduce((acc, cf, i) => acc + cf / Math.pow(1 + rate, i+1), 0);\n}\n```\n\n## Follow-up Questions\n- How would you handle non-linear cost factors (e.g., burst traffic) in your model?\n- How would you present results to non-technical stakeholders?","diagram":"flowchart TD\n  DataInputs[Data Inputs] --> Option[Model Option]\n  Option --> Decision{Decision}\n  Decision --> Deploy[Deploy to Prod]\n  Decision --> Iterate[Iterate Optimization]","difficulty":"advanced","tags":["cba"],"channel":"cba","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:54:31.848Z","createdAt":"2026-01-12T13:54:31.848Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":92,"beginner":32,"intermediate":28,"advanced":32,"newThisWeek":36}}