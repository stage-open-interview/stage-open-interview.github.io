{"questions":[{"id":"q-1124","question":"On a NUMA‑aware Linux host with a fixed‑size worker pool handling high‑frequency RPCs, cross‑socket memory traffic is a bottleneck. Propose a concrete plan to minimize inter‑socket traffic: pin threads to sockets, allocate per‑socket data, and choose memory policies (numa_bind/numa_alloc_onnode). Include measurement steps and success criteria?","answer":"Pin threads to sockets, allocate per‑socket queues and data, and ensure memory allocations stay on the same node via numa_bind/numa_alloc_onnode. Favor per‑socket pools to avoid remote RAM. Validate w","explanation":"## Why This Is Asked\nTests practical skills in NUMA locality, thread affinity, and measurable performance impact.\n\n## Key Concepts\n- NUMA locality and memory policies\n- Thread and data placement\n- Per‑socket pools and queues\n- Performance measurement with perf and hardware counters\n\n## Code Example\n```javascript\n// Pin a thread to CPU set on Linux (example approach)\n#include <pthread.h>\n#include <sched.h>\n\nvoid pin_thread(int cpu) {\n  cpu_set_t cpuset; CPU_ZERO(&cpuset); CPU_SET(cpu, &cpuset);\n  pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &cpuset);\n}\n```\n\n## Follow-up Questions\n- How would you validate that per‑socket locality reduces cache misses under bursty load?\n- What trade‑offs arise if a worker must be migrated for load balancing?","diagram":null,"difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Netflix","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T23:31:03.248Z","createdAt":"2026-01-12T23:31:03.248Z"},{"id":"q-1299","question":"Scenario: two threads share a global 32-bit counter. Thread A increments it in a tight loop; Thread B logs the value once per second. Without synchronization, describe a concrete interleaving that yields a stale read or lost update, and explain the cache/coherence mechanics behind it. Then outline the minimal fix and how it ensures atomicity and visibility (e.g., atomic fetch_add or a mutex)?","answer":"Interleaving: A reads 0, B reads 0, A increments to 1 and writes 1, B logs 0. The write may not be visible immediately due to per-core caches and cache-coherence delays, so B observes 0. Fix with atom","explanation":"## Why This Is Asked\n\nConveys understanding of data races and real hardware behavior, not just theory.\n\n## Key Concepts\n\n- Data races and undefined behavior\n- Cache coherence and MESI, visibility across cores\n- Atomic operations vs locking; memory ordering guarantees\n- How preemption and context switches interact with shared data\n\n## Code Example\n\n```javascript\n// Pseudocode demonstrating the race (not executable in JS)\nlet counter = 0;\nfunction A(){ while(true){ let x = counter; counter = x + 1; } }\nfunction B(){ setInterval(()=> console.log(counter), 1000); }\n```\n\n## Follow-up Questions\n\n- How does memory_order_seq_cst affect visibility on modern CPUs?\n- How would you test this race condition in a real system?\n","diagram":null,"difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T08:45:09.473Z","createdAt":"2026-01-13T08:45:09.473Z"},{"id":"q-1354","question":"You implement a lock-free ring buffer with two atomics (head, tail) and a data array for inter-thread communication. Describe a concrete interleaving on a weak memory model (e.g., ARM64) where the consumer observes a stale value or an invalid read due to missing ordering. Propose a minimal fix using memory_order_release on the data write/tail update and memory_order_acquire on the consumer read, and sketch a safe patch?","answer":"Introduce acquire/release ordering in a ring buffer. Producer writes data[tail] then tail.store(next, memory_order_release). Consumer loads tail with memory_order_acquire and reads data[head] if head ","explanation":"## Why This Is Asked\n\nTests understanding of memory ordering, lock-free data structures, and weak memory models in OS CPUs. It probes ability to identify a real race and reason about inter-thread visibility across architectures.\n\n## Key Concepts\n\n- Lock-free synchronization\n- memory_order_release\n- memory_order_acquire\n- Data visibility before signaling availability\n\n## Code Example\n\n```cpp\n// Pseudo-C++ lock-free sketch\nstd::atomic<size_t> head{0}, tail{0};\nint data[N];\n\nvoid produce(int v){\n  auto t = tail.load(std::memory_order_relaxed);\n  data[t % N] = v;\n  tail.store((t+1) % N, std::memory_order_release);\n}\n\nint consume(){\n  auto t = tail.load(std::memory_order_acquire);\n  auto h = head.load(std::memory_order_relaxed);\n  if (h == t) return -1;\n  int v = data[h % N];\n  head.store((h+1) % N, std::memory_order_release);\n  return v;\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt this pattern for multiple producers/consumers?\n- What tests would you add to validate correctness across ARM/x86?","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hugging Face","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T13:10:06.434Z","createdAt":"2026-01-13T13:10:06.434Z"},{"id":"q-1578","question":"On a Linux server, four worker processes share a 100GB read-only memory-mapped dataset loaded from disk on demand via mmap. Describe the sequence of page fault handling, TLB behavior, and how the kernel page cache and optional swap interact with this pattern. Propose two concrete knobs to maximize throughput without starving others (e.g., MADV_WILLNEED with madvise, NUMA binding with mbind) and how you would measure success?","answer":"When a worker process accesses the memory-mapped dataset for the first time, the CPU generates a page fault. The kernel's page fault handler checks the Process Table Entry (PTE), finds the page not present, and initiates a block I/O operation to read the required page from disk into the page cache. Once the page is loaded, the kernel updates the PTE, invalidates the relevant TLB entries, and the process resumes execution. Subsequent accesses hit the page cache directly, and the TLB caches the virtual-to-physical translations for fast lookup. The kernel may evict less frequently used pages from the page cache under memory pressure, potentially writing them to swap if they were modified (though in this read-only scenario, swap usage would be minimal).\n\nTo maximize throughput without starving other processes, I recommend two concrete knobs:\n\n1. **MADV_WILLNEED with madvise**: Pre-fault frequently accessed pages into memory using `madvise(addr, length, MADV_WILLNEED)`. This hints to the kernel to load these pages proactively, reducing page fault latency during critical operations.\n\n2. **NUMA binding with mbind**: Bind the memory-mapped region to specific NUMA nodes using `mbind()` to optimize memory locality. This ensures that pages are allocated on the same NUMA node as the worker processes accessing them, reducing remote memory access latency.\n\nSuccess would be measured through metrics like reduced page fault rate (via `/proc/vmstat`), improved cache hit ratios, lower memory access latency, and sustained throughput under load.","explanation":"## Why This Is Asked\nTests understanding of OS memory management, paging, and NUMA in realistic workloads.\n\n## Key Concepts\n- Page fault handling, TLB, page cache\n- madvise hints and memory binding\n- THP trade-offs and NUMA effects\n\n## Code Example\n```c\n// C example for memory optimization\nint prefetch_dataset(void* addr, size_t len) {\n    // Pre-fault pages to reduce page fault latency\n    return madvise(addr, len, MADV_WILLNEED);\n}\n\nint bind_numa(void* addr, size_t len, int node) {\n    // Bind memory to specific NUMA node for locality\n    return mbind(addr, len, MPOL_BIND, &node, sizeof(node));\n}\n```","diagram":"flowchart TD\n  UserProcess[Worker Process] -->|mmap fault| Kernel[Page Fault Handler]\n  Kernel --> Disk[Disk I/O]\n  Disk --> Cache[Page Cache]\n  Cache --> TLB[TLB Update]\n  TLB --> UserProcess","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","DoorDash","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:56:33.159Z","createdAt":"2026-01-13T22:44:52.758Z"},{"id":"q-1733","question":"Scenario: After a fork, a child writes to a Copy-On-Write (COW) page. Describe the end-to-end kernel steps from the write fault to the point where the parent and child have separate views, including page table updates, TLB changes, and how the private copy is created and isolated from the parent's mapping?","answer":"On first write to a COW page, the kernel traps the fault, allocates a private page for the child, copies data from the shared page, updates the child's PTE to the new page with write permission, clear","explanation":"## Why This Is Asked\nTests understanding of Copy-On-Write, page tables, TLB, and the fork semantics at the OS boundary.\n\n## Key Concepts\n- Fork and Copy-On-Write\n- Page Table Entries (PTEs) and flags (present, read/write, dirty, COW)\n- TLB behavior and flushing\n- Paging and memory isolation between processes\n\n## Code Example\n```javascript\n// Demonstrative COW map usage (pseudo; actual OS calls abstracted)\nchar *p = mmap(NULL, 4096, 0, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);\n// fork and write path would trigger COW internals\n```\n\n## Follow-up Questions\n- How would behavior differ with MADV_DONTNEED or memory pressure?\n- How can you observe COW pages at runtime from user space?","diagram":"flowchart TD\n  A[Fork creates COW mappings] --> B[Child writes to page]\n  B --> C{Page fault on write}\n  C --> D[Allocate private child page]\n  C --> E[Copy data old -> new]\n  C --> F[Update child PTE to new page RW]\n  C --> G[Clear COW flag]\n  C --> H[TLB flush for child mapping]\n  D --> I[Child sees private copy]\n  I --> J[Parent mapping unchanged]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Discord","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T08:51:29.400Z","createdAt":"2026-01-14T08:51:29.400Z"},{"id":"q-1759","question":"In a multithreaded server, each worker maintains a per-thread stats counter in an array of N 64-byte structs (one per thread). A separate thread periodically sums these counters every second. Without padding, explain a concrete interleaving that leads to cache line false sharing and degraded throughput, and propose a fix (padding, alignas cache-line, or per-thread local counters plus a reduction) that preserves correctness and improves performance?","answer":"False sharing occurs when multiple threads write to nearby fields within a single cache line, causing repeated invalidations and cross-core traffic. If thread i updates its own counter in a shared lin","explanation":"## Why This Is Asked\nThe question probes practical understanding of cache coherence and false sharing in real multithreaded code.\n\n## Key Concepts\n- False sharing\n- Cache coherence protocols (MESI)\n- Data padding and alignment\n- Reduction patterns across threads\n\n## Code Example\n```c\ntypedef struct {\n  long long count;\n} PerThreadStat;\n```\n\n```c\ntypedef struct {\n  long long count;\n  char pad[64 - sizeof(long long)];\n} PerThreadStatPadded;\n```\n\n## Follow-up Questions\n- How would you measure throughput improvements after padding?\n- What changes if thread count isn’t a multiple of 64?","diagram":null,"difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:46:15.412Z","createdAt":"2026-01-14T09:46:15.412Z"},{"id":"q-2084","question":"In a kernel memory allocator with per-core freelists and a global free pool protected by a spinlock, describe a concrete interleaving that yields a use-after-free for a block still in use by a reader, and explain how either hazard pointers or epoch-based reclamation prevents it, including the required memory-order guarantees on x86-64 and how grace periods are enforced?","answer":"Thread A loads a block from its per-core freelist and begins use; Thread B, holding the pool lock, frees the same block into the global pool. A's subsequent access may hit freed memory or see partially corrupted data, creating a use-after-free vulnerability.","explanation":"## Why This Is Asked\nThis question tests safe memory reclamation in shared-data paths, especially with per-core freelists and a global pool. It probes understanding of use-after-free scenarios, memory ordering guarantees, and the trade-offs between hazard pointers and epoch-based reclamation mechanisms.\n\n## Key Concepts\n- Hazard pointers\n- Epoch-based reclamation\n- Grace period enforcement\n- Per-core freelists\n- Spinlock vs lock-free interaction\n- Cache coherence and memory barriers\n\n## Code Example\n```\nBlock *blk = percore_freelist_pop();\nuse(blk);\nlock(global_lock);\nfree_block(blk);\nunlock(global_lock);\n```","diagram":"flowchart TD\n  A[Reader holds reference to block] --> B[Block freed under pool lock]\n  B --> C[Block enters global pool]\n  C --> D[Reader may still access block]\n  D --> E[Grace period / hazard pointers delay reclaim]\n","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Netflix","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:31:06.509Z","createdAt":"2026-01-14T23:35:52.358Z"},{"id":"q-448","question":"Explain how process scheduling works in an operating system. Which scheduling algorithm would you choose for a real-time system and why?","answer":"Process scheduling manages CPU allocation among processes by determining which process gets CPU time and when. For real-time systems, I would choose **Rate Monotonic Scheduling (RMS)** because it's predictable, has bounded response times, and is optimal for fixed-priority real-time systems. RMS assigns higher priorities to tasks with shorter periods, ensuring critical tasks meet their deadlines while maintaining system stability.","explanation":"## Key Concepts\n- **Process scheduling**: Determines CPU allocation order among competing processes\n- **Scheduling algorithms**: FCFS, SJF, Priority, Round Robin, Rate Monotonic Scheduling\n- **Real-time requirements**: Deterministic timing behavior and deadline guarantees\n\n## Real-time Considerations\n- **Predictability**: Must guarantee task completion before deadlines\n- **Priority assignment**: Shorter periods receive higher priorities in RMS\n- **CPU utilization**: RMS can safely utilize up to 69% of CPU capacity\n\n## Implementation\n```c\n// RMS priority calculation\npriority = 1000 / period; // Higher values for shorter periods\n```\n\n## Trade-offs\n- RMS offers simplicity and predictability but is less flexible compared to dynamic scheduling algorithms\n- Optimal for fixed-priority systems but may require careful task design","diagram":"flowchart TD\n  A[Process Request] --> B{Scheduler}\n  B --> C[RMS Priority Check]\n  C --> D[CPU Allocation]\n  D --> E[Process Execution]\n  E --> F[Deadline Check]\n  F --> G{Met Deadline?}\n  G -->|Yes| H[Continue]\n  G -->|No| I[Priority Boost]\n  H --> B","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","OpenAI","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T08:53:46.358Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-471","question":"You're designing a GPU memory manager for CUDA applications. How would you implement a memory allocator that handles both unified memory and explicit device memory, considering fragmentation, coalescing, and the 48-bit address space limitations?","answer":"I would implement a hybrid memory allocator combining segregated free lists for different size classes with a buddy system for large allocations, utilizing virtual memory techniques to efficiently manage the 48-bit address space. The allocator would support both unified memory and explicit device memory through separate allocation pools while providing automatic coalescing and fragmentation management.","explanation":"## Memory Management Architecture\n\n### Allocation Strategies\n- **Segregated Lists**: Separate free lists optimized for small (4KB-64KB), medium (64KB-1MB), and large (>1MB) allocations to minimize search time\n- **Buddy System**: Power-of-2 allocation for large blocks enabling efficient coalescing and reducing external fragmentation\n- **Slab Allocation**: Specialized pools for frequently used small objects to minimize internal fragmentation\n\n### Address Space Management\n```c\n// 48-bit virtual address layout\ntypedef struct {\n    uint64_t prefix : 16;  // Reserved for future expansion\n    uint64_t vaddr   : 48;  // Actual virtual address\n} gpu_vaddr_t;\n```\n\n### Unified Memory Optimization\n- **Migration Policies**: Implement adaptive page migration based on access patterns and locality\n- **Prefetching**: Hardware-guided prefetching for anticipated memory accesses\n- **Page Fault Handling**: Efficient page fault resolution with minimal CPU intervention","diagram":"flowchart TD\n  A[Application Request] --> B{Memory Type}\n  B -->|Unified| C[Unified Memory Path]\n  B -->|Device| D[Explicit Device Memory]\n  C --> E[Page Fault Handler]\n  E --> F[Migration Heuristics]\n  F --> G[Allocate Virtual Pages]\n  D --> H[Size Class Selection]\n  H --> I[Segregated Free List]\n  I --> J[Buddy System Check]\n  J --> K[Physical Allocation]\n  G --> L[GPU Memory Mapping]\n  K --> L","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":["memory allocator","fragmentation","coalescing","segregated free lists","buddy system","virtual memory","cuda"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-09T09:04:49.876Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-530","question":"A process is stuck in 'D' state (uninterruptible sleep) during I/O operations. How would you debug this, what causes it, and how does it differ from 'Z' zombie state?","answer":"The 'D' state occurs during kernel-level I/O operations that cannot be interrupted, such as disk access or network operations. To identify stuck processes, use `ps aux | awk '$8 ~ /^D/ {print}'`. For debugging, `strace -p <pid>` shows the blocked system calls, while `lsof -p <pid>` reveals open file descriptors and identifies the specific I/O operation causing the block.","explanation":"## Debugging D State\n\n- Use `ps aux` to identify processes in D state\n- `strace -p <pid>` shows blocked system calls\n- `lsof -p <pid>` reveals open file descriptors\n- Check `/proc/<pid>/stack` for kernel stack trace\n\n## Common Causes\n\n- Faulty storage devices or network mounts\n- NFS/SMB server unresponsiveness\n- Hardware driver issues\n- Kernel bugs in I/O subsystem\n\n## D vs Z State\n\n- **D state**: Process alive, blocked in kernel, cannot be killed\n- **Z state**: Process terminated, resources freed, waiting for parent\n- D state consumes kernel resources, Z state only holds PID\n\n## Resolution\n\n- Address underlying hardware or network issues\n- Restart problematic services or mounts\n- Update drivers or kernel if needed\n- As last resort, reboot the system to clear D state processes","diagram":"flowchart TD\n  A[Process Running] --> B{I/O Request}\n  B -->|Kernel Space| C[D State - Uninterruptible Sleep]\n  C -->|I/O Complete| D[Return to Running]\n  C -->|Hardware/Network Issue| E[Stuck in D State]\n  F[Process Terminated] --> G[Parent Reads Exit Status]\n  G --> H[Z State - Zombie]\n  H -->|Parent Cleanup| I[PID Released]","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","LinkedIn","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":["uninterruptible sleep","strace","lsof","zombie state","kernel i/o","debugging"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:42:54.966Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-556","question":"How would you debug a process that's consuming 100% CPU but not responding to signals? What tools and steps would you use?","answer":"Start by identifying the process ID using `top` or `htop` to confirm the high CPU usage. Then attach `strace -p <PID>` to monitor system calls and determine if the process is stuck in user space or kernel mode. Check `/proc/<PID>/status` for the process state and examine `/proc/<PID>/stack` for kernel stack information. If the process remains unresponsive, use `gdb -p <PID>` to obtain stack traces and analyze the execution context.","explanation":"## Debugging Methodology\n\n1. **Process Identification**: Use `top`, `htop`, or `ps aux` to identify the PID of the CPU-intensive process\n2. **Process State Analysis**: Examine `/proc/<PID>/status` to understand current state (running, sleeping, zombie, etc.)\n3. **System Call Monitoring**: Deploy `strace -p <PID>` to trace system calls in real-time and identify blocking operations\n4. **Stack Trace Analysis**: Attach with `gdb -p <PID>` to inspect call stacks and identify infinite loops or deadlock conditions\n5. **Kernel-level Investigation**: Review `dmesg` output and `/proc/<PID>/stack` for kernel-space issues\n\n## Common Root Causes\n\n- Infinite loops in application code\n- Thread synchronization deadlocks\n- Blocking I/O operations without timeouts\n- Memory corruption leading to undefined behavior\n- Kernel bugs or driver issues\n\n## Resolution Strategies\n\n- Send `SIGKILL` as last resort if process remains unresponsive\n- Analyze application logs for error patterns preceding the issue\n- Implement proper error handling and timeout mechanisms\n- Address underlying code quality issues in the identified problem areas","diagram":"flowchart TD\n  A[High CPU Detected] --> B[top/htop - identify PID]\n  B --> C[Check /proc/<PID>/status]\n  C --> D[strace -p <PID>]\n  D --> E{System calls active?}\n  E -->|Yes| F[gdb -p <PID> - analyze stack]\n  E -->|No| G[Check for kernel deadlock]\n  F --> H[Identify root cause]\n  G --> H\n  H --> I[Apply fix/kill process]","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:56:11.138Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-585","question":"How would you implement a lock-free concurrent queue using atomic operations and memory barriers? What are the trade-offs between ABA problem solutions?","answer":"Implement using atomic head and tail pointers with compare-and-swap operations. Use hazard pointers or versioned references to solve the ABA problem. Trade-offs: hazard pointers have higher memory overhead but simpler implementation, while versioned references offer lower overhead with more complex compare-and-swap logic.","explanation":"## Lock-Free Queue Implementation\n\n- Use atomic head and tail pointers to manage queue boundaries\n- Enqueue: Atomically update tail pointer using CAS, then link new node\n- Dequeue: Atomically update head pointer using CAS, then read next node\n- Memory barriers: Use acquire semantics on loads and release semantics on stores\n\n## ABA Problem Solutions\n\n- **Hazard pointers**: Track protected nodes per thread to prevent premature reclamation\n- **Versioned references**: Combine pointer with monotonically increasing counter\n- **Epoch-based reclamation**: Batch deallocation operations by epoch boundaries\n\n## Trade-offs\n\n- Hazard pointers: Higher memory usage, simpler logic\n- Versioned references: Lower overhead, more complex CAS operations\n- Epoch-based reclamation: Better scalability, more complex implementation\n- Performance depends on contention levels and allocation patterns","diagram":"flowchart TD\n  A[Enqueue] --> B[CAS Tail Pointer]\n  B --> C[Link New Node]\n  D[Dequeue] --> E[CAS Head Pointer]\n  E --> F[Read Next Node]\n  G[ABA Problem] --> H[Hazard Pointers]\n  G --> I[Versioned References]\n  G --> J[Epoch Reclamation]","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","OpenAI","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:51:08.528Z","createdAt":"2025-12-27T01:14:13.089Z"},{"id":"q-927","question":"In a system with a fixed-size circular buffer of size N shared by a producer and a consumer thread, implement a thread-safe producer-consumer solution using semaphores and a mutex in C. Include initialization, edge cases (buffer full/empty), and show how you would test for deadlocks and correctness under concurrent producers/consumers?","answer":"Use two counting semaphores and a mutex: empty initialized to N, full to 0, and a mutex protecting the circular buffer. Producer waits on empty, locks, writes at in, in=(in+1)%N, unlocks, signals full","explanation":"## Why This Is Asked\nTests understanding of basic synchronization primitives and common OS patterns, especially producer-consumer, deadlock avoidance, and correct semaphore/mutex usage in C.\n\n## Key Concepts\n- Semaphores\n- Mutex\n- Circular buffer\n- Deadlock detection\n- Boundary conditions (full/empty)\n\n## Code Example\n```c\n#include <pthread.h>\n#include <semaphore.h>\n#include <stdio.h>\n\n#define N 10\n\nint buffer[N];\nint in = 0, out = 0;\nsem_t empty, full;\npthread_mutex_t mutex;\n\nvoid produce(int item) {\n  sem_wait(&empty);\n  pthread_mutex_lock(&mutex);\n  buffer[in] = item;\n  in = (in + 1) % N;\n  pthread_mutex_unlock(&mutex);\n  sem_post(&full);\n}\n\nint consume() {\n  int item;\n  sem_wait(&full);\n  pthread_mutex_lock(&mutex);\n  item = buffer[out];\n  out = (out + 1) % N;\n  pthread_mutex_unlock(&mutex);\n  sem_post(&empty);\n  return item;\n}\n```\n\n## Follow-up Questions\n- How would you extend to multiple producers/consumers and ensure fairness?\n- What are trade-offs vs. using condition variables instead of semaphores?","diagram":null,"difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","OpenAI","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T15:37:15.531Z","createdAt":"2026-01-12T15:37:15.531Z"},{"id":"q-995","question":"In a system using paging with a TLB, describe the end-to-end sequence when a 4 KB page accessed by a process is not mapped in RAM, from fault to resume, including the fault handler, page-table walk, TLB update, disk I/O to swap, and how the eviction policy decides which page to replace?","answer":"On a fault: hardware traps to the kernel; verify access rights; pick a free physical frame or evict a victim; perform a page-table walk to map VPN to the chosen PPN and set present/dirty bits; flush o","explanation":"## Why This Is Asked\n\nTests understanding of demand paging, TLB coherency, and eviction trade-offs in real OS.\n\n## Key Concepts\n\n- Paging, page tables, TLB, and ASIDs\n- Page fault handling, disk I/O, swap vs page file\n- Eviction policies (LRU, Clock) and dirty page handling\n\n## Code Example\n\n```c\n// Pseudo: handle_page_fault()\n// 1) check rights, 2) allocate frame, 3) page_table[VPN] = PPN|present, 4) TLB.invalidate(VPN); TLB.update(VPN, PPN);\n```\n\n## Follow-up Questions\n\n- How does asynchronous I/O affect fault handling?\n- What mitigations reduce thrashing in a busy workload?\n","diagram":"flowchart TD\n  A[Page fault] --> B[Trap to kernel]\n  B --> C{Free frame?}\n  C -- yes --> D[Map VPN->PPN; set present]\n  C -- no --> E[Evict victim]\n  E --> F[If dirty: write back]\n  F --> D\n  D --> G[Update TLB]\n  G --> H[Resume]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T18:39:46.467Z","createdAt":"2026-01-12T18:39:46.467Z"},{"id":"q-263","question":"How does demand paging optimize memory utilization in virtual memory systems, what triggers page faults, and which algorithms handle page replacement when physical memory is full?","answer":"Demand paging loads pages only on first access, triggering page faults handled by the OS. Page replacement algorithms like LRU, FIFO, or Clock evict pages when memory is full. The TLB caches recent translations to reduce overhead, while working set models prevent thrashing by tracking active pages. Performance depends on locality, page size, and replacement efficiency.","explanation":"## Memory Efficiency\nDemand paging eliminates loading unused pages, reducing initial memory overhead and I/O. Pages load lazily when first accessed via page faults.\n\n## Page Fault Handling\n1. CPU traps to OS\n2. OS validates the access\n3. Locates page on disk\n4. Selects victim page if needed\n5. Loads page into free frame\n6. Updates page tables\n7. Restarts instruction\n\n## Page Replacement Algorithms\n**LRU**: Evicts least recently used page - optimal locality but expensive tracking. **FIFO**: Simple but can suffer from Belady's anomaly. **Clock**: Approximates LRU with reference bits, balancing performance and overhead.\n\n## TLB Role\nThe Translation Lookaside Buffer caches recent virtual-to-physical mappings, reducing page table walks from memory access to single cycle lookup.\n\n## Thrashing Prevention\nWorking set models track each process's active pages. Local replacement prevents one process from evicting another's pages. Page fault frequency monitoring detects thrashing and may suspend processes.\n\n## Performance Trade-offs\nPage size affects internal fragmentation (larger pages) vs. TLB miss rate (smaller pages). Prepaging can reduce page faults but wastes memory. Copy-on-write optimizes fork() operations by sharing pages until modification.","diagram":"flowchart TD\n    A[Process accesses memory] --> B{Page in RAM?}\n    B -->|Yes| C[Access data directly]\n    B -->|No| D[Page fault triggered]\n    D --> E[OS handles fault]\n    E --> F[Load page from disk]\n    F --> G[Update page table]\n    G --> H[Restart instruction]\n    H --> C","difficulty":"intermediate","tags":["virtual-memory","paging","segmentation","cache"],"channel":"operating-systems","subChannel":"memory","sourceUrl":null,"videos":null,"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-26T16:40:56.531Z","createdAt":"2025-12-26 12:51:07"}],"subChannels":["general","memory"],"companies":["Adobe","Amazon","Apple","Citadel","Coinbase","Databricks","Discord","DoorDash","Google","Hugging Face","Instacart","LinkedIn","Lyft","Meta","Microsoft","NVIDIA","Netflix","OpenAI","Oracle","Salesforce","Scale Ai","Snap","Snowflake","Square","Stripe","Two Sigma"],"stats":{"total":15,"beginner":6,"intermediate":5,"advanced":4,"newThisWeek":9}}