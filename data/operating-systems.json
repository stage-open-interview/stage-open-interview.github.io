{"questions":[{"id":"q-1124","question":"On a NUMA‑aware Linux host with a fixed‑size worker pool handling high‑frequency RPCs, cross‑socket memory traffic is a bottleneck. Propose a concrete plan to minimize inter‑socket traffic: pin threads to sockets, allocate per‑socket data, and choose memory policies (numa_bind/numa_alloc_onnode). Include measurement steps and success criteria?","answer":"Pin threads to sockets, allocate per‑socket queues and data, and ensure memory allocations stay on the same node via numa_bind/numa_alloc_onnode. Favor per‑socket pools to avoid remote RAM. Validate w","explanation":"## Why This Is Asked\nTests practical skills in NUMA locality, thread affinity, and measurable performance impact.\n\n## Key Concepts\n- NUMA locality and memory policies\n- Thread and data placement\n- Per‑socket pools and queues\n- Performance measurement with perf and hardware counters\n\n## Code Example\n```javascript\n// Pin a thread to CPU set on Linux (example approach)\n#include <pthread.h>\n#include <sched.h>\n\nvoid pin_thread(int cpu) {\n  cpu_set_t cpuset; CPU_ZERO(&cpuset); CPU_SET(cpu, &cpuset);\n  pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &cpuset);\n}\n```\n\n## Follow-up Questions\n- How would you validate that per‑socket locality reduces cache misses under bursty load?\n- What trade‑offs arise if a worker must be migrated for load balancing?","diagram":null,"difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Netflix","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:31:03.248Z","createdAt":"2026-01-12T23:31:03.248Z"},{"id":"q-1299","question":"Scenario: two threads share a global 32-bit counter. Thread A increments it in a tight loop; Thread B logs the value once per second. Without synchronization, describe a concrete interleaving that yields a stale read or lost update, and explain the cache/coherence mechanics behind it. Then outline the minimal fix and how it ensures atomicity and visibility (e.g., atomic fetch_add or a mutex)?","answer":"Interleaving: A reads 0, B reads 0, A increments to 1 and writes 1, B logs 0. The write may not be visible immediately due to per-core caches and cache-coherence delays, so B observes 0. Fix with atom","explanation":"## Why This Is Asked\n\nConveys understanding of data races and real hardware behavior, not just theory.\n\n## Key Concepts\n\n- Data races and undefined behavior\n- Cache coherence and MESI, visibility across cores\n- Atomic operations vs locking; memory ordering guarantees\n- How preemption and context switches interact with shared data\n\n## Code Example\n\n```javascript\n// Pseudocode demonstrating the race (not executable in JS)\nlet counter = 0;\nfunction A(){ while(true){ let x = counter; counter = x + 1; } }\nfunction B(){ setInterval(()=> console.log(counter), 1000); }\n```\n\n## Follow-up Questions\n\n- How does memory_order_seq_cst affect visibility on modern CPUs?\n- How would you test this race condition in a real system?\n","diagram":null,"difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:45:09.473Z","createdAt":"2026-01-13T08:45:09.473Z"},{"id":"q-1354","question":"You implement a lock-free ring buffer with two atomics (head, tail) and a data array for inter-thread communication. Describe a concrete interleaving on a weak memory model (e.g., ARM64) where the consumer observes a stale value or an invalid read due to missing ordering. Propose a minimal fix using memory_order_release on the data write/tail update and memory_order_acquire on the consumer read, and sketch a safe patch?","answer":"Introduce acquire/release ordering in a ring buffer. Producer writes data[tail] then tail.store(next, memory_order_release). Consumer loads tail with memory_order_acquire and reads data[head] if head ","explanation":"## Why This Is Asked\n\nTests understanding of memory ordering, lock-free data structures, and weak memory models in OS CPUs. It probes ability to identify a real race and reason about inter-thread visibility across architectures.\n\n## Key Concepts\n\n- Lock-free synchronization\n- memory_order_release\n- memory_order_acquire\n- Data visibility before signaling availability\n\n## Code Example\n\n```cpp\n// Pseudo-C++ lock-free sketch\nstd::atomic<size_t> head{0}, tail{0};\nint data[N];\n\nvoid produce(int v){\n  auto t = tail.load(std::memory_order_relaxed);\n  data[t % N] = v;\n  tail.store((t+1) % N, std::memory_order_release);\n}\n\nint consume(){\n  auto t = tail.load(std::memory_order_acquire);\n  auto h = head.load(std::memory_order_relaxed);\n  if (h == t) return -1;\n  int v = data[h % N];\n  head.store((h+1) % N, std::memory_order_release);\n  return v;\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt this pattern for multiple producers/consumers?\n- What tests would you add to validate correctness across ARM/x86?","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hugging Face","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T13:10:06.434Z","createdAt":"2026-01-13T13:10:06.434Z"},{"id":"q-1578","question":"On a Linux server, four worker processes share a 100GB read-only memory-mapped dataset loaded from disk on demand via mmap. Describe the sequence of page fault handling, TLB behavior, and how the kernel page cache and optional swap interact with this pattern. Propose two concrete knobs to maximize throughput without starving others (e.g., MADV_WILLNEED with madvise, NUMA binding with mbind) and how you would measure success?","answer":"When a worker process accesses the memory-mapped dataset for the first time, the CPU generates a page fault. The kernel's page fault handler checks the Process Table Entry (PTE), finds the page not present, and initiates a block I/O operation to read the required page from disk into the page cache. Once the page is loaded, the kernel updates the PTE, invalidates the relevant TLB entries, and the process resumes execution. Subsequent accesses hit the page cache directly, and the TLB caches the virtual-to-physical translations for fast lookup. The kernel may evict less frequently used pages from the page cache under memory pressure, potentially writing them to swap if they were modified (though in this read-only scenario, swap usage would be minimal).\n\nTo maximize throughput without starving other processes, I recommend two concrete knobs:\n\n1. **MADV_WILLNEED with madvise**: Pre-fault frequently accessed pages into memory using `madvise(addr, length, MADV_WILLNEED)`. This hints to the kernel to load these pages proactively, reducing page fault latency during critical operations.\n\n2. **NUMA binding with mbind**: Bind the memory-mapped region to specific NUMA nodes using `mbind()` to optimize memory locality. This ensures that pages are allocated on the same NUMA node as the worker processes accessing them, reducing remote memory access latency.\n\nSuccess would be measured through metrics like reduced page fault rate (via `/proc/vmstat`), improved cache hit ratios, lower memory access latency, and sustained throughput under load.","explanation":"## Why This Is Asked\nTests understanding of OS memory management, paging, and NUMA in realistic workloads.\n\n## Key Concepts\n- Page fault handling, TLB, page cache\n- madvise hints and memory binding\n- THP trade-offs and NUMA effects\n\n## Code Example\n```c\n// C example for memory optimization\nint prefetch_dataset(void* addr, size_t len) {\n    // Pre-fault pages to reduce page fault latency\n    return madvise(addr, len, MADV_WILLNEED);\n}\n\nint bind_numa(void* addr, size_t len, int node) {\n    // Bind memory to specific NUMA node for locality\n    return mbind(addr, len, MPOL_BIND, &node, sizeof(node));\n}\n```","diagram":"flowchart TD\n  UserProcess[Worker Process] -->|mmap fault| Kernel[Page Fault Handler]\n  Kernel --> Disk[Disk I/O]\n  Disk --> Cache[Page Cache]\n  Cache --> TLB[TLB Update]\n  TLB --> UserProcess","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","DoorDash","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T05:56:33.159Z","createdAt":"2026-01-13T22:44:52.758Z"},{"id":"q-1733","question":"Scenario: After a fork, a child writes to a Copy-On-Write (COW) page. Describe the end-to-end kernel steps from the write fault to the point where the parent and child have separate views, including page table updates, TLB changes, and how the private copy is created and isolated from the parent's mapping?","answer":"On first write to a COW page, the kernel traps the fault, allocates a private page for the child, copies data from the shared page, updates the child's PTE to the new page with write permission, clear","explanation":"## Why This Is Asked\nTests understanding of Copy-On-Write, page tables, TLB, and the fork semantics at the OS boundary.\n\n## Key Concepts\n- Fork and Copy-On-Write\n- Page Table Entries (PTEs) and flags (present, read/write, dirty, COW)\n- TLB behavior and flushing\n- Paging and memory isolation between processes\n\n## Code Example\n```javascript\n// Demonstrative COW map usage (pseudo; actual OS calls abstracted)\nchar *p = mmap(NULL, 4096, 0, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);\n// fork and write path would trigger COW internals\n```\n\n## Follow-up Questions\n- How would behavior differ with MADV_DONTNEED or memory pressure?\n- How can you observe COW pages at runtime from user space?","diagram":"flowchart TD\n  A[Fork creates COW mappings] --> B[Child writes to page]\n  B --> C{Page fault on write}\n  C --> D[Allocate private child page]\n  C --> E[Copy data old -> new]\n  C --> F[Update child PTE to new page RW]\n  C --> G[Clear COW flag]\n  C --> H[TLB flush for child mapping]\n  D --> I[Child sees private copy]\n  I --> J[Parent mapping unchanged]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Discord","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T08:51:29.400Z","createdAt":"2026-01-14T08:51:29.400Z"},{"id":"q-1759","question":"In a multithreaded server, each worker maintains a per-thread stats counter in an array of N 64-byte structs (one per thread). A separate thread periodically sums these counters every second. Without padding, explain a concrete interleaving that leads to cache line false sharing and degraded throughput, and propose a fix (padding, alignas cache-line, or per-thread local counters plus a reduction) that preserves correctness and improves performance?","answer":"False sharing occurs when multiple threads write to nearby fields within a single cache line, causing repeated invalidations and cross-core traffic. If thread i updates its own counter in a shared lin","explanation":"## Why This Is Asked\nThe question probes practical understanding of cache coherence and false sharing in real multithreaded code.\n\n## Key Concepts\n- False sharing\n- Cache coherence protocols (MESI)\n- Data padding and alignment\n- Reduction patterns across threads\n\n## Code Example\n```c\ntypedef struct {\n  long long count;\n} PerThreadStat;\n```\n\n```c\ntypedef struct {\n  long long count;\n  char pad[64 - sizeof(long long)];\n} PerThreadStatPadded;\n```\n\n## Follow-up Questions\n- How would you measure throughput improvements after padding?\n- What changes if thread count isn’t a multiple of 64?","diagram":null,"difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T09:46:15.412Z","createdAt":"2026-01-14T09:46:15.412Z"},{"id":"q-2084","question":"In a kernel memory allocator with per-core freelists and a global free pool protected by a spinlock, describe a concrete interleaving that yields a use-after-free for a block still in use by a reader, and explain how either hazard pointers or epoch-based reclamation prevents it, including the required memory-order guarantees on x86-64 and how grace periods are enforced?","answer":"Thread A loads a block from its per-core freelist and begins use; Thread B, holding the pool lock, frees the same block into the global pool. A's subsequent access may hit freed memory or see partially corrupted data, creating a use-after-free vulnerability.","explanation":"## Why This Is Asked\nThis question tests safe memory reclamation in shared-data paths, especially with per-core freelists and a global pool. It probes understanding of use-after-free scenarios, memory ordering guarantees, and the trade-offs between hazard pointers and epoch-based reclamation mechanisms.\n\n## Key Concepts\n- Hazard pointers\n- Epoch-based reclamation\n- Grace period enforcement\n- Per-core freelists\n- Spinlock vs lock-free interaction\n- Cache coherence and memory barriers\n\n## Code Example\n```\nBlock *blk = percore_freelist_pop();\nuse(blk);\nlock(global_lock);\nfree_block(blk);\nunlock(global_lock);\n```","diagram":"flowchart TD\n  A[Reader holds reference to block] --> B[Block freed under pool lock]\n  B --> C[Block enters global pool]\n  C --> D[Reader may still access block]\n  D --> E[Grace period / hazard pointers delay reclaim]\n","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Netflix","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:31:06.509Z","createdAt":"2026-01-14T23:35:52.358Z"},{"id":"q-2220","question":"Scenario: A process has two threads: T1 holds a mutex to update a shared log buffer; T2 is blocked trying to acquire the same mutex. A SIGINT is delivered to the process while T1 is in the middle of updating. Explain the sequence of events from signal delivery to termination, including the mutex state, potential race conditions, and safe patterns for signal handling in multi-threaded programs (like async-signal-safe handlers and deferred cleanup via a dedicated signal-handling thread or self-pipe)?","answer":"Signals target a single thread; if that thread holds a mutex, the handler runs with the mutex still held, risking deadlock or data corruption unless the handler uses only async-signal-safe calls. Prac","explanation":"## Why This Is Asked\nAssess understanding of signal semantics in multi-threaded processes and safe cleanup patterns.\n\n## Key Concepts\n- Signals and thread-level delivery\n- Async-signal-safety and mutex interactions\n- Self-pipe trick and deferred cleanup\n\n## Code Example\n```javascript\n#include <signal.h>\n#include <unistd.h>\n#include <stdlib.h>\n#include <stdio.h>\n\nint pipefd[2];\n\nvoid sigint(int s){\n  char c = 1;\n  write(pipefd[1], &c, 1);\n}\n\nint main(){\n  pipe(pipefd);\n  struct sigaction sa;\n  sa.sa_handler = sigint;\n  sigemptyset(&sa.sa_mask);\n  sa.sa_flags = 0;\n  sigaction(SIGINT, &sa, NULL);\n\n  char buf;\n  while (read(pipefd[0], &buf, 1) > 0) {\n    // termination requested\n  }\n  return 0;\n}\n```\n\n## Follow-up Questions\n- What happens if multiple signals arrive rapidly?\n- How would you implement a robust termination sequence in a real program?","diagram":"flowchart TD\n  Signal Arrival --> TargetThread\n  TargetThread --> AsyncSafe{In async-signal-safe?}\n  AsyncSafe -->|Yes| QuickCleanup\n  AsyncSafe -->|No| DeferCleanup","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T07:47:33.249Z","createdAt":"2026-01-15T07:47:33.250Z"},{"id":"q-2280","question":"In a simplified OS with a file-backed page cache, a process repeatedly calls read() to fetch 4 KiB blocks from a file. If a 4 KiB block is not in the file system page cache, describe step-by-step what the kernel does from read() entry to user-space return, including page-cache lookup, block-device reads, potential read-ahead, and how the data ends up in the user buffer. Compare a cache miss vs a cache hit path and timing?","answer":"On a miss, read() triggers a page-cache lookup miss, the kernel issues a disk I/O for the file block, loads the page into the page cache, updates inode/page-table metadata, wakes the waiting process, ","explanation":"## Why This Is Asked\nTests understanding of the buffered I/O path and page cache interaction in a simple OS.\n\n## Key Concepts\n- File-backed page cache\n- Read path for misses vs hits\n- Read-ahead optimization and trade-offs\n- Data copy to user buffer and I/O completion\n\n## Code Example\n```javascript\n// Pseudo path (illustrative, not real code)\nfunction read(fd, buf, count) {\n  if (cacheHasBlock(fd)) {\n    copyFromCache(fd, buf, count)\n  } else {\n    submitDiskRead(fd)\n    waitForIO()\n    copyFromCache(fd, buf, count)\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test the timing difference between hit and miss paths?\n- How does read-ahead decide which blocks to fetch and when to disable it?","diagram":"flowchart TD\n  A[read() entry] --> B[lookup page cache]\n  B --> C{hit?}\n  C -- Yes --> D[copy to user buffer]\n  C -- No --> E[submit disk read]\n  E --> F[load into page cache]\n  F --> D","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T10:40:24.723Z","createdAt":"2026-01-15T10:40:24.723Z"},{"id":"q-2404","question":"A multi-threaded app writes to a single file via buffered I/O on a journaling filesystem. Thread A appends 64KiB blocks; Thread B occasionally calls fsync(). Describe the path from write through the page cache to disk, including journal commits and barriers, and give a concrete interleaving where a crash can lose data or leave metadata only. Propose a minimal patch to guarantee durability (e.g., fdatasync after writes)?","answer":"Path: write sits in page cache; fsync flushes data blocks + metadata to disk with barrier ordering. Race: writer appends 64KiB, then crash before journal commit completes; data may be lost or metadata","explanation":"## Why This Is Asked\n\nThis probes understanding of durability semantics in buffered I/O with journaling filesystems, including the roles of the page cache, journal commits, and barriers. It also checks practical mitigation strategies.\n\n## Key Concepts\n\n- Data vs metadata flush semantics and barriers\n- Journal commit ordering and writeback paths\n- fdatasync, fsync, and O_SYNC tradeoffs in throughput vs durability\n\n## Code Example\n\n```javascript\n// Pseudocode: durability after write\nconst n = write(fd, buf, len);\nif (n === len) {\n  fdatasync(fd);\n}\n```\n\n## Follow-up Questions\n\n- How would you test durability guarantees under simulated crashes?\n- Compare fdatasync vs O_SYNC in a high-throughput server and discuss performance implications.","diagram":"flowchart TD\n  A[64KiB write to page cache] --> B[page cache flush]\n  B --> C{fsync called?}\n  C -->|yes| D[journal + data flushed to disk with barriers]\n  C -->|no| E[crash may lose data or leave metadata only]","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Databricks","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T16:54:08.605Z","createdAt":"2026-01-15T16:54:08.605Z"},{"id":"q-2477","question":"Explain how Linux handles a write to a 2MB Transparent Huge Page (THP) that only partially overlaps with a 4KB region. Describe the fault path, the split_huge_page path (and khugepaged if applicable), PTE updates, TLB shootdowns, and the performance implications vs pre-splitting THPs?","answer":"On a write, a page fault splits the 2MB THP into 512 4KB pages via split_huge_page, coordinated by khugepaged or on-demand. The kernel copies the write into the new 4KB page, updates the PTEs to point","explanation":"## Why This Is Asked\nTests understanding of THP in Linux, on-demand page splitting, and the impact on performance when only a small region is written.\n\n## Key Concepts\n- Transparent Huge Pages (THP) and 4KB subpage mapping\n- split_huge_page and khugepaged coordination\n- Page Table Entries (PTEs) updates and COW semantics\n- TLB shootdowns across CPUs and performance impact\n\n## Code Example\n```c\n// high-level sketch of handling a THP write fault\nint handle_thp_write(struct vm_area_struct *vma, unsigned long addr) {\n  if (is_hugepage(addr)) {\n    split_huge_page(vma, addr);\n    // fault restarts, now maps 4KB pages\n  }\n  // mark dirty, proceed with normal write\n  return 0;\n}\n```\n\n## Follow-up Questions\n- How does THP splitting interact with NUMA memory policies?\n- What are the trade-offs of aggressive THP splitting vs keeping large pages for workload with mixed write sizes?","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T19:39:45.795Z","createdAt":"2026-01-15T19:39:45.795Z"},{"id":"q-2544","question":"Scenario: A process memory-maps a 4 KiB device block via mmap with MAP_SHARED and then writes through a user pointer that maps to that page. Explain end-to-end how the kernel handles the write, including the MMU page fault, page cache interaction, dirty bit propagation, writeback, and how the data reaches the physical device, highlighting synchronization with other mappings and potential coherence issues?","answer":"On a MAP_SHARED mmap write, the kernel first handles a page fault if the page isn't present, loading it from the backing file or device. The kernel then maps the page into the process's address space, updates the page tables, and refreshes the TLB. When the user writes through the pointer, the data is copied directly into the page cache entry. The kernel marks the page as dirty in its page cache metadata and sets the dirty bit in the page table entry. The same page cache entry serves all mappings of this file region, ensuring coherence between different processes. The kernel later schedules writeback, where the dirty page is written to the physical block device through the block I/O layer, potentially involving the device's own write cache.","explanation":"## Why This Is Asked\n\nTests understanding of mmap, page cache, and writeback in a realistic I/O path that combines memory management and filesystem semantics.\n\n## Key Concepts\n\n- MAP_SHARED semantics and page cache sharing\n- Page fault handling and page table updates\n- Page cache interaction and dirty bit propagation\n- Writeback path and block device I/O\n- TLB and PTE coherence across mappings\n\n## Code Example\n\n```c\n// user-space example (high-level)\nchar *p = mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0);\nstrncpy(p, data, len);  // triggers the full kernel path described above\n```","diagram":"flowchart TD\n  A[User write via mmap] --> B[Page fault or hit in page cache]\n  B --> C[Copy to page and mark dirty]\n  C --> D[Schedule writeback to block device]\n  D --> E[Device updated; other mappings see data]\n  E --> F[TLB/PTE updated to reflect dirty page]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","OpenAI","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:33:07.635Z","createdAt":"2026-01-15T22:31:53.942Z"},{"id":"q-2714","question":"In a high-throughput key-value service on Linux/x86_64, two threads share a 128-byte struct: a 64-bit value and a 64-bit sequence counter used for a sequence lock (seqlock). Describe how to implement a lock-free reader with a writer that updates the value safely, specifying the exact operation order, required memory barriers, and how you verify correctness under contention. Compare against a mutex approach and practical tradeoffs?","answer":"Use a seqlock: writer increments seq to odd, writes value, uses a memory barrier, then increments seq to even. Reader samples seq, reads value, re-samples seq, and repeats if seq changed or is odd. Us","explanation":"## Why This Is Asked\nTests lock-free synchronization understanding, memory ordering, and practical tradeoffs for low-latency services.\n\n## Key Concepts\n- Sequence locks\n- Memory ordering (release/acquire, barriers)\n- Contention and livelock vs mutex\n\n## Code Example\n```c\n// sketch: seqlock usage\n```\n\n## Follow-up Questions\n- How would you extend to 128-bit values and non-scalar data?\n- How to measure correctness and contention in a microbenchmark?","diagram":"flowchart TD\n  W[Writer: update value] --> S{seq++ to odd}\n  S --> D[Write value]\n  D --> E{seq++ to even}\n  R[Reader: sample seq] --> V[Read value]\n  V --> R2[Resample seq]\n  R2 -->|unchanged and even| OK\n  R2 -->|changed or odd| RETRY","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Instacart","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T07:47:24.528Z","createdAt":"2026-01-16T07:47:24.528Z"},{"id":"q-2734","question":"In a Linux‑like system, a user‑space producer and a kernel driver share a lock‑free 256‑entry ring buffer using per‑entry sequence numbers. Describe a safe publish/claim/consume protocol that guarantees no lost entries and a consistent view under concurrent updates, specifying the exact write order and memory barriers on x86‑64. Include a concrete test harness to validate under contention?","answer":"Use per-slot sequence numbers and release/acquire semantics. Producer writes data, then performs a release write of slot.seq to signal readiness; consumer reads slot.seq with acquire, validates, then ","explanation":"## Why This Is Asked\nProbes practical lock-free IPC correctness between user space and kernel with memory ordering.\n\n## Key Concepts\n- Lock-free ring buffers with per-slot sequence numbers\n- Release/acquire semantics on x86-64\n- Visibility guarantees under contention\n- Test harness design for correctness\n\n## Code Example\n```c\n// sketch of publish/consume\n```\n\n## Follow-up Questions\n- How to extend to multiple kernel consumers?\n- How would you detect/repair corrupted seq invariants?\n","diagram":"flowchart TD\n  A[User-Space Producer] --> B[Kernel Driver]\n  B --> C[Lock-Free Ring Buffer]\n  C --> D[Consumer Reads Entry]\n  D --> A","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T09:45:31.982Z","createdAt":"2026-01-16T09:45:31.982Z"},{"id":"q-2943","question":"Describe a robust strategy for a parent process to collect exit statuses of multiple forked children in Linux. Include waitpid usage in a loop, handling of SIGCHLD, and zombie prevention. Also cover edge cases like stopped/continued children and race-free accounting?","answer":"Use a parent loop to reap children without races. Method A: in main thread, repeatedly call waitpid(-1, &status, 0) and process each pid; break when returns -1 with errno==ECHILD. Method B: install a ","explanation":"## Why This Is Asked\nTests understanding of process lifecycle, waitpid usage, and zombie avoidance in a practical multi-child scenario.\n\n## Key Concepts\n- waitpid semantics and looping\n- SIGCHLD handling and reaping\n- WNOHANG and non-blocking variants\n- Zombie processes and edge cases (stopped/continued)\n\n## Code Example\n```javascript\n#include <sys/types.h>\n#include <sys/wait.h>\n#include <unistd.h>\n#include <errno.h>\n\nint main() {\n  // spawn N children ...\n  int finished = 0; int status;\n  while (finished < N) {\n    pid_t pid = waitpid(-1, &status, 0);\n    if (pid > 0) { finished++; /* process status */ }\n    else if (pid == -1 && errno == ECHILD) break;\n  }\n  return 0;\n}\n```\n\n## Follow-up Questions\n- How would you adapt this for SIGCHLD storms or large N?\n- What are the risks of ignoring zombie cleanup in long-running daemons?","diagram":"flowchart TD\n  Start([Start]) --> Fork[N children]\n  Fork --> Wait[Wait for children]\n  Wait --> Done{All finished?}\n  Done -- Yes --> End([End])\n  Done -- No --> Reap[Reap with waitpid or handler]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T18:48:16.280Z","createdAt":"2026-01-16T18:48:16.280Z"},{"id":"q-2966","question":"Scenario: A latency‑sensitive service pins 32 worker threads to NUMA node 0 on a dual‑node server. A background writer allocates a large shared ring buffer used by all workers, but allocator fragmentation causes many pages to reside on Node 1. Explain how page allocation, NUMA policies, and TLB coherence interact to produce remote misses and increased cross‑node traffic. Propose a minimal policy (e.g., mbind/sysfs knobs or numactl) to keep hot data local and describe validation via microbenchmarks?","answer":"Remote pages cause extra page walks, higher TLB misses, and cross-node memory bandwidth when data sits on Node 1. Bind the ring buffer to Node 0 (mbind, numactl) and allocate producer/consumer data to","explanation":"## Why This Is Asked\n\nTests understanding of NUMA locality, page allocator behavior under fragmentation, and how TLB coherence affects latency in multi‑node setups; requires concrete mitigation strategies rather than abstract theory.\n\n## Key Concepts\n\n- NUMA memory locality and policy\n- Page allocation and migration\n- TLB coherence and remote memory traffic\n- mbind, numactl, and policy choices\n- Microbenchmark validation of locality\n\n## Code Example\n\n```bash\nnumactl --cpunodebind=0 --membind=0 ./latency_test\n```\n```\n\n## Follow-up Questions\n\n- What if background writer uses large pages to reduce metadata overhead?\n- How would you monitor remote memory traffic with perf or pidstat, and validate improvements across workloads?","diagram":"flowchart TD\n  A[Ring buffer allocation] --> B{Locality?}\n  B -- Local (Node 0) --> C[Fast local accesses]\n  B -- Remote (Node 1) --> D[Remote accesses, TLB misses]\n  D --> E[Increased cross-node bandwidth, latency]\n  E --> F[Validate with microbenchmarks]","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T19:28:36.151Z","createdAt":"2026-01-16T19:28:36.151Z"},{"id":"q-3028","question":"In Linux, two processes map the same 4 KiB file region with MAP_SHARED. One writer updates eight bytes; a reader loops to read the same region concurrently. Describe end-to-end what happens from the write through the page cache to the reader, including dirty/writeback, cache coherence, and TLB; then propose a minimal synchronization mechanism (e.g., a 4-byte lock or seqlock in the shared mapping) to guarantee a predictable visibility order and discuss tradeoffs?","answer":"End-to-end: Process A writes to a MAP_SHARED mmap page; the update propagates to the page cache and marks the page dirty, then the kernel may write it back during writeback or page eviction. CPU cache coherence ensures Process B sees changes when they reach its cache, while TLB entries remain valid since the mapping doesn't change. The writer's modification becomes visible to the reader through hardware cache coherence, though timing varies based on writeback policies and cache coherence latency.\n\nFor minimal synchronization, implement a 4-byte lock in the shared mapping: Process A atomically acquires the lock using compare-and-swap, performs the 8-byte write, then releases the lock with a store. Process B checks the lock state before reading, ensuring it observes updates in the correct order. Alternatively, use a seqlock where readers check sequence numbers before and after reading to detect concurrent updates.","explanation":"## Why This Is Asked\nThis tests understanding of inter-process sharing via mmap, page cache semantics, and memory visibility without user-space locking.\n\n## Key Concepts\n- MAP_SHARED page mappings\n- Page cache dirty bits and writeback\n- Cache coherence and TLB effects\n- Lightweight synchronization in shared memory (CAS, acquire/release)\n\n## Code Example\n```c\n// Pseudo: writer uses CAS to acquire lock, writes, then releases\nuint32_t *lock = (uint32_t*)addr; // shared region\nwhile (__sync_lock_test_and_set(lock, 1)) {}\n// write 8 bytes to region after lock\nmemcpy((void*)(addr+8), data, 8);\n__sync_lock_release(lock);\n\n// Reader checks lock before reading\nwhile (*lock) {} // wait for writer\nmemcpy(data, (void*)(addr+8), 8);\n```\n\n## Tradeoffs\nLock: Simple, ensures exclusive access but blocks readers during writes.\nSeqlock: Readers can proceed concurrently but must retry if write occurs mid-read; better for read-heavy workloads.","diagram":null,"difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Bloomberg","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T05:43:40.674Z","createdAt":"2026-01-16T21:44:28.984Z"},{"id":"q-3184","question":"Scenario: A two-socket NUMA Linux server uses a shared 8 GiB mmap region for request data. Threads on both sockets touch pages frequently; explain how NUMA balancing, page migration, TLB shootdowns, and cache coherence interact to affect latency. Propose a concrete microbenchmark to measure cross-node memory access and a pragmatic fix (pin memory to local nodes or adjust policies) with expected trade-offs?","answer":"On a two-socket NUMA Linux server, cross-node touches trigger page migration, TLB shootdowns, and cache coherence traffic, causing latency spikes and lower throughput under bursts. A microbenchmark sh","explanation":"## Why This Is Asked\n\nThe interviewer wants to see understanding of NUMA locality, kernel memory management, and real-world performance tuning under contention.\n\n## Key Concepts\n\n- NUMA locality and page migration\n- TLB shootdowns and cache coherence traffic\n- Memory policy and tooling (numactl, libnuma)\n- Trade-offs of pinning vs balancing\n\n## Code Example\n\n```javascript\n// Conceptual: pin allocation to node 0 (illustrative)\nint policy = 0; // placeholder\n// mbind/numa APIs would be used in C to pin memory to a node\n```\n\n## Follow-up Questions\n\n- How would you design a microbenchmark to separate CPU-bound vs memory-bound effects?\n- What happens if you disable NUMA balancing during peak load?","diagram":"flowchart TD\n  A[Worker on Node0 touches local page] --> B[Page already local]\n  B --> C[Low latency]\n  A --> D[Worker on Node1 accesses remote page]\n  D --> E[Page migration & TLB shootdown]\n  E --> F[Latency spike]","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T05:43:45.371Z","createdAt":"2026-01-17T05:43:45.372Z"},{"id":"q-3243","question":"Explain a concrete race in a Linux-like kernel when a memory-mapped page is faulted in while a concurrent writeback evicts the same page. How does page cache coherence, dirty flags, and TLB invalidation interact to yield a stale read or lost write? Propose a minimal fix to guarantee visibility without sacrificing performance?","answer":"A concrete race: a reader faults in a mmap’d page while a writer’s writeback evicts that same page, yielding a stale read. Coherence relies on page dirty flags, writeback, and cache invalidation on fa","explanation":"## Why This Is Asked\nTests understanding of page fault paths, page cache coherence, TLB/invalidation, and race conditions between page cache eviction and fault handling.\n\n## Key Concepts\n- Page cache lifecycle (cache hits, misses, eviction)\n- Page fault handling and faulting threads\n- TLB invalidation and cache coherence across cores\n\n## Code Example\n```javascript\n// Simplified fault path sketch (not actual kernel code)\nfunction handleMmapFault(page, proc) {\n  if (!page.inCache) {\n    loadPageIntoCache(page, proc);\n  }\n  // else, return cached data\n}\n```\n\n## Follow-up Questions\n- How would you implement per-page pinning to prevent eviction during fault handling without harming performance?\n- How would you test this race with stress tests and performance counters?","diagram":"flowchart TD\nA[Reader faults mmap page] --> B{Page present?}\nB -- Yes --> C[Read from page cache]\nB -- No --> D[Page fault handler fetches page]\nD --> E[Page loaded into cache]\nE --> F[Resume read; invalidate TLB if needed]","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Microsoft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T08:39:36.711Z","createdAt":"2026-01-17T08:39:36.711Z"},{"id":"q-3490","question":"Explain the exact behavior and safety implications when one process mmap()s a file region with MAP_SHARED, and another process truncates the same file while the mapping exists; detail what happens on access at an offset beyond the new end, which kernel data structures are involved, and minimal steps to guarantee correctness in a multi-process program?","answer":"Serialization is required. If truncation and access race, the mapping may serve stale data or fault with SIGBUS. A safe pattern is to lock around both operations, unmap the region before truncation an","explanation":"## Why This Is Asked\nTests understanding of mmap, page cache, locking, and cross-process synchronization when file size changes.\n\n## Key Concepts\n- MAP_SHARED semantics\n- Page cache invalidation and TLB coherence\n- Synchronization patterns: locks, munmap/remap, msync\n- Truncation behavior and SIGBUS vs stale data\n\n## Code Example\n```javascript\n// Example: pseudo C-like code showing lock around ftruncate and remap\nint fd = open(\"data.bin\", O_RDWR);\npthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\npthread_mutex_lock(&lock);\nvoid *p = mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0);\n// reader uses p...\n// Truncator later:\nftruncate(fd, 2048); // shorten file\nmunmap(p, 4096);\n// remap after truncation\np = mmap(NULL, 2048, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0);\npthread_mutex_unlock(&lock);\n```\n\n## Follow-up Questions\n- How would you adapt this pattern for IPC via memory-mapped files?\n- What changes on Windows for similar semantics?","diagram":"flowchart TD\nA(Process A maps region) --> B(Process B truncates) \nB --> C{Access occurs concurrently?}\nC --> D[Page cache may be stale or trigger fault]\nD --> E[TLB/page-table coherence considerations]\nE --> F[Use munmap/remap with locking to guarantee correctness]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T18:47:16.702Z","createdAt":"2026-01-17T18:47:16.702Z"},{"id":"q-3525","question":"Design a lock-free bounded ring buffer for 1 producer and 1 consumer sharing memory. Each slot stores a 64-byte payload and a 64-bit sequence. The producer writes payload, then updates the sequence to indicate new data; the consumer reads only when the sequence indicates fresh data, then advances. Describe exact write/read order, x86-64 memory barriers, wraparound handling, and empty/full detection without locks?","answer":"Use a bounded ring with per-slot 64-bit sequence counters. Producer writes payload, then increments the slot's sequence with a release store and advances the tail. Consumer uses an acquire load of the","explanation":"## Why This Is Asked\nAssesses practical lock-free IPC, memory ordering, and correctness under contention. Tests knowledge of release/acquire semantics, false sharing avoidance, and wraparound handling in a real kernel-like primitive.\n\n## Key Concepts\n- Lock-free synchronization\n- Acquire/release memory order on x86-64\n- Per-slot sequencing and wraparound handling\n- False sharing mitigation via padding\n\n## Code Example\n```javascript\n// Pseudocode illustrating the producer side (simplified)\nslot.seq.store(slot.seq.read() + 1, RELEASE);\nslot.payload = data;\nslot.seq.store(slot.seq.read(), RELEASE);\n```\n\n```javascript\n// Pseudocode illustrating the consumer side (simplified)\nlet s = slot.seq.load(ACQUIRE);\nif (s % 2 === 1) {\n  let data = slot.payload;\n  // process data\n  slot.seq.store(s + 1, RELEASE);\n}\n```\n\n## Follow-up Questions\n- How would you extend to multiple producers/consumers while preserving correctness?\n- What diagnostics would you add to detect stalled or stuck queues under CPU throttling?","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T19:37:49.782Z","createdAt":"2026-01-17T19:37:49.782Z"},{"id":"q-3554","question":"In a Linux-like OS using io_uring for async I/O, an 8-core NUMA system with a shared submission/completion ring handles bursts of 4KiB reads to NVMe. A per-I/O sequence is proposed; describe a concrete interleaving that could cause a completion to be lost or delivered out of order, and specify the memory ordering needed to guarantee in-order, at-least-once completions. Propose a minimal fix (e.g., per-I/O sequence numbers) and discuss performance trade-offs?","answer":"In a high-throughput io_uring scenario, a problematic interleaving occurs when multiple cores submit I/O operations during burst conditions. Specifically, Core A submits a 4KiB read, increments the per-I/O sequence counter, and posts completion before the data is fully visible in the user buffer. Meanwhile, Core B reuses the same submission slot, potentially overwriting the completion ring entry before Core A's consumer processes it. This creates two failure modes: lost completions when ring entries are overwritten, and out-of-order delivery when memory barriers are insufficient across NUMA boundaries.\n\nTo guarantee in-order, at-least-once completions, you must establish proper memory ordering: release semantics on the producer side after writing completion data, acquire semantics on the consumer side before reading completion data, and atomic synchronization of completion ring head/tail indices. The minimal fix introduces per-I/O sequence numbers with atomic compare-and-swap operations on completion ring updates, ensuring each completion is uniquely identifiable and processed exactly once.","explanation":"## Why This Is Asked\nThis question tests deep understanding of io_uring's concurrency model, memory ordering semantics, and real-world race conditions in high-performance I/O systems.\n\n## Key Concepts\n- io_uring submission/completion ring ordering semantics\n- Cross-core memory barriers and NUMA-aware synchronization\n- Per-I/O sequencing for completion tracking\n- Ring buffer management under burst conditions\n- Atomic operations for completion ring updates\n\n## Code Example\n```c\n// Pseudo-C outline showing proper barriers\nstruct io_slot *s = &ring->slots[prod_head];\n\n// Ensure data is visible before updating ring indices\natomic_store_explicit(&s->data, completion_data, memory_order_release);\natomic_store_explicit(&ring->prod_tail, new_tail, memory_order_release);\n\n// Consumer side with acquire semantics\nuint32_t tail = atomic_load_explicit(&ring->prod_tail, memory_order_acquire);\ncompletion_data = atomic_load_explicit(&s->data, memory_order_acquire);\n```\n\n## Performance Trade-offs\nPer-I/O sequence numbers add minimal overhead (8 bytes per operation) but provide strong correctness guarantees. The atomic operations introduce ~5-10ns latency per completion, which is negligible compared to NVMe access times (~100μs). However, under extreme burst conditions, contention on the completion ring indices can become a bottleneck, potentially requiring sharding or per-core completion queues.","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","IBM","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:50:20.690Z","createdAt":"2026-01-17T21:26:05.837Z"},{"id":"q-3614","question":"On a Linux-like kernel using hazard-pointer-based reclamation for a shared in-memory index, writers replace entries by atomic pointer swaps and reclaim old nodes after a grace period. Describe a concrete interleaving on a 2-socket NUMA system that could cause a reader to observe a newly published but uninitialized node, and explain the required memory ordering to prevent it (publish with release, read with acquire, or use a ready flag). Propose a minimal fix and discuss trade-offs?","answer":"An interleaving: a writer performs a compare-and-swap to publish a new node pointer before fully initializing its fields; a reader loads the pointer and dereferences it before initialization completes, reading partially initialized data. The fix is to publish with release semantics, read with acquire semantics, or use a ready flag.","explanation":"## Why This Is Asked\nTests understanding of hazard-pointer reclamation, publication-order guarantees, and cross-CPU memory ordering under NUMA. It probes correctness in lock-free updates and safe reclamation with real-world implications.\n\n## Key Concepts\n- Hazard pointers and grace periods\n- Atomic publish vs initialization order\n- Release/acquire memory barriers\n- NUMA visibility and cross-node ordering\n- Safe reclamation trade-offs\n\n## Code Example\n```javascript\n// C-like pseudo\nstruct Node { int data; int ready; };\nNode* n = alloc();\nn->data = 123;\n// no barrier yet\ncas(&head, old, n); // publish before initialization\n```\n\n## Trade-offs\n- Release/acquire: minimal overhead but requires careful ordering\n- Ready flag: simpler logic but adds extra field and check\n- Full fence: strongest guarantees but highest performance cost","diagram":"flowchart TD\n  A[Writer allocates new node] --> B[Initialize fields]\n  B --> C[Publish pointer (CAS)]\n  C --> D[Old node reclaimed after grace period]\n  E[Reader loads head pointer] --> F[Reads ready flag]\n  F --> G[If ready==1, reads valid data]\n  F --> H[If ready==0, waits or retries]","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Meta","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:14:52.278Z","createdAt":"2026-01-17T23:37:38.642Z"},{"id":"q-3625","question":"In a Linux kernel driver for a PCIe DMA device, the host uses a shared descriptor ring with the device. A producer thread writes descriptors and updates a head pointer; the device consumes descriptors and raises an interrupt on completion. Describe a concrete interleaving that could yield a lost or out-of-order completion if there is no proper synchronization between producer writes and device processing. Propose a minimal fix using per-descriptor sequence numbers and memory barriers (e.g., smp_wmb, dma_wmb) and explain how this guarantees in-order, at-least-once completion, including cache coherence and DMA semantics?","answer":"Interleaving: producer writes desc0, then desc1, then updates head to 1; device may observe head=1 and fetch desc1 before desc0 is visible due to missing barriers. Fix: add a per-descriptor seq field; device validates seq matches expected index before processing, using dma_wmb() before seq write and smp_wmb() before head update to ensure proper ordering.","explanation":"## Why This Is Asked\nTests concrete reasoning about DMA rings, ordering guarantees, and barrier semantics in drivers.\n\n## Key Concepts\n- PCIe DMA descriptor rings, producer/consumer synchronization\n- Per-descriptor sequence numbers, publish semantics\n- Memory barriers (dma_wmb, smp_wmb) and DMA coherence\n- Cache lines and visibility between CPU and device\n\n## Code Example\n```c\n// pseudo-C like\nstruct desc { uint64_t addr; uint32_t len; uint32_t flags; uint64_t seq; };\ndesc[i].addr = data; \ndesc[i].len = n; \ndma_wmb();\ndesc[i].seq = i; \nsmp_wmb();\nhead = i; // publish\n```\n\n## Follow-up Questions\n- How would you handle sequence number wraparound?\n- What if the device doesn't support sequence numbers?","diagram":"flowchart TD\n  A[Producer writes descriptor] --> B[Publish with barriers]\n  B --> C[Device sees descriptor]\n  C --> D[Device completes descriptor]\n  D --> E[Interrupt signals completion]\n  E --> F[Host validates sequence numbers]","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Google","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:08:17.913Z","createdAt":"2026-01-18T02:29:02.980Z"},{"id":"q-3764","question":"Scenario: A 10 Gbps NIC with multiple receive queues experiences a burst that floods the RX path. Explain the end-to-end path from RX interrupt to user-space delivery, including hardirq, NAPI polling, and the spillover to ksoftirqd, and how CPU affinity and irqbalance affect latency. Provide concrete measurement steps and mitigations (NAPI budget, backlog tuning, per-queue IRQ affinity, CPU isolation)?","answer":"End-to-end: RX interrupt -> hardirq -> NAPI poll; if burst exceeds budget, remaining packets queued to ksoftirqd, raising latency. Measure with ftrace, perf, and latency tests. Mitigations: enable pro","explanation":"## Why This Is Asked\nTests understanding of network I/O paths and how backlog affects latency in practice.\n\n## Key Concepts\n- RX interrupt handling, hardirq vs softirq\n- NAPI, poll budget, and ksoftirqd\n- IRQ affinity,CPU isolation, and irqbalance\n- Latency measurement techniques\n\n## Code Example\n\n```bash\n# Example: monitor per-queue backlog and interrupts\nwatch -n1 'cat /proc/interrupts; cat /proc/net/softnet_stat'\n```\n\n## Follow-up Questions\n- How would you diagnose a regression after enabling IRQ affinity?\n- What kernel parameter changes would you test first on a live system?","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Snap","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T08:44:57.605Z","createdAt":"2026-01-18T08:44:57.605Z"},{"id":"q-3892","question":"Two processes write concurrently to the same file opened with O_APPEND. Describe end-to-end how the kernel ensures atomic appends, what can cause atomicity to fail (buffered I/O, network FS), and how you would guarantee visibility and ordering in practice?","answer":"On a Linux-like kernel, O_APPEND makes each write execute atomically with respect to other O_APPEND writes by updating the per-file offset in an atomic operation and appending the data to the inode bu","explanation":"## Why This Is Asked\nTests understanding of atomic file semantics, VFS buffering, and real-world filesystem behavior under concurrent writes.\n\n## Key Concepts\n- Atomic appends with O_APPEND\n- VFS inode buffering and writeback\n- fsync/fdatasync for visibility guarantees\n- Atomicity caveats on network/remote FS\n\n## Code Example\n```c\nint fd = open(\"data.log\", O_WRONLY | O_APPEND);\nssize_t n = write(fd, buf, len);\n// Expect write to append atomically with respect to other O_APPEND writes\n```\n\n## Follow-up Questions\n- How would you test atomicity across a network filesystem? \n- What changes when using O_SYNC or writing via a dedicated append-logging mechanism?","diagram":"flowchart TD\n  A[Proc A write] --> B[VFS append path]\n  B --> C[Inode append buffer]\n  C --> D[Disk write]\n  E[Proc B write] --> F[VFS append path]\n  F --> G[Inode append buffer]\n  G --> D\n  H[Reader observes data] --> D","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Instacart","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T13:56:18.302Z","createdAt":"2026-01-18T13:56:18.302Z"},{"id":"q-4229","question":"On a Linux host running multiple containers, memory pressure spikes and kswapd starts reclaiming pages while some containers still serve high request rates. Describe the full sequence from a user-space page fault to eviction, including page cache lookup, anonym vs file-backed handling, reclaim, writeback, and LRU state movement. Explain how THP interacts with this path and the trade-offs of swapping vs not swapping under pressure?","answer":"Fault -> page walk → page cache miss → allocate page and populate (anon/file-backed); under pressure, kswapd reclaims by scanning Active/Inactive LRU, buffers unevictables, shrinks pages, writes dirty","explanation":"## Why This Is Asked\nThis tests deep understanding of Linux memory reclaim in multi-tenant, containerized workloads, including page cache, swap, and THP interactions.\n\n## Key Concepts\n- kswapd, LRU, shrinkers, page cache maintenance\n- Anonymous vs file-backed pages, writeback, and dirty page reclaim\n- Transparent Huge Pages (THP) impact on reclaim and fragmentation\n- Swap vs no-swap trade-offs and memory-cgroup effects\n\n## Code Example\n```c\n/* simplified sketch: trigger mempressure, observe reclaim */\nwhile (1) {\n  munmap(...);\n  // force a reclaim observation point\n}\n```\n\n## Follow-up Questions\n- How to diagnose reclaim stalls with slabtop/vmstat? \n- What knobs optimize container memory pressure without swap? \n- How does compaction interact with THP under memory pressure?","diagram":"flowchart TD\n  A(User Fault) --> B[Page Cache Lookup]\n  B --> C{Hit}\n  C -- Yes --> D[Return Data to User]\n  C -- No --> E[Allocate Page & Populate]\n  E --> F{Memory Pressure}\n  F -- Yes --> G[kswapd reclaim & shrinkers]\n  G --> H[Writeback & Evict to Disk/Swap]\n  H --> D\n  F -- No --> D","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Airbnb","Citadel","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T09:44:49.511Z","createdAt":"2026-01-19T09:44:49.511Z"},{"id":"q-4258","question":"Explain, at a practical level, the sequence of events when a process touches a stack address just beyond the current stack limit and triggers a stack-growth fault in a simple Linux-like VM. Include how the kernel decides to grow the stack, allocates and zeros the new page, updates the user page table, adjusts the stack pointer, and invalidates the TLB. Mention edge cases such as concurrent growth and guard-page handling?","answer":"Describe the fault path: the trap handler checks the fault address against the process's stack growth policy and RLIMIT_STACK; if allowed, allocate a 4KB page, zero it, map it at the next lower addres","explanation":"## Why This Is Asked\n\nStack growth is a core OS mechanism; understanding it reveals how memory safety and performance interact in practice.\n\n## Key Concepts\n\n- Stack growth policy and guard pages\n- RLIMIT_STACK and address space layout\n- Page fault handling and page-table updates\n- TLB invalidation and re-execution\n- Concurrency with multiple threads touching stack boundaries\n\n## Code Example\n\n```javascript\n// high-level pseudo\nif (fault_addr in stack && allowed) {\n  pa = alloc_page();\n  map(p, pa, fault_addr_page, 'RW');\n  zero_page(pa);\n  flush_tlb(p, fault_addr_page);\n} else {\n  handle_fault_error();\n}\n```\n\n## Follow-up Questions\n\n- How would you test stack growth under multithreaded conditions?\n- What changes would you make to support very large stacks or reduce guard-page fragmentation?","diagram":"flowchart TD\n  A[Stack growth fault occurs (touch beyond limit)] --> B[Kernel validates growth policy & RLIMIT_STACK]\n  B --> C[Allocate 4KB page and zero it]\n  C --> D[Map into user-space at next lower address with RW]\n  D --> E[Update stack pointer in the PCB/TSS if needed]\n  E --> F[Invalidate relevant TLB entries]\n  F --> G[Resume execution at faulting instruction]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Adobe","Plaid","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T10:48:03.022Z","createdAt":"2026-01-19T10:48:03.022Z"},{"id":"q-4313","question":"Scenario: A database process uses a 1 GiB region mapped with MAP_ANONYMOUS and userfaultfd to handle on-demand paging. Multiple workers fault pages concurrently to populate the region with hot data. Describe the race conditions when two workers fault the same page, how to coordinate with per-page state, and how to ensure visibility and ordering across faults. Propose a concrete synchronization strategy (e.g., per-page lock with double-checked loading) and discuss performance trade-offs?","answer":"Use per-page state machine: 0=UNLOADED,1=LOADING,2=READY. On fault, thread uses an atomic compare-and-swap to set LOADING if UNLOADED; if it wins, it populates the page and then stores READY with a re","explanation":"## Why This Is Asked\nInteracting workers faulting the same page risks double work and stale data; userfaultfd paths require precise coordination to avoid data races and ensure memory visibility across cores. The scenario tests low-level synchronization, memory ordering, and fault handling.\n\n## Key Concepts\n- Per-page state machine (UNLOADED, LOADING, READY)\n- Atomic CAS and release/acquire semantics\n- Futex or condition variable waiting for other threads\n- Correct interaction with userfaultfd and page fault semantics\n- Performance trade-offs of fine-grained locking vs batching\n\n## Code Example\n```c\n// Pseudo C: per-page loading protocol\ntypedef enum { UNLOADED=0, LOADING=1, READY=2 } page_state_t;\ntypedef struct { _Atomic int state; /* 0/1/2 */ } page_t;\n\nvoid access_page(page_t *p, void *dst) {\n  int s = atomic_load_explicit(&p->state, memory_order_acquire);\n  if (s == READY) { /* copy data to dst from page */ return; }\n  if (s == UNLOADED) {\n    if (atomic_compare_exchange_strong(&p->state, &UNLOADED, LOADING)) {\n      // fault will be resolved by loader\n      trigger_fault_and_load(p);\n      atomic_store_explicit(&p->state, READY, memory_order_release);\n      wake_waiters(p);\n      /* copy data to dst */\n      return;\n    }\n  }\n  // wait until READY\n  wait_on_futex(&p->state, READY);\n  /* copy data */\n}\n```\n\n## Follow-up Questions\n- How would you adapt this to NUMA and cache-line alignment?\n- What failure modes exist if loader crashes or out-of-memory occurs?","diagram":"flowchart TD\n  A[Fault event] --> B[Check per-page state]\n  B --> C{LOADING?}\n  C -- yes --> D[Wait/subscribe]\n  C -- no --> E[Load data]\n  E --> F[Publish READY]\n","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Apple","Google","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T13:15:37.696Z","createdAt":"2026-01-19T13:15:37.696Z"},{"id":"q-4356","question":"In a MAP_SHARED region, two threads access a 4-byte integer at offset 0 without synchronization. Provide a concrete interleaving that can yield a torn/partial read and explain why. Then specify the fix: use atomic stores or memory barriers, or a proper synchronization primitive; describe which memory_order to use and why, and how to ensure visibility across cores?","answer":"Concrete interleaving: two 16-bit stores to a single 32-bit word write high half 0xAABB then low half 0xCCDD. Reader may observe 0xAABB0000 or 0x0000CCDD, i.e., a torn read since writes aren’t atomic.","explanation":"## Why This Is Asked\n\nTests understanding of memory ordering, atomicity, and visibility when sharing memory across threads without OS-provided synchronization.\n\n## Key Concepts\n\n- Atomicity of multi-byte writes\n- Memory ordering (memory_order_seq_cst, acquire/release)\n- Visibility across cores\n- Practical synchronization primitives (mutex, atomic ops)\n\n## Code Example\n\n```javascript\n// Illustrative C-like example using C11 atomics\n#include <stdatomic.h>\n#include <stdint.h>\n\nvoid write_shared(atomic_uint32_t *addr, uint32_t value) {\n  atomic_store(addr, value);\n}\nuint32_t read_shared(atomic_uint32_t *addr) {\n  return atomic_load(addr);\n}\n```\n\n## Follow-up Questions\n\n- How would you test-reproduce the torn read scenario in a unit test?\n- What differences might you expect on ARM vs x86 architectures regarding atomicity?\n- How would you adapt the solution if the value were two 32-bit halves stored in a 64-bit word?","diagram":null,"difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Databricks","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T15:42:04.989Z","createdAt":"2026-01-19T15:42:04.989Z"},{"id":"q-4373","question":"In a Linux server with Transparent Huge Pages (THP) enabled and memory overcommit disabled, a spike in allocations causes THP allocations to fail while small pages are still free. Describe the exact THP allocation path, why fragmentation triggers this failure, and provide a concrete mitigation plan with tuning steps and measurable signals?","answer":"THP allocations coalesce free 4KB pages into 2MB hunks; during a spike fragmentation prevents a contiguous 2MB region, so THP fails and callers stall. Mitigate by provisioning peak huge pages (vm.nr_h","explanation":"## Why This Is Asked\n\nTests practical understanding of Linux THP path, fragmentation, and tuning.\n\n## Key Concepts\n\n- THP allocation path from buddy allocator to large page\n- Fragmentation and defrag triggers\n- sysfs/sysctl knobs: vm.nr_hugepages, /sys/kernel/mm/transparent_hugepage/enabled, defrag\n- Trade-offs: latency vs memory efficiency\n\n## Code Example\n\n```c\n#include <sys/mman.h>\n#include <stddef.h>\nvoid hint_hugepage(void *addr, size_t len){\n    (void)addr; (void)len;\n    madvise(addr, len, MADV_HUGEPAGE);\n}\n```\n\n## Follow-up Questions\n\n- How to measure THP fragmentation and HugePages usage?\n- What changes for latency-sensitive services?","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","LinkedIn","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T16:41:07.963Z","createdAt":"2026-01-19T16:41:07.963Z"},{"id":"q-4410","question":"Scenario: a Linux server uses an anonymous mmap buffer as a circular IPC queue shared by 8 workers. Each slot contains an 8-byte header (seq, done) and a 256-byte payload. Writers update header before/after writing payload; readers rely on header to detect complete entries. Propose a precise, lock-free protocol that guarantees readers never observe partially updated entries, including the exact write/read order, required memory barriers, and a concrete contention test on real hardware?","answer":"Use a per-slot seqlock-like protocol. Writer: publish 0 to seq, write payload, then set seq to 1 (commit). Reader: read seq; if 0 or if seq changes before reading payload, retry; otherwise read payloa","explanation":"## Why This Is Asked\n\nTests lock-free IPC design, precise memory ordering, and verification under contention.\n\n## Key Concepts\n\n- lock-free primitives, per-slot sequencing, memory barriers, mmap-based IPC, cache coherence.\n- detection of partial writes and ensuring visibility under contention.\n\n## Code Example\n\n```c\ntypedef struct { uint64_t seq; char payload[256]; } slot_t;\n\nvoid write_slot(slot_t *s, void *data, size_t n){\n  __atomic_store_n(&s->seq, 0, __ATOMIC_RELEASE);\n  memcpy(s->payload, data, n);\n  __atomic_store_n(&s->seq, 1, __ATOMIC_RELEASE);\n}\n\nbool read_slot(slot_t *s, void *out){\n  uint64_t a = __atomic_load_n(&s->seq, __ATOMIC_ACQUIRE);\n  if (a != 1) return false;\n  memcpy(out, s->payload, 256);\n  uint64_t b = __atomic_load_n(&s->seq, __ATOMIC_ACQUIRE);\n  return (a == b && a == 1);\n}\n```","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Netflix","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T17:46:37.921Z","createdAt":"2026-01-19T17:46:37.921Z"},{"id":"q-448","question":"Explain how process scheduling works in an operating system. Which scheduling algorithm would you choose for a real-time system and why?","answer":"Process scheduling manages CPU allocation among processes by determining which process gets CPU time and when. For real-time systems, I would choose **Rate Monotonic Scheduling (RMS)** because it's predictable, has bounded response times, and is optimal for fixed-priority real-time systems. RMS assigns higher priorities to tasks with shorter periods, ensuring critical tasks meet their deadlines while maintaining system stability.","explanation":"## Key Concepts\n- **Process scheduling**: Determines CPU allocation order among competing processes\n- **Scheduling algorithms**: FCFS, SJF, Priority, Round Robin, Rate Monotonic Scheduling\n- **Real-time requirements**: Deterministic timing behavior and deadline guarantees\n\n## Real-time Considerations\n- **Predictability**: Must guarantee task completion before deadlines\n- **Priority assignment**: Shorter periods receive higher priorities in RMS\n- **CPU utilization**: RMS can safely utilize up to 69% of CPU capacity\n\n## Implementation\n```c\n// RMS priority calculation\npriority = 1000 / period; // Higher values for shorter periods\n```\n\n## Trade-offs\n- RMS offers simplicity and predictability but is less flexible compared to dynamic scheduling algorithms\n- Optimal for fixed-priority systems but may require careful task design","diagram":"flowchart TD\n  A[Process Request] --> B{Scheduler}\n  B --> C[RMS Priority Check]\n  C --> D[CPU Allocation]\n  D --> E[Process Execution]\n  E --> F[Deadline Check]\n  F --> G{Met Deadline?}\n  G -->|Yes| H[Continue]\n  G -->|No| I[Priority Boost]\n  H --> B","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","OpenAI","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T08:53:46.358Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-4484","question":"In the Linux kernel, design a lock-free single-producer, multiple-consumer ring buffer for a device interrupt that enqueues event pointers. Describe the exact sequence and memory ordering you would enforce (which atomic ops, release/acquire semantics, and ABA avoidance), how you prevent data races across slots, and why a spinlock would be slower. Illustrate with a concrete interleaving involving two consumers and one producer?","answer":"Use a per-slot sequence counter and a single atomic tail. Producer CASes to reserve a slot, writes the event pointer, then releases with a memory barrier. Consumers read with acquire, verify slot.seq ","explanation":"## Why This Is Asked\n\nThis probes memory ordering, ABA avoidance, and lock-free queue design in kernel contexts.\n\n## Key Concepts\n\n- Lock-free queues (SPMC)\n- Acquire/release barriers\n- ABA/versioning\n- Cache coherence\n\n## Code Example\n\n```c\ntypedef struct { void *ptr; uint64_t seq; } slot_t;\n// producer: reserve slot, write ptr, release\n// consumer: load slot, check seq, consume\n```\n\n## Follow-up Questions\n\n- How would you benchmark latency under bursty interrupts?\n- How would you handle degraded perf if a consumer stalls?","diagram":null,"difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["LinkedIn","Microsoft","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T20:46:25.410Z","createdAt":"2026-01-19T20:46:25.410Z"},{"id":"q-4502","question":"On a single-core Linux-like system, two threads share a CPU: a CPU-bound worker and an I/O-bound worker that often sleeps on I/O. Explain how the Completely Fair Scheduler (CFS) uses vruntime and per-thread weights to decide which thread runs next, and why the I/O-bound thread tends to break up CPU time. Propose a minimal, practical adjustment (e.g., using cgroups cpu.shares or adjusting nice values) to improve fairness without sacrificing throughput?","answer":"CFS uses a red-black tree ordered by vruntime; the task with the smallest vruntime runs next. CPU-bound tasks accumulate vruntime quickly, while I/O-bound tasks sleep, so their vruntime grows slowly, giving them priority when they wake up and causing them to frequently interrupt CPU-bound work.","explanation":"## Why This Is Asked\n\nTo assess knowledge of OS scheduler internals and practical tuning knobs under realistic workloads.\n\n## Key Concepts\n\n- Completely Fair Scheduler (CFS) and vruntime\n- Per-task weights (CPU shares, nice values)\n- cgroups CPU quotas and fairness tuning\n- How I/O sleeps affect scheduling and potential starvation\n\n## Code Example\n\n```text\n// Pseudo sequence for one scheduler tick\ntask1.vruntime += task1_runtime / task1.weight\ntask2.vruntime += task2_runtime / task2.weight\n\nnext = min(vruntime)\n```\n\n## Follow-up Questions\n\n- How would you diagnose a scenario where CPU-bound tasks are being starved?\n- What metrics would you monitor to verify the effectiveness of your adjustments?\n- How do cgroups cpu.shares differ from nice values in practice?","diagram":"flowchart TD\n  A[Threads] --> B[CFS picks min vruntime]\n  B --> C[Run thread]\n  C --> D[vruntime updated]\n  D --> B","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Databricks","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T07:47:31.323Z","createdAt":"2026-01-19T21:44:13.048Z"},{"id":"q-4538","question":"On a NUMA Linux host running latency-sensitive containers, 2–5 ms spikes appear under heavy CPU load. Describe an end-to-end investigation and fix focusing on IRQ affinity, softirq, and scheduler interplay. Include how to identify root cause with ftrace or perf, isolate a CPU, pin the latency task, redirect device interrupts away, and validate that throughput remains stable?","answer":"Pin the latency-sensitive task to a dedicated CPU core using taskset, isolate CPUs via the isolcpus kernel parameter, and redirect device interrupts away using smp_affinity. Employ ftrace with trace_irq_handler_entry/exit, trace_softirq, and trace_sched_switch to identify where interrupt processing disrupts the latency task. Validate that throughput remains stable using perf stat for system-wide metrics and targeted latency measurements.","explanation":"## Why This Is Asked\nTests practical understanding of Linux kernel interrupt and scheduler interactions under NUMA pressure and containerization. Requires concrete tooling and steps, not high-level theory.\n\n## Key Concepts\n- NUMA locality, IRQ affinity, smp_affinity, isolcpus\n- ftrace/perf for end-to-end latency paths\n- Softirq, hardirq, scheduler preemption, and task pinning\n- Validation: throughput impact vs latency guarantees\n\n## Code Example\n```bash\n# Example commands (not executed here):\n# 1) isolate CPUs 4-7\n#   isolcpus=4-7 in bootloader\n# 2) pin latency task\n#   taskset -c 4 <latency_task>\n# 3) redirect interrupts\n#   echo 0 > /proc/irq/default_smp_affinity\n#   echo f0 > /proc/irq/24/smp_affinity\n# 4) trace with ftrace\n#   echo 1 > /sys/kernel/debug/tracing/events/irq/irq_handler_entry/enable\n#   echo 1 > /sys/kernel/debug/tracing/events/irq/softirq_entry/enable\n#   echo 1 > /sys/kernel/debug/tracing/events/sched/sched_switch/enable\n```","diagram":"flowchart TD\n  A[Heavy CPU load] --> B[IRQ bursts]\n  B --> C{Is latency task isolated?}\n  C -->|Yes| D[Latency stable on isolation core]\n  C -->|No| E[Spikes spread across cores]","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Adobe","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T06:22:26.040Z","createdAt":"2026-01-19T22:50:36.658Z"},{"id":"q-4554","question":"Explain, at the kernel level, how a blocking read() on a pipe is woken up when data arrives. Describe the sequence from data entering the pipe buffer to read() returning in user space, including the pipe's wait queue, the wakeup path, context switch, and the copy_to_user step?","answer":"When a blocking read() encounters an empty pipe, the kernel blocks the current task by enqueuing it on the pipe's wait queue and putting it to sleep. When a writer appends data, the pipe buffer length becomes non-zero, the kernel wakes up the waiting task via the wait queue mechanism, triggers a context switch back to the blocked process, copies the data from the pipe buffer to user space using copy_to_user(), and finally returns control to the caller in user space.","explanation":"## Why This Is Asked\nThis question tests understanding of kernel-level blocking I/O, wait queues, wakeups, and the data transfer path from kernel to user space. It's accessible to beginners while revealing familiarity with synchronization hazards and scheduler interactions.\n\n## Key Concepts\n- Blocking I/O and wait queues\n- Wakeup mechanisms and scheduler interactions\n- copy_to_user data transfer path from kernel to user space\n\n## Code Example\n```c\n// Pseudo kernel path sketch (simplified)\nssize_t pipe_read(struct file *f, void __user *buf, size_t count) {\n  if (pipe_len(f) == 0) {\n    wait_on_","diagram":null,"difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Airbnb","Robinhood","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T06:10:47.093Z","createdAt":"2026-01-19T23:44:05.026Z"},{"id":"q-4643","question":"In Linux, two processes share a 256-byte ring buffer mapped from persistent memory. The producer writes items with a 64-bit sequence number and a 64-bit payload. Propose a lock-free protocol guaranteeing no torn reads, in-order delivery, and crash-consistent recovery. Include exact operation order, memory barriers, and how to validate correctness on both x86-64 and ARM64?","answer":"Use a seqlock-like protocol: writer increments seq to an odd value, writes payload, flushes cache lines, issues a memory barrier, then increments seq to even. Reader reads seq, reads payload, re-reads","explanation":"## Why This Is Asked\nTests lock-free design with persistent memory, ensuring visibility, ordering, and crash-safety across architectures.\n\n## Key Concepts\n- Seqlocks and lock-free data paths\n- Persistent memory semantics and cache-line flushing\n- Memory barriers and cross-ISA ordering (x86-64 vs ARM64)\n- Crash-consistency guarantees and recovery\n\n## Code Example\n```javascript\n// Writer (pseudo-C in JS for illustration)\nfunction writeItem(item, payload) {\n  item.seq += 1n; // now odd\n  item.payload = payload;\n  clwb(item.payload_addr); // persist payload\n  mfence(); // ensure payload is visible before seq\n  item.seq += 1n; // now even\n}\n\nfunction readItem(item) {\n  let s1 = item.seq;\n  if (s1 % 2n === 1n) return null; // in-progress\n  let payload = item.payload;\n  let s2 = item.seq;\n  if (s1 !== s2) return null; // torn read\n  return payload;\n}\n```\n\n## Follow-up Questions\n- How would you adapt this for multiple producers and a single consumer?\n- What tests would validate reliability under power loss and cache-coherence delays?","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T06:49:58.156Z","createdAt":"2026-01-20T06:49:58.156Z"},{"id":"q-4692","question":"Design a robust lock-free multi-producer, single-consumer ring buffer used for inter-thread messaging with two producers and one consumer. Each slot holds a 64-bit key, a 64-bit value, and a 64-bit sequence. Describe the protocol (indices, per-slot sequence), the exact memory barriers required on x86_64 and ARM64, and a minimal code sketch for enqueue and dequeue. Include a concrete test plan that stresses visibility, wrap-around, and ABA issues?","answer":"Use a lock-free MPMC-like scheme with a per-slot sequence and a shared head counter updated via CAS. Producers obtain an index, write data, then publish by updating the slot’s sequence with a release-","explanation":"## Why This Is Asked\nExposes real-world lock-free design pitfalls: memory ordering, ABA, and visibility in a high-throughput ring buffer.\n\n## Key Concepts\n- Lock-free data structures and CAS\n- Per-slot sequencing to avoid ABA\n- Memory ordering on x86_64 vs ARM64\n- Correctness under contention and wrap-around\n\n## Code Example\n```c\n// Minimal enqueue (producer)\nbool enqueue(slot_t* buf, int n, uint64_t key, uint64_t val) {\n  static thread_local uint64_t prod = 0; // per-producer sequence\n  uint64_t idx = atomic_fetch_add(&head, 1) % n;\n  slot_t* s = &buf[idx];\n  s->key = key;\n  s->val = val;\n  atomic_store_explicit(&s->seq, prod*2+1, memory_order_release);\n  prod++;\n  return true;\n}\n\n// Minimal dequeue (consumer)\nbool dequeue(slot_t* buf, int n, uint64_t* out_key, uint64_t* out_val) {\n  static uint64_t tail = 0;\n  slot_t* s = &buf[tail % n];\n  uint64_t seq = atomic_load_acquire(&s->seq);\n  if ((seq & 1) == 0) return false; // empty\n  *out_key = s->key;\n  *out_val = s->val;\n  atomic_store_explicit(&s->seq, seq+1, memory_order_release);\n  tail++;\n  return true;\n}\n```\n\n## Follow-up Questions\n- How would you adapt to multiple consumer threads?\n- How to test for ABA and late-arriving writes under high contention?","diagram":"flowchart TD\n  A[Two Producers] --> B[Ring Buffer with slots]\n  B --> C[Consumer]\n  D[Head CAS] --> B\n  C --> E[Data visible to consumer after seq published]","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Anthropic","Salesforce","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T08:47:06.899Z","createdAt":"2026-01-20T08:47:06.899Z"},{"id":"q-471","question":"You're designing a GPU memory manager for CUDA applications. How would you implement a memory allocator that handles both unified memory and explicit device memory, considering fragmentation, coalescing, and the 48-bit address space limitations?","answer":"I would implement a hybrid memory allocator combining segregated free lists for different size classes with a buddy system for large allocations, utilizing virtual memory techniques to efficiently manage the 48-bit address space. The allocator would support both unified memory and explicit device memory through separate allocation pools while providing automatic coalescing and fragmentation management.","explanation":"## Memory Management Architecture\n\n### Allocation Strategies\n- **Segregated Lists**: Separate free lists optimized for small (4KB-64KB), medium (64KB-1MB), and large (>1MB) allocations to minimize search time\n- **Buddy System**: Power-of-2 allocation for large blocks enabling efficient coalescing and reducing external fragmentation\n- **Slab Allocation**: Specialized pools for frequently used small objects to minimize internal fragmentation\n\n### Address Space Management\n```c\n// 48-bit virtual address layout\ntypedef struct {\n    uint64_t prefix : 16;  // Reserved for future expansion\n    uint64_t vaddr   : 48;  // Actual virtual address\n} gpu_vaddr_t;\n```\n\n### Unified Memory Optimization\n- **Migration Policies**: Implement adaptive page migration based on access patterns and locality\n- **Prefetching**: Hardware-guided prefetching for anticipated memory accesses\n- **Page Fault Handling**: Efficient page fault resolution with minimal CPU intervention","diagram":"flowchart TD\n  A[Application Request] --> B{Memory Type}\n  B -->|Unified| C[Unified Memory Path]\n  B -->|Device| D[Explicit Device Memory]\n  C --> E[Page Fault Handler]\n  E --> F[Migration Heuristics]\n  F --> G[Allocate Virtual Pages]\n  D --> H[Size Class Selection]\n  H --> I[Segregated Free List]\n  I --> J[Buddy System Check]\n  J --> K[Physical Allocation]\n  G --> L[GPU Memory Mapping]\n  K --> L","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":["memory allocator","fragmentation","coalescing","segregated free lists","buddy system","virtual memory","cuda"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-09T09:04:49.876Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-4714","question":"In Linux, two processes share a 64-bit counter via a memory-mapped shared region (MAP_SHARED). Process A writes the counter as a single 64-bit write; Process B reads the counter once per second. Describe a concrete interleaving that yields a torn read on 32-bit architectures and explain why atomicity of 64-bit writes is not guaranteed there. Propose a minimal safe protocol to ensure atomicity and visibility, including exact operation order (atomic 64-bit ops vs version-tag approach)?","answer":"Interleaving: V0 = 0x00000000_00000000. A writes high 32 bits to 0x00000001; B reads; low 32 still 0, so B sees 0x00000001_00000000—a torn value. Minimal fixes: (1) atomic 64-bit stores on 64-bit CPUs","explanation":"## Why This Is Asked\nTests understanding of 64-bit atomicity, tearing, and how shareable memory interacts with differing CPU word sizes. It probes practical strategies to enforce visibility without resorting to heavyweight locks.\n\n## Key Concepts\n- Atomicity of 64-bit writes on 32-bit vs 64-bit CPUs\n- Tearing and memory ordering in shared mmap regions\n- Version-tag or atomic 64-bit ops as minimal guards\n\n## Code Example\n```c\n/* bad: two-step non-atomic write on 32-bit arch */\nvolatile uint64_t *p; /* mapped shared */\np[0] = (uint64_t)high << 32 | low;\n\n/* better: versioned write (conceptual) */\nuint32_t *vers = (uint32_t*)p; // version at p[0]\nuint64_t *val = (uint64_t*)(p+1);\n*vers = *vers + 1; // pre-update version\n*val = new_val;     // write actual value\n*vers = *vers + 1; // post-update version\n```\n\n## Follow-up Questions\n- How would you implement a portable reader that never returns torn values?\n- What are the trade-offs of using atomic64_t vs a separate version tag?","diagram":"flowchart TD\n  A[Start read] --> B[Read ver1]\n  B --> C[Read value]\n  C --> D[Read ver2]\n  D --> E{ver1==ver2}\n  E -->|yes| F[Return value]\n  E -->|no| A","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T09:42:45.178Z","createdAt":"2026-01-20T09:42:45.178Z"},{"id":"q-4786","question":"Explain end-to-end how the Linux-like kernel handles a page fault that triggers stack growth for a thread due to deep recursion. Describe the steps from faulting instruction to resumed execution: fault diagnosis, stack VMA checks, guard pages, allocating a zero-filled page, updating the stack PTE, and issuing a TLB shootdown; address synchronization for multi-threaded growth and potential performance costs?","answer":"On fault, kernel checks if the address is in the thread's stack VMA and if growth is allowed. If yes, allocate a zero-filled page, map it into the user page table, and perform a TLB shootdown. The fau","explanation":"## Why This Is Asked\n\nTests understanding of on-demand stack growth, page faults, and VM synchronization in multi-threaded contexts.\n\n## Key Concepts\n\n- Stack growth via page faults\n- Stack guard pages and VMA checks\n- Page table updates and TLB coherence\n- Concurrency control for per-thread stack growth\n\n## Code Example\n\n```javascript\n// Pseudo steps for stack growth\n```\n\n## Follow-up Questions\n\n- How would guard-page mispredictions impact performance?\n- What optimizations or data structures help reduce contention during growth?","diagram":"flowchart TD\n  Fault[Page Fault] --> Check[Check VMA/Stack Growth]\n  Check --> Allocate[Allocate Page]\n  Allocate --> UpdatePT[Update PTE + TLB]\n  UpdatePT --> Resume[Resume Instruction]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Adobe","Bloomberg","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T13:08:43.558Z","createdAt":"2026-01-20T13:08:43.558Z"},{"id":"q-4820","question":"Design an in-memory, shared-hash-map for a high-concurrency database worker pool. Implement an RCU-based read path so reads never block even during hash-table resizing, and writers perform safe updates by replacing a bucket structure and deferring reclamation. Explain grace periods, memory barriers on x86_64, and how to reclaim old nodes without use-after-free. Include a minimal code sketch and a microbenchmark outline?","answer":"RCU per bucket: readers run non-blocking; writers replace a bucket by allocating a fresh node, copying data, linking into the bucket list, and swapping the head with a release store, followed by synch","explanation":"## Why This Is Asked\\nReasoning: probes practical memory reclamation and non-blocking synchronization in kernel-like data structures.\\n\\n## Key Concepts\\n- Read-Copy-Update (RCU) semantics for updates\\n- Grace period, synchronize_rcu, safe reclamation\\n- Memory ordering on x86_64: release stores, READ_ONCE/WRITE_ONCE\\n- Per-bucket isolation to minimize cross-core traffic\\n\\n## Code Example\\n```c\n// Sketch\nstruct bucket { int value; struct bucket *next; };\nvoid reader(struct bucket *head) {\n  rcu_read_lock();\n  struct bucket *b = READ_ONCE(head);\n  int v = READ_ONCE(b->value);\n  rcu_read_unlock();\n}\nvoid writer(struct bucket **head) {\n  struct bucket *nb = malloc(...);\n  nb->value = newval;\n  nb->next = READ_ONCE(*head);\n  if (cmpxchg(head, nb->next, nb)) {\n    synchronize_rcu();\n  }\n}\n```\n\\n## Follow-up Questions\\n- How would you test for use-after-free under concurrency?\\n- Compare performance against a mutex-based approach under varying read/write ratios.","diagram":null,"difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Citadel","MongoDB","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T14:59:54.431Z","createdAt":"2026-01-20T14:59:54.431Z"},{"id":"q-5001","question":"Two worker threads share a single io_uring instance and submit I/O to the same file descriptor concurrently. Describe a concrete interleaving where two threads call io_uring_get_sqe() and io_uring_submit() in overlapping fashion, racing the tail pointer and potentially losing a submission or delivering out-of-order completions. What exact synchronization and memory-barrier guarantees are required to ensure atomic reservation of SQEs, and how would you structure code to guarantee in-order completions for logging while preserving high throughput?","answer":"Two worker threads contend for the submission queue tail pointer. Thread A obtains an SQE slot and writes its submission entry, while Thread B races to advance the kernel tail pointer and submit. Without proper memory barriers, one SQE can be overwritten or visibility delays can cause lost submissions or out-of-order completions. Correct synchronization requires atomic tail updates with release semantics when publishing SQEs and acquire semantics when consuming completions, combined with sequence numbering or per-thread submission queues to guarantee ordering while maintaining high throughput.","explanation":"## Why This Is Asked\nRaces in io_uring multi-producer submission and memory ordering are subtle in real systems; candidates demonstrate understanding of ring buffers, memory barriers, and ordering guarantees.\n\n## Key Concepts\n- io_uring submission and completion rings (SQ/CQ)\n- Multi-producer correctness in shared ring buffers\n- Memory barriers (write/read, acquire/release semantics)\n- In-order completion guarantees and request tagging\n\n## Code Example\n```c\n// Example: multi-producer submission with atomic SQE reservation\nstruct io_uring ring;\npthread_mutex_t sqe_mutex = PTHREAD_MUTEX_INITIALIZER;\n\n// Atomic SQE reservation with sequence numbers\nstatic atomic_uint_fast64_t seq_counter = ATOMIC_VAR_INIT(0);\n\nint submit_io_safe(int fd, void *buf, size_t len, uint64_t *seq_out) {\n    // Reserve SQE atomically\n    pthread_mutex_lock(&sqe_mutex);\n    struct io_uring_sqe *sqe = io_uring_get_sqe(&ring);\n    if (!sqe) {\n        pthread_mutex_unlock(&sqe_mutex);\n        return -EBUSY;\n    }\n    \n    // Assign sequence number for ordering\n    uint64_t seq = atomic_fetch_add_explicit(&seq_counter, 1, \n                                            memory_order_relaxed);\n    *seq_out = seq;\n    \n    // Prepare submission with sequence tag\n    io_uring_prep_write(sqe, fd, buf, len, 0);\n    sqe->user_data = seq;\n    \n    // Release barrier before submit\n    atomic_thread_fence(memory_order_release);\n    int ret = io_uring_submit(&ring);\n    pthread_mutex_unlock(&sqe_mutex);\n    \n    return ret;\n}\n\n// Completion processing with ordering\nvoid process_completions(void) {\n    struct io_uring_cqe *cqe;\n    \n    while (io_uring_peek_cqe(&ring, &cqe) == 0) {\n        // Acquire barrier before reading completion\n        atomic_thread_fence(memory_order_acquire);\n        \n        uint64_t seq = cqe->user_data;\n        // Process completion in sequence order...\n        \n        io_uring_cqe_seen(&ring, cqe);\n    }\n}\n```\n\n## Implementation Strategy\nFor high-throughput ordered logging, implement per-thread submission queues with a single consumer thread that preserves order while allowing parallel I/O preparation.","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Airbnb","Hugging Face","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-21T05:44:10.374Z","createdAt":"2026-01-20T23:00:05.951Z"},{"id":"q-5024","question":"In a POSIX shared-memory ring buffer used by two producers and one consumer, describe a lock-free MPMC queue using per-slot sequence numbers to avoid ABA. Specify: slot layout, enqueue/dequeue order, how to detect full/empty, and the exact memory barriers required on x86_64 and ARM64. How would you validate correctness under contention?","answer":"Implement a fixed-size ring buffer with per-slot sequence numbers and separate head/tail indices. Writers atomically reserve slots by advancing the tail index, write the payload data, then publish by updating the slot's sequence number with a release barrier. Readers verify the slot's sequence number with an acquire barrier, read the payload, and advance the head index. The buffer is full when tail - head equals capacity, and empty when head equals tail. On x86_64, release/acquire barriers suffice; on ARM64, full dmb ish is required for release and dmb ishld for acquire. Validate correctness through stress testing with randomized producer/consumer patterns, thread sanitizer analysis, and formal model checking.","explanation":"## Why This Is Asked\nThis question evaluates understanding of lock-free data structures, memory ordering semantics, and cross-platform memory barrier requirements in a production system context.\n\n## Key Concepts\n- MPMC ring buffer with per-slot sequence numbers\n- ABA problem mitigation via publish/subscribe pattern\n- Architecture-specific memory barrier requirements\n- Contention handling and wraparound correctness\n\n## Code Example\n```c\n// Pseudo-code for enqueue/dequeue operations\nbool enqueue(struct ring *r, void *data) {\n    size_t pos = atomic_fetch_add_explicit(&r->tail, 1, memory_order_relaxed);\n    size_t slot = pos % r->capacity;\n    \n    // Wait for slot to be available\n    while (r->slots[slot].seq != pos) {\n        // Backoff or handle contention\n    }\n    \n    // Write payload\n    r->slots[slot].data = data;\n    \n    // Publish with release barrier\n    atomic_store_explicit(&r->slots[slot].seq, pos + 1, memory_order_release);\n    return true;\n}\n\nbool dequeue(struct ring *r, void **data) {\n    size_t pos = atomic_load_explicit(&r->head, memory_order_relaxed);\n    size_t slot = pos % r->capacity;\n    \n    // Check if slot is ready with acquire barrier\n    size_t seq = atomic_load_explicit(&r->slots[slot].seq, memory_order_acquire);\n    if (seq != pos + 1) return false;\n    \n    // Read payload\n    *data = r->slots[slot].data;\n    \n    // Advance head\n    atomic_store_explicit(&r->head, pos + 1, memory_order_relaxed);\n    return true;\n}\n```\n\n## Memory Barriers by Architecture\n- **x86_64**: `memory_order_release`/`memory_order_acquire` (no additional barriers needed)\n- **ARM64**: `dmb ish` for release, `dmb ishld` for acquire operations\n\n## Validation Approach\n- Stress testing with multiple producers/consumers under high contention\n- ThreadSanitizer and AddressSanitizer for race condition detection\n- Formal verification using TLA+ or similar model checking tools\n- fuzzing with random production/consumption patterns","diagram":"flowchart TD\n  A[Producers] --> B[Tail increments]\n  B --> C[Slot write payload]\n  C --> D[Publish slot.seq (release)]\n  E[Consumer] --> F[Read slot.seq (acquire)]\n  F --> G[Read payload]\n  G --> H[Advance head]\n  D --> I[Ring progress]\n  H --> I","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Plaid","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-21T05:19:19.435Z","createdAt":"2026-01-21T00:03:16.086Z"},{"id":"q-5075","question":"On a Linux server using blk-mq with bfq, a worker pool handles both small random I/O (4 KB) and large sequential I/O (1 MB). Under peak load, tail latency for small I/O degrades unpredictably. Provide a concrete diagnostic plan to attribute latency to the IO scheduler, and outline a fix using per-cgroup IO weights or deadlines, including exact commands and measurement criteria?","answer":"Create two cgroups: foreground (io.weight=200) and background (io.weight=10); move short I/O threads into foreground. Run fio with randread 4k, and record p95/p99 latency; monitor throughput with iost","explanation":"## Why This Is Asked\n\nTests practical OS skills: IO scheduling, cgroups, and performance measurement under mixed workloads. It asks for a reproducible diagnostic protocol and a concrete fix that respects production constraints.\n\n## Key Concepts\n\n- Block IO schedulers (bfq/mq-deadline) and their fairness\n- cgroup-based QoS (io.weight, io.max)\n- Telemetry: p95/p99 latency, IOPS, throughput\n- Benchmarking with fio and iostat\n\n## Code Example\n\n```bash\n# Example: create cgroups (v2)\nmkdir -p /sys/fs/cgroup/io/foreground\necho 200 > /sys/fs/cgroup/io/foreground/io.weight\n# move pid 1234 to foreground\necho 1234 > /sys/fs/cgroup/io/foreground/cgroup.procs\n```\n\n## Follow-up Questions\n\n- How would you extend this to multi-tenant backends?\n- What if the device uses network-backed storage?","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Citadel","Coinbase","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T04:38:41.996Z","createdAt":"2026-01-21T04:38:41.996Z"},{"id":"q-5141","question":"Design a lock-free single-producer/single-consumer ring buffer implemented in a memory-mapped file shared between two processes. Specify the layout (head, tail, data region), the exact order of atomic operations for writes and reads, and how memory barriers ensure visibility across cores. Include a concrete interleaving example and show minimal code paths for producer and consumer?","answer":"Use a mmap’d ring with 64-bit head/tail and a data region. Producer writes the payload to the slot at tail, performs a release-store to the slot, then a release-store to tail. Consumer loops acquiring","explanation":"## Why This Is Asked\nTests practical IPC with shared memory and demonstrates mastery of memory ordering in a real, implementable primitive.\n\n## Key Concepts\n- Single-producer/single-consumer lock-free design\n- Atomic operations and acquire/release semantics\n- Memory visibility across cores and cache-line padding\n- mmap-based IPC and data layout in shared memory\n\n## Code Example\n```c\n// Simplified sketch (not production-ready)\ntypedef struct {\n  uint64_t head; // read index\n  uint64_t tail; // write index\n  char data[4096];\n} ring_t;\n\nvoid prod(ring_t *r, const void *src, size_t n){\n  uint64_t t = __atomic_load_n(&r->tail, __ATOMIC_RELAXED);\n  // write data to slot (omitting slot indexing details)\n  memcpy(r->data + (t % 1024) * 4, src, n);\n  __atomic_store_n(&r->tail, t+1, __ATOMIC_RELEASE);\n}\n\nvoid cons(ring_t *r, void *dst, size_t n){\n  uint64_t t = __atomic_load_n(&r->tail, __ATOMIC_ACQUIRE);\n  uint64_t h = __atomic_load_n(&r->head, __ATOMIC_RELAXED);\n  if (h < t){\n    memcpy(dst, r->data + (h % 1024) * 4, n);\n    __atomic_store_n(&r->head, h+1, __ATOMIC_RELEASE);\n  }\n}\n```\n\n## Follow-up Questions\n- How would you adapt for multiple producers? discuss ring partitioning or a per-slot sequence counter.\n- What changes for varying message sizes or overflow handling? discuss padding and data-copy costs.","diagram":null,"difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Lyft","Netflix","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T07:56:44.865Z","createdAt":"2026-01-21T07:56:44.865Z"},{"id":"q-5171","question":"Scenario: A 256 KiB MAP_SHARED ring buffer is exposed to user-space; a single producer writes 64-byte messages and updates a 32-bit write index after completing a write; multiple readers poll a per-slot header to fetch messages. Describe a concrete interleaving that can yield a stale read: header updated but data not visible, or vice versa. Explain cache/fence semantics and design a robust pattern (per-slot sequence numbers with acquire/release semantics and a release-store after payload) and a test plan to reproduce under contention?","answer":"Use per-slot sequence numbers with acquire/release semantics. Writer writes payload then updates seq to an odd value (release). Reader loads seq (acquire); if even, retry; if odd, reads payload and re","explanation":"## Why This Is Asked\nTests mastery of memory ordering and IPC patterns beyond high-level concepts, focusing on practical correctness under contention.\n\n## Key Concepts\n- Acquire/release semantics on per-slot sequencing\n- Avoiding torn reads in shared memory IPC\n- Cache coherence and memory fencing implications\n\n## Code Example\n```c\n// Pseudocode sketch of per-slot slot\ntypedef struct { uint64_t seq; uint8_t payload[64]; } Slot;\nSlot slot;\n```\n\n## Follow-up Questions\n- How would you test under high contention to detect rare races?\n- What architecture-specific pitfalls might you encounter (x86 vs ARM)?\n","diagram":"flowchart TD\n  A[Producer writes payload] --> B[Update seq to odd (release)]\n  C[Reader loads seq (acquire)] --> D{Is seq odd?}\n  D -->|No| E[Retry]\n  D -->|Yes| F[Reader reads payload]\n  F --> G[Re-check seq value]\n  G --> H{Seq unchanged?}\n  H -->|Yes| I[Delivery to consumer]\n  H -->|No| E","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Coinbase","Databricks","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T09:08:16.736Z","createdAt":"2026-01-21T09:08:16.736Z"},{"id":"q-5180","question":"Implementing a memory-mapped dataset backed by a file, lazily populate pages on first access with Linux userfaultfd. Describe the end-to-end flow from a page touch to fault handling, including concurrent fault races, page-in pathways (IO, zero-fill, read-ahead), and how to guarantee correctness and high throughput under contention?","answer":"Trap page faults via userfaultfd and implement a per-page guard to serialize concurrent faults. When a fault occurs, one thread fills the 4 KiB page from the backing file (pread) or zeros it, then use","explanation":"## Why This Is Asked\nTests understanding of userfaultfd workflows, race handling, and high-throughput memory-mapped I/O.\n\n## Key Concepts\n- userfaultfd lifecycle and UFFDIO_COPY\n- per-page synchronization to avoid duplicate fills\n- read-ahead strategies and cache behavior\n- memory ordering and TLB coherence\n\n## Code Example\n```javascript\n// Placeholder: not actual kernel code; concept illustration only\n```\n\n## Follow-up Questions\n- How would you extend to anonymous memory without a backing file?\n- What are performance pitfalls with read-ahead in sparse mappings?","diagram":"flowchart TD\n  A[Page Touch] --> B[Fault via userfaultfd]\n  B --> C[Fault Handler]\n  C --> D[Populate Page (ONE thread)]\n  D --> E[UFFDIO_COPY to page]\n  E --> F[Page Visible to All Threads]\n  F --> G[TLB Coherence / Shootdown]","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Apple","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T09:58:49.890Z","createdAt":"2026-01-21T09:58:49.890Z"},{"id":"q-5255","question":"In a multi-core OS scheduler, design a scalable per-core work queue that supports work-stealing across cores. Implement as a lock-free double-ended queue used by local producers and the local dequeuer, with a theft path from other cores. Describe data structures, memory ordering (acquire/release), ABA avoidance, and how you ensure correctness under high contention and preemption. Provide pseudo-code for enqueue, dequeue, and steal and discuss testing?","answer":"A per-core circular deque with a head/tail index and a small version stamp to guard against ABA; local enqueue/dequeue use atomic fetch_add/ncas with release/acquire semantics; other cores steal by CA","explanation":"## Why This Is Asked\n\nThis question probes lock-free concurrency, memory ordering, and real-world scheduler design under high contention across many cores.\n\n## Key Concepts\n\n- Lock-free deques and work-stealing semantics (Chase-Lev style)\n- Acquire/release semantics and memory barriers\n- ABA avoidance (version stamps, pointer tagging)\n- Safe memory reclamation (hazard pointers, epochs)\n- Correctness under preemption and high thread counts\n\n## Code Example\n\n```c\n// Pseudo: Chase-Lev style per-core deque (simplified)\ntypedef struct {\n  void* slots[DEQUE_SIZE];\n  _Atomic size_t head; // stealing/read by others\n  _Atomic size_t tail; // local producer/consumer index\n  // versioning to mitigate ABA\n  _Atomic size_t stamp;\n} Deque;\n\nvoid enqueue_bottom(Deque* d, void* v){\n  size_t t = atomic_load_explicit(&d->tail, memory_order_relaxed);\n  d->slots[t & MASK] = v;\n  atomic_store_explicit(&d->tail, t+1, memory_order_release);\n}\n\nvoid* dequeue_bottom(Deque* d){\n  size_t t = atomic_load_explicit(&d->tail, memory_order_relaxed);\n  if (t == atomic_load_explicit(&d->head, memory_order_acquire)) return NULL;\n  void* v = d->slots[(t-1) & MASK];\n  atomic_store_explicit(&d->tail, t-1, memory_order_relaxed);\n  return v;\n}\n\nvoid* steal_top(Deque* d){\n  size_t h = atomic_load_explicit(&d->head, memory_order_acquire);\n  size_t t = atomic_load_explicit(&d->tail, memory_order_acquire);\n  if (t <= h) return NULL;\n  void* v = d->slots[h & MASK];\n  if (atomic_compare_exchange_weak_explicit(&d->head, &h, h+1, memory_order_acquire, memory_order_relaxed))\n    return v;\n  return NULL;\n}\n```\n\n## Follow-up Questions\n\n- How would you extend for priority tasks and dynamic resizing?\n- How would you validate absence of ABA, memory leaks, and task loss under stress?","diagram":"flowchart TD\n  A[Local enqueue] --> B[Update tail]\n  B --> C[Local dequeue]\n  D[Steal attempt] --> E[CAS on head]\n  E --> F[Task processed or retry]\n  C --> G[Task completion]\n  F --> G","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Netflix","Oracle","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T13:22:34.221Z","createdAt":"2026-01-21T13:22:34.221Z"},{"id":"q-5291","question":"In a tiny preemptive OS with per-CPU run queues and a single disk I/O device, describe the end-to-end flow when a thread calls read() on a file not yet in memory. From syscall entry to user-space return, explain blocking, how the I/O request is queued, how the interrupt signals completion, how the waking thread is re-queued and scheduled, and how to avoid starving other ready tasks. Include minimal pseudo-code for the blocking path and the wakeup path?","answer":"read() blocks the thread, moves it to a blocked state, queues a disk-read request, and yields the CPU. The I/O subsystem issues the request; on completion, the IRQ marks the request done, wakes the bl","explanation":"## Why This Is Asked\nTests understanding of blocking I/O, interrupts, wakeups, and basic scheduler interaction in a simple OS.\n\n## Key Concepts\n- Blocking I/O lifecycle and task states\n- ISR vs thread wakeup and race avoidance\n- Run queues and fairness to prevent starvation\n- Memory ordering and visibility on wakeup\n\n## Code Example\n```javascript\n// Pseudo blocking path\nfunction do_read(fd, buf, len){\n  if (not_in_memory(fd)) {\n    req = new_io_request(fd, buf, len);\n    enqueue_io(req);\n    block_current_task();\n  }\n  return_read_to_user(buf, len);\n}\n```\n\n## Follow-up Questions\n- How would you test starvation between CPU-bound and I/O-bound threads?\n- What race conditions might occur between interrupt and scheduler, and how would you prevent them?","diagram":"flowchart TD\n  A[Syscall read()] --> B[Block on IO]\n  B --> C[Queue disk IO request]\n  C --> D[Disk IRQ triggers completion]\n  D --> E[Wake blocked task]\n  E --> F[Place on ready queue]\n  F --> G[Scheduler selects next task]\n  G --> H[Return data to user space]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Goldman Sachs","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T15:11:55.007Z","createdAt":"2026-01-21T15:11:55.007Z"},{"id":"q-530","question":"A process is stuck in 'D' state (uninterruptible sleep) during I/O operations. How would you debug this, what causes it, and how does it differ from 'Z' zombie state?","answer":"The 'D' state occurs during kernel-level I/O operations that cannot be interrupted, such as disk access or network operations. To identify stuck processes, use `ps aux | awk '$8 ~ /^D/ {print}'`. For debugging, `strace -p <pid>` shows the blocked system calls, while `lsof -p <pid>` reveals open file descriptors and identifies the specific I/O operation causing the block.","explanation":"## Debugging D State\n\n- Use `ps aux` to identify processes in D state\n- `strace -p <pid>` shows blocked system calls\n- `lsof -p <pid>` reveals open file descriptors\n- Check `/proc/<pid>/stack` for kernel stack trace\n\n## Common Causes\n\n- Faulty storage devices or network mounts\n- NFS/SMB server unresponsiveness\n- Hardware driver issues\n- Kernel bugs in I/O subsystem\n\n## D vs Z State\n\n- **D state**: Process alive, blocked in kernel, cannot be killed\n- **Z state**: Process terminated, resources freed, waiting for parent\n- D state consumes kernel resources, Z state only holds PID\n\n## Resolution\n\n- Address underlying hardware or network issues\n- Restart problematic services or mounts\n- Update drivers or kernel if needed\n- As last resort, reboot the system to clear D state processes","diagram":"flowchart TD\n  A[Process Running] --> B{I/O Request}\n  B -->|Kernel Space| C[D State - Uninterruptible Sleep]\n  C -->|I/O Complete| D[Return to Running]\n  C -->|Hardware/Network Issue| E[Stuck in D State]\n  F[Process Terminated] --> G[Parent Reads Exit Status]\n  G --> H[Z State - Zombie]\n  H -->|Parent Cleanup| I[PID Released]","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","LinkedIn","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":["uninterruptible sleep","strace","lsof","zombie state","kernel i/o","debugging"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:42:54.966Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-5335","question":"In a Linux-like kernel, implement a lazy, cross-process page-fault handler for a shared, file-backed region that several processes map with different NUMA affinities. Describe end-to-end fault flow, per-page state, how to avoid double fetch with CAS, how to choose source (IO, zero-fill, read-ahead), and how to guarantee fairness and minimal stalls under contention?","answer":"Describe a per-page in-flight flag and a CAS-based owner election. On fault, a thread atomically claims ownership, populates via file I/O or zero-fill, then updates page-table entries and notifies wai","explanation":"## Why This Is Asked\nTests understanding of synchronized lazy paging, cross-process visibility, and NUMA-aware fairness in a realistic kernel path.\n\n## Key Concepts\n- Per-page state machine (idle/in-flight/ready)\n- CAS-based owner election to avoid duplicative work\n- Notification via wait queues and memory barriers\n- Source selection: IO, zero-fill, read-ahead, with fairness under contention\n- NUMA affinity and cross-core contention\n\n## Code Example\n```c\n// sketch: per-page state and atomic owner\ntypedef struct {\n  atomic_int owner; // PID or thread id, 0 = none\n  atomic_int state; // 0 idle, 1 in-flight, 2 ready\n  // ... other fields\n} page_t;\n```\n\n## Follow-up Questions\n- How would you test for livelock under high contention?\n- How would you extend this to support prefetch hints and read-ahead when multiple faults occur?\n","diagram":"flowchart TD\n  Fault --> Claim[Claim ownership]\n  Claim --> Fetch[Populate page]\n  Fetch --> Publish[Publish to waiters]\n  Publish --> Done[User mapping receives page]","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Discord","Oracle","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T17:30:56.296Z","createdAt":"2026-01-21T17:30:56.296Z"},{"id":"q-5375","question":"In a tiny OS kernel, implement a preemptive Round-Robin scheduler with a fixed time quantum of 4 ticks. Describe the end-to-end flow from timer interrupt to context switch in this scheduler, including ready-queue management, tick handling, and a scheme to prevent starvation via aging?","answer":"Implement preemptive RR with quantum=4 ticks. On each timer tick: decrement running.remaining_quantum; if zero then enqueue the running process at tail and pop next from the ready queue; perform a con","explanation":"## Why This Is Asked\nTests understanding of timer interrupts, context switches, and simple schedulers at kernel level. It also probes how aging prevents starvation.\n\n## Key Concepts\n- Preemptive scheduling and round-robin\n- Timer interrupt handling and context-switch protocol\n- Per-process state and a ready-queue data structure\n- Aging to prevent starvation under load\n\n## Code Example\n```javascript\n// Pseudo-kernel RR tick\nfunction timerTick(running, ready) {\n  running.quantum--;\n  if (running.quantum <= 0) {\n    ready.enqueue(running);\n    const next = ready.dequeue();\n    contextSwitch(running, next);\n  }\n}\n```\n\n## Follow-up Questions\n- How to implement tickless mode with RR?\n- How would you test aging efficacy under bursty workloads?","diagram":"flowchart TD\n  A[Timer tick] --> B[Decrement quantum]\n  B --> C{Quantum exhausted?}\n  C -- Yes --> D[Enqueue running to ready]\n  C -- No --> E[Continue running]\n  D --> F[Dequeue next]\n  F --> G[Context switch to next]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["DoorDash","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T19:50:24.958Z","createdAt":"2026-01-21T19:50:24.958Z"},{"id":"q-5478","question":"In a Linux-like kernel, implement a NUMA-aware page-migration policy for a shared memory region used by multi-tenant workloads. When a thread touches a page residing on a distant node, decide whether to migrate based on a simple cost model comparing inter-node latency and bandwidth to the penalty of repeated remote accesses. Describe data structures, race-free flow, and how you test correctness and throughput under contention?","answer":"I would implement a NUMA-aware page migration policy with per-page affinity state tracking (owner_node, migrating flag, access_frequency) and a cost-based decision model. The policy compares the cumulative penalty of remote accesses against the migration overhead (inter-node latency + bandwidth cost). When migration is beneficial, we use atomic operations for race-free state transitions, implement batch migrations to prevent migration stampedes, and ensure coherence through TLB shootdowns.","explanation":"## Why This Is Asked\nTests practical, low-level understanding of NUMA-aware memory management, race-free migration, and TLB coherence under contention.\n\n## Key Concepts\n- NUMA-aware page migration\n- Per-page state and atomic synchronization\n- Cost-model estimation for migration vs remote access\n- Batch migration to prevent stampedes\n- TLB shootdowns and correctness guarantees\n\n## Code Example\n```c\n// Pseudo: per-page struct and migrate() path\ntypedef struct { int owner_node; int migrating; int refcount; } PageState;\nbool try_start_migration(PageState *p){\n  return __atomic_compare_exchange_n(&","diagram":"flowchart TD\n  A[Touch remote page] --> B{Cost eval}\n  B --> C[Decide migrate]\n  B --> D[Decide stay]\n  C --> E[Enqueue batch]\n  E --> F[Migration]\n  F --> G[TLB shootdown]","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Apple","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T05:33:52.099Z","createdAt":"2026-01-21T23:50:48.575Z"},{"id":"q-556","question":"How would you debug a process that's consuming 100% CPU but not responding to signals? What tools and steps would you use?","answer":"Start by identifying the process ID using `top` or `htop` to confirm the high CPU usage. Then attach `strace -p <PID>` to monitor system calls and determine if the process is stuck in user space or kernel mode. Check `/proc/<PID>/status` for the process state and examine `/proc/<PID>/stack` for kernel stack information. If the process remains unresponsive, use `gdb -p <PID>` to obtain stack traces and analyze the execution context.","explanation":"## Debugging Methodology\n\n1. **Process Identification**: Use `top`, `htop`, or `ps aux` to identify the PID of the CPU-intensive process\n2. **Process State Analysis**: Examine `/proc/<PID>/status` to understand current state (running, sleeping, zombie, etc.)\n3. **System Call Monitoring**: Deploy `strace -p <PID>` to trace system calls in real-time and identify blocking operations\n4. **Stack Trace Analysis**: Attach with `gdb -p <PID>` to inspect call stacks and identify infinite loops or deadlock conditions\n5. **Kernel-level Investigation**: Review `dmesg` output and `/proc/<PID>/stack` for kernel-space issues\n\n## Common Root Causes\n\n- Infinite loops in application code\n- Thread synchronization deadlocks\n- Blocking I/O operations without timeouts\n- Memory corruption leading to undefined behavior\n- Kernel bugs or driver issues\n\n## Resolution Strategies\n\n- Send `SIGKILL` as last resort if process remains unresponsive\n- Analyze application logs for error patterns preceding the issue\n- Implement proper error handling and timeout mechanisms\n- Address underlying code quality issues in the identified problem areas","diagram":"flowchart TD\n  A[High CPU Detected] --> B[top/htop - identify PID]\n  B --> C[Check /proc/<PID>/status]\n  C --> D[strace -p <PID>]\n  D --> E{System calls active?}\n  E -->|Yes| F[gdb -p <PID> - analyze stack]\n  E -->|No| G[Check for kernel deadlock]\n  F --> H[Identify root cause]\n  G --> H\n  H --> I[Apply fix/kill process]","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:56:11.138Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-5826","question":"Implement a minimal sleep/wakeup primitive in a tiny OS kernel: two threads share a boolean flag and a per-flag wait queue. Thread 1 waits for the flag to become true; Thread 2 sets the flag and wakes up all waiters. Describe the data structures and provide a C-like sketch for wait_on(flag, q) and wake_up(flag, q) that handle missed wakeups, spurious wakes, and contention?","answer":"Two threads share a boolean flag and a per-flag wait queue protected by a lock. wait_on locks, rechecks flag, and if false enqueues and blocks; wake_up sets flag and wakes all waiters, then unlocks. U","explanation":"## Why This Is Asked\nTests practical synchronization primitives in a kernel-like setting, not just theory.\n\n## Key Concepts\n- Wait queues, locks, memory ordering\n- Spurious wakeups and rechecking condition\n- Correct wakeup under contention\n\n## Code Example\n```c\ntypedef struct waiter { struct waiter *next; task_t *t; } waiter_t;\n\ntypedef struct wq { spinlock_t lock; waiter_t *head; bool *flag; } wq_t;\n\nvoid wait_on(wq_t *q, bool *flag){\n  acquire(&q->lock);\n  while (!*flag){\n    enqueue(q, current); // add to wait list\n    release(&q->lock);\n    schedule();\n    acquire(&q->lock);\n  }\n  release(&q->lock);\n}\n\nvoid wake_up(wq_t *q){\n  acquire(&q->lock);\n  *q->flag = true;\n  wake_all(q); // wake all waiters\n  release(&q->lock);\n}\n``` \n\n## Follow-up Questions\n- How would you extend to handle timeouts?\n- How to prevent missed wakeups when wake happens before wait is enqueued?\n- Compare to condition variables vs. spin-wait approaches.","diagram":"flowchart TD\n  A[Thread waits on flag] --> B[Enqueue in wait-queue]\n  B --> C[Thread blocks]\n  D[Thread sets flag & signals] --> E[Wakeup waiters]\n  E --> C","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Instacart","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T18:08:25.452Z","createdAt":"2026-01-22T18:08:25.452Z"},{"id":"q-585","question":"How would you implement a lock-free concurrent queue using atomic operations and memory barriers? What are the trade-offs between ABA problem solutions?","answer":"Implement using atomic head and tail pointers with compare-and-swap operations. Use hazard pointers or versioned references to solve the ABA problem. Trade-offs: hazard pointers have higher memory overhead but simpler implementation, while versioned references offer lower overhead with more complex compare-and-swap logic.","explanation":"## Lock-Free Queue Implementation\n\n- Use atomic head and tail pointers to manage queue boundaries\n- Enqueue: Atomically update tail pointer using CAS, then link new node\n- Dequeue: Atomically update head pointer using CAS, then read next node\n- Memory barriers: Use acquire semantics on loads and release semantics on stores\n\n## ABA Problem Solutions\n\n- **Hazard pointers**: Track protected nodes per thread to prevent premature reclamation\n- **Versioned references**: Combine pointer with monotonically increasing counter\n- **Epoch-based reclamation**: Batch deallocation operations by epoch boundaries\n\n## Trade-offs\n\n- Hazard pointers: Higher memory usage, simpler logic\n- Versioned references: Lower overhead, more complex CAS operations\n- Epoch-based reclamation: Better scalability, more complex implementation\n- Performance depends on contention levels and allocation patterns","diagram":"flowchart TD\n  A[Enqueue] --> B[CAS Tail Pointer]\n  B --> C[Link New Node]\n  D[Dequeue] --> E[CAS Head Pointer]\n  E --> F[Read Next Node]\n  G[ABA Problem] --> H[Hazard Pointers]\n  G --> I[Versioned References]\n  G --> J[Epoch Reclamation]","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","OpenAI","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:51:08.528Z","createdAt":"2025-12-27T01:14:13.089Z"},{"id":"q-5945","question":"In a minimal Linux-like OS, design a blocking block read API backed by a RAM cache. When a thread reads a non-cached 4KB block, it blocks until a background I/O thread loads it. Describe the end-to-end flow: how the request is represented, how the per-block cache entry is created (state machine with EMPTY, PENDING, LOADED, ERROR), how multiple threads contend without thrash, how the I/O thread signals completion, and how errors propagate to the caller?","answer":"Per-block cache entry with states: EMPTY, PENDING, LOADED, ERROR. Fast path reads LOADED; on cache miss, thread atomically transitions state to PENDING using compare-and-swap, enqueues a disk fetch request, and blocks on a futex. Background I/O thread processes the queue, fills the block with data, updates state to LOADED, and wakes all waiting threads via futex wake. Multiple contending threads avoid thundering herd by checking state after acquiring the futex - only the first thread performs the state transition to PENDING. Errors propagate by setting state to ERROR and waking waiters, who then return the error code to their callers.","explanation":"## Why This Is Asked\n\nTests understanding of user-space vs kernel-space concept interplay, synchronization primitives, and designing a lazy I/O cache with proper wakeups.\n\n## Key Concepts\n\n- Per-block state machine (EMPTY, PENDING, LOADED, ERROR)\n- Fast path vs slow path with contention handling\n- Wait queues (futex/condvar) and wakeup semantics\n- Avoiding thundering herd with CAS and double-checked locking\n- Error propagation from I/O to callers\n\n## Code Example\n\n```c\ntypedef struct {\n  void *data;\n  atomic_int state; // 0=EMPTY,1=PENDING,2=LOADED,3=ERROR\n  int waiters;\n  pthread_mutex_t mutex;\n  pthread_cond_t cond;\n} cache_entry;\n\n#define EMPTY 0\n#define PENDING 1\n#define LOADED 2\n#define ERROR 3\n\nvoid* block_read(int block_id) {\n  cache_entry *entry = &cache[block_id];\n  \n  // Fast path\n  if (atomic_load(&entry->state) == LOADED) {\n    return entry->data;\n  }\n  \n  pthread_mutex_lock(&entry->mutex);\n  \n  // Double-check after acquiring lock\n  if (atomic_load(&entry->state) == LOADED) {\n    pthread_mutex_unlock(&entry->mutex);\n    return entry->data;\n  }\n  \n  // Transition to PENDING if first thread\n  if (atomic_compare_exchange_strong(&entry->state, &EMPTY, PENDING)) {\n    enqueue_io_request(block_id);\n  }\n  \n  // Block until I/O completes\n  while (atomic_load(&entry->state) == PENDING) {\n    entry->waiters++;\n    pthread_cond_wait(&entry->cond, &entry->mutex);\n    entry->waiters--;\n  }\n  \n  pthread_mutex_unlock(&entry->mutex);\n  \n  if (atomic_load(&entry->state) == ERROR) {\n    return NULL; // Propagate error\n  }\n  \n  return entry->data;\n}\n```","diagram":"flowchart TD\n  A[Block Read Request] --> B{Cache Hit?}\n  B -- Yes --> C[Return Data]\n  B -- No --> D[Queue Disk Read & Block]\n  D --> E[I/O completes]\n  E --> F[Populate Cache] --> G[Wake Waiters] --> H[Return Data]\n  E --> I[On Error] --> J[Propagate Error to Callers]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Robinhood","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T04:20:56.688Z","createdAt":"2026-01-22T23:30:07.953Z"},{"id":"q-5997","question":"Design and implement a kernel-level page cache eviction strategy for a shared memory region mapped by multiple processes from a single file. Enforce per-process memory caps, enable adaptive read-ahead, and prevent starvation under contention. Describe end-to-end flow from first touch to eviction, including per-page state, synchronization, and test plans?","answer":"Propose per-page metadata including last_access, refcount, and owner_mask; impose per-process quotas; use an aging score combining recency and per-process fairness to pick eviction candidates. On faul","explanation":"## Why This Is Asked\nExplores practical OS page cache design in multi-process environments, emphasizing correctness, fairness, and performance under contention.\n\n## Key Concepts\n- Shared memory and page cache dynamics\n- Per-page metadata and per-process quotas\n- Synchronization primitives (CAS, memory barriers)\n- Adaptive read-ahead and starvation prevention\n\n## Code Example\n\n```c\ntypedef struct {\n  uint64_t last_access;\n  uint32_t refcount;\n  uint16_t owner_mask;\n  uint16_t flags;\n} PageMeta;\n```\n\n## Follow-up Questions\n- How would you extend for NUMA-aware eviction decisions?\n- How would you verify eviction correctness under heavy write contention?","diagram":"flowchart TD\n  A[First Touch] --> B[Page Fault]\n  B --> C[Load Page]\n  C --> D[Update Metadata]\n  D --> E[Return to Process]\n  E --> F[Eviction Decision]\n  F --> G[Page Evicted or Dirty]","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Instacart","Scale Ai","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T04:12:05.102Z","createdAt":"2026-01-23T04:12:05.102Z"},{"id":"q-6046","question":"Design a beginner-friendly, user-space page-fault simulator: an 8-page, file-backed region shared by two threads. On first touch of a page, allocate a frame, load data from a mock I/O or zero-fill holes, and perform a read-ahead for the next two pages. Ensure that concurrent faults for the same page do not race and that pages populate atomically. Describe the data structures and synchronization strategy you would use?","answer":"Use per-page state with atomic flags and a CAS path to guarantee a single fault. On fault, atomically claim invalid->busy, allocate a frame, load data or zero-fill, then mark valid and release. Trigge","explanation":"## Why This Is Asked\nTests understanding of page fault flow, race conditions, and practical synchronization in a simplified setting.\n\n## Key Concepts\n- Per-page state and atomic flags\n- Compare-and-swap to guarantee single fault\n- Read-ahead and lazy loading in user space\n- Testing concurrent fault scenarios\n\n## Code Example\n```c\n#include <stdatomic.h>\ntypedef struct {\n  atomic_bool valid;\n  atomic_bool busy;\n  int frame;\n} Page;\n\nvoid fault(Page *p, int page_num) {\n  bool expected = false;\n  if (atomic_compare_exchange_strong(&p->valid, &expected, true)) {\n    p->frame = allocate_frame();\n    if (load_or_zero(p->frame, page_num)) {\n      // success\n    } else {\n      // handle error\n    }\n  } else {\n    // wait for valid\n  }\n}\n```\n\n## Follow-up Questions\n- How would you scale this to larger regions or NUMA-aware placement?\n- How would you test race conditions reliably?","diagram":"flowchart TD\n  A[Touch Page] --> B{Valid?}\n  B -- Yes --> C[Return]\n  B -- No --> D[Fault_Handler]\n  D --> E[Acquire Per-Page Lock]\n  E --> F[Allocate Frame]\n  F --> G[Load Data or Zero-Fill]\n  G --> H[Prefetch Next 2 Pages]\n  H --> I[Mark Valid]\n  I --> J[Return]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Lyft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T06:51:51.003Z","createdAt":"2026-01-23T06:51:51.003Z"},{"id":"q-6184","question":"Design and implement a per-cgroup aware page cache eviction policy for a shared file-backed cache in a Linux-like kernel. Each cgroup has a memory cap for cached pages; the system must still support read-ahead and multi-thread contention, and must avoid starvation of small cgroups. Describe end-to-end workflow from a cache miss on a process within a cgroup to victim selection and eviction, including per-page state, synchronization, how to account charges, and testing strategy?","answer":"On a miss, allocate a page, attach it to the requesting cgroup, and charge that cgroup. Track per-page flags (ref, dirty, exclusive) and per-cgroup watermarks. Eviction uses a hierarchical LRU with pe","explanation":"## Why This Is Asked\nTests ability to design memory accounting and eviction under multi-tenant workloads, a common production pressure point. It evaluates correctness, performance trade-offs, and how to prevent starvation while honoring isolation guarantees.\n\n## Key Concepts\n- Per-cgroup memory accounting and quotas\n- Shared file-backed page cache eviction\n- Per-page state and synchronization\n- Read-ahead interactions and fairness under contention\n- Testing under memory pressure and mixed workloads\n\n## Code Example\n```javascript\n// Pseudo sketch: decide_eviction_target(cgroup, cache)\nfunction decide_eviction_target(cgroup, cache){\n  // choose victim from cgroup's pages with lowest refcount and clean state\n  let candidates = cache.pages.filter(p => p.owner === cgroup && p.refcount <= 1 && !p.dirty);\n  return candidates.sort((a,b)=> a.lastUsed - b.lastUsed)[0];\n}\n```\n\n## Follow-up Questions\n- How would you handle page-dirty handling and write-back races during eviction?\n- How would you extend the policy to prioritize hot pages across multiple cgroups while enforcing caps?","diagram":"flowchart TD\n  A[Cache Miss] --> B[Allocate Page Frame]\n  B --> C[Attach to Requesting CGroup]\n  C --> D[Charge CGroup]\n  D --> E[Insert into Cache Lists]\n  E --> F{Under Cap?}\n  F -->|Yes| G[Return Page]\n  F -->|No| H[Select Victim]\n  H --> I[Handle Dirty State]\n  I --> J[Evict and Refill] \n  J --> G","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Instacart","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T12:06:08.500Z","createdAt":"2026-01-23T12:06:08.500Z"},{"id":"q-6216","question":"Design a per-cgroup memory-aware page reclaim policy in a Linux-like kernel. Given two services on the same host—A is memory-hungry and latency-sensitive, B is cache-heavy—describe the end-to-end path for reclaim: how per-cgroup accounting influences decisions; how kswapd and direct reclaim interact; how to pick candidates (anon vs file-backed); how to avoid starvation and thrash; and how to guarantee fairness and latency under contention?","answer":"Leverage memory.cgroups to track per-page charges and extend the per-cgroup LRU with separate anon and file-backed lists. On reclaim, kswapd runs per‑cgroup shrinkers, prioritizing anon pages from the","explanation":"## Why This Is Asked\nThis question probes practical OS skills: per-cgroup accounting, page reclaim flow, and the interaction between kswapd and direct reclaim under contention.\n\n## Key Concepts\n- memory.cgroups per-page accounting\n- per-cgroup LRU separation for anon vs file-backed\n- reclaim coordination between kswapd and direct reclaim\n- starvation avoidance and fairness across cgroups\n- measuring latency and throughput under load\n\n## Code Example\n```c\n// Pseudo data-structures for per-cgroup page lists\nstruct memcg_page {\n  struct list_head anon_list;\n  struct list_head file_list;\n  unsigned long charge;\n};\n```\n\n## Follow-up Questions\n- How would you test per-cgroup reclaim fairness on a mixed workload?\n- What metrics would you expose to operators to detect thrash vs healthy reclamation?","diagram":"flowchart TD\n  A[Memory pressure] --> B[memcg accounting]\n  B --> C{Candidate type}\n  C --> D[Anon pages]\n  C --> E[File-backed pages]\n  D --> F[Reclaim/evict]\n  E --> F\n  F --> G[Update per-cgroup stats] ","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Bloomberg","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T14:49:41.635Z","createdAt":"2026-01-23T14:49:41.635Z"},{"id":"q-6230","question":"In a Linux-like kernel, implement lazy splitting of hugepages for a memory-mapped, file-backed region: when a first write touches a region originally backed by a 2MB hugepage, the kernel should split it into 4KiB pages and preserve correctness across multiple processes mapping the same region. Describe the end-to-end flow from the write fault to the completion of splitting, including how to coordinate with per-page tables, TLB invalidation, and writeback semantics for sharing processes, while keeping latency low under high concurrency?","answer":"On first write fault to a hugepage, serialize with per-region lock; split the 2MB page into 4KiB frames by materializing new PTEs and updating the page-table tree. If shared, perform Copy-On-Write and","explanation":"## Why This Is Asked\nTests practicality of handling on-demand page fragmentation, cross-process consistency, and low-latency fault handling. It touches page-table walking, synchronization, TLB invalidation, and cache coherence under contention.\n\n## Key Concepts\n- Hugepage fragmentation and 4KiB granularity\n- Copy-On-Write semantics for shared mappings\n- CAS-based page-table updates and memory barriers\n- TLB shootdown and cross-CPU visibility\n- Interaction with the file-backed page cache\n\n## Code Example\n```c\n// Pseudo: split_hugepage(...) { /* acquire region lock, allocate 4KiB pages, update PTEs, invalidateTLBs */ }\n```\n\n## Follow-up Questions\n- How would you extend to mixed page sizes or partial splits?\n- How would you test correctness under high contention and NUMA configurations?","diagram":"flowchart TD\n  A[First touch on hugepage] --> B[Split into 4KiB pages]\n  B --> C[Update per-page table using CAS]\n  C --> D[TLB shootdown across CPUs]\n  D --> E[New mappings visible to all processes]","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Discord","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T15:40:53.470Z","createdAt":"2026-01-23T15:40:53.470Z"},{"id":"q-6406","question":"You're adding a transparent compressed, demand-paged anonymous memory cache. On a touch fault for a 4KB page not resident and without a backing file, describe the end-to-end flow: allocate a page, decide fill source (zero-fill, decompress from a shared compressed cache, or swap-in), update per-page state, handle concurrent faults across CPUs to avoid duplicate work, and ensure fairness under memory pressure. Include TLB handling and prefetch hints as needed?","answer":"On a touch fault for a 4KB page not resident and without a backing file, the handler first checks page presence and performs an atomic test-and-set operation to reserve the page. A 4KB page is allocated from the buddy allocator and marked in-flight. The system determines the fill source: if a compressed cache entry exists, decompress into the allocated page; otherwise, zero-fill for clean pages, decompress from the shared compressed cache if available, or swap-in from secondary storage based on system state. Per-page state is updated to reflect the new contents and the in-flight marker is cleared. Concurrent faults across CPUs are handled using compare-and-swap operations to prevent duplicate work, with exponential backoff and retry mechanisms for contention scenarios. Fairness under memory pressure is maintained through admission control and LRU-based eviction policies for the compressed cache. TLB entries are updated after successful page allocation, and stale entries are invalidated across CPUs using shootdown IPIs. Prefetch hints are applied for sequential access patterns, and NUMA-aware placement is considered for optimal memory locality.","explanation":"## Why This Is Asked\n\nTests reasoning about a modern OS feature (compressed memory) integrated with the fault path, under contention and memory pressure.\n\n## Key Concepts\n\n- Demand paging, per-page state, and a shared compressed cache\n- Concurrency control for faults across CPUs (CAS, in-flight markers)\n- Cache coherence, TLB shootdowns, and NUMA-aware placement\n\n## Code Example\n\n```c\n// Sketch of fault handler (not complete)\nint handle_fault(page_t *p, vm_area_t *v) {\n  if (p->present) return 0;\n  if (!atomic_try_lock(&p->lock)) return retry;\n  if (p->present) { unlock(p); return 0; }\n  \n  //\n```","diagram":"flowchart TD\n  A[Page fault] --> B[Check per-page state]\n  B --> C{Present?}\n  C -- Yes --> D[Update access bits and return]\n  C -- No --> E[Allocate page]\n  E --> F[Fill source: zero, decompress, or swap-in]\n  F --> G[Fill and update PTE]\n  G --> H[TLB shootdown]\n  H --> I[Wake waiters and resume]\n  I --> J[Return to faulting process]","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","Plaid","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T05:12:28.124Z","createdAt":"2026-01-23T22:38:05.335Z"},{"id":"q-6524","question":"Design a simple 3-level MLFQ scheduler for a toy OS. Levels have quantum 4, 8, and 16 ticks; aging promotes waiting tasks to higher levels after a threshold to prevent starvation. Given two processes arriving together—P1 CPU-bound, P2 I/O-bound—describe their scheduling order, how aging affects priorities, and provide a minimal C-like sketch for enqueue, dequeue, and choosing the next task. How would you test fairness?","answer":"Three ready queues Q0–Q2 with quanta 4, 8, 16. Aging promotes waiting tasks by one level after A ticks to prevent starvation. On quantum expiry, move the running task to the next lower level; on IO co","explanation":"## Why This Is Asked\nTests understanding of practical scheduling design, aging to avoid starvation, and handling preemption in a simple OS.\n\n## Key Concepts\n- Multi-level feedback queue structure\n- Quantum slicing and preemption\n- Aging to prevent starvation\n- Correctness under concurrent events and IO wakeups\n\n## Code Example\n```javascript\n// Minimal MLFQ skeleton\nconst QUANTUM = [4,8,16];\nlet queues = [[], [], []];\nfunction enqueue(t){ queues[t.level].push(t); }\nfunction dequeue(){ for(let i=0;i<3;i++) if(queues[i].length) return queues[i].shift(); return null; }\nfunction pick(){ let t = dequeue(); if(!t) return null; t.remaining = QUANTUM[t.level]; return t; }\n```\n\n## Follow-up Questions\n- How would you test aging effectiveness and starvation resistance?\n- What edge cases (burstiness, bursty IO) would you simulate to validate correctness?","diagram":"flowchart TD\nA[Arrival] --> B[Enqueue in Q0]\nB --> C{Q0 non-empty?}\nC -->|yes| D[Run Q0]\nC -->|no| E{Q1 non-empty?}\nE -->|yes| F[Run Q1]\nE -->|no| G{Q2 non-empty?}\nF --> H[On quantum expiry: demote]\nH --> I[Requeue]\nI --> J[Next scheduling decision]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Cloudflare","LinkedIn","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T07:02:34.263Z","createdAt":"2026-01-24T07:02:34.263Z"},{"id":"q-6645","question":"Implement a beginner-friendly kernel concept: a character device that uses a fixed-size ring buffer (16 bytes) for data produced by an ISR and consumed by a user-space process via read(). Explain how to safely write from ISR to the buffer and read from user context, including wraparound, blocking vs non-blocking semantics, and how to avoid data races. Provide a minimal C-like sketch of the ring buffer operations and the read() path?","answer":"Use a 16-byte circular buffer with head and tail indices. In the ISR, compute next=(head+1)&15; if next==tail, overflow; else buf[head]=byte; head=next; memory barriers; wake reader. In read(), block ","explanation":"## Why This Is Asked\n\nTests understanding of IRQ-safe producer-consumer in a tiny kernel.\n\n## Key Concepts\n\n- IRQ-safe synchronization\n- Ring buffer wraparound\n- Blocking vs non-blocking reads\n- Memory barriers and visibility\n\n## Code Example\n\n```javascript\ntypedef struct {\n  uint8_t buf[16];\n  volatile uint8_t head;\n  volatile uint8_t tail;\n} ring_t;\n\nvoid isr_write(ring_t *r, uint8_t v){\n  uint8_t next = (r->head + 1) & 15;\n  if (next == r->tail) return; // overflow\n  r->buf[r->head] = v;\n  __sync_synchronize();\n  r->head = next;\n  // wake reader if blocked\n}\n\nint read_user(ring_t *r, uint8_t *out){\n  while (r->head == r->tail) {\n    // block or yield\n  }\n  *out = r->buf[r->tail];\n  r->tail = (r->tail + 1) & 15;\n  return 1;\n}\n```\n\n## Follow-up Questions\n\n- How would you test a bursty ISR and blocked reader?\n- How would you generalize to multiple producers/consumers?","diagram":"flowchart TD\n  ISR[ISR] --> C1{Space?}\n  C1 -- Yes --> C2[Write data to buf]\n  C2 --> C3[Update head]\n  C3 --> C4[Wake reader]\n  Reader[Reader] --> C5{Has data?}\n  C5 -- Yes --> C6[Read data from tail]\n  C6 --> C7[Advance tail]\n  C5 -- No --> C8[Sleep/Block]\n  C8 --> ISR","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Coinbase","Hugging Face","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T11:32:23.200Z","createdAt":"2026-01-24T11:32:23.200Z"},{"id":"q-6730","question":"You are implementing a NUMA-aware memory allocator in a Linux-like kernel. When a thread touches a page on a remote NUMA node, migrate the page to the thread's local node if beneficial, subject to per-node caps and a migration budget per interval. Describe the data structures, the decay-based heat counter, the synchronization to avoid races, the safe TLB shootdown, and a minimal test plan to measure latency under contention?","answer":"On a touch of a page on a non-local node, update a per-page heat counter and, if heat exceeds a threshold and the local node cap permits, enqueue a migration to the thread’s local NUMA node. Use an at","explanation":"## Why This Is Asked\n\nThis question probes practical NUMA-aware memory management, race-free migration, and measurement under contention.\n\n## Key Concepts\n\n- NUMA locality\n- Page migration and pte updates\n- Atomic synchronization and CAS\n- TLB shootdown and cache coherence\n- Contention control and testing\n\n## Code Example\n\n```c\ntypedef struct {\n  int node;\n  atomic_int heat;\n  atomic_bool migrating;\n} page_meta_t;\n\nvoid try_migrate(page_t *p, int new_node) {\n  if (atomic_load(&p->migrating)) return;\n  if (atomic_exchange(&p->migrating, true)) return;\n  // update pte, migrate data to new_node\n  atomic_store(&p->node, new_node);\n  // flush/invalidate as needed\n  atomic_store(&p->migrating, false);\n}\n```\n\n## Follow-up Questions\n\n- How would you scale migrations across thousands of pages without starving local allocations?\n- How would you instrument and validate latency vs. throughput under synthetic CPU/memory-bound workloads?","diagram":"flowchart TD\n  A[Page touched on remote NUMA node] --> B[Increment per-page heat counter]\n  B --> C{Heat high & local cap available}\n  C -->|Yes| D[Enqueue page migration to local node]\n  C -->|No| E[Defer]\n  D --> F[Update PTE & TLB shootdown]\n  F --> G[Migration complete]\n  E --> H[Wait for next access]","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Netflix","Twitter","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T14:50:04.021Z","createdAt":"2026-01-24T14:50:04.021Z"},{"id":"q-6797","question":"Implement a minimal Copy-On-Write fork in a toy OS. After fork, parent and child share pages; mark them read-only. On a write fault, allocate a new page, copy data, remap writable, and update page tables atomically. Explain how to prevent races if both touch the same page before handling the fault, how to flush TLBs, and provide a compact C-like fault handler sketch?","answer":"A candidate would describe: set both PTEs to read-only with a refcount; on write fault, allocate a new page, copy data, update both mappings to point to the private page, atomically clear RO flag, and","explanation":"## Why This Is Asked\nTests memory-management basics: forking, page sharing, and Copy-On-Write semantics, plus race-free fault handling and TLB coherence.\n\n## Key Concepts\n- Copy-On-Write (COW)\n- Page tables and PTE flags (RO, Writable, Present)\n- Atomic update to mappings and per-page refcounts\n- TLB flushing and race avoidance on faults\n- Testing with concurrent writes and rapid forking\n\n## Code Example\n```c\n// Pseudo-skeleton for a COW fault handler\nvoid handle_cow_fault(Process *p, void *fault_addr) {\n  Page *shared = get_page(p, fault_addr);\n  if (!shared->cow) return; // unexpected\n  Page *private = alloc_page();\n  copy_page(private, shared->addr);\n  // atomically remap fault_addr to private with W permissions\n  update_pte(p, fault_addr, private, WRITE);\n  decrement_ref(shared);\n  flush_tlb(p, fault_addr);\n}\n```\n\n## Follow-up Questions\n- How would you test for race conditions between two forks touching the same page?\n- How would you extend to multi-threaded processes and NUMA-aware allocations?","diagram":"flowchart TD\n  A[Fork] --> B[Pages shared read-only]\n  B --> C{Write fault at page}\n  C --> D[Allocate private page]\n  D --> E[Copy data]\n  E --> F[Remap writable; update PTEs]\n  F --> G[TLB flush]\n  G --> H[Execution resumes]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["DoorDash","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T17:48:30.518Z","createdAt":"2026-01-24T17:48:30.518Z"},{"id":"q-7088","question":"In a toy OS kernel, two threads contend for a single device using a FIFO ticket lock paired with a per-thread wait queue. Describe end-to-end how acquire and release work, how a thread blocks, and how wakeups wake the next waiter. Provide a minimal C-like sketch for the data structures and the acquire/release paths, including how to avoid deadlock and minimize spinning under contention. Include how you would test for fairness?","answer":"Use a FIFO ticket lock with a wait queue. Acquire: ticket = fetch_add(&lk->next, 1); spin until nowServing == ticket; Enter critical section; Release: nowServing++ and wake the next waiter via the que","explanation":"Why This Is Asked: tests understanding of synchronization primitives, fairness, and memory ordering. Key Concepts: ticket locks, wake queues, avoiding starvation, spurious wakeups, memory barriers. Code Example: see below. Follow-up: discuss adaptation to multicore/NUMA, impact of preemption, and testing strategies.\n\nCode Example:\n```c\n#include <stdatomic.h>\n\ntypedef struct {\n  atomic_uint next;\n  atomic_uint nowServing;\n} ticket_lock_t;\n\nvoid ticket_lock_init(ticket_lock_t *lk){\n  atomic_store(&lk->next, 0);\n  atomic_store(&lk->nowServing, 0);\n}\n\nunsigned ticket_lock_acquire(ticket_lock_t *lk){\n  unsigned ticket = atomic_fetch_add_explicit(&lk->next, 1, memory_order_relaxed);\n  while (atomic_load_explicit(&lk->nowServing, memory_order_acquire) != ticket) {\n    _mm_pause();\n  }\n  return ticket;\n}\n\nvoid ticket_lock_release(ticket_lock_t *lk){\n  atomic_fetch_add_explicit(&lk->nowServing, 1, memory_order_release);\n}\n```\n\nFollow-up Questions:\n- How would you test fairness under heavy contention?\n- How would you extend to multiple locks and prevent deadlock?","diagram":null,"difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Adobe","Amazon","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T08:50:41.266Z","createdAt":"2026-01-25T08:50:41.266Z"},{"id":"q-7136","question":"In a toy OS with a two-level page table and copy-on-write, implement a minimal page-fault handler that: (a) loads a non-present page from a backing file on demand, or (b) allocates a zero-filled page. Describe end-to-end flow from fault to mapping, how to synchronize concurrent faults for the same page (per-page lock or CAS), and how to add simple read-ahead under contention?","answer":"On a non-present page fault, allocate a frame and lock the page. Re-check presence; if still missing and backed by a file, read into the frame and map it; otherwise zero-fill and map. Use a per-page C","explanation":"## Why This Is Asked\nTests understanding of demand paging, synchronization, and simple I/O integration in a minimal kernel.\n\n## Key Concepts\n- Page tables and TLB\n- Copy-on-write semantics\n- Per-page locking and CAS to avoid duplicate fetches\n- Read-ahead and fairness under contention\n\n## Code Example\n```c\n// Pseudo: handle_page_fault(vaddr, proc)\nFrame f = alloc_frame();\nif (!lock_try_acquire(pte)) return fault_again();\nif (pte_present(pte)) { unlock(); return; }\nif (pte_backing_file(pte)) {\n  io_read_async(pte_file(pte), f->addr, PAGE_SIZE, on_complete); \n} else {\n  zero_fill(f->addr, PAGE_SIZE);\n  map_page(pte, f->addr);\n  unlock();\n  wake_up_faulting_thread();\n}\n```\n\n## Follow-up Questions\n- How would you implement a simple read-ahead heuristic?\n- How would you test race-free behavior for concurrent faults?","diagram":"flowchart TD\n  A[Page fault] --> B{Present?}\n  B -- No --> C[Lock page]\n  C --> D{Backing file?}\n  D -- Yes --> E[Read from file into frame]\n  D -- No --> F[Zero-fill frame]\n  E --> G[Map page and invalidate TLB]\n  F --> G\n  G --> H[Wake faulting task]\n  H --> I[Optional read-ahead]\n  I --> A","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Meta","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T10:40:32.895Z","createdAt":"2026-01-25T10:40:32.895Z"},{"id":"q-7272","question":"Design and implement a high-resolution timer system using a two-level hierarchical timer wheel in a Linux-like kernel. Explain the end-to-end path from timer creation to task wakeup, including per-CPU queues, tick processing, bucket expiration, and cancellation races, plus a realistic workload scenario such as cache eviction timeouts in a data processing service. How do you bound drift and test correctness under heavy timer density?","answer":"Two-level per-CPU timer wheel: near is 0–1 ms with 64 slots, far is 1–32 ms with 64 slots. Timer struct has deadline, callback, and waiter. Creation inserts into current CPU’s near wheel; on tick, adv","explanation":"## Why This Is Asked\n\nTests end-to-end timer path, per-CPU data, and race-free cancellation in a high-throughput kernel context.\n\n## Key Concepts\n\n- High-resolution timers\n- Per-CPU data structures\n- Time wheel algorithms\n- Lock-free cancellation\n- Wakeup paths (futex/RFutex)\n\n## Code Example\n\n```c\n// Pseudo structures (illustrative only)\ntypedef struct Timer {\n  unsigned long long deadline;\n  void (*cb)(void*);\n  void *arg;\n  struct Timer *next;\n} Timer;\n```\n\n## Follow-up Questions\n\n- How would you extend to cross-core timers?\n- How would you validate drift under dense timer loads?","diagram":"flowchart TD\n  A[TIMER CREATED] --> B[Insert into near wheel]\n  B --> C[TICK OCCURS]\n  C --> D[Advance near wheel]\n  D --> E[Expire & wakeup]\n  E --> F[Potential cancel path]\n  F --> G[Done]","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Databricks","Oracle","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T16:37:54.564Z","createdAt":"2026-01-25T16:37:54.564Z"},{"id":"q-7359","question":"Design a per-thread I/O throttling mechanism for a Linux-like block scheduler: implement a per-task token bucket that caps I/O bandwidth (MB/s) for each task and integrate with the IO elevator so requests respect budgets even under bursts. Describe how you track in-flight IO, refill logic, and how you test fairness between CPU-bound and IO-bound tasks with bursty I/O?","answer":"Per-thread token bucket: each task has tokens and a budget B; tokens refill at R MB/s. On IO submit, if tokens < size, stall or queue; on completion, refund. Attach tokens to the submitting task in th","explanation":"## Why This Is Asked\n\nTests practical experience adding fine-grained I/O controls, integrating budgeted QoS with a block scheduler, and validating fairness under bursts—relevant to large-scale platforms with mixed workloads.\n\n## Key Concepts\n\n- Token bucket budgeting for per-task I/O\n- Integration with an IO elevator (CFQ-like) and per-task accounting\n- In-flight IO tracking and refill mechanics\n- Fairness under bursty workloads and starvation avoidance\n\n## Code Example\n\n```javascript\n// Minimal, language-agnostic sketch (C-like semantics)\nint throttle_check(struct task_struct *t, size_t bytes){\n  if (t->tokens < bytes) return -1; // need to wait\n  t->tokens -= bytes;\n  return 0;\n}\n```\n\n## Follow-up Questions\n\n- How would you handle asynchronous IO completions and token refunds?\n- How would you adapt budgets when processes migrate across CPUs with different NIC or storage paths?","diagram":"flowchart TD\n  A[IO Submit] --> B{Has Tokens?}\n  B -- Yes --> C[Submit IO and Deduct Tokens]\n  B -- No --> D[Queue or Sleep]\n  C --> E[IO Completion] --> F[Refund Tokens]\n  D --> G[Token Refill Timer]\n  F --> H[Scheduler Reassess]\n  G --> B","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Goldman Sachs","Plaid","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T19:48:14.555Z","createdAt":"2026-01-25T19:48:14.555Z"},{"id":"q-7424","question":"Design a thread-safe PID allocator for a toy OS with 16-bit PIDs. Ensure uniqueness across threads, safe reuse after a thread exits, and no races that allow stale PIDs to collide. Provide a data structure and a minimal C-like sketch for alloc_pid() and release_pid(pid)?","answer":"Use a global 16-bit PID bitmap with a generation counter per PID and a small lock. On alloc, scan for a free slot, mark in_use, bump generation, and return pid = (gen << 16) | index. On release, clear","explanation":"## Why This Is Asked\n\nTests understanding of race conditions, safe resource reuse, and lightweight OS primitives in a beginner-friendly context.\n\n## Key Concepts\n\n- Bitmaps for ID allocation\n- Generation counters to distinguish reuse\n- Simple locking strategy to avoid races\n- Handling exhaustion and graceful error paths\n\n## Code Example\n\n```c\ntypedef struct {\n  uint16_t id;      // 0..65535\n  uint16_t gen;     // generation counter\n  bool in_use;\n} pid_slot;\n\n// Global state (simplified)\npid_slot table[65536];\npthread_mutex_t pid_lock;\n\nint alloc_pid(void) {\n  pthread_mutex_lock(&pid_lock);\n  for (uint32_t i = 1; i < 65536; i++) {\n    if (!table[i].in_use) {\n      table[i].in_use = true;\n      table[i].gen++;\n      int pid = (table[i].gen << 16) | i;\n      pthread_mutex_unlock(&pid_lock);\n      return pid;\n    }\n  }\n  pthread_mutex_unlock(&pid_lock);\n  return -1; // exhausted\n}\n\nvoid release_pid(uint32_t pid) {\n  uint16_t idx = pid & 0xFFFF;\n  pthread_mutex_lock(&pid_lock);\n  table[idx].in_use = false;\n  pthread_mutex_unlock(&pid_lock);\n}\n```\n\n## Follow-up Questions\n\n- How would you implement a non-blocking alloc_pid() without a global lock?\n- How to handle PID namespace isolation across containers?","diagram":"flowchart TD\n  A[Request PID] --> B[Scan bitmap for free entry]\n  B --> C{Found?}\n  C -- Yes --> D[Mark in_use, bump gen, return PID]\n  C -- No --> E[Error: out of PIDs]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["IBM","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T22:39:29.746Z","createdAt":"2026-01-25T22:39:29.746Z"},{"id":"q-7498","question":"Design and implement a scalable, NUMA-aware timer subsystem in a Linux-like kernel that supports millions of timers per process, with per-core buckets and a global wheel for long timers. Describe data structures, cancellation, and how to maintain sub‑millisecond accuracy under load, including contention handling, cross-socket effects, and interactions with tickless CPUs. Provide a minimal C-like sketch for timer insert and tick processing?","answer":"Use a hierarchical timer wheel: per-core buckets for fast timers, a global wheel for long timers, and a central clock source. Each timer holds expires, cb, and arg; insertion uses a core-local hash wi","explanation":"## Why This Is Asked\nTests designing scalable, low-latency timing primitives under heavy load and NUMA constraints; checks concurrency, memory locality, and wakeup efficiency for high-performance services.\n\n## Key Concepts\n- Timer data structures: hierarchical timer wheel with per-core buckets and a global wheel\n- NUMA-aware placement and cross-socket wakeups\n- Concurrency: fine-grained locking or lock-free structures, generation IDs, and cancel semantics\n- Tickless vs periodic: clock sources and tick alignment\n- Testing: microbenchmarks, drift under load, and cross-core wakeups\n\n## Code Example\n```javascript\n// Minimal sketch for timer insert and tick\nclass Timer {\n  constructor(expires, cb, arg) {\n    this.expires = expires;\n    this.cb = cb;\n    this.arg = arg;\n  }\n}\nfunction timerInsert(t) { /* placeholder for insertion into per-core bucket */ }\nfunction timerTick(now) { /* advance wheel and run due callbacks */ }\n```\n\n## Follow-up Questions\n- How would you validate timer accuracy and jitter on a multi-socket system with tickless CPUs?\n- What changes are needed to support dynamic timer ranges without ballooning memory or increasing wakeup overhead?","diagram":null,"difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Coinbase","Lyft","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T04:32:29.764Z","createdAt":"2026-01-26T04:32:29.764Z"},{"id":"q-7623","question":"Design a kernel path for a 10G NIC with multi-queue support to run multiple tenants. Describe a per-tenant, per-queue transmit design that preserves per-flow in-order delivery, provides isolation under bursty traffic, and avoids head-of-line blocking. Include flow-to-queue mapping, backpressure, and dynamic queue rebalancing strategies, plus a minimal testing harness?","answer":"Design a per-tenant, per-queue path: map flows to fixed TX queues with a hash; preserve per-flow order using monotonic sequence numbers and per-flow credits; enforce tenant rate limits via a token-buc","explanation":"## Why This Is Asked\n\nTests knowledge of high-performance IO paths, isolation between tenants, and fairness under contention.\n\n## Key Concepts\n\n- NIC multi-queue and per-queue pacing\n- Flow-to-queue mapping and in-order guarantees\n- Per-tenant rate limiting and backpressure\n- Dynamic queue management and reordering risks\n\n## Code Example\n\n```javascript\n// Pseudo-sketch: per-tenant enqueue with token check\nlet canSend = tenant.updateTokens(tokensNeeded);\nif (!canSend) return backpressure();\nenqueueToQueue(flowId, tenantId, pkt);\n```\n\n## Follow-up Questions\n\n- How would you verify tail-latency under bursty traffic?\n- What happens if a tenant floods a single queue?","diagram":"flowchart TD\n  A[Arrive Flow] --> B[Hash to TX Queue]\n  B --> C[Enqueue]\n  C --> D[NIC]\n  D --> E[Completion/Credit]","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T10:06:26.124Z","createdAt":"2026-01-26T10:06:26.124Z"},{"id":"q-7653","question":"Design a kernel-level per-tenant cache partitioning mechanism for a multi-tenant data platform. Propose a page-coloring-like policy integrated with the Linux page cache and TLB management. How would you implement a lightweight per-tenant allocator, eviction under contention, and ensure per-tenant tail latency bounds while preserving overall hit rate? Include minimal C-like pseudocode for allocation and eviction decisions?","answer":"Per-tenant color tags guard a fixed quota; use atomic counters for usage and an isolation-aware eviction heuristic. On fault, assign the page to tenant’s bucket; eviction picks a victim from the tenan","explanation":"## Why This Is Asked\nTests understanding of memory subsystems, cross-tenant isolation, and real-time constraints at OS level.\n\n## Key Concepts\n- Page cache, page coloring, TLB maintenance\n- Per-tenant quotas and atomic counters\n- Lock-free vs. locking in eviction, per-tenant LRU\n- Tail latency controls and read-ahead throttling\n\n## Code Example\n```javascript\n// Pseudo: allocation with per-tenant quota\nfunction allocate_page(tenant_id){\n  if (atomic_fetch_add(tenant.used, 1) > tenant.quota){\n    atomic_fetch_sub(tenant.used, 1);\n    return FALLBACK;\n  }\n  const page = select_victim(tenant_id);\n  map_page_to_tenant(page, tenant_id);\n  return page;\n}\n```\n\n## Follow-up Questions\n- How would you validate isolation with synthetic workloads?\n- What data structures would you use to minimize cross-tenant contention and TLB flushes?","diagram":null,"difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Coinbase","Snowflake","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T11:30:08.486Z","createdAt":"2026-01-26T11:30:08.486Z"},{"id":"q-7749","question":"Describe a tiny kernel-safe per-core credit-based IO-rate limiter: each tick, threads earn one credit; CPU work consumes 1 credit per tick; I/O operations consume 2 credits. If a thread's credits would go negative, it yields and blocks on a per-core wait queue until the next tick. Provide data structures, race-free update rules, and a small C-like sketch for grant_credit, spend_credit, and tick_handler. How would you test fairness under mixed CPU/I/O workloads?","answer":"Per-thread atomic credits with per-core run queues. On each tick, increment credits up to a cap; CPU work spends 1 credit; I/O spends 2 credits and blocks if balance would go negative. A tick handler ","explanation":"## Why This Is Asked\nTests practical scheduling control, race-free updates, and wakeup semantics; probes fairness under contention.\n\n## Key Concepts\n- Per-thread atomic credit accounting\n- Per-core run queues and locality\n- Blocking/wakeups with per-core wait queues\n- Cost accounting and fairness guarantees\n\n## Code Example\n```javascript\n// Simplified sketch (C-like, using JS-style syntax for clarity)\ntypedef struct { atomic_int credits; int core; waitqueue_t wq; } thread_t;\n\nvoid on_tick(int core){\n  for (each t in core_list[core]){\n    int c = atomic_load(&t->credits);\n    if (c < MAX) atomic_fetch_add(&t->credits, 1);\n  }\n  wake_one_from_wq(core);\n}\n\nint spend_credit(thread_t* t, int cost){\n  int prev = atomic_fetch_sub(&t->credits, cost);\n  return prev - cost;\n}\n``` \n\n## Follow-up Questions\n- How would you handle dynamic core migration of threads while preserving correctness?\n- How would you adjust MAX to prevent starvation without starving CPU-bound threads?","diagram":"flowchart TD\n  A[Tick] --> B{Earn credits}\n  B --> C[Core Run Queue]\n  C --> D[Pick Task]\n  D --> E[Execute / spend credits]\n  E --> F{Blocked?}\n  F -->|Yes| G[Per-core Wait Queue]\n  F -->|No| C","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Bloomberg","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T16:04:11.884Z","createdAt":"2026-01-26T16:04:11.884Z"},{"id":"q-7774","question":"Design a hybrid I/O scheduler for a system with SSDs and HDDs. The policy must (a) prioritize latency-sensitive reads on SSDs, (b) optimize throughput for sequential writes on HDDs, (c) respect per-process I/O priorities, (d) adapt when devices differ in queue depth, and (e) ensure fairness under mixed workloads. Provide a data-structure sketch and a minimal C-like enqueue/dequeue, plus a test scenario with P1: SSD read, P2: HDD write; describe expected scheduling?","answer":"Use a two-queue policy per device: a latency-first read queue for SSDs and a throughput queue for HDD writes. Each IO carries pid, prio, type, deadline/target, address. Scheduler picks: SSD reads by e","explanation":"## Why This Is Asked\nThis tests practical OS I/O scheduling knowledge, device heterogeneity, and fairness under mixed workloads. It requires concrete data structures and a concrete scheduling policy.\n\n## Key Concepts\n- Block I/O scheduling and per-device queues\n- Priority-aware fairness across heterogeneous devices\n- Read/write path optimization and deadline vs throughput trade-offs\n- Data structures: per-device heaps/queues, per-process weights\n\n## Code Example\n```javascript\n// Pseudo-C-like sketch\ntypedef struct IOReq { int pid; int type; int prio; int deadline; int addr; int len; } IOReq;\ntypedef struct DeviceQueue { Heap reads; Heap writes; int device_id; } DeviceQueue;\nIOReq* pick_next(DeviceQueue* dq) { /* if has reads, pop min by deadline; else pop writes by addr */ }\n```\n\n## Follow-up Questions\n- How would you validate fairness under bursty mixed workloads?\n- How to adjust dynamically when SSD queue depth changes or new devices hot-plugged?","diagram":"flowchart TD\n  Enqueue(IO) --> Decide\n  Decide -->|SSD read| ScheduleSSD\n  Decide -->|HDD write| ScheduleHDD\n  ScheduleSSD -->|issue IO| Complete\n  ScheduleHDD --> Complete","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","Hashicorp","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T17:14:12.141Z","createdAt":"2026-01-26T17:14:12.141Z"},{"id":"q-7859","question":"Implement a minimal IPC channel between two processes using a fixed-size shared ring buffer. The producer writes messages, the consumer reads them; design with two atomic indices (head, tail) and no locks. Explain how to handle wraparound, full/empty, and visibility. Provide a small C-like pseudocode for send and recv using CAS and memory barriers. ?","answer":"Propose a lock-free ring buffer with two atomics: head (consumer) and tail (producer). Producer reserves a slot with CAS on tail, writes, then releases; consumer reads from head and advances with CAS ","explanation":"## Why This Is Asked\nTests IPC basics, race-free data sharing, and practical use of CAS and memory barriers in a beginner-friendly setting.\n\n## Key Concepts\n- Lock-free synchronization\n- Atomic operations and memory ordering\n- Ring buffer wraparound\n- Empty/full conditions and visibility\n\n## Code Example\n```c\ntypedef struct { atomic_uint head, tail; Message buf[N]; } Channel;\n\nbool send(Channel *c, Message m){\n  unsigned long t = atomic_load_explicit(&c->tail, memory_order_relaxed);\n  unsigned long w = (t + 1) % N;\n  if (w == atomic_load_explicit(&c->head, memory_order_acquire)) return false; // full\n  while (!atomic_compare_exchange_weak(&c->tail, &t, w, memory_order_acq_rel, memory_order_relaxed));\n  c->buf[t] = m;\n  atomic_thread_fence(memory_order_release);\n  return true;\n}\n\nbool recv(Channel *c, Message *out){\n  unsigned long h;\n  do {\n    h = atomic_load_explicit(&c->head, memory_order_relaxed);\n    if (h == atomic_load_explicit(&c->tail, memory_order_acquire)) return false; // empty\n  } while (!atomic_compare_exchange_weak(&c->head, &h, (h+1)%N, memory_order_acq_rel, memory_order_relaxed));\n  *out = c->buf[h];\n  return true;\n}\n```\n\n## Follow-up Questions\n- How would you add a timeout or backoff when the buffer is full/empty?\n- How would you adapt this for multiple producers or consumers without locks?","diagram":null,"difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Instacart","LinkedIn","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T20:45:50.374Z","createdAt":"2026-01-26T20:45:50.374Z"},{"id":"q-7878","question":"Design and implement a kernel-space robust futex primitive to synchronize user-space threads on a shared address, addressing termination safety, spurious wakeups, and wake-order guarantees under preemption. Provide end-to-end flow: user-space futex_wait on addr; kernel enqueues and blocks per-address; futex_wake wakes one/all waiters; how the robust list ensures safety when threads exit while blocked; memory barriers; and a minimal C-like sketch of fast-path, slow-path, and a test matrix for races (exit during wait, wake before wait, requeue)?","answer":"Design and implement a kernel-space robust futex primitive to synchronize user-space threads on a shared address, addressing termination safety, spurious wakeups, and wake-order guarantees under preemption. Provide end-to-end flow: user-space futex_wait on addr; kernel enqueues and blocks per-address; futex_wake wakes one/all waiters; how the robust list ensures safety when threads exit while blocked; memory barriers; and a minimal C-like sketch of fast-path, slow-path, and a test matrix for races (exit during wait, wake before wait, requeue)?","explanation":"## Why This Is Asked\nTests understanding of low-level synchronization, race conditions with thread lifecycle, and memory-ordering guarantees in kernel-space futex design.\n\n## Key Concepts\n- Futex fast vs slow path\n- Per-address wait queues\n- Robust futex list and exit handling\n- Race conditions during thread termination\n- Memory barriers and wake ordering\n\n## Code Example\n```c\n// Minimal sketch of fast-path vs slow-path for futex_wait\ntypedef struct { atomic_int *addr; wait_queue_t *wq; robust_list_t *robust; } futex_t;\nint futex_wait(int *addr, int expected) {\n  // Fast path: if *addr != expected, return immediately\n  if (atomic_load(addr) != expected) return EAGAIN;\n  \n  // Slow path: enqueue and block\n  futex_t f = {addr, get_wait_queue(addr), current->robust_list};\n  add_to_robust_list(&f);\n  smp_mb(); // Memory barrier before sleep\n  return wait_event_interruptible(f.wq, atomic_load(addr) != expected);\n}\n```\n\n## Test Matrix\n| Race Condition | Expected Behavior |\n|----------------|------------------|\n| Thread exits during wait | Robust list cleanup wakes waiters |\n| Wake before wait | Waiter sees updated value, returns immediately |\n| Requeue race | Waiter moves to new queue without lost wakeups |","diagram":"flowchart TD\n  A[Futex Wait Start] --> B[Enqueue and Block]\n  B --> C{Wake One?}\n  C -->|Yes| D[Wake Up One Waiter]\n  C -->|No| E[Wake All Waiters]\n  D --> F[Return to User Space]\n  E --> F","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T06:47:34.076Z","createdAt":"2026-01-26T21:36:14.355Z"},{"id":"q-7923","question":"Describe and implement a minimal user-space simulation of Copy-On-Write memory for a shared anonymous region after fork. Two threads initially share a page; when one writes, a new page is allocated, data copied, and mappings updated. Show race conditions during cloning and how atomic refcounts prevent double copies. Provide a minimal C-like sketch for map, write fault handler, and cloning logic?","answer":"After fork, pages are initially shared as read-only. On the first write fault, allocate a new private page, copy the original data, update only the writer's page table entry, and atomically decrement the original page's reference count. The page is freed when its refcount reaches zero. Atomic reference counting prevents duplicate page allocation during concurrent write faults.","explanation":"## Why This Is Asked\n\nThis question evaluates understanding of Copy-On-Write semantics, page fault-driven memory management, and race condition handling in user-space implementations.\n\n## Key Concepts\n\n- Copy-On-Write (COW) mechanism\n- Page table management\n- Atomic operations and reference counting\n- Memory barriers and synchronization\n- Race condition prevention\n\n## Code Example\n\n```c\n// Pseudo-code: per-page reference counting with CAS-based cloning\nvoid handle_write_fault(Page *p) {\n  if (atomic_fetch_sub(&p->refcount, 1) <= 0) {\n    // Another thread already consumed this page\n    return;\n  }\n  Page *clone = allocate_page();\n  memcpy(clone->data, p->data, PAGE_SIZE);\n  update_pte(thread->ttbr, vaddr, clone->phys);\n}\n```\n\n## Follow-up Questions","diagram":null,"difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["DoorDash","Google","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T06:02:55.217Z","createdAt":"2026-01-26T23:33:48.502Z"},{"id":"q-7968","question":"In a preemptive kernel running on 4 CPUs, design and implement a mutex with priority inheritance to fix priority inversion when a high-priority task waits for a mutex held by an I/O-bound task. Provide a minimal C-like API: mutex_lock, mutex_unlock, and data structures to track base and current priorities. Sketch how to update priorities on lock/unlock, show a concrete interleaving where inheritance prevents starvation, and discuss multicore pitfalls and timer preemption?","answer":"Elevate the holder's priority to the maximum priority among all waiting tasks when acquiring the lock. Track both base and inflated priorities per task. On unlock, restore the holder to its base priority or the next highest waiting task's priority. Maintain a priority inheritance chain to handle nested locking scenarios and prevent unbounded priority inversion.","explanation":"## Why This Is Asked\nTests understanding of real-time constraints, priority inheritance mechanisms, and multicore synchronization challenges. Requires concrete data structures and reasoning about nested locking, IRQ interactions, and race conditions.\n\n## Key Concepts\n- Priority Inheritance Protocol\n- Mutex design with priority tracking\n- Multicore scheduling interactions\n- Deadlock and priority inversion edge cases\n\n## Code Example\n\n```c\n// Minimal sketch of priority inheritance mutex\n// (conceptual C-like pseudocode for interview setting)\n\ntypedef struct task {\n    int base_prio;\n    int cur_prio;\n    // ... other fields\n} task_t;\n\ntypedef struct mutex {\n    task_t *holder;\n    // queue of waiting tasks, sorted by priority\n    struct list_head wait_queue;\n} mutex_t;\n\nvoid mutex_lock(mutex_t *m, task_t *t) {\n    if (m->holder == NULL) {\n        m->holder = t;\n        return;\n    }\n    \n    // Add to wait queue\n    enqueue_by_prio(&m->wait_queue, t);\n    \n    // Priority inheritance: boost holder if needed\n    if (t->cur_prio > m->holder->cur_prio) {\n        m->holder->cur_prio = t->cur_prio;\n        // Reschedule holder to run immediately\n        schedule_task(m->holder);\n    }\n    \n    // Block current task\n    block_task(t);\n}\n\nvoid mutex_unlock(mutex_t *m, task_t *t) {\n    if (m->holder != t) return;\n    \n    // Find highest priority waiter\n    task_t *next_waiter = peek_highest_prio(&m->wait_queue);\n    \n    if (next_waiter) {\n        // Transfer lock to next waiter\n        m->holder = next_waiter;\n        dequeue(&m->wait_queue, next_waiter);\n        unblock_task(next_waiter);\n    } else {\n        m->holder = NULL;\n    }\n    \n    // Restore holder's priority\n    t->cur_prio = t->base_prio;\n}\n```\n\n## Priority Update Logic\n- **Lock**: When task T waits for mutex held by H, set H.cur_prio = max(H.cur_prio, T.cur_prio)\n- **Unlock**: Restore holder to base_prio or highest waiting task's priority\n- **Chain Propagation**: Handle nested mutexes by recursively updating priority chains\n\n## Concrete Interleaving Example\n``nTime  High(T1)  Medium(T2)  Low(T3)  Mutex Action\n0     -         -           Run     T3 acquires M1\n1     -         Run         -       \n2     Run       Blocked     -       T1 tries M1, blocks\n3     -         -           Elevated T3 priority = max(T3.base, T1.cur) = HIGH\n4     -         -           Run     T3 releases M1, T1 acquires it\n```\nWithout inheritance: T1 starves while T3 performs I/O.\nWith inheritance: T3 gets temporary boost to complete critical section quickly.\n\n## Multicore Pitfalls\n- **Race Conditions**: Priority updates must be atomic across cores\n- **Cache Coherence**: Priority fields need proper memory barriers\n- **Timer Preemption**: Inherited priorities should survive timer interrupts\n- **Scheduler Migration**: Tasks may migrate between CPUs during priority boost\n- **Nested Locking**: Complex inheritance chains with multiple mutexes\n\n## Timer Preemption Considerations\n- Inherited priorities override timer tick rescheduling\n- Boost duration should be limited to critical section execution\n- Timer interrupts should not demote boosted tasks prematurely\n```","diagram":"flowchart TD\n  A[High-priority task waits on mutex] --> B{Mutex owner has lower priority}\n  B --> C[Apply priority inheritance: elevate owner]\n  C --> D[Owner runs, high-priority task progresses]\n  D --> E[Unlock mutex]\n  E --> F[Restore owner to base priorities or next waiter]\n  F --> G[System continues with normal scheduling]","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Databricks","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T04:07:14.964Z","createdAt":"2026-01-27T02:50:35.934Z"},{"id":"q-8113","question":"In a tiny preemptive OS, implement a sleep(n) API and wakeup mechanism. Describe the data structures for a per-task sleep queue, how the timer interrupt wakes tasks, and provide minimal C-like pseudo-code for enqueuing sleeping tasks and handling ticks to wake them, including race-condition avoidance between enqueue and tick?","answer":"On sleep(n): wake_tick = current_tick + n; disable_interrupts(); insert task into a single, ordered sleep_list by wake_tick; suspend task. Timer ISR: current_tick++; while (sleep_list.head && sleep_li","explanation":"## Why This Is Asked\nTests practical skills for implementing sleep/wake in a preemptive kernel, focusing on data structures, ISR interaction, and race conditions.\n\n## Key Concepts\n- Sleep list ordered by wake_tick\n- Per-task wake_tick and state fields\n- Interrupt masking or locks to avoid races between enqueue and timer\n- Immediate wakeup and re-scheduling to ready queue\n\n## Code Example\n```javascript\n// Pseudo C-like snippet\ntypedef struct Task { int wake_tick; int state; struct Task* next; } Task;\nTask* sleep_list = NULL;\nvoid sleep(int n) {\n  int w = current_tick + n;\n  task->wake_tick = w;\n  disable_interrupts();\n  insert_ordered(&sleep_list, task, w);\n  task->state = SLEEPING;\n  schedule();\n  enable_interrupts();\n}\nvoid timer_handler() {\n  current_tick++;\n  while (sleep_list && sleep_list->wake_tick <= current_tick) {\n    Task* t = pop_head(&sleep_list);\n    t->state = READY;\n    enqueue_ready(t);\n  }\n}\n```","diagram":null,"difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Anthropic","Google","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T10:08:26.545Z","createdAt":"2026-01-27T10:08:26.545Z"},{"id":"q-8248","question":"Build a simple barrier primitive for N tasks in a toy OS scheduler. Each task calls barrier_wait(barrier_id). The barrier blocks until all participants arrive, then all wake simultaneously. Explain how you handle barrier reuse, arrival races, and wakeup races. Include a minimal C-like skeleton for barrier_init and barrier_wait and a quick test plan to verify correctness and fairness?","answer":"Use a barrier with a mutex, a condition variable, a counter, and a generation id. barrier_init(b,n) sets count=0, needed=n, gen=0. barrier_wait(b) locks; if (++count==needed){ count=0; gen++; wake all","explanation":"## Why This Is Asked\n\nTests understanding of synchronization primitives and memory ordering in a toy OS context.\n\n## Key Concepts\n\n- Barrier reuse via a generation counter\n- Race-free wakeups with a condition variable analogue\n- Memory visibility and minimal locking\n- Testing for progress and fairness under contention\n\n## Code Example\n\n```c\ntypedef struct {\n  int count;\n  int needed;\n  int gen;\n  pthread_mutex_t m;\n  pthread_cond_t cv;\n} barrier_t;\n\nvoid barrier_init(barrier_t *b, int n) {\n  b->count = 0;\n  b->needed = n;\n  b->gen = 0;\n  pthread_mutex_init(&b->m, NULL);\n  pthread_cond_init(&b->cv, NULL);\n}\n\nvoid barrier_wait(barrier_t *b) {\n  pthread_mutex_lock(&b->m);\n  int g = b->gen;\n  if (++b->count == b->needed) {\n    b->count = 0;\n    b->gen++;\n    pthread_cond_broadcast(&b->cv);\n  } else {\n    while (b->gen == g) {\n      pthread_cond_wait(&b->cv, &b->m);\n    }\n  }\n  pthread_mutex_unlock(&b->m);\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt this for dynamic participant counts?\n- How would you test for spurious wakeups and ensure progress under contention?","diagram":"flowchart TD\n  A[Barrier Init] --> B[Task Arrives]\n  B --> C{All Arrived?}\n  C -- Yes --> D[Release & Reset]\n  C -- No --> E[Task Waits]\n  E --> C\n  D --> F[Barrier Reuse]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Netflix","Scale Ai","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T16:52:10.855Z","createdAt":"2026-01-27T16:52:10.855Z"},{"id":"q-8302","question":"Design and implement a scalable synchronization primitive for a toy OS kernel: an MCS lock to guard a shared per-CPU queue. Provide interfaces: void mcs_lock(struct mcs_node *n, volatile struct mcs_lock *lock); void mcs_unlock(struct mcs_node *n, volatile struct mcs_lock *lock); where struct mcs_node { struct mcs_node *next; int locked; }; Explain how enqueuing, per-thread spinning, and wake-up work, why it reduces cache bouncing, and how to test fairness under high contention with many threads contending for the queue. Include a minimal C-like sketch and a simple bench outline?","answer":"An MCS lock uses per-thread nodes; acquire by enqueuing on tail and spinning on your node’s flag; unlock by signaling the successor via next or CAS tail to NULL. It provides FIFO ordering, minimal cro","explanation":"## Why This Is Asked\nTests knowledge of scalable synchronization primitives, cache locality, and deadlock avoidance in kernel-like environments.\n\n## Key Concepts\n- FIFO fairness of MCS locks\n- Per-thread node state and local spinning\n- Cache-coherence benefits; minimal invalidations\n- Safe unlock with successor wake-up and memory barriers\n\n## Code Example\n```javascript\ntypedef struct mcs_node { struct mcs_node *next; int locked; } mcs_node;\ntypedef struct mcs_lock { mcs_node *tail; } mcs_lock;\nvoid mcs_lock_acquire(mcs_node *node, volatile mcs_lock *lock) {\n  node->next = NULL;\n  node->locked = 1;\n  mcs_node *pred = swap(&lock->tail, node);\n  if (pred) {\n    pred->next = node;\n    while (node->locked) __sync_synchronize();\n  }\n}\nvoid mcs_lock_release(mcs_node *node, volatile mcs_lock *lock) {\n  if (!node->next) {\n    if (__sync_bool_compare_and_swap(&lock->tail, node, NULL)) return;\n    while (!node->next) { /* spin until successor appears */ }\n  }\n  node->next->locked = 0;\n}\n```\n\n## Follow-up Questions\n- How to adapt to 64-bit architectures (CAS and memory barriers)?\n- How to handle preemption and interrupt safety in the acquire path?","diagram":"flowchart TD\n  A[MCS lock] --> B[enqueue node]\n  B --> C[predecessor pointer set]\n  C --> D[spin on local node]\n  D --> E[unlock path sets successor\n  E --> F[successful wake-up]]","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Cloudflare","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T19:15:14.114Z","createdAt":"2026-01-27T19:15:14.114Z"},{"id":"q-8498","question":"Design a per-thread deadline timer using a timer wheel in a toy OS. Explain the data structures, how to insert a new timer, how expirations are processed on each tick, and how to avoid races when multiple threads add or wake timers concurrently. Include a minimal C-like skeleton for a timer node and an insert function?","answer":"Use a circular timer wheel with B buckets. Each timer holds expiry_tick, task, and next. On insert, compute delta = expiry - now; bucket[(now+delta) % B].push(timer). On each timer tick, process bucke","explanation":"## Why This Is Asked\nTests understanding of timer data structures, concurrency, and practical OS scheduling under contention.\n\n## Key Concepts\n- Timer wheel: bucketed deadlines for O(1) insert\n- Concurrency: per-bucket locking or lock-free heads\n- Expiration path: wake task, reschedule, handle cancellations\n\n## Code Example\n```c\ntypedef struct Timer {\n  uint64_t expiry; // system ticks\n  struct Task* t;\n  struct Timer* next;\n} Timer;\n\n#define WHEEL_SIZE 256\n\ntypedef struct TimerWheel {\n  Timer* buckets[WHEEL_SIZE];\n  uint64_t current;\n  spinlock_t lock;\n} TimerWheel;\n\nvoid insert_timer(TimerWheel* w, Timer* tm, uint64_t expiry) {\n  uint64_t now = get_current_tick();\n  uint64_t delta = (expiry > now) ? (expiry - now) : 0;\n  uint64_t idx = (w->current + (delta % WHEEL_SIZE)) % WHEEL_SIZE;\n  tm->expiry = expiry;\n  tm->next = w->buckets[idx];\n  w->buckets[idx] = tm;\n}\n```\n\n## Follow-up Questions\n- How would you cancel a timer already in the wheel?\n- How does this scale with multiple CPUs and large timer counts?\n- How would you handle tick drift or long sleeps?\n","diagram":"flowchart TD\n  A[User thread inserts timer] --> B[Compute bucket index]\n  B --> C[Place in bucket]\n  C --> D[On tick, current advances]\n  D --> E[Process bucket and wake thread]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Bloomberg","DoorDash","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T05:54:35.559Z","createdAt":"2026-01-28T05:54:35.559Z"},{"id":"q-927","question":"In a system with a fixed-size circular buffer of size N shared by a producer and a consumer thread, implement a thread-safe producer-consumer solution using semaphores and a mutex in C. Include initialization, edge cases (buffer full/empty), and show how you would test for deadlocks and correctness under concurrent producers/consumers?","answer":"Use two counting semaphores and a mutex: empty initialized to N, full to 0, and a mutex protecting the circular buffer. Producer waits on empty, locks, writes at in, in=(in+1)%N, unlocks, signals full","explanation":"## Why This Is Asked\nTests understanding of basic synchronization primitives and common OS patterns, especially producer-consumer, deadlock avoidance, and correct semaphore/mutex usage in C.\n\n## Key Concepts\n- Semaphores\n- Mutex\n- Circular buffer\n- Deadlock detection\n- Boundary conditions (full/empty)\n\n## Code Example\n```c\n#include <pthread.h>\n#include <semaphore.h>\n#include <stdio.h>\n\n#define N 10\n\nint buffer[N];\nint in = 0, out = 0;\nsem_t empty, full;\npthread_mutex_t mutex;\n\nvoid produce(int item) {\n  sem_wait(&empty);\n  pthread_mutex_lock(&mutex);\n  buffer[in] = item;\n  in = (in + 1) % N;\n  pthread_mutex_unlock(&mutex);\n  sem_post(&full);\n}\n\nint consume() {\n  int item;\n  sem_wait(&full);\n  pthread_mutex_lock(&mutex);\n  item = buffer[out];\n  out = (out + 1) % N;\n  pthread_mutex_unlock(&mutex);\n  sem_post(&empty);\n  return item;\n}\n```\n\n## Follow-up Questions\n- How would you extend to multiple producers/consumers and ensure fairness?\n- What are trade-offs vs. using condition variables instead of semaphores?","diagram":null,"difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","OpenAI","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:37:15.531Z","createdAt":"2026-01-12T15:37:15.531Z"},{"id":"q-995","question":"In a system using paging with a TLB, describe the end-to-end sequence when a 4 KB page accessed by a process is not mapped in RAM, from fault to resume, including the fault handler, page-table walk, TLB update, disk I/O to swap, and how the eviction policy decides which page to replace?","answer":"On a fault: hardware traps to the kernel; verify access rights; pick a free physical frame or evict a victim; perform a page-table walk to map VPN to the chosen PPN and set present/dirty bits; flush o","explanation":"## Why This Is Asked\n\nTests understanding of demand paging, TLB coherency, and eviction trade-offs in real OS.\n\n## Key Concepts\n\n- Paging, page tables, TLB, and ASIDs\n- Page fault handling, disk I/O, swap vs page file\n- Eviction policies (LRU, Clock) and dirty page handling\n\n## Code Example\n\n```c\n// Pseudo: handle_page_fault()\n// 1) check rights, 2) allocate frame, 3) page_table[VPN] = PPN|present, 4) TLB.invalidate(VPN); TLB.update(VPN, PPN);\n```\n\n## Follow-up Questions\n\n- How does asynchronous I/O affect fault handling?\n- What mitigations reduce thrashing in a busy workload?\n","diagram":"flowchart TD\n  A[Page fault] --> B[Trap to kernel]\n  B --> C{Free frame?}\n  C -- yes --> D[Map VPN->PPN; set present]\n  C -- no --> E[Evict victim]\n  E --> F[If dirty: write back]\n  F --> D\n  D --> G[Update TLB]\n  G --> H[Resume]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:39:46.467Z","createdAt":"2026-01-12T18:39:46.467Z"},{"id":"q-263","question":"How does demand paging optimize memory utilization in virtual memory systems, what triggers page faults, and which algorithms handle page replacement when physical memory is full?","answer":"Demand paging loads pages only on first access, triggering page faults handled by the OS. Page replacement algorithms like LRU, FIFO, or Clock evict pages when memory is full. The TLB caches recent translations to reduce overhead, while working set models prevent thrashing by tracking active pages. Performance depends on locality, page size, and replacement efficiency.","explanation":"## Memory Efficiency\nDemand paging eliminates loading unused pages, reducing initial memory overhead and I/O. Pages load lazily when first accessed via page faults.\n\n## Page Fault Handling\n1. CPU traps to OS\n2. OS validates the access\n3. Locates page on disk\n4. Selects victim page if needed\n5. Loads page into free frame\n6. Updates page tables\n7. Restarts instruction\n\n## Page Replacement Algorithms\n**LRU**: Evicts least recently used page - optimal locality but expensive tracking. **FIFO**: Simple but can suffer from Belady's anomaly. **Clock**: Approximates LRU with reference bits, balancing performance and overhead.\n\n## TLB Role\nThe Translation Lookaside Buffer caches recent virtual-to-physical mappings, reducing page table walks from memory access to single cycle lookup.\n\n## Thrashing Prevention\nWorking set models track each process's active pages. Local replacement prevents one process from evicting another's pages. Page fault frequency monitoring detects thrashing and may suspend processes.\n\n## Performance Trade-offs\nPage size affects internal fragmentation (larger pages) vs. TLB miss rate (smaller pages). Prepaging can reduce page faults but wastes memory. Copy-on-write optimizes fork() operations by sharing pages until modification.","diagram":"flowchart TD\n    A[Process accesses memory] --> B{Page in RAM?}\n    B -->|Yes| C[Access data directly]\n    B -->|No| D[Page fault triggered]\n    D --> E[OS handles fault]\n    E --> F[Load page from disk]\n    F --> G[Update page table]\n    G --> H[Restart instruction]\n    H --> C","difficulty":"intermediate","tags":["virtual-memory","paging","segmentation","cache"],"channel":"operating-systems","subChannel":"memory","sourceUrl":null,"videos":null,"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-26T16:40:56.531Z","createdAt":"2025-12-26 12:51:07"}],"subChannels":["general","memory"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":90,"beginner":37,"intermediate":30,"advanced":23,"newThisWeek":38}}