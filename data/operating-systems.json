{"questions":[{"id":"q-448","question":"Explain how process scheduling works in an operating system. Which scheduling algorithm would you choose for a real-time system and why?","answer":"Process scheduling manages CPU allocation among processes by determining which process gets CPU time and when. For real-time systems, I would choose **Rate Monotonic Scheduling (RMS)** because it's predictable, has bounded response times, and is optimal for fixed-priority real-time systems. RMS assigns higher priorities to tasks with shorter periods, ensuring critical tasks meet their deadlines while maintaining system stability.","explanation":"## Key Concepts\n- **Process scheduling**: Determines CPU allocation order among competing processes\n- **Scheduling algorithms**: FCFS, SJF, Priority, Round Robin, Rate Monotonic Scheduling\n- **Real-time requirements**: Deterministic timing behavior and deadline guarantees\n\n## Real-time Considerations\n- **Predictability**: Must guarantee task completion before deadlines\n- **Priority assignment**: Shorter periods receive higher priorities in RMS\n- **CPU utilization**: RMS can safely utilize up to 69% of CPU capacity\n\n## Implementation\n```c\n// RMS priority calculation\npriority = 1000 / period; // Higher values for shorter periods\n```\n\n## Trade-offs\n- RMS offers simplicity and predictability but is less flexible compared to dynamic scheduling algorithms\n- Optimal for fixed-priority systems but may require careful task design","diagram":"flowchart TD\n  A[Process Request] --> B{Scheduler}\n  B --> C[RMS Priority Check]\n  C --> D[CPU Allocation]\n  D --> E[Process Execution]\n  E --> F[Deadline Check]\n  F --> G{Met Deadline?}\n  G -->|Yes| H[Continue]\n  G -->|No| I[Priority Boost]\n  H --> B","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","OpenAI","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T08:53:46.358Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-471","question":"You're designing a GPU memory manager for CUDA applications. How would you implement a memory allocator that handles both unified memory and explicit device memory, considering fragmentation, coalescing, and the 48-bit address space limitations?","answer":"I would implement a hybrid memory allocator combining segregated free lists for different size classes with a buddy system for large allocations, utilizing virtual memory techniques to efficiently manage the 48-bit address space. The allocator would support both unified memory and explicit device memory through separate allocation pools while providing automatic coalescing and fragmentation management.","explanation":"## Memory Management Architecture\n\n### Allocation Strategies\n- **Segregated Lists**: Separate free lists optimized for small (4KB-64KB), medium (64KB-1MB), and large (>1MB) allocations to minimize search time\n- **Buddy System**: Power-of-2 allocation for large blocks enabling efficient coalescing and reducing external fragmentation\n- **Slab Allocation**: Specialized pools for frequently used small objects to minimize internal fragmentation\n\n### Address Space Management\n```c\n// 48-bit virtual address layout\ntypedef struct {\n    uint64_t prefix : 16;  // Reserved for future expansion\n    uint64_t vaddr   : 48;  // Actual virtual address\n} gpu_vaddr_t;\n```\n\n### Unified Memory Optimization\n- **Migration Policies**: Implement adaptive page migration based on access patterns and locality\n- **Prefetching**: Hardware-guided prefetching for anticipated memory accesses\n- **Page Fault Handling**: Efficient page fault resolution with minimal CPU intervention","diagram":"flowchart TD\n  A[Application Request] --> B{Memory Type}\n  B -->|Unified| C[Unified Memory Path]\n  B -->|Device| D[Explicit Device Memory]\n  C --> E[Page Fault Handler]\n  E --> F[Migration Heuristics]\n  F --> G[Allocate Virtual Pages]\n  D --> H[Size Class Selection]\n  H --> I[Segregated Free List]\n  I --> J[Buddy System Check]\n  J --> K[Physical Allocation]\n  G --> L[GPU Memory Mapping]\n  K --> L","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":["memory allocator","fragmentation","coalescing","segregated free lists","buddy system","virtual memory","cuda"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-09T09:04:49.876Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-530","question":"A process is stuck in 'D' state (uninterruptible sleep) during I/O operations. How would you debug this, what causes it, and how does it differ from 'Z' zombie state?","answer":"The 'D' state occurs during kernel-level I/O operations that cannot be interrupted, such as disk access or network operations. To identify stuck processes, use `ps aux | awk '$8 ~ /^D/ {print}'`. For debugging, `strace -p <pid>` shows the blocked system calls, while `lsof -p <pid>` reveals open file descriptors and identifies the specific I/O operation causing the block.","explanation":"## Debugging D State\n\n- Use `ps aux` to identify processes in D state\n- `strace -p <pid>` shows blocked system calls\n- `lsof -p <pid>` reveals open file descriptors\n- Check `/proc/<pid>/stack` for kernel stack trace\n\n## Common Causes\n\n- Faulty storage devices or network mounts\n- NFS/SMB server unresponsiveness\n- Hardware driver issues\n- Kernel bugs in I/O subsystem\n\n## D vs Z State\n\n- **D state**: Process alive, blocked in kernel, cannot be killed\n- **Z state**: Process terminated, resources freed, waiting for parent\n- D state consumes kernel resources, Z state only holds PID\n\n## Resolution\n\n- Address underlying hardware or network issues\n- Restart problematic services or mounts\n- Update drivers or kernel if needed\n- As last resort, reboot the system to clear D state processes","diagram":"flowchart TD\n  A[Process Running] --> B{I/O Request}\n  B -->|Kernel Space| C[D State - Uninterruptible Sleep]\n  C -->|I/O Complete| D[Return to Running]\n  C -->|Hardware/Network Issue| E[Stuck in D State]\n  F[Process Terminated] --> G[Parent Reads Exit Status]\n  G --> H[Z State - Zombie]\n  H -->|Parent Cleanup| I[PID Released]","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","LinkedIn","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":["uninterruptible sleep","strace","lsof","zombie state","kernel i/o","debugging"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:42:54.966Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-556","question":"How would you debug a process that's consuming 100% CPU but not responding to signals? What tools and steps would you use?","answer":"Start by identifying the process ID using `top` or `htop` to confirm the high CPU usage. Then attach `strace -p <PID>` to monitor system calls and determine if the process is stuck in user space or kernel mode. Check `/proc/<PID>/status` for the process state and examine `/proc/<PID>/stack` for kernel stack information. If the process remains unresponsive, use `gdb -p <PID>` to obtain stack traces and analyze the execution context.","explanation":"## Debugging Methodology\n\n1. **Process Identification**: Use `top`, `htop`, or `ps aux` to identify the PID of the CPU-intensive process\n2. **Process State Analysis**: Examine `/proc/<PID>/status` to understand current state (running, sleeping, zombie, etc.)\n3. **System Call Monitoring**: Deploy `strace -p <PID>` to trace system calls in real-time and identify blocking operations\n4. **Stack Trace Analysis**: Attach with `gdb -p <PID>` to inspect call stacks and identify infinite loops or deadlock conditions\n5. **Kernel-level Investigation**: Review `dmesg` output and `/proc/<PID>/stack` for kernel-space issues\n\n## Common Root Causes\n\n- Infinite loops in application code\n- Thread synchronization deadlocks\n- Blocking I/O operations without timeouts\n- Memory corruption leading to undefined behavior\n- Kernel bugs or driver issues\n\n## Resolution Strategies\n\n- Send `SIGKILL` as last resort if process remains unresponsive\n- Analyze application logs for error patterns preceding the issue\n- Implement proper error handling and timeout mechanisms\n- Address underlying code quality issues in the identified problem areas","diagram":"flowchart TD\n  A[High CPU Detected] --> B[top/htop - identify PID]\n  B --> C[Check /proc/<PID>/status]\n  C --> D[strace -p <PID>]\n  D --> E{System calls active?}\n  E -->|Yes| F[gdb -p <PID> - analyze stack]\n  E -->|No| G[Check for kernel deadlock]\n  F --> H[Identify root cause]\n  G --> H\n  H --> I[Apply fix/kill process]","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:56:11.138Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-585","question":"How would you implement a lock-free concurrent queue using atomic operations and memory barriers? What are the trade-offs between ABA problem solutions?","answer":"Implement using atomic head and tail pointers with compare-and-swap operations. Use hazard pointers or versioned references to solve the ABA problem. Trade-offs: hazard pointers have higher memory overhead but simpler implementation, while versioned references offer lower overhead with more complex compare-and-swap logic.","explanation":"## Lock-Free Queue Implementation\n\n- Use atomic head and tail pointers to manage queue boundaries\n- Enqueue: Atomically update tail pointer using CAS, then link new node\n- Dequeue: Atomically update head pointer using CAS, then read next node\n- Memory barriers: Use acquire semantics on loads and release semantics on stores\n\n## ABA Problem Solutions\n\n- **Hazard pointers**: Track protected nodes per thread to prevent premature reclamation\n- **Versioned references**: Combine pointer with monotonically increasing counter\n- **Epoch-based reclamation**: Batch deallocation operations by epoch boundaries\n\n## Trade-offs\n\n- Hazard pointers: Higher memory usage, simpler logic\n- Versioned references: Lower overhead, more complex CAS operations\n- Epoch-based reclamation: Better scalability, more complex implementation\n- Performance depends on contention levels and allocation patterns","diagram":"flowchart TD\n  A[Enqueue] --> B[CAS Tail Pointer]\n  B --> C[Link New Node]\n  D[Dequeue] --> E[CAS Head Pointer]\n  E --> F[Read Next Node]\n  G[ABA Problem] --> H[Hazard Pointers]\n  G --> I[Versioned References]\n  G --> J[Epoch Reclamation]","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","OpenAI","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:51:08.528Z","createdAt":"2025-12-27T01:14:13.089Z"},{"id":"q-263","question":"How does demand paging optimize memory utilization in virtual memory systems, what triggers page faults, and which algorithms handle page replacement when physical memory is full?","answer":"Demand paging loads pages only on first access, triggering page faults handled by the OS. Page replacement algorithms like LRU, FIFO, or Clock evict pages when memory is full. The TLB caches recent translations to reduce overhead, while working set models prevent thrashing by tracking active pages. Performance depends on locality, page size, and replacement efficiency.","explanation":"## Memory Efficiency\nDemand paging eliminates loading unused pages, reducing initial memory overhead and I/O. Pages load lazily when first accessed via page faults.\n\n## Page Fault Handling\n1. CPU traps to OS\n2. OS validates the access\n3. Locates page on disk\n4. Selects victim page if needed\n5. Loads page into free frame\n6. Updates page tables\n7. Restarts instruction\n\n## Page Replacement Algorithms\n**LRU**: Evicts least recently used page - optimal locality but expensive tracking. **FIFO**: Simple but can suffer from Belady's anomaly. **Clock**: Approximates LRU with reference bits, balancing performance and overhead.\n\n## TLB Role\nThe Translation Lookaside Buffer caches recent virtual-to-physical mappings, reducing page table walks from memory access to single cycle lookup.\n\n## Thrashing Prevention\nWorking set models track each process's active pages. Local replacement prevents one process from evicting another's pages. Page fault frequency monitoring detects thrashing and may suspend processes.\n\n## Performance Trade-offs\nPage size affects internal fragmentation (larger pages) vs. TLB miss rate (smaller pages). Prepaging can reduce page faults but wastes memory. Copy-on-write optimizes fork() operations by sharing pages until modification.","diagram":"flowchart TD\n    A[Process accesses memory] --> B{Page in RAM?}\n    B -->|Yes| C[Access data directly]\n    B -->|No| D[Page fault triggered]\n    D --> E[OS handles fault]\n    E --> F[Load page from disk]\n    F --> G[Update page table]\n    G --> H[Restart instruction]\n    H --> C","difficulty":"intermediate","tags":["virtual-memory","paging","segmentation","cache"],"channel":"operating-systems","subChannel":"memory","sourceUrl":null,"videos":null,"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-26T16:40:56.531Z","createdAt":"2025-12-26 12:51:07"}],"subChannels":["general","memory"],"companies":["Adobe","Amazon","Apple","Citadel","DoorDash","Google","LinkedIn","Meta","Microsoft","NVIDIA","Netflix","OpenAI","Snap","Snowflake","Square","Two Sigma"],"stats":{"total":6,"beginner":1,"intermediate":3,"advanced":2,"newThisWeek":0}}