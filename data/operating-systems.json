{"questions":[{"id":"q-1124","question":"On a NUMA‑aware Linux host with a fixed‑size worker pool handling high‑frequency RPCs, cross‑socket memory traffic is a bottleneck. Propose a concrete plan to minimize inter‑socket traffic: pin threads to sockets, allocate per‑socket data, and choose memory policies (numa_bind/numa_alloc_onnode). Include measurement steps and success criteria?","answer":"Pin threads to sockets, allocate per‑socket queues and data, and ensure memory allocations stay on the same node via numa_bind/numa_alloc_onnode. Favor per‑socket pools to avoid remote RAM. Validate w","explanation":"## Why This Is Asked\nTests practical skills in NUMA locality, thread affinity, and measurable performance impact.\n\n## Key Concepts\n- NUMA locality and memory policies\n- Thread and data placement\n- Per‑socket pools and queues\n- Performance measurement with perf and hardware counters\n\n## Code Example\n```javascript\n// Pin a thread to CPU set on Linux (example approach)\n#include <pthread.h>\n#include <sched.h>\n\nvoid pin_thread(int cpu) {\n  cpu_set_t cpuset; CPU_ZERO(&cpuset); CPU_SET(cpu, &cpuset);\n  pthread_setaffinity_np(pthread_self(), sizeof(cpu_set_t), &cpuset);\n}\n```\n\n## Follow-up Questions\n- How would you validate that per‑socket locality reduces cache misses under bursty load?\n- What trade‑offs arise if a worker must be migrated for load balancing?","diagram":null,"difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Netflix","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:31:03.248Z","createdAt":"2026-01-12T23:31:03.248Z"},{"id":"q-1299","question":"Scenario: two threads share a global 32-bit counter. Thread A increments it in a tight loop; Thread B logs the value once per second. Without synchronization, describe a concrete interleaving that yields a stale read or lost update, and explain the cache/coherence mechanics behind it. Then outline the minimal fix and how it ensures atomicity and visibility (e.g., atomic fetch_add or a mutex)?","answer":"Interleaving: A reads 0, B reads 0, A increments to 1 and writes 1, B logs 0. The write may not be visible immediately due to per-core caches and cache-coherence delays, so B observes 0. Fix with atom","explanation":"## Why This Is Asked\n\nConveys understanding of data races and real hardware behavior, not just theory.\n\n## Key Concepts\n\n- Data races and undefined behavior\n- Cache coherence and MESI, visibility across cores\n- Atomic operations vs locking; memory ordering guarantees\n- How preemption and context switches interact with shared data\n\n## Code Example\n\n```javascript\n// Pseudocode demonstrating the race (not executable in JS)\nlet counter = 0;\nfunction A(){ while(true){ let x = counter; counter = x + 1; } }\nfunction B(){ setInterval(()=> console.log(counter), 1000); }\n```\n\n## Follow-up Questions\n\n- How does memory_order_seq_cst affect visibility on modern CPUs?\n- How would you test this race condition in a real system?\n","diagram":null,"difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:45:09.473Z","createdAt":"2026-01-13T08:45:09.473Z"},{"id":"q-1354","question":"You implement a lock-free ring buffer with two atomics (head, tail) and a data array for inter-thread communication. Describe a concrete interleaving on a weak memory model (e.g., ARM64) where the consumer observes a stale value or an invalid read due to missing ordering. Propose a minimal fix using memory_order_release on the data write/tail update and memory_order_acquire on the consumer read, and sketch a safe patch?","answer":"Introduce acquire/release ordering in a ring buffer. Producer writes data[tail] then tail.store(next, memory_order_release). Consumer loads tail with memory_order_acquire and reads data[head] if head ","explanation":"## Why This Is Asked\n\nTests understanding of memory ordering, lock-free data structures, and weak memory models in OS CPUs. It probes ability to identify a real race and reason about inter-thread visibility across architectures.\n\n## Key Concepts\n\n- Lock-free synchronization\n- memory_order_release\n- memory_order_acquire\n- Data visibility before signaling availability\n\n## Code Example\n\n```cpp\n// Pseudo-C++ lock-free sketch\nstd::atomic<size_t> head{0}, tail{0};\nint data[N];\n\nvoid produce(int v){\n  auto t = tail.load(std::memory_order_relaxed);\n  data[t % N] = v;\n  tail.store((t+1) % N, std::memory_order_release);\n}\n\nint consume(){\n  auto t = tail.load(std::memory_order_acquire);\n  auto h = head.load(std::memory_order_relaxed);\n  if (h == t) return -1;\n  int v = data[h % N];\n  head.store((h+1) % N, std::memory_order_release);\n  return v;\n}\n```\n\n## Follow-up Questions\n\n- How would you adapt this pattern for multiple producers/consumers?\n- What tests would you add to validate correctness across ARM/x86?","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hugging Face","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T13:10:06.434Z","createdAt":"2026-01-13T13:10:06.434Z"},{"id":"q-1578","question":"On a Linux server, four worker processes share a 100GB read-only memory-mapped dataset loaded from disk on demand via mmap. Describe the sequence of page fault handling, TLB behavior, and how the kernel page cache and optional swap interact with this pattern. Propose two concrete knobs to maximize throughput without starving others (e.g., MADV_WILLNEED with madvise, NUMA binding with mbind) and how you would measure success?","answer":"When a worker process accesses the memory-mapped dataset for the first time, the CPU generates a page fault. The kernel's page fault handler checks the Process Table Entry (PTE), finds the page not present, and initiates a block I/O operation to read the required page from disk into the page cache. Once the page is loaded, the kernel updates the PTE, invalidates the relevant TLB entries, and the process resumes execution. Subsequent accesses hit the page cache directly, and the TLB caches the virtual-to-physical translations for fast lookup. The kernel may evict less frequently used pages from the page cache under memory pressure, potentially writing them to swap if they were modified (though in this read-only scenario, swap usage would be minimal).\n\nTo maximize throughput without starving other processes, I recommend two concrete knobs:\n\n1. **MADV_WILLNEED with madvise**: Pre-fault frequently accessed pages into memory using `madvise(addr, length, MADV_WILLNEED)`. This hints to the kernel to load these pages proactively, reducing page fault latency during critical operations.\n\n2. **NUMA binding with mbind**: Bind the memory-mapped region to specific NUMA nodes using `mbind()` to optimize memory locality. This ensures that pages are allocated on the same NUMA node as the worker processes accessing them, reducing remote memory access latency.\n\nSuccess would be measured through metrics like reduced page fault rate (via `/proc/vmstat`), improved cache hit ratios, lower memory access latency, and sustained throughput under load.","explanation":"## Why This Is Asked\nTests understanding of OS memory management, paging, and NUMA in realistic workloads.\n\n## Key Concepts\n- Page fault handling, TLB, page cache\n- madvise hints and memory binding\n- THP trade-offs and NUMA effects\n\n## Code Example\n```c\n// C example for memory optimization\nint prefetch_dataset(void* addr, size_t len) {\n    // Pre-fault pages to reduce page fault latency\n    return madvise(addr, len, MADV_WILLNEED);\n}\n\nint bind_numa(void* addr, size_t len, int node) {\n    // Bind memory to specific NUMA node for locality\n    return mbind(addr, len, MPOL_BIND, &node, sizeof(node));\n}\n```","diagram":"flowchart TD\n  UserProcess[Worker Process] -->|mmap fault| Kernel[Page Fault Handler]\n  Kernel --> Disk[Disk I/O]\n  Disk --> Cache[Page Cache]\n  Cache --> TLB[TLB Update]\n  TLB --> UserProcess","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","DoorDash","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T05:56:33.159Z","createdAt":"2026-01-13T22:44:52.758Z"},{"id":"q-1733","question":"Scenario: After a fork, a child writes to a Copy-On-Write (COW) page. Describe the end-to-end kernel steps from the write fault to the point where the parent and child have separate views, including page table updates, TLB changes, and how the private copy is created and isolated from the parent's mapping?","answer":"On first write to a COW page, the kernel traps the fault, allocates a private page for the child, copies data from the shared page, updates the child's PTE to the new page with write permission, clear","explanation":"## Why This Is Asked\nTests understanding of Copy-On-Write, page tables, TLB, and the fork semantics at the OS boundary.\n\n## Key Concepts\n- Fork and Copy-On-Write\n- Page Table Entries (PTEs) and flags (present, read/write, dirty, COW)\n- TLB behavior and flushing\n- Paging and memory isolation between processes\n\n## Code Example\n```javascript\n// Demonstrative COW map usage (pseudo; actual OS calls abstracted)\nchar *p = mmap(NULL, 4096, 0, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);\n// fork and write path would trigger COW internals\n```\n\n## Follow-up Questions\n- How would behavior differ with MADV_DONTNEED or memory pressure?\n- How can you observe COW pages at runtime from user space?","diagram":"flowchart TD\n  A[Fork creates COW mappings] --> B[Child writes to page]\n  B --> C{Page fault on write}\n  C --> D[Allocate private child page]\n  C --> E[Copy data old -> new]\n  C --> F[Update child PTE to new page RW]\n  C --> G[Clear COW flag]\n  C --> H[TLB flush for child mapping]\n  D --> I[Child sees private copy]\n  I --> J[Parent mapping unchanged]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Discord","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T08:51:29.400Z","createdAt":"2026-01-14T08:51:29.400Z"},{"id":"q-1759","question":"In a multithreaded server, each worker maintains a per-thread stats counter in an array of N 64-byte structs (one per thread). A separate thread periodically sums these counters every second. Without padding, explain a concrete interleaving that leads to cache line false sharing and degraded throughput, and propose a fix (padding, alignas cache-line, or per-thread local counters plus a reduction) that preserves correctness and improves performance?","answer":"False sharing occurs when multiple threads write to nearby fields within a single cache line, causing repeated invalidations and cross-core traffic. If thread i updates its own counter in a shared lin","explanation":"## Why This Is Asked\nThe question probes practical understanding of cache coherence and false sharing in real multithreaded code.\n\n## Key Concepts\n- False sharing\n- Cache coherence protocols (MESI)\n- Data padding and alignment\n- Reduction patterns across threads\n\n## Code Example\n```c\ntypedef struct {\n  long long count;\n} PerThreadStat;\n```\n\n```c\ntypedef struct {\n  long long count;\n  char pad[64 - sizeof(long long)];\n} PerThreadStatPadded;\n```\n\n## Follow-up Questions\n- How would you measure throughput improvements after padding?\n- What changes if thread count isn’t a multiple of 64?","diagram":null,"difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:46:15.412Z","createdAt":"2026-01-14T09:46:15.412Z"},{"id":"q-2084","question":"In a kernel memory allocator with per-core freelists and a global free pool protected by a spinlock, describe a concrete interleaving that yields a use-after-free for a block still in use by a reader, and explain how either hazard pointers or epoch-based reclamation prevents it, including the required memory-order guarantees on x86-64 and how grace periods are enforced?","answer":"Thread A loads a block from its per-core freelist and begins use; Thread B, holding the pool lock, frees the same block into the global pool. A's subsequent access may hit freed memory or see partially corrupted data, creating a use-after-free vulnerability.","explanation":"## Why This Is Asked\nThis question tests safe memory reclamation in shared-data paths, especially with per-core freelists and a global pool. It probes understanding of use-after-free scenarios, memory ordering guarantees, and the trade-offs between hazard pointers and epoch-based reclamation mechanisms.\n\n## Key Concepts\n- Hazard pointers\n- Epoch-based reclamation\n- Grace period enforcement\n- Per-core freelists\n- Spinlock vs lock-free interaction\n- Cache coherence and memory barriers\n\n## Code Example\n```\nBlock *blk = percore_freelist_pop();\nuse(blk);\nlock(global_lock);\nfree_block(blk);\nunlock(global_lock);\n```","diagram":"flowchart TD\n  A[Reader holds reference to block] --> B[Block freed under pool lock]\n  B --> C[Block enters global pool]\n  C --> D[Reader may still access block]\n  D --> E[Grace period / hazard pointers delay reclaim]\n","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Netflix","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:31:06.509Z","createdAt":"2026-01-14T23:35:52.358Z"},{"id":"q-2220","question":"Scenario: A process has two threads: T1 holds a mutex to update a shared log buffer; T2 is blocked trying to acquire the same mutex. A SIGINT is delivered to the process while T1 is in the middle of updating. Explain the sequence of events from signal delivery to termination, including the mutex state, potential race conditions, and safe patterns for signal handling in multi-threaded programs (like async-signal-safe handlers and deferred cleanup via a dedicated signal-handling thread or self-pipe)?","answer":"Signals target a single thread; if that thread holds a mutex, the handler runs with the mutex still held, risking deadlock or data corruption unless the handler uses only async-signal-safe calls. Prac","explanation":"## Why This Is Asked\nAssess understanding of signal semantics in multi-threaded processes and safe cleanup patterns.\n\n## Key Concepts\n- Signals and thread-level delivery\n- Async-signal-safety and mutex interactions\n- Self-pipe trick and deferred cleanup\n\n## Code Example\n```javascript\n#include <signal.h>\n#include <unistd.h>\n#include <stdlib.h>\n#include <stdio.h>\n\nint pipefd[2];\n\nvoid sigint(int s){\n  char c = 1;\n  write(pipefd[1], &c, 1);\n}\n\nint main(){\n  pipe(pipefd);\n  struct sigaction sa;\n  sa.sa_handler = sigint;\n  sigemptyset(&sa.sa_mask);\n  sa.sa_flags = 0;\n  sigaction(SIGINT, &sa, NULL);\n\n  char buf;\n  while (read(pipefd[0], &buf, 1) > 0) {\n    // termination requested\n  }\n  return 0;\n}\n```\n\n## Follow-up Questions\n- What happens if multiple signals arrive rapidly?\n- How would you implement a robust termination sequence in a real program?","diagram":"flowchart TD\n  Signal Arrival --> TargetThread\n  TargetThread --> AsyncSafe{In async-signal-safe?}\n  AsyncSafe -->|Yes| QuickCleanup\n  AsyncSafe -->|No| DeferCleanup","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T07:47:33.249Z","createdAt":"2026-01-15T07:47:33.250Z"},{"id":"q-2280","question":"In a simplified OS with a file-backed page cache, a process repeatedly calls read() to fetch 4 KiB blocks from a file. If a 4 KiB block is not in the file system page cache, describe step-by-step what the kernel does from read() entry to user-space return, including page-cache lookup, block-device reads, potential read-ahead, and how the data ends up in the user buffer. Compare a cache miss vs a cache hit path and timing?","answer":"On a miss, read() triggers a page-cache lookup miss, the kernel issues a disk I/O for the file block, loads the page into the page cache, updates inode/page-table metadata, wakes the waiting process, ","explanation":"## Why This Is Asked\nTests understanding of the buffered I/O path and page cache interaction in a simple OS.\n\n## Key Concepts\n- File-backed page cache\n- Read path for misses vs hits\n- Read-ahead optimization and trade-offs\n- Data copy to user buffer and I/O completion\n\n## Code Example\n```javascript\n// Pseudo path (illustrative, not real code)\nfunction read(fd, buf, count) {\n  if (cacheHasBlock(fd)) {\n    copyFromCache(fd, buf, count)\n  } else {\n    submitDiskRead(fd)\n    waitForIO()\n    copyFromCache(fd, buf, count)\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test the timing difference between hit and miss paths?\n- How does read-ahead decide which blocks to fetch and when to disable it?","diagram":"flowchart TD\n  A[read() entry] --> B[lookup page cache]\n  B --> C{hit?}\n  C -- Yes --> D[copy to user buffer]\n  C -- No --> E[submit disk read]\n  E --> F[load into page cache]\n  F --> D","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T10:40:24.723Z","createdAt":"2026-01-15T10:40:24.723Z"},{"id":"q-2404","question":"A multi-threaded app writes to a single file via buffered I/O on a journaling filesystem. Thread A appends 64KiB blocks; Thread B occasionally calls fsync(). Describe the path from write through the page cache to disk, including journal commits and barriers, and give a concrete interleaving where a crash can lose data or leave metadata only. Propose a minimal patch to guarantee durability (e.g., fdatasync after writes)?","answer":"Path: write sits in page cache; fsync flushes data blocks + metadata to disk with barrier ordering. Race: writer appends 64KiB, then crash before journal commit completes; data may be lost or metadata","explanation":"## Why This Is Asked\n\nThis probes understanding of durability semantics in buffered I/O with journaling filesystems, including the roles of the page cache, journal commits, and barriers. It also checks practical mitigation strategies.\n\n## Key Concepts\n\n- Data vs metadata flush semantics and barriers\n- Journal commit ordering and writeback paths\n- fdatasync, fsync, and O_SYNC tradeoffs in throughput vs durability\n\n## Code Example\n\n```javascript\n// Pseudocode: durability after write\nconst n = write(fd, buf, len);\nif (n === len) {\n  fdatasync(fd);\n}\n```\n\n## Follow-up Questions\n\n- How would you test durability guarantees under simulated crashes?\n- Compare fdatasync vs O_SYNC in a high-throughput server and discuss performance implications.","diagram":"flowchart TD\n  A[64KiB write to page cache] --> B[page cache flush]\n  B --> C{fsync called?}\n  C -->|yes| D[journal + data flushed to disk with barriers]\n  C -->|no| E[crash may lose data or leave metadata only]","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Databricks","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T16:54:08.605Z","createdAt":"2026-01-15T16:54:08.605Z"},{"id":"q-2477","question":"Explain how Linux handles a write to a 2MB Transparent Huge Page (THP) that only partially overlaps with a 4KB region. Describe the fault path, the split_huge_page path (and khugepaged if applicable), PTE updates, TLB shootdowns, and the performance implications vs pre-splitting THPs?","answer":"On a write, a page fault splits the 2MB THP into 512 4KB pages via split_huge_page, coordinated by khugepaged or on-demand. The kernel copies the write into the new 4KB page, updates the PTEs to point","explanation":"## Why This Is Asked\nTests understanding of THP in Linux, on-demand page splitting, and the impact on performance when only a small region is written.\n\n## Key Concepts\n- Transparent Huge Pages (THP) and 4KB subpage mapping\n- split_huge_page and khugepaged coordination\n- Page Table Entries (PTEs) updates and COW semantics\n- TLB shootdowns across CPUs and performance impact\n\n## Code Example\n```c\n// high-level sketch of handling a THP write fault\nint handle_thp_write(struct vm_area_struct *vma, unsigned long addr) {\n  if (is_hugepage(addr)) {\n    split_huge_page(vma, addr);\n    // fault restarts, now maps 4KB pages\n  }\n  // mark dirty, proceed with normal write\n  return 0;\n}\n```\n\n## Follow-up Questions\n- How does THP splitting interact with NUMA memory policies?\n- What are the trade-offs of aggressive THP splitting vs keeping large pages for workload with mixed write sizes?","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T19:39:45.795Z","createdAt":"2026-01-15T19:39:45.795Z"},{"id":"q-2544","question":"Scenario: A process memory-maps a 4 KiB device block via mmap with MAP_SHARED and then writes through a user pointer that maps to that page. Explain end-to-end how the kernel handles the write, including the MMU page fault, page cache interaction, dirty bit propagation, writeback, and how the data reaches the physical device, highlighting synchronization with other mappings and potential coherence issues?","answer":"On a MAP_SHARED mmap write, the kernel first handles a page fault if the page isn't present, loading it from the backing file or device. The kernel then maps the page into the process's address space, updates the page tables, and refreshes the TLB. When the user writes through the pointer, the data is copied directly into the page cache entry. The kernel marks the page as dirty in its page cache metadata and sets the dirty bit in the page table entry. The same page cache entry serves all mappings of this file region, ensuring coherence between different processes. The kernel later schedules writeback, where the dirty page is written to the physical block device through the block I/O layer, potentially involving the device's own write cache.","explanation":"## Why This Is Asked\n\nTests understanding of mmap, page cache, and writeback in a realistic I/O path that combines memory management and filesystem semantics.\n\n## Key Concepts\n\n- MAP_SHARED semantics and page cache sharing\n- Page fault handling and page table updates\n- Page cache interaction and dirty bit propagation\n- Writeback path and block device I/O\n- TLB and PTE coherence across mappings\n\n## Code Example\n\n```c\n// user-space example (high-level)\nchar *p = mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0);\nstrncpy(p, data, len);  // triggers the full kernel path described above\n```","diagram":"flowchart TD\n  A[User write via mmap] --> B[Page fault or hit in page cache]\n  B --> C[Copy to page and mark dirty]\n  C --> D[Schedule writeback to block device]\n  D --> E[Device updated; other mappings see data]\n  E --> F[TLB/PTE updated to reflect dirty page]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","OpenAI","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:33:07.635Z","createdAt":"2026-01-15T22:31:53.942Z"},{"id":"q-2714","question":"In a high-throughput key-value service on Linux/x86_64, two threads share a 128-byte struct: a 64-bit value and a 64-bit sequence counter used for a sequence lock (seqlock). Describe how to implement a lock-free reader with a writer that updates the value safely, specifying the exact operation order, required memory barriers, and how you verify correctness under contention. Compare against a mutex approach and practical tradeoffs?","answer":"Use a seqlock: writer increments seq to odd, writes value, uses a memory barrier, then increments seq to even. Reader samples seq, reads value, re-samples seq, and repeats if seq changed or is odd. Us","explanation":"## Why This Is Asked\nTests lock-free synchronization understanding, memory ordering, and practical tradeoffs for low-latency services.\n\n## Key Concepts\n- Sequence locks\n- Memory ordering (release/acquire, barriers)\n- Contention and livelock vs mutex\n\n## Code Example\n```c\n// sketch: seqlock usage\n```\n\n## Follow-up Questions\n- How would you extend to 128-bit values and non-scalar data?\n- How to measure correctness and contention in a microbenchmark?","diagram":"flowchart TD\n  W[Writer: update value] --> S{seq++ to odd}\n  S --> D[Write value]\n  D --> E{seq++ to even}\n  R[Reader: sample seq] --> V[Read value]\n  V --> R2[Resample seq]\n  R2 -->|unchanged and even| OK\n  R2 -->|changed or odd| RETRY","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Instacart","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T07:47:24.528Z","createdAt":"2026-01-16T07:47:24.528Z"},{"id":"q-2734","question":"In a Linux‑like system, a user‑space producer and a kernel driver share a lock‑free 256‑entry ring buffer using per‑entry sequence numbers. Describe a safe publish/claim/consume protocol that guarantees no lost entries and a consistent view under concurrent updates, specifying the exact write order and memory barriers on x86‑64. Include a concrete test harness to validate under contention?","answer":"Use per-slot sequence numbers and release/acquire semantics. Producer writes data, then performs a release write of slot.seq to signal readiness; consumer reads slot.seq with acquire, validates, then ","explanation":"## Why This Is Asked\nProbes practical lock-free IPC correctness between user space and kernel with memory ordering.\n\n## Key Concepts\n- Lock-free ring buffers with per-slot sequence numbers\n- Release/acquire semantics on x86-64\n- Visibility guarantees under contention\n- Test harness design for correctness\n\n## Code Example\n```c\n// sketch of publish/consume\n```\n\n## Follow-up Questions\n- How to extend to multiple kernel consumers?\n- How would you detect/repair corrupted seq invariants?\n","diagram":"flowchart TD\n  A[User-Space Producer] --> B[Kernel Driver]\n  B --> C[Lock-Free Ring Buffer]\n  C --> D[Consumer Reads Entry]\n  D --> A","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T09:45:31.982Z","createdAt":"2026-01-16T09:45:31.982Z"},{"id":"q-2943","question":"Describe a robust strategy for a parent process to collect exit statuses of multiple forked children in Linux. Include waitpid usage in a loop, handling of SIGCHLD, and zombie prevention. Also cover edge cases like stopped/continued children and race-free accounting?","answer":"Use a parent loop to reap children without races. Method A: in main thread, repeatedly call waitpid(-1, &status, 0) and process each pid; break when returns -1 with errno==ECHILD. Method B: install a ","explanation":"## Why This Is Asked\nTests understanding of process lifecycle, waitpid usage, and zombie avoidance in a practical multi-child scenario.\n\n## Key Concepts\n- waitpid semantics and looping\n- SIGCHLD handling and reaping\n- WNOHANG and non-blocking variants\n- Zombie processes and edge cases (stopped/continued)\n\n## Code Example\n```javascript\n#include <sys/types.h>\n#include <sys/wait.h>\n#include <unistd.h>\n#include <errno.h>\n\nint main() {\n  // spawn N children ...\n  int finished = 0; int status;\n  while (finished < N) {\n    pid_t pid = waitpid(-1, &status, 0);\n    if (pid > 0) { finished++; /* process status */ }\n    else if (pid == -1 && errno == ECHILD) break;\n  }\n  return 0;\n}\n```\n\n## Follow-up Questions\n- How would you adapt this for SIGCHLD storms or large N?\n- What are the risks of ignoring zombie cleanup in long-running daemons?","diagram":"flowchart TD\n  Start([Start]) --> Fork[N children]\n  Fork --> Wait[Wait for children]\n  Wait --> Done{All finished?}\n  Done -- Yes --> End([End])\n  Done -- No --> Reap[Reap with waitpid or handler]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T18:48:16.280Z","createdAt":"2026-01-16T18:48:16.280Z"},{"id":"q-2966","question":"Scenario: A latency‑sensitive service pins 32 worker threads to NUMA node 0 on a dual‑node server. A background writer allocates a large shared ring buffer used by all workers, but allocator fragmentation causes many pages to reside on Node 1. Explain how page allocation, NUMA policies, and TLB coherence interact to produce remote misses and increased cross‑node traffic. Propose a minimal policy (e.g., mbind/sysfs knobs or numactl) to keep hot data local and describe validation via microbenchmarks?","answer":"Remote pages cause extra page walks, higher TLB misses, and cross-node memory bandwidth when data sits on Node 1. Bind the ring buffer to Node 0 (mbind, numactl) and allocate producer/consumer data to","explanation":"## Why This Is Asked\n\nTests understanding of NUMA locality, page allocator behavior under fragmentation, and how TLB coherence affects latency in multi‑node setups; requires concrete mitigation strategies rather than abstract theory.\n\n## Key Concepts\n\n- NUMA memory locality and policy\n- Page allocation and migration\n- TLB coherence and remote memory traffic\n- mbind, numactl, and policy choices\n- Microbenchmark validation of locality\n\n## Code Example\n\n```bash\nnumactl --cpunodebind=0 --membind=0 ./latency_test\n```\n```\n\n## Follow-up Questions\n\n- What if background writer uses large pages to reduce metadata overhead?\n- How would you monitor remote memory traffic with perf or pidstat, and validate improvements across workloads?","diagram":"flowchart TD\n  A[Ring buffer allocation] --> B{Locality?}\n  B -- Local (Node 0) --> C[Fast local accesses]\n  B -- Remote (Node 1) --> D[Remote accesses, TLB misses]\n  D --> E[Increased cross-node bandwidth, latency]\n  E --> F[Validate with microbenchmarks]","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T19:28:36.151Z","createdAt":"2026-01-16T19:28:36.151Z"},{"id":"q-3028","question":"In Linux, two processes map the same 4 KiB file region with MAP_SHARED. One writer updates eight bytes; a reader loops to read the same region concurrently. Describe end-to-end what happens from the write through the page cache to the reader, including dirty/writeback, cache coherence, and TLB; then propose a minimal synchronization mechanism (e.g., a 4-byte lock or seqlock in the shared mapping) to guarantee a predictable visibility order and discuss tradeoffs?","answer":"End-to-end: Process A writes to a MAP_SHARED mmap page; the update propagates to the page cache and marks the page dirty, then the kernel may write it back during writeback or page eviction. CPU cache coherence ensures Process B sees changes when they reach its cache, while TLB entries remain valid since the mapping doesn't change. The writer's modification becomes visible to the reader through hardware cache coherence, though timing varies based on writeback policies and cache coherence latency.\n\nFor minimal synchronization, implement a 4-byte lock in the shared mapping: Process A atomically acquires the lock using compare-and-swap, performs the 8-byte write, then releases the lock with a store. Process B checks the lock state before reading, ensuring it observes updates in the correct order. Alternatively, use a seqlock where readers check sequence numbers before and after reading to detect concurrent updates.","explanation":"## Why This Is Asked\nThis tests understanding of inter-process sharing via mmap, page cache semantics, and memory visibility without user-space locking.\n\n## Key Concepts\n- MAP_SHARED page mappings\n- Page cache dirty bits and writeback\n- Cache coherence and TLB effects\n- Lightweight synchronization in shared memory (CAS, acquire/release)\n\n## Code Example\n```c\n// Pseudo: writer uses CAS to acquire lock, writes, then releases\nuint32_t *lock = (uint32_t*)addr; // shared region\nwhile (__sync_lock_test_and_set(lock, 1)) {}\n// write 8 bytes to region after lock\nmemcpy((void*)(addr+8), data, 8);\n__sync_lock_release(lock);\n\n// Reader checks lock before reading\nwhile (*lock) {} // wait for writer\nmemcpy(data, (void*)(addr+8), 8);\n```\n\n## Tradeoffs\nLock: Simple, ensures exclusive access but blocks readers during writes.\nSeqlock: Readers can proceed concurrently but must retry if write occurs mid-read; better for read-heavy workloads.","diagram":null,"difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Bloomberg","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T05:43:40.674Z","createdAt":"2026-01-16T21:44:28.984Z"},{"id":"q-3184","question":"Scenario: A two-socket NUMA Linux server uses a shared 8 GiB mmap region for request data. Threads on both sockets touch pages frequently; explain how NUMA balancing, page migration, TLB shootdowns, and cache coherence interact to affect latency. Propose a concrete microbenchmark to measure cross-node memory access and a pragmatic fix (pin memory to local nodes or adjust policies) with expected trade-offs?","answer":"On a two-socket NUMA Linux server, cross-node touches trigger page migration, TLB shootdowns, and cache coherence traffic, causing latency spikes and lower throughput under bursts. A microbenchmark sh","explanation":"## Why This Is Asked\n\nThe interviewer wants to see understanding of NUMA locality, kernel memory management, and real-world performance tuning under contention.\n\n## Key Concepts\n\n- NUMA locality and page migration\n- TLB shootdowns and cache coherence traffic\n- Memory policy and tooling (numactl, libnuma)\n- Trade-offs of pinning vs balancing\n\n## Code Example\n\n```javascript\n// Conceptual: pin allocation to node 0 (illustrative)\nint policy = 0; // placeholder\n// mbind/numa APIs would be used in C to pin memory to a node\n```\n\n## Follow-up Questions\n\n- How would you design a microbenchmark to separate CPU-bound vs memory-bound effects?\n- What happens if you disable NUMA balancing during peak load?","diagram":"flowchart TD\n  A[Worker on Node0 touches local page] --> B[Page already local]\n  B --> C[Low latency]\n  A --> D[Worker on Node1 accesses remote page]\n  D --> E[Page migration & TLB shootdown]\n  E --> F[Latency spike]","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T05:43:45.371Z","createdAt":"2026-01-17T05:43:45.372Z"},{"id":"q-3243","question":"Explain a concrete race in a Linux-like kernel when a memory-mapped page is faulted in while a concurrent writeback evicts the same page. How does page cache coherence, dirty flags, and TLB invalidation interact to yield a stale read or lost write? Propose a minimal fix to guarantee visibility without sacrificing performance?","answer":"A concrete race: a reader faults in a mmap’d page while a writer’s writeback evicts that same page, yielding a stale read. Coherence relies on page dirty flags, writeback, and cache invalidation on fa","explanation":"## Why This Is Asked\nTests understanding of page fault paths, page cache coherence, TLB/invalidation, and race conditions between page cache eviction and fault handling.\n\n## Key Concepts\n- Page cache lifecycle (cache hits, misses, eviction)\n- Page fault handling and faulting threads\n- TLB invalidation and cache coherence across cores\n\n## Code Example\n```javascript\n// Simplified fault path sketch (not actual kernel code)\nfunction handleMmapFault(page, proc) {\n  if (!page.inCache) {\n    loadPageIntoCache(page, proc);\n  }\n  // else, return cached data\n}\n```\n\n## Follow-up Questions\n- How would you implement per-page pinning to prevent eviction during fault handling without harming performance?\n- How would you test this race with stress tests and performance counters?","diagram":"flowchart TD\nA[Reader faults mmap page] --> B{Page present?}\nB -- Yes --> C[Read from page cache]\nB -- No --> D[Page fault handler fetches page]\nD --> E[Page loaded into cache]\nE --> F[Resume read; invalidate TLB if needed]","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Microsoft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T08:39:36.711Z","createdAt":"2026-01-17T08:39:36.711Z"},{"id":"q-3490","question":"Explain the exact behavior and safety implications when one process mmap()s a file region with MAP_SHARED, and another process truncates the same file while the mapping exists; detail what happens on access at an offset beyond the new end, which kernel data structures are involved, and minimal steps to guarantee correctness in a multi-process program?","answer":"Serialization is required. If truncation and access race, the mapping may serve stale data or fault with SIGBUS. A safe pattern is to lock around both operations, unmap the region before truncation an","explanation":"## Why This Is Asked\nTests understanding of mmap, page cache, locking, and cross-process synchronization when file size changes.\n\n## Key Concepts\n- MAP_SHARED semantics\n- Page cache invalidation and TLB coherence\n- Synchronization patterns: locks, munmap/remap, msync\n- Truncation behavior and SIGBUS vs stale data\n\n## Code Example\n```javascript\n// Example: pseudo C-like code showing lock around ftruncate and remap\nint fd = open(\"data.bin\", O_RDWR);\npthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;\npthread_mutex_lock(&lock);\nvoid *p = mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0);\n// reader uses p...\n// Truncator later:\nftruncate(fd, 2048); // shorten file\nmunmap(p, 4096);\n// remap after truncation\np = mmap(NULL, 2048, PROT_READ|PROT_WRITE, MAP_SHARED, fd, 0);\npthread_mutex_unlock(&lock);\n```\n\n## Follow-up Questions\n- How would you adapt this pattern for IPC via memory-mapped files?\n- What changes on Windows for similar semantics?","diagram":"flowchart TD\nA(Process A maps region) --> B(Process B truncates) \nB --> C{Access occurs concurrently?}\nC --> D[Page cache may be stale or trigger fault]\nD --> E[TLB/page-table coherence considerations]\nE --> F[Use munmap/remap with locking to guarantee correctness]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T18:47:16.702Z","createdAt":"2026-01-17T18:47:16.702Z"},{"id":"q-3525","question":"Design a lock-free bounded ring buffer for 1 producer and 1 consumer sharing memory. Each slot stores a 64-byte payload and a 64-bit sequence. The producer writes payload, then updates the sequence to indicate new data; the consumer reads only when the sequence indicates fresh data, then advances. Describe exact write/read order, x86-64 memory barriers, wraparound handling, and empty/full detection without locks?","answer":"Use a bounded ring with per-slot 64-bit sequence counters. Producer writes payload, then increments the slot's sequence with a release store and advances the tail. Consumer uses an acquire load of the","explanation":"## Why This Is Asked\nAssesses practical lock-free IPC, memory ordering, and correctness under contention. Tests knowledge of release/acquire semantics, false sharing avoidance, and wraparound handling in a real kernel-like primitive.\n\n## Key Concepts\n- Lock-free synchronization\n- Acquire/release memory order on x86-64\n- Per-slot sequencing and wraparound handling\n- False sharing mitigation via padding\n\n## Code Example\n```javascript\n// Pseudocode illustrating the producer side (simplified)\nslot.seq.store(slot.seq.read() + 1, RELEASE);\nslot.payload = data;\nslot.seq.store(slot.seq.read(), RELEASE);\n```\n\n```javascript\n// Pseudocode illustrating the consumer side (simplified)\nlet s = slot.seq.load(ACQUIRE);\nif (s % 2 === 1) {\n  let data = slot.payload;\n  // process data\n  slot.seq.store(s + 1, RELEASE);\n}\n```\n\n## Follow-up Questions\n- How would you extend to multiple producers/consumers while preserving correctness?\n- What diagnostics would you add to detect stalled or stuck queues under CPU throttling?","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T19:37:49.782Z","createdAt":"2026-01-17T19:37:49.782Z"},{"id":"q-3554","question":"In a Linux-like OS using io_uring for async I/O, an 8-core NUMA system with a shared submission/completion ring handles bursts of 4KiB reads to NVMe. A per-I/O sequence is proposed; describe a concrete interleaving that could cause a completion to be lost or delivered out of order, and specify the memory ordering needed to guarantee in-order, at-least-once completions. Propose a minimal fix (e.g., per-I/O sequence numbers) and discuss performance trade-offs?","answer":"In a high-throughput io_uring scenario, a problematic interleaving occurs when multiple cores submit I/O operations during burst conditions. Specifically, Core A submits a 4KiB read, increments the per-I/O sequence counter, and posts completion before the data is fully visible in the user buffer. Meanwhile, Core B reuses the same submission slot, potentially overwriting the completion ring entry before Core A's consumer processes it. This creates two failure modes: lost completions when ring entries are overwritten, and out-of-order delivery when memory barriers are insufficient across NUMA boundaries.\n\nTo guarantee in-order, at-least-once completions, you must establish proper memory ordering: release semantics on the producer side after writing completion data, acquire semantics on the consumer side before reading completion data, and atomic synchronization of completion ring head/tail indices. The minimal fix introduces per-I/O sequence numbers with atomic compare-and-swap operations on completion ring updates, ensuring each completion is uniquely identifiable and processed exactly once.","explanation":"## Why This Is Asked\nThis question tests deep understanding of io_uring's concurrency model, memory ordering semantics, and real-world race conditions in high-performance I/O systems.\n\n## Key Concepts\n- io_uring submission/completion ring ordering semantics\n- Cross-core memory barriers and NUMA-aware synchronization\n- Per-I/O sequencing for completion tracking\n- Ring buffer management under burst conditions\n- Atomic operations for completion ring updates\n\n## Code Example\n```c\n// Pseudo-C outline showing proper barriers\nstruct io_slot *s = &ring->slots[prod_head];\n\n// Ensure data is visible before updating ring indices\natomic_store_explicit(&s->data, completion_data, memory_order_release);\natomic_store_explicit(&ring->prod_tail, new_tail, memory_order_release);\n\n// Consumer side with acquire semantics\nuint32_t tail = atomic_load_explicit(&ring->prod_tail, memory_order_acquire);\ncompletion_data = atomic_load_explicit(&s->data, memory_order_acquire);\n```\n\n## Performance Trade-offs\nPer-I/O sequence numbers add minimal overhead (8 bytes per operation) but provide strong correctness guarantees. The atomic operations introduce ~5-10ns latency per completion, which is negligible compared to NVMe access times (~100μs). However, under extreme burst conditions, contention on the completion ring indices can become a bottleneck, potentially requiring sharding or per-core completion queues.","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","IBM","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:50:20.690Z","createdAt":"2026-01-17T21:26:05.837Z"},{"id":"q-3614","question":"On a Linux-like kernel using hazard-pointer-based reclamation for a shared in-memory index, writers replace entries by atomic pointer swaps and reclaim old nodes after a grace period. Describe a concrete interleaving on a 2-socket NUMA system that could cause a reader to observe a newly published but uninitialized node, and explain the required memory ordering to prevent it (publish with release, read with acquire, or use a ready flag). Propose a minimal fix and discuss trade-offs?","answer":"An interleaving: a writer performs a compare-and-swap to publish a new node pointer before fully initializing its fields; a reader loads the pointer and dereferences it before initialization completes, reading partially initialized data. The fix is to publish with release semantics, read with acquire semantics, or use a ready flag.","explanation":"## Why This Is Asked\nTests understanding of hazard-pointer reclamation, publication-order guarantees, and cross-CPU memory ordering under NUMA. It probes correctness in lock-free updates and safe reclamation with real-world implications.\n\n## Key Concepts\n- Hazard pointers and grace periods\n- Atomic publish vs initialization order\n- Release/acquire memory barriers\n- NUMA visibility and cross-node ordering\n- Safe reclamation trade-offs\n\n## Code Example\n```javascript\n// C-like pseudo\nstruct Node { int data; int ready; };\nNode* n = alloc();\nn->data = 123;\n// no barrier yet\ncas(&head, old, n); // publish before initialization\n```\n\n## Trade-offs\n- Release/acquire: minimal overhead but requires careful ordering\n- Ready flag: simpler logic but adds extra field and check\n- Full fence: strongest guarantees but highest performance cost","diagram":"flowchart TD\n  A[Writer allocates new node] --> B[Initialize fields]\n  B --> C[Publish pointer (CAS)]\n  C --> D[Old node reclaimed after grace period]\n  E[Reader loads head pointer] --> F[Reads ready flag]\n  F --> G[If ready==1, reads valid data]\n  F --> H[If ready==0, waits or retries]","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Meta","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:14:52.278Z","createdAt":"2026-01-17T23:37:38.642Z"},{"id":"q-3625","question":"In a Linux kernel driver for a PCIe DMA device, the host uses a shared descriptor ring with the device. A producer thread writes descriptors and updates a head pointer; the device consumes descriptors and raises an interrupt on completion. Describe a concrete interleaving that could yield a lost or out-of-order completion if there is no proper synchronization between producer writes and device processing. Propose a minimal fix using per-descriptor sequence numbers and memory barriers (e.g., smp_wmb, dma_wmb) and explain how this guarantees in-order, at-least-once completion, including cache coherence and DMA semantics?","answer":"Interleaving: producer writes desc0, then desc1, then updates head to 1; device may observe head=1 and fetch desc1 before desc0 is visible due to missing barriers. Fix: add a per-descriptor seq field; device validates seq matches expected index before processing, using dma_wmb() before seq write and smp_wmb() before head update to ensure proper ordering.","explanation":"## Why This Is Asked\nTests concrete reasoning about DMA rings, ordering guarantees, and barrier semantics in drivers.\n\n## Key Concepts\n- PCIe DMA descriptor rings, producer/consumer synchronization\n- Per-descriptor sequence numbers, publish semantics\n- Memory barriers (dma_wmb, smp_wmb) and DMA coherence\n- Cache lines and visibility between CPU and device\n\n## Code Example\n```c\n// pseudo-C like\nstruct desc { uint64_t addr; uint32_t len; uint32_t flags; uint64_t seq; };\ndesc[i].addr = data; \ndesc[i].len = n; \ndma_wmb();\ndesc[i].seq = i; \nsmp_wmb();\nhead = i; // publish\n```\n\n## Follow-up Questions\n- How would you handle sequence number wraparound?\n- What if the device doesn't support sequence numbers?","diagram":"flowchart TD\n  A[Producer writes descriptor] --> B[Publish with barriers]\n  B --> C[Device sees descriptor]\n  C --> D[Device completes descriptor]\n  D --> E[Interrupt signals completion]\n  E --> F[Host validates sequence numbers]","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Google","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:08:17.913Z","createdAt":"2026-01-18T02:29:02.980Z"},{"id":"q-3764","question":"Scenario: A 10 Gbps NIC with multiple receive queues experiences a burst that floods the RX path. Explain the end-to-end path from RX interrupt to user-space delivery, including hardirq, NAPI polling, and the spillover to ksoftirqd, and how CPU affinity and irqbalance affect latency. Provide concrete measurement steps and mitigations (NAPI budget, backlog tuning, per-queue IRQ affinity, CPU isolation)?","answer":"End-to-end: RX interrupt -> hardirq -> NAPI poll; if burst exceeds budget, remaining packets queued to ksoftirqd, raising latency. Measure with ftrace, perf, and latency tests. Mitigations: enable pro","explanation":"## Why This Is Asked\nTests understanding of network I/O paths and how backlog affects latency in practice.\n\n## Key Concepts\n- RX interrupt handling, hardirq vs softirq\n- NAPI, poll budget, and ksoftirqd\n- IRQ affinity,CPU isolation, and irqbalance\n- Latency measurement techniques\n\n## Code Example\n\n```bash\n# Example: monitor per-queue backlog and interrupts\nwatch -n1 'cat /proc/interrupts; cat /proc/net/softnet_stat'\n```\n\n## Follow-up Questions\n- How would you diagnose a regression after enabling IRQ affinity?\n- What kernel parameter changes would you test first on a live system?","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Snap","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T08:44:57.605Z","createdAt":"2026-01-18T08:44:57.605Z"},{"id":"q-3892","question":"Two processes write concurrently to the same file opened with O_APPEND. Describe end-to-end how the kernel ensures atomic appends, what can cause atomicity to fail (buffered I/O, network FS), and how you would guarantee visibility and ordering in practice?","answer":"On a Linux-like kernel, O_APPEND makes each write execute atomically with respect to other O_APPEND writes by updating the per-file offset in an atomic operation and appending the data to the inode bu","explanation":"## Why This Is Asked\nTests understanding of atomic file semantics, VFS buffering, and real-world filesystem behavior under concurrent writes.\n\n## Key Concepts\n- Atomic appends with O_APPEND\n- VFS inode buffering and writeback\n- fsync/fdatasync for visibility guarantees\n- Atomicity caveats on network/remote FS\n\n## Code Example\n```c\nint fd = open(\"data.log\", O_WRONLY | O_APPEND);\nssize_t n = write(fd, buf, len);\n// Expect write to append atomically with respect to other O_APPEND writes\n```\n\n## Follow-up Questions\n- How would you test atomicity across a network filesystem? \n- What changes when using O_SYNC or writing via a dedicated append-logging mechanism?","diagram":"flowchart TD\n  A[Proc A write] --> B[VFS append path]\n  B --> C[Inode append buffer]\n  C --> D[Disk write]\n  E[Proc B write] --> F[VFS append path]\n  F --> G[Inode append buffer]\n  G --> D\n  H[Reader observes data] --> D","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Instacart","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T13:56:18.302Z","createdAt":"2026-01-18T13:56:18.302Z"},{"id":"q-4229","question":"On a Linux host running multiple containers, memory pressure spikes and kswapd starts reclaiming pages while some containers still serve high request rates. Describe the full sequence from a user-space page fault to eviction, including page cache lookup, anonym vs file-backed handling, reclaim, writeback, and LRU state movement. Explain how THP interacts with this path and the trade-offs of swapping vs not swapping under pressure?","answer":"Fault -> page walk → page cache miss → allocate page and populate (anon/file-backed); under pressure, kswapd reclaims by scanning Active/Inactive LRU, buffers unevictables, shrinks pages, writes dirty","explanation":"## Why This Is Asked\nThis tests deep understanding of Linux memory reclaim in multi-tenant, containerized workloads, including page cache, swap, and THP interactions.\n\n## Key Concepts\n- kswapd, LRU, shrinkers, page cache maintenance\n- Anonymous vs file-backed pages, writeback, and dirty page reclaim\n- Transparent Huge Pages (THP) impact on reclaim and fragmentation\n- Swap vs no-swap trade-offs and memory-cgroup effects\n\n## Code Example\n```c\n/* simplified sketch: trigger mempressure, observe reclaim */\nwhile (1) {\n  munmap(...);\n  // force a reclaim observation point\n}\n```\n\n## Follow-up Questions\n- How to diagnose reclaim stalls with slabtop/vmstat? \n- What knobs optimize container memory pressure without swap? \n- How does compaction interact with THP under memory pressure?","diagram":"flowchart TD\n  A(User Fault) --> B[Page Cache Lookup]\n  B --> C{Hit}\n  C -- Yes --> D[Return Data to User]\n  C -- No --> E[Allocate Page & Populate]\n  E --> F{Memory Pressure}\n  F -- Yes --> G[kswapd reclaim & shrinkers]\n  G --> H[Writeback & Evict to Disk/Swap]\n  H --> D\n  F -- No --> D","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Airbnb","Citadel","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T09:44:49.511Z","createdAt":"2026-01-19T09:44:49.511Z"},{"id":"q-4258","question":"Explain, at a practical level, the sequence of events when a process touches a stack address just beyond the current stack limit and triggers a stack-growth fault in a simple Linux-like VM. Include how the kernel decides to grow the stack, allocates and zeros the new page, updates the user page table, adjusts the stack pointer, and invalidates the TLB. Mention edge cases such as concurrent growth and guard-page handling?","answer":"Describe the fault path: the trap handler checks the fault address against the process's stack growth policy and RLIMIT_STACK; if allowed, allocate a 4KB page, zero it, map it at the next lower addres","explanation":"## Why This Is Asked\n\nStack growth is a core OS mechanism; understanding it reveals how memory safety and performance interact in practice.\n\n## Key Concepts\n\n- Stack growth policy and guard pages\n- RLIMIT_STACK and address space layout\n- Page fault handling and page-table updates\n- TLB invalidation and re-execution\n- Concurrency with multiple threads touching stack boundaries\n\n## Code Example\n\n```javascript\n// high-level pseudo\nif (fault_addr in stack && allowed) {\n  pa = alloc_page();\n  map(p, pa, fault_addr_page, 'RW');\n  zero_page(pa);\n  flush_tlb(p, fault_addr_page);\n} else {\n  handle_fault_error();\n}\n```\n\n## Follow-up Questions\n\n- How would you test stack growth under multithreaded conditions?\n- What changes would you make to support very large stacks or reduce guard-page fragmentation?","diagram":"flowchart TD\n  A[Stack growth fault occurs (touch beyond limit)] --> B[Kernel validates growth policy & RLIMIT_STACK]\n  B --> C[Allocate 4KB page and zero it]\n  C --> D[Map into user-space at next lower address with RW]\n  D --> E[Update stack pointer in the PCB/TSS if needed]\n  E --> F[Invalidate relevant TLB entries]\n  F --> G[Resume execution at faulting instruction]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Adobe","Plaid","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T10:48:03.022Z","createdAt":"2026-01-19T10:48:03.022Z"},{"id":"q-4313","question":"Scenario: A database process uses a 1 GiB region mapped with MAP_ANONYMOUS and userfaultfd to handle on-demand paging. Multiple workers fault pages concurrently to populate the region with hot data. Describe the race conditions when two workers fault the same page, how to coordinate with per-page state, and how to ensure visibility and ordering across faults. Propose a concrete synchronization strategy (e.g., per-page lock with double-checked loading) and discuss performance trade-offs?","answer":"Use per-page state machine: 0=UNLOADED,1=LOADING,2=READY. On fault, thread uses an atomic compare-and-swap to set LOADING if UNLOADED; if it wins, it populates the page and then stores READY with a re","explanation":"## Why This Is Asked\nInteracting workers faulting the same page risks double work and stale data; userfaultfd paths require precise coordination to avoid data races and ensure memory visibility across cores. The scenario tests low-level synchronization, memory ordering, and fault handling.\n\n## Key Concepts\n- Per-page state machine (UNLOADED, LOADING, READY)\n- Atomic CAS and release/acquire semantics\n- Futex or condition variable waiting for other threads\n- Correct interaction with userfaultfd and page fault semantics\n- Performance trade-offs of fine-grained locking vs batching\n\n## Code Example\n```c\n// Pseudo C: per-page loading protocol\ntypedef enum { UNLOADED=0, LOADING=1, READY=2 } page_state_t;\ntypedef struct { _Atomic int state; /* 0/1/2 */ } page_t;\n\nvoid access_page(page_t *p, void *dst) {\n  int s = atomic_load_explicit(&p->state, memory_order_acquire);\n  if (s == READY) { /* copy data to dst from page */ return; }\n  if (s == UNLOADED) {\n    if (atomic_compare_exchange_strong(&p->state, &UNLOADED, LOADING)) {\n      // fault will be resolved by loader\n      trigger_fault_and_load(p);\n      atomic_store_explicit(&p->state, READY, memory_order_release);\n      wake_waiters(p);\n      /* copy data to dst */\n      return;\n    }\n  }\n  // wait until READY\n  wait_on_futex(&p->state, READY);\n  /* copy data */\n}\n```\n\n## Follow-up Questions\n- How would you adapt this to NUMA and cache-line alignment?\n- What failure modes exist if loader crashes or out-of-memory occurs?","diagram":"flowchart TD\n  A[Fault event] --> B[Check per-page state]\n  B --> C{LOADING?}\n  C -- yes --> D[Wait/subscribe]\n  C -- no --> E[Load data]\n  E --> F[Publish READY]\n","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Apple","Google","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T13:15:37.696Z","createdAt":"2026-01-19T13:15:37.696Z"},{"id":"q-4356","question":"In a MAP_SHARED region, two threads access a 4-byte integer at offset 0 without synchronization. Provide a concrete interleaving that can yield a torn/partial read and explain why. Then specify the fix: use atomic stores or memory barriers, or a proper synchronization primitive; describe which memory_order to use and why, and how to ensure visibility across cores?","answer":"Concrete interleaving: two 16-bit stores to a single 32-bit word write high half 0xAABB then low half 0xCCDD. Reader may observe 0xAABB0000 or 0x0000CCDD, i.e., a torn read since writes aren’t atomic.","explanation":"## Why This Is Asked\n\nTests understanding of memory ordering, atomicity, and visibility when sharing memory across threads without OS-provided synchronization.\n\n## Key Concepts\n\n- Atomicity of multi-byte writes\n- Memory ordering (memory_order_seq_cst, acquire/release)\n- Visibility across cores\n- Practical synchronization primitives (mutex, atomic ops)\n\n## Code Example\n\n```javascript\n// Illustrative C-like example using C11 atomics\n#include <stdatomic.h>\n#include <stdint.h>\n\nvoid write_shared(atomic_uint32_t *addr, uint32_t value) {\n  atomic_store(addr, value);\n}\nuint32_t read_shared(atomic_uint32_t *addr) {\n  return atomic_load(addr);\n}\n```\n\n## Follow-up Questions\n\n- How would you test-reproduce the torn read scenario in a unit test?\n- What differences might you expect on ARM vs x86 architectures regarding atomicity?\n- How would you adapt the solution if the value were two 32-bit halves stored in a 64-bit word?","diagram":null,"difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Databricks","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T15:42:04.989Z","createdAt":"2026-01-19T15:42:04.989Z"},{"id":"q-4373","question":"In a Linux server with Transparent Huge Pages (THP) enabled and memory overcommit disabled, a spike in allocations causes THP allocations to fail while small pages are still free. Describe the exact THP allocation path, why fragmentation triggers this failure, and provide a concrete mitigation plan with tuning steps and measurable signals?","answer":"THP allocations coalesce free 4KB pages into 2MB hunks; during a spike fragmentation prevents a contiguous 2MB region, so THP fails and callers stall. Mitigate by provisioning peak huge pages (vm.nr_h","explanation":"## Why This Is Asked\n\nTests practical understanding of Linux THP path, fragmentation, and tuning.\n\n## Key Concepts\n\n- THP allocation path from buddy allocator to large page\n- Fragmentation and defrag triggers\n- sysfs/sysctl knobs: vm.nr_hugepages, /sys/kernel/mm/transparent_hugepage/enabled, defrag\n- Trade-offs: latency vs memory efficiency\n\n## Code Example\n\n```c\n#include <sys/mman.h>\n#include <stddef.h>\nvoid hint_hugepage(void *addr, size_t len){\n    (void)addr; (void)len;\n    madvise(addr, len, MADV_HUGEPAGE);\n}\n```\n\n## Follow-up Questions\n\n- How to measure THP fragmentation and HugePages usage?\n- What changes for latency-sensitive services?","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Amazon","LinkedIn","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T16:41:07.963Z","createdAt":"2026-01-19T16:41:07.963Z"},{"id":"q-4410","question":"Scenario: a Linux server uses an anonymous mmap buffer as a circular IPC queue shared by 8 workers. Each slot contains an 8-byte header (seq, done) and a 256-byte payload. Writers update header before/after writing payload; readers rely on header to detect complete entries. Propose a precise, lock-free protocol that guarantees readers never observe partially updated entries, including the exact write/read order, required memory barriers, and a concrete contention test on real hardware?","answer":"Use a per-slot seqlock-like protocol. Writer: publish 0 to seq, write payload, then set seq to 1 (commit). Reader: read seq; if 0 or if seq changes before reading payload, retry; otherwise read payloa","explanation":"## Why This Is Asked\n\nTests lock-free IPC design, precise memory ordering, and verification under contention.\n\n## Key Concepts\n\n- lock-free primitives, per-slot sequencing, memory barriers, mmap-based IPC, cache coherence.\n- detection of partial writes and ensuring visibility under contention.\n\n## Code Example\n\n```c\ntypedef struct { uint64_t seq; char payload[256]; } slot_t;\n\nvoid write_slot(slot_t *s, void *data, size_t n){\n  __atomic_store_n(&s->seq, 0, __ATOMIC_RELEASE);\n  memcpy(s->payload, data, n);\n  __atomic_store_n(&s->seq, 1, __ATOMIC_RELEASE);\n}\n\nbool read_slot(slot_t *s, void *out){\n  uint64_t a = __atomic_load_n(&s->seq, __ATOMIC_ACQUIRE);\n  if (a != 1) return false;\n  memcpy(out, s->payload, 256);\n  uint64_t b = __atomic_load_n(&s->seq, __ATOMIC_ACQUIRE);\n  return (a == b && a == 1);\n}\n```","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Netflix","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T17:46:37.921Z","createdAt":"2026-01-19T17:46:37.921Z"},{"id":"q-448","question":"Explain how process scheduling works in an operating system. Which scheduling algorithm would you choose for a real-time system and why?","answer":"Process scheduling manages CPU allocation among processes by determining which process gets CPU time and when. For real-time systems, I would choose **Rate Monotonic Scheduling (RMS)** because it's predictable, has bounded response times, and is optimal for fixed-priority real-time systems. RMS assigns higher priorities to tasks with shorter periods, ensuring critical tasks meet their deadlines while maintaining system stability.","explanation":"## Key Concepts\n- **Process scheduling**: Determines CPU allocation order among competing processes\n- **Scheduling algorithms**: FCFS, SJF, Priority, Round Robin, Rate Monotonic Scheduling\n- **Real-time requirements**: Deterministic timing behavior and deadline guarantees\n\n## Real-time Considerations\n- **Predictability**: Must guarantee task completion before deadlines\n- **Priority assignment**: Shorter periods receive higher priorities in RMS\n- **CPU utilization**: RMS can safely utilize up to 69% of CPU capacity\n\n## Implementation\n```c\n// RMS priority calculation\npriority = 1000 / period; // Higher values for shorter periods\n```\n\n## Trade-offs\n- RMS offers simplicity and predictability but is less flexible compared to dynamic scheduling algorithms\n- Optimal for fixed-priority systems but may require careful task design","diagram":"flowchart TD\n  A[Process Request] --> B{Scheduler}\n  B --> C[RMS Priority Check]\n  C --> D[CPU Allocation]\n  D --> E[Process Execution]\n  E --> F[Deadline Check]\n  F --> G{Met Deadline?}\n  G -->|Yes| H[Continue]\n  G -->|No| I[Priority Boost]\n  H --> B","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","OpenAI","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-09T08:53:46.358Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-4484","question":"In the Linux kernel, design a lock-free single-producer, multiple-consumer ring buffer for a device interrupt that enqueues event pointers. Describe the exact sequence and memory ordering you would enforce (which atomic ops, release/acquire semantics, and ABA avoidance), how you prevent data races across slots, and why a spinlock would be slower. Illustrate with a concrete interleaving involving two consumers and one producer?","answer":"Use a per-slot sequence counter and a single atomic tail. Producer CASes to reserve a slot, writes the event pointer, then releases with a memory barrier. Consumers read with acquire, verify slot.seq ","explanation":"## Why This Is Asked\n\nThis probes memory ordering, ABA avoidance, and lock-free queue design in kernel contexts.\n\n## Key Concepts\n\n- Lock-free queues (SPMC)\n- Acquire/release barriers\n- ABA/versioning\n- Cache coherence\n\n## Code Example\n\n```c\ntypedef struct { void *ptr; uint64_t seq; } slot_t;\n// producer: reserve slot, write ptr, release\n// consumer: load slot, check seq, consume\n```\n\n## Follow-up Questions\n\n- How would you benchmark latency under bursty interrupts?\n- How would you handle degraded perf if a consumer stalls?","diagram":null,"difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["LinkedIn","Microsoft","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T20:46:25.410Z","createdAt":"2026-01-19T20:46:25.410Z"},{"id":"q-4502","question":"On a single-core Linux-like system, two threads share a CPU: a CPU-bound worker and an I/O-bound worker that often sleeps on I/O. Explain how the Completely Fair Scheduler (CFS) uses vruntime and per-thread weights to decide which thread runs next, and why the I/O-bound thread tends to break up CPU time. Propose a minimal, practical adjustment (e.g., using cgroups cpu.shares or adjusting nice values) to improve fairness without sacrificing throughput?","answer":"CFS uses a red-black tree ordered by vruntime; the task with the smallest vruntime runs next. CPU-bound tasks accumulate vruntime quickly, while I/O-bound tasks sleep, so their vruntime grows slowly, giving them priority when they wake up and causing them to frequently interrupt CPU-bound work.","explanation":"## Why This Is Asked\n\nTo assess knowledge of OS scheduler internals and practical tuning knobs under realistic workloads.\n\n## Key Concepts\n\n- Completely Fair Scheduler (CFS) and vruntime\n- Per-task weights (CPU shares, nice values)\n- cgroups CPU quotas and fairness tuning\n- How I/O sleeps affect scheduling and potential starvation\n\n## Code Example\n\n```text\n// Pseudo sequence for one scheduler tick\ntask1.vruntime += task1_runtime / task1.weight\ntask2.vruntime += task2_runtime / task2.weight\n\nnext = min(vruntime)\n```\n\n## Follow-up Questions\n\n- How would you diagnose a scenario where CPU-bound tasks are being starved?\n- What metrics would you monitor to verify the effectiveness of your adjustments?\n- How do cgroups cpu.shares differ from nice values in practice?","diagram":"flowchart TD\n  A[Threads] --> B[CFS picks min vruntime]\n  B --> C[Run thread]\n  C --> D[vruntime updated]\n  D --> B","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Databricks","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T07:47:31.323Z","createdAt":"2026-01-19T21:44:13.048Z"},{"id":"q-4538","question":"On a NUMA Linux host running latency-sensitive containers, 2–5 ms spikes appear under heavy CPU load. Describe an end-to-end investigation and fix focusing on IRQ affinity, softirq, and scheduler interplay. Include how to identify root cause with ftrace or perf, isolate a CPU, pin the latency task, redirect device interrupts away, and validate that throughput remains stable?","answer":"Pin the latency-sensitive task to a dedicated CPU core using taskset, isolate CPUs via the isolcpus kernel parameter, and redirect device interrupts away using smp_affinity. Employ ftrace with trace_irq_handler_entry/exit, trace_softirq, and trace_sched_switch to identify where interrupt processing disrupts the latency task. Validate that throughput remains stable using perf stat for system-wide metrics and targeted latency measurements.","explanation":"## Why This Is Asked\nTests practical understanding of Linux kernel interrupt and scheduler interactions under NUMA pressure and containerization. Requires concrete tooling and steps, not high-level theory.\n\n## Key Concepts\n- NUMA locality, IRQ affinity, smp_affinity, isolcpus\n- ftrace/perf for end-to-end latency paths\n- Softirq, hardirq, scheduler preemption, and task pinning\n- Validation: throughput impact vs latency guarantees\n\n## Code Example\n```bash\n# Example commands (not executed here):\n# 1) isolate CPUs 4-7\n#   isolcpus=4-7 in bootloader\n# 2) pin latency task\n#   taskset -c 4 <latency_task>\n# 3) redirect interrupts\n#   echo 0 > /proc/irq/default_smp_affinity\n#   echo f0 > /proc/irq/24/smp_affinity\n# 4) trace with ftrace\n#   echo 1 > /sys/kernel/debug/tracing/events/irq/irq_handler_entry/enable\n#   echo 1 > /sys/kernel/debug/tracing/events/irq/softirq_entry/enable\n#   echo 1 > /sys/kernel/debug/tracing/events/sched/sched_switch/enable\n```","diagram":"flowchart TD\n  A[Heavy CPU load] --> B[IRQ bursts]\n  B --> C{Is latency task isolated?}\n  C -->|Yes| D[Latency stable on isolation core]\n  C -->|No| E[Spikes spread across cores]","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Adobe","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T06:22:26.040Z","createdAt":"2026-01-19T22:50:36.658Z"},{"id":"q-4554","question":"Explain, at the kernel level, how a blocking read() on a pipe is woken up when data arrives. Describe the sequence from data entering the pipe buffer to read() returning in user space, including the pipe's wait queue, the wakeup path, context switch, and the copy_to_user step?","answer":"When a blocking read() encounters an empty pipe, the kernel blocks the current task by enqueuing it on the pipe's wait queue and putting it to sleep. When a writer appends data, the pipe buffer length becomes non-zero, the kernel wakes up the waiting task via the wait queue mechanism, triggers a context switch back to the blocked process, copies the data from the pipe buffer to user space using copy_to_user(), and finally returns control to the caller in user space.","explanation":"## Why This Is Asked\nThis question tests understanding of kernel-level blocking I/O, wait queues, wakeups, and the data transfer path from kernel to user space. It's accessible to beginners while revealing familiarity with synchronization hazards and scheduler interactions.\n\n## Key Concepts\n- Blocking I/O and wait queues\n- Wakeup mechanisms and scheduler interactions\n- copy_to_user data transfer path from kernel to user space\n\n## Code Example\n```c\n// Pseudo kernel path sketch (simplified)\nssize_t pipe_read(struct file *f, void __user *buf, size_t count) {\n  if (pipe_len(f) == 0) {\n    wait_on_","diagram":null,"difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Airbnb","Robinhood","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T06:10:47.093Z","createdAt":"2026-01-19T23:44:05.026Z"},{"id":"q-4643","question":"In Linux, two processes share a 256-byte ring buffer mapped from persistent memory. The producer writes items with a 64-bit sequence number and a 64-bit payload. Propose a lock-free protocol guaranteeing no torn reads, in-order delivery, and crash-consistent recovery. Include exact operation order, memory barriers, and how to validate correctness on both x86-64 and ARM64?","answer":"Use a seqlock-like protocol: writer increments seq to an odd value, writes payload, flushes cache lines, issues a memory barrier, then increments seq to even. Reader reads seq, reads payload, re-reads","explanation":"## Why This Is Asked\nTests lock-free design with persistent memory, ensuring visibility, ordering, and crash-safety across architectures.\n\n## Key Concepts\n- Seqlocks and lock-free data paths\n- Persistent memory semantics and cache-line flushing\n- Memory barriers and cross-ISA ordering (x86-64 vs ARM64)\n- Crash-consistency guarantees and recovery\n\n## Code Example\n```javascript\n// Writer (pseudo-C in JS for illustration)\nfunction writeItem(item, payload) {\n  item.seq += 1n; // now odd\n  item.payload = payload;\n  clwb(item.payload_addr); // persist payload\n  mfence(); // ensure payload is visible before seq\n  item.seq += 1n; // now even\n}\n\nfunction readItem(item) {\n  let s1 = item.seq;\n  if (s1 % 2n === 1n) return null; // in-progress\n  let payload = item.payload;\n  let s2 = item.seq;\n  if (s1 !== s2) return null; // torn read\n  return payload;\n}\n```\n\n## Follow-up Questions\n- How would you adapt this for multiple producers and a single consumer?\n- What tests would validate reliability under power loss and cache-coherence delays?","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T06:49:58.156Z","createdAt":"2026-01-20T06:49:58.156Z"},{"id":"q-4692","question":"Design a robust lock-free multi-producer, single-consumer ring buffer used for inter-thread messaging with two producers and one consumer. Each slot holds a 64-bit key, a 64-bit value, and a 64-bit sequence. Describe the protocol (indices, per-slot sequence), the exact memory barriers required on x86_64 and ARM64, and a minimal code sketch for enqueue and dequeue. Include a concrete test plan that stresses visibility, wrap-around, and ABA issues?","answer":"Use a lock-free MPMC-like scheme with a per-slot sequence and a shared head counter updated via CAS. Producers obtain an index, write data, then publish by updating the slot’s sequence with a release-","explanation":"## Why This Is Asked\nExposes real-world lock-free design pitfalls: memory ordering, ABA, and visibility in a high-throughput ring buffer.\n\n## Key Concepts\n- Lock-free data structures and CAS\n- Per-slot sequencing to avoid ABA\n- Memory ordering on x86_64 vs ARM64\n- Correctness under contention and wrap-around\n\n## Code Example\n```c\n// Minimal enqueue (producer)\nbool enqueue(slot_t* buf, int n, uint64_t key, uint64_t val) {\n  static thread_local uint64_t prod = 0; // per-producer sequence\n  uint64_t idx = atomic_fetch_add(&head, 1) % n;\n  slot_t* s = &buf[idx];\n  s->key = key;\n  s->val = val;\n  atomic_store_explicit(&s->seq, prod*2+1, memory_order_release);\n  prod++;\n  return true;\n}\n\n// Minimal dequeue (consumer)\nbool dequeue(slot_t* buf, int n, uint64_t* out_key, uint64_t* out_val) {\n  static uint64_t tail = 0;\n  slot_t* s = &buf[tail % n];\n  uint64_t seq = atomic_load_acquire(&s->seq);\n  if ((seq & 1) == 0) return false; // empty\n  *out_key = s->key;\n  *out_val = s->val;\n  atomic_store_explicit(&s->seq, seq+1, memory_order_release);\n  tail++;\n  return true;\n}\n```\n\n## Follow-up Questions\n- How would you adapt to multiple consumer threads?\n- How to test for ABA and late-arriving writes under high contention?","diagram":"flowchart TD\n  A[Two Producers] --> B[Ring Buffer with slots]\n  B --> C[Consumer]\n  D[Head CAS] --> B\n  C --> E[Data visible to consumer after seq published]","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Anthropic","Salesforce","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T08:47:06.899Z","createdAt":"2026-01-20T08:47:06.899Z"},{"id":"q-471","question":"You're designing a GPU memory manager for CUDA applications. How would you implement a memory allocator that handles both unified memory and explicit device memory, considering fragmentation, coalescing, and the 48-bit address space limitations?","answer":"I would implement a hybrid memory allocator combining segregated free lists for different size classes with a buddy system for large allocations, utilizing virtual memory techniques to efficiently manage the 48-bit address space. The allocator would support both unified memory and explicit device memory through separate allocation pools while providing automatic coalescing and fragmentation management.","explanation":"## Memory Management Architecture\n\n### Allocation Strategies\n- **Segregated Lists**: Separate free lists optimized for small (4KB-64KB), medium (64KB-1MB), and large (>1MB) allocations to minimize search time\n- **Buddy System**: Power-of-2 allocation for large blocks enabling efficient coalescing and reducing external fragmentation\n- **Slab Allocation**: Specialized pools for frequently used small objects to minimize internal fragmentation\n\n### Address Space Management\n```c\n// 48-bit virtual address layout\ntypedef struct {\n    uint64_t prefix : 16;  // Reserved for future expansion\n    uint64_t vaddr   : 48;  // Actual virtual address\n} gpu_vaddr_t;\n```\n\n### Unified Memory Optimization\n- **Migration Policies**: Implement adaptive page migration based on access patterns and locality\n- **Prefetching**: Hardware-guided prefetching for anticipated memory accesses\n- **Page Fault Handling**: Efficient page fault resolution with minimal CPU intervention","diagram":"flowchart TD\n  A[Application Request] --> B{Memory Type}\n  B -->|Unified| C[Unified Memory Path]\n  B -->|Device| D[Explicit Device Memory]\n  C --> E[Page Fault Handler]\n  E --> F[Migration Heuristics]\n  F --> G[Allocate Virtual Pages]\n  D --> H[Size Class Selection]\n  H --> I[Segregated Free List]\n  I --> J[Buddy System Check]\n  J --> K[Physical Allocation]\n  G --> L[GPU Memory Mapping]\n  K --> L","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":["memory allocator","fragmentation","coalescing","segregated free lists","buddy system","virtual memory","cuda"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-09T09:04:49.876Z","createdAt":"2025-12-26 12:51:05"},{"id":"q-4714","question":"In Linux, two processes share a 64-bit counter via a memory-mapped shared region (MAP_SHARED). Process A writes the counter as a single 64-bit write; Process B reads the counter once per second. Describe a concrete interleaving that yields a torn read on 32-bit architectures and explain why atomicity of 64-bit writes is not guaranteed there. Propose a minimal safe protocol to ensure atomicity and visibility, including exact operation order (atomic 64-bit ops vs version-tag approach)?","answer":"Interleaving: V0 = 0x00000000_00000000. A writes high 32 bits to 0x00000001; B reads; low 32 still 0, so B sees 0x00000001_00000000—a torn value. Minimal fixes: (1) atomic 64-bit stores on 64-bit CPUs","explanation":"## Why This Is Asked\nTests understanding of 64-bit atomicity, tearing, and how shareable memory interacts with differing CPU word sizes. It probes practical strategies to enforce visibility without resorting to heavyweight locks.\n\n## Key Concepts\n- Atomicity of 64-bit writes on 32-bit vs 64-bit CPUs\n- Tearing and memory ordering in shared mmap regions\n- Version-tag or atomic 64-bit ops as minimal guards\n\n## Code Example\n```c\n/* bad: two-step non-atomic write on 32-bit arch */\nvolatile uint64_t *p; /* mapped shared */\np[0] = (uint64_t)high << 32 | low;\n\n/* better: versioned write (conceptual) */\nuint32_t *vers = (uint32_t*)p; // version at p[0]\nuint64_t *val = (uint64_t*)(p+1);\n*vers = *vers + 1; // pre-update version\n*val = new_val;     // write actual value\n*vers = *vers + 1; // post-update version\n```\n\n## Follow-up Questions\n- How would you implement a portable reader that never returns torn values?\n- What are the trade-offs of using atomic64_t vs a separate version tag?","diagram":"flowchart TD\n  A[Start read] --> B[Read ver1]\n  B --> C[Read value]\n  C --> D[Read ver2]\n  D --> E{ver1==ver2}\n  E -->|yes| F[Return value]\n  E -->|no| A","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Google","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T09:42:45.178Z","createdAt":"2026-01-20T09:42:45.178Z"},{"id":"q-4786","question":"Explain end-to-end how the Linux-like kernel handles a page fault that triggers stack growth for a thread due to deep recursion. Describe the steps from faulting instruction to resumed execution: fault diagnosis, stack VMA checks, guard pages, allocating a zero-filled page, updating the stack PTE, and issuing a TLB shootdown; address synchronization for multi-threaded growth and potential performance costs?","answer":"On fault, kernel checks if the address is in the thread's stack VMA and if growth is allowed. If yes, allocate a zero-filled page, map it into the user page table, and perform a TLB shootdown. The fau","explanation":"## Why This Is Asked\n\nTests understanding of on-demand stack growth, page faults, and VM synchronization in multi-threaded contexts.\n\n## Key Concepts\n\n- Stack growth via page faults\n- Stack guard pages and VMA checks\n- Page table updates and TLB coherence\n- Concurrency control for per-thread stack growth\n\n## Code Example\n\n```javascript\n// Pseudo steps for stack growth\n```\n\n## Follow-up Questions\n\n- How would guard-page mispredictions impact performance?\n- What optimizations or data structures help reduce contention during growth?","diagram":"flowchart TD\n  Fault[Page Fault] --> Check[Check VMA/Stack Growth]\n  Check --> Allocate[Allocate Page]\n  Allocate --> UpdatePT[Update PTE + TLB]\n  UpdatePT --> Resume[Resume Instruction]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Adobe","Bloomberg","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T13:08:43.558Z","createdAt":"2026-01-20T13:08:43.558Z"},{"id":"q-4820","question":"Design an in-memory, shared-hash-map for a high-concurrency database worker pool. Implement an RCU-based read path so reads never block even during hash-table resizing, and writers perform safe updates by replacing a bucket structure and deferring reclamation. Explain grace periods, memory barriers on x86_64, and how to reclaim old nodes without use-after-free. Include a minimal code sketch and a microbenchmark outline?","answer":"RCU per bucket: readers run non-blocking; writers replace a bucket by allocating a fresh node, copying data, linking into the bucket list, and swapping the head with a release store, followed by synch","explanation":"## Why This Is Asked\\nReasoning: probes practical memory reclamation and non-blocking synchronization in kernel-like data structures.\\n\\n## Key Concepts\\n- Read-Copy-Update (RCU) semantics for updates\\n- Grace period, synchronize_rcu, safe reclamation\\n- Memory ordering on x86_64: release stores, READ_ONCE/WRITE_ONCE\\n- Per-bucket isolation to minimize cross-core traffic\\n\\n## Code Example\\n```c\n// Sketch\nstruct bucket { int value; struct bucket *next; };\nvoid reader(struct bucket *head) {\n  rcu_read_lock();\n  struct bucket *b = READ_ONCE(head);\n  int v = READ_ONCE(b->value);\n  rcu_read_unlock();\n}\nvoid writer(struct bucket **head) {\n  struct bucket *nb = malloc(...);\n  nb->value = newval;\n  nb->next = READ_ONCE(*head);\n  if (cmpxchg(head, nb->next, nb)) {\n    synchronize_rcu();\n  }\n}\n```\n\\n## Follow-up Questions\\n- How would you test for use-after-free under concurrency?\\n- Compare performance against a mutex-based approach under varying read/write ratios.","diagram":null,"difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Citadel","MongoDB","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T14:59:54.431Z","createdAt":"2026-01-20T14:59:54.431Z"},{"id":"q-5001","question":"Two worker threads share a single io_uring instance and submit I/O to the same file descriptor concurrently. Describe a concrete interleaving where two threads call io_uring_get_sqe() and io_uring_submit() in overlapping fashion, racing the tail pointer and potentially losing a submission or delivering out-of-order completions. What exact synchronization and memory-barrier guarantees are required to ensure atomic reservation of SQEs, and how would you structure code to guarantee in-order completions for logging while preserving high throughput?","answer":"Two worker threads contend for the submission queue tail pointer. Thread A obtains an SQE slot and writes its submission entry, while Thread B races to advance the kernel tail pointer and submit. Without proper memory barriers, one SQE can be overwritten or visibility delays can cause lost submissions or out-of-order completions. Correct synchronization requires atomic tail updates with release semantics when publishing SQEs and acquire semantics when consuming completions, combined with sequence numbering or per-thread submission queues to guarantee ordering while maintaining high throughput.","explanation":"## Why This Is Asked\nRaces in io_uring multi-producer submission and memory ordering are subtle in real systems; candidates demonstrate understanding of ring buffers, memory barriers, and ordering guarantees.\n\n## Key Concepts\n- io_uring submission and completion rings (SQ/CQ)\n- Multi-producer correctness in shared ring buffers\n- Memory barriers (write/read, acquire/release semantics)\n- In-order completion guarantees and request tagging\n\n## Code Example\n```c\n// Example: multi-producer submission with atomic SQE reservation\nstruct io_uring ring;\npthread_mutex_t sqe_mutex = PTHREAD_MUTEX_INITIALIZER;\n\n// Atomic SQE reservation with sequence numbers\nstatic atomic_uint_fast64_t seq_counter = ATOMIC_VAR_INIT(0);\n\nint submit_io_safe(int fd, void *buf, size_t len, uint64_t *seq_out) {\n    // Reserve SQE atomically\n    pthread_mutex_lock(&sqe_mutex);\n    struct io_uring_sqe *sqe = io_uring_get_sqe(&ring);\n    if (!sqe) {\n        pthread_mutex_unlock(&sqe_mutex);\n        return -EBUSY;\n    }\n    \n    // Assign sequence number for ordering\n    uint64_t seq = atomic_fetch_add_explicit(&seq_counter, 1, \n                                            memory_order_relaxed);\n    *seq_out = seq;\n    \n    // Prepare submission with sequence tag\n    io_uring_prep_write(sqe, fd, buf, len, 0);\n    sqe->user_data = seq;\n    \n    // Release barrier before submit\n    atomic_thread_fence(memory_order_release);\n    int ret = io_uring_submit(&ring);\n    pthread_mutex_unlock(&sqe_mutex);\n    \n    return ret;\n}\n\n// Completion processing with ordering\nvoid process_completions(void) {\n    struct io_uring_cqe *cqe;\n    \n    while (io_uring_peek_cqe(&ring, &cqe) == 0) {\n        // Acquire barrier before reading completion\n        atomic_thread_fence(memory_order_acquire);\n        \n        uint64_t seq = cqe->user_data;\n        // Process completion in sequence order...\n        \n        io_uring_cqe_seen(&ring, cqe);\n    }\n}\n```\n\n## Implementation Strategy\nFor high-throughput ordered logging, implement per-thread submission queues with a single consumer thread that preserves order while allowing parallel I/O preparation.","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Airbnb","Hugging Face","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:44:10.374Z","createdAt":"2026-01-20T23:00:05.951Z"},{"id":"q-5024","question":"In a POSIX shared-memory ring buffer used by two producers and one consumer, describe a lock-free MPMC queue using per-slot sequence numbers to avoid ABA. Specify: slot layout, enqueue/dequeue order, how to detect full/empty, and the exact memory barriers required on x86_64 and ARM64. How would you validate correctness under contention?","answer":"Implement a fixed-size ring buffer with per-slot sequence numbers and separate head/tail indices. Writers atomically reserve slots by advancing the tail index, write the payload data, then publish by updating the slot's sequence number with a release barrier. Readers verify the slot's sequence number with an acquire barrier, read the payload, and advance the head index. The buffer is full when tail - head equals capacity, and empty when head equals tail. On x86_64, release/acquire barriers suffice; on ARM64, full dmb ish is required for release and dmb ishld for acquire. Validate correctness through stress testing with randomized producer/consumer patterns, thread sanitizer analysis, and formal model checking.","explanation":"## Why This Is Asked\nThis question evaluates understanding of lock-free data structures, memory ordering semantics, and cross-platform memory barrier requirements in a production system context.\n\n## Key Concepts\n- MPMC ring buffer with per-slot sequence numbers\n- ABA problem mitigation via publish/subscribe pattern\n- Architecture-specific memory barrier requirements\n- Contention handling and wraparound correctness\n\n## Code Example\n```c\n// Pseudo-code for enqueue/dequeue operations\nbool enqueue(struct ring *r, void *data) {\n    size_t pos = atomic_fetch_add_explicit(&r->tail, 1, memory_order_relaxed);\n    size_t slot = pos % r->capacity;\n    \n    // Wait for slot to be available\n    while (r->slots[slot].seq != pos) {\n        // Backoff or handle contention\n    }\n    \n    // Write payload\n    r->slots[slot].data = data;\n    \n    // Publish with release barrier\n    atomic_store_explicit(&r->slots[slot].seq, pos + 1, memory_order_release);\n    return true;\n}\n\nbool dequeue(struct ring *r, void **data) {\n    size_t pos = atomic_load_explicit(&r->head, memory_order_relaxed);\n    size_t slot = pos % r->capacity;\n    \n    // Check if slot is ready with acquire barrier\n    size_t seq = atomic_load_explicit(&r->slots[slot].seq, memory_order_acquire);\n    if (seq != pos + 1) return false;\n    \n    // Read payload\n    *data = r->slots[slot].data;\n    \n    // Advance head\n    atomic_store_explicit(&r->head, pos + 1, memory_order_relaxed);\n    return true;\n}\n```\n\n## Memory Barriers by Architecture\n- **x86_64**: `memory_order_release`/`memory_order_acquire` (no additional barriers needed)\n- **ARM64**: `dmb ish` for release, `dmb ishld` for acquire operations\n\n## Validation Approach\n- Stress testing with multiple producers/consumers under high contention\n- ThreadSanitizer and AddressSanitizer for race condition detection\n- Formal verification using TLA+ or similar model checking tools\n- fuzzing with random production/consumption patterns","diagram":"flowchart TD\n  A[Producers] --> B[Tail increments]\n  B --> C[Slot write payload]\n  C --> D[Publish slot.seq (release)]\n  E[Consumer] --> F[Read slot.seq (acquire)]\n  F --> G[Read payload]\n  G --> H[Advance head]\n  D --> I[Ring progress]\n  H --> I","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Plaid","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:19:19.435Z","createdAt":"2026-01-21T00:03:16.086Z"},{"id":"q-5075","question":"On a Linux server using blk-mq with bfq, a worker pool handles both small random I/O (4 KB) and large sequential I/O (1 MB). Under peak load, tail latency for small I/O degrades unpredictably. Provide a concrete diagnostic plan to attribute latency to the IO scheduler, and outline a fix using per-cgroup IO weights or deadlines, including exact commands and measurement criteria?","answer":"Create two cgroups: foreground (io.weight=200) and background (io.weight=10); move short I/O threads into foreground. Run fio with randread 4k, and record p95/p99 latency; monitor throughput with iost","explanation":"## Why This Is Asked\n\nTests practical OS skills: IO scheduling, cgroups, and performance measurement under mixed workloads. It asks for a reproducible diagnostic protocol and a concrete fix that respects production constraints.\n\n## Key Concepts\n\n- Block IO schedulers (bfq/mq-deadline) and their fairness\n- cgroup-based QoS (io.weight, io.max)\n- Telemetry: p95/p99 latency, IOPS, throughput\n- Benchmarking with fio and iostat\n\n## Code Example\n\n```bash\n# Example: create cgroups (v2)\nmkdir -p /sys/fs/cgroup/io/foreground\necho 200 > /sys/fs/cgroup/io/foreground/io.weight\n# move pid 1234 to foreground\necho 1234 > /sys/fs/cgroup/io/foreground/cgroup.procs\n```\n\n## Follow-up Questions\n\n- How would you extend this to multi-tenant backends?\n- What if the device uses network-backed storage?","diagram":null,"difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":null,"companies":["Citadel","Coinbase","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T04:38:41.996Z","createdAt":"2026-01-21T04:38:41.996Z"},{"id":"q-530","question":"A process is stuck in 'D' state (uninterruptible sleep) during I/O operations. How would you debug this, what causes it, and how does it differ from 'Z' zombie state?","answer":"The 'D' state occurs during kernel-level I/O operations that cannot be interrupted, such as disk access or network operations. To identify stuck processes, use `ps aux | awk '$8 ~ /^D/ {print}'`. For debugging, `strace -p <pid>` shows the blocked system calls, while `lsof -p <pid>` reveals open file descriptors and identifies the specific I/O operation causing the block.","explanation":"## Debugging D State\n\n- Use `ps aux` to identify processes in D state\n- `strace -p <pid>` shows blocked system calls\n- `lsof -p <pid>` reveals open file descriptors\n- Check `/proc/<pid>/stack` for kernel stack trace\n\n## Common Causes\n\n- Faulty storage devices or network mounts\n- NFS/SMB server unresponsiveness\n- Hardware driver issues\n- Kernel bugs in I/O subsystem\n\n## D vs Z State\n\n- **D state**: Process alive, blocked in kernel, cannot be killed\n- **Z state**: Process terminated, resources freed, waiting for parent\n- D state consumes kernel resources, Z state only holds PID\n\n## Resolution\n\n- Address underlying hardware or network issues\n- Restart problematic services or mounts\n- Update drivers or kernel if needed\n- As last resort, reboot the system to clear D state processes","diagram":"flowchart TD\n  A[Process Running] --> B{I/O Request}\n  B -->|Kernel Space| C[D State - Uninterruptible Sleep]\n  C -->|I/O Complete| D[Return to Running]\n  C -->|Hardware/Network Issue| E[Stuck in D State]\n  F[Process Terminated] --> G[Parent Reads Exit Status]\n  G --> H[Z State - Zombie]\n  H -->|Parent Cleanup| I[PID Released]","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","LinkedIn","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":["uninterruptible sleep","strace","lsof","zombie state","kernel i/o","debugging"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-08T11:42:54.966Z","createdAt":"2025-12-26 12:51:06"},{"id":"q-556","question":"How would you debug a process that's consuming 100% CPU but not responding to signals? What tools and steps would you use?","answer":"Start by identifying the process ID using `top` or `htop` to confirm the high CPU usage. Then attach `strace -p <PID>` to monitor system calls and determine if the process is stuck in user space or kernel mode. Check `/proc/<PID>/status` for the process state and examine `/proc/<PID>/stack` for kernel stack information. If the process remains unresponsive, use `gdb -p <PID>` to obtain stack traces and analyze the execution context.","explanation":"## Debugging Methodology\n\n1. **Process Identification**: Use `top`, `htop`, or `ps aux` to identify the PID of the CPU-intensive process\n2. **Process State Analysis**: Examine `/proc/<PID>/status` to understand current state (running, sleeping, zombie, etc.)\n3. **System Call Monitoring**: Deploy `strace -p <PID>` to trace system calls in real-time and identify blocking operations\n4. **Stack Trace Analysis**: Attach with `gdb -p <PID>` to inspect call stacks and identify infinite loops or deadlock conditions\n5. **Kernel-level Investigation**: Review `dmesg` output and `/proc/<PID>/stack` for kernel-space issues\n\n## Common Root Causes\n\n- Infinite loops in application code\n- Thread synchronization deadlocks\n- Blocking I/O operations without timeouts\n- Memory corruption leading to undefined behavior\n- Kernel bugs or driver issues\n\n## Resolution Strategies\n\n- Send `SIGKILL` as last resort if process remains unresponsive\n- Analyze application logs for error patterns preceding the issue\n- Implement proper error handling and timeout mechanisms\n- Address underlying code quality issues in the identified problem areas","diagram":"flowchart TD\n  A[High CPU Detected] --> B[top/htop - identify PID]\n  B --> C[Check /proc/<PID>/status]\n  C --> D[strace -p <PID>]\n  D --> E{System calls active?}\n  E -->|Yes| F[gdb -p <PID> - analyze stack]\n  E -->|No| G[Check for kernel deadlock]\n  F --> H[Identify root cause]\n  G --> H\n  H --> I[Apply fix/kill process]","difficulty":"intermediate","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:56:11.138Z","createdAt":"2025-12-26 12:51:07"},{"id":"q-585","question":"How would you implement a lock-free concurrent queue using atomic operations and memory barriers? What are the trade-offs between ABA problem solutions?","answer":"Implement using atomic head and tail pointers with compare-and-swap operations. Use hazard pointers or versioned references to solve the ABA problem. Trade-offs: hazard pointers have higher memory overhead but simpler implementation, while versioned references offer lower overhead with more complex compare-and-swap logic.","explanation":"## Lock-Free Queue Implementation\n\n- Use atomic head and tail pointers to manage queue boundaries\n- Enqueue: Atomically update tail pointer using CAS, then link new node\n- Dequeue: Atomically update head pointer using CAS, then read next node\n- Memory barriers: Use acquire semantics on loads and release semantics on stores\n\n## ABA Problem Solutions\n\n- **Hazard pointers**: Track protected nodes per thread to prevent premature reclamation\n- **Versioned references**: Combine pointer with monotonically increasing counter\n- **Epoch-based reclamation**: Batch deallocation operations by epoch boundaries\n\n## Trade-offs\n\n- Hazard pointers: Higher memory usage, simpler logic\n- Versioned references: Lower overhead, more complex CAS operations\n- Epoch-based reclamation: Better scalability, more complex implementation\n- Performance depends on contention levels and allocation patterns","diagram":"flowchart TD\n  A[Enqueue] --> B[CAS Tail Pointer]\n  B --> C[Link New Node]\n  D[Dequeue] --> E[CAS Head Pointer]\n  E --> F[Read Next Node]\n  G[ABA Problem] --> H[Hazard Pointers]\n  G --> I[Versioned References]\n  G --> J[Epoch Reclamation]","difficulty":"advanced","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","OpenAI","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-08T11:51:08.528Z","createdAt":"2025-12-27T01:14:13.089Z"},{"id":"q-927","question":"In a system with a fixed-size circular buffer of size N shared by a producer and a consumer thread, implement a thread-safe producer-consumer solution using semaphores and a mutex in C. Include initialization, edge cases (buffer full/empty), and show how you would test for deadlocks and correctness under concurrent producers/consumers?","answer":"Use two counting semaphores and a mutex: empty initialized to N, full to 0, and a mutex protecting the circular buffer. Producer waits on empty, locks, writes at in, in=(in+1)%N, unlocks, signals full","explanation":"## Why This Is Asked\nTests understanding of basic synchronization primitives and common OS patterns, especially producer-consumer, deadlock avoidance, and correct semaphore/mutex usage in C.\n\n## Key Concepts\n- Semaphores\n- Mutex\n- Circular buffer\n- Deadlock detection\n- Boundary conditions (full/empty)\n\n## Code Example\n```c\n#include <pthread.h>\n#include <semaphore.h>\n#include <stdio.h>\n\n#define N 10\n\nint buffer[N];\nint in = 0, out = 0;\nsem_t empty, full;\npthread_mutex_t mutex;\n\nvoid produce(int item) {\n  sem_wait(&empty);\n  pthread_mutex_lock(&mutex);\n  buffer[in] = item;\n  in = (in + 1) % N;\n  pthread_mutex_unlock(&mutex);\n  sem_post(&full);\n}\n\nint consume() {\n  int item;\n  sem_wait(&full);\n  pthread_mutex_lock(&mutex);\n  item = buffer[out];\n  out = (out + 1) % N;\n  pthread_mutex_unlock(&mutex);\n  sem_post(&empty);\n  return item;\n}\n```\n\n## Follow-up Questions\n- How would you extend to multiple producers/consumers and ensure fairness?\n- What are trade-offs vs. using condition variables instead of semaphores?","diagram":null,"difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","OpenAI","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:37:15.531Z","createdAt":"2026-01-12T15:37:15.531Z"},{"id":"q-995","question":"In a system using paging with a TLB, describe the end-to-end sequence when a 4 KB page accessed by a process is not mapped in RAM, from fault to resume, including the fault handler, page-table walk, TLB update, disk I/O to swap, and how the eviction policy decides which page to replace?","answer":"On a fault: hardware traps to the kernel; verify access rights; pick a free physical frame or evict a victim; perform a page-table walk to map VPN to the chosen PPN and set present/dirty bits; flush o","explanation":"## Why This Is Asked\n\nTests understanding of demand paging, TLB coherency, and eviction trade-offs in real OS.\n\n## Key Concepts\n\n- Paging, page tables, TLB, and ASIDs\n- Page fault handling, disk I/O, swap vs page file\n- Eviction policies (LRU, Clock) and dirty page handling\n\n## Code Example\n\n```c\n// Pseudo: handle_page_fault()\n// 1) check rights, 2) allocate frame, 3) page_table[VPN] = PPN|present, 4) TLB.invalidate(VPN); TLB.update(VPN, PPN);\n```\n\n## Follow-up Questions\n\n- How does asynchronous I/O affect fault handling?\n- What mitigations reduce thrashing in a busy workload?\n","diagram":"flowchart TD\n  A[Page fault] --> B[Trap to kernel]\n  B --> C{Free frame?}\n  C -- yes --> D[Map VPN->PPN; set present]\n  C -- no --> E[Evict victim]\n  E --> F[If dirty: write back]\n  F --> D\n  D --> G[Update TLB]\n  G --> H[Resume]","difficulty":"beginner","tags":["operating-systems"],"channel":"operating-systems","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:39:46.467Z","createdAt":"2026-01-12T18:39:46.467Z"},{"id":"q-263","question":"How does demand paging optimize memory utilization in virtual memory systems, what triggers page faults, and which algorithms handle page replacement when physical memory is full?","answer":"Demand paging loads pages only on first access, triggering page faults handled by the OS. Page replacement algorithms like LRU, FIFO, or Clock evict pages when memory is full. The TLB caches recent translations to reduce overhead, while working set models prevent thrashing by tracking active pages. Performance depends on locality, page size, and replacement efficiency.","explanation":"## Memory Efficiency\nDemand paging eliminates loading unused pages, reducing initial memory overhead and I/O. Pages load lazily when first accessed via page faults.\n\n## Page Fault Handling\n1. CPU traps to OS\n2. OS validates the access\n3. Locates page on disk\n4. Selects victim page if needed\n5. Loads page into free frame\n6. Updates page tables\n7. Restarts instruction\n\n## Page Replacement Algorithms\n**LRU**: Evicts least recently used page - optimal locality but expensive tracking. **FIFO**: Simple but can suffer from Belady's anomaly. **Clock**: Approximates LRU with reference bits, balancing performance and overhead.\n\n## TLB Role\nThe Translation Lookaside Buffer caches recent virtual-to-physical mappings, reducing page table walks from memory access to single cycle lookup.\n\n## Thrashing Prevention\nWorking set models track each process's active pages. Local replacement prevents one process from evicting another's pages. Page fault frequency monitoring detects thrashing and may suspend processes.\n\n## Performance Trade-offs\nPage size affects internal fragmentation (larger pages) vs. TLB miss rate (smaller pages). Prepaging can reduce page faults but wastes memory. Copy-on-write optimizes fork() operations by sharing pages until modification.","diagram":"flowchart TD\n    A[Process accesses memory] --> B{Page in RAM?}\n    B -->|Yes| C[Access data directly]\n    B -->|No| D[Page fault triggered]\n    D --> E[OS handles fault]\n    E --> F[Load page from disk]\n    F --> G[Update page table]\n    G --> H[Restart instruction]\n    H --> C","difficulty":"intermediate","tags":["virtual-memory","paging","segmentation","cache"],"channel":"operating-systems","subChannel":"memory","sourceUrl":null,"videos":null,"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2025-12-26T16:40:56.531Z","createdAt":"2025-12-26 12:51:07"}],"subChannels":["general","memory"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Coinbase","Databricks","Discord","DoorDash","Google","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":52,"beginner":19,"intermediate":21,"advanced":12,"newThisWeek":40}}