{"questions":[{"id":"gcp-devops-engineer-bootstrap-gcp-1768243011258-0","question":"Which action most effectively ensures all identities accessing the Google Cloud Organization are verified with 2-step verification before gaining access?","answer":"[{\"id\":\"a\",\"text\":\"Enforce 2-step verification for all identities via Cloud Identity\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Grant the Owner role to all administrators at the org level\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enable Security Command Center findings only for high-risk users\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Require MFA through an inline IAM condition on each project\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct: Enforcing 2-step verification (MFA) through Cloud Identity ensures all identities accessing the org must use MFA, providing a strong, centralized control during bootstrap.\n\n## Why Other Options Are Wrong\n- Option b: Granting Owner broadly increases risk and does not implement MFA enforcement.\n- Option c: Security Command Center findings help visibility but do not enforce MFA.\n- Option d: MFA enforcement per project is not a centralized, scalable control and is not the standard bootstrap approach.\n\n## Key Concepts\n- 2-step verification (MFA)\n- Cloud Identity / Google Workspace integration\n- Organization-level access control\n\n## Real-World Application\nDuring org bootstrap, enforce MFA at identity provider level to prevent credential abuse and establish a secure baseline for all administrative access.","diagram":null,"difficulty":"intermediate","tags":["GCP","IAM","OrgPolicy","Kubernetes","Terraform","AWS","certification-mcq","domain-weight-17"],"channel":"gcp-devops-engineer","subChannel":"bootstrap-gcp","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:36:51.259Z","createdAt":"2026-01-12 18:36:51"},{"id":"gcp-devops-engineer-bootstrap-gcp-1768243011258-1","question":"To minimize public exposure of VM instances, you want to disable external IP addresses by default across the entire organization. Which Organization Policy constraint should you configure?","answer":"[{\"id\":\"a\",\"text\":\"constraints/compute.vmExternalIpAccess set to false\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"constraints/compute.vmExternalIpAccess set to true\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"constraints/compute.networkAllowExternalTcpOutbound set to denied\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"constraints/compute.publicIpAccessPolicy set to deny-all\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct: Setting constraints/compute.vmExternalIpAccess to false prevents assignment of external IPs for VM instances, aligning with a defense-in-depth posture.\n\n## Why Other Options Are Wrong\n- Option b: This would allow external IPs, defeating the goal.\n- Option c: There is no standard constraint named compute.networkAllowExternalTcpOutbound; this is not a real or effective constraint for this purpose.\n- Option d: There is no general constraint with that exact name; this option is not a valid mechanism.\n\n## Key Concepts\n- Organization Policy constraints\n- Compute VM external IP access\n- Default-deny security posture\n\n## Real-World Application\nHardening an org to reduce attack surface by ensuring VMs cannot receive public IPs unless explicitly allowed on a per-project basis.","diagram":null,"difficulty":"intermediate","tags":["GCP","OrgPolicy","IAM","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-17"],"channel":"gcp-devops-engineer","subChannel":"bootstrap-gcp","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:36:51.736Z","createdAt":"2026-01-12 18:36:52"},{"id":"gcp-devops-engineer-bootstrap-gcp-1768243011258-2","question":"You want new projects to automatically bill to a central account without manual linking. Which approach best implements this at bootstrap time?","answer":"[{\"id\":\"a\",\"text\":\"Use the Cloud Billing API to link new projects to the central billing account on creation\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Attach the central billing account as a default in the Organization's IAM roles\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Rely on manual linking after project creation\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Create a separate billing project and share it via IAM roles\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct: The Cloud Billing API can be used during or immediately after project creation to attach the project to a central billing account, enabling consistent billing without manual steps.\n\n## Why Other Options Are Wrong\n- Option b: IAM roles do not automatically link billing accounts to projects.\n- Option c: Manual linking introduces a delay and inconsistency during bootstrap.\n- Option d: Sharing a billing project via IAM does not automatically attach new projects to the central account.\n\n## Key Concepts\n- Cloud Billing API\n- Centralized billing accounts\n- Automation in provisioning\n\n## Real-World Application\nAutomate new project provisioning to ensure all spend is consolidated under a single billing account for budgeting and governance.","diagram":null,"difficulty":"intermediate","tags":["GCP","Billing","Terraform","Kubernetes","AWS","certification-mcq","domain-weight-17"],"channel":"gcp-devops-engineer","subChannel":"bootstrap-gcp","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:36:52.224Z","createdAt":"2026-01-12 18:36:52"},{"id":"gcp-devops-engineer-bootstrap-gcp-1768243011258-3","question":"To reduce credential risk, you need to prevent creation of any new service account keys across the org. Which Organization Policy should you apply?","answer":"[{\"id\":\"a\",\"text\":\"constraints/iam.disableServiceAccountKeyCreation\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"constraints/iam.disableAllKeyAccess\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"constraints/iam.enforceKeyRotation\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"constraints/compute.vmExternalIpAccess\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct: disableServiceAccountKeyCreation is the standard Organization Policy constraint to prevent creating new keys for service accounts, reducing credential exposure.\n\n## Why Other Options Are Wrong\n- Option b: There is no broad constraint named disableAllKeyAccess; this option is not valid.\n- Option c: Key rotation is about existing keys, not preventing key creation.\n- Option d: This constraint controls VM external IPs, not service account keys.\n\n## Key Concepts\n- IAM service accounts\n- Service account key management\n- Organization Policy for security controls\n\n## Real-World Application\nBy disabling SA key creation at the org level, you enforce a credential hygiene baseline and reduce risk from leaked keys during bootstrap.","diagram":null,"difficulty":"intermediate","tags":["GCP","IAM","OrgPolicy","Security","AWS","certification-mcq","domain-weight-17"],"channel":"gcp-devops-engineer","subChannel":"bootstrap-gcp","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:36:52.394Z","createdAt":"2026-01-12 18:36:52"},{"id":"gcp-devops-engineer-bootstrap-gcp-1768243011258-4","question":"You are implementing a folder-based environment structure: Organization -> Prod, Nonprod, Shared. You want to ensure new projects can only be created under permitted folders Prod and Shared, not directly under the Organization root. What is the most effective approach?","answer":"[{\"id\":\"a\",\"text\":\"Apply an Organization Policy on projects.create to allow only the Prod and Shared folder parents\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create projects manually under Prod and Shared and block under root\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Grant folders Admin roles at the Org root to Prod/Shared admins\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable project creation entirely and use a separate script to provision\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct: By applying an Organization Policy constraint on projects.create that restricts the allowed parent resources to Prod and Shared folders, you enforce the intended hierarchy for all new projects.\n\n## Why Other Options Are Wrong\n- Option b: Operationally possible but manual and error-prone; does not enforce at creation time.\n- Option c: Admin roles at the root can be bypassed; it does not restrict project creation to only Prod/Shared.\n- Option d: Disabling creation globally is too restrictive and not practical for bootstrap.\n\n## Key Concepts\n- Organization Policy constraints for resource parents\n- Folder-based hierarchy in GCP\n- Least-privilege provisioning at bootstrap\n\n## Real-World Application\nEnforce environment separation by ensuring all new projects land under approved folders via policy, preventing drift during rapid bootstrap.","diagram":null,"difficulty":"intermediate","tags":["GCP","OrgPolicy","IAM","Kubernetes","Terraform","AWS","certification-mcq","domain-weight-17"],"channel":"gcp-devops-engineer","subChannel":"bootstrap-gcp","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:36:52.556Z","createdAt":"2026-01-12 18:36:52"},{"id":"gcp-devops-engineer-build-delivery-1768159039158-0","question":"In a microservices CI/CD pipeline on GCP, you want new code changes to be automatically built, containerized, scanned, and deployed to Cloud Run with a canary rollout to 10% traffic, then to 50% if no issues. Which combination and approach best achieves this using GCP-native tools?","answer":"[{\"id\":\"a\",\"text\":\"Use Cloud Build to build and push images to Artifact Registry, then manually adjust Cloud Run traffic to 10% and 50% in steps.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Cloud Build to trigger deployment via Cloud Deploy with a canary release configuration for the Cloud Run service, enabling traffic shifting.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use Jenkins on Compute Engine to build and push images, then update Cloud Run traffic manually.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Cloud Run by itself to build and deploy code changes.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption B is the best choice because Cloud Deploy supports progressive delivery and canary deployments for Cloud Run, enabling automated traffic shifting across defined stages. Cloud Build handles the CI portion (build, test, and scan) while Cloud Deploy handles the CD flow.\n\n## Why Other Options Are Wrong\n\n- A describes manual traffic shifts instead of a governed canary flow; lacks auditable deployment policy.\n- C uses a non-GCP-native tool (Jenkins) which adds management overhead and does not leverage Cloud Deploy's canary capabilities.\n- D Cloud Run cannot orchestrate a multi-stage, progressive delivery pipeline by itself.\n\n## Key Concepts\n\n- Progressive delivery with Cloud Deploy\n- Canary releases and traffic splitting\n- CI/CD integration with Cloud Build\n\n## Real-World Application\n\n- Create a Cloud Build trigger to build and push images to Artifact Registry\n- Define a Cloud Deploy delivery pipeline for the Cloud Run service with canary stages (e.g., 10% -> 50%) and automated promotions/rollbacks\n- Monitor health and metrics to trigger automatic promotions or rollbacks","diagram":null,"difficulty":"intermediate","tags":["GCP","CloudBuild","CloudDeploy","CloudRun","CI/CD","Kubernetes","certification-mcq","domain-weight-23"],"channel":"gcp-devops-engineer","subChannel":"build-delivery","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T19:17:19.159Z","createdAt":"2026-01-11 19:17:19"},{"id":"gcp-devops-engineer-build-delivery-1768159039158-1","question":"You want to securely manage credentials used by Cloud Build steps. Which approach ensures least exposure and best auditability?","answer":"[{\"id\":\"a\",\"text\":\"Store credentials in a private Cloud Storage bucket with restricted access and fetch them at build time.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Secret Manager to store credentials and grant Cloud Build access to retrieve them at runtime, using built-in secret handling.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Embed credentials in the cloudbuild.yaml as plain text secrets.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use environment variables in Cloud Run to pass secrets to build steps.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption B is correct because Secret Manager provides versioned, auditable secret storage with fine-grained IAM controls. Cloud Build can reference secrets securely within build steps, avoiding exposure in code or logs.\n\n## Why Other Options Are Wrong\n\n- A stores secrets in Cloud Storage with restricted access but lacks built-in secret rotation and fine-grained auditing.\n- C embeds secrets in cloudbuild.yaml, which risks exposure in version control and build logs.\n- D environment variables in Cloud Run pertain to runtime deployments, not secure build-time secret handling, and can be exposed in process listings and logs.\n\n## Key Concepts\n\n- Secret Manager integration with Cloud Build\n- IAM-based access control and secret versioning\n- Build-time secret exposure avoidance\n\n## Real-World Application\n\n- Create a secret in Secret Manager, grant the Cloud Build service account read access, and reference it in cloudbuild.yaml; rotate secrets regularly and audit access logs.","diagram":null,"difficulty":"intermediate","tags":["GCP","SecretManager","CloudBuild","IAM","certification-mcq","domain-weight-23"],"channel":"gcp-devops-engineer","subChannel":"build-delivery","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T19:17:19.847Z","createdAt":"2026-01-11 19:17:20"},{"id":"gcp-devops-engineer-build-delivery-1768159039158-2","question":"To speed up builds by reusing downloaded dependencies across Cloud Build runs, which feature should you enable and configure?","answer":"[{\"id\":\"a\",\"text\":\"Enable build steps to run in a custom worker pool with caching.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable Cloud Build caching by configuring the cloudbuild.yaml with cache paths to dependency directories.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use Artifact Registry to cache artifacts.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Cloud Functions to cache build results.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption B is correct because Cloud Build supports caching of specified directories (cache paths) to persist dependencies across builds, dramatically reducing download times for languages like Node.js, Java, etc.\n\n## Why Other Options Are Wrong\n\n- A describes using a custom worker pool, which does not inherently enable dependency caching across builds.\n- C Artifact Registry caches artifacts produced by builds, not the dependencies downloaded during builds.\n- D Cloud Functions are not a caching mechanism for Cloud Build builds.\n\n## Key Concepts\n\n- Cloud Build cache feature\n- Cache paths for dependencies\n- Build performance optimization\n\n## Real-World Application\n\n- Add cache: paths in cloudbuild.yaml (e.g., node_modules, ~/.m2, ~/.gradle caches) to persist dependencies; ensure caches are invalidated when dependencies change.","diagram":null,"difficulty":"intermediate","tags":["GCP","CloudBuild","Cache","CI/CD","certification-mcq","domain-weight-23"],"channel":"gcp-devops-engineer","subChannel":"build-delivery","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T19:17:20.302Z","createdAt":"2026-01-11 19:17:20"},{"id":"gcp-devops-engineer-build-delivery-1768282005271-0","question":"You want pull request builds to run unit tests and pushes to the main branch to deploy to production. Which Cloud Build configuration best accomplishes this in GCP?","answer":"[{\"id\":\"a\",\"text\":\"Use a single Cloud Build trigger that runs on push to any branch and uses a conditional step to deploy only when the branch is main.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create two separate triggers: one for pushes to main that runs deploy steps, and one for pull_request events that only runs unit tests.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rely on a GitHub Actions workflow that triggers Cloud Build on PRs and pushes.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Configure a single trigger with includedFiles to distinguish PRs and main.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. Separate triggers align event types with the appropriate workflow: PRs run tests, pushes to main deploy. This keeps PR validation isolated from production deployments.\n\n## Why Other Options Are Wrong\n- Option A: Although possible, mixing PR and push logic in one trigger increases complexity and risk of accidental deployments.\n- Option C: While feasible, relying on GitHub Actions adds external tooling and may reduce the visibility and control of Cloud Build pipelines.\n- Option D: includedFiles cannot reliably distinguish PR events from push events; deployments may trigger unintentionally.\n\n## Key Concepts\n- Cloud Build triggers support separate events (push vs pull_request).\n- Isolation of PR validation and production deployment improves safety.\n\n## Real-World Application\n- In teams practicing protected main branches, this pattern ensures feature validation before production changes are deployed, reducing blast radius for faulty PRs.","diagram":null,"difficulty":"intermediate","tags":["GCP","Cloud Build","CI/CD","GitHub","certification-mcq","domain-weight-23"],"channel":"gcp-devops-engineer","subChannel":"build-delivery","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:26:45.272Z","createdAt":"2026-01-13 05:26:45"},{"id":"gcp-devops-engineer-build-delivery-1768282005271-1","question":"You are deploying a new version of an application to a GKE cluster using Google Cloud Deploy. You want to implement a canary deployment that gradually shifts 10%, 30%, then 100% of production traffic, with automatic rollback if metrics worsen. Which option correctly describes Cloud Deploy canary behavior?","answer":"[{\"id\":\"a\",\"text\":\"Cloud Deploy cannot do canary; you must implement canary with Istio or Argo Rollouts separately.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Cloud Deploy can manage progressive delivery and supports canary with traffic shifting percentages and automatic rollback.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Canary deployments are not supported on Cloud Run, only on GKE, so this scenario is impossible with Cloud Deploy.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"You cannot rollback automatically once a canary deployment is initiated.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. Google Cloud Deploy supports progressive delivery, including canary deployments with configurable traffic percentages and automatic rollback based on observed metrics.\n\n## Why Other Options Are Wrong\n- Option A: Cloud Deploy itself supports canary without needing separate tools.\n- Option C: Canary is not exclusive to Cloud Run and can be used with GKE via Cloud Deploy.\n- Option D: Cloud Deploy provides automatic rollback when canary validation fails.\n\n## Key Concepts\n- Progressive delivery in Cloud Deploy (canary, blue-green).\n- Traffic shifting and rollback based on metrics.\n\n## Real-World Application\n- Teams releasing risky changes can start with small percentages, monitor SLIs/ML-based signals, and automatically rollback if violations occur, reducing production risk.","diagram":null,"difficulty":"intermediate","tags":["GCP","Cloud Deploy","Kubernetes Engine","Canary","Terraform","certification-mcq","domain-weight-23"],"channel":"gcp-devops-engineer","subChannel":"build-delivery","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:26:45.834Z","createdAt":"2026-01-13 05:26:46"},{"id":"gcp-devops-engineer-build-delivery-1768282005271-2","question":"A Cloud Build pipeline needs to fetch a database credential from Secret Manager securely during build. Which approach is correct?","answer":"[{\"id\":\"a\",\"text\":\"Store the secret in plaintext inside cloudbuild.yaml as an environment variable.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Secret Manager integration to expose the secret to a build step via secretEnv or secrets, with proper IAM permissions for the Cloud Build service account.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Encode the secret in base64 inside the repository and decode it at runtime in the build step.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store the secret in Artifact Registry and fetch it from there during the build.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. Secret Manager integration with Cloud Build allows controlled exposure of secrets to build steps without plaintext exposure, using secretEnv or secret access with proper IAM.\n\n## Why Other Options Are Wrong\n- Option A: Exposes secrets in plaintext, a major security risk.\n- Option C: Obscuring secrets with base64 is not secure and easily reversible.\n- Option D: Artifact Registry is for artifacts, not secret storage.\n\n## Key Concepts\n- Secrets management in CI/CD pipelines.\n- IAM permissions and minimal access for secrets.\n\n## Real-World Application\n- Ensures credentials like DB passwords are never committed or exposed in logs, meeting security and compliance requirements.","diagram":null,"difficulty":"intermediate","tags":["GCP","Secret Manager","Cloud Build","IAM","Terraform","certification-mcq","domain-weight-23"],"channel":"gcp-devops-engineer","subChannel":"build-delivery","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:26:46.415Z","createdAt":"2026-01-13 05:26:46"},{"id":"gcp-devops-engineer-build-delivery-1768282005271-3","question":"To implement canary deployments on GKE with GitOps-like continuous delivery, which is the minimal required setup?","answer":"[{\"id\":\"a\",\"text\":\"Create a Kubernetes Deployment and perform manual updates.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Install Argo Rollouts controller in the cluster and create a Rollout resource with a canary strategy and traffic routing.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rely on Cloud Deploy canary features alone without any additional tooling.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Spinnaker exclusively for canary, ignoring Kubernetes native resources.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B. Argo Rollouts provides native canary and progressive delivery capabilities on Kubernetes, requiring installation of the controller and a Rollout resource to define the canary strategy.\n\n## Why Other Options Are Wrong\n- Option A: Manual updates do not provide automated progressive delivery.\n- Option C: Cloud Deploy can do canary, but when using a GitOps-like workflow on GKE, Argo Rollouts is the canonical minimal setup for canary on Kubernetes.\n- Option D: Spinnaker is an alternative but not the minimal required setup for Kubernetes canary in a GitOps context.\n\n## Key Concepts\n- Argo Rollouts canary strategy and traffic routing.\n- Kubernetes-native progressive delivery patterns.\n\n## Real-World Application\n- Teams deploying frequent changes can progressively route traffic and automatically rollback on failure signals using a declarative Rollout resource.\n","diagram":null,"difficulty":"intermediate","tags":["GCP","Kubernetes Engine","Argo Rollouts","Canary","Terraform","certification-mcq","domain-weight-23"],"channel":"gcp-devops-engineer","subChannel":"build-delivery","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:26:46.601Z","createdAt":"2026-01-13 05:26:46"},{"id":"gcp-devops-engineer-build-delivery-1768282005271-4","question":"In a multi-stage CI/CD pipeline, you want to enforce that a security scan fails the build when high-severity vulnerabilities are found in a container image. Which approach accomplishes this in Cloud Build?","answer":"[{\"id\":\"a\",\"text\":\"Add a scanner step (for example, Trivy) that exits non-zero on high severity findings.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Run the scanner as a post-build step and email the results to the team without failing the build.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store results in a build artifact and manually approve the deployment if no vulnerabilities are found.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable scanning to avoid build flakiness and rely on runtime security checks only.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A. Running a security scanner as a build step and returning a non-zero exit code for high-severity findings fails the build, enforcing the gate.\n\n## Why Other Options Are Wrong\n- Option B: Emailing results without failing the build does not enforce the gate.\n- Option C: Relying on post-build approval introduces manual steps and delays; automated gates are preferred.\n- Option D: Disabling scanning defeats the security requirement and increases risk.\n\n## Key Concepts\n- Build-time security gates.\n- Automated failure on critical findings.\n\n## Real-World Application\n- Integrates security scanning into CI/CD to prevent vulnerable images from progressing to production, aligning with DevSecOps practices.","diagram":null,"difficulty":"intermediate","tags":["GCP","Cloud Build","Trivy","Security","Terraform","certification-mcq","domain-weight-23"],"channel":"gcp-devops-engineer","subChannel":"build-delivery","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:26:46.792Z","createdAt":"2026-01-13 05:26:47"},{"id":"q-1013","question":"You're managing a multi-tenant SaaS on GCP across five projects connected via Shared VPC. You must enforce per-tenant network isolation, IAM conditions, and budget governance while keeping CI/CD simple. Propose an end-to-end setup using Shared VPC, IAM Conditions, VPC Service Controls, Billing Budgets, and Cloud Asset Inventory, and outline testing and rollback steps?","answer":"Design a per-tenant isolation strategy: use Shared VPC, per-tenant IAM roles with conditional bindings, VPC Service Controls, and per-tenant budgets; implement CI/CD using Cloud Build to apply IaC (Te","explanation":"## Why This Is Asked\n\nThis question probes practical multi-tenant isolation, policy-as-code, and operational testing in GCP.\n\n## Key Concepts\n\n- Shared VPC and host projects\n- IAM Conditions for per-tenant access\n- VPC Service Controls for data exfil prevention\n- Billing Budgets and cost monitoring per tenant\n- Cloud Asset Inventory for drift detection\n- CI/CD with Terraform and Cloud Build\n- Policy-as-code (OPA) and guardrails\n\n## Code Example\n\n```javascript\n// sample budget per tenant\nresource \"google_billing_budget\" \"tenant_budget\" {\n  billing_account = var.billing_account_id\n  display_name    = \"tenant-${var.tenant_id}-budget\"\n\n  amount {\n    specified_amount {\n      currency_code = \"USD\"\n      units         = 1000\n    }\n  }\n\n  threshold_rules {\n    threshold_percent = 0.8\n    spend_basis       = \"CURRENT_SPEND\"\n  }\n\n  // optional fields omitted for brevity\n}\n```\n\n```javascript\n// per-tenant IAM condition (example)\nresource \"google_project_iam_member\" \"tenant_view\" {\n  project = var.tenant_project_id\n  role    = \"roles/viewer\"\n  member  = \"user:${var.user_email}\"\n  condition {\n    title       = \"tenant_is_owner\"\n    expression  = \"resource.name.startsWith('projects/${var.tenant_project_id}')\"\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you test drift between intended IAM bindings and actual bindings?  \n- How would you handle onboarding/offboarding tenants without disruptive outages?","diagram":"flowchart TD\n  A[Tenant Creation] --> B[Configure Shared VPC & IAM] \n  B --> C[Apply Budget & Guardrails] \n  C --> D[CI/CD Deployment] \n  D --> E[Monitoring & Auditing]","difficulty":"intermediate","tags":["gcp-devops-engineer"],"channel":"gcp-devops-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","DoorDash","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:27:58.378Z","createdAt":"2026-01-12T19:27:58.378Z"},{"id":"q-1041","question":"Design a scalable, compliant log routing pipeline on GCP that collects logs from Kubernetes clusters, Cloud Run, and Cloud Functions, redacts PII, and stores in BigQuery with environment separation. Include data flow sinks, IAM and CMEK governance, failure modes and retries, and an end-to-end testing plan?","answer":"Export logs from GKE, Cloud Run, and Cloud Functions via Cloud Logging Log Router to GCS per environment; run a Cloud Dataflow pipeline that uses the DLP API to redact PII and writes to a partitioned ","explanation":"Why This Is Asked: Assesses ability to design a compliant, scalable log pipeline across multiple services with data redaction and governance. Key trade-offs include latency vs. cost, code vs managed services, and testing rigor.","diagram":"flowchart TD\n  A[Logs: GKE/Cloud Run/Functions] --> B[Router Sink to GCS]\n  B --> C[Dataflow + DLP Redaction]\n  C --> D[BigQuery: env-specific]\n  D --> E[Access & Compliance]","difficulty":"intermediate","tags":["gcp-devops-engineer"],"channel":"gcp-devops-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T20:29:17.568Z","createdAt":"2026-01-12T20:29:17.568Z"},{"id":"q-1155","question":"You're deploying a globally distributed service on GKE across three regions, with Cloud Run and Cloud Functions used for specific workloads. Design a deployment pipeline that enforces policy-as-code, encryption at rest via CMEK, drift detection, automated rollback, and cross-region failover. Describe tooling, data planes, tests, and how you handle outages and compliance?","answer":"Use a GitOps approach with Cloud Deploy across all regions, anchoring policies in OPA (policy-as-code) and enforcing CMEK with IAM Conditions. Implement canary releases via traffic-splitting between G","explanation":"## Why This Is Asked\nAssesses real-world skills in GitOps, policy-as-code, cross-region DR, and automated rollback.\n\n## Key Concepts\n- GitOps and Cloud Deploy\n- Policy-as-code with OPA\n- CMEK and IAM Conditions\n- Canary traffic routing across GKE/Cloud Run\n- Drift detection and regional failover\n\n## Code Example\n```javascript\n// Pseudo-code: trigger a rollback if a metric breaches a threshold\nasync function maybeRollback(metrics) {\n  if (metrics.latency > 500 || metrics.errorRate > 0.02) {\n    await cloudDeploy.rollback();\n  }\n}\n```\n\n## Follow-up Questions\n- How would you test drift detection across regions?\n- How do you simulate regional outages and verify failover?","diagram":"flowchart TD\nA[GitOps Repo] --> B[Cloud Deploy Pipeline]\nB --> C[Regions: us, eu, ap]\nC --> D[GKE/Cloud Run/Functions]\nD --> E[Drift Checks]\nE --> F{Status}\nF --> G[Rollback]\nF --> H[Continue]","difficulty":"advanced","tags":["gcp-devops-engineer"],"channel":"gcp-devops-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","NVIDIA","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:28:10.252Z","createdAt":"2026-01-13T03:28:10.252Z"},{"id":"q-1245","question":"You operate a global chat app with components on **GKE**, **Cloud Run**, and **Cloud Functions**. Latency spikes in one region go unnoticed in aggregated metrics. Design an end-to-end observability approach: (1) how to unify traces across runtimes, (2) how to instrument with **OpenTelemetry** and **OTLP** to a central collector, (3) how to build region-scoped dashboards and **SLO-based alerts**, and (4) how to validate during release with **canary** and **chaos testing**. Include tool choices, sample metrics, and a minimal config sketch?","answer":"Implement a unified OpenTelemetry pipeline across GKE, Cloud Run, and Cloud Functions, exporting traces via OTLP to a centralized collector and tagging spans with region, service, and version. Build r","explanation":"## Why This Is Asked\n\nTests the ability to design end-to-end observability across diverse runtimes, ensuring consistent tracing, metrics, and alerting. It also probes practical integration details with OpenTelemetry, OTLP, and regional dashboards, plus validation via canary and chaos testing.\n\n## Key Concepts\n\n- OpenTelemetry and OTLP exporters\n- End-to-end tracing across GKE, Cloud Run, Cloud Functions\n- Region-scoped dashboards and SLO-based alerts\n- Canary releases and chaos engineering validation\n\n## Code Example\n\n```javascript\n// Minimal OpenTelemetry setup sketch (Node.js)\nconst { NodeTracerProvider } = require('@opentelemetry/node');\nconst { SimpleSpanProcessor } = require('@opentelemetry/tracing');\n// ... configure OTLP exporter targeting central collector\n```\n\n## Follow-up Questions\n\n- How would you handle sampling decisions during traffic spikes?\n- How would you enforce consistent trace propagation across services?\n- What metrics would you expose to Cloud Monitoring for fast attribution of regional latency spikes?","diagram":null,"difficulty":"intermediate","tags":["gcp-devops-engineer"],"channel":"gcp-devops-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:40:56.056Z","createdAt":"2026-01-13T06:40:56.056Z"},{"id":"q-1281","question":"You're maintaining a Cloud Run Python service that reads an API key from Secret Manager. Implement a 30-day secret rotation using Cloud Scheduler to publish a rotation event to Pub/Sub, and enable the Cloud Run instance to fetch updated secret without a restart. Detail the IAM permissions, wiring, and a test plan to verify end-to-end rotation?","answer":"Use a Cloud Run service account with roles/secretmanager.secretAccessor on the secret, and a rotation mechanism (Secret Manager secret with a rotation schedule). Schedule a Cloud Scheduler job to publ","explanation":"## Why This Is Asked\n\nTests practical secret management and runtime retrieval in GCP.\n\n## Key Concepts\n\n- Secret Manager rotation, Cloud Scheduler, Pub/Sub, Cloud Run IAM bindings, request-time secret fetch.\n\n## Code Example\n\n```javascript\n// Runtime fetch of Secret Manager secret in Cloud Run (Node.js)\nconst {SecretManagerServiceClient} = require('@google-cloud/secret-manager');\nconst client = new SecretManagerServiceClient();\nasync function getApiKey() {\n  const name = 'projects/PROJECT/secrets/API_KEY/versions/latest';\n  const [version] = await client.accessSecretVersion({name});\n  const apiKey = version.payload.data.toString('utf8');\n  return apiKey;\n}\n```\n\n## Follow-up Questions\n\n- How would you test failure of rotation propagation?\n- How would you handle secret versioning and rollback?","diagram":null,"difficulty":"beginner","tags":["gcp-devops-engineer"],"channel":"gcp-devops-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Plaid","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:28:58.600Z","createdAt":"2026-01-13T08:28:58.601Z"},{"id":"q-877","question":"Design a cross-region disaster recovery plan for a streaming data pipeline on GCP (Pub/Sub, Dataflow, BigQuery) that must survive a regional outage with RTO < 15 minutes and RPO < 5 minutes. The primary region is us-central1; second region is us-east1. Include data paths, failover triggers, data integrity guarantees, and operational testing steps?","answer":"Active-active DR: run identical streaming pipelines in us-central1 and us-east1. Publish to regional Pub/Sub with cross-region replication; Dataflow in each region writes to separate, schema-stable Bi","explanation":"## Why This Is Asked\nTests ability to design DR for streaming pipelines across regions, focusing on data integrity, operational testing, and real-world failover trade-offs.\n\n## Key Concepts\n- Active-active cross-region DR patterns\n- Pub/Sub regional replication strategies\n- Dataflow multi-region pipelines and idempotent sinks\n- BigQuery datasets per region with consistent schemas\n- Cloud SQL regional replicas for metadata\n- Global load balancer + DNS failover for traffic steering\n\n## Code Example\n```python\nclass DedupDoFn(beam.DoFn):\n  def process(self, element, window=beam.DoFn.WindowParam,\n              state=beam.DoFn.StateParam):\n    uid = element.get('id')\n    if not state.read():\n      state.write(True)\n      yield element\n```\n\n## Follow-up Questions\n- How would you validate RTO/RPO with drills without impacting production?\n- How would you handle schema drift and ensure backward compatibility across regions?","diagram":"flowchart TD\n  A[Ingest] --> B[Regional Pub/Sub] \n  B --> C[Dataflow (us-central1)] \n  B --> D[Dataflow (us-east1)] \n  C --> E[BigQuery (us-central1)] \n  D --> F[BigQuery (us-east1)] \n  G[Outage Detected] --> H[Failover to Healthy Region] \n  H --> I[DNS + LB Route]","difficulty":"advanced","tags":["gcp-devops-engineer"],"channel":"gcp-devops-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:57:17.903Z","createdAt":"2026-01-12T13:57:17.903Z"},{"id":"q-921","question":"In a multi-tenant GKE deployment across two GCP projects, you must enforce strict per-tenant network isolation and controlled egress to external services. Design a scalable architecture using Shared VPC, Private Service Connect, and per-tenant firewall policies to ensure tenants only reach whitelisted external endpoints, while preventing cross-tenant access. Include identity management, auditing, drift control, and operational notes for adding new tenants?","answer":"Propose a Shared VPC with per-tenant subnets, PSA endpoints for approved SaaS services, deny-by-default firewall rules, per-tenant IAM bindings, and VPC Service Controls for data exfil. Use Cloud Logg","explanation":"## Why This Is Asked\nTests practical VPC, PSA, and IAM patterns for multi-tenant isolation; covers governance and onboarding.\n\n## Key Concepts\n- Shared VPC\n- Private Service Connect\n- VPC Service Controls\n- Per-tenant firewalls\n- Drift detection and automations\n\n## Code Example\n```hcl\n# Terraform example: create per-tenant subnets and firewall rules\nresource \"\"google_compute_network\"\" \"tenant_shared_vpc\" {\n  name                    = \"shared-vpc-demo\"\n  auto_create_subnetworks = false\n}\n```\n\n## Follow-up Questions\n- How would you onboard a new tenant with zero-downtime networking changes?\n- How would you test the isolation and whitelisting in a staging environment?","diagram":null,"difficulty":"intermediate","tags":["gcp-devops-engineer"],"channel":"gcp-devops-engineer","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:32:32.583Z","createdAt":"2026-01-12T15:32:32.583Z"},{"id":"gcp-devops-engineer-optimize-performance-1768256815601-0","question":"You operate a web service behind a Google Cloud HTTP(S) Load Balancer using a Managed Instance Group in Compute Engine. During peak traffic, latency spikes while CPU utilization remains low. Which scaling strategy is most likely to reduce tail latency?","answer":"[{\"id\":\"a\",\"text\":\"Increase the MIG's max size to handle more instances\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable autoscaling based on CPU utilization of the MIG\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enable autoscaling based on a custom metric that reflects latency or queue depth\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Move traffic to a different region to balance load\",\"isCorrect\":false}]","explanation":"During peak traffic, tail latency is often driven by request queues or backend saturation rather than average CPU usage. Enabling autoscaling based on a latency-related custom metric (e.g., p95 latency or queue depth) allows the group to scale out precisely when latency worsens, reducing tail latency. Pure CPU-based scaling can be ineffective when CPU is not the bottleneck. Regional migration does not address latency within the current region, and simply increasing max instances can waste resources without addressing the tail latency cause.","diagram":null,"difficulty":"intermediate","tags":["GCP","Kubernetes","Terraform","GlobalLoadBalancing","CloudMonitoring","certification-mcq","domain-weight-17"],"channel":"gcp-devops-engineer","subChannel":"optimize-performance","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:26:55.602Z","createdAt":"2026-01-12 22:26:56"},{"id":"gcp-devops-engineer-optimize-performance-1768256815601-1","question":"A service deployed on Cloud Run experiences cold starts causing higher latency at scale. Which configuration best reduces end-to-end latency under load?","answer":"[{\"id\":\"a\",\"text\":\"Increase max instances\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Increase concurrency per instance\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Set a non-zero min instances to keep a subset warm\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Disable autoscaling\",\"isCorrect\":false}]","explanation":"Cold starts in Cloud Run occur when no instances are warm. Setting a non-zero min instances keeps a stable set of instances warm, reducing the impact of cold starts and delivering lower latency during spikes. Increasing max instances can help, but it doesn't guarantee warm instances. Increasing per-instance concurrency reduces the number of instances but not warm-start time. Disabling autoscaling would worsen latency during load spikes.","diagram":null,"difficulty":"intermediate","tags":["GCP","CloudRun","Kubernetes","Terraform","CloudMonitoring","certification-mcq","domain-weight-17"],"channel":"gcp-devops-engineer","subChannel":"optimize-performance","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:26:56.094Z","createdAt":"2026-01-12 22:26:56"},{"id":"gcp-devops-engineer-optimize-performance-1768256815601-2","question":"To ensure global low-latency responses, you deployed services across multiple regions behind a single HTTP(S) Load Balancer with Global load balancing. A region becomes unhealthy; what happens to user requests?","answer":"[{\"id\":\"a\",\"text\":\"Traffic is rerouted to the nearest healthy region automatically by Global Load Balancing\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Traffic is paused until an administrator manually reroutes it\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"The system fails over to the second region but with higher latency\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Cloud CDN caches are invalidated to force fresh content\",\"isCorrect\":false}]","explanation":"Global HTTP(S) Load Balancing continuously monitors backend health across regions. If a region becomes unhealthy, traffic is automatically redirected to the nearest healthy region, minimizing user impact. Manual intervention is not required. Failover latency is minimized by the global network. Cloud CDN invalidation is unnecessary unless you’re updating cached content, which is unrelated to regional health.","diagram":null,"difficulty":"intermediate","tags":["GCP","GlobalLoadBalancing","GKE","Terraform","Kubernetes","certification-mcq","domain-weight-17"],"channel":"gcp-devops-engineer","subChannel":"optimize-performance","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:26:56.556Z","createdAt":"2026-01-12 22:26:56"},{"id":"gcp-devops-engineer-optimize-performance-1768256815601-3","question":"You are optimizing read-heavy workloads in Cloud SQL (PostgreSQL). Which pattern provides the best read-scaling with minimal application code changes?","answer":"[{\"id\":\"a\",\"text\":\"Move all reads to a read replica and implement application-side routing\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Add index hints to each query\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Increase the VM size of the primary\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Cloud Spanner instead\",\"isCorrect\":false}]","explanation":"Cloud SQL read replicas allow reads to be offloaded from the primary, providing horizontal read scalability with relatively low operational overhead. Implementing application-side routing to direct reads to replicas enables this pattern with minimal changes. Adding index hints can help some queries but won’t scale reads. Upgrading the primary VM helps writes more than reads and may be cost-inefficient. Cloud Spanner is a different database paradigm and would require more substantial changes.","diagram":null,"difficulty":"intermediate","tags":["GCP","CloudSQL","Kubernetes","Terraform","AWS","certification-mcq","domain-weight-17"],"channel":"gcp-devops-engineer","subChannel":"optimize-performance","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:26:56.722Z","createdAt":"2026-01-12 22:26:56"},{"id":"gcp-devops-engineer-optimize-performance-1768256815601-4","question":"In a multi-region GKE deployment, you want to protect against regional outages and minimize user impact during a region failure. Which combination is best practice?","answer":"[{\"id\":\"a\",\"text\":\"Use a single region with autoscaling\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Deploy clusters in multiple regions and use a Global HTTP(S) Load Balancer to route traffic to healthy regions\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use only Cloud Run\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Cloud Functions exclusively for all traffic\",\"isCorrect\":false}]","explanation":"The recommended approach for regional resilience is to deploy clusters in multiple regions and front them with a Global HTTP(S) Load Balancer, enabling automatic routing to healthy regions with low latency. A single-region deployment cannot tolerate regional outages. Cloud Run or Cloud Functions can complement, but relying exclusively on them without cross-region routing leaves you vulnerable to regional failures.","diagram":null,"difficulty":"intermediate","tags":["GCP","Kubernetes","Terraform","GlobalLoadBalancing","AWS","certification-mcq","domain-weight-17"],"channel":"gcp-devops-engineer","subChannel":"optimize-performance","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:26:56.888Z","createdAt":"2026-01-12 22:26:56"},{"id":"gcp-devops-engineer-service-monitoring-1768224622960-0","question":"You are deploying a microservices application on GKE and want to monitor per-service latency with automated SLO-based alerts. Which approach best enables precise per-service SLIs and alerting in Cloud Monitoring?","answer":"[{\"id\":\"a\",\"text\":\"Create a single global SLO across all services and rely on aggregate latency\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Define a Cloud Monitoring Service for each microservice, instrument metrics for latency, configure per-service SLOs, and set separate alerting policies\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use only log-based metrics and compute SLIs offline in dashboards\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely solely on a single uptime check to measure availability\",\"isCorrect\":false}]","explanation":"## Correct Answer\nDefine a Cloud Monitoring Service for each microservice, instrument latency metrics, configure per-service SLOs, and attach separate alerting policies. This enables precise SLIs per service and targeted alerts.\n\n## Why Other Options Are Wrong\n- A: A global SLO hides per-service performance differences and can miss service-specific regressions.\n- C: Log-based metrics alone do not provide structured SLI/SLO calculation or per-service alerting capabilities.\n- D: A single uptime check only covers availability, not latency or per-service SLOs.\n\n## Key Concepts\n- Cloud Monitoring Service, SLI, SLO\n- Per-service metrics in multi-service environments\n- Alerting policies and dashboards per service\n- GKE instrumentation and service-level visibility\n\n## Real-World Application\nTeams implement per-service SLOs for critical microservices, create dashboards showing latency distributions per service, and configure alerts that fire only for the affected service, reducing noise and speeding incident response.","diagram":null,"difficulty":"intermediate","tags":["GKE","Cloud Monitoring","OpenTelemetry","Prometheus","Kubernetes","SLI","SLO","GCP","certification-mcq","domain-weight-20"],"channel":"gcp-devops-engineer","subChannel":"service-monitoring","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:30:22.961Z","createdAt":"2026-01-12 13:30:23"},{"id":"gcp-devops-engineer-service-monitoring-1768224622960-1","question":"To detect cold starts in a serverless workload (Cloud Run / Cloud Functions) and ensure timely responses, which monitoring approach best captures latency distribution and cold-start duration?","answer":"[{\"id\":\"a\",\"text\":\"Use Cloud Monitoring metrics for serverless services including request_latency and enable Cloud Trace to detect cold starts\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely on application logs alone to infer latency without metrics or traces\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use only uptime checks to measure latency\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a third-party monitoring tool that is not integrated with Cloud Trace\",\"isCorrect\":false}]","explanation":"## Correct Answer\nLeverage Cloud Monitoring metrics for serverless latency (e.g., request_latency) and enable Cloud Trace to capture traces, which together reveal cold-start durations and overall latency distribution.\n\n## Why Other Options Are Wrong\n- B: Logs alone do not provide structured latency distributions or cold-start visibility.\n- C: Uptime checks measure availability, not latency or cold-start metrics.\n- D: External tools may not integrate with Cloud Trace, reducing visibility for Cloud-native serverless workloads.\n\n## Key Concepts\n- Cloud Monitoring metrics for serverless\n- Cloud Trace integration\n- Cold-start analysis\n- Latency distribution\n\n## Real-World Application\nOperations teams monitor latency spikes during traffic ramps and use traces to identify cold-start delays, enabling faster tuning of memory/shutdown settings or concurrency configurations.","diagram":null,"difficulty":"intermediate","tags":["Cloud Run","Cloud Functions","Cloud Monitoring","Cloud Trace","Kubernetes","OpenTelemetry","GCP","certification-mcq","domain-weight-20"],"channel":"gcp-devops-engineer","subChannel":"service-monitoring","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:30:23.491Z","createdAt":"2026-01-12 13:30:23"},{"id":"gcp-devops-engineer-service-monitoring-1768224622960-2","question":"Your GKE deployment spans multiple namespaces and services; you want to avoid alert fatigue by deduplicating alerts and routing incidents to the right on-call rotation. What approach should you take?","answer":"[{\"id\":\"a\",\"text\":\"Create a single global alerting policy for all metrics to simplify notifications\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create per-service alerting policies with consistent labels and a multi-channel notification setup, enabling incident routing and dedup across related conditions\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use only email alerts and avoid on-call schedules to minimize overhead\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely exclusively on logs-based alerts without metrics-based conditions\",\"isCorrect\":false}]","explanation":"## Correct Answer\nPer-service alerting policies with consistent labels and a centralized notification channel enable targeted routing and dedup across related conditions, reducing alert fatigue while preserving visibility.\n\n## Why Other Options Are Wrong\n- A: A single global policy increases noise and makes it hard to distinguish which service is affected.\n- C: Email-only alerts and no on-call rotation lead to slow response and missed issues.\n- D: Logs-based alerts without metrics can miss quantitative thresholds and cross-service correlations.\n\n## Key Concepts\n- Alert policy scoping and labeling\n- Incident routing and deduplication\n- Multi-channel notifications\n- Service-level visibility in a multi-namespace cluster\n\n## Real-World Application\nOn incident, the on-call engineer receives a concise, service-specific incident with context, enabling faster triage and resolution without sifting through unrelated alerts.","diagram":null,"difficulty":"intermediate","tags":["GKE","Cloud Monitoring","Kubernetes","Prometheus","Incident Management","OpenTelemetry","GCP","certification-mcq","domain-weight-20"],"channel":"gcp-devops-engineer","subChannel":"service-monitoring","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:30:24.030Z","createdAt":"2026-01-12 13:30:24"},{"id":"gcp-devops-engineer-service-monitoring-1768224622960-3","question":"To reliably monitor external dependencies (e.g., third-party APIs) from multiple regions, which monitoring approach provides visibility into both availability and latency across geographies?","answer":"[{\"id\":\"a\",\"text\":\"Rely on internal metrics only and assume external dependencies mirror internal behavior\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Cloud Monitoring uptime checks to poll external endpoints from multiple regions and attach them to a composite SLO for external dependencies\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use only logs from your service to infer external dependency failure\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a single synthetic check from a single region to represent global availability\",\"isCorrect\":false}]","explanation":"## Correct Answer\nUtilize Cloud Monitoring uptime checks to poll external dependency endpoints from multiple regions and bind them to a composite SLO, giving visibility into regional availability and latency.\n\n## Why Other Options Are Wrong\n- A: Internal metrics cannot reflect external dependency health across regions.\n- C: Logs do not provide proactive, region-aware availability measurements for external calls.\n- D: A single-region check misses regional variations in availability and latency.\n\n## Key Concepts\n- Uptime checks for external endpoints\n- Multi-region monitoring\n- Service dependencies and SLOs\n- Proactive availability visibility\n\n## Real-World Application\nDuring outages, teams can verify if an external API is unreachable regionally or if latency deteriorates in specific geographies, enabling targeted remediation or failover planning.","diagram":null,"difficulty":"intermediate","tags":["GKE","Cloud Monitoring","Cloud Uptime Checks","External Dependencies","SLO","Multi-Region","GCP","certification-mcq","domain-weight-20"],"channel":"gcp-devops-engineer","subChannel":"service-monitoring","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:30:24.214Z","createdAt":"2026-01-12 13:30:24"},{"id":"gcp-devops-engineer-service-monitoring-1768224622960-4","question":"To minimize instrumentation effort across many services while gaining unified visibility, which setup is most appropriate for Cloud Monitoring using OpenTelemetry?","answer":"[{\"id\":\"a\",\"text\":\"Instrument each service with bespoke metrics manually and export them individually to Cloud Monitoring\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Deploy an OpenTelemetry Collector in a centralized manner and configure automatic instrumentation/export to Cloud Monitoring for metrics and traces\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rely solely on application logs and derive metrics post hoc\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a third-party agent that does not integrate with Cloud Monitoring\",\"isCorrect\":false}]","explanation":"## Correct Answer\nUsing a centralized OpenTelemetry Collector with automatic instrumentation and exporting to Cloud Monitoring provides broad visibility with minimal per-service code changes.\n\n## Why Other Options Are Wrong\n- A: Manual per-service instrumentation is labor-intensive and error-prone, especially at scale.\n- C: Logs alone do not provide structured metrics or real-time dashboards.\n- D: Non-integrated third-party tools reduce visibility and complicate incident response.\n\n## Key Concepts\n- OpenTelemetry Collector architecture\n- Automatic instrumentation and exporters\n- Cloud Monitoring integration\n- Unified telemetry across services\n\n## Real-World Application\nA mid-to-large organization rolls out a centralized OTEL collector to standardize metrics and traces, reducing engineering toil while enabling consistent dashboards and alerting in Cloud Monitoring.","diagram":null,"difficulty":"intermediate","tags":["OpenTelemetry","Cloud Monitoring","GKE","Cloud Run","Kubernetes","Prometheus","GCP","certification-mcq","domain-weight-20"],"channel":"gcp-devops-engineer","subChannel":"service-monitoring","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:30:24.394Z","createdAt":"2026-01-12 13:30:24"},{"id":"gcp-devops-engineer-site-reliability-1768199630277-0","question":"Which approach best ensures alerting is meaningful and reduces noise when latency spikes occur in a microservices app deployed on GKE?","answer":"[{\"id\":\"a\",\"text\":\"Alert on the 99th percentile latency of a single service across all regions\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Define SLOs and alert on the burn rate of the error budget using an aggregated SLI across affected services\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Monitor CPU usage of pods and alert when it exceeds a fixed threshold\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Only alert when the entire system is unavailable\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because it ties alerts to user-facing reliability by using SLIs/SLOs and monitoring the error-budget burn rate across services, enabling timely, focused remediation.\n\n## Why Other Options Are Wrong\n- A: Per-service percentile latency can miss cross-service impact and may produce noisy alerts if not scoped properly.\n- C: CPU usage is a resource metric, not a direct reliability signal for end-user experience.\n- D: Waiting for a full outage ignores partial degradations that impact users.\n\n## Key Concepts\n- SLIs, SLOs, and error budgets\n- Burn rate-based alerting\n- Service-wide observability\n\n## Real-World Application\n- Use Cloud Monitoring to define SLIs for latency and error rate across services, configure alerting policies on burn rate, and trigger runbooks to rollback or scale resources when thresholds are breached.","diagram":null,"difficulty":"intermediate","tags":["GKE","Kubernetes","SRE","Cloud Monitoring","SLI","Terraform","certification-mcq","domain-weight-23"],"channel":"gcp-devops-engineer","subChannel":"site-reliability","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:33:50.279Z","createdAt":"2026-01-12 06:33:50"},{"id":"gcp-devops-engineer-site-reliability-1768199630277-1","question":"You are releasing a critical microservice on GKE and want to roll out a new version to a small fraction of traffic for canary testing with automatic rollback if metrics degrade. Which approach should you choose?","answer":"[{\"id\":\"a\",\"text\":\"RollingUpdate with default maxSurge and maxUnavailable\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Canary deployment using a service mesh (for example Istio or Traffic Director) with traffic splitting and metrics-driven rollback\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Blue/Green deployment with manual switch after health checks\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Deploy to production with no canary or monitoring\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because a service mesh enables fine-grained traffic splitting for canary rollouts and allows automatic rollback based on defined metrics (latency, error rate).\n\n## Why Other Options Are Wrong\n- A: RollingUpdate does not natively support granular canary traffic percentages or automated rollback based on metrics.\n- C: Blue/Green can be safe but is less flexible for gradual exposure and automated rollback compared to a canary with a mesh.\n- D: Deploying to production without gates increases risk and does not meet canary safety criteria.\n\n## Key Concepts\n- Canary deployments\n- Traffic splitting via Istio/Traffic Director\n- Metrics-driven rollback\n\n## Real-World Application\n- Deploy new revision to 5–10% of traffic, monitor latency/error rate, and incrementally increase traffic or revert automatically if metrics exceed thresholds.","diagram":null,"difficulty":"intermediate","tags":["GKE","Istio","Traffic Director","SRE","Kubernetes","certification-mcq","domain-weight-23"],"channel":"gcp-devops-engineer","subChannel":"site-reliability","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:33:51.060Z","createdAt":"2026-01-12 06:33:51"},{"id":"gcp-devops-engineer-site-reliability-1768199630277-2","question":"To achieve low RPO and automatic failover for a globally accessed application, which approach is recommended in GCP?","answer":"[{\"id\":\"a\",\"text\":\"Use a single-region database with nightly backups\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Cloud Spanner in multi-region configuration with synchronous replication and a global load balancer\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use Cloud SQL with cross-region read replicas and manual failover\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Firestore in regional mode with failover to another region configured manually\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because Cloud Spanner in multi-region mode provides globally distributed, strongly consistent data with automatic failover, delivering near-zero RPO and rapid recovery.\n\n## Why Other Options Are Wrong\n- A: Single-region DBs cannot meet global RPO requirements and rely on backups rather than live failover.\n- C: Cross-region read replicas in Cloud SQL are typically asynchronous and require manual failover, increasing RPO.\n- D: Firestore regional mode does not provide the same strong global consistency and automatic regional failover guarantees as Spanner for workloads requiring global distribution.\n\n## Key Concepts\n- Global distribution and multi-region replication\n- Automatic failover and low RPO\n- Global load balancing\n\n## Real-World Application\n- For a globally active app, deploy the primary database in Spanner multi-region with Cloud Load Balancing to route traffic across continents, ensuring uninterrupted access during regional outages.","diagram":null,"difficulty":"intermediate","tags":["GCP","Cloud Spanner","Global Load Balancer","SRE","Terraform","GKE","certification-mcq","domain-weight-23"],"channel":"gcp-devops-engineer","subChannel":"site-reliability","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T06:33:51.611Z","createdAt":"2026-01-12 06:33:51"}],"subChannels":["bootstrap-gcp","build-delivery","general","optimize-performance","service-monitoring","site-reliability"],"companies":["Adobe","Amazon","Apple","Databricks","Discord","DoorDash","Hashicorp","Meta","Microsoft","NVIDIA","Plaid","Robinhood","Scale Ai","Snap","Square","Tesla","Zoom"],"stats":{"total":33,"beginner":1,"intermediate":30,"advanced":2,"newThisWeek":33}}