[{"id":"q-1158","question":"How would you implement a health check mechanism for a load balancer that uses exponential backoff for failed servers, and how does this approach prevent cascading failures during partial outages?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["load-balancing","health-checks","exponential-backoff","fault-tolerance","cascading-failures"],"companies":[]},{"id":"q-2121","question":"How would you implement a load balancer that uses predictive scaling based on request patterns and historical data, and what machine learning techniques would you use to forecast traffic spikes?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["predictive-scaling","machine-learning","time-series-forecasting","load-balancing"],"companies":[]},{"id":"q-2617","question":"How would you implement a consistent hashing load balancer that minimizes server remapping when adding or removing nodes, and what data structures would you use to achieve O(log n) lookup time?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["consistent-hashing","load-balancing","distributed-systems","data-structures"],"companies":[]},{"id":"q-2620","question":"How would you implement a consistent hashing load balancer that minimizes data remapping when servers are added or removed, and what trade-offs exist between ring size and lookup performance?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["consistent-hashing","load-balancing","distributed-systems","hash-rings","scalability"],"companies":[]},{"id":"q-3108","question":"How would you implement a load balancer with health checks that automatically removes failed servers and reintegrates them when they recover, while maintaining session affinity and handling graceful degradation?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["load-balancing","health-checks","session-affinity","circuit-breaker","graceful-degradation"],"companies":[]},{"id":"q-3109","question":"How would you implement a load balancer that uses exponential weighted moving average (EWMA) to track server response times and dynamically adjust traffic distribution, and how would you handle cold start problems for new servers?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["load-balancing","ewma","performance-metrics","cold-start","weighted-round-robin"],"companies":[]},{"id":"q-4595","question":"How would you implement a load balancer with health checks that automatically removes failed servers and reintegrates them when they recover, while maintaining consistent request distribution during server state changes?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["load-balancing","health-checks","fault-tolerance","distributed-systems"],"companies":[]},{"id":"q-4597","question":"How would you implement a load balancer with health checks and circuit breaker patterns to handle server failures gracefully, and what strategies would you use to prevent cascading failures during partial outages?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["load-balancing","health-checks","circuit-breaker","fault-tolerance","distributed-systems"],"companies":[]},{"id":"q-653","question":"How would you implement a consistent hashing load balancer that handles server additions and removals with minimal key remapping? What data structures would you use and how would you handle virtual nodes?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["consistent-hashing","load-balancing","distributed-systems","hash-ring","virtual-nodes"],"companies":[]},{"id":"q-659","question":"How would you implement a consistent hashing load balancer and what advantages does it provide over traditional hash-based load balancing when servers are added or removed?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["load-balancing","consistent-hashing","distributed-systems","scalability"],"companies":[]},{"id":"q-767","question":"Design a load balancer that implements adaptive load balancing using real-time server metrics. How would you collect and weight server performance data, and what algorithm would you use to dynamically adjust traffic distribution?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["adaptive-load-balancing","performance-monitoring","dynamic-weighting","real-time-metrics"],"companies":[]},{"id":"al-1","question":"When would you choose a Linked List over an Array and what are the key trade-offs for each data structure?","channel":"algorithms","subChannel":"data-structures","difficulty":"beginner","tags":["struct","comparison","basics"],"companies":["Adobe","Amazon","Apple","Google","Meta","Microsoft"]},{"id":"al-165","question":"Implement a Trie data structure for efficient prefix search with insert, search, and startsWith operations. What are its advantages over hash maps for autocomplete systems, and what are the trade-offs?","channel":"algorithms","subChannel":"data-structures","difficulty":"intermediate","tags":["struct","basics"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"q-187","question":"How would you implement a thread-safe LRU cache using a HashMap and DoublyLinkedList, considering eviction policy and O(1) operations?","channel":"algorithms","subChannel":"data-structures","difficulty":"intermediate","tags":["arrays","linkedlist","hashtable","heap"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Uber"]},{"id":"q-277","question":"How would you efficiently process a 50GB log file to extract the top 10 most frequent IP addresses from millions of entries while handling memory constraints and optimizing for performance?","channel":"algorithms","subChannel":"data-structures","difficulty":"advanced","tags":["find","xargs","cut","sort"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Snowflake"]},{"id":"q-377","question":"Implement a min-heap using an array that supports insert, extractMin, and peek operations in O(log n) time. Include time/space complexity analysis and edge cases?","channel":"algorithms","subChannel":"data-structures","difficulty":"beginner","tags":["arrays","linkedlist","hashtable","heap"],"companies":null},{"id":"q-407","question":"Given a stream of log events with timestamps, design an algorithm to find the top K most frequent error messages in the last N minutes using O(K) space, where each event contains timestamp, error type, and message?","channel":"algorithms","subChannel":"data-structures","difficulty":"intermediate","tags":["arrays","linkedlist","hashtable","heap"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-418","question":"Design a data structure that supports range sum queries and point updates on a dynamic array with O(log n) operations. How would you implement this using a segment tree, and what are the trade-offs compared to a Binary Indexed Tree?","channel":"algorithms","subChannel":"data-structures","difficulty":"advanced","tags":["bst","avl","trie","segment-tree"],"companies":null},{"id":"q-425","question":"Given an array of integers and a target sum, find two numbers that add up to the target. How would you implement this efficiently and what's the time complexity?","channel":"algorithms","subChannel":"data-structures","difficulty":"beginner","tags":["arrays","linkedlist","hashtable","heap"],"companies":["Adobe","Amazon","Two Sigma"]},{"id":"q-442","question":"Given a stream of user actions with timestamps, design a system to find the top K most frequent actions in the last N minutes using O(1) time per query?","channel":"algorithms","subChannel":"data-structures","difficulty":"intermediate","tags":["arrays","linkedlist","hashtable","heap"],"companies":["LinkedIn","OpenAI"]},{"id":"q-565","question":"Given a stream of timestamped events, find the maximum number of concurrent events at any time?","channel":"algorithms","subChannel":"data-structures","difficulty":"advanced","tags":["arrays","linkedlist","hashtable","heap"],"companies":["Salesforce","Slack","Square"]},{"id":"al-152","question":"You have a staircase with n steps. You can climb 1, 2, or 3 steps at a time. How many distinct ways can you reach the top? Implement a solution with O(n) time and O(1) space?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"intermediate","tags":["dp","optimization"],"companies":null},{"id":"al-166","question":"Given a string, find the minimum cost to transform it into a palindrome where insertions cost 2 and deletions cost 1. What is the optimal dynamic programming approach?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"intermediate","tags":["dp","optimization"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"al-167","question":"Given a target sum n, count the number of ways to reach it using dice rolls where each roll can be 1-6. Return the result modulo 10^9+7. Optimize for O(n) time and O(1) space?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"intermediate","tags":["dp","optimization"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"al-170","question":"Given an array of integers where each element represents the maximum number of steps you can jump forward from that position, find the minimum number of jumps required to reach the last index. If it's not possible to reach the end, return -1. How would you implement this efficiently?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"advanced","tags":["dp","optimization"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"al-3","question":"What is Dynamic Programming and how does it differ from plain recursion? When would you choose one over the other?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"advanced","tags":["dp","optimization","theory"],"companies":["Amazon","Google","Meta"]},{"id":"q-328","question":"Given a grid of size m x n where each cell contains a non-negative integer representing the cost to enter that cell, find the minimum cost path from the top-left corner (0,0) to the bottom-right corner (m-1,n-1) moving only right or down. Return both the minimum cost and the path itself?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"intermediate","tags":["dp","memoization","tabulation"],"companies":["Amazon","Apple","Google","Meta","Microsoft","NVIDIA"]},{"id":"q-440","question":"Given a string s and dictionary wordDict, return all possible sentences where s can be segmented into space-separated words from wordDict. Handle overlapping subproblems efficiently?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"advanced","tags":["dp","memoization","tabulation"],"companies":["Adobe","Google","NVIDIA"]},{"id":"q-540","question":"Given a string s and a dictionary wordDict, return all possible sentences formed by inserting spaces in s such that each word exists in wordDict. Use DP with memoization to avoid exponential recomputation?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"advanced","tags":["dp","memoization","tabulation"],"companies":["Goldman Sachs","LinkedIn","Slack"]},{"id":"q-214","question":"Given a directed weighted graph with up to 10^6 edges and frequent edge weight updates, design a data structure that supports dynamic shortest path queries with sub-millisecond response time?","channel":"algorithms","subChannel":"graphs","difficulty":"advanced","tags":["bfs","dfs","dijkstra","topological"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Uber"]},{"id":"q-286","question":"Explain the difference between BFS and DFS and when would you use each?","channel":"algorithms","subChannel":"graphs","difficulty":"intermediate","tags":["bfs","dfs","dijkstra","topological"],"companies":["Amazon","Google","Meta"]},{"id":"q-350","question":"Given a directed graph representing city intersections and one-way streets, implement a function to find if there's a valid route from point A to point B using BFS. Return the shortest path distance or -1 if no route exists?","channel":"algorithms","subChannel":"graphs","difficulty":"beginner","tags":["bfs","dfs","dijkstra","topological"],"companies":["Amazon","Apple","Cruise","Google","Meta","Microsoft","Netflix","Vercel"]},{"id":"q-394","question":"Given a directed acyclic graph representing task dependencies where each task takes 1 unit of time and you have unlimited workers, what is the minimum time to complete all tasks?","channel":"algorithms","subChannel":"graphs","difficulty":"advanced","tags":["bfs","dfs","dijkstra","topological"],"companies":null},{"id":"q-4825","question":"Given a weighted directed graph with N nodes and M edges, positive weights, and two nodes s and t, implement a function that returns the shortest distance from s to t and the number of distinct shortest paths achieving that distance. Provide a signature and concise outline of the update rules, and explain complexity and edge cases?","channel":"algorithms","subChannel":"graphs","difficulty":"intermediate","tags":["bfs","dfs","dijkstra","topological"],"companies":["IBM","Scale Ai","Stripe"]},{"id":"q-662","question":"Given a directed graph with N nodes and M weighted edges (positive weights) and a source s, describe an implementation to (1) identify nodes reachable from s via BFS, (2) compute shortest distances dist[] to all nodes with Dijkstra, and (3) count the number of distinct shortest s→v paths for every v (mod 1e9+7). How would you build a DAG of edges on shortest paths and perform a topological DP over dist-ordered nodes to obtain path counts?","channel":"algorithms","subChannel":"graphs","difficulty":"advanced","tags":["bfs","dfs","dijkstra","topological"],"companies":["Anthropic","NVIDIA","Robinhood"]},{"id":"q-596","question":"Explain the differences between round-robin, least connections, and IP hash load balancing algorithms. When would you choose each one?","channel":"algorithms","subChannel":"load-balancing-algorithms","difficulty":"intermediate","tags":["load-balancing","algorithms","networking","scalability","system-design"],"companies":["Google","Amazon","Meta","Netflix","Microsoft","Twitter","Uber","Airbnb"]},{"id":"q-627","question":"Explain the difference between round-robin and weighted round-robin load balancing algorithms. When would you choose one over the other?","channel":"algorithms","subChannel":"load-balancing-algorithms","difficulty":"intermediate","tags":["load-balancing","algorithms","distributed-systems","networking"],"companies":["Amazon","Google","Microsoft","Netflix","Facebook","Twitter"]},{"id":"al-163","question":"You have an array where each element appears twice except one element that appears once. Sort the array in O(n) time without using extra space for sorting. How would you approach this?","channel":"algorithms","subChannel":"sorting","difficulty":"intermediate","tags":["sort","complexity"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"al-2","question":"Compare QuickSort, MergeSort, and Timsort. When would you choose each algorithm and what are their key trade-offs in production systems?","channel":"algorithms","subChannel":"sorting","difficulty":"intermediate","tags":["sort","recursion","complexity"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-300","question":"Explain the difference between quicksort and mergesort, including their time and space complexities?","channel":"algorithms","subChannel":"sorting","difficulty":"beginner","tags":["quicksort","mergesort","complexity"],"companies":["Amazon","Google","Meta"]},{"id":"q-362","question":"Given an array of integers, implement quicksort with proper partitioning, explain its O(n log n) average vs O(n²) worst-case complexity, and compare with mergesort in terms of stability, space usage, and practical performance?","channel":"algorithms","subChannel":"sorting","difficulty":"beginner","tags":["quicksort","mergesort","complexity"],"companies":null},{"id":"q-4032","question":"Design an in-place, stable sort for an array of n integers that uses only O(1) extra space and runs in O(n log n) time. Implement a stable in-place merge for two adjacent sorted blocks without extra buffers. Provide a function signature and code sketch, and analyze how you preserve order of equal elements when duplicates exist. Target large inputs with good cache behavior?","channel":"algorithms","subChannel":"sorting","difficulty":"advanced","tags":["quicksort","mergesort","complexity"],"companies":["Anthropic","Instacart"]},{"id":"q-4083","question":"Design a hybrid sort for an array of integers that blends 3-way quicksort and in-place stable mergesort. For partitions larger than 64 elements, use 3-way partition quicksort with median-of-three pivot; for partitions of size 64 or less, sort by in-place stable merge of consecutive runs. Achieve O(n log n) average time and O(1) extra space; explain pivot strategy, base cases, and how you guarantee stability and space bounds?","channel":"algorithms","subChannel":"sorting","difficulty":"intermediate","tags":["quicksort","mergesort","complexity"],"companies":["Coinbase","Google","LinkedIn"]},{"id":"q-433","question":"Implement quicksort and explain when you'd choose it over mergesort. What's the worst-case scenario and how do you avoid it?","channel":"algorithms","subChannel":"sorting","difficulty":"beginner","tags":["quicksort","mergesort","complexity"],"companies":["Hashicorp","Oracle","Snowflake"]},{"id":"q-4766","question":"In a high-throughput analytics pipeline, you must sort a large array of event timestamps in place. Design an in-place sort that guarantees O(n log n) worst-case. Implement an introsort-style hybrid: quicksort with median-of-three and 3-way partitioning, depth threshold 2 log2(n) to switch to heapsort, and use insertion sort for small partitions. Explain your choices and how this avoids O(n^2) on adversarial input?","channel":"algorithms","subChannel":"sorting","difficulty":"intermediate","tags":["quicksort","mergesort","complexity"],"companies":["Bloomberg","Hashicorp","Slack"]},{"id":"q-4848","question":"Given an array A of length N with many duplicates and D distinct keys (D << N). Design an in-place sort using a 3-way partition quicksort (less, equal, greater) that achieves expected time O(N log D). Explain your pivot strategy (randomized or sampled medians) to avoid O(N^2), and show how to limit extra space to O(1) aside from a tiny recursion/loop stack. Include edge cases like all elements equal?","channel":"algorithms","subChannel":"sorting","difficulty":"advanced","tags":["quicksort","mergesort","complexity"],"companies":["Apple","Oracle","Twitter"]},{"id":"q-167","question":"Write a function to find the maximum depth of a binary tree using both recursive DFS and iterative BFS approaches. Discuss time/space complexity and handle edge cases?","channel":"algorithms","subChannel":"trees","difficulty":"beginner","tags":["tree","binary"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-314","question":"Given a binary search tree with n nodes, find the kth smallest element where 1 ≤ k ≤ n. Discuss both recursive and iterative approaches with their time and space complexities?","channel":"algorithms","subChannel":"trees","difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"companies":null},{"id":"q-340","question":"Given a BST that may have duplicate values, implement a function to find the kth smallest element considering duplicates. What's the time complexity and how would you handle edge cases?","channel":"algorithms","subChannel":"trees","difficulty":"intermediate","tags":["bst","avl","trie","segment-tree"],"companies":["Hrt","New Relic","Sap"]},{"id":"q-4059","question":"Scenario: A code editor needs path auto-complete. Implement a Trie that stores file paths as sequences of characters, with methods insertPath(path) and autocomplete(prefix, maxResults). Explain how you would treat '/' as a separator, choices for child storage, and the time/space trade-offs; ensure insert is O(length) and autocomplete is O(length(prefix) + maxResults)?","channel":"algorithms","subChannel":"trees","difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"companies":["Adobe","Bloomberg","NVIDIA"]},{"id":"q-4249","question":"Design a dynamic dictionary that stores words with frequencies and supports insert(word, delta), delete(word), and queryTopK(prefix, k) returning the k highest-frequency words starting with prefix (ties broken lexicographically). Propose a Trie-based solution where each node keeps a BST of (freq, word) for its subtree; explain how updates propagate and the expected complexities under 2e5 words and 1e5 operations?","channel":"algorithms","subChannel":"trees","difficulty":"intermediate","tags":["bst","avl","trie","segment-tree"],"companies":["Google","Netflix","Scale Ai"]},{"id":"q-4383","question":"Design a real-time autocomplete structure for a high-traffic analytics portal. Implement insert(word) that increments word frequency, delete(word) to remove a word entirely, and getSuggestions(prefix, limit) returning up to limit words starting with prefix ordered by frequency (desc) then lexicographically. Target scales: ~2e6 inserts, ~1e5 prefix queries per second. Propose a Trie where each node holds an AVL of (freq, word); explain update mechanics, complexity, and memory trade-offs; sketch a concise implementation outline?","channel":"algorithms","subChannel":"trees","difficulty":"intermediate","tags":["bst","avl","trie","segment-tree"],"companies":["MongoDB","Plaid","Uber"]},{"id":"q-451","question":"Given a BST, write a function to find the kth smallest element using O(h) space and O(n) time, where h is height and n is nodes?","channel":"algorithms","subChannel":"trees","difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"companies":["Anthropic","Cloudflare"]},{"id":"q-660","question":"You’re building a chat app that autocompletes words as users type. Implement a Trie with insert(word), search(word), and startsWith(prefix). Provide a compact JavaScript class with these methods, assuming lowercase a-z. After inserting 'apple','app','application', does search('app') return true and does startsWith('appl') return true?","channel":"algorithms","subChannel":"trees","difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"companies":["Cloudflare","IBM","Slack"]},{"id":"q-258","question":"How would you design a reactive Android ViewModel using StateFlow with sealed classes to handle network API responses, ensuring proper error handling and loading states?","channel":"android","subChannel":"architecture","difficulty":"intermediate","tags":["coroutines","flow","sealed-classes"],"companies":["Airbnb","Google","Meta","Microsoft","Uber"]},{"id":"q-1022","question":"You're building an Android field-inspection app where users collect forms and photos offline. When connectivity returns, design a robust sync engine that uploads only new or updated items, resolves conflicts via last_modified, and handles intermittent networks. Use WorkManager with network constraints and a foreground service for long syncs; include backoff and tests?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Airbnb","Goldman Sachs","Scale Ai"]},{"id":"q-1127","question":"Design a beginner Android feature: a simple offline notes app. Notes stored in Room with fields id, text, tag, last_modified. Provide a tag-based search, and a background export to cloud via WorkManager that runs only on WiFi and while charging. Ensure deduplication by last_modified, handle restarts, and outline a minimal test plan?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Anthropic","Robinhood"]},{"id":"q-1143","question":"You’re building a chat-like Android app. Messages are stored in Room with fields: id (UUID), text (String), timestamp (Long), status (pending, sending, sent, failed). When sending, insert a pending message and enqueue a WorkManager task to upload unsent messages when online, using exponential backoff. On success, save serverId and set status to sent; on failure, keep failed with a retriable option. Outline the data flow, DAO/Worker skeleton, and offline→online test plan?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Snowflake","Twitter"]},{"id":"q-1267","question":"You're building an Android offline-first app that records field observations (id, timestamp, value) in Room. Changes are queued when offline and synced to a REST backend when online. Implement a robust sync with versioning, conflict resolution (server-wins, client-wins, or merge), and tombstones. Describe data flow, DAO/Repository, a WorkManager worker, and a testing strategy for edge cases like concurrent edits and delayed pushes?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Cloudflare","Hugging Face","Tesla"]},{"id":"q-1388","question":"Implement a field survey photo capture in Android: tap Capture to take a photo, request CAMERA permission via the Activity Result API, use TakePicturePreview to obtain a Bitmap, save it to internal storage with a timestamped filename, and display a preview in an ImageView. Ensure rotation safety and a graceful denial/retry flow. What code would you write?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Adobe","Bloomberg","Google"]},{"id":"q-1715","question":"You're building an Android module to support background, continuous BLE logging from an OBD-II dongle for a fleet-tracking app. Requirements: maintain a stable BLE connection with automatic reconnect and backoff; buffer data locally in Room during disconnect; periodically upload batched logs via WorkManager with exponential backoff; use a ForegroundService while in the background; respect Doze/App Standby; outline data flow, architecture, and a minimal skeleton of the BLE manager, DAO, and Worker; include a test plan for connectivity changes and battery constraints?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Databricks","Lyft"]},{"id":"q-1880","question":"You’re building an Android app that streams live telemetry video from a drone's camera over an unstable network. Using CameraX, encoding with H.264, and a hybrid UDP/TCP transport, design a solution that includes buffering and backpressure, a foreground service with Doze-friendly behavior, adaptive bitrate, and robust error handling. Provide data flow, key classes (StreamingManager, PacketSender), and a skeleton implementation plus an end-to-end test plan for packet loss and latency?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Cloudflare","Oracle"]},{"id":"q-2076","question":"You're building a beginner Android app that logs field visits. The main screen shows a list of visits (id, siteName, timestamp). Implement adding a visit via a dialog, persist with Room, and a DataStore-backed sort toggle (timestamp ascending/descending) that preserves the choice across rotations. Describe data flow, DAO/Repository, ViewModel, and a minimal UI test plan?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Amazon","Salesforce","Uber"]},{"id":"q-2143","question":"You're building a beginner Android checklist app: store items in Room (id, title, completed, dueDate), display them in a RecyclerView, and support add/edit/delete. Implement a one-time reminder using WorkManager that triggers a Notification at dueDate, with a backoff if scheduling fails or the device is asleep. Describe data flow (DAO/Repository), Worker implementation, and a test plan for offline item creation and overdue reminders?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Apple","Meta"]},{"id":"q-2195","question":"You're building an Android field tool app used by technicians to annotate images and run on-device ML for scene classification with offline-first data collection. The app must train a lightweight model locally from user-labeled samples, support incremental model updates delivered from the backend, and handle intermittent connectivity. Design end-to-end architecture and data flow using Room, DataStore, WorkManager, and TensorFlow Lite. Address offline training, model versioning, delta updates, rollback, and testing under Doze and flaky networks?","channel":"android","subChannel":"general","difficulty":"advanced","tags":["android"],"companies":["Hugging Face","Lyft","Oracle"]},{"id":"q-2216","question":"You're building an Android offline-first note app that provides fast full-text search across local notes and syncs with a central server. Implement using Room + FTS for search on title/content, a tombstone strategy, and a versioned sync via WorkManager with exponential backoff. Describe the data model (Note: id, title, content, updatedAt, version, tombstone), DAO/FTS setup, sync flow, and a test plan for offline edits, conflicts, and index consistency?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Airbnb","Hashicorp","MongoDB"]},{"id":"q-2447","question":"Design an Android enterprise app feature delivery strategy using dynamic feature modules and server-driven flags. Describe how you would (1) ship features on demand with Play Feature Delivery, (2) implement server-driven flags with offline caching, versioning, and safe rollbacks, (3) support canary and percentage rollout per device, (4) ensure startup safety and crash resilience, and (5) validate with test plans and telemetry?","channel":"android","subChannel":"general","difficulty":"advanced","tags":["android"],"companies":["Hashicorp","Microsoft"]},{"id":"q-2547","question":"Design an Android module that performs on-device NLP inference using a Hugging Face model for chat intent detection. Build a ModelManager with components: ModelRepo (versioned storage), InferenceEngine (off-UI thread), MemoryMonitor, and Updater (WorkManager with backoff). It should load from internal storage, download updates when online, switch between a large and compact model based on memory, support cancellation with timeouts, and rollback on failure. Describe architecture, data flow, and a concrete test plan for offline updates, memory pressure, and model-load failure?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Hashicorp","Hugging Face"]},{"id":"q-2587","question":"You're building a beginner Android feature that records in-app interactions as Event records (id, screen, action, ts) in Room. When online, batch-upload unsynced events to a REST API via WorkManager with a network constraint and exponential backoff. Show your data model, DAO/repository, how you mark events synced, batch sizing, and a basic test plan for offline queueing and retry behavior?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["IBM","Instacart","Tesla"]},{"id":"q-2664","question":"You're building an Android module for real-time, offline-first collaborative document editing. Each document uses a CRDT to merge concurrent edits locally and remotely. The app syncs with a central server over WebSocket with a REST fallback; local changes are persisted in Room; security via per-document encryption keys. Describe architecture, data model, CRDT design, sync protocol, and testing plan?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["IBM","Robinhood"]},{"id":"q-2715","question":"Design an Android chat module that delivers end-to-end encrypted real-time messaging for a multi-device collaboration app. Implement a Signal-like ratchet, store device keys in Android Keystore, perform nightly ephemeral key rotation, and queue outgoing messages in Room for offline delivery. Describe architecture, data flow, storage schema, and a test plan for offline queuing, key rotation, and out-of-order delivery under churn?","channel":"android","subChannel":"general","difficulty":"advanced","tags":["android"],"companies":["Hashicorp","MongoDB","Zoom"]},{"id":"q-2809","question":"Design an Android module that streams a live collaborative whiteboard using WebRTC data channels for real-time updates, with offline queuing when network is unavailable, persisting events in Room, and ensuring eventual consistency under churn. Include architecture, data model, delta-encoding, and a test plan for offline/online transitions?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Adobe","Slack","Snap"]},{"id":"q-2824","question":"Design an Android module for offline-first, real-time co-op game state using CRDTs to merge concurrent updates; use Room for local persistence, WebSocket for server sync, and per-user encryption keys stored in Android Keystore with nightly ephemeral key rotation. Describe architecture, data model, CRDT design, sync protocol, and a test plan for latency, churn, and conflict resolution?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Discord","Goldman Sachs","Meta"]},{"id":"q-2944","question":"Design an Android module to support dynamic, per-user feature flags delivered via Play Feature Delivery. Include: a per-user flag cache in Room, a module loader with SplitCompat, a policy for safe rollout/rollback, and a testing strategy for offline-first fetch and partial module failures. How would you structure components and data flow?","channel":"android","subChannel":"general","difficulty":"advanced","tags":["android"],"companies":["Lyft","Microsoft","PayPal"]},{"id":"q-3082","question":"Design an Android module for a messaging app that delivers messages primarily through device-to-device P2P (Nearby Connections or WebRTC data channels) when devices are in proximity, with seamless server fallback. Requirements: Kotlin, Coroutines, Room-based offline log, per-device keys stored in Android Keystore with nightly rotation, and a WorkManager-based reconciliation when online. Define data model, conflict resolution, and a concrete test plan for proximity churn, out-of-order delivery, and offline queues?","channel":"android","subChannel":"general","difficulty":"advanced","tags":["android"],"companies":["Cloudflare","Square"]},{"id":"q-3159","question":"Design an Android module for a privacy-first group audio chat app (3+ participants) with end-to-end encryption. Use per-device ephemeral keys rotated nightly in Android Keystore, a group-key envelope for media, and WebRTC for low-latency audio with a jitter buffer. Signaling can fallback to a server. Transcripts stored in Room for offline access. Describe architecture, key management, data flow, and a concrete test plan for membership changes, rotation, offline queueing, and audio dropout?","channel":"android","subChannel":"general","difficulty":"advanced","tags":["android"],"companies":["Discord","DoorDash","Twitter"]},{"id":"q-3214","question":"Design an Android module for an offline-first wallet that queues card-present payments while offline and reconciles them when online, guaranteeing no duplicate charges. Use Kotlin, Coroutines, Room for local log, per-device signing keys in Android Keystore with nightly rotation, and a WorkManager-based reconciliation flow. Describe data model, idempotency strategy, conflict resolution, and a test plan for offline queues, replays, and out-of-order delivery?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Meta","Plaid"]},{"id":"q-3273","question":"Design an Android module for a real-time, offline-first collaborative data editor (like a shared spreadsheet) that uses CRDTs to merge edits across devices when online, supports per-device cryptographic identities stored in Android Keystore with nightly rotation, and reconciles with a central server via WebSocket. Describe data model, conflict resolution, offline queues, and test plan for churn, out-of-order edits, and data integrity?","channel":"android","subChannel":"general","difficulty":"advanced","tags":["android"],"companies":["Apple","Instacart","Plaid"]},{"id":"q-3459","question":"Design an Android module for offline-first moderation flags: users can flag posts while offline; flags stored in Room with per-device signing keys in Android Keystore (nightly rotation); on connectivity, reconcile with a central moderation service via REST; ensure idempotent submissions, deduplication, and conflict resolution when devices assign different verdicts. Describe data model, sync protocol, and a test plan for offline queues, duplicates, and reconciliation latency?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Cloudflare","MongoDB","Plaid"]},{"id":"q-3679","question":"Design an Android module for a live, synchronized music player that allows nearby devices to form a low-latency playback mesh using Nearby Connections, while seamlessly falling back to a central streaming server when devices are out of range. Requirements: Kotlin, Coroutines, ExoPlayer for playback, Room for an offline queue, per-device signing keys stored in Android Keystore with nightly rotation, and a WorkManager-based reconciliation when back online. Describe architecture, data flow, synchronization protocol for drift, conflict resolution, and a concrete test plan for clock drift, churn, and offline queue recovery?","channel":"android","subChannel":"general","difficulty":"advanced","tags":["android"],"companies":["Google","Netflix","Uber"]},{"id":"q-3714","question":"Design an Android module for privacy-preserving offline sensor data collection: collect accelerometer, gyroscope, and GPS samples locally; store in Room with per-device cryptographic identity in Android Keystore with nightly rotation; batch and upload when online via REST, applying privacy transforms before sending; ensure dedup, data integrity, and minimal battery impact. Describe data model, privacy pipeline, sync protocol, and tests for data drift and rotation?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Apple","Twitter"]},{"id":"q-3766","question":"Design an Android module for an offline-first, privacy-preserving feature-flag system for a delivery app. Each device signs flag changes with a per-device key stored in Android Keystore, rotated nightly. Flags are stored locally (Room or DataStore). When online, reconcile with the central service via REST/WebSocket, ensuring idempotent updates and cross-device conflict resolution within the same account. Include data model, sync protocol, and a test plan for offline toggles and churn?","channel":"android","subChannel":"general","difficulty":"advanced","tags":["android"],"companies":["Instacart","Lyft"]},{"id":"q-3807","question":"Design an Android module for offline-first code-review feedback in a mobile code-review app used by engineers at Goldman Sachs and IBM. Allow creating threads with comments and replies while offline; store in Room; sign edits with per-device keys in Android Keystore with nightly rotation; reconcile with a central server via WorkManager when online. Describe data model, idempotency, conflict resolution, and a test plan for offline queues, replays, and out-of-order edits?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Goldman Sachs","IBM"]},{"id":"q-3857","question":"Design an Android analytics module for a DoorDash-like app that collects user events offline and uploads in batches while preserving tenant isolation. Include per-device identity in Android Keystore with nightly rotation, a Room-based append-only log, and encrypted at-rest storage. Detail data schema, encryption/signing, batch sizing, and a WorkManager-driven upload with retry, plus a test plan for offline bursts and rotation?","channel":"android","subChannel":"general","difficulty":"advanced","tags":["android"],"companies":["DoorDash","Plaid"]},{"id":"q-3901","question":"Design an Android module that enables on-device, offline-first personalized ML model updates via delta patches from a central server for a messaging/collaboration app; patch payloads are stored in Room, applied with WorkManager idle windows; per-device signing keys live in Android Keystore with nightly rotation; verify integrity with HMAC, support rollback on patch failure, and reconcile versions when online; expose API for current model version and inference results; outline data model, patch format, consistency guarantees, and a test plan for patch failures, offline-first scenarios, and reconciliation latency?","channel":"android","subChannel":"general","difficulty":"advanced","tags":["android"],"companies":["Instacart","Lyft","Slack"]},{"id":"q-3936","question":"Design an Android module for a multi-tenant enterprise app that isolates data per tenant while devices can switch tenants on the fly. Each tenant has its own encryption envelope stored in Android Keystore; local data in Room partitions by tenant; nightly policy issuance from a central server may rotate keys and re-key data. Implement data model, key lifecycle, migration/rekey strategy, and a test plan for tenant switch, key rotation, offline edits, and reconciliation latency?","channel":"android","subChannel":"general","difficulty":"advanced","tags":["android"],"companies":["Bloomberg","LinkedIn","Netflix"]},{"id":"q-4002","question":"Design a beginner-friendly Android module to provide weather data with offline caching. The app fetches from a REST API when online, stores responses in Room, and shows the last cached data when offline. Include a simple WeatherData schema, an idempotent fetch/retry strategy, and a WorkManager-based background refresh that runs when connectivity returns. Explain data flow and a minimal test plan?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Google","IBM","NVIDIA"]},{"id":"q-4227","question":"Design an Android module that maintains an on-device full-text search index for a chat app using Room's FTS, and keeps it in sync with the server via incremental delta patches. Patches include document updates and deletions, plus a version. Apply patches using a WorkManager idle window; keys live in Android Keystore and rotate nightly; patch integrity is verified via HMAC. Support rollback on failure and reconciliation when back online. Provide data model, patch format, update strategy, conflict handling, and a test plan for patch failures, offline edits, and search latency?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["MongoDB","PayPal"]},{"id":"q-4338","question":"Design an Android module for a real-time collaborative document editor that uses CRDTs to enable offline edits and conflict-free merging when devices reconnect. Explain the data model (document as a CRDT sequence), operation encoding, offline queue, per-device encryption with Android Keystore, and server reconciliation with a MongoDB Atlas backend. Include a concrete test plan for multi-device offline edits, latency, and audit trails?","channel":"android","subChannel":"general","difficulty":"advanced","tags":["android"],"companies":["IBM","MongoDB"]},{"id":"q-4424","question":"Design an Android module for privacy-preserving contact sync in a messaging app. The client never uploads plaintext contacts; use deterministic IDs, Bloom filters, and patch-based deltas from a server to update local contact groups stored in Room. Use Android Keystore keys with nightly rotation for signing, and WorkManager for reconciliation when online. Detail data model, patch format, privacy guarantees, rollback, and a pragmatic test plan?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Discord","Google"]},{"id":"q-452","question":"How would you implement a RecyclerView with multiple view types while maintaining smooth scrolling performance on large datasets?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Adobe","Citadel","Google"]},{"id":"q-4639","question":"Design an Android module for on-device personalized text suggestions using federated learning for a messaging app. Requirements: load a shared quantized model on-device, collect user interaction deltas offline, encrypt deltas with per-device keys in Android Keystore (with attestation), nightly updates via WorkManager, send encrypted deltas to a central aggregator via gRPC using secure aggregation and differential privacy, apply aggregated model locally when online, and support per-tenant data isolation in enterprise contexts. Explain data flow, encryption, model format, and a concrete test plan?","channel":"android","subChannel":"general","difficulty":"advanced","tags":["android"],"companies":["Amazon","Slack","Twitter"]},{"id":"q-4683","question":"Design an Android module for energy-aware offline analytics: queue events in Room, batch up to 100 events or 512KB, and flush only when the device is charging and connected to unmetered Wi-Fi; use WorkManager with constraints, Retrofit for batch upload, and a server endpoint that returns batch-ids for idempotent processing. Explain data model, batching strategy, failure handling, and testing across Doze and network churn?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Databricks","Google","PayPal"]},{"id":"q-4760","question":"Design an Android module for offline-first asset-tracking in a manufacturing setting. Devices collect telemetry and RFID scans; central server publishes delta patches to annotate assets and adjust safety thresholds. Patch application must work offline using WorkManager idle windows, with per-device signing keys stored in Android Keystore and rotated nightly. Reconcile state on reconnect, expose API for current asset map version and enforcement results, and support idempotent patching with rollback. Include data model, patch format, consistency guarantees, and a test plan for offline queues, drift, and multi-device conflicts?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Coinbase","Google","Tesla"]},{"id":"q-482","question":"How would you handle Activity lifecycle when screen rotates and you need to preserve user input data?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Amazon","Google","Oracle"]},{"id":"q-512","question":"How would you implement a simple RecyclerView in Android to display a list of user profiles with name and email?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["MongoDB","Tesla","Two Sigma"]},{"id":"q-541","question":"How would you implement a RecyclerView with ViewHolder pattern to display a list of user profiles efficiently?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Databricks","Goldman Sachs","Tesla"]},{"id":"q-976","question":"You are building an Android app that tracks a delivery ride; location updates every 5 seconds; battery life; Doze; Provide plan using FusedLocationProvider, ForegroundService, and WorkManager; include backoff; testing; intermittent connectivity?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Goldman Sachs","Oracle","Uber"]},{"id":"q-205","question":"How would you implement Compose Navigation with nested graphs, shared ViewModels, configuration change handling, and deep linking in a production Android app?","channel":"android","subChannel":"jetpack-compose","difficulty":"intermediate","tags":["composables","state","navigation"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-182","question":"What is the first lifecycle method called when an Android Activity is created, and what critical initialization tasks must be performed within it?","channel":"android","subChannel":"lifecycle","difficulty":"beginner","tags":["lifecycle","components"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-236","question":"How would you implement a comprehensive contract testing strategy using MSW (Mock Service Worker) with OpenAPI to ensure frontend API mocks stay synchronized with backend specifications, including CI/CD integration and drift detection?","channel":"api-testing","subChannel":"contract-testing","difficulty":"intermediate","tags":["wiremock","mockserver","msw"],"companies":["Amazon","Microsoft","Netflix","Salesforce","Square","Stripe"]},{"id":"q-1030","question":"Design a test strategy for an API gateway that enforces per-tenant sliding-window rate limits with dynamic quotas updated via admin API. Outline how you'd simulate high-concurrency traffic, verify quota propagation across nodes, validate headers and 429 responses, and test failure modes (Redis outage or misconfig). Include concrete test cases and tooling suggestions?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Citadel","Google","Meta"]},{"id":"q-1050","question":"Design an automated test plan for a REST + streaming API: /inventory/{sku}/status returns current stock via a streaming endpoint /inventory/stream (Server-Sent Events). The plan should cover stream resilience, event deduplication, per-warehouse aggregation under bursts, and failure modes when downstream storage becomes partially unavailable. Provide concrete test cases, tooling suggestions, and expected outcomes, with emphasis on realism for high-scale retail backends?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Amazon","Apple","DoorDash"]},{"id":"q-1061","question":"Design a practical test plan for a GraphQL API that aggregates data from products, pricing, and reviews. Include how you validate query depth limits, detect N+1 issues, test caching and cache invalidation under high concurrency, and ensure partial responses when some downstream services fail. Provide concrete test cases and tooling?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Instacart","NVIDIA","PayPal"]},{"id":"q-1166","question":"You manage a public REST API with versioning: /v1/... and /v2/... Design a practical, automated test plan to validate backward compatibility as v2 introduces a new field (tags) and changes a field type (price from int to decimal). Include concrete test cases, OpenAPI contract checks, cross-version schema validation, and how to verify deprecation behavior via a warnings header and 429s for old clients. Outline tooling and steps?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["Google","Lyft","NVIDIA"]},{"id":"q-1251","question":"You're testing an inventory REST API for a global retail platform. `/inventory/{sku}` returns current stock and scheduled restock ETA from a separate availability service that uses eventual consistency and a message bus (Kafka). Design a practical test plan to verify consistency windows, eventual accuracy under bursts, and cross-region cache coherency, including late-arriving events, out-of-order messages, and failure modes of Kafka and Redis caching. Include concrete test cases, tooling, and expected outcomes?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Discord","Square"]},{"id":"q-1284","question":"You maintain a simple REST API with POST /checkout that creates a payment intent for a cart. As a beginner, outline an end-to-end test plan to verify input validation, idempotent retries, and error handling under transient failures. Include concrete test cases, tooling suggestions, and expected responses?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["DoorDash","MongoDB"]},{"id":"q-1720","question":"Design a test strategy for a multi-tenant GraphQL API exposing persisted queries. Include N+1 detection, depth limits, per-tenant access controls, cross-tenant isolation, mutation cache invalidation, and canary schema rollout?","channel":"api-testing","subChannel":"general","difficulty":"advanced","tags":["api-testing"],"companies":["Google","Hashicorp","Microsoft"]},{"id":"q-1744","question":"Design a test for a bulk user import API: POST /imports/users accepts up to 100k records, uses an idempotency-key, and publishes per-record results to a downstream analytics queue. Specify test data, idempotency scenarios, partial downstream failures, retry/backoff, and observability. Include concrete cases, tooling, and expected outcomes?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Airbnb","Hashicorp","Meta"]},{"id":"q-1957","question":"Design a concrete, end-to-end automated test plan for an asynchronous data-processing API: POST /data/process returns 202 with an operation-id; messages flow via Kafka to downstream workers; results exposed at GET /data/process/{operationId}/result. Include idempotency tests (Idempotency-Key), eventual consistency with versioning, and fault-injection scenarios (Kafka outage, consumer restart, DB outage) with concrete test cases and tooling recommendations?","channel":"api-testing","subChannel":"general","difficulty":"advanced","tags":["api-testing"],"companies":["Google","Plaid","Two Sigma"]},{"id":"q-2017","question":"Design a beginner-friendly test plan for an API endpoint GET /products/{id} that uses a real-time feature flag to toggle response fields. Validate that additional fields (e.g., discount, promotionTag) appear when the flag is ON and disappear when OFF; cover admin-API downtime fallback; and test mid-request toggles with concurrent requests to ensure each response matches its observed flag. Include tooling suggestions?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["Adobe","Meta","Plaid"]},{"id":"q-2187","question":"You add a new endpoint POST /imports/candidates to ingest candidate records from a CSV file for a recruitment platform. The endpoint returns 202 with a jobId and status 'accepted'; processing is asynchronous via a worker queue. Design a beginner-friendly test plan covering input validation (file type/size), idempotency with Idempotency-Key, correct 202 response and jobId, status polling with GET /imports/{jobId}, and failure modes (malformed CSV, partial failure, queue downtime). Include concrete test cases and tooling?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["Amazon","LinkedIn"]},{"id":"q-2275","question":"Design an advanced API testing plan for a high-scale analytics API with asynchronous batch report generation. The endpoint GET /tenant/{tenantId}/reports/{reportId} returns a report produced by a background worker that reads from a write-ahead log and stores results in a read-optimized store. Outline concrete tests for data correctness across tenants and roles, eventual consistency across regions, idempotent retries, failure modes (worker crash, MQ outage), and performance under burst traffic. Include tooling and observability requirements?","channel":"api-testing","subChannel":"general","difficulty":"advanced","tags":["api-testing"],"companies":["Coinbase","Discord","IBM"]},{"id":"q-2343","question":"Design a beginner-friendly test plan for POST /webhook that validates an HMAC-SHA256 signature in X-Signature over the JSON payload with a shared secret. Include: happy-path with valid signature; missing/invalid header; invalid JSON; replay protection via timestamp/nonce; and basic rate-limiting checks. Recommend tooling and expected responses?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["OpenAI","Oracle","Uber"]},{"id":"q-2369","question":"Design an automated test plan for a webhook listener at /webhooks/ci ingesting push events from a CI service. Each payload is signed with HMAC-SHA256 and delivered at-least-once to a Redis-backed event store. Outline concrete test cases for signature validation, replay/duplicate handling, event ordering or out-of-order resilience, retries/backoff, and failure modes (secret rotation, Redis outage). Include tooling suggestions and expected outcomes?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["MongoDB","Slack"]},{"id":"q-2585","question":"Design a practical test plan for a WebSocket-based real-time update API at /ws/updates, where clients authenticate with JWT and receive per-tenant event streams. Include test cases for authentication failure, tenant isolation, message ordering and deduplication across reconnects, backpressure, and failure modes (broker outage, client disconnects). Recommend tooling and expected outcomes?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Cloudflare","Hashicorp","Uber"]},{"id":"q-2623","question":"Design a practical API-test plan for a multi-tenant image-processing service. The REST endpoint POST /v1/process/image returns a signed URL to the processed result. Create concrete test cases for: idempotent retries with an Idempotency-Key; streaming upload of large payloads; per-tenant routing and SLA verification across backends; failure modes (storage outage, worker pool exhaustion, network partition) with graceful fallbacks; and security checks for tenant scoping and signed URL expiry. Include tooling and metrics?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Anthropic","Discord","Hugging Face"]},{"id":"q-3037","question":"You're testing a public product catalog API: GET /products with pagination (page, limit), optional filters category and minPrice, and returns fields id, name, category, price, inStock. Draft a practical, beginner-friendly test plan: include concrete test cases for pagination boundaries, filter correctness, invalid inputs, and response headers (Cache-Control, ETag). Propose tooling (Postman/Newman or curl scripts) and sample asserts?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["IBM","MongoDB","NVIDIA"]},{"id":"q-3144","question":"Design a beginner-friendly test plan for a REST API POST /events that accepts analytics events, validates payload, and enqueues to a queue. Include idempotency via Idempotency-Key header, test 201, 400, 429, 503 responses, ensure same key yields single event, and cover queue-down fallback logging. Provide concrete test cases, tooling suggestions, and expected outcomes?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["Hugging Face","Square"]},{"id":"q-3290","question":"Design an end-to-end API testing strategy for a real-time ride ETA service with POST /rides, GET /rides/{id}, and /rides/stream (SSE). Include contract tests, OpenAPI schema validation, rotating auth tokens, rate-limiting checks, idempotent POSTs with idempotency keys, retry semantics, and resilience under downstream outages. Also cover canary rollout and feature-flag driven behavior?","channel":"api-testing","subChannel":"general","difficulty":"advanced","tags":["api-testing"],"companies":["Cloudflare","LinkedIn","Lyft"]},{"id":"q-3331","question":"Design an automated test plan for a multi-service aggregator API: GET /cart/summary?userId={id}&region={region}. It merges data from pricing-service, tax-service, and shipping-service into a single payload with total, line items, and estimated shipping. Assume some downstream latency spikes or failures. Outline concrete test cases, tolerances, and tooling to validate data fusion, currency rules, idempotent retries, and circuit-breaker behavior?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Instacart","Scale Ai","Square"]},{"id":"q-3517","question":"Design a practical automated test plan for a REST API powering a dataset ETL service: POST /datasets with an idempotency-key to start a long-running job; GET /datasets/{id}/status to poll for PENDING/RUNNING/DONE/FAILED; GET /datasets/{id}/result once DONE. Include tests for idempotent submission (same key -> single job), high-concurrency submissions, eventual consistency of status and result, and failure modes (downstream storage, message broker outages) with DLQ and backoff. Provide concrete test cases and tooling?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Google","Plaid","Snowflake"]},{"id":"q-3529","question":"In a system where POST /orders creates an order via an orchestration service that triggers inventory, payment, and shipping asynchronously, design a practical API testing plan to validate end-to-end consistency. Include idempotency, eventually consistent updates, distributed tracing checks, and failure-mode tests (partial outages, replayed events)?","channel":"api-testing","subChannel":"general","difficulty":"advanced","tags":["api-testing"],"companies":["Adobe","Apple","IBM"]},{"id":"q-3633","question":"Design a practical test plan for a payments API with POST /payments/process that uses an idempotency-key to guard against duplicates across ledger, order, and an event bus. Outline concrete test cases for high-concurrency identical keys, verify a single ledger write and a single order update, and validate eventual ledger consistency via polling. Include backoff/retry validation and simulate ledger outages or event-bus delays?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Bloomberg","Google","MongoDB"]},{"id":"q-3671","question":"Design an end-to-end automated test plan for a global payments API that routes through regional gateways with active-active replicas. Include latency SLO verification across regions, idempotent POST /payments retry handling, regional failover with DNS/routing changes, and currency conversion edge-cases; provide concrete test cases and tooling?","channel":"api-testing","subChannel":"general","difficulty":"advanced","tags":["api-testing"],"companies":["NVIDIA","Uber"]},{"id":"q-3743","question":"Scenario: A public API exposes **POST /webhooks/{provider}** to receive events from external providers. Webhooks may be retried with exponential backoff, and the service must deduplicate using an **Idempotency-Key** header. Design a beginner-friendly test plan that validates **signature verification**, idempotent processing, retry/backoff behavior, and resilience to datastore outages. Include concrete cases and tooling suggestions?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["Citadel","Lyft"]},{"id":"q-3838","question":"Design an end-to-end API test plan for a write-heavy REST endpoint POST /items that writes to a distributed log (Kafka) and materializes a read model in Redis and Postgres. Validate exactly-once processing, idempotency via Idempotency-Key, consumer lag, read-model convergence, and end-to-end traceability with OpenTelemetry. Include concrete test cases and tooling?","channel":"api-testing","subChannel":"general","difficulty":"advanced","tags":["api-testing"],"companies":["Airbnb","Scale Ai","Square"]},{"id":"q-3880","question":"Design an end-to-end automated test plan for a multi-tenant trading analytics API exposing REST GET /quotes/{symbol} and a streaming WebSocket /ws/quotes. Tenants have per-minute quotas stored in Redis, adjustable by an admin API. Deployed across 5 nodes with Kafka for price updates. Provide concrete test cases to verify quota enforcement under burst, streaming integrity during partial outages, and propagation of quota updates within seconds. Include tooling, data generation, success/failure criteria, and how you'd simulate high concurrency and verify 429s?","channel":"api-testing","subChannel":"general","difficulty":"advanced","tags":["api-testing"],"companies":["IBM","Plaid","Tesla"]},{"id":"q-4273","question":"Design an advanced test plan for a REST /checkout endpoint in a multi-region e-commerce platform where checkout triggers downstream services (inventory, payment, fulfillment) asynchronously via a message bus. Include idempotency with idempotencyKey, eventual consistency across regions, partial outages, and latency budgets. Provide concrete test cases, tooling, and validation for tombstones and reconciliation?","channel":"api-testing","subChannel":"general","difficulty":"advanced","tags":["api-testing"],"companies":["Goldman Sachs","Lyft"]},{"id":"q-4279","question":"Design a practical test plan for a webhook-driven API that delivers events to per-tenant listeners. Validate at-least-once delivery, idempotent event IDs, and HMAC signatures; simulate listener outages and network partitions; verify backoff/retry behavior and per-tenant rate limits; include concrete test cases and tooling recommendations?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Airbnb","LinkedIn","Snowflake"]},{"id":"q-4341","question":"Design an end-to-end test strategy for an asynchronous order placement API: POST /orders with Idempotency-Key, which queues the order to a downstream processor (Kafka) and updates status via GET /orders/{id}/status. Include tests for duplicate submissions, exactly-once processing, DLQ routing on consumer failure, backoff/retry behavior, and end-to-end visibility using tracing?","channel":"api-testing","subChannel":"general","difficulty":"advanced","tags":["api-testing"],"companies":["DoorDash","Oracle"]},{"id":"q-4366","question":"You have REST endpoint POST /payments/events that ingests payment events and guarantees exactly-once processing using idempotency keys and a transactional outbox that publishes to Kafka. Design an automated test plan with at least 6 concrete test cases (duplicate events, concurrent retries, idempotency key reuse, broker outages, replay safety), including tooling and expected outcomes?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Databricks","PayPal","Stripe"]},{"id":"q-4438","question":"Design a test plan for an asynchronous job API: POST /jobs starts a data transformation, returns 202 with a Location header; status can be checked via GET /jobs/{id} or via webhook callbacks. Include idempotency with Idempotency-Key, at-least-once delivery across a distributed queue, and end-to-end tracing (OpenTelemetry) across services. Provide concrete test cases, tooling, and expected outcomes. How would you structure this plan?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Google","Netflix","Plaid"]},{"id":"q-453","question":"You're testing a REST API that returns paginated results. The endpoint has a rate limit of 100 requests per minute and sometimes returns 500 errors under load. How would you design a comprehensive test strategy?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Airbnb","Google","Lyft"]},{"id":"q-4532","question":"Design and implement a test strategy for a high-volume Order Management API used by a global fleet (Tesla, DoorDash, Citadel-scale) exposing POST /orders, GET /orders/{id}, and a real-time WebSocket stream at /orders/stream. Ensure idempotent order creation, exactly-once delivery for status events, out-of-order handling, and cross-region data consistency with Kafka and PostgreSQL; include concrete test cases, tooling, and failure scenarios?","channel":"api-testing","subChannel":"general","difficulty":"advanced","tags":["api-testing"],"companies":["Citadel","DoorDash","Tesla"]},{"id":"q-4633","question":"Design a beginner-friendly test plan for a REST endpoint POST /webhooks/payment-completed that receives a JSON payload and a header X-Signature computed with HMAC-SHA256 using a shared secret. Validate signature, reject on invalid/missing signature, implement dedup with a unique event_id in payload, and ensure idempotent processing for duplicates within 10 minutes. Include concrete test cases and tooling suggestions?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["Hashicorp","Robinhood"]},{"id":"q-4664","question":"Design an automated end-to-end test plan for an API that exposes a REST endpoint GET /products/{id} and a persistent WebSocket stream at /ws/products/updates that pushes real-time delta updates (price, availability, promotions) per product across regions. The plan should test cross-service consistency, per-tenant isolation, reconnection/backoff, and failure modes (downstream cache or search index). Include concrete test cases, tooling suggestions, and expected outcomes?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Lyft","NVIDIA","Square"]},{"id":"q-4738","question":"You're adding a multi-tenant API that returns a paginated user list for a tenant via GET /tenants/{tenantId}/users. A separate POST /tenants/{tenantId}/users/refresh queues a refresh job in a downstream service to recompute each user's complianceScore. Design a concrete end-to-end test plan to validate correctness, idempotency of refresh, tenant isolation, cache invalidation latency, and failure handling when the downstream service is slow or unavailable. Include concrete test cases, tooling, and expected outcomes?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Bloomberg","Microsoft","Plaid"]},{"id":"q-4785","question":"Design a beginner-friendly test plan for GET /weather?city={city} that aggregates data from two third‑party providers. The service caches results for 60 seconds and imposes a 5-second timeout per provider. Include tests for provider outage, caching behavior, input validation, timeout handling, and per-IP rate limits returning 429. Propose concrete test cases and tooling?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["Meta","Snap","Square"]},{"id":"q-483","question":"You're testing a REST API that returns paginated results. How would you design a comprehensive test strategy to verify pagination works correctly across different page sizes and edge cases?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Discord","Netflix","Salesforce"]},{"id":"q-513","question":"How would you test a REST API endpoint that returns user data, including both success and error scenarios?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["Bloomberg","OpenAI"]},{"id":"q-542","question":"You're testing a payment API that processes transactions. How would you design test cases to verify idempotency, and what specific HTTP status codes would you expect for duplicate requests?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Cloudflare","NVIDIA","Stripe"]},{"id":"q-566","question":"How would you design a comprehensive API testing strategy for a machine learning model deployment pipeline that handles real-time inference requests?","channel":"api-testing","subChannel":"general","difficulty":"advanced","tags":["api-testing"],"companies":["Goldman Sachs","Hugging Face","Snap"]},{"id":"q-909","question":"Design a practical test plan for an asynchronous data ingestion API: POST / ing est accepts CSV payload and returns 202 with a job_id. It enqueues to a queue, then a worker writes to storage and updates a /status/{job_id} endpoint. Some tenants require PII redaction controlled by a tenant flag; a 'force' param bypasses CSV schema validation. Outline concrete test cases to verify correctness, privacy, idempotency, race conditions, and failure modes when queue or storage fail. Include sample CSV payloads and expected outcomes?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Robinhood","Snowflake","Two Sigma"]},{"id":"q-209","question":"How would you design a REST API testing framework that handles rate limiting, circuit breaking, and distributed tracing for microservices with 10,000+ concurrent requests?","channel":"api-testing","subChannel":"rest-testing","difficulty":"advanced","tags":["postman","rest-assured","supertest"],"companies":["Amazon","Goldman Sachs","Microsoft","Netflix","Stripe"]},{"id":"gh-12","question":"What are the three main service models of cloud computing and how do they differ?","channel":"aws","subChannel":"compute","difficulty":"beginner","tags":["cloud","aws","azure","gcp"],"companies":["Amazon","Google","Meta"]},{"id":"gh-13","question":"What is AWS (Amazon Web Services)?","channel":"aws","subChannel":"compute","difficulty":"beginner","tags":["cloud","aws","azure","gcp"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Netflix"]},{"id":"gh-15","question":"Compare AWS IaaS, PaaS, and SaaS service models with specific examples and use cases?","channel":"aws","subChannel":"compute","difficulty":"intermediate","tags":["cloud","aws","azure","gcp"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-34","question":"How would you design an Auto Scaling configuration for a high-traffic e-commerce application that handles 10,000 RPS with 99.99% availability, including scaling policies, health checks, and cost optimization?","channel":"aws","subChannel":"compute","difficulty":"advanced","tags":["scale","ha"],"companies":null},{"id":"gh-57","question":"What is Cloud Cost Optimization and what are the key strategies to reduce cloud spending in production environments?","channel":"aws","subChannel":"compute","difficulty":"beginner","tags":["finops","cost"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-58","question":"What are AWS Reserved Instances and how do they compare to On-Demand pricing?","channel":"aws","subChannel":"compute","difficulty":"intermediate","tags":["finops","cost"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Uber"]},{"id":"gh-83","question":"How do you evaluate cloud services for business needs using TCO analysis, SLA metrics, and migration strategies?","channel":"aws","subChannel":"compute","difficulty":"advanced","tags":["migration","cloud"],"companies":["Amazon","Google","IBM","Microsoft","Oracle","Salesforce"]},{"id":"gh-85","question":"How do cloud migration tools automate application and data transfer between on-premise and cloud environments, and what are the key technical challenges in ensuring data consistency and minimal downtime?","channel":"aws","subChannel":"compute","difficulty":"intermediate","tags":["migration","cloud"],"companies":["Amazon","Citadel","Goldman Sachs","Google","Microsoft"]},{"id":"gh-87","question":"How would you implement a multi-cloud cost allocation system using tagging strategies and automation APIs?","channel":"aws","subChannel":"compute","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Stripe","Uber"]},{"id":"q-174","question":"You have an EC2 instance that suddenly becomes unresponsive. What step-by-step troubleshooting methodology would you follow, which specific AWS tools and commands would you use at each stage, and how would you handle different instance states and recovery scenarios?","channel":"aws","subChannel":"compute","difficulty":"intermediate","tags":["ec2","compute"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-321","question":"You have a containerized web application that needs to handle variable traffic loads. When would you choose ECS Fargate over EKS and what are the key trade-offs?","channel":"aws","subChannel":"compute","difficulty":"beginner","tags":["ec2","ecs","eks","fargate"],"companies":["Cloudflare","Figma","MongoDB"]},{"id":"q-216","question":"How would you design an eventual consistency strategy for a multi-region DynamoDB application using Global Tables to handle write conflicts, ensure data convergence, and minimize latency?","channel":"aws","subChannel":"database","difficulty":"intermediate","tags":["mongodb","dynamodb","cassandra","redis"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-357","question":"You're designing a security monitoring system that needs to store 10M+ events per day with millisecond read latency. How would you choose between DynamoDB, Aurora, and ElastiCache, and what's your data partitioning strategy?","channel":"aws","subChannel":"database","difficulty":"intermediate","tags":["rds","aurora","dynamodb","elasticache"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix","Palo Alto Networks"]},{"id":"q-401","question":"You're designing a real-time analytics dashboard for Scale AI that needs to handle 10,000 events/second. Your team is debating between using DynamoDB with DAX vs. Aurora with ElastiCache. What are the key trade-offs you'd consider, and which would you choose for this use case?","channel":"aws","subChannel":"database","difficulty":"intermediate","tags":["rds","aurora","dynamodb","elasticache"],"companies":["Cohere","Oscar Health","Scale Ai"]},{"id":"q-413","question":"You're designing a real-time analytics dashboard for an IoT application that receives 10,000 events per second. The dashboard needs to show current metrics and historical trends. How would you design the database architecture using AWS services, and what caching strategy would you implement?","channel":"aws","subChannel":"database","difficulty":"intermediate","tags":["rds","aurora","dynamodb","elasticache"],"companies":["Amazon","Apple","Databricks","Google","Micron","Microsoft","Netflix","Snowflake"]},{"id":"q-1256","question":"You're operating a global real-time analytics pipeline on AWS: data streams from mobile apps ingest via Kinesis Data Streams, processed by Lambda, stored in DynamoDB and S3 Parquet. A new release causes timeouts and duplicate processing under peak load. Propose a concrete plan to fix cold starts and throttling, ensure exactly-once semantics, and safely deploy with minimal data loss. Include services, config values, and rollout steps?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Amazon","Robinhood","Two Sigma"]},{"id":"q-2571","question":"How would you implement a cross-region, multi-account data ingestion pipeline for real-time analytics on AWS, ensuring tenant isolation, least-privilege IAM roles, cross-account access, and automatic CMK rotation, using Kinesis Streams, S3, Lake Formation, and Glue?","channel":"aws","subChannel":"general","difficulty":"intermediate","tags":["aws"],"companies":["Adobe","Google","Meta"]},{"id":"q-2694","question":"In a real-time recommendation platform on AWS, eu-west-1 outage impacts feature storage and SageMaker endpoints. Propose a tested disaster-recovery plan that preserves data availability and latency: include cross-region S3 replication, multi-region SageMaker endpoints with traffic routing, a cross-account feature store, and an automated failover workflow using Step Functions. Outline validation steps for RTO and RPO?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Hugging Face","Snap"]},{"id":"q-2817","question":"You run a data analytics platform on AWS spanning two regions with a central S3 data lake and a production Redshift (or Lakehouse) cluster. Design a disaster-recovery strategy to meet RPO of 5 minutes and RTO of 15 minutes, covering cross-region data replication, IAM boundaries, Secrets Manager rotation, and automated failover. Explain components, data-integrity checks, and failure modes?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Cloudflare","Oracle"]},{"id":"q-2927","question":"Design a cross-region, multi-account data ingestion and analytics platform for a regulated partner. The data must remain resident in the partner's region, be isolated per tenant, support automatic CMK rotation, use AWS Lake Formation for cataloging, KMS for encryption, cross-account roles with least privilege, and enforce SCPs and VPC endpoints to prevent egress. Describe the architecture, data flow, and failure modes?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Amazon","DoorDash","Google"]},{"id":"q-3122","question":"You run a distributed job orchestration platform across 3 AWS regions. Job state is stored in DynamoDB, and workers pull tasks via region-specific SQS queues. To achieve true exactly-once processing across regions, outline an architecture using DynamoDB, SQS, EventBridge or Kinesis, and Step Functions, with idempotency keys and tombstones. Describe data flow, conflict resolution, replay handling, failure modes, and cost implications?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Oracle","Twitter","Uber"]},{"id":"q-3202","question":"You're running a real-time telemetry pipeline on AWS: producers send to Kinesis Data Streams, a Lambda consumer writes to DynamoDB. During event spikes, latency spikes and some records back up. Describe a concrete, end-to-end plan to diagnose and fix, including how you determine shard count, Lambda concurrency, data partitioning, and DynamoDB throughput; also what changes you would roll back if the plan fails?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Coinbase","Hashicorp","Netflix"]},{"id":"q-3353","question":"You deploy a static site to S3 behind CloudFront. After a release, users in a new region report 404/403 for assets; outline a concrete, beginner-friendly diagnostic and fix using AWS tools (S3, CloudFront, CloudWatch, IAM). Include steps to verify object existence, bucket policies, OAI if used, and how to invalidate paths and confirm resolution?","channel":"aws","subChannel":"general","difficulty":"beginner","tags":["aws"],"companies":["Databricks","LinkedIn"]},{"id":"q-3361","question":"You operate a multi-region, multi-account SaaS platform with strict data residency requirements: some tenants require data to stay in a specific region, others allow global storage. Design an AWS-based solution to enforce per-tenant data locality using region-bound S3 buckets, cross-account IAM boundaries, and automated onboarding/offboarding. Include data migration, lifecycle management, auditing, and failure handling?","channel":"aws","subChannel":"general","difficulty":"intermediate","tags":["aws"],"companies":["Airbnb","Netflix","Oracle"]},{"id":"q-3473","question":"Design a cross-region DR plan for a 2-region, multi-account deployment with Aurora PostgreSQL and a large analytics pipeline. Target RPO < 5 min and RTO < 10 min. Propose architecture using Aurora Global Database, S3 cross-region replication, cross-account IAM, Route 53 failover, and IaC automation. Include backups, testing, and rollback?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["DoorDash","Hashicorp"]},{"id":"q-3661","question":"Advanced AWS design: You run a multi-tenant analytics SaaS where per-tenant data residency is mandatory (data must stay in the tenant's region), but global dashboards must aggregate across tenants. Describe an end-to-end architecture using AWS services such as S3 IAM KMS Lambda Glue Athena Redshift API Gateway Cognito QuickSight that enforces per-tenant locality, supports automated on boarding off boarding, cross region analytics replication, and a disaster recovery strategy with defined RTO and RPO. Include security cost and scalability considerations?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Amazon","Discord","Tesla"]},{"id":"q-3763","question":"You operate a global data lake for a rideshare platform. Ingest tens of thousands of events per second from multiple tenants. Design an AWS-native pipeline that enforces per-tenant isolation, supports on-demand tenant data export to customer-owned accounts, and handles schema evolution and retention. Describe ingress, storage, cataloging, access control, and auditing?","channel":"aws","subChannel":"general","difficulty":"intermediate","tags":["aws"],"companies":["Databricks","Microsoft","Uber"]},{"id":"q-3789","question":"Design a tamper-evident, multi-account, multi-region AWS logging pipeline for a high-traffic service. The system must store raw logs in per-region S3 with object lock and versioning, replicate to a central analytics account for long-term retention, use cross-account IAM roles, and provide auditable provenance with CloudTrail data events. Outline components, data flow, failure modes, and cost considerations?","channel":"aws","subChannel":"general","difficulty":"intermediate","tags":["aws"],"companies":["Anthropic","Meta","Snowflake"]},{"id":"q-3937","question":"You operate a multi-account AWS SaaS where each tenant's data must stay within its designated region. Propose a practical onboarding/offboarding flow that enforces per-tenant region isolation using per-tenant S3 buckets in the correct region, cross-account IAM boundaries, and automated remediation for cross-region writes. Include: policy guardrails (SCPs), event-driven detection (EventBridge + Config), data migration paths, and auditing?","channel":"aws","subChannel":"general","difficulty":"intermediate","tags":["aws"],"companies":["Apple","Discord","LinkedIn"]},{"id":"q-4216","question":"Design a scalable, multi-tenant ingestion pipeline where each tenant's data is isolated, region-bound, and access-controlled across accounts. Use EventBridge, Lambda, S3, and Glue with Lake Formation. Include onboarding/offboarding, per-tenant CMKs, usage metering, and robust failure handling; also describe testing tenant isolation and failure scenarios?","channel":"aws","subChannel":"general","difficulty":"intermediate","tags":["aws"],"companies":["Amazon","Bloomberg","Netflix"]},{"id":"q-4295","question":"You operate a multi-tenant AI inference platform on AWS with per-tenant data locality, strict auditability, and dynamic model routing. Design an end-to-end architecture for real-time inferences that ensures per-tenant isolation, region-bound data, tenant-based quotas, and disaster recovery across regions. Describe data paths, deployment strategy, failure modes, and monitoring?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Anthropic","Discord"]},{"id":"q-4363","question":"Design a beginner-friendly AWS setup: static frontend on S3+CloudFront, API via API Gateway to Lambda, and data in DynamoDB. Explain how to enforce least-privilege IAM roles, isolate environments with separate buckets, and implement a simple backup/restore plan. Include a minimal IAM policy for Lambda to read DynamoDB and write to S3?","channel":"aws","subChannel":"general","difficulty":"beginner","tags":["aws"],"companies":["Apple","Cloudflare","Databricks"]},{"id":"q-4499","question":"Design a GPU-accelerated inference platform for multi-tenant workloads on AWS with data residency guarantees for each tenant. Compare using SageMaker endpoints (per-tenant or shared) vs an EKS cluster with Nvidia GPUs. Include onboarding/offboarding, model/versioning, tenant quotas, cost controls, auto-scaling, failure handling, monitoring, and auditability?","channel":"aws","subChannel":"general","difficulty":"intermediate","tags":["aws"],"companies":["Amazon","NVIDIA","Plaid"]},{"id":"q-454","question":"You need to host a static website with high availability and low latency globally. How would you configure AWS S3 and CloudFront to achieve this?","channel":"aws","subChannel":"general","difficulty":"beginner","tags":["aws"],"companies":["Cloudflare","Google","Netflix"]},{"id":"q-4565","question":"Design a per-tenant data isolation model for a multi-tenant data lake on AWS with strict data residency. Use region-bound S3 buckets, Lake Formation, and IAM roles to enforce access; implement per-tenant CMKs, Glue catalog scoping, and automated onboarding/offboarding. Include cross-account sharing, CloudTrail/Lake Formation audit, and DR with regional replication?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Oracle","Snowflake"]},{"id":"q-4642","question":"Design a latency-sensitive multi-tenant real-time analytics pipeline on AWS for trading data: ingest from venues, deduplicate and enrich events, enforce per-tenant data locality, and provide tamper-evident audit trails with on-demand queries and cost controls. Which services, data models, and deployment patterns would you use, and how would you handle exactly-once processing, retries, outages, and tenant onboarding?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Google","Robinhood"]},{"id":"q-4706","question":"Design a multi-tenant data lake on AWS where each tenant's data must be isolated, with dynamic onboarding/offboarding and strict data residency. Propose an architecture using AWS Lake Formation, per-tenant S3 Access Points, and per-tenant KMS CMKs; include cross-account IAM/SCPs, and auditing. Describe the data access grant/revoke workflow, automation (CDK/CloudFormation), and monitoring to enforce compliance?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["IBM","Salesforce"]},{"id":"q-4721","question":"Design a beginner-friendly AWS setup for a small app: static frontend on S3 + CloudFront and a REST API backed by Lambda that reads/writes to DynamoDB. Place the Lambda in private subnets behind API Gateway, use a DynamoDB VPC endpoint, and keep cost and security in mind. Describe the VPC structure, IAM least-privilege, and a simple test plan to verify connectivity and data operations?","channel":"aws","subChannel":"general","difficulty":"beginner","tags":["aws"],"companies":["Lyft","MongoDB"]},{"id":"q-4801","question":"In a multi-tenant AWS SaaS, each tenant can opt into maintenance windows and canaries. Design an automated deployment strategy that (1) performs per-tenant blue/green releases across regions, (2) routes traffic with per-tenant weighted API Gateway stages and Route 53 latency-based routing, (3) drains a tenant's traffic during maintenance without affecting others, and (4) audits changes with CloudTrail and Lake Formation. Include required IAM roles, data isolation, and rollback plan?","channel":"aws","subChannel":"general","difficulty":"intermediate","tags":["aws"],"companies":["Adobe","Airbnb","Citadel"]},{"id":"q-4819","question":"Design a compliant, scalable real-time analytics pipeline for a global fintech with per-tenant data residency. Each tenant's raw data stays in their region; aggregated insights may be centralized. Propose an AWS-based solution using Kinesis Data Streams, Lake Formation/S3, Glue, Athena, and IAM/KMS. Include data partitioning, tenancy isolation, onboarding/offboarding, access auditing, and cost controls?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Coinbase","Goldman Sachs","Robinhood"]},{"id":"q-4838","question":"You're operating a data-intensive analytics pipeline on AWS that ingests millions of events per minute from multiple regions into a central data lake. Design an end-to-end solution using Kinesis (or MSK), S3, Lambda/Fargate, and DynamoDB. Include data locality, fault tolerance, replay semantics, idempotency, and observability. What are your trade-offs?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Cloudflare","Netflix"]},{"id":"q-484","question":"You're designing a real-time ML inference pipeline on AWS that must process 10,000 requests/second with sub-100ms latency. How would you architect this using serverless components, and what trade-offs would you consider?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Databricks","Hugging Face","Snowflake"]},{"id":"q-514","question":"You're building a serverless application that needs to process user uploads. How would you design an architecture using S3, Lambda, and API Gateway to handle file uploads securely and efficiently?","channel":"aws","subChannel":"general","difficulty":"beginner","tags":["aws"],"companies":["OpenAI","Stripe"]},{"id":"q-543","question":"You're deploying a microservices application on AWS ECS. One service is experiencing intermittent 503 errors during peak traffic. How would you diagnose and resolve this issue?","channel":"aws","subChannel":"general","difficulty":"intermediate","tags":["aws"],"companies":["Apple","Bloomberg","Meta"]},{"id":"q-567","question":"How would you design a multi-region serverless architecture for a real-time chat application using AWS services, ensuring low latency and high availability?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Slack","Tesla"]},{"id":"q-220","question":"How would you design a multi-AZ VPC architecture with Route53 latency-based routing to CloudFront, ALB, and private EC2 instances while ensuring failover within 30 seconds?","channel":"aws","subChannel":"networking","difficulty":"intermediate","tags":["vpc","route53","cloudfront","alb"],"companies":["Amazon","Databricks","Goldman Sachs","Microsoft","Netflix"]},{"id":"q-384","question":"You're designing a multi-region SaaS application with users in North America and Europe. How would you configure Route53, CloudFront, ALB, and VPC to ensure low latency and high availability? What are the key trade-offs?","channel":"aws","subChannel":"networking","difficulty":"intermediate","tags":["vpc","route53","cloudfront","alb"],"companies":["Airtable","Cisco","Epic Games"]},{"id":"gh-66","question":"How does serverless computing abstract infrastructure management and what are its key execution characteristics?","channel":"aws","subChannel":"serverless","difficulty":"beginner","tags":["serverless","lambda"],"companies":["Airbnb","Amazon","Google","Microsoft","Uber"]},{"id":"q-246","question":"How would you design a serverless order processing workflow using AWS Step Functions with Lambda functions, implementing specific retry patterns, error handling, and state management?","channel":"aws","subChannel":"serverless","difficulty":"intermediate","tags":["lambda","api-gateway","step-functions"],"companies":["Amazon","Google","Microsoft","Netflix","Salesforce","Stripe"]},{"id":"q-292","question":"How would you design a data lifecycle strategy for a media company storing petabytes of video content requiring immediate access, archiving, and cost optimization across AWS storage services?","channel":"aws","subChannel":"storage","difficulty":"advanced","tags":["s3","ebs","efs","glacier"],"companies":["Meta","Netflix","Youtube"]},{"id":"q-307","question":"What are the key differences between S3, EBS, and EFS in terms of performance, scalability, and use cases?","channel":"aws","subChannel":"storage","difficulty":"intermediate","tags":["s3","ebs","efs","glacier"],"companies":["Amazon","Google","Meta"]},{"id":"q-370","question":"You're designing a file storage system for Canva's design assets. Users upload large PSD files (up to 10GB) that need versioning and quick access. How would you architect this using AWS storage services, considering cost, performance, and durability?","channel":"aws","subChannel":"storage","difficulty":"intermediate","tags":["s3","ebs","efs","glacier"],"companies":["Affirm","Booking.com","Canva"]},{"id":"q-1001","question":"You're building a beginner-friendly AWS-only pipeline to ingest customer chat transcripts (text files up to 50 KB) uploaded to S3. Design how to automatically redact PII using Amazon Comprehend PII detection, store the redacted transcript back to S3, and index metadata in DynamoDB. Include data flow, IAM permissions, error handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Snowflake","Zoom"]},{"id":"q-1058","question":"Design a cross region, multi account AI inference platform for real time pricing and risk scoring in a fintech setting. Ingest streaming data, enforce per tenant data residency, and meet sub 100 ms latency. Describe data flow, services, IAM boundaries, model registry, feature store, drift monitoring, error handling, and cost controls?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Coinbase","NVIDIA","Stripe"]},{"id":"q-1259","question":"Design an end-to-end AWS-native real-time fraud detection pipeline for a global e-commerce platform. Ingest event streams (Kinesis Data Streams), redact PII, create real-time features stored in SageMaker Feature Store (online) and offline store, with governance, lineage, access control, and cost constraints. Include data flow, IAM, retry logic, backpressure, testing, and incident response?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Airbnb","Cloudflare","Salesforce"]},{"id":"q-1292","question":"Design a real-time fraud-detection pipeline on AWS for a FinTech use-case. Ingest streaming transactions via Kinesis Data Streams, preprocess with Lambda, and invoke a SageMaker endpoint for real-time scores. Persist results to DynamoDB with audit logs in S3. Address latency (<200 ms), data privacy (KMS, VPC endpoints), IAM, drift monitoring, error handling, and cost control. Provide concrete components and trade-offs?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Citadel","Coinbase"]},{"id":"q-1313","question":"You're designing a beginner-friendly AWS-only pipeline for user-submitted PDFs (max 5 MB) uploaded to S3. Build an end-to-end workflow that uses Textract to extract text, runs a lightweight topic model via Comprehend or a tiny SageMaker endpoint to derive a topic label, stores a compact summary and a searchable index in DynamoDB, and exposes a read path via API Gateway + Lambda. Include data flow, IAM roles, error handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Google","Snap"]},{"id":"q-1366","question":"You're operating a real-time text classifier API on a SageMaker hosting endpoint behind API Gateway. You need a canary deployment strategy using CodeDeploy for SageMaker endpoints, with 2% of traffic to the new version, ramping to 50% over 2 hours, then full if no regressions. Describe data flow, required services, IAM, drift detection via Model Monitor, and rollback/failover logic, plus privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["NVIDIA","Netflix","Salesforce"]},{"id":"q-1437","question":"You're building a beginner-friendly, AWS‑only pipeline to process user-submitted emails stored as JSON in S3 (max 20 KB per file). Design an end-to-end workflow that detects language, translates to English, analyzes sentiment, and stores results in DynamoDB, with a simple read path via API Gateway + Lambda. Include data flow, services, error handling, IAM roles, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["LinkedIn","Robinhood","Tesla"]},{"id":"q-1460","question":"Design an advanced, low-latency fraud-detection pipeline for real-time payments in AWS. Ingest events via Kinesis Data Streams, derive features into SageMaker Feature Store, train/deploy with SageMaker Pipelines, score via a real-time SageMaker Endpoint called from Lambda, and log audits in CloudTrail. Enforce KMS encryption, data minimization/retention policies, IAM least privilege, and a rollback/failover plan. Include data flow, IAM, privacy, and failure handling?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Bloomberg","PayPal"]},{"id":"q-1482","question":"Design a production-ready, AWS-native content moderation pipeline for a multi-tenant SaaS platform handling images and captions uploaded to S3. Must route per-tenant policies to a custom SageMaker multi-model endpoint, apply per-tenant threshold overrides, enforce data isolation, log audit trails, and keep costs predictable with auto-scaling and caching. Describe data flow, IAM, encryption, error handling, and privacy implications?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Adobe","Cloudflare","MongoDB"]},{"id":"q-1552","question":"You're deploying a privacy-preserving AI rating model for financial support tickets on AWS. Incoming tickets arrive as JSON (up to 10 KB) in S3; design an end-to-end pipeline that uses SageMaker for inference, writes per-ticket audit logs to DynamoDB, encrypts data at rest with SSE-KMS, and supports reliable, rate-limited processing with error handling and drift monitoring. How would you implement this?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["PayPal","Scale Ai"]},{"id":"q-1583","question":"You're running a real-time fraud-detection model for payments. Data arrives in a Kinesis stream; design an end-to-end AWS-native pipeline that performs per-record inference, monitors drift and bias with SageMaker Clarify/Model Monitor, archives data in S3 with metadata, and auto-triggers a rollback to a previous model version via Step Functions if drift thresholds are exceeded. Include data flow, IAM, encryption, backpressure handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Amazon","Anthropic","Snowflake"]},{"id":"q-1627","question":"You're building an AWS-native ML platform that serves a global analytics product with data residing in Snowflake and Databricks across regions. Design a pipeline that ingests from both sources, uses SageMaker Feature Store, trains/registers with a human approval, serves real-time inference, and uses HashiCorp Vault for secrets. Include data lineage, IAM, encryption, drift monitoring, rollback policy, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Databricks","Hashicorp","Snowflake"]},{"id":"q-1750","question":"You're building a beginner-friendly AWS-only pipeline for multilingual customer recordings. Audio files (up to 90 seconds) are uploaded to S3. Design an event-driven flow that uses Amazon Transcribe to produce a transcript in the source language, Amazon Translate to produce a Spanish version, stores both transcripts in S3, and writes a compact index in DynamoDB with fields like customerId, fileKey, sourceLang, translatedKey. Include data flow, IAM, error handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Anthropic","PayPal"]},{"id":"q-1789","question":"Design an AWS-native, real-time fraud-detection pipeline: ingest streaming transactions from Kinesis Data Streams, score risk with a SageMaker Inference endpoint (Transformer-based), store scores in DynamoDB, and trigger a Step Functions workflow for high-risk events. Include data flow, IAM, privacy controls, drift monitoring, and cost governance?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["DoorDash","Goldman Sachs"]},{"id":"q-1800","question":"You're building a compliant multi-tenant, multilingual content moderation service on AWS. Users upload video to S3; extract audio, transcribe with Amazon Transcribe, run a SageMaker classifier to flag policy-violating content, redact PII, and store transcripts and decisions, indexing per-tenant metadata in DynamoDB. Design an end-to-end, event-driven pipeline with data isolation, data sovereignty, error handling, and privacy controls?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Google","Square"]},{"id":"q-1831","question":"You're building a beginner AWS-only pipeline to process user-uploaded audio stories (up to 120 seconds). When an audio file lands in S3, design an event-driven flow that uses Amazon Transcribe to produce a transcript, Amazon Comprehend to extract keywords, and a SageMaker endpoint to classify genre. Save transcript and metadata to S3 and index in DynamoDB. Include data flow, IAM permissions, error handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Adobe","Anthropic","Plaid"]},{"id":"q-1902","question":"You're building a beginner AWS-only pipeline to moderate user-uploaded profile pictures (JPEG/PNG, up to 2MB) in S3; on upload, design an event-driven flow using Rekognition to detect unsafe content and top labels, write a JSON report to S3, and upsert a summary in DynamoDB with fields like userId, objectKey, safeFlag, topLabel. Include data flow, IAM, error handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Databricks","MongoDB"]},{"id":"q-1935","question":"Design a real-time, multi-tenant fraud-detection pipeline for a SaaS platform. Ingestion uses Kinesis Data Streams per tenant; a central SageMaker detector scores each event; PII is redacted before storage in per-tenant S3 buckets; an index is kept in DynamoDB with fields tenantId, sessionId, eventTime, fraudScore. Explain data isolation, cross-account IAM, error handling, privacy controls, and idempotency/replay safety?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Adobe","PayPal"]},{"id":"q-2018","question":"Design an end-to-end, event-driven AWS pipeline for a regulated financial analytics service. Tenants upload encrypted JSON trade logs (up to 5 MB) to S3. Build per-tenant, region-isolated processing: decrypt with KMS, run a SageMaker multi-tenant analytics model, redact PII with Comprehend and regex, and store outputs in region-scoped S3 prefixes. Maintain an immutable audit trail in DynamoDB with versioning, implement retry and DLQ, and enforce strict IAM boundaries?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Adobe","Discord","Uber"]},{"id":"q-2045","question":"Design a real-time, AWS-only PII redaction pipeline for streaming chat messages. Ingest messages enter regional Kinesis Data Streams with tenant isolation. Use SageMaker endpoint (or Comprehend) to detect PII, redact spans, store redacted messages in S3, and index per-tenant audit logs in DynamoDB (tenantId, messageId, detectedPII, redacted, timestamp). Include data flow, IAM, error handling, privacy controls, and rate-limiting?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["DoorDash","Lyft","Netflix"]},{"id":"q-2075","question":"Design a real-time fraud detection pipeline for a high-volume platform used by Lyft, PayPal, and Snap. Ingest payment events into Kinesis, enrich with per-tenant risk data from DynamoDB via Lambda, compute a fraud score with a SageMaker endpoint, and enforce decisions in DynamoDB while routing high-risk events to a Step Functions human-in-the-loop workflow and notifying security via SNS. Include data isolation, sub-200 ms latency budget, and privacy controls with PII masking and audit trails?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Lyft","PayPal","Snap"]},{"id":"q-2175","question":"You're building a real-time, AWS-native, multi-tenant text inference service for content safety at scale. Incoming messages arrive with tenant context and must be isolated either via per-tenant endpoints or tenant-scoped routing. Describe architecture for throughput, deployment strategy (canary/rolling), privacy controls (encryption, PII redaction), drift monitoring, data retention, and cost governance. Include data flow, IAM, and observability?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Hugging Face","Meta","NVIDIA"]},{"id":"q-2208","question":"You're building a privacy-preserving, multi-tenant receipt processor on AWS. Clients upload receipts (PDF/JPG up to 5MB) to S3. Design an event-driven flow: Textract extracts fields, a SageMaker endpoint classifies line items, redact PII, and write structured data to DynamoDB with tenant isolation. Include data flow, IAM, SSE-KMS, retries, drift monitoring, audit logs, privacy controls, and testing strategy?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Airbnb","DoorDash","OpenAI"]},{"id":"q-2387","question":"You're processing PDFs uploaded to S3 (up to 5 MB). Design an event-driven AWS pipeline that uses Textract to extract text and tables, Comprehend to identify keywords, stores outputs in S3, and writes a compact index to DynamoDB with fields like documentId, s3Key, textSnippet, phrases. Include IAM, error handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Instacart","MongoDB","Oracle"]},{"id":"q-2445","question":"You're designing a real-time, multi-tenant chat moderation pipeline on AWS. Ingest messages from multiple tenants via Kinesis Data Streams, score content with a versioned SageMaker endpoint, redact PII, and persist per-tenant results to DynamoDB with TTL, while archiving raw and redacted messages to S3. Design end-to-end architecture with latency target ~150 ms, strong data isolation, model versioning, encryption, and cost controls; include IAM roles, VPC endpoints, and monitoring?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Amazon","Twitter"]},{"id":"q-2733","question":"You're building a beginner AWS-only pipeline for customer support call recordings uploaded to S3 as WAV/MP3 (≤2 minutes). Design an event-driven flow using Amazon Transcribe (auto language), translate non-English transcripts to English, generate an SRT subtitle, redact PII in transcripts, store both outputs in S3, and upsert a DynamoDB index with callId, transcriptKey, subtitleKey, language, and sentiment. Include IAM, error handling, privacy, and cost considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["NVIDIA","Square"]},{"id":"q-2754","question":"You're building a beginner AWS-only event-driven pipeline to process user-uploaded handwritten receipts (JPEG/PNG, ≤4MB) stored in S3. Design the flow using Amazon Textract to extract text and key-value data, store outputs in S3, and upsert a DynamoDB index with receiptId, userId, totalAmount, currency, and receiptDate. Include IAM roles, error handling, privacy considerations, and cost controls?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Apple","Google","Meta"]},{"id":"q-2770","question":"You're building an AWS-based data-lake intake for user-submitted datasets (CSV/Parquet, up to 500MB) uploaded to S3. Design an event-driven flow: validate schema and basic quality, trigger a SageMaker Processing job to compute metadata (columns, data types, missing values, sample stats), store outputs in S3, and upsert a DynamoDB index with datasetId, s3Key, schemaHash, qualityScore, ownerId. Include IAM, error handling, privacy controls, and cost considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Coinbase","Google","IBM"]},{"id":"q-2846","question":"You're deploying a real-time text moderation model on AWS that flags policy-violating comments. Design an end-to-end AWS-native pipeline that ingests messages, runs inference on a SageMaker endpoint, uses SageMaker Clarify for per-request attributions, stores input, predictions, and explanations in S3, upserts a DynamoDB audit index (requestId, userId, modelVersion, latency, score, topFeatures), redacts PII before storage, enforces least-privilege IAM, and triggers retraining via SageMaker Pipelines on drift or performance degradation while balancing cost and latency?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Amazon","Twitter"]},{"id":"q-3068","question":"You're processing multilingual chatbot logs uploaded as JSON to S3 (≤1 MB). Design an end-to-end AWS AI/ML pipeline that triggers on upload to: 1) detect language with Comprehend, 2) run a SageMaker endpoint for sentiment and abusive content, 3) translate non-English content to English, 4) redact PII, 5) write a combined NDJSON to a data lake and upsert a DynamoDB index with conversationId, translatedKey, language, sentiment, abuseFlag. Include error handling, idempotent retries, data privacy, and cost considerations; outline monitoring with CloudWatch/X-Ray and EventBridge?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Google","Instacart","Meta"]},{"id":"q-3127","question":"You're building a beginner AWS-only pipeline for multilingual customer support chat transcripts uploaded as JSON to S3 (≤1MB per file). Design an event-driven flow that triggers on upload, translates non-English content to English using Amazon Translate, analyzes sentiment and named entities with Amazon Comprehend, stores both original and translated JSON in S3, and upserts an index in DynamoDB with fields like chatId, originalLang, translatedKey, sentiment, entities. Include IAM, error handling, privacy, and cost considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["IBM","Netflix"]},{"id":"q-3138","question":"You're collecting IoT sensor CSV files uploaded to S3 (deviceId, timestamp, temp, humidity). Design an event-driven AWS pipeline that validates CSV schema, computes per-device daily aggregates (minTemp, maxTemp, avgTemp), stores raw and processed outputs in S3, and upserts a DynamoDB index with deviceId, date, minTemp, maxTemp, avgTemp, and status. Include IAM permissions, error handling, and data retention?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Apple","Hashicorp","Tesla"]},{"id":"q-3191","question":"You're building a real-time, event-driven AWS pipeline to ingest live webinar audio streams that arrive as 30-second chunks uploaded to S3. Design an end-to-end flow that uses Amazon Transcribe for streaming transcription with speaker diarization, Amazon Comprehend to compute per-speaker sentiment and key entities, and stores transcripts, per-speaker metrics, and a compact index in DynamoDB. Include privacy controls (PII redaction), data retention, IAM roles, error handling, idempotency, and cost considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Apple","IBM"]},{"id":"q-3225","question":"You're building a beginner AWS-only pipeline to ingest sensor CSV uploads into S3 and produce deduplicated aggregates. Each file <= 500KB with headers: timestamp, deviceId, value. On upload, compute SHA-256, check a DynamoDB dedup table; if new, validate schema, compute per-device hourly averages, write aggregates as JSON to S3 under aggregates/{date}/{fileKey}. Upsert a metadata item in DynamoDB with fileKey, hash, deviceCount, earliestTs, latestTs, aggregateKey. Include IAM, error handling, privacy, and cost considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Google","Oracle","Scale Ai"]},{"id":"q-3245","question":"You're building a privacy-preserving, real-time data pipeline for streaming customer events from IoT devices into AWS. Ingest via AWS IoT Core, buffer with Kinesis Streams, run online inference in SageMaker endpoints to flag anomalies, redact PII from event payloads, store raw data in S3 and anomalies index in DynamoDB with fields tenantId, eventId, timestamp, anomalyScore, remediationAction, and enforce per-tenant isolation with IAM roles and KMS encryption. What is your design, including error handling, backpressure, and audit trails?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Goldman Sachs","Google","IBM"]},{"id":"q-3373","question":"You're building a beginner AWS-only pipeline to process user event logs uploaded as newline-delimited JSON (JSONL) to S3 (≤1 MB per file). On upload, design an event-driven flow using Lambda to validate schema, extract userId, classify events with a lightweight AI step, redact PII in the payload, write sanitized events back to S3, and upsert a DynamoDB index with logId, s3Key, eventCategory, timestamp, and userHash. Include IAM, error handling, privacy, and cost considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Coinbase","Snap"]},{"id":"q-3430","question":"You're building an AWS-native pipeline to process enterprise chat transcripts uploaded as JSONL files to S3 (≤5 MB per file). On upload, design an event-driven flow that uses Amazon Comprehend for entity and sentiment analysis, a SageMaker endpoint for topic modeling to classify conversations, redact PII, write a sanitized transcript to S3, and upsert a DynamoDB index with chatId, transcriptKey, topics, sentiment, and redactedFlag. Include IAM, KMS encryption, privacy controls, and cost considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Instacart","Meta"]},{"id":"q-3460","question":"You're building a beginner AWS-only data ingestion pipeline for ML datasets. CSV files (up to 50 MB) are uploaded to S3. On upload, design an event-driven flow using Lambda to validate headers, detect duplicates by datasetId + timestamp, partition data into a date-based path, trigger a Glue crawler to catalog, and write a manifest JSON to S3 with fields datasetId, fileKey, recordCount, partitionPath, and upsert a DynamoDB index with datasetId, s3Root, lastUpdated. Include IAM, error handling, privacy considerations, and cost?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Apple","Instacart","Uber"]},{"id":"q-3538","question":"You're building a multi-tenant AWS AI pipeline for incoming chat logs (JSON, up to 10 KB each) uploaded to S3. Design an event-driven flow (S3 -> Step Functions) that detects language, translates non-English to English, performs sentiment and entity analysis, redacts PII, stores both original and sanitized transcripts in S3, and upserts a DynamoDB index with chatId, originalKey, sanitizedKey, language, sentiment. Include IAM, privacy controls, and data isolation?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["DoorDash","OpenAI","Slack"]},{"id":"q-3733","question":"You’re building an AWS-only, event-driven pipeline for PDFs uploaded by external partners to S3 (up to 20 MB). Design a flow using Textract to extract text and tables, a SageMaker classifier to flag risk clauses, and Comprehend for entity detection. Store outputs in partner-scoped S3, and write a per-partner index to DynamoDB (partnerId, docKey, riskFlag, clauses). Enforce data sovereignty with region replication and customer-managed keys, plus IAM least privilege, error handling, and cost controls?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Bloomberg","Google"]},{"id":"q-3800","question":"You're ingesting customer reviews stored as JSONL files in S3 (up to 20 MB per file). Design an advanced, privacy-conscious event-driven AWS pipeline that uses SageMaker endpoints for sentiment and topic modeling, applies PII redaction, stores outputs in S3, and upserts a compact DynamoDB index. Include data governance with Lake Formation, KMS, and a drift-aware retraining trigger?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Hashicorp","OpenAI"]},{"id":"q-3833","question":"You’re building an AWS-native, event-driven pipeline for ingesting training videos uploaded to S3 (≤1 GB). On upload, design a workflow that uses MediaConvert to transcode to multiple bitrates, Transcribe for captions, Rekognition for scene labels and unsafe content, a SageMaker endpoint to summarize transcripts and extract topics, redact PII, write outputs to S3, and upsert a DynamoDB index with videoId, originalKey, captionKey, summaryKey, topics, and safetyFlag. Include IAM, KMS, privacy controls, and cost considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Citadel","IBM","Netflix"]},{"id":"q-3890","question":"You're building a beginner AWS-only pipeline for user-uploaded product photos (JPEG/PNG, ≤3 MB) stored in S3. On upload, design an event-driven flow that uses Amazon Rekognition to generate the top 3 object labels and a simple color palette, writes a JSON descriptor to S3, and upserts a DynamoDB item with productId, imageKey, labels, colors, and privacyFlag. Include IAM roles, error handling with DLQ, and cost considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Goldman Sachs","NVIDIA","Oracle"]},{"id":"q-3924","question":"Design an AWS‑only, event‑driven pipeline to process real‑time multilingual customer chat transcripts arriving via Kinesis Data Streams. Requirements: detect and redact PII using Comprehend, enforce per‑tenant data isolation, store redacted transcripts in S3 under tenant prefixes, and maintain a DynamoDB index with tenantId, transcriptKey, language, redactedFlag, and processingTime. Include IAM roles, error handling, privacy controls, and cost considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["IBM","Meta"]},{"id":"q-4043","question":"You're building an event-driven AWS pipeline for real-time content moderation of global chat messages on a video platform. Messages arrive via Kinesis Data Streams, an initial Lambda redacts PII, then a SageMaker real-time endpoint classifies for hate speech and scams. Store sanitized logs in S3 and upsert a DynamoDB index with messageId, userHash, timestamp, riskScore, category. Include model drift monitoring, IAM/KMS, privacy, and cost considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Coinbase","Netflix"]},{"id":"q-4158","question":"Design a beginner AWS-only pipeline to classify images (≤5 MB) uploaded to S3. On upload, use an event-driven flow with Lambda to call Amazon Rekognition to produce labels with confidence, store labels as metadata.json next to the image in S3, and upsert a DynamoDB index with imageId, s3Key, primaryLabel, and timestamp. Include IAM roles, error handling via a DLQ, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Bloomberg","Discord"]},{"id":"q-4251","question":"You're building a beginner AWS-only pipeline that processes image uploads to S3 (JPEG/PNG, up to 5 MB) for an e-commerce site. On upload, design an event-driven flow: Lambda triggers Rekognition to extract labels and detect explicit content, uses a lightweight caption heuristic to generate a caption, writes the processed metadata as JSON to S3, and upserts a DynamoDB index with imageId, imageKey, captionKey, labels, explicitFlag, and timestamp. Include IAM, error handling, privacy controls, and cost considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Databricks","Goldman Sachs","Tesla"]},{"id":"q-4312","question":"You're building an AWS-only pipeline for multilingual real-time chat transcripts generated by a customer-support web app. Transcripts arrive as JSON in S3. Design an event-driven flow that uses Amazon Comprehend for multilingual sentiment and key phrase extraction, and SageMaker for language-specific topic modeling. Store enriched artifacts in S3 and upsert a DynamoDB index with chatId, lang, topTopic, sentiment, and timestamp. Include IAM roles, error handling, privacy safeguards, and cost considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Google","Tesla","Uber"]},{"id":"q-4378","question":"You're building a real-time fraud risk scoring pipeline for card transactions in AWS. Ingest events from Kinesis Data Streams, compute streaming features in Lambda, score with a SageMaker endpoint, persist risk metadata in DynamoDB, and trigger remediation via EventBridge. Include model versioning, feature store integration, IAM/KMS controls, data privacy, and cost scaling to 10k TPS?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Cloudflare","Microsoft","Slack"]},{"id":"q-4493","question":"You're building an AWS-only, event-driven video processing pipeline for user-uploaded webinars (MP4) up to 4 GB. On upload to S3, design a flow that uses Step Functions to orchestrate: (1) Amazon Transcribe with speaker diarization for a multi-speaker transcript in the original language, (2) Translate to English for captions, (3) generate SRT captions with accurate timestamps, (4) use Comprehend to extract topics per speaker, (5) store transcripts, captions, and topic summaries in S3, and upsert a DynamoDB index with videoId, originalTranscriptKey, captionsKey, translatedKey, topics, and duration. Include IAM, error handling, privacy, and cost considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Google","Meta","Netflix"]},{"id":"q-951","question":"You're building a low‑ops internal voice assistant for support scripts. Audio files up to 60 seconds are uploaded to S3. Design an AWS‑only pipeline that transcribes, analyzes sentiment, and stores a brief summary plus an index in DynamoDB, with minimal cost and ops. Include data flow, services, error handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["MongoDB","Snap"]},{"id":"q-1336","question":"In a multi-account AWS data lake used by a global product analytics team, ingest semi-structured data from several SaaS APIs into S3, enforce schema evolution and data quality checks, and expose curated tables with time-travel queries. Describe the end-to-end design including data formats, upsert strategy, and governance controls you would apply using AWS services?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Amazon","Apple"]},{"id":"q-1419","question":"Design an advanced, multi-region data ingestion and governance pipeline for a global fintech platform. Data from partners arrives as JSON and Parquet; must enforce per-tenant isolation, support near real-time analytics, and GDPR erasure. Outline architecture using AWS Kinesis (streams), DMS for CDC, Glue (ETL), Iceberg on S3 for schema-evolving tables, Lake Formation/IAM for access, and Athena/Redshift Spectrum for queries. Include format choices, CDC vs batch mix, lineage, and failure modes?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Meta","Stripe"]},{"id":"q-1487","question":"You operate an AWS data lake where streaming user activity flows from Kinesis Data Streams into S3 via Firehose, with Glue Data Catalog, Athena queries, and dashboards. Schemas evolve and data quality is mission-critical. Design a real-time data quality and drift detection framework: schema drift, per-record quality checks, quarantine failing files, alert owners, and preserve cross-account auditability?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Adobe","Coinbase","Oracle"]},{"id":"q-1506","question":"Scenario: A partner API provides daily batches of image assets with evolving metadata. Build an end-to-end ingest to a multi-region data lake: landing in S3, metadata in a versioned, upsertable table, idempotent processing, schema evolution, and governance across accounts. Describe data formats, partitioning, dedupe strategy, and cross-account orchestration?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Instacart","Slack","Stripe"]},{"id":"q-1521","question":"You manage a global, multi-tenant data platform where streaming events bucket into S3 and downstream engines like Redshift and Athena rely on per-tenant schemas. Design a resilient end-to-end architecture that versions datasets, enforces per-tenant schema contracts, detects drift, quarantines bad records, and supports rollback without downtime. Specify services, data formats, governance, and observability?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Adobe","Google","NVIDIA"]},{"id":"q-1543","question":"Design an event-driven data platform to ingest order and driver events from regional services into a centralized data lake for near-real-time analytics. Data arrives as evolving JSON schemas; must handle schema evolution, efficient partitioning, cross-account access, and cost. Which AWS services and data design patterns would you implement, and how would you validate data quality and enable fast ad-hoc queries?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["DoorDash","Square"]},{"id":"q-1608","question":"Scenario: A new source streams JSON events from a mobile app via Kinesis Data Streams. Design a beginner-level ingestion pipeline to land data in S3 as Parquet with daily partitions, using AWS Glue for ETL and cataloging. Include services, data formats, schema evolution approach, basic data quality checks, and how you would test end-to-end before analytics?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["NVIDIA","Netflix","Zoom"]},{"id":"q-1631","question":"You receive a daily nested JSON file and a separate CSV in an S3 bucket. Design a beginner-level ingestion using AWS Glue Studio to flatten the JSON, normalize types, parse the CSV, and write Parquet under a date-partitioned path s3://data-lake/events/date=YYYY-MM-DD/. Register a Glue catalog table, outline simple schema-drift handling, and include basic data-quality checks plus a quick end-to-end test plan with sample files?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["Instacart","Oracle","Twitter"]},{"id":"q-1654","question":"New data source writes daily CSVs to S3. Design a beginner pipeline to validate a defined schema, fail-fast on missing required fields, and route bad records to a quarantine bucket with a reason. Store valid data as Parquet in S3 with daily partitions, and catalog via Glue so Athena can query. Include a basic end-to-end test plan?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["Plaid","Two Sigma","Zoom"]},{"id":"q-1785","question":"Two-account AWS data lake (us-east-1 and eu-west-1) ingests partner JSON events via API into S3. Design an end-to-end pipeline using Glue for ETL and cataloging, store Parquet with daily partitions by country/date, enable Athena queries with Lake Formation access controls, and handle schema evolution, data quality, late-arrivals, and cross-region replication trade-offs?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Lyft","Snap"]},{"id":"q-1816","question":"You collect protobuf-encoded telemetry from thousands of IoT devices via AWS IoT Core; design an end-to-end ingestion that lands as daily-partitioned Parquet in S3, uses AWS Glue Schema Registry for evolving schemas, and implements idempotent deduplication plus a 3-sigma anomaly check. Include data formats, partition keys, testing strategy?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Amazon","Meta","Square"]},{"id":"q-1883","question":"Design a streaming data contract and quality workflow for events arriving from multiple teams via MSK and Kinesis. Use AWS Glue Schema Registry to enforce evolving Avro/JSON schemas with versioning and compatibility (backward/forward). Describe publishing schemas, validating payloads at ingest, storing Parquet in S3 with daily partitions, and monitoring for drift and quality across regions?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Cloudflare","Google"]},{"id":"q-1968","question":"Design an end-to-end streaming-to-lake pipeline: ingest real-time events from AWS MSK, consolidate into Apache Iceberg tables on S3 with daily partitions, support CDC-style upserts/deletes, and implement schema evolution. Include governance with Lake Formation/IAM, testing strategy (canaries, synthetic data), and data quality/lineage monitoring?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Anthropic","Google"]},{"id":"q-1981","question":"Design a data lake ingestion pipeline on S3 for semi-structured SaaS data that must support upserts and deletes with time-travel queries; choose between Apache Iceberg, Hudi, or Delta Lake on AWS (Glue, EMR, Athena) and justify: schema evolution, compaction, CDC handling, partitioning, and data quality checks; include testing plan?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Hashicorp","Instacart","Two Sigma"]},{"id":"q-2019","question":"Scenario: IoT sensors on factory floors emit JSON telemetry with evolving schemas (new metrics appear over time). Design a streaming ingestion pipeline that reads from Kinesis Data Streams, writes to S3 as Parquet with daily partitions, and uses an Iceberg catalog (Glue-backed) to handle schema evolution. Compare upsert strategies for late data (Iceberg MERGE INTO vs Hudi), outline data quality checks, and a testing plan including sandbox data and end-to-end validation. Include governance via Lake Formation?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Citadel","IBM","Tesla"]},{"id":"q-2060","question":"Design a multi-region, streaming-plus-batch ingestion pipeline for a mobile app that emits JSON events at high volume. Ingest via Kinesis Data Streams to S3 Parquet using Apache Iceberg tables with a Glue catalog, supporting upserts, deletes, and schema evolution. Include multi-source CDC, data quality checks, governance via Lake Formation, and end-to-end testing?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["LinkedIn","Meta","PayPal"]},{"id":"q-2095","question":"You receive a financial dataset hourly from a partner API in JSON with nested fields. Design a beginner-level ingestion pipeline to land data in S3 as Parquet with hourly partitions, using AWS Glue for ETL and cataloging. Include how you flatten nested JSON, define a stable schema, enable basic data quality checks, ensure encryption with KMS, and implement least-privilege IAM roles and Lake Formation permissions. Outline how you'd test end-to-end before analytics?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["Robinhood","Snap"]},{"id":"q-2144","question":"Design a real-time fraud signals pipeline: ingest 50 banks via Kinesis Data Streams, use Glue Schema Registry for evolving Avro schemas, run Spark Streaming to land Parquet in S3 partitioned by date and merchantId, with a raw zone and a masked curated zone. Apply IP masking and immutability retention; catalog with Glue Data Catalog; enforce access via Lake Formation; emit lineage/audit logs. Include failover tests and latency considerations?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Plaid","Square"]},{"id":"q-2204","question":"Design a real-world, multi-tenant, multi-account ingestion pipeline: Ingest JSON events from microservices via Kinesis Data Streams, store as Parquet in S3 with daily partitioning by tenant_id, and catalog with AWS Glue. Enforce tenant isolation with Lake Formation across accounts, handle schema evolution, embed data quality checks, and outline end-to-end testing strategies?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Airbnb","Bloomberg","Google"]},{"id":"q-2244","question":"You receive daily JSON event files from a mobile app stored in S3 with nested arrays. Design a beginner-friendly ingestion pipeline to flatten into Parquet with daily partitions, catalog in AWS Glue, and implement a basic data quality gate (required fields, non-null user_id, ISO8601 event_time). Include testing approach and handling of schema drift?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["Amazon","LinkedIn","Lyft"]},{"id":"q-2264","question":"In a cross-account AWS data lake spanning us-east-1 and eu-west-1, ingest streaming events from Kinesis into an Apache Hudi dataset on S3 to support upserts and time-travel. Design the end-to-end architecture including how to implement upserts, schema evolution, compaction, cross-account Lake Formation policies, and an end-to-end test and rollback plan, with concrete services and data formats?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Databricks","Google"]},{"id":"q-2312","question":"Scenario: you manage a multi-tenant SaaS data lake across three AWS accounts. Ingest telemetry events via Kinesis Data Streams, land as Parquet in S3 with daily partitions, and register metadata in the Glue Data Catalog. Enforce schema evolution and tenant-level masking using Lake Formation; enable cross-account analytics with Athena. Describe architecture, governance, and testing strategy?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Lyft","Microsoft","MongoDB"]},{"id":"q-2350","question":"Design an end-to-end ingestion and upsert pipeline for customer records with SCD Type 2 using Apache Iceberg on S3. Ingest JSON events from Kinesis and batch CDC from RDS; ensure schema evolution, partitioning, and time travel; discuss how to reconcile late-arriving changes and record-level lineage. Include services, table format, upsert strategy, and testing?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Adobe","Apple","Meta"]},{"id":"q-2440","question":"Scenario: Daily JSON app-logs arrive in S3 at s3://bucket/raw/app-logs with fields userId, timestamp, action, and optional device {model,os}. For a beginner, design an ingestion pipeline that flattens to Parquet with daily partitions into s3://bucket/curated/app-logs/yyyy/mm/dd/, catalogs in Glue, and uses a defined schema (userId string, timestamp timestamp, action string, device_model string, device_os string) with schema evolution enabled. Include data quality gates (required userId, non-null timestamp, restricted action set), drift handling, and end-to-end testing plan?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["Coinbase","Plaid","Tesla"]},{"id":"q-2494","question":"Ingest a hybrid workload into an Iceberg table on S3: streaming JSON events and nightly relational extracts, partitioned by date. Outline a concrete pipeline using AWS Glue, S3, and query engines (Athena/Databricks), with data formats, schema evolution, upserts, late-arriving data, and end-to-end validation. How would you monitor partitions and performance and enforce access controls?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Anthropic","Databricks","Plaid"]},{"id":"q-2594","question":"Design an end-to-end AWS data lake ingestion and governance solution that provides complete data lineage across multiple sources (RDS, SaaS API, IoT), transformations in AWS Glue, and consumption in Athena. Include how you model lineage metadata (source, transform, target), handle schema evolution, and how you validate lineage during failing ETL runs and across re-processing, with Lake Formation fencing?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["PayPal","Snap"]},{"id":"q-2653","question":"Scenario: A global product analytics data lake ingests streaming clickstream events from mobile apps with evolving JSON schemas containing PII. You must mask PII in the shared dataset before analytics, support cross-account access, and keep raw data auditable for compliance. Design end-to-end ingestion using AWS (Kinesis or MSK, Glue, S3, Lake Formation/Macie, KMS). Address schema evolution, late data, data quality, testing, and rollout?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Meta","MongoDB","Scale Ai"]},{"id":"q-2686","question":"Design an ingestion pipeline for a new SaaS data stream of JSON events that lands in S3 with daily partitions. Use AWS Glue (Spark) and Apache Iceberg on S3 to materialize updatable tables with idempotent merges, handle schema evolution, and cope with late-arriving data. Describe partitioning, upsert strategy, schema evolution, data quality checks, testing, and monitoring?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Instacart","Snap","Tesla"]},{"id":"q-2875","question":"In a regulated financial environment, design an end-to-end AWS data pipeline that ingests daily batch CSV exports and real-time credit-event streams into a centralized data lake. Explain how you would land data in S3 with partitions, implement idempotent upserts and schema evolution, enforce governance via Lake Formation and the Glue Data Catalog, implement data quality checks and tests, and observe and rollback on failures?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Hashicorp","MongoDB","Oracle"]},{"id":"q-2925","question":"You have three data sources: a REST API streaming JSON via Kinesis Data Streams, a CSV feed landed to S3, and an on-prem CDC stream to AWS. Design an end-to-end pipeline that lands data in S3 as Parquet with per-source/date partitions, uses Apache Hudi for upserts, supports schema evolution via the Glue Catalog, handles late data, includes data quality checks, and a practical end-to-end test plan?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["DoorDash","Meta","Slack"]},{"id":"q-2985","question":"Scenario: You must ingest daily JSONL logs from an on-prem SFTP into S3. Design a beginner-friendly, serverless ingestion using AWS Transfer Family to pull files, convert to Parquet with daily partitions, catalog via Glue, and orchestrate via Step Functions. What steps would you implement, including naming, partitioning, quality checks, and testing?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["Goldman Sachs","Microsoft","Salesforce"]},{"id":"q-3199","question":"In a 3-region data lake ingesting mobile-app events, design an end-to-end pipeline to ingest JSON events from Kinesis Data Streams, normalize to Parquet, partition by tenant_id/date, and mask PII. Ensure schema evolution, data lineage, and cross-account sharing with Snowflake, IBM, and Airbnb. Include services, upsert strategy (CDC-based) for updates, testing, and monitoring?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Airbnb","IBM","Snowflake"]},{"id":"q-3302","question":"In a cross-account AWS data lake, ingest real-time financial trades (JSON) from Kinesis Data Streams, land as Parquet in S3 with daily partitioning by trade_date and tenant_id for tenant isolation, and catalog with Glue. Enforce column-level PII masking and cross-account Lake Formation access, support schema evolution, and provide an end-to-end test plan. What is your architecture and approach?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Bloomberg","Databricks","IBM"]},{"id":"q-3358","question":"In a multi-account data lake for regulated analytics, ingest data from multiple SaaS apps into S3 as Parquet. Implement PII masking at ingest, data lineage, and cross-account access controls using Lake Formation. Design end-to-end pipeline with per-source ingestion paths, masking rules, schema evolution, and a robust test plan. Include the roles, services, and hand-off between ingestion, masking, and BI layers?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Citadel","LinkedIn","Meta"]},{"id":"q-3422","question":"A SaaS provider drops a daily CSV export into S3 at s3://data-logs/saasA/YYYY/MM/DD/export.csv.gz. Outline a beginner-level AWS data pipeline using AWS Glue to ingest, convert to Parquet, partition by date, store in s3://data-lake/saasA/YYYY/MM/DD/, register in the Glue Data Catalog, and query with Athena. Include how you’d handle a new column 'region' added today (schema evolution), basic data quality checks, and a minimal end-to-end test plan?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["Amazon","Hashicorp"]},{"id":"q-3464","question":"Design a hybrid streaming/batch data pipeline on AWS for a multi-tenant analytics platform. Ingest streaming events from Kinesis and batch exports to S3; normalize to Parquet with per-tenant prefixes and date partitions. Implement upserts on S3 with Apache Hudi (Glue/EMR), enforce per-tenant RBAC via Lake Formation, and manage schema evolution with Glue Data Catalog. Include data quality using Deequ and an end-to-end test strategy?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Citadel","Databricks","Oracle"]},{"id":"q-3560","question":"Ingest data from three SaaS feeds arriving in S3 with mixed timestamp formats and time zones. Design a beginner AWS data pipeline that uses AWS Glue to normalize timestamps to UTC, convert to Parquet, partition by ingestion date (YYYY/MM/DD), and catalog with Glue Data Catalog; ensure schema evolution with a new 'deviceId' field; implement a basic data quality check (null counts) and a minimal end-to-end test plan. Include how you'd validate a sample dataset and what tests you’d run at each stage?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["Citadel","LinkedIn"]},{"id":"q-3654","question":"Ingest multi-source JSON logs from trading platforms into a centralized data lake on AWS. Platforms evolve schema weekly; events arrive out of order and duplicates exist. Design an end-to-end pipeline using S3, Glue, and Athena that handles schema evolution, deduplication, daily Parquet partitions, and data quality checks. Include testing and cross-region reliability?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Bloomberg","Citadel","Goldman Sachs"]},{"id":"q-3708","question":"A fintech app emits daily NDJSON logs (nested fields) to S3: s3://transact-logs/TEAM/YYYY/MM/DD/tx.ndjson.gz. Outline a beginner pipeline using AWS Glue to crawl, flatten, and convert to Parquet with daily partitions at s3://lake/fintech/TEAM/YYYY/MM/DD/, cataloged in Glue. Include simple data quality checks (null rates, range checks), how to handle future schema drift (new fields), and a minimal end-to-end test plan with Athena validation?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["Anthropic","Meta","PayPal"]},{"id":"q-3725","question":"Beginner-level: Daily multi-tenant event logs arrive as JSON in s3://raw/tenant_id/date/. Design an end-to-end pipeline using S3, AWS Glue (ETL + Data Catalog), and Athena to land data as Parquet with tenant/date partitions, handle evolving schemas with new optional fields, add a lineage table capturing source_path and tenant_id, implement basic data quality checks, and provide a minimal test plan?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["DoorDash","Hashicorp"]},{"id":"q-3912","question":"Land daily CSVs via AWS Transfer Family to s3://reports/raw/YYYY/MM/DD/report.csv.gz. Glue Spark job reads, enforces types, writes Parquet to s3://reports/curated/YYYY/MM/DD/report.parquet partitioned by date. Use Glue Crawler for catalog; handle schema evolution for a new 'region' column. Data quality: row count >0 and non-null keys. End-to-end test with sample file and a quick Athena query?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["LinkedIn","Lyft"]},{"id":"q-4017","question":"Context: Three vendors publish daily JSON files to S3 at s3://fin-data/vendorX/YYYY/MM/DD/data.json.gz. Design a compliant analytics pipeline that preserves data lineage and field-level provenance, supports schema evolution, and enables time-travel queries. Propose end-to-end design using AWS Glue, Glue Data Catalog, Lake Formation, Athena/Redshift Spectrum, and CloudTrail. Include governance, upsert strategy, and tests?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Goldman Sachs","IBM","PayPal"]},{"id":"q-4060","question":"You’re building a beginner-level data ingestion workflow for a fintech telemetry app. Daily JSON logs land in S3; design a pipeline using AWS Glue and Athena that ingests to Parquet with daily partitions, supports simple schema evolution, and emits a lightweight data-lineage record (source, transform, destination) to a separate S3 bucket. Include a basic test plan and cost considerations?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["Citadel","PayPal"]},{"id":"q-4078","question":"Design an end-to-end AWS ingestion and governance pipeline for a multi-tenant SaaS data lake. Data from partners arrives as streaming JSON/Avro; land in S3 as Parquet with daily partitions; govern across accounts using Lake Formation; support schema evolution, late-arriving data, and Athena time-travel queries. Outline architecture, formats, partitioning, quality checks, and testing strategy?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["OpenAI","PayPal","Square"]},{"id":"q-4098","question":"A two-account AWS data lake receives a daily CSV telemetry feed from a 3rd-party vendor into s3://telemetry/vendorX/YYYY/MM/DD/data.csv.gz. Design an end-to-end pipeline to ingest, clean, and store as Parquet in the data lake, with per-tenant isolation, schema evolution for new columns, basic data quality checks, and time-travel via Athena. Include services, formats, partitioning, governance, and a testing plan?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Plaid","Tesla"]},{"id":"q-4186","question":"Design an end-to-end ingestion pipeline for hourly encrypted ORC files from a third-party source that requires upsert-like behavior with deletes. Implement schema evolution, partitioning by hour, and cross-account access using AWS Glue Data Catalog, Lake Formation, S3, Athena, and optional DynamoDB for offset tracking. Include testing, backfill strategy, and rollback plans?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Amazon","Databricks","Twitter"]},{"id":"q-4444","question":"Design a cross-account, multi-region ingestion and governance pipeline: a partner app streams JSON to Kinesis in Account A; use Kinesis Firehose to land Parquet in S3 with daily partitions; Glue ETL masks PII via Lake Formation permissions and handles schema evolution; grant Account B analytics access via trusted roles; include end-to-end tests, dedupe, backfill, and cost controls?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Microsoft","PayPal","Slack"]},{"id":"q-4592","question":"Scenario: A SaaS vendor streams daily JSON events into a central S3 data lake across accounts. Data arrives with nested fields and occasional new keys. Design an end-to-end ingestion and governance plan to land data as Apache Iceberg tables in S3 (partitioned by event_date), with schema evolution via Glue Schema Registry, and upserts for dedup by (customer_id, event_id). Include ingestion path, late-arriving data handling, testing plan, monitoring, and trade-offs between Iceberg vs Glue tables?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Airbnb","Hugging Face","MongoDB"]},{"id":"q-4618","question":"You manage a beginner-level AWS data lake for a daily JSON feed arriving into s3://fin-data/raw/apiX/YYYY/MM/DD/*.json. Design a cost-conscious pipeline to convert to Parquet, partition by date and product, and catalog in Glue. Add basic data quality checks (nulls, data types) and a schema evolution plan for added fields. Include a monitoring/alert plan using CloudWatch/SNS for missing daily files or failed runs?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["Cloudflare","Robinhood"]},{"id":"q-4797","question":"New SaaS nightly export: a gzipped CSV lands in s3://cust-data/raw/saasBeta/YYYY/MM/DD/export.csv.gz. The schema may gain a new column over time (eg region). Design a beginner-level, cost-conscious AWS data ingestion pipeline that lands raw data, converts to Parquet, partitions by date, and catalogs in Glue. Include a schema evolution strategy, basic data quality checks (nulls, types, date parsing), and a test plan for end-to-end verification with alerts via CloudWatch/SNS?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["Discord","Snap"]},{"id":"q-4835","question":"Design an end-to-end pipeline to ingest CDC updates from three operational databases (MySQL, PostgreSQL, DynamoDB) into an S3 data lake as Parquet, using Apache Iceberg for upserts and deletes. Include partitioning strategy, schema evolution, late data handling, compaction, and multi-region replication with Glue Data Catalog and Lake Formation permissions. Compare against Delta/Lakehouse trade-offs?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Anthropic","Apple","Instacart"]},{"id":"q-886","question":"Scenario: you manage a financial data lake with multiple transactional sources feeding S3 via DMS. Stakeholders require upserts, full history for compliance, fast dashboards using a latest-state table, and seamless schema evolution. Compare Iceberg, Hudi, and Delta Lake for this scenario and outline a concrete pipeline (CDC, ETL, compaction, governance)?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Goldman Sachs","Tesla"]},{"id":"q-948","question":"In a multi-tenant data lake on S3 across two AWS accounts, each tenant's data must be isolated, with auditable access, cost accounting, and fast analytics for dashboards. Design a solution using AWS Lake Formation for governance, S3 prefixes per tenant, and Athena/Glue for analytics. Explain cross-account sharing, tag-based access, and data lineage, plus failure modes?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Apple","Discord","Netflix"]},{"id":"q-1074","question":"Scenario: A global time-series platform ingests 1M events/hour in us-west-2; dashboards in eu-central-1 and ap-southeast-2 need sub-200ms reads on the latest window. Data must be immutable for 90 days for compliance. Compare DynamoDB Global Tables with DAX vs Aurora PostgreSQL Global Database with cross-region backups. Provide topology, replication, PITR/backup plans, and RPO/RTO targets?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Apple","Microsoft"]},{"id":"q-1131","question":"**Hybrid Analytics Path for Multiregion Aurora**\n\nYou're running an Aurora PostgreSQL OLTP cluster with tenant isolation via RLS in us-east-1. A regulatory BI team in eu-west-1 requires near real-time analytics with masked PII. Design a hybrid analytics path using Aurora Global Database for OLTP replicas and a CDC-based analytic store (Redshift or DynamoDB+Lambda) in eu-west-1. Describe data flow, masking strategy, encryption, failover, and how to meet RPO 5s and RTO 60s, including cost considerations?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Coinbase","Goldman Sachs","Two Sigma"]},{"id":"q-1279","question":"In a multi-tenant SaaS on AWS, run a single Aurora PostgreSQL cluster with per-tenant schemas and RLS to isolate data. An analytics team in eu-west-1 requires cross-tenant BI with masked PII in near real-time dashboards. Design a cost-aware architecture that delivers masking, auditing, and SLA, comparing per-tenant schemas in a single cluster vs separate clusters per tenant. Include data flow, backup, and failover?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Amazon","Hugging Face","Robinhood"]},{"id":"q-1303","question":"In a multi-tenant SaaS using Aurora PostgreSQL with Global Database spanning us-west-2 and us-east-1, tenants must have isolated data access and BI dashboards must mask PII in real time. Propose an end-to-end design using per-tenant RLS, dynamic masking for BI, and a separate analytics store fed by CDC (DMS/Debezium). Include cross-region DR with RPO <5s and RTO <60s, data flow, encryption, backups, and a concrete sizing plan (replicas, window, network)?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Databricks","Lyft","Tesla"]},{"id":"q-1314","question":"Design a GDPR-compliant data deletion strategy for a multi-region Aurora PostgreSQL Global Database that uses us-east-1 as the writer and replicas in multiple regions. How would you implement Right-to-Erasure for tenant data, propagate deletions with minimal latency, handle referential integrity, and maintain an auditable trail while meeting RPO/RTO targets? Include practical steps and trade-offs?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Google","Hashicorp","Lyft"]},{"id":"q-1382","question":"In an Aurora PostgreSQL Global Database with a writer in us-east-1 and replicas in eu-west-1, a financial balance update must be atomic across regions. Explain why cross-region distributed transactions are not supported and propose a practical pattern to achieve atomic-ish behavior with low latency, including data flow, failover handling, and cost/latency trade-offs?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Airbnb","Stripe"]},{"id":"q-1441","question":"In Aurora PostgreSQL (us-east-1) with tenant isolation via RLS, design a near real-time analytics path for a eu-west-1 consumer needing masked data and <5s lag. Use Aurora Global Database for OLTP and a CDC store in eu-west-1 (Redshift or DynamoDB+Lambda). Explain data flow, masking/encryption, consistency, failover, and cost with concrete config sketches?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Amazon","Microsoft"]},{"id":"q-1447","question":"You run a high-volume ecommerce on Aurora PostgreSQL Global Database with a single writer in us-east-1 and read replicas in eu-west-1. An outage in us-east-1 requires routing writes to eu-west-1 within 60s while ensuring RPO<5s, idempotent writes, and no double billing. Design the architecture and concrete configuration (replica counts, failover procedures, analytics CDC path, masking, PITR) to meet these goals?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Airbnb","Hugging Face"]},{"id":"q-1550","question":"In a two-region deployment with a single writer in us-east-1 and analytic reads in eu-west-1, design a CDC pipeline to keep a near real-time analytic store updated within 5 seconds of commits, while masking per-tenant data and enforcing encryption at rest and in transit. Compare AWS DMS, Debezium/Kafka, and native Aurora logical replication, and provide concrete configuration (engine, instance types, replica counts, PITR, KMS keys, VPC endpoints, and network topology) to meet RPO 5s and RTO 60s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Discord","DoorDash","Snowflake"]},{"id":"q-1747","question":"You run a multi-region SaaS with Aurora PostgreSQL as the OLTP in us-east-1 and read replicas in eu-west-1. A new requirement enforces strict per-tenant data isolation via Row-Level Security and data residency controls for backups. Design a concrete approach: RLS policy skeletons for all tables, session-based tenant_id from authentication, per-tenant restore strategy, cross-region backup copy schedule, and a testing/validation plan that proves no cross-tenant leakage under burden. Include concrete config knobs and a sample policy?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["MongoDB","Slack"]},{"id":"q-1775","question":"In a multi-region Aurora PostgreSQL Global Database setup (writer in us-east-1; readers in eu-west-1 and ap-south-1) with strict tenant-level data residency, design a scalable architecture that provides sub-50ms reads for hot paths in each region while ensuring RPO <= 5s and RTO <= 60s, using row-level security and a CDC-based analytic store; explain data partitioning, access controls, and failover strategy, plus cost trade-offs?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Adobe","Zoom"]},{"id":"q-1882","question":"Design a cross-region analytic path for a SaaS app with an Aurora PostgreSQL OLTP cluster in us-east-1 as the single writer and regional read replicas in us-west-2. The goal: near real-time analytics with masked PII in the analytics store. Propose a CDC-based pipeline (Aurora CDC, DMS, Debezium, or Kinesis) to load into Redshift or DynamoDB in us-west-2, choose masking strategy, encryption, data freshness target (RPO), failover plan, and cost considerations. Include concrete config choices (instance types, retention, network, and security)?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Snowflake","Tesla"]},{"id":"q-2139","question":"You run OLTP in Aurora PostgreSQL us-east-1 and need near real-time BI in us-west-2. Design a CDC pipeline: enable a dedicated logical replication slot in Aurora; use DMS in CDC mode to stream changes to Redshift in us-west-2 via a staging S3 bucket; apply PII masking at BI layer; enable PITR and cross-region backups; target end-to-end latency ~2s and RTO <60s. Include data flow, failover, and cost trade-offs?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Google","Netflix"]},{"id":"q-2163","question":"A multi-region SaaS app needs sub-20ms reads for hot tenants across three continents, writes allowed in any region, and PCI-DSS data residency constraints. Design a data-layer using AWS: compare Aurora PostgreSQL Global Database with DynamoDB Global Tables plus CDC to an analytics store, including consistency, DR, backups, and concrete configurations to meet RPO 5s and RTO 60s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Databricks","Google"]},{"id":"q-2211","question":"Two-region, multi-tenant SaaS with strict data residency: EU tenants' data must stay in EU, US tenants' data in US. Needs sub-15ms reads for hot tenants, writes in any region, and cross-region analytics. Compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables with analytics options; provide a concrete topology, replication, backups, and DR plan to meet RPO 5s and RTO 60s, including per-tenant routing and residency enforcement?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Hugging Face","Salesforce","Twitter"]},{"id":"q-2248","question":"A SaaS news site logs user events at ~1000 writes/s, with dashboards needing sub-200 ms reads. Data is append-only; hot data kept 30 days, archived after. Compare DynamoDB (on-demand, TTL, GSI) vs Aurora PostgreSQL (partitioned tables, read replicas) for this workload. Provide concrete configs (primary key design, indexes, TTL window, backup/retention, RPO/RTO) and justify choice?","channel":"aws-database-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-database-specialty"],"companies":["Netflix","Square","Two Sigma"]},{"id":"q-2268","question":"Your app uses AWS Lambda functions that connect to an RDS PostgreSQL instance; during bursts, you see many connections causing failures. How would you leverage Amazon RDS Proxy to manage connections, configure auth, and ensure stable performance? Include what to monitor, any pricing considerations, and a basic setup outline?","channel":"aws-database-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-database-specialty"],"companies":["Amazon","Goldman Sachs","MongoDB"]},{"id":"q-2320","question":"Scenario: A SaaS app stores event data in DynamoDB and must retain 90 days in DynamoDB and archive older events to S3 for analytics. Design a pragmatic lifecycle: data model, TTL, export to S3, per-tenant/year/month partitioning, storage class selection, and validation plan to ensure data integrity and queryability via Athena. Include concrete steps and considerations for cost?","channel":"aws-database-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-database-specialty"],"companies":["Databricks","Scale Ai"]},{"id":"q-2331","question":"For a multi-tenant SaaS app storing per-tenant PII with EU/US data residency, compare Aurora PostgreSQL with Row-Level Security (RLS) vs DynamoDB with per-tenant access patterns. Explain schema design, replication strategy, consistency, DR, and cost. Provide a concrete configuration to meet RPO < 5s and RTO < 60s, including region placement, replica counts, backup windows, and key management?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["MongoDB","Plaid","Uber"]},{"id":"q-2411","question":"In an IoT platform, 100k writes/sec of time-series data arrive from devices worldwide. You must ingest region-locally with sub-20ms latency, perform near real-time analytics in a separate region, retain 30 days of data, and ensure PCI-DSS residency. Compare AWS Timestream, DynamoDB Global Tables with a CDC pipeline, and an Aurora-based time-series schema. Propose architecture, data model, retention, DR, and concrete config to meet RPO 5s and RTO 60s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["IBM","Twitter","Two Sigma"]},{"id":"q-2465","question":"An Aurora PostgreSQL cluster in us-east-1 serves 10k+ tenants via IAM database authentication. Each tenant must only access its own rows using Row-Level Security. A separate analytics workload must run against a eu-west-1 replica with data within 5 seconds of writes. Design the architecture: RLS policy and session management for per-tenant isolation, how analytics access is granted without leaking data, replication strategy (global DB vs separate KV store), backup/PITR, and a practical test plan to verify isolation and latency?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Citadel","Oracle","Two Sigma"]},{"id":"q-2511","question":"In a globally distributed SaaS, Aurora PostgreSQL Global Database writer in us-east-1 and read replicas in eu-west-1 and ap-southeast-2. To deliver sub-50ms reads for hot tenants during peak while writes can occur anywhere, design a hybrid path using ElastiCache Redis in each region plus a cache-aside strategy. Include concrete config: DB instance classes, replica counts, Redis node types, TTL, invalidation mechanism, and a failover plan that meets RPO 5s and RTO 60s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["IBM","MongoDB","Scale Ai"]},{"id":"q-2533","question":"In a multi-region SaaS app using Aurora PostgreSQL with a single writer in us-east-1 and reads in eu-west-1 and ap-south-1, implement row-level security to restrict each tenant's data. Explain how you would design the RLS policy, index usage, and the impact on cross-region CDC via DMS or logical replication, and outline monitoring for unauthorized access. Include testing steps?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["LinkedIn","Netflix","Uber"]},{"id":"q-2581","question":"Global SaaS with EU data residency: Writes in us-east-1; reads in EU must be sub-20ms; PCI-DSS data residency constraints; design a cross-region OLTP data layer and compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables with CDC. Provide concrete configurations (engine/edition, instance types, replica counts, PITR window, backup cadence, KMS keys, VPC design, inter-region networking), DR plan, and how you meet RPO <5s and RTO <60s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Coinbase","NVIDIA","Stripe"]},{"id":"q-2701","question":"In Aurora PostgreSQL design per-tenant data isolation using Row-Level Security and partitioned tables for a global setup: writer in us-east-1; regional reads in eu-west-1 and ap-south-1. Propose a concrete data path and DR strategy to meet RPO 5s and RTO 60s, including instance types, backup windows, PITR, KMS, cross-region replication, and a real failover plan?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Discord","Microsoft"]},{"id":"q-2907","question":"A multi-region fintech app stores trades in Aurora PostgreSQL us-west-2 with sub-5ms writes and analytics in eu-central-1. Design an AWS-native architecture to meet RPO 5s and RTO 60s, ensure immutable audit logs in S3 with Object Lock, and maintain data residency. Compare Aurora Global Database writer+region replica vs DynamoDB+DMS analytics path, with concrete config (instance classes, replica counts, backup windows, KMS keys, IAM roles) and failover plan?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Microsoft","MongoDB","Robinhood"]},{"id":"q-2935","question":"Design a cross-region fintech data path where writes land in DynamoDB Global Tables (multi-master) in us-east-1 and eu-west-1, while analytics are powered by an Aurora PostgreSQL cluster in eu-west-1. Explain conflict handling, data residency for PCI-DSS, and how you meet RPO <5s and RTO <60s. Include concrete configurations: DynamoDB table keys and streams, Lambda, DMS or Debezium, Aurora size, backups, and network topology?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Hashicorp","Meta"]},{"id":"q-2979","question":"Beginner scenario: An online storefront runs an AWS RDS MySQL instance in a single region with 1–2 read replicas. Compliance requires RPO 15 minutes and RTO 60 seconds during disaster recovery. Propose a practical backup and recovery plan using automated backups, PITR, Multi-AZ, and read replicas, with concrete values for backup retention, PITR window, and snapshot cadence, plus a reproducible failover procedure?","channel":"aws-database-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-database-specialty"],"companies":["NVIDIA","Oracle","Twitter"]},{"id":"q-3137","question":"Aurora PostgreSQL is deployed for a new product with highly variable traffic but strict latency targets. Compare Aurora Serverless v2 vs provisioned Aurora with read replicas for cost, latency, and maintenance. Provide a concrete setup example: (A) Serverless v2 with min 2 ACU, max 32 ACU, pause after idle, and RDS Proxy for connection pooling; (B) provisioned Aurora with 2 writer instances and 3 read replicas in the same region, plus a PgBouncer pool. Conclude when to choose each?","channel":"aws-database-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-database-specialty"],"companies":["Airbnb","Scale Ai"]},{"id":"q-3289","question":"Two-region SaaS with strict tenant data residency: tenants assigned to a region cannot write in the other region, but hot tenants require sub-20ms reads globally. OLTP store is Aurora PostgreSQL and needs to support a single writer region with read replicas in the other region. Design a data layer that enforces per-tenant residency, delivers fast reads, and meets RPO 5s and RTO 60s. Include concrete configurations, data partitioning, routing rules, and a rollback plan for residency violations?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Anthropic","Instacart"]},{"id":"q-3367","question":"In a three-region deployment (us-east-1, eu-west-1, ap-southeast-2) with a single Aurora PostgreSQL Global Database writer in us-east-1, design a 2-tier data path that delivers sub-20 ms reads for hot tenants region-locally while writes can occur anywhere. Propose a cross-region caching strategy using Redis Global Datastore, plus a per-region analytics store. Include concrete configurations (replica counts, Redis node types, TTLs, backup windows, cross-region data sharing, and data residency controls) to meet RPO 5s and RTO 60s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Anthropic","Oracle"]},{"id":"q-3440","question":"Design and compare two data-architecture options for a PCI-DSS-compliant, multi-tenant SaaS using AWS databases: option A uses a single Aurora PostgreSQL cluster with Row-Level Security (RLS) to isolate tenants and KMS-encrypted backups plus cross-region snapshot replication; option B uses separate clusters per tenant across regions. Which approach would you pick and why?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["IBM","LinkedIn","Robinhood"]},{"id":"q-3465","question":"Scenario: A real-time fraud graph workload spans three regions. Primary data sits in Neptune with cross-region replication. Need sub-50ms neighbor lookups on random 1k-node subgraphs; writes across regions; DR targets: RPO 5s, RTO 60s. Design a concrete architecture comparing Neptune Global Database alone vs a hybrid approach using Neptune in each region plus ElastiCache Redis caches and a streaming path to OpenSearch for analytics. Provide concrete config: engine versions, instance counts, TTLs, and explicit failover steps?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Anthropic","Citadel","Hugging Face"]},{"id":"q-3587","question":"Compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables for a read-heavy user profile store spanning us-east-1 and us-west-2. Provide concrete configurations to meet RPO 5s and RTO 60s; detail data model fit, backup plans, and failover steps. Assume a SQL workload with FK constraints and occasional transactional updates; keep latency targets <50ms in primary region?","channel":"aws-database-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-database-specialty"],"companies":["Google","Slack"]},{"id":"q-3673","question":"Scenario: You manage an Aurora PostgreSQL Global Database with writer in us-east-1 and read replicas in eu-west-1 and ap-southeast-2. A new tenant requires strict data isolation via Row-Level Security and an auditable analytics path. Design a plan that (a) enforces per-tenant isolation in the global cluster (RLS + SECURITY DEFINER wrappers), (b) maintains cross-region replication with RPO <= 10s while writes land in writer only, (c) captures tenant-access events to S3 with a near-real-time pipeline (DMS) across regions, and (d) specifies concrete DB settings (instance class, replica count, parameter group, backup window, and cross-region KMS keys) to meet an RTO <= 60s. Include concrete configuration choices?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["DoorDash","Google","Salesforce"]},{"id":"q-3711","question":"Scenario: Build a mobile analytics backend in us-east-1 handling 1.5k–2k writes/sec and per-user reads for dashboards. Compare DynamoDB with Global Tables vs Aurora MySQL Serverless v2 for this workload. Provide a concrete setup (schema, indexes, backups, DR, failover steps) to meet RPO 5s and RTO 60s, and note consistency and cost trade-offs?","channel":"aws-database-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-database-specialty"],"companies":["Adobe","DoorDash","Zoom"]},{"id":"q-3772","question":"In a multi-region deployment, need strong OLTP in us-east-1 and real-time analytics in eu-west-1. The dashboards must reflect writes within ~5 seconds with writes allowed in both regions. Design a practical CDC path using AWS tools (DMS vs Debezium), specify data flow, latency targets, conflict handling, backups, and monitoring to meet RPO 5s and RTO 60s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Amazon","Meta","OpenAI"]},{"id":"q-3816","question":"Two-region SaaS using Aurora PostgreSQL with writes allowed in both regions. Propose a concrete, auditable design to meet RPO 5s and RTO 60s while ensuring tenant data isolation. Compare Aurora PostgreSQL Global Database (single writer) vs independent clusters with bidirectional logical replication (DMS or Debezium), including conflict resolution, auditing, backups, and monitoring. Provide a recommended concrete configuration and rationale?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Citadel","Databricks","DoorDash"]},{"id":"q-3977","question":"Scenario: A SaaS app needs per-tenant data isolation and fast reads in two AWS regions, with ~100 tenants and light writes. Compare DynamoDB Global Tables vs Aurora PostgreSQL cross-region replicas, focusing on data model, transactional guarantees, backups/DR, latency, and cost. Propose a concrete starter config to meet an RPO of 5 seconds and an RTO of 60 seconds (include region pair and basic sizing)?","channel":"aws-database-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-database-specialty"],"companies":["MongoDB","NVIDIA"]},{"id":"q-4194","question":"A startup SaaS app experiences highly variable traffic with 2-5x daily peaks and a need for both relational queries and fast session lookups. Design a cost-conscious AWS data layer comparing Aurora Serverless v2 (PostgreSQL) versus DynamoDB (on-demand) for this workload. Include data model, indexing, consistency, backups, failover, latency targets, and concrete configs to meet an RPO of 15s and an RTO of 60s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-database-specialty"],"companies":["Google","Oracle","Salesforce"]},{"id":"q-4210","question":"Beginner scenario: An Aurora PostgreSQL cluster stores PCI data in **us-east-1**. To meet RPO < 15s and RTO < 2m after accidental data deletion, propose a practical backup/DR plan using automated backups, PITR, cross-region replication (Global Database), and cross-account DR drills. Include concrete settings (backup retention days, PITR window, replica count, failover priority) and the exact steps to run a DR drill?","channel":"aws-database-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-database-specialty"],"companies":["Google","LinkedIn"]},{"id":"q-4310","question":"You run an Aurora PostgreSQL Global Database with a writer in us-east-1 and read replicas in eu-west-1 and ap-south-1. A new requirement demands sub-50ms per-tenant OLTP reads and near-real-time analytics per tenant using a separate analytics store. Propose a concrete hybrid architecture (OLTP + analytics) using AWS services (e.g., DMS, DynamoDB, Redshift/Glue), detail data flows, replication settings, consistency, failover, and a plan to validate SLAs before production?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Amazon","Google","Oracle"]},{"id":"q-4355","question":"You maintain an Aurora PostgreSQL cluster serving 100+ tenants in a SaaS product. Data for all tenants lives in the same database but must be strictly isolated in reads; tenants have varying access patterns and analytics needs. Design a solution using PostgreSQL Row-Level Security and tenant_id partitioning, plus AWS tools (Global Database, PITR, cross-region replicas) to meet isolation, sub-second hot-tenant reads, and near real-time analytics for aggregates. Include credential rotation and audit considerations. What is your approach?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Adobe","Hugging Face","Twitter"]},{"id":"q-4372","question":"Cross-account analytics mirror: A SaaS app writes to an RDS PostgreSQL in us-east-1. A separate analytics account needs a read-only mirror for dashboards with low lag. Design a beginner-friendly pattern using cross-account RDS snapshot sharing and a refreshed read replica, specify backup retention, sharing steps, and the exact process to refresh daily?","channel":"aws-database-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-database-specialty"],"companies":["IBM","Meta","Snowflake"]},{"id":"q-4403","question":"Scenario: A fintech SaaS stores PCI-DSS data for some tenants in us-east-1 and non-PCI data globally. Design an architecture using Aurora PostgreSQL Global Database to minimize latency, ensure strict data residency, and provide auditable backups and DR. Include topology, data isolation (RLS), backup strategy, and DR test steps?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Adobe","Instacart","Salesforce"]},{"id":"q-4467","question":"In a two-region Aurora PostgreSQL Global Database with a primary cluster in us-east-1 and an analytics sink in eu-west-1 (Redshift), design a practical CDC pipeline to feed near-real-time analytics. Choose between AWS DMS and Debezium, justify the choice, and specify data flow, latency targets (RPO <5s, RTO <60s), conflict handling for mirrored updates, backups, and monitoring. Include concrete settings (PITR window, replication task settings, lag budgets) and DR testing cadence?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Amazon","Apple","Snap"]},{"id":"q-4619","question":"A PCI-compliant SaaS stores customer data in Aurora PostgreSQL in us-west-2. An analytics sink in eu-central-1 must receive masked PII with near real-time updates. Design a streaming path using DMS or Debezium (or AWS Glue) that masks PII before replication, specify RPO <5s, RTO <60s, encryption (TLS at transit, KMS at rest), PITR windows, failover steps, and monitoring. Include data masking rules and DR testing cadence?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Netflix","Twitter","Two Sigma"]},{"id":"q-4634","question":"In a production Aurora PostgreSQL cluster in us-east-1 handling PCI data, you need a cost-effective staging clone for weekly integration tests that does not impact production performance. Explain how you would use Aurora Fast Database Clones (and/or Snapshots), setup, automation, and rollback strategy, including backup retention, PITR window, and permissions. Include concrete settings and steps to refresh weekly?","channel":"aws-database-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-database-specialty"],"companies":["Apple","NVIDIA","Slack"]},{"id":"q-4799","question":"Real-time bidding platform uses Aurora PostgreSQL Global Database with a primary in us-east-1 ingesting ~200k inserts/sec from a streaming pipeline; read replicas in eu-west-1 run analytics. Peak load spikes WAL generation and replica lag. Propose concrete optimizations to reduce WAL pressure and lag: batch commit sizing, WAL-related parameters (wal_level, max_wal_senders, wal_buffers), autovacuum tuning, and a connection-pooling strategy. Include exact parameter ranges, testing steps, and validation metrics?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Anthropic","Microsoft","NVIDIA"]},{"id":"q-851","question":"Two-region OLTP SaaS with a single writer in us-east-1 and read replicas in eu-west-1. Compare Aurora Global Database (PostgreSQL) vs DynamoDB Global Tables for this workload: latency targets, consistency model, failover behavior, and cost. Which approach would you pick and why, and what concrete configuration (replica count, failover window, write routing) would you implement to meet RTO < 60s and RPO < 5s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Robinhood","Snowflake"]},{"id":"q-871","question":"Migration plan: An OLTP app runs on Aurora PostgreSQL provisioned; traffic is bursty; you want to evaluate Aurora Serverless v2. Provide a concrete plan to migrate, including: (1) start/stop criteria and scaling configuration; (2) handling of long-running transactions and prepared statements; (3) how to keep reads consistent during scaling; (4) testing approach for failover/RTO targets; (5) cost considerations and potential pitfalls with Serverless v2?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Hugging Face","IBM","Uber"]},{"id":"q-895","question":"Your multi-region SaaS needs an audit-friendly cross-tenant analytics store with writes transactional in us-east-1 and analytics queries in eu-west-1 under GDPR. Compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables for this workload, focusing on transactional integrity, analytics capability, PITR/retention, cross-region latency, and cost. Recommend a concrete configuration (writer region, replica counts, PITR window, tenant isolation, ETL approach) to meet RPO 15 minutes and RTO 1 hour?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Google","Slack","Square"]},{"id":"q-956","question":"For a real-time fraud graph application needing sub-100ms neighbor lookups across two AWS regions, compare Amazon Neptune Global Database with DynamoDB (using graph patterns and DAX) for this workload. Writer region us-east-1; readers in eu-west-1; assess graph traversal latency, consistency guarantees, failover behavior, and total cost. Provide a concrete setup (cluster engine and size, replica counts, PITR window, backup schedule, and network/config) to meet an RPO of 5s and an RTO of 60s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Meta","NVIDIA","Snap"]},{"id":"q-964","question":"You run an Amazon RDS PostgreSQL in **us-east-1** with automated backups. A regional outage blocks access from that region. How would you achieve **RPO ≤ 60s** and **RTO ≤ 15 minutes** by restoring to **eu-west-1**? Compare cross-region read replicas, backup copy, and Aurora Global Database, and outline concrete steps, knobs, and caveats?","channel":"aws-database-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-database-specialty"],"companies":["LinkedIn","Slack","Tesla"]},{"id":"q-1183","question":"Design a cross-account AWS CI/CD flow for a real-time analytics platform with data plane in Account A, model training in Account B, and API endpoints in Account C. Implement Terraform-driven IaC, policy-driven approvals, drift detection, canary promotion with synthetic monitoring, and automated rollback. How do you enforce least privilege, secret rotation, and auditable artifact pipelines across accounts?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Coinbase","Microsoft","MongoDB"]},{"id":"q-1416","question":"Design a beginner-friendly AWS CI/CD pipeline for a Dockerized REST API deployed to ECS Fargate behind an ALB. Source from GitHub; CodeBuild runs pytest and a Trivy container image scan; pushes image to ECR; deploys with a blue/green traffic shift across ECS target groups. Explain IAM least-privilege and Secrets Manager usage; include manual Prod approval and basic observability?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Airbnb","Microsoft","Stripe"]},{"id":"q-1553","question":"Design a cross-region, risk-aware deployment for a real-time streaming data platform (Kinesis Data Streams → Lambda → DynamoDB) using CodePipeline/CodeBuild and CDK. Implement blue/green rollout across us-east-1 and us-west-2, with synthetic canary tests, latency/SLA-based automatic rollback, and drift detection. Explain IAM roles, Secrets rotation, and audit logging?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Robinhood","Square","Zoom"]},{"id":"q-1613","question":"Design a Git-driven CI/CD workflow to deploy a two-region microservice API on EKS (us-east-1, us-west-2) with Argo Rollouts canary, Terraform IaC, and DynamoDB Global Tables. Include region-specific config via AppConfig, IAM least privilege, drift detection, and automated rollback on latency or error-rate spikes?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Airbnb","Netflix","Zoom"]},{"id":"q-1728","question":"Design a beginner-friendly AWS CI/CD workflow that builds a Docker image from a GitHub repo, runs unit tests, runs a static security scan, pushes to ECR, and deploys to an ECS Fargate service using CodeDeploy blue/green with a canary (15%). Include rollback on failure, health checks, and IAM least-privilege. Explain staging vs prod isolation and how you’d structure permissions and secrets?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["NVIDIA","Snowflake","Two Sigma"]},{"id":"q-1811","question":"Design a GitHub-driven CI/CD pipeline that builds microservices to ECR, tests in CodeBuild, and deploys via ArgoCD to per-service namespaces on EKS. Store per-service configs in Secrets Manager/SSM; enforce IAM least privilege with scoped roles; implement canary rollouts with synthetic checks and automatic rollback on failure. Include artifact/config flow and audit strategy?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Google","Oracle","Square"]},{"id":"q-1822","question":"Design a cross-account, multi-region deployment for a real-time fraud-detection platform: data plane in Account A (Kinesis/S3), model training in Account B (SageMaker), live REST endpoints in Account C (API Gateway/Lambda/DynamoDB). Provide a concrete plan using Terraform, CodePipeline, cross-account roles, drift checks, end-to-end tracing, and canary promotions with automated rollback. Include IAM, secret rotation, and success criteria?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Coinbase","Databricks","Scale Ai"]},{"id":"q-1865","question":"You have a small containerized web app in GitHub, deployed to AWS ECS Fargate behind an Application Load Balancer. Design a beginner CI/CD pipeline using GitHub Actions to build and push a Docker image to ECR, register a new ECS task definition, update the staging service, run a smoke test against the staging ALB, and require a manual approval before production. How would you implement IAM least-privilege and secrets management (Secrets Manager or SSM) for this flow?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Amazon","OpenAI","Robinhood"]},{"id":"q-1895","question":"In a three-region deployment for a real-time personalization service using Lambda@Edge and CloudFront, design a CI/CD workflow that sources code from Git, provisions infra with Terraform, and coordinates region-local deployments via CodePipeline. Implement per-region canaries with traffic weights, automated rollback on synthetic checks, Secrets Manager rotation, KMS key rotation, and auditable artifacts. Separate staging and prod accounts and validate edge caching with synthetic tests?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Salesforce","Scale Ai","Snap"]},{"id":"q-1907","question":"Design a beginner-friendly AWS DevOps pipeline for a small Node.js API containerized app deployed to ECS Fargate behind an Application Load Balancer. Source from GitHub; CodeBuild runs npm test and a lightweight security scan (Trivy); build Docker image and push to ECR; deploy via CodePipeline using ECS blue/green (CodeDeploy) with canary shifts across dev, stage, prod. Explain IAM least privilege and Secrets Manager use for env vars, plus rollback triggers?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Meta","Microsoft","Slack"]},{"id":"q-1923","question":"Design a two-account AWS CI/CD pipeline for a multi-tenant SaaS app deployed to ECS Fargate, sourced from GitHub. Use Terraform for IaC, enforce per-tenant isolation via tag-based deployment, and AppConfig feature flags to enable tenant-specific features. Implement cross-account artifact sharing, canary promotions with synthetic monitoring, and automatic rollback on latency or error-rate thresholds. Explain IAM guardrails and auditability across accounts?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Anthropic","Databricks","Zoom"]},{"id":"q-1960","question":"You’re deploying a simple REST API on AWS Lambda behind API Gateway with a GitHub source. Implement a runtime feature flag using AWS AppConfig that can be toggled without redeploying. Describe the minimal IAM permissions, how Lambda fetches the flag on cold starts and keeps it refreshed, and a safe canary rollout workflow with rollback. Include a brief code snippet in your preferred language?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Apple","Lyft"]},{"id":"q-1984","question":"Design a two-region CodePipeline per region (us-east-1, eu-west-1) for a three-service ECS/Fargate app behind an ALB. Source from GitHub; CodeBuild runs unit/integration tests plus Trivy/CodeQL scans; artifacts encrypted in S3; deploy via CDK to StackSets; blue/green per service; Route 53 latency routing; automatic rollback on health checks; drift detection with AWS Config; Secrets in Secrets Manager; enforce least-privilege IAM?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Airbnb","OpenAI","Stripe"]},{"id":"q-1998","question":"You're deploying a small REST API to AWS ECS Fargate in us-east-1 with GitHub as the source and a basic CodePipeline. Implement a beginner-friendly disaster-recovery workflow: cross-region deployment to us-west-2, Route 53 failover, and S3 artifact replication. Show minimal IaC (CloudFormation), a simple health-check-based failover policy, and how you test failover?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Citadel","OpenAI"]},{"id":"q-2029","question":"Design a GitOps-based CI/CD workflow for a microservices app running on two AWS accounts with an EKS cluster in each (staging and prod), using FluxCD to deploy Helm charts, IRSA for service accounts, and OPA Gatekeeper for policy enforcement, plus Istio for canary promotions and automatic rollback on 5xx errors or latency spikes. Include repo structure, IAM roles, and example configurations?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Goldman Sachs","Google","PayPal"]},{"id":"q-2058","question":"Design a multi-region, zero-downtime deployment for a serverless REST API using AWS Lambda, API Gateway, and DynamoDB, with cross-region replication, automated canary shifts, and regional failover testing. Provide Terraform that wires cross-account roles, Secrets Manager, and CloudWatch logs, plus a rollback plan?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Google","OpenAI","PayPal"]},{"id":"q-2115","question":"You're deploying a small REST API container to AWS ECS Fargate in us-east-1 behind an Application Load Balancer, with GitHub as the source and a basic CodePipeline. Implement a beginner-friendly blue/green deployment using CodeDeploy for ECS: provide a minimal CloudFormation snippet to enable CODE_DEPLOY deployments, configure a 10% canary start, include a simple health check path, and outline rollback criteria if health checks fail or error rate exceeds threshold. How would you test failover and enforce least-privilege IAM roles?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Coinbase","Square","Twitter"]},{"id":"q-2142","question":"Design a beginner-friendly AWS CI/CD pipeline for a small ECS Fargate app where the Infrastructure as Code is written in Terraform and source is GitHub. Implement a remote Terraform backend using S3 with DynamoDB locking, and a CodePipeline that runs `terraform init` and `terraform plan` on pushes, requiring a manual approval before `terraform apply`. Add drift-detection that halts deployments when `terraform plan` shows changes, and outline a safe rollback mechanism to the previous Terraform state. Include minimal Terraform snippets for the S3 backend and a pipeline step?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Adobe","Robinhood"]},{"id":"q-2191","question":"Design a cross-account CI/CD workflow for a data platform where Snowflake is the analytics warehouse and an AWS-based ETL stack runs in two accounts (prod and analytics). Implement GitOps-style deployment for Snowflake objects and Snowpipe pipelines, while deploying application updates to an EKS cluster. Include drift detection with automatic rollback, canary data tests using synthetic events, least-privilege IAM, and example IaC snippets in Terraform and SnowSQL. How would you structure this end-to-end pipeline and what are the key failure modes and mitigations?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["DoorDash","Lyft","Snowflake"]},{"id":"q-2317","question":"You’re deploying a small REST API container to AWS ECS Fargate in us-east-1 via CodePipeline with GitHub as the source. Design a beginner-friendly observability-based rollback: add a CloudWatch Synthetics canary that exercises the endpoint every minute, emit a CanarySuccess metric, and a rollback alarm that triggers CodePipeline to rollback if canary success rate falls below 98% for 3 consecutive runs. Include minimal CloudFormation snippets and testing steps?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["NVIDIA","PayPal","Two Sigma"]},{"id":"q-2348","question":"Design a two-account AWS CI/CD pipeline for a microservices app running on ECS Fargate behind an ALB, source from GitHub, with CodePipeline and CodeBuild. Deploy to staging in Account A and production in Account B. Use CodeDeploy for blue/green deployments, store artifacts in a central S3 bucket, and implement promotion to prod only after 24h of synthetic canaries (CloudWatch Synthetics) and real-user latency/error-rate below thresholds. Include IAM roles, Secrets Manager rotation, and minimal IaC sketches?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Salesforce","Snap"]},{"id":"q-2535","question":"You're deploying a small stateless Node.js REST API to AWS ECS Fargate in us-east-1, sourced from GitHub, via a beginner-friendly CodePipeline. Provide a minimal CloudFormation snippet for the ECS cluster, task definition, service, an Application Load Balancer and target group, and an ECR repo. Enable a 10% canary deployment via CodeDeploy with a 90/10 traffic split, a /health endpoint health check, and a plan to test failover and rollback in CI?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Snap","Square"]},{"id":"q-2590","question":"Design a controlled chaos plan for a multi-region AWS setup hosting microservices on EKS and Lambda, with data stores in DynamoDB and RDS. Use AWS Fault Injection Simulator (FIS) to inject faults (latency, throttling, container crash) in one region at a time. Define blast radius, guardrails, and automatic rollback. How would you orchestrate tests with Step Functions, capture metrics (SLA latency, error rate), and ensure safe rollback with automated rollback and audit trails? Include minimal IaC references?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Databricks","Google","NVIDIA"]},{"id":"q-2759","question":"You're maintaining a small REST API deployed as AWS Lambda behind API Gateway, with dev/stage/prod in a single account. Implement a beginner-friendly maintenance-mode workflow: store a MAINTENANCE_MODE flag in SSM Parameter Store; create a lightweight enforcement layer so when the flag is true, the API returns 503 with a friendly message, otherwise normal responses. Provide minimal CloudFormation snippets to create the parameter and a Lambda function that enforces the flag, plus a simple script to toggle the flag and curl the endpoint. Include IAM least-privilege notes, and explain how you'd test it end-to-end?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Discord","IBM"]},{"id":"q-2773","question":"Design a beginner-friendly GitHub Actions workflow to deploy a serverless API to AWS Lambda via CloudFormation, using OpenID Connect (OIDC) for assuming a least-privilege IAM role. Source from GitHub; artifacts in S3; CloudFormation stack provisions Lambda + API Gateway with a /health endpoint. Include health checks, automated rollback on failure, and minimal IaC?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Netflix","Snowflake"]},{"id":"q-2972","question":"Design a cross-account, Terraform-driven CI/CD pipeline for a microservices app deployed to three AWS accounts (dev, staging, prod) using CodePipeline and Git integration. Include drift detection with AWS Config, automatic remediation via Lambda, Secrets rotation, and auditable artifact pipelines. Explain repo layout, state management, secret handling, and rollback strategies?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Citadel","MongoDB","Oracle"]},{"id":"q-3004","question":"Design an ephemeral, per-PR deployment environment for a microservices app running on Amazon EKS in a single AWS account. Use Argo CD for GitOps to deploy Helm charts into a PR-scoped namespace, enforce policies with OPA Gatekeeper, isolate traffic via Calico network policies, and bind workloads with IRSA. Include repo layout and example configs, plus auto-teardown on PR close?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["LinkedIn","Scale Ai","Uber"]},{"id":"q-3114","question":"You manage a small REST API currently deployed to ECS Fargate in us-east-1 with GitHub as source. Design a beginner-friendly GitHub Actions workflow to create an ephemeral preview environment per feature branch. Each preview runs in its own VPC, deploys a minimal ECS Fargate service via CloudFormation, exposes a Route53 subdomain (branch.yourdomain), and includes a /health check. Describe teardown on PR close and testing steps?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Amazon","Databricks","Meta"]},{"id":"q-3165","question":"Design a cross-account, cross-region data processing pipeline for a real-time analytics API across three AWS accounts (Prod, Staging, Data). Ingest via SQS, process with Lambda, and store results in S3/Glue. Include idempotency, dead-letter handling, cross-account IAM roles, and a canary-like rollout using Route 53 latency routing with automatic rollback on error/latency. Provide repo layout and sample configs?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Amazon","Citadel","Discord"]},{"id":"q-3255","question":"You're deploying a small REST API container to AWS ECS Fargate in us-east-1 via CodePipeline with GitHub as the source. How would you implement beginner-friendly cost and security guardrails: (1) an AWS Budget that triggers a pipeline rollback if projected monthly costs exceed $20, (2) least-privilege IAM policies for the pipeline, and (3) a simple 5xx alert with a test plan, plus minimal CloudFormation snippets?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Airbnb","Salesforce"]},{"id":"q-3332","question":"In a two-account, two-region setup with real-time streaming microservices on EKS, design an advanced GitOps deployment for a Kinesis-powered pipeline. Use FluxCD to manage Helm releases across both clusters, IRSA for per-service least-privilege access, and OPA Gatekeeper for policy compliance. Implement synthetic canaries (latency, error rate) with automated promotion and rollback, and provide repo structure, IAM roles, and config examples?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Cloudflare","Google","Netflix"]},{"id":"q-3346","question":"Scenario: A payment service spans two AWS accounts (prod/staging) behind an ALB. Design an automated incident response pipeline that triggers on prod 5xx spikes and performs: 1) a synthetic canary check, 2) analysis of X-Ray traces to locate hotspots, 3) a rollback to the last green deployment or auto-scale if under load, and 4) a Slack notification. Include IAM roles, tooling, and a minimal Step Functions workflow?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Adobe","Meta"]},{"id":"q-3451","question":"Design an intermediate AWS DevOps pipeline for a multi-tenant Node.js API behind API Gateway and ECS Fargate. Each tenant must be isolated in CI/CD, with canary promotions and automatic rollback on latency/spike. Use GitHub as source, CodePipeline/CodeBuild for tests, CloudFormation IaC, and per-tenant deployment configurations in CloudFormation/ECS. Include repo layout, IAM roles, sample CloudFormation snippets, and a plan for tenant onboarding/offboarding and cost controls?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["DoorDash","Slack"]},{"id":"q-3565","question":"Design a beginner-friendly CodePipeline-based CI/CD flow for a small ECS Fargate microservice in us-east-1 behind an ALB. Sourced from GitHub, add a feature flag via AWS AppConfig to gate a new endpoint. Include a minimal CloudFormation snippet for the ECS service and AppConfig configuration, plus a health-check-driven rollback if the flag causes 5xx errors. Outline testing steps?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["LinkedIn","Robinhood","Uber"]},{"id":"q-3648","question":"Design a two-region, two-account deployment for a REST API (API Gateway + Lambda) backed by DynamoDB. Implement progressive canary releases using API Gateway stage canary and Route 53 weighted routing, with a CDK-based pipeline to promote revisions. Add CloudWatch Synthetics chaos tests in staging, and automatic rollback on latency spikes or error bursts. Include IAM trust and guardrails?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Citadel","LinkedIn","Plaid"]},{"id":"q-3731","question":"Design a beginner-friendly CI/CD for a small REST API deployed to AWS Lambda behind API Gateway using CDK (Python) for IaC, with GitHub as the source, Secrets in Parameter Store, and a CloudWatch-based health-check rollback that redeploys a previous stable version after 3 consecutive minutes with error rate > 2%? Provide a minimal CDK snippet and a simple test plan?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Cloudflare","Square","Tesla"]},{"id":"q-3767","question":"Design a verifiable, tamper-evident deployment workflow for a real-time data-processing service across two AWS accounts (staging and prod) with EKS in each. Implement a GitOps flow (FluxCD or ArgoCD) that signs manifests with a KMS key, stores them in S3 with object-lock, and enforces policy checks via OPA Gatekeeper. Include cross-account IAM roles, canary promotions via Istio, auto-rollback on 5xx latency spikes, and a rollback audit trail?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Cloudflare","Instacart"]},{"id":"q-3810","question":"You have a small REST API deployed to AWS ECS Fargate in us-east-1 with GitHub as the source and a basic CodePipeline. Your cost target requires turning off the service during off-peak hours on weekdays and turning it back on in the morning. Outline a beginner-friendly approach to implement scheduled scaling using ECS Fargate, CloudWatch EventBridge, and a minimal CloudFormation snippet to update the service's desiredCount. Include testing and rollback considerations?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["IBM","Twitter"]},{"id":"q-3842","question":"Context: A two-region, multi-account AWS deployment with EKS clusters in each region and FluxCD deploying Helm charts. You must enforce per-namespace compliance via OPA Gatekeeper and a cross-account audit trail with AWS Config, CloudTrail, and Security Hub, while ensuring drift is detected and fixed without downtime. Design the solution and testing approach?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Meta","Tesla"]},{"id":"q-3906","question":"Design a secure software supply chain for a production-grade serverless API on AWS (Lambda + API Gateway) with GitHub as the source and CloudFormation for IaC. Implement provenance (SLSA-like), SBOM generation, and OPA-based policies to enforce: only approved dependencies, signed artifacts, and no drift in IaC between declared snapshot and deployed stack. Outline workflow, required IAM roles, and example policy and build spec?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Apple","Citadel","Meta"]},{"id":"q-4000","question":"Deploy a static SPA to AWS with a beginner-friendly GitHub Actions workflow that uses CloudFormation to provision an S3 bucket with versioning, an Origin Access Identity, and a CloudFront distribution in us-east-1. Include a /health object for a simple health check, CloudFront invalidation on deploy, and a basic rollback if /health fails. Describe steps and minimal IaC?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Cloudflare","Google","Tesla"]},{"id":"q-4113","question":"You're deploying a containerized API to ECS Fargate in us-east-1 via CodePipeline with GitHub as the source. Design a beginner-friendly pipeline that uses a blue/green deployment with CodeDeploy, includes a manual prod-approval gate, and introduces a feature flag stored in SSM Parameter Store to toggle a new response header. Provide minimal CloudFormation for the flag and deployment group, and outline tests and rollback steps?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Hugging Face","MongoDB"]},{"id":"q-4133","question":"You operate a multi-tenant data platform on AWS with three tenants, each in separate VPCs, services deployed to an EKS cluster in us-east-1. Implement a cross-account, tenant-scoped CI/CD using FluxCD to promote per-tenant Helm overlays from staging to prod, with Canary upgrades and 5xx rollback. Add a hot-standby DR in us-west-2 with RDS Postgres, Global Accelerator, and Route 53 failover. Include IAM SCPs, per-tenant roles, and minimal tenant values + FluxCD Kustomization examples. What would you build and how would you verify resilience and cost controls?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Amazon","Bloomberg","MongoDB"]},{"id":"q-4316","question":"Design a GitOps-based CI/CD workflow for a data-processing microservice stack on AWS EKS across two accounts (staging and prod) using FluxCD to deploy Helm charts, IRSA for service accounts, and CDK for immutable infrastructure. Include per-environment data-domain isolation, automated cost budgets, data-quality checks, and a rollback trigger on SLA violation?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Apple","Netflix","Slack"]},{"id":"q-4469","question":"Design an intermediate AWS DevOps pipeline to deploy a data-processing workflow across two AWS accounts using GitHub Actions. Use S3, Glue ETL, and Step Functions to orchestrate, with containers in ECR and OIDC-based least-privilege IAM roles. Enforce data residency via Config rules and KMS encryption; provide repo layout, roles, and sample configs?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Databricks","Lyft","Microsoft"]},{"id":"q-4505","question":"Design a cross-region, multi-account release for a data platform: two AWS regions, staging and prod per region, FluxCD in EKS for microservices, CloudFormation for non-K8s resources, and S3/DynamoDB data lake with cross-region replication. Define per-account IAM roles/trusts, OPA Gatekeeper policies, and a Canary rollout with automatic rollback on 5xx or latency spike. Include repo layout, sample IaC snippets, and a DR test plan injecting failures in staging before prod?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Bloomberg","IBM","Stripe"]},{"id":"q-4528","question":"Design an automated disaster-recovery (DR) strategy for a two-account, two-region AWS deployment. Prod in us-east-1 with EKS managed by FluxCD; DR in us-west-2. Include data replication (DynamoDB Global Tables, S3 cross-region replication), DNS failover (Route 53), IAM least-privilege roles and cross-account access, and an automated DR validation that runs weekly and can rollback with minimal data loss. Provide concrete steps and artifact examples?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Amazon","Apple"]},{"id":"q-4576","question":"Design a beginner-friendly CI/CD for a static site deployed to S3 with CloudFront in us-east-1. Source from GitHub; deploy via GitHub Actions; implement dev/stage/prod with separate S3 buckets and CloudFront distributions. Add a CloudWatch Synthetics canary to verify the homepage, failing the deployment if latency > 2s or availability < 99%. Include a minimal CloudFormation snippet and testing steps?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Cloudflare","Databricks","IBM"]},{"id":"q-4655","question":"Design an automated disaster-recovery (DR) workflow for a multi-region microservices app deployed on EKS in us-east-1 (primary) and us-west-2 (secondary). Ensure data consistency, sub-15-minute RTO and sub-5-minute RPO, with FluxCD syncing across regions, Route 53 failover routing, DynamoDB Global Tables and S3 cross-region replication, and a tested, auditable DR switch manifest. Include IAM boundaries, CI/CD steps, and a minimal cutover plan?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["IBM","Slack"]},{"id":"q-676","question":"You have a Node.js app deployed on EC2 instances behind an Application Load Balancer in a private VPC. You want a beginner-friendly, repeatable CI/CD pipeline that triggers on git pushes, runs tests, and safely deploys with rollback. Describe a practical setup using AWS CodePipeline, CodeBuild, and CodeDeploy (blue/green) to auto-build, test, and deploy with artifact flow in S3, and IAM roles with least privilege, including how to isolate staging vs production using separate target groups?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Cloudflare","Discord","Google"]},{"id":"q-890","question":"Design an AWS-based CI/CD pipeline for a data platform (S3 data lake, Lambda ETL, ECS) that sources from Git, runs unit tests and data quality checks (Great Expectations) on staging data, then plans/applies Terraform in staging and promotes to production with canary deployment and traffic shift. Explain IAM least-privilege, S3 artifact flow, and staging vs prod isolation?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Anthropic","Citadel","Databricks"]},{"id":"q-924","question":"Design a beginner-friendly AWS CI/CD pipeline for a serverless REST API deployed to AWS Lambda behind API Gateway. Source from GitHub; CodeBuild runs unit tests with pytest and a basic security scan with bandit; artifacts stored in S3; deployment uses SAM/CFN to Lambda with separate stages dev/stage/prod and a canary traffic shift via Lambda aliases. Explain IAM least-privilege and secure env vars (Secrets Manager or SSM)?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Microsoft","Square","Uber"]},{"id":"q-998","question":"In a 3-account AWS setup (Dev, SecProd, Prod), you need a Git-driven CI/CD that builds a container image in Dev, promotes IaC and app config via Terraform, and signals canary tests in Prod before full rollout. Explain cross-account CodePipeline stages, IAM roles with least privilege, Secrets Manager rotation, and canary deployment with rollback triggers. Include artifact flow and auditing considerations?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Apple","Databricks"]},{"id":"q-1025","question":"In a multi-account AWS DVA data platform, streaming IoT telemetry into Kinesis Data Firehose feeding an S3 data lake with Glue catalog. Data schemas evolve and backfills are needed without full reprocessing. Design an end-to-end approach for schema evolution, idempotent writes, and backfill using Glue Schema Registry, Iceberg on S3, and partition pruning. Include data validation, DLQ, and observability?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Goldman Sachs","Instacart","Snap"]},{"id":"q-1268","question":"Design an AWS data lake pattern for multi-tenant analytics where each tenant's data sits under /tenants/{tenantId} in S3 and is exposed to Athena and QuickSight. How would you implement strict tenant isolation, least-privilege access, and automated policy-driven discovery and auditing using Lake Formation, IAM, CMKs, and SCPs? Include governance, testing, and performance considerations?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Discord","Oracle","Salesforce"]},{"id":"q-1381","question":"A serverless data ingestion pipeline: an S3 PUT triggers a Lambda that transforms JSON logs into CSV and writes to a separate bucket; failed records go to a DLQ. Explain how you implement idempotent writes, choose between DLQ mechanisms (Lambda DLQ vs SQS), and set up minimal monitoring/alerts to catch processing failures?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Discord","OpenAI"]},{"id":"q-1436","question":"In a beginner AWS DVA workflow, JSON logs arrive to S3 at s3://data-logs/raw/. Propose a minimal pipeline where a Lambda validates each record against a JSON schema, writes valid records as Parquet to s3://data-logs/processed/YYYY/MM/DD/, and routes invalid ones to a DLQ. Explain idempotent writes, choose between Lambda DLQ vs SQS, and basic monitoring setup?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["LinkedIn","Meta","Salesforce"]},{"id":"q-1549","question":"Design a secure, scalable cross-account analytics pattern for a multi-tenant data lake. Data for tenants live at /tenants/{tenantId}/ in S3 and must be queryable via Athena/QuickSight with strict isolation. Explain how Lake Formation, per-tenant LF permissions, and cross-account IAM roles control access from a central analytics account. Include encryption (CMKs), cross-account RAM/trust, schema evolution handling, and a testing plan for isolation and governance?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["LinkedIn","Meta","Slack"]},{"id":"q-1575","question":"In a multi-tenant data platform on AWS, tenants stream JSON events into a single Kinesis Data Stream; you aggregate into per-tenant Parquet files in S3 using Firehose. Design an end-to-end pipeline with idempotence, schema validation, and auditing. Include DLQ handling and isolation via Lake Formation. What are your concrete steps?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Hugging Face","Lyft","Salesforce"]},{"id":"q-1617","question":"Design a per-tenant data lake access system on AWS where billions of Parquet files in S3 are consumed by Athena/Glue; describe how you would implement tenant isolation, masking, and row-level security using Lake Formation, and how you would validate auditing and performance under burst workloads?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Discord","IBM","Robinhood"]},{"id":"q-1637","question":"In a beginner AWS DVA ingestion pipeline, a CSV file uploads to s3://data/tenant-{tenantId}/uploads/YYYY/MM/DD/file.csv triggers a Lambda that validates the header has exactly [timestamp, tenant_id, metric], converts to Parquet, and writes to s3://data/tenant-{tenantId}/processed/YYYY/MM/DD/file.parquet; design for idempotent writes (no duplicates), choose a DLQ strategy, and add minimal CloudWatch alarms for failures. How would you implement this end-to-end, and why?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Adobe","LinkedIn","Salesforce"]},{"id":"q-1706","question":"Design a beginner-friendly, end-to-end data quality check for a daily Parquet dataset stored in S3: s3://telemetry/processed/YYYY/MM/DD/. The data is produced by a Glue job from JSON input. Propose a minimal workflow (using Lambda, Glue, or Athena) that validates schema conformance, computes a day-over-day row-count delta, and emits a quality score JSON to s3://telemetry/quality-reports/YYYY/MM/DD/. Include how you would trigger, idempotency, and basic monitoring?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Amazon","Discord","DoorDash"]},{"id":"q-1772","question":"In a multi-account AWS data lake, ingested data lands in s3://lake/raw from several tenants via Kinesis Firehose into a shared account. Propose an end-to-end DVA pipeline that enforces per-tenant isolation, uses Lake Formation for access control, handles schema evolution with Parquet, and provides tenant-aware lineage and auditing. Include how you test isolation and how you monitor for cross-tenant data leakage?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Discord","Google","Snowflake"]},{"id":"q-1801","question":"Ingest JSON events from multiple payment rails into a single streaming layer, partitioned by exchange, and store per-exchange Parquet data in an Iceberg-backed table on S3. Describe the end-to-end design focusing on idempotent writes, late-arriving data, schema evolution, and auditability with time travel. Include concrete steps and trade-offs between Iceberg vs Glue Catalog?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Coinbase","Square","Stripe"]},{"id":"q-1821","question":"In a real-world data platform on AWS, streaming JSON events from many producers land in a single Kinesis Data Stream and are ingested into per-tenant Parquet files in S3. Over time the schema evolves and late data arrives. Describe an end-to-end approach that ensures idempotent writes, supports schema evolution, isolates tenants with Lake Formation, and provides reliable auditing and monitoring. Include specific services, data formats, and trade-offs?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Apple","Databricks","Goldman Sachs"]},{"id":"q-1881","question":"Design an advanced cross-region data ingestion and governance pattern on AWS for a high-volume streaming platform. In us-east-1 raw JSON events arrive via Kinesis Firehose into S3, then replicate to us-west-2 with minimal latency. Propose an architecture that guarantees exactly-once ingestion, supports schema evolution via Iceberg, and enforces per-tenant isolation with Lake Formation. Explain idempotent writes, cross-region replication, date-based partitioning, and robust monitoring (lag, drift, backfills) with minimal tenant impact?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["MongoDB","NVIDIA"]},{"id":"q-1925","question":"You're building a beginner AWS DVA pipeline: 2,000 devices emit JSON telemetry to S3 as daily JSONL; a Lambda validates lines and writes valid records as Parquet to a partitioned dataset, while invalid lines go to a DLQ. Propose a minimal approach to ensure idempotent processing, schema evolution, and late-data handling, with concrete services, data formats, and a simple monitoring plan?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Apple","Meta"]},{"id":"q-2001","question":"Design a cross-account, multi-region ingestion pipeline: devices in Account A stream telemetry via Kinesis to a central data lake in Account B. Use Firehose to write Parquet to S3, Glue Catalog for schema, and Lake Formation for access control. Ensure idempotent writes with a dedup key and conditional Put, support schema evolution via Glue Schema Registry, handle late data with watermarks, and implement observability with CloudWatch metrics and a DLQ?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["LinkedIn","Lyft","Stripe"]},{"id":"q-2064","question":"In a cross-region telemetry pipeline for millions of devices, ensure exactly-once processing, schema evolution with optional fields, and late-arriving data backfill within 24 hours. Propose a concrete architecture using AWS DVA primitives (Kinesis Data Streams, Lambda/Fargate, S3, Glue, Athena, DynamoDB), describe idempotent writes, partitioning, late data handling, monitoring, and failure plans. Include trade-offs?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Meta","Netflix","Zoom"]},{"id":"q-2113","question":"Design a minimal AWS DVA ingestion for 2,000 devices publishing JSON to a Kinesis Data Stream. Implement a Lambda dedupe layer using DynamoDB (keyed by deviceId and eventId), then write validated records as Parquet to S3 partitioned by date/deviceId. Use Glue Schema Registry for optional fields and versioning. Backfill late data within 24 hours; include a basic monitoring plan and DLQ for invalid records?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Google","Robinhood","Slack"]},{"id":"q-2202","question":"Design a real-time payments fraud detector handling 100k events/sec peak with Kinesis Data Streams. End-to-end latency <200 ms, exactly-once processing, and cross-region DR. Data schemas evolve with optional fields; late data allowed within 10 minutes backfill. Propose concrete AWS DVA architecture using KDS, Lambda/Fargate, S3, Glue, Athena, DynamoDB; explain idempotent writes, partitioning, backpressure, monitoring, and failure plans?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Stripe","Two Sigma"]},{"id":"q-2316","question":"Beginner AWS DVA task: 2,000 devices send telemetry as JSON lines to a Kinesis Data Stream, consumed by a Lambda that writes Parquet to S3 and uses a DynamoDB table for dedupe. A burst from one device creates hot shards and latency. Propose concrete steps to identify/mitigate shard hotspots, adjust partition keys, tune Lambda concurrency, implement idempotent writes, preserve schema evolution, and plan late-data backfill within 24 hours. Include basic monitoring?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Databricks","Google","Uber"]},{"id":"q-2336","question":"Beginner AWS DVA task: 2,000 devices emit JSON telemetry to a Kinesis Data Stream. Build a minimal pipeline that (a) validates and redacts PII fields (e.g., userId, deviceId) in real time, (b) writes sanitized records to S3 as Parquet partitioned by date and region, (c) preserves optional fields for schema evolution, and (d) routes and logs any failed records. Propose concrete services, data formats, and a simple monitoring plan?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Coinbase","Hugging Face","Netflix"]},{"id":"q-2412","question":"**Beginner** AWS DVA task: 10k devices emit telemetry as JSON to Kinesis in Account A. Propose a minimal cross-account pipeline (A->B) using KDS, Lambda/Fargate, S3, Glue, Athena, DynamoDB that validates schema, deduplicates, enriches with device metadata, and writes partitioned Parquet to S3. Include data lineage, cross-account access controls, late-arrival handling within 48 hours, and a simple monitoring plan. Outline trade-offs?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Amazon","Google","PayPal"]},{"id":"q-2716","question":"Beginner AWS DVA task: 2,000 devices emit telemetry as JSON lines to a Kinesis Data Stream. Build a minimal pipeline with Lambda to validate and redact PII, and to write Parquet to S3 partitioned by region and date. Use DynamoDB for dedupe (deviceId+seq). Plan schema evolution for optional firmware.version and geo, and a 7-day backfill. Include monitoring and cost considerations?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Snap","Twitter","Two Sigma"]},{"id":"q-2751","question":"In a beginner AWS DVA pipeline, implement end-to-end data lineage for regional telemetry. 2,000 devices emit JSON lines to Kinesis Data Streams. A Fargate task validates schema, enforces UTC timestamps, and writes Parquet to S3 partitioned by region/date. Preserve immutable raw JSON in S3 and create a DynamoDB audit table capturing event_id, shard_id, ingest_ts, process_ts, status, and a hash of applied transformations. How would you guarantee end-to-end lineage, idempotent writes, and observability, including failure handling and cost considerations?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Anthropic","NVIDIA","Two Sigma"]},{"id":"q-2788","question":"Beginner AWS DVA task: 5,000 IoT sensors publish JSON telemetry to a Kinesis Data Stream. Design a resilient pipeline that produces 1-minute windowed aggregates stored as Parquet in S3, with a Glue catalog and Athena table. Ensure idempotent writes, late data handling up to 15 minutes, and backfill capability for prior days. Include monitoring, cost considerations, and a simple failure plan?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Coinbase","DoorDash"]},{"id":"q-2856","question":"Design a multi-tenant telemetry ingestion pipeline where each tenant defines evolving JSON schemas. Propose an end-to-end AWS DVA solution that guarantees tenant isolation, supports per-tenant schema evolution, idempotent writes, PII masking, and a 24-hour late-arrival window. Include concrete services: Kinesis Data Streams, Lambda/Fargate, S3, Glue, Athena, Lake Formation, DynamoDB, and discuss trade-offs?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Lyft","Uber"]},{"id":"q-2906","question":"In a real-time telemetry pipeline for thousands of devices across regions, design a per-tenant dynamic data-correction flow: each event must be corrected according to a tenant-specific policy version stored in DynamoDB, applied deterministically in Lambda, and written to S3 as Parquet partitioned by tenant/date. Policies can change; ensure 24h replayability, idempotency, and auditable lineage using Glue/Athena. Include trade-offs?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["IBM","Microsoft","Netflix"]},{"id":"q-3054","question":"In a multi-region, multi-tenant telemetry pipeline for a mobile game with tens of millions events per second, design an AWS DVA-based architecture that guarantees per-tenant SLA, at-least-once delivery, and schema evolution. Include ingestion, processing, storage, deduplication, backfill for late data, and cross-region replication. Compare Lambda vs Fargate for ETL, and outline monitoring and failure plans. Provide concrete component interactions?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Apple","Netflix"]},{"id":"q-3150","question":"Design a secure, multi-tenant telemetry pipeline in AWS DVA with per-tenant isolation in a shared data lake. Ingest via a single Kinesis stream with tenant_id, route to S3 prefixes per tenant, and use Lake Formation + CMKs for access control. Include schema evolution, late-arriving data, and idempotent writes with a dedupe store; log changes in DynamoDB. Trade-offs: isolation granularity vs. ops complexity?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Hashicorp","LinkedIn","Square"]},{"id":"q-3176","question":"You’re building a beginner AWS DVA pipeline for telemetry from 5,000 devices across two regions. Ingest JSON Lines to region-specific Kinesis streams, validate and redact PII in Lambda, enrich with a device registry in DynamoDB (e.g., deviceAgeDays), deduplicate using deviceId+seq, and store Parquet data in S3 partitioned by region/date. Add 2-day backfill; expose simple lineage in a DynamoDB table; outline partition keys, data formats, monitoring, and cost implications?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Discord","NVIDIA","Uber"]},{"id":"q-3192","question":"In a real-time telemetry pipeline handling millions of events per minute for multi-tenant platforms (Airbnb-like and Instacart-like tenants), design an end-to-end solution that enables reproducible analytics via Glue Data Catalog versioning. Describe catalog versioning, per-record lineage (recordId, tenantId, ingestTime, catalogVersion), schema evolution with optional fields, late-arriving data handling, and queries tied to a catalog version. Use concrete AWS DVA primitives and trade-offs?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Airbnb","Instacart"]},{"id":"q-3242","question":"Beginner AWS DVA task: 1,500 devices emit JSON events to a regional Kinesis Data Stream. Build a pipeline using Lambda to validate against a versioned Glue Schema Registry, redact PII, and write Parquet to S3 partitioned by region/date. Use DynamoDB for dedupe (deviceId+seq). Introduce schema versioning for backward compatibility and plan a 7-day backfill. Include monitoring, cost considerations, and test scenarios (invalid schema, duplicates, late data)?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Hashicorp","Slack","Uber"]},{"id":"q-3281","question":"Design a beginner-level real-time telemetry pipeline across two AWS regions where 1,000 devices emit JSON lines to region-specific Kinesis streams. Implement end-to-end observability by instrumenting Lambda steps with OpenTelemetry, capture per-record processing latency, and surface a 5-minute SLA breach alert in CloudWatch. Explain how you'd wire traces, logs, metrics, and dashboards, plus cost/complexity trade-offs?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Meta","Snowflake","Zoom"]},{"id":"q-3293","question":"In a beginner AWS DVA telemetry pipeline, 5k devices across two regions emit JSON events to regional Kinesis streams. Build a minimal end-to-end pipeline that (1) validates against a versioned schema, (2) computes a 0–1 data-quality score using required fields and value ranges, (3) routes score <0.8 to a DLQ, (4) writes valid records to S3 Parquet partitioned by region/date, and (5) updates DynamoDB with per-device quality. Backfill 24 hours via replay. Observability via CloudWatch and X-Ray?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Coinbase","Meta","Microsoft"]},{"id":"q-3389","question":"Design a cross-tenant data lineage and governance pipeline on AWS DVA. Ingest trillions of app events across regions, with evolving JSON schemas and strict per-tenant isolation. Describe an end-to-end architecture using Kinesis Data Streams, Lambda/Fargate, S3 with Iceberg/Parquet, Glue, Athena, Lake Formation, and DynamoDB, plus a lineage map. Include schema drift detection, late data handling, access controls, and failure plans?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Coinbase","LinkedIn","Netflix"]},{"id":"q-3435","question":"Design a global telemetry pipeline with data residency rules: APAC streams stay in APAC, with optional cross-region replication only where allowed; guarantee exactly-once processing; support per-tenant schema evolution; enforce retention/legal hold; and mask PII in real-time. Propose concrete AWS DVA architecture and discuss idempotent writes, cross-region replication, watermarking, failure modes, and monitoring?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Snap","Tesla"]},{"id":"q-3603","question":"**Multi-Tenancy Telemetry Isolation**: You’re ingesting telemetry from 100 tenants in two regions via a Kinesis Data Stream. Each event contains tenant_id, device_id, and metrics. Design a tenancy-aware pipeline using AWS DVA primitives (Kinesis, Lambda/Fargate, S3, Glue, Athena, DynamoDB) to isolate tenants, enforce access control, and handle schema drift. Address partitioning by tenant_id, per-tenant retention, idempotent writes, schema evolution, late data up to 48 hours, and robust monitoring. What concrete architecture would you implement?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Google","Tesla"]},{"id":"q-3690","question":"Design a compliant, multi-tenant telemetry pipeline on AWS DVA that isolates tenants in S3 prefixes, uses per-tenant KMS CMKs, and supports late-arriving data for 24 hours. A single Kinesis stream carries events with a tenantId field; explain routing, access controls, audit, and failure handling using Kinesis, Lambda/Fargate, S3, Glue, Athena, DynamoDB. Include trade-offs?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Airbnb","Citadel","Tesla"]},{"id":"q-3808","question":"In a real-time telemetry pipeline, millions of devices stream events to a central Kinesis Data Stream. Design an observability-first architecture using AWS DVA primitives (Kinesis, Lambda, Fargate, S3, Glue, Athena) and OpenTelemetry. Requirements: end-to-end tracing across Lambda/Fargate, trace propagation, sampling with low overhead, and a central trace sink (X-Ray or OpenTelemetry Collector + Jaeger). Include how to handle late data, correlation IDs, and cost?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Bloomberg","Hugging Face","Stripe"]},{"id":"q-3867","question":"Design a multi-tenant telemetry pipeline across 3 AWS accounts/regions using AWS DVA primitives (Kinesis Data Streams, Lambda/Fargate, S3, Glue, Athena, DynamoDB) that enforces per-tenant rate limits, supports exact-once processing, and handles schema evolution with optional fields. Include late-data handling, PII masking before storage, and cost-conscious backpressure. Describe partitioning, idempotency, observability, and failure plans?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Google","NVIDIA","Twitter"]},{"id":"q-3882","question":"In a multi-region telemetry ingestion pipeline, devices in three regions emit JSON events to region-local Kinesis streams. Design a beginner-friendly data-lineage approach that records a lineageId at ingest, stores provenance in DynamoDB, and writes Parquet files to S3 with region/date partitions, ensuring lineage is traceable across stages. Describe how to backfill the last 24 hours and how you would validate lineage integrity during queries in Athena. Include a simple monitoring plan?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Airbnb","Cloudflare","LinkedIn"]},{"id":"q-3896","question":"Design an advanced cross-region multi-tenant telemetry pipeline with per-tenant isolation, exactly-once processing, and purge requests within 48 hours. Use AWS DVA primitives: Kinesis Data Streams, Lambda/Fargate, S3, Glue, Athena, DynamoDB, plus OpenTelemetry. Explain tenant-scoped partitioning, idempotent writes, late data handling, audit trails, and cost controls. How would you implement it?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Apple","Stripe","Two Sigma"]},{"id":"q-3969","question":"In a multi-region telemetry pipeline for a multi-tenant SaaS platform, implement dynamic per-tenant sampling caps to cap costs while still writing critical events exactly once. Use AWS DVA primitives: Kinesis Data Streams, Lambda/Fargate, S3, Glue, Athena, DynamoDB, plus OpenTelemetry. Explain how quotas are enforced, how backpressure is managed during spikes, and how you maintain per-tenant isolation and auditability?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Coinbase","LinkedIn","Oracle"]},{"id":"q-3982","question":"In a Tesla-like fleet telemetry pipeline, implement end-to-end distributed tracing with OpenTelemetry from device to Kinesis, Lambda/Fargate, and central analytics. Describe trace propagation in records, sampling strategy, idempotent writes, tenant isolation, and failure observability, with concrete trade-offs?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Airbnb","Tesla"]},{"id":"q-4064","question":"Architect a cross-region, multi-tenant telemetry pipeline that isolates tenants at the data plane, achieves exactly-once ingestion, and supports schema evolution. Use AWS DVA primitives (Kinesis, Lambda/Fargate, S3, Glue, Athena, DynamoDB) and OpenTelemetry. Include data-at-rest/in-transit encryption, cross-region replication, late data handling, and cost controls; compare centralized vs tenant-sharded consumption, and show failure modes and observability plan?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["MongoDB","Oracle","Slack"]},{"id":"q-4129","question":"Design a cross-region telemetry ingestion path that guarantees per-tenant data residency, complete immutable audit trails, and on-demand data replay for incidents. Use AWS DVA primitives: Kinesis Data Streams, Lambda/Fargate, S3, Glue, Athena, DynamoDB, plus OpenTelemetry. Explain tenant-scoped partition keys, data immutability with S3 Object Lock, cross-region replication, and how you would implement on-demand replay without breaking exactly-once semantics?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Coinbase","NVIDIA"]},{"id":"q-4176","question":"Beginner AWS DVA task: design a two-tenant telemetry pipeline where each tenant's data lands in a separate S3 prefix and DynamoDB table; implement per-tenant IAM roles to restrict access; specify data schema choices, deduplication strategy, and a simple 7-day backfill plan. Use Kinesis, Lambda, S3, Glue, Athena, and DynamoDB. How would you implement tenant isolation and governance end-to-end?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Discord","Robinhood","Salesforce"]},{"id":"q-4283","question":"Design a cross-region telemetry pipeline that enforces per-tenant data contracts, detects schema drift at ingestion, and supports automated backfills with traceable lineage. Use AWS DVA primitives (Kinesis Data Streams, Lambda/Fargate, S3, Glue, Athena, DynamoDB, Lake Formation) and include versioned contracts, drift gates, and rollback/repair strategies?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Microsoft","Stripe"]},{"id":"q-4464","question":"Design a privacy-preserving, multi-tenant telemetry pipeline on AWS DVA where each tenant's events are isolated by S3 prefixes and per-tenant DynamoDB tables, with OpenTelemetry trace context propagated across Lambda/Fargate, and aggregated reads allowed only in Athena for approved tenants. Include schema evolution, idempotent writes, audit trails, cross-region considerations, and cost controls?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Hugging Face","MongoDB"]},{"id":"q-4656","question":"Beginner AWS DVA task: 2,000 devices stream JSON events to a regional Kinesis Data Stream. Build a minimal pipeline that validates the JSON, computes a data quality score per record, routes valid records to S3 Parquet partitioned by region/date, and sends invalid records to a DLQ. Use DynamoDB for dedupe (deviceId+ts). Plan for optional fields and a 7-day backfill. Include basic observability and cost considerations?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Microsoft","MongoDB","Uber"]},{"id":"q-4682","question":"How would you build a privacy-preserving, cross-region telemetry ingestion pipeline that masks PII at ingestion, supports tenant-specific retention and immutable audit logs, and still allows analytics queries? Use AWS DVA primitives: Kinesis Data Streams, Lambda/Fargate, S3, Glue, Athena, DynamoDB, plus OpenTelemetry. Describe per-tenant masking policies, policy storage, audit trail integrity, and retention enforcement without breaking analytics?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["NVIDIA","Tesla","Two Sigma"]},{"id":"q-4711","question":"Design an ingestion path for multi-tenant telemetry that captures end-to-end data lineage for every event (source, transforms, load) with immutable provenance, queryable in Athena. Use AWS DVA primitives: Kinesis Data Streams, Lambda/Fargate, S3, Glue, DynamoDB, and OpenTelemetry. How would you model lineage as a graph, store provenance, handle schema evolution, and support backfill without breaking processing guarantees?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Salesforce","Slack"]},{"id":"q-4746","question":"Design an ingestion path for multi-tenant telemetry where an in-flight data quality gate validates schema and field types, enforces value ranges, and flags anomalies with a per-tenant score. Valid events pass to per-tenant Parquet in S3; invalid go to quarantine with alerts. Guarantee exactly-once semantics across retries and enable backfill within 48 hours without duplicates. Use AWS DVA primitives: Kinesis Data Streams, Lambda/Fargate, S3, Glue, Athena, DynamoDB, and OpenTelemetry. How would you implement per-tenant validation schemas, idempotent writes, and monitoring?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Anthropic","Discord","Oracle"]},{"id":"q-674","question":"You're building an AWS DVA data-analytics pipeline ingesting telemetry from 2000 devices/sec via Kinesis Data Streams. A Spark/Glue path processes windows and writes Parquet to S3; aggregation state in DynamoDB. Data loss or duplicates occur during retries and outages; costs spike at peak. Design a resilient, cost-efficient approach: shard sizing, processing path, idempotent writes, error handling with DLQ, and observability. What would you implement and why?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Microsoft","OpenAI"]},{"id":"q-916","question":"In an AWS-based DVA pipeline ingesting telemetry from 2000 devices/sec via Kinesis Data Streams, with a Spark/Glue path writing Parquet to S3 and cataloged in Glue, design an end-to-end strategy for robust schema evolution, data validation, and partitioning that minimizes reprocessing and supports backfills. Include schema registry, idempotence, DLQ, and observability?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["MongoDB","Snowflake","Tesla"]},{"id":"q-1228","question":"Design a drift-aware continuous training and multi-region deployment workflow for a fraud-detection model, using SageMaker Model Monitor, Pipelines, and Model Registry. Explain how you detect data and feature drift (PSI/KS against baselines), retrain triggers, versioning, canary validation, rollback, and how cross-region consistency is maintained?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Amazon","Bloomberg","Square"]},{"id":"q-1297","question":"You're deploying a multilingual sentiment-analysis model for a global customer-support chatbot. To minimize downtime when updating language adapters, design a SageMaker-based deployment with per-language variants, Model Registry, and canary rollouts that preserve latency SLAs and isolate traffic. Describe autoscaling, traffic routing, validation, and rollback criteria with concrete values?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Amazon","Google"]},{"id":"q-1324","question":"In a two-region SageMaker real-time inference setup for fraud detection, data drift is likely between regions and latency targets are strict. Outline a concrete canary deployment with per-region endpoint configs, Drift Detection thresholds, and Feature Store versioning/replication. Include traffic split, rollback criteria, validation plan, and monitoring strategy?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Cloudflare","LinkedIn","Twitter"]},{"id":"q-1378","question":"You’re building a SageMaker ML workflow that validates incoming data in a processing step before training. Design a minimal Processing Job using Python to check (i) all required features exist, (ii) numeric columns have ≤5% missing values, (iii) categoricals are within allowed sets. How would you trigger it from a SageMaker Pipeline, store results, and surface metrics? Include concrete resource choices?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Adobe","Instacart"]},{"id":"q-1457","question":"You're building a real-time risk-scoring model for a multi-region e-commerce platform. The model consumes streaming events, uses SageMaker Feature Store for features, and is deployed as a real-time endpoint with cross-region routing. Describe a concrete end-to-end deployment and monitoring design that ensures deterministic latency, supports feature versioning, detects data drift, and handles canary rollouts with rollback triggers; include governance, cost controls, and testing strategy?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Amazon","MongoDB"]},{"id":"q-1564","question":"You're deploying a global churn-prediction model for a SaaS app that requires real-time scoring in-app and nightly analytics reports, while complying with data residency rules. Propose an end-to-end AWS pattern using SageMaker real-time endpoints for live inference, Batch Transform for nightly analytics, per-region Feature Store isolation, and a governance framework with Model Registry versioning, drift detection, automated rollback, and cost controls. Include testing and validation steps?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Amazon","Google"]},{"id":"q-1572","question":"Design a multi-tenant, per-tenant inference service on SageMaker for a financial risk model where each client has isolated data, separate feature store namespace, and per-tenant model version, yet share a common endpoint. Describe the architecture, how you isolate data and billing, how you route requests by tenant_id, how you handle feature/version drift, and how you implement canary rollouts and rollback?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Coinbase","Hugging Face","Tesla"]},{"id":"q-1618","question":"You're deploying a predictive maintenance model to 10,000 industrial edge devices using SageMaker Edge Manager. Outline a phased OTA rollout with canary groups, offline devices, and automated rollback triggers based on telemetry. Specify packaging and signing process, versioned Edge Manifest in S3, device-grouping strategy, and how you monitor model accuracy, latency, and update success; discuss cost controls and governance?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Adobe","IBM"]},{"id":"q-1690","question":"You're deploying a SageMaker real-time endpoint for a financial risk model that ingests customer PII from EU and US users. Propose a compliant deployment pattern that enforces data locality (EU data stays EU), supports active-active regional endpoints, and provides GA-ready drift and privacy controls. Include resource layout, data flow, encryption, IAM/KMS, auditing, canaries, and cost controls?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Robinhood","Slack"]},{"id":"q-1725","question":"Design an end-to-end, multi-account, multi-region real-time fraud-detection pipeline on AWS. The model is deployed as SageMaker endpoints in two regions with cross-region routing. Provide concrete choices for endpoint configuration, SageMaker Feature Store versioning, canary rollout, drift detection, monitoring, autoscaling, governance, and cost controls; include validation steps and rollback criteria?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Amazon","Salesforce","Square"]},{"id":"q-1761","question":"You're building a privacy-preserving, multi-tenant real-time inference platform on AWS where each tenant's data must stay isolated (data locality + encryption) and costs are allocated per tenant. Propose an architecture using SageMaker Endpoints behind PrivateLink, per-tenant Feature Store versions, and a tenant-scoped Model Registry with canary rollouts. Explain how you validate latency, monitor drift, trigger retraining via SageMaker Pipelines, and enforce governance and per-tenant cost controls?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Google","Meta","Stripe"]},{"id":"q-1868","question":"Design a hybrid on-prem plus AWS inference workflow for a regulated financial service where customer data must never leave the on-prem site, but model updates are deployed from SageMaker. Propose an architecture using SageMaker Edge Manager for edge endpoints, PrivateLink to AWS backends, and a regulated Model Registry with per-tenant access controls. Include latency targets, canary rollouts to edge devices, drift detection, retraining triggers, and auditability?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Microsoft","Square","Uber"]},{"id":"q-1887","question":"Design a beginner-friendly SageMaker multi-model endpoint setup that serves two small text classifiers from a single endpoint. Route requests by a tenant_id included in the JSON input, ensuring models load on demand, monitor latency with CloudWatch, and implement a simple canary switch to compare model A vs B for a subset of tenants before full rollout. Include basic file structure, IAM roles, and a minimal test plan?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Bloomberg","Discord","IBM"]},{"id":"q-1913","question":"You’re deploying a beginner-friendly real-time sentiment moderation model for a global social app on SageMaker. End-user data must stay in one region and be routed through PrivateLink. Propose a concrete deployment: a single SageMaker endpoint behind PrivateLink, basic drift and bias checks with SageMaker Clarify, and a versioned Feature Store for user interactions, plus a simple canary and rollback plan. Include latency targets, retraining triggers, and governance basics?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Bloomberg","Snap","Uber"]},{"id":"q-2039","question":"You're deploying a beginner-friendly text classification service with SageMaker Serverless Inference for a low-traffic social app. Data must remain in-region, and endpoint credentials must rotate every 90 days. Propose a concrete setup: packaging and artifact storage, a single variant serverless endpoint, monthly drift-driven retraining triggers, a rollback plan, and basic monitoring/alerts for latency and errors?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Cloudflare","Twitter"]},{"id":"q-2052","question":"Design an automated rollout/rollback strategy for a multi-tenant real-time SageMaker inference platform with per-tenant Feature Store variants and cross-region canaries. How would you implement retraining triggers, drift validation, and per-tenant cost controls? Include concrete thresholds, duration, and rollback criteria?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Goldman Sachs","Meta","Snowflake"]},{"id":"q-2078","question":"Design a multi-tenant, privacy-preserving inference path on AWS that returns per-prediction explanations without leaking tenant data. Propose using SageMaker Endpoints behind PrivateLink, SageMaker Clarify explainability, per-tenant Feature Store versions, and a tenant-scoped Model Registry with canaries. Include drift detection, retraining triggers, and auditability; specify latency targets and rollback criteria?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["IBM","Plaid","Two Sigma"]},{"id":"q-2096","question":"Design an end-to-end deployment for a real-time anomaly detection pipeline across three data-residency regions with strict data sovereignty. Raw data must stay on-prem; only aggregated signals may traverse to AWS. Propose an architecture using SageMaker Endpoints in each region, PrivateLink to on-prem data sources, and a Global data-plane that routes latency-critical inferences. Include canary rollouts, drift detection, automated retraining, and rollback criteria?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Google","Meta","Twitter"]},{"id":"q-2151","question":"Design a data-quality and feature-drift monitoring plan for a real-time fraud-detection inference service deployed as SageMaker Endpoints across four Regions using PrivateLink. Outline detection of shifts in feature distributions before inference, retraining triggers via SageMaker Pipelines, and canary rollouts with rollback criteria. Include data provenance, access control, and governance?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Instacart","MongoDB","Zoom"]},{"id":"q-2243","question":"You're deploying a beginner-friendly real-time image classifier on a SageMaker Endpoint in a VPC with PrivateLink to a private data lake. Requirement: average latency <= 200 ms at 100 req/s, data never leaves the VPC, cost under $30/day. Propose endpoint sizing, autoscaling policy (min, max, metric, target, cooldown), a canary rollout, and a validation plan (latency samples, error rate, PrivateLink verification)?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Airbnb","Anthropic","MongoDB"]},{"id":"q-2257","question":"You're building a beginner-friendly NLP inference path that runs on edge devices with intermittent connectivity. Data never leaves the device in raw form; when online, anonymized summaries are sent to a cloud endpoint for improvement. Propose architecture using **SageMaker Edge Manager**, a **PrivateLink**-backed cloud channel, and a two-version model registry with a canary rollout. Include latency targets and a basic retraining trigger?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Google","Meta","Oracle"]},{"id":"q-2296","question":"You're running a real-time credit-risk inference across two regulated regions with strict data residency. Propose an architecture using SageMaker Endpoints behind PrivateLink, per-tenant data isolation, SageMaker Clarify for per-prediction explanations, and Model Monitor drift alerts. Outline a canary rollout for a feature, automated rollback on drift, and a practical validation plan including latency targets, explainability latency, and audit logging?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Microsoft","PayPal","Two Sigma"]},{"id":"q-2392","question":"You're evaluating two beginner-friendly, built-in classifiers on a small tabular dataset stored in S3. Propose an end-to-end workflow: data split, training with cost-conscious instance types, evaluation plan, model registry, and a private real-time endpoint behind a VPC/PrivateLink. Include a simple A/B testing plan and basic monitoring?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Discord","Lyft"]},{"id":"q-2431","question":"Design a real-time sentiment and intent inference service for a multi-tenant SaaS chat platform with strict data residency: raw data must stay in each region; aggregated signals may be shared for model improvement. Propose SageMaker Endpoints behind PrivateLink, per-tenant isolation via Feature Store, drift detection with automated retraining, and a canary rollout. Include latency targets, explainability, audit logging, and cost?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Adobe","Square"]},{"id":"q-2531","question":"You're deploying an edge-first anomaly-detection model on factory telemetry with intermittent connectivity. Design a SageMaker Edge Manager OTA rollout: per-device versioning, canary progression 5% → 20% → full, and rollback criteria based on latency and drift; support offline fallback to last-good model; ensure full audit logging. What would your concrete plan look like?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Amazon","Discord","Google"]},{"id":"q-2583","question":"You're deploying a real-time fraud-detection SageMaker Endpoint for a multi-tenant fintech app. Each tenant's data must be de-identified before inference and never logged with tenant identifiers. Propose an end-to-end architecture using PrivateLink to isolate endpoints, per-tenant KMS keys for envelope encryption, a tenant-scoped Feature Store, and a guarded model registry. Include canary rollouts, drift detection, rollback criteria, and latency targets (<=150 ms at 500 rps)?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Netflix","Plaid","Twitter"]},{"id":"q-2702","question":"You're deploying a multi-tenant real-time moderation inference service for user-generated content across regions, with per-tenant quotas, data residency, and cost controls. Propose a architecture using SageMaker Endpoints behind PrivateLink, per-tenant model versions, and a feature store for inputs; ensure region routing and tenancy isolation. Include canary rollout strategy, rollback criteria, and tenant-specific auditing?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Cloudflare","Discord"]},{"id":"q-2847","question":"You're designing a multi-tenant, regulator-friendly ML platform on AWS. Propose a production-ready inference architecture where each tenant has isolated data, per-tenant feature schemas, and tenant-aware explainability. Detail the endpoints, data routing, per-tenant Feature Store, PrivateLink access, model registry, drift/quality monitoring, canary rollout, and rollback criteria?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Databricks","Two Sigma"]},{"id":"q-2914","question":"You're building a multilingual real-time inference service for regulated customer data across regions. Data residency requires per-tenant data never leaves its country, yet models are shared. Propose an architecture using SageMaker Endpoints (Multi-Model Endpoint) behind PrivateLink, per-tenant packaging in a shared registry, and per-tenant drift alerts with automatic rollback. Include canary rollout, latency targets, and cost controls, plus per-tenant explainability outputs?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Airbnb","Apple","Twitter"]},{"id":"q-3057","question":"In a multi-tenant real-time fraud-detection service for digital payments, every tenant requires strict data isolation and per-tenant pricing. Design an AWS solution using SageMaker Endpoints behind PrivateLink, per-tenant SageMaker Feature Store, and a centralized Model Registry with tenant-scoped access. Include latency targets, canary rollouts, drift monitoring with automated retraining triggers, and per-tenant autoscaling/cost controls. How would you ensure data governance and auditability?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["PayPal","Snowflake"]},{"id":"q-3075","question":"You're deploying a beginner-friendly sentiment-analysis model for customer reviews in a private AWS environment. Training data resides in a cross-account private S3 bucket; access is via a cross-account role and PrivateLink to a data lake. Propose a minimal pipeline: preprocessing in SageMaker Data Wrangler, a training job (logistic regression or built-in algorithm), host as a SageMaker endpoint with endpoint autoscaling (min 1, max 3), and a basic validation plan (accuracy, precision/recall, latency under 300 ms for 50 req/s). Ensure data never leaves VPC and cost under $15/day?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Apple","Citadel","Snowflake"]},{"id":"q-3148","question":"Design a scalable, privacy-preserving real-time inference path for a multilingual sentiment model deployed on SageMaker, where multiple tenants share a single endpoint but tenant data must remain logically isolated and billed per-tenant. Outline architecture (VPC, PrivateLink, API Gateway, per-tenant tokenization, and endpoint autoscaling), tenant quotas, canary rollout, encryption, and drift monitoring?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Snap","Snowflake"]},{"id":"q-3320","question":"You're building an edge-first, cross-region inference platform for an automotive telemetry company. Ground stations run on-device models with low latency; the central AWS control plane maintains a private SageMaker model registry and uses PrivateLink to gated data sources. Propose an architecture that (1) guarantees on-device inference latency targets, (2) prevents data leaving restricted networks, (3) supports canary model updates per gateway, (4) implements drift detection and automated rollback, and (5) provides auditability and per-tenant access controls. Include a sample rollout plan and validation steps?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Apple","Snowflake","Tesla"]},{"id":"q-3352","question":"You're building a beginner-friendly real-time sentiment classifier for product reviews in a SaaS dashboard. Data resides in an S3 bucket in a single region and must not leave the VPC; use SageMaker with PrivateLink to a private data lake. Propose: (1) training approach (data prep, model choice, hyperparameters, offline evaluation), (2) endpoint deployment and autoscaling policy (min/max, target latency), (3) canary rollout steps, (4) validation plan including latency samples, throughput, accuracy, privacy checks; and a rough monthly cost estimate?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Hugging Face","Robinhood","Salesforce"]},{"id":"q-3431","question":"You're deploying a HIPAA-bound medical image classifier with SageMaker Inference Enclave. Data stays in the enclave; VPC with PrivateLink to a private data lake; latency target 250 ms at 60 req/s; budget under $40/day. Propose enclave-enabled endpoint config, autoscaling (min 2, max 8, target 250 ms), canary rollout (start 10%), and a validation plan covering enclave attestation, privacy checks, latency/throughput, and cost tracking?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Cloudflare","Discord","Netflix"]},{"id":"q-3475","question":"You're building a multi-tenant real-time recommender for a SaaS platform on AWS. Each customer must have isolated data residency, and inferences/telemetry may not cross tenant boundaries. Propose an architecture using PrivateLink/API Gateway, per-tenant SageMaker Feature Store and per-tenant Model Registry, and governance with Lake Formation and CloudTrail. How would you implement tenancy isolation, canary rollouts, drift retraining triggers, and an auditable cost model with latency targets?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Coinbase","Hashicorp","LinkedIn"]},{"id":"q-3642","question":"You're deploying a beginner-friendly real-time product ranking model for an e-commerce dashboard. Data sits in a private S3 data lake in a single region and must never leave the VPC; the endpoint must support 300 requests/sec with latency <= 120 ms. Propose: (1) training approach and offline eval; (2) SageMaker endpoint deployment inside a VPC with PrivateLink to the data lake; (3) monitoring and bias checks using SageMaker Clarify or Model Monitor; (4) a canary rollout and rollback criteria; (5) rough monthly cost estimate?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Amazon","Google","Tesla"]},{"id":"q-3889","question":"You're deploying a beginner-friendly text classifier for customer feedback on SageMaker. Data sits in S3 in a single region and must not leave AWS; access via PrivateLink to a private data lake. Propose: (1) training approach (data prep, model choice such as DistilBERT vs TF-IDF, hyperparameters), (2) endpoint autoscaling (min/max/target latency), (3) SageMaker Model Registry versioning and a canary rollout, (4) a practical drift/monitoring plan, and (5) rough monthly cost?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Apple","Coinbase","Two Sigma"]},{"id":"q-3941","question":"Scenario: You need a single SageMaker endpoint serving three tenants with strict data residency: raw data never leaves each tenant's VPC, and model versions are tenant-scoped. Design an architecture using SageMaker Multi-Model Endpoint + PrivateLink with a tenant-aware router. Include per-tenant autoscaling, canary rollout across regions, drift detection, and cost governance. Outline endpoint config, security, validation steps?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Discord","Hashicorp"]},{"id":"q-3964","question":"You are asked to deploy a real-time fraud detection service for three financial partners where each partner's data must stay entirely within its own VPC; data lakes are cross-account. Design architecture using SageMaker PrivateLink to partner data sources, a SageMaker Multi-Model Endpoint with per-tenant models from a centralized immutable Model Registry. Include per-tenant drift detection, Feature Store-driven retraining triggers, canary rollouts, endpoint autoscaling, rollback criteria and cost controls. Latency target: p95 <= 150 ms at 800 rps?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Goldman Sachs","Google","Snowflake"]},{"id":"q-4087","question":"You're deploying a real-time fraud-detection model across 10 regions for a multi-tenant SaaS. Each tenant's data must stay within its VPC and never be exposed to others; design a SageMaker-based multi-tenant architecture (Model Registry, Clarify, endpoints, PrivateLink), implement per-tenant canary rollouts, drift detection, retraining triggers, explainability dashboards, and an auditable data-residency/compliance log. How would you implement this and what are the key trade-offs?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Discord","Microsoft"]},{"id":"q-4143","question":"You're deploying a beginner-friendly real-time audio command classifier on SageMaker Edge/Greengrass for factory devices. Data never leaves edge devices; model updates must roll out to thousands with zero-downtime. Propose architecture, edge packaging (container vs Lambda), update strategy (canary/blue-green), drift monitoring, and a basic cost estimate?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Anthropic","Coinbase","IBM"]},{"id":"q-4200","question":"You're deploying a beginner-friendly real-time text classifier for customer support tickets inside a VPC. The model runs via SageMaker Endpoint with PrivateLink to a private data lake; latency target 150 ms at 100 req/s; budget under $25/day. Propose: model choice, quantization strategy, endpoint sizing, autoscaling, canary rollout plan, and how you would use SageMaker Clarify and drift monitoring to trigger retraining?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Meta","NVIDIA"]},{"id":"q-4232","question":"You're deploying a beginner-friendly real-time text classifier for chat moderation using SageMaker Serverless Inference with data in S3. Target latency: 700 ms at 30 req/s, budget under $4/day. Propose: model choice (TF‑IDF + Logistic Regression vs. lightweight Transformer), quantization, memory/timeout, cold-start handling, a two-stage canary (10% → 50% → 100%), and a validation plan with latency, accuracy, and cost checks?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Discord","MongoDB"]},{"id":"q-4285","question":"You're deploying a real-time fraud detection model for a multi-tenant SaaS platform. Each tenant's data must never exit its own VPC, while model artifacts and data assets reside in a shared, private data lake via PrivateLink. Propose: a multi-tenant SageMaker endpoint strategy (per-tenant routing and versioning with Multi-Model Endpoints), isolation mechanisms, canary rollout by tenant, per-tenant scaling, and drift-triggered retraining?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Lyft","MongoDB","Zoom"]},{"id":"q-4362","question":"You're deploying a real-time fraud-detection model in a regulated financial domain. Data in two S3 regions must stay in-region; use SageMaker PrivateLink to a VPC to a private data lake, with multi-tenant isolation via SageMaker Multi-Model Endpoints. Design for 1500 rps, p99 latency < 20 ms, per-tenant cost controls, and canary rollout. Include endpoint sizing, autoscaling, retraining triggers, and security/audit considerations?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Bloomberg","Citadel","Discord"]},{"id":"q-4414","question":"You're deploying a real-time, multi-tenant content moderation service on SageMaker. Inference requests flow through a regional API endpoint into a PrivateLink-backed SageMaker Multi-Model Endpoint. Data lakes are private in S3; tenancy requires isolation and A/B testing of models. Latency target: 200 ms at 150 req/s per tenant; budget under $40/day. Propose: architecture (tenancy model, VPC, keys), endpoint autoscaling (min/max/target), canary rollout across tenants, feature store integration, drift monitoring, and validation plan?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Discord","Google","Lyft"]},{"id":"q-4515","question":"You're deploying a real-time malware-detection model to enterprise endpoints using SageMaker Edge Manager. Data must never leave endpoints; updates originate in SageMaker and are delivered to devices via PrivateLink. Propose: (1) edge deployment strategy, (2) canary rollout (5% of devices) and rollback plan, (3) latency targets (avg <= 120 ms, p95 <= 250 ms) and telemetry design, (4) retraining triggers for drift or performance drop, (5) rough 30-day cloud cost estimate and monitoring setup?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Discord","IBM","Microsoft"]},{"id":"q-4547","question":"You're building a real-time inference service for a multi-tenant SaaS platform used by Adobe, Zoom, and Scale AI. Each tenant has its own VPC and data lake; data must stay within the VPC and be accessible via PrivateLink. The system must meet per-tenant latency SLAs, burst traffic, and strict cost caps. Propose: (1) architecture for per-tenant routing and model versioning (shared endpoint vs fan-out); (2) per-tenant autoscaling with QoS, including metrics, min/max, and cooldowns; (3) canary rollout strategy and rollback by tenant; (4) drift detection and retraining triggers per tenant; (5) telemetry, dashboards, and per-tenant cost allocation; (6) rough 30-day cost estimate?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Adobe","Scale Ai","Zoom"]},{"id":"q-4661","question":"You're building a beginner-friendly fraud-detection model for a fintech product. Data resides in a single-region S3 bucket. Use SageMaker Pipelines to train, register two models (base and challenger) in Model Registry, and deploy a two-model endpoint (Multi-Model Endpoint) with a 5% canary for 48 hours. Roll back if challenger accuracy drops by more than 1% relative to base or latency exceeds 300 ms; provide a practical plan including pipeline steps, metrics, rollback criteria, and a 30-day cost estimate?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Coinbase","Instacart"]},{"id":"q-4720","question":"You're deploying a cross‑account real‑time fraud-detection model for a fintech app. Inbound data must never leave Account A’s VPC, but the inference endpoint runs in Account B and is accessible via PrivateLink. Propose the architecture, latency targets (<150 ms at 250–350 rps), autoscaling (min/max), canary rollout, and an end‑to‑end validation plan including drift detection, privacy checks, and per‑tenant access controls?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Meta","Plaid","Scale Ai"]},{"id":"q-876","question":"You're deploying a SageMaker real-time endpoint for a model expected to see bursty, unpredictable traffic. Propose a concrete autoscaling setup using AWS Application Auto Scaling that keeps latency under a target while never scaling to zero. Specify min and max instances, the metric and target value (latency or invocations), the policy type, and cooldowns; discuss validation steps?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Bloomberg","Lyft"]},{"id":"q-896","question":"You run a SageMaker real-time endpoint serving a risk-scoring model for payments. After a drift alert, outline a canary deployment plan using endpoint variants and the Model Registry to shift 20% of traffic to a new version while preserving latency and safety. Describe how you automate metric validation (latency, error rate, and drift), rollback triggers, and guardrails, and how you promote a stable canary to baseline?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Bloomberg","PayPal","Robinhood"]},{"id":"q-969","question":"In a production AWS ML pipeline, you must serve multiple fraud-detection models across two regions using a SageMaker Multi-Model Endpoint (MME). Propose a concrete deployment and autoscaling strategy that keeps p95 latency under 200 ms during peak, prevents cold starts, and optimizes memory by loading only active models. Describe per-model versioning with SageMaker Model Registry, traffic routing, canary validation, rollback triggers, cost implications, and cross-region consistency?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Bloomberg","Netflix","Tesla"]},{"id":"q-1321","question":"In a multi-account, multi-VPC setup in us-west-2, VPCs A and B must reach external SaaS apps via private endpoints only, while all outbound internet egress is inspected by a centralized firewall in VPC C. Propose a beginner-friendly design using Transit Gateway, PrivateLink, and Route 53 Resolver DNS. Describe routing, DNS resolution, and observability?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["LinkedIn","Meta","Square"]},{"id":"q-1349","question":"Two AWS accounts across us-east-1 and us-west-2 host a data science platform with multiple VPCs. Design a scalable, secure network that minimizes cross-region data transfer, keeps east-west private, and supports easy onboarding of new accounts. Requirements: hub Transit Gateway across regions, centralized outbound egress via a shared firewall stack, private access to S3/data lake using PrivateLink with Private DNS, IPv6 dual-stack readiness, and automatic account onboarding. Explain architecture, routing, firewall policy model, cost controls, and observability?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["NVIDIA","Snowflake"]},{"id":"q-1433","question":"In a single AWS account, two VPCs exist: a private app VPC (no Internet gateway) and a separate VPC hosting the patch bucket. You want the private VPC to pull software updates from S3 in the same region without any public Internet access, and you must restrict access to only the vendor-patches bucket. Design and describe the steps to implement an S3 VPC Endpoint with an endpoint policy, route table changes, and any security considerations?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Coinbase","IBM","Robinhood"]},{"id":"q-1466","question":"Design a two-region hub-and-spoke network for a SaaS platform with 3 AWS accounts per region and 4 VPCs per region. They require private east-west traffic between services, deterministic private access to S3/DynamoDB via PrivateLink, centralized firewall egress, and easy onboarding of new tenants via an account factory. Propose a design using Transit Gateway, PrivateLink, and centralized firewalling; detail routing, tenancy isolation, scaling, and observability?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Instacart","Meta"]},{"id":"q-1469","question":"Design a two-region, multi-tenant SaaS network: per-tenant VPCs in Region A and Region B, all funneling to a centralized Transit Gateway and a regional Service VPC hosting API gateways and Network Firewall. Use PrivateLink for S3/DynamoDB, Route 53 private DNS for discovery, and an automated account factory for onboarding. Explain tenancy isolation, onboarding, cross-region failover, and observability?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["NVIDIA","Salesforce","Slack"]},{"id":"q-1545","question":"In two AWS accounts with two VPCs in a single region, internal services require private DNS resolution for api.internal and auth.internal without public endpoints. Design a simple cross‑account Route 53 Resolver setup using a central DNS hub VPC with inbound/outbound endpoints and forwarding rules, and outline onboarding for new VPCs?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Meta","Robinhood","Tesla"]},{"id":"q-1576","question":"In a two-region deployment (Region A and Region B) across two AWS accounts per region, tenants are isolated in VPCs attached to regional Transit Gateways. Design a DR-ready network that preserves private east–west traffic, ensures tenant isolation during regional outage, and enables automatic failover of control-plane services to the DR region. Propose TGW topology with a DR region, synchronized PrivateLink services, replicated firewall rules, and a testing/validation plan with telemetry?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Google","Instacart","LinkedIn"]},{"id":"q-1600","question":"Design a DR-ready, multi-region network for a SaaS platform with 3 tenants per region, each tenant in its own VPC attached to a regional Transit Gateway. Propose topology to keep private east-west traffic between tenants and control plane across regions, enable automatic failover of the control-plane to DR region using PrivateLink, support tenant onboarding, and specify telemetry and testing plans?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Google","Hashicorp"]},{"id":"q-1820","question":"Beginner-level: You operate a SaaS API in Account A, Region us-east-1. You must share the API with a partner in Account B using PrivateLink, with automatic DR failover to DR region us-west-2 and DNS-based failover via Route 53 private hosted zones. Design the minimal topology, specify resources (PrivateLink service, VPC endpoints, private DNS records) and a basic testing plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Anthropic","Lyft","Twitter"]},{"id":"q-1835","question":"Design a DR-ready topology for a SaaS platform with per-tenant VPCs attached to regional Transit Gateways in two AWS regions. Ensure private east-west traffic, tenant isolation during regional outage, and automatic control-plane failover to the DR region. Explain TGW topology, PrivateLink replication, DNS failover, cross-region storage replication, and telemetry-based validation. **Edge cases** considered?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Microsoft","NVIDIA"]},{"id":"q-1862","question":"Design a cross-region, multi-account SaaS network using AWS VPC Lattice for per-tenant service discovery and authorization across Region A and Region B. Tenants reside in isolated VPCs yet must privately reach a central API service in a hub region. Outline: tenant-to-service mapping with Lattice; inter-region PrivateLink; centralized firewall strategy; DR with DNS failover and telemetry; onboarding and revocation processes?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["MongoDB","Slack"]},{"id":"q-1908","question":"Scenario: A SaaS company hosts 5 tenants in 5 VPCs in Region us-east-1; all attach to a central Transit Gateway in a shared-services account. Each tenant must access a per-tenant PrivateLink API service while keeping strict east-west isolation. Design the topology, attach VPCs, implement per-tenant firewall rules, and provide telemetry to verify isolation and scalability. Include onboarding a new tenant?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Google","Scale Ai","Uber"]},{"id":"q-2158","question":"In a two-region deployment for a multi-tenant SaaS, each region houses 4 tenant VPCs attached to a regional Transit Gateway, plus a central Service VPC hosting API gateways and a Firewall fleet. Design a DR-ready topology that keeps tenant traffic private to the local region, ensures isolation during regional outages, and enables automatic control-plane failover to the DR region via PrivateLink. Include routing, PrivateLink replication, firewall policy synchronization, tenant onboarding, telemetry, and a DR validation plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Hugging Face","OpenAI","Oracle"]},{"id":"q-2240","question":"Beginner-level: You run a multi-tenant SaaS in AWS with 3 tenants, each in its own VPC within account A, region us-east-1. You want to onboard new tenants quickly using AWS RAM to share a central Security/Service VPC that hosts a Firewall, PrivateLink services, and a NAT. Design a minimal topology: how to share the central VPC, connect tenants via Transit Gateway, expose private access to shared services, enforce tenant isolation, and prepare telemetry and DR testing?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Lyft","Two Sigma"]},{"id":"q-2260","question":"Two-region SaaS with 4 tenant VPCs per region attached to regional TGWs. Design a DR-ready topology that keeps private east–west traffic during normal operations, ensures tenant isolation during regional outage, and enables automatic control-plane failover to the DR region. Specify: (a) TGW/VPC layout and route propagation, (b) PrivateLink service replication for the control plane API across regions, (c) per-tenant firewall/rules synchronization, (d) cross-region service discovery with Cloud Map and private DNS, (e) telemetry, validation, and failover testing plan, including rollback?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Microsoft","Salesforce","Tesla"]},{"id":"q-2385","question":"Design a DR-ready, two-region, multi-account AWS networking solution where each tenant sits in its own VPC connected to a regional Transit Gateway in Region A and Region B. Include an on-premises Direct Connect link via a DX Gateway, a central Analytics VPC reachable only through PrivateLink endpoints, automatic control-plane failover to DR region, and tenant onboarding. Outline TGW topology, PrivateLink replication, DX Gateway setup, observability, and DR validation?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Anthropic","Goldman Sachs","Google"]},{"id":"q-2463","question":"Design a DR-ready, two-region, multi-account AWS networking fabric where each tenant lives in its own VPC connected to regional Transit Gateway hubs. Include cross-region PrivateLink to analytics in a central VPC, IPv6 dual-stack readiness with NAT64/GNAT for IPv6-only workloads, and automated cross-region control-plane failover via Route 53 health checks and Lambda. Include DX gateway integration, observability, and a DR validation plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Google","Instacart","Lyft"]},{"id":"q-2481","question":"Beginner-level: In Account A, Region us-east-1, you host a private REST API exposed to tenants via PrivateLink. Tenants own their domain and want it mapped to the PrivateLink endpoint using Route 53 private hosted zones, with automatic DR failover to us-west-2. Design the minimal topology, cross-account sharing, and a testing plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Google","Hugging Face","Tesla"]},{"id":"q-2524","question":"You operate a two-region SaaS with 20 tenants; each tenant has a dedicated VPC in Region A and Region B attached to per-region Transit Gateways. Design a DR-ready onboarding flow where a central Onboarding Service in a Service VPC uses PrivateLink to onboard/offboard tenants in both regions, with automatic DR failover of the control plane and tenant data paths staying local. Outline TGW topology, PrivateLink replication, DX connectivity, DNS failover, telemetry, and a DR validation plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Microsoft","PayPal"]},{"id":"q-2562","question":"In a two-region SaaS deployment, each tenant owns a VPC in Region A and a mirrored VPC in Region B, both attached to their region’s Transit Gateway. Tenant onboarding is automated via an account factory. Design an end-to-end onboarding workflow that creates/attaches VPCs, configures TGW attachments and route tables, and exposes per-tenant PrivateLink endpoints to a central Analytics VPC. Include DR failover of the control plane and telemetry?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Databricks","Snowflake"]},{"id":"q-2609","question":"Two-region, multi-account SaaS: every tenant has a dedicated VPC attached to a regional Transit Gateway in Region A or Region B. Design a centralized telemetry fabric that aggregates VPC Flow Logs and firewall logs to a shared data lake via Kinesis Data Firehose, with DR failover of the telemetry stack and guaranteed data integrity?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Citadel","Netflix"]},{"id":"q-2622","question":"In a two-region, multi-account SaaS deployment, each tenant resides in its own VPC connected to a shared Transit Gateway in each region. Central Analytics is exposed as a PrivateLink endpoint in a Shared Services VPC and shared to tenants via RAM. The DR region must auto-takeover control-plane using Route 53 failover and PrivateLink. Outline topology, onboarding, DNS strategy, telemetry, and a DR verification plan. How would you implement this end-to-end?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["IBM","LinkedIn","Netflix"]},{"id":"q-2827","question":"Design a DR-ready, multi-region AWS networking pattern for a SaaS where tenants reside in isolated VPCs per region and attach to per-region Transit Gateways. Include an automated onboarding/decommissioning workflow using StackSets, a PrivateLink-based private analytics path, cross-region telemetry, and a DR validation plan. How would you enforce guardrails and rollback if a mid-flight change fails?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Cloudflare","Instacart"]},{"id":"q-2863","question":"Design a two-region, multi-account AWS networking stack for a privacy-focused SaaS. Each tenant has a dedicated VPC attached to a regional Transit Gateway. The new requirement: inter-tenant data-plane traffic should be IPv6-first with IPv4 fallback, while control-plane remains dual-stack. Include on-prem connectivity via a DX Gateway, a central Analytics VPC reachable only through PrivateLink, automatic DR failover for the control-plane, and straightforward tenant onboarding. How would you implement end-to-end?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Databricks","Robinhood","Salesforce"]},{"id":"q-2902","question":"In a three-region SaaS deployment (Region A primary; Regions B and C as DR), each tenant has a dedicated VPC in Region A attached to a shared per-region Transit Gateway, with an Analytics API in a central Shared Services VPC reachable via PrivateLink. Design an automated DR strategy that can fail over control-plane to Regions B or C while keeping data paths local, and validate failover with telemetry. Outline TGW topology, PrivateLink replication, DNS failover, DX usage, onboarding, and a DR test plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Google","Instacart","Robinhood"]},{"id":"q-2974","question":"Design a scalable, multi-tenant AWS networking model where each tenant has a dedicated VPC in a single region, connected to a central Shared Services VPC via a hub Transit Gateway. The Analytics API is exposed as a PrivateLink service in Shared Services and must be accessible by all tenants with per-tenant access controls, on-demand onboarding/offboarding, and telemetry-driven validation. Include TGW topology, RAM-sharing of PrivateLink, per-tenant endpoint policies, firewall synchronization, and a test plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Anthropic","Citadel","MongoDB"]},{"id":"q-3060","question":"Design a global AWS networking fabric for a multi-tenant SaaS: each tenant has a dedicated VPC in Region A; a central Analytics API resides in a Shared Services VPC and is exposed via PrivateLink. Tenants require sub-50ms latency and strict egress control; implement cross-region access using AWS Global Accelerator to route to the nearest region's PrivateLink endpoints, while enabling automatic DR failover to a secondary region and preserving tenant isolation. Detail TGW topology, PrivateLink service replication, Global Accelerator config, firewall rules, observability, and a DR test plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Apple","Netflix","Tesla"]},{"id":"q-3136","question":"In a two-region, multi-account SaaS, tenants have dedicated VPCs in Region A; Analytics API sits in Shared Services VPC and is exposed via PrivateLink. With IPv6 dual-stack enabled end-to-end, design a low-latency dual-stack path to Analytics API that preserves tenant isolation and minimizes cross-region data transfer costs, with automatic DR failover to Region B using Global Accelerator and Route 53. Describe TGW topology, PrivateLink endpoints (IPv6), firewall sync, telemetry, and a DR test plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Cloudflare","Snap"]},{"id":"q-3200","question":"In a two-region, multi-account setup, each tenant has a dedicated VPC in Region A connected to a central Analytics API in Shared Services via PrivateLink. Design an AWS service-mesh strategy (App Mesh or VPC Lattice) to achieve per-tenant isolation, mutual TLS, cross-account service discovery, and canary releases, with automatic cert rotation via ACM PCA, strict egress controls, and DR failover to Region B. Describe topology, auth model, telemetry, and a DR validation plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Citadel","IBM","Uber"]},{"id":"q-3237","question":"In a single-region hub-and-spoke setup, TenantA and TenantB each own a dedicated VPC. A SharedServices VPC hosts a PrivateLink API. All VPCs connect via a Transit Gateway. Design a per-tenant egress policy so each tenant can reach only a whitelisted set of external endpoints (and the PrivateLink service), using VPC endpoint policies and TGW route controls. Include topology, endpoint policies, and a basic test plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Meta","Tesla","Twitter"]},{"id":"q-3276","question":"In a single AWS region, four development VPCs must securely access a central artifact store in a dedicated VPC and a private API in Shared Services via PrivateLink. Design the minimal topology with Interface Endpoints, per‑VPC endpoint policies, private DNS configuration, and a scalable plan to add a new dev VPC later with zero downtime?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Adobe","Robinhood"]},{"id":"q-3313","question":"Beginner-level: You’re building a globally used SaaS API. Expose public API via CloudFront in front of an ALB in a VPC with two AZs, backend in private subnets. Use NAT Gateway for outbound and Route 53 for simple DR failover to Region B. Describe VPC CIDRs, subnet layout, security groups, and a basic DR test plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Discord","Plaid"]},{"id":"q-3369","question":"Two-region, multi-account SaaS: 100 tenants, each in Region A with its own VPC; Analytics API in Shared Services VPC exposed via PrivateLink. Each tenant has a latency budget (gold ≤20ms, silver ≤40ms, bronze ≤80ms). Design per-tenant routing using Global Accelerator endpoint groups and Route 53 latency routing to nearest PrivateLink, with automatic DR failover to Region B and PrivateLink replication. Describe TGW topology, DNS records, firewall sync, telemetry, and DR test plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Lyft","Stripe"]},{"id":"q-3383","question":"In a two-region, multi-account SaaS, a central Analytics API must be accessed by thousands of tenants via PrivateLink with strict per-tenant isolation and egress controls. Propose a new angle: implement a two-region hub-and-spoke topology using regional Transit Gateways, replicate PrivateLink endpoints in both regions, and route tenants with GA endpoint groups and Route 53 latency routing to the nearest healthy region. Describe topology, egress controls, telemetry, and a DR test plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Apple","Hashicorp","LinkedIn"]},{"id":"q-3641","question":"In a two-region, multi-account SaaS, tenants have VPCs in us-east-1 and us-west-2; Analytics API resides in a Shared Services VPC in us-east-1. Propose a beginner-friendly design using regional Transit Gateways with inter-region peering or PrivateLink to provide private, low-latency access to Analytics, while enforcing strict isolation. Outline routing, security boundaries, and a basic DR check?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Adobe","Plaid"]},{"id":"q-3761","question":"You operate a single-region SaaS with VPC-A (Web/App) and VPC-S (Analytics) in us-east-1. Expose Analytics API via AWS PrivateLink and create per-tenant PrivateLink endpoints in VPC-A to achieve tenant isolation and sub-20ms latency. Describe: PrivateLink service setup; per-tenant endpoint policies; Route 53 private zones and DNS records; security groups/NACLs; observability; and a basic DR test plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Airbnb","Discord","Tesla"]},{"id":"q-3846","question":"Two-region SaaS: tenants in VPCs in us-east-1 and us-west-2 need private, low-latency access to a central Analytics API in a Shared Services VPC in us-east-1, exposed via PrivateLink as two endpoint services (read and write). Design a beginner-friendly solution using PrivateLink, per-tenant VPC endpoints, and Route 53 private DNS to resolve to the correct ENIs. Include onboarding, cross-region reachability, and a basic DR test plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Cloudflare","Hugging Face"]},{"id":"q-3916","question":"Two-region, multi-account SaaS: 100 tenants, each in Region A with dedicated VPCs attached to a centralized Transit Gateway; Analytics API in Shared Services VPC exposed via PrivateLink. Enforce strict data-residency egress: tenants can reach Analytics API only; all other outbound blocked. Design per-tenant DNS, endpoint policies, and firewall rules with automatic DR failover to Region B using Global Accelerator; include observability and DR test plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Goldman Sachs","Hashicorp","Oracle"]},{"id":"q-3946","question":"Design a beginner-friendly, single-region, multi-account setup: five tenant VPCs connect to a central analytics API exposed as a PrivateLink service in a Shared Services VPC. Explain how to publish a per-tenant PrivateLink endpoint, enforce strict isolation with per-tenant endpoint policies, enable cross-account DNS resolution using Route 53 Resolver rules, and validate reachability and costs, including a basic DR drill?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Databricks","Tesla"]},{"id":"q-4027","question":"In a two-region, multi-account SaaS, each tenant has a dedicated VPC in Region A connected to a central Analytics PrivateLink in Shared Services. Design a zero-trust, per-tenant network plane using a Firewall VPC and Transit Gateway, ensuring sub-50ms latency, tenant isolation, and automatic DR failover to Region B. Describe TGW topology, PrivateLink replication, firewall policy propagation, observability, onboarding, and a DR test plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Adobe","Discord","IBM"]},{"id":"q-4214","question":"Two-region, multi-account: each tenant has a VPC in Region A and a central Analytics API in a Shared Services VPC exposed via PrivateLink. Introduce a per-tenant service mesh with AWS VPC Lattice to enforce network-tenant authorization, using per-tenant endpoint policies and ACM TLS. Describe topology, replication, DR failover with Global Accelerator and Route 53, plus a test plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Citadel","Salesforce"]},{"id":"q-4245","question":"Design a cross-region, multi-tenant AWS network where each tenant has a VPC in Region A and accesses a central Analytics API via PrivateLink in Shared Services. Data residency requires Region A unless DR to Region B is active. Propose per-tenant PrivateLink replication, a Transit Gateway layout, and Global Accelerator with per-tenant endpoint groups, Route 53 routing, and automated DR. Include per-tenant mTLS with ACM Private CA and cert rotation, observability, and a DR test plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Netflix","Square","Tesla"]},{"id":"q-4311","question":"In a single-region, multi-VPC SaaS, central Analytics API lives in Shared Services VPC and is exposed as a PrivateLink service. Each tenant has its own VPC and a VPC Endpoint to the service. Describe how you would set up PrivateLink, share it via RAM, enforce per-tenant isolation with endpoint policies, configure DNS, and observability, plus a minimal DR plan if Shared Services VPC becomes unavailable?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["DoorDash","Goldman Sachs","Microsoft"]},{"id":"q-4326","question":"In a two-region SaaS, Analytics API sits in Shared Services VPCs in Region A and Region B, exposed to tenants via PrivateLink. Design PCI-compliant per-tenant rate limiting and cross-region audit logging without compromising isolation. Explain: per-tenant throttling approach, PrivateLink endpoint policies, cross-region routing, log sink and encryption, and a DR test plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Cloudflare","Coinbase","Robinhood"]},{"id":"q-4365","question":"Scenario: In a two-region, multi-account SaaS, each tenant has a dedicated VPC in Region A and a central Analytics API in Shared Services VPC exposed via PrivateLink. Implement per-tenant egress control and data residency by routing outbound traffic through a tenant-scoped egress VPC with NAT and a stateful firewall managed via Firewall Manager. Design TGW topology, per-tenant PrivateLink endpoints, cross-region DR with Global Accelerator, observability, and a DR test plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Cloudflare","Meta","Slack"]},{"id":"q-4545","question":"Two-region, two-account SaaS: central Analytics API replicated in Shared Services VPCs in Region A and Region B, exposed to 3 tenant VPCs via PrivateLink endpoints. Design a beginner-friendly solution that guarantees per-tenant isolation, minimizes cross-region egress, and enables automatic DR failover using a Transit Gateway topology and Route 53. Include a basic observability plan and a simple DNS/PrivateLink layout?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["MongoDB","Snowflake"]},{"id":"q-4638","question":"In a two-region SaaS, Region A hub TGW and 60 tenant VPCs, Analytics API in Shared Services VPC exposed via PrivateLink. Propose a per-tenant service mesh (App Mesh) with mTLS between tenants and Analytics API, plus per-tenant routing rules, PrivateLink replication to Region B, and automatic DR failover via Global Accelerator. Include observability and a DR test plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["NVIDIA","Scale Ai","Two Sigma"]},{"id":"q-4650","question":"In a two-region, multi-account SaaS, each tenant has a dedicated VPC in Region A and Analytics API is exposed via PrivateLink in a central Shared Services VPC. Design a practical plan to diagnose and mitigate intermittent cross-region latency spikes that breach SLA (<50ms). Include TGW route tables, PrivateLink health checks, GA endpoint groups, per-tenant endpoint policies, and a testing workflow using VPC Flow Logs, Reachability Analyzer, and CloudWatch dashboards to isolate root causes and validate fixes?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Coinbase","Robinhood","Snowflake"]},{"id":"q-4700","question":"In a single-region SaaS, three tenant VPCs must privately reach a central Analytics API in a Shared Services VPC via AWS PrivateLink. Design using a single Transit Gateway. Provide per-tenant isolation with VPC Endpoint Policies and RAM sharing; outline TGW topology, Endpoint Service, and per-tenant Interface Endpoints. Include basic SG/NACL controls and a simple DR plan to a second region using Route 53 failover?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Airbnb","Netflix","Square"]},{"id":"q-4754","question":"Design a beginner-friendly per-tenant isolation for a Shared Services PrivateLink API: five tenant VPCs in the same region access a central Analytics API hosted in the Shared Services VPC via a PrivateLink service. Describe endpoint policies and security group rules that ensure tenants cannot reach other tenants, include a concrete sample endpoint policy for one tenant, and outline a basic test plan to verify cross-tenant access is blocked while legitimate access remains allowed?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Scale Ai","Snap","Twitter"]},{"id":"q-4805","question":"Advanced AWS networking challenge: Each tenant has a dedicated VPC in Region A; a central Analytics API resides in Shared Services VPC and is exposed via PrivateLink. Now implement a cross-region, multi-account service mesh using AWS App Mesh to enable per-tenant routing, mTLS, and strict egress control, while minimizing cross-region data transfer. Describe TGW topology, PrivateLink replication per tenant, App Mesh control plane across accounts/regions, Route 53 DNS routing, automatic DR failover, and observability?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Slack","Tesla"]},{"id":"q-4832","question":"Design a cross-account, multi-region private API exposure for a real-time telemetry ingestion service. Producers live in Tenant accounts across Region A and Region B; a centralized Analytics API runs in Shared Services VPC. Requirements: Private connectivity via PrivateLink, strict per-tenant isolation, sub-50ms end-to-end latency within region, automatic cross-region DR with Global Accelerator, and per-tenant observability. Include TGW topology, PrivateLink DNS setup, firewall rules, and a DR test plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Citadel","MongoDB","Snowflake"]},{"id":"q-902","question":"A multinational bank operates 6 VPCs across 3 AWS accounts in two regions. They want to centralize north-south internet egress for security inspection, keep east-west traffic private, minimize inter-region data transfer costs, and streamline onboarding of new accounts. Propose a hub-and-spoke design using AWS Transit Gateway, Network Firewall, and VPC Endpoints. Describe routing, policy model, and observability considerations?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Goldman Sachs","Google","Tesla"]},{"id":"q-923","question":"In a multi-account AWS environment with four accounts (prod, dev, staging, shared-services) and two regions, internal services (e.g., app.internal, db.internal) must resolve to private IPs across accounts, with public DNS unchanged for customers. Propose a scalable private DNS design using a private hosted zone and Route 53 Resolver (inbound/outbound as needed). Explain setup steps, minimal cross-account boundaries, and basic validation methods?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["DoorDash","Hashicorp","Meta"]},{"id":"q-939","question":"An enterprise runs 4 AWS accounts across 2 regions with 10 VPCs. They require centralized north-south internet egress through a firewall inspection stack, private east-west traffic, cross-region replication, and scalable SaaS access via PrivateLink. Design an end-to-end network using Transit Gateway, VPC Endpoints, Private DNS, and optional Direct Connect/Interconnect. Describe routing, policy model, failover, and observability considerations?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["DoorDash","Microsoft","Square"]},{"id":"q-1047","question":"Design a multi-tenant analytics data lake on AWS for 3,000 tenants with strict data isolation. Propose an architecture using a shared S3 data lake with per-tenant prefixes, Lake Formation permissions, and ABAC via tenant tags. Outline governance (IAM, KMS, RAM), onboarding/offboarding automation, cost accounting, and cross-region DR. Include testing strategies to validate isolation and detect privilege escalation?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Cloudflare","Robinhood","Snowflake"]},{"id":"q-1203","question":"A web app runs in a private subnet in a VPC with no Internet access. It must fetch a config.json from a single S3 bucket owned by the same account. Design the minimal IAM role for the EC2 instance, a bucket policy, and a VPC endpoint setup to allow access via the VPC endpoint while denying access to other buckets. What steps and policies would you implement?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Cloudflare","DoorDash","Snowflake"]},{"id":"q-1318","question":"A Secrets Manager secret in account A stores DB credentials used by an app in account B. You need a Lambda in account B to rotate the secret daily. Describe the cross-account access setup, including the secret's resource policy, the Lambda's execution role, and the minimal privileges on the DB user, plus a simple test plan?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Cloudflare","Coinbase"]},{"id":"q-1390","question":"In a managed SaaS environment, each customer gets its own AWS account under a shared Organization. You require SSO-based access to their account via IAM Identity Center, provisioning per-customer roles via SCIM, with temporary credentials; ensure strict tenant isolation and prevent cross-tenant access. Describe the IAM role structure, trust/policy design, SSO mappings, and use of SCPs, tagging, and audit strategy. Include how you'd validate isolation and detect misconfigurations?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["DoorDash","Lyft","Tesla"]},{"id":"q-1402","question":"Scenario: You operate a multi-tenant SaaS on AWS. Each tenant stores its data in separate accounts with per-tenant prefixes in S3 and DynamoDB. Partners require selective data exports to their accounts for analytics. Design a scalable, secure data export and access model that guarantees tenant isolation, supports per-tenant export scopes, uses least-privilege cross-account roles, per-tenant KMS keys, and catalog governance (Lake Formation/Data Catalog). Include onboarding/offboarding, cost controls, monitoring, and testability?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Airbnb","Databricks","Netflix"]},{"id":"q-1425","question":"In a multi-tenant SaaS on AWS with per-tenant accounts and a central analytics account, design secure cross-account access so tenants can query their data without seeing others. Describe the cross-account role structure, trust policies, session controls, least-privilege, and validation/audit approach for onboarding/offboarding?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Microsoft","Salesforce","Scale Ai"]},{"id":"q-1515","question":"Scenario: A Lambda function in a VPC must securely retrieve a DB password from Secrets Manager to connect to an RDS instance. The secret is encrypted with a customer-managed CMK in the same account. Describe the exact IAM policy statements for the Lambda execution role to permit GetSecretValue and DescribeSecret on that secret, and kms:Decrypt on the CMK; include any needed resource-based or key policies and how you’d validate least privilege and rotation compatibility?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Databricks","Meta","Snowflake"]},{"id":"q-1532","question":"Design a per-tenant, region-aware analytics pipeline for a multi-account AWS SaaS where each tenant's data residency must stay in its region. Explain how you’d enforce data locality, isolate tenant data, share insights back to a central analytics account, and test DR?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Bloomberg","Google"]},{"id":"q-1566","question":"In a multi-tenant SaaS on AWS, each tenant's data sits in a separate prefix under a shared data lake. Propose a beginner-friendly, cost-conscious approach to let tenants access their own analytics dashboards through a central BI tool without cross-tenant data exposure. Specify IAM patterns, data access controls, and how you'd validate isolation and cost?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Microsoft","Robinhood"]},{"id":"q-1592","question":"An analytics SaaS runs in a single AWS region with private data stores. You must enable cross‑region DR with automated failover to a secondary region within 30 minutes, preserving tenant isolation and continuous client access. Design the architecture using Aurora Global Database, S3 cross‑region replication, Route 53 failover, per‑tenant data partitioning, and regionally isolated KMS keys. Include data‑sync, cutover automation, and a testing plan?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Databricks","Microsoft"]},{"id":"q-1778","question":"You run a multi-tenant SaaS on AWS with a central control plane and per-tenant accounts. On signup, you must provision a dedicated VPC Endpoint service and per-tenant IAM roles with least privilege for cross-account access, revocable within 60 seconds. Describe the architecture, trust policies, RAM/PrivateLink usage, SCIM provisioning, and how you would automate validation and rollback with EventBridge and Step Functions?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Cloudflare","Instacart","NVIDIA"]},{"id":"q-1796","question":"Scenario: A SaaS runs per-tenant accounts across AWS Organizations. A tenant's IAM role is suspected to be compromised, enabling sideways access to the central control plane. Propose a zero-downtime incident response workflow that (a) detects via CloudTrail/GuardDuty/EventBridge, (b) revokes the compromised role’s trust, (c) isolates the tenant from cross-account access, and (d) notifies security and customer teams within 60 seconds. Include workflow steps and rollback?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Adobe","Goldman Sachs","Google"]},{"id":"q-1826","question":"Design a self-serve cross-account restore workflow: a tenant requests data restore from a central backup vault into their own AWS account. Outline the per-tenant cross-account role with TTL, trust policy, least-privilege permissions, and how you orchestrate AWS Backup restore, validation, and rollback within SLA?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Airbnb","Apple","Oracle"]},{"id":"q-1866","question":"You operate a multi-tenant SaaS on AWS with per-tenant data stored in separate accounts. Upon signup, you need to provision an isolated workspace with a per-tenant VPC, a dedicated KMS CMK for data at rest, and strictly limited cross-account access for support tooling that can revoke within 60 seconds. Describe the end-to-end architecture, including IAM/RAM roles, SCPs, KMS key management, S3 and DynamoDB data isolation, and how you test onboarding/rollback and audits at scale?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Salesforce","Twitter"]},{"id":"q-1920","question":"You operate a multi-tenant SaaS on AWS with per-tenant accounts under an Organization. A tenant triggers suspicious activity suggesting cross-account data exfiltration. Design a zero-downtime incident response that detects via CloudTrail/EventBridge/GuardDuty, revokes trust and rotates CMKs, quarantines the tenant with SCPs, and notifies teams within 60 seconds. Include rollback steps and testing strategy?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Oracle","Tesla","Two Sigma"]},{"id":"q-1974","question":"You run a multi-tenant SaaS on AWS that provisions per-tenant data pipelines (Glue/EMR) on signup. Tenants upload ETL scripts; you must sandbox execution with strict runtime and cost caps, guarantee per-tenant data isolation, and enable revocation within 60 seconds. Describe the control plane, roles, policies, data isolation, and rollback/audit strategy?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Salesforce","Scale Ai"]},{"id":"q-2056","question":"Scenario: A multi-tenant SaaS stores each customer's data in a regional S3 bucket owned by that customer, with your service ingesting data via cross-account roles. On signup you must provision a per-tenant IAM role, per-tenant KMS CMK, and per-tenant Lake Formation permissions, with revocation within 60 seconds if policy is violated. Describe the architecture, trust policies, and automation (EventBridge, Step Functions, RAM) to onboard, rotate keys, and revoke access?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Coinbase","MongoDB","Square"]},{"id":"q-2149","question":"In a two-account setup (dev and prod), how would you grant a developer permission to deploy to prod S3 and prod ECR via a cross-account role with MFA, using minimal permissions and fast revocation? Describe the trust policy, role inline permissions, and a basic CI check to verify access?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Hugging Face","LinkedIn","NVIDIA"]},{"id":"q-2177","question":"Design a scalable, private cross-tenant analytics model for a SaaS where each tenant has an isolated AWS account. Propose how to share aggregated data to a central analytics account using cross-account IAM roles, Lake Formation permissions, and KMS; outline onboarding/offboarding with EventBridge and Step Functions, data access revocation within 60 seconds, auditing, and failure recovery?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Coinbase","Instacart","Zoom"]},{"id":"q-2269","question":"You operate a SaaS on AWS with 1,000 tenant accounts under an Organization. Tenants demand data residency in their own region and private cross-region analytics access to a centralized data lake. Design an end-to-end solution that (1) isolates per-tenant data in region-scoped S3 buckets with per-tenant KMS keys, (2) enforces Lake Formation permissions per tenant, (3) connects privately to regional hubs via PrivateLink, (4) supports cross-region replication with controlled consent, and (5) provides a 60-second rollback workflow using EventBridge and Step Functions, plus automated tests and drift checks?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Airbnb","Databricks","Tesla"]},{"id":"q-2284","question":"Design an automated containment workflow for a 50-account AWS environment. When GuardDuty flags a compromised EC2 in any development account, automatically isolate the host within 60 seconds by detaching the ENI, revoking outbound traffic via Security Groups, and exporting forensics data to the central security account. Minimize production impact and ensure a rollback path with false-positive handling. Describe architecture, cross-account IAM roles, EventBridge/Step Functions flow, and testing strategy?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Hashicorp","Meta"]},{"id":"q-2381","question":"Design a scalable data-access pattern for a multi-tenant SaaS on AWS where each tenant’s data must be privately accessible by a third-party ML partner (Databricks, Snowflake, or OpenAI) via private connectivity, without data leakage or public Internet. Describe per-tenant PrivateLink endpoints, cross-account RAM shares, strict IAM trusts, governance via Lake Formation or bucket policies, 60-second revocation, and auditing?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Databricks","OpenAI","Snowflake"]},{"id":"q-2567","question":"You run a small web app in a single AWS region with minimal downtime constraints. Propose a beginner-friendly disaster recovery plan that uses S3 for backups, RDS automated backups or DynamoDB backups, Route 53 failover routing to a secondary region, and a one-click failover test. Describe how you'd validate RTO/RPO and the cost implications, with minimal operational effort?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["NVIDIA","Tesla"]},{"id":"q-2629","question":"Scenario: A SaaS running across two AWS regions must ensure tenant data residency by storing each tenant's data only in their assigned region and enabling revocation of access within 60 seconds; design an end-to-end architecture using AWS Organizations, PrivateLink Endpoint Services, per-tenant IAM roles with SCPs, DynamoDB Local/Global Tables and S3 region replication, and an automated revocation workflow via Step Functions and EventBridge, including failure modes and DR/cost tradeoffs?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Cloudflare","Microsoft"]},{"id":"q-2784","question":"You need to grant a trusted partner ephemeral access to a single S3 bucket in your account for 60 minutes without exposing public endpoints. Describe end-to-end mechanism using a cross-account IAM role, STS AssumeRole, bucket policy with constraints, and auditing; include how you'd revoke access within 60 seconds and verify?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Apple","Lyft","Two Sigma"]},{"id":"q-2862","question":"Scenario: A SaaS runs in multiple AWS regions; each tenant must have data residency in their assigned region with per-tenant S3 buckets and DynamoDB replicas. On signup, automatically provision region-scoped resources and issue an ephemeral onboarding role for centralized operations with 60-second revocation. Describe the end-to-end automation, trust policies, and rollback/validation strategy, including failure modes and cost considerations?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Google","Netflix"]},{"id":"q-2898","question":"Scenario: A multi-tenant SaaS stores customer data encrypted with a central AWS KMS CMK in a security/account. Each tenant’s workloads run in their own AWS account and must decrypt only their own data, with revocation within 60 seconds. Design a cross-account access pattern using KMS Grants, per-tenant IAM roles, per-tenant S3/DynamoDB policies, and an automated revocation workflow (EventBridge + Step Functions) plus DR/cost tradeoffs?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["LinkedIn","NVIDIA","Two Sigma"]},{"id":"q-2975","question":"Advanced AWS SaaS: design per-tenant data isolation across accounts. On signup you must automatically provision a per-tenant data plane: an S3 bucket, a CMK, and a cross-account role your API uses to read/write data. Access must be ABAC based on tenant_id and revocable within 60 seconds. Outline architecture, IAM policies, automation, testing, and DR?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Instacart","Netflix"]},{"id":"q-3223","question":"Two-region disaster recovery for a small web app: primary in us-east-1 and DR in us-west-2. Use RDS cross-region read replicas, S3 backup replication, and Route 53 failover. Outline the exact steps, required IAM roles, and how you would test failover and rollback with minimal data loss?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Airbnb","Microsoft","Plaid"]},{"id":"q-3271","question":"You run a regulated fintech analytics platform across 3 AWS accounts (prod, audit, shared). New tenants require ad-hoc analytics with strict data isolation and region residency. Propose a cross-account data access model that enables per-tenant Athena queries against a central S3 data lake, while ensuring tenant data never leaves its assigned region, uses tenant-specific IAM roles and KMS CMKs, and provides revocation, cost control, and audit trails. Include data catalog exposure, encryption, and failure modes?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Goldman Sachs","IBM"]},{"id":"q-3414","question":"Scenario: A SaaS creates a per-tenant data silo in S3 and DynamoDB, and must grant a partner service read access only to that tenant's data through a PrivateLink-connected API. Propose an end-to-end onboarding workflow (VPC, PrivateLink, IAM roles, bucket prefix and table scoping, key management) and explain how you would revoke access within 60 seconds, including validation steps?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Discord","LinkedIn","Two Sigma"]},{"id":"q-3524","question":"You must deploy a new service in a single AWS account that reads from a private S3 bucket and writes logs to CloudWatch, with no public Internet access. Outline the minimal secure setup: S3 gateway endpoint in the VPC, a bucket policy restricted to the endpoint and the service's IAM role, an IAM role with least privileges, and a test plan to verify private access and absence of public access?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Microsoft","Tesla"]},{"id":"q-3547","question":"Scenario: You run a SaaS that ingests per-tenant telemetry via a single API in AWS. Data must be isolated by tenant, stored under an S3 prefix, and publicly inaccessible. Propose a minimal, beginner-friendly architecture using regional API Gateway, a Lambda authorizer that validates a JWT with tenant_id, per-tenant IAM policies, and an S3 bucket policy; describe least privilege, revocation, and a simple test plan to verify cross-tenant isolation and absence of public access?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["DoorDash","OpenAI"]},{"id":"q-3605","question":"Design a private cross-account API proxy: a central control plane in account A securely calls per-tenant APIs in tenant accounts B..Z without the public Internet, using PrivateLink Endpoint Services and RAM. Describe cross-account trust, per-tenant IAM roles with least privilege and SCPs, encryption, logging, and a 60-second revocation workflow via EventBridge/Step Functions, plus validation and rollback?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["MongoDB","Salesforce"]},{"id":"q-3675","question":"You operate a multi-account SaaS on AWS with primary workloads in us-east-1 and a DR region in eu-west-1. A regional outage must be recovered within 60 seconds and meet an RPO of 5 seconds. Design a concrete cross-region DR for a service that uses S3 and DynamoDB (or RDS), detailing replication strategies, IAM least privilege, Route 53 failover, automation, and a test plan?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Meta","Oracle"]},{"id":"q-3715","question":"You operate a multi-tenant SaaS with a central REST API in a dedicated AWS account. All tenant traffic must stay private, not traverse the public Internet. Propose a concrete PrivateLink-based design with per-tenant interface endpoints, per-tenant IAM roles, and a token-based auth flow behind API Gateway. Explain onboarding/offboarding, isolation tests, and revocation timing?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Amazon","Cloudflare"]},{"id":"q-3738","question":"You have a small service running on EC2 in Account A that must read a credential stored as a secret in AWS Secrets Manager in Account B every 15 minutes. Design the minimal cross-account setup: (1) the secret's resource-based policy in Account B, (2) the IAM role in Account A that EC2 uses, (3) the instance profile policy to allow GetSecretValue, and (4) a simple test plan to confirm no secret is ever exposed publicly. Include how to rotate the secret and how to audit access?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Google","Robinhood","Zoom"]},{"id":"q-3799","question":"Your company runs a private REST API behind API Gateway and Lambda in one AWS account. A partner in another account must invoke the API from their VPC without using the public Internet. Propose a minimal PrivateLink-based design: interface endpoint in the partner VPC, a PrivateLink endpoint service for the API, and a simple per-partner auth flow behind API Gateway. Include onboarding/offboarding steps and basic tests?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Hugging Face","Microsoft","Tesla"]},{"id":"q-3806","question":"Design an end-to-end cross-account analytics workflow where each tenant's data resides in per-tenant S3 prefixes in a shared SaaS account, must be accessible to Snowflake in a partner account without public Internet, and isolated by tenant. Propose concrete use of per-tenant S3 Access Points, Lake Formation permissions, a PrivateLink-based analytics service, and a token-based auth flow via API Gateway/Lambda. Include onboarding/offboarding and audit requirements?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["MongoDB","Snowflake"]},{"id":"q-3909","question":"You offer a global API behind API Gateway and Lambda in account A. A tenant selects a region at signup and must call the API privately from their VPC in that region (no public Internet). Propose a minimal PrivateLink-based design with a per-tenant Interface Endpoint in the tenant's region, a regional Endpoint Service, and a token-based auth flow behind API Gateway. Include onboarding/offboarding, regional failover considerations, and a lightweight test plan?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Databricks","Netflix","Scale Ai"]},{"id":"q-3932","question":"Your SaaS stores per-tenant data in customer accounts and uses a central control plane in a shared services account. Propose a per-tenant AWS KMS CMK strategy using Grants for cross-account access, including rotation without downtime, tenant onboarding/offboarding tests to ensure no cross-tenant access or leakage?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Cloudflare","NVIDIA","Twitter"]},{"id":"q-4077","question":"You operate a public SaaS API behind API Gateway that accepts webhooks from hundreds of customers. To verify payload integrity, you require an HMAC signature per request using a per-customer secret stored securely. Propose a minimal, practical solution using API Gateway and a Lambda authorizer to validate signatures, with per-customer Secrets Manager entries, a scalable onboarding/offboarding process for secrets, and a testing strategy that includes a dry-run mode and rotation?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Apple","Instacart","Oracle"]},{"id":"q-4188","question":"Design a centralized API surface for a 1000-tenant SaaS where each tenant's data is isolated in per-tenant S3 prefixes and DynamoDB tables in a shared region. Expose the API privately to each tenant via PrivateLink from their VPCs. Describe the end-to-end flow: per-tenant authentication with short-lived tokens, per-tenant IAM roles, data access patterns, onboarding/offboarding, and a test plan proving strict data separation and revocation timing?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Google","Netflix","Tesla"]},{"id":"q-4220","question":"Design a scalable per-tenant data lake ingestion in a multi-account AWS setup: 50 tenants, each in their own account, must contribute logs to a central data lake in a shared account. Use S3 Access Points for per-tenant isolation, per-tenant IAM roles, and a cross-account delivery path (Kinesis Firehose) to the central S3 bucket. Outline onboarding/offboarding, encryption (KMS CMK), auditability, and a test plan that proves isolation and revocation?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Coinbase","Snap"]},{"id":"q-4399","question":"Scenario: a multi-tenant SaaS hosts a central analytics API in a single AWS account. Tenants from separate AWS accounts access the API only over private connectivity (no public Internet). Propose an end-to-end PrivateLink design with per-tenant interface endpoints in each tenant account, a PrivateLink endpoint service in the API account, and a cross-account role the provider assumes with least privilege. Include data isolation in DynamoDB (tenant_id as partition key) and per-tenant KMS keys, plus automated onboarding/offboarding (60s revocation) and a thorough test plan for private access and isolation?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Cloudflare","MongoDB","Tesla"]},{"id":"q-4428","question":"You operate a private ML inference API hosted in a central AWS account. To serve customers in multiple AWS accounts across regions while avoiding Internet, design a cross-region PrivateLink solution with per-tenant onboarding/offboarding and 60-second revocation. Explain how to publish an endpoint service in each region, create per-tenant interface endpoints, implement Private DNS with Route 53, and constrain access with IAM trust policies and permission boundaries. Include onboarding/testing and cross-region failover steps?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Databricks","OpenAI","Tesla"]},{"id":"q-4477","question":"Two AWS accounts, A and B. A publishes user-activity events to EventBridge for real-time processing by a service in B, without public Internet access. Design a minimal cross-account EventBridge setup: (1) how to publish events from A, (2) how B consumes them (Lambda/SQS), (3) DLQ and retries, (4) a quick test plan to verify delivery and at-least-once semantics. Include security and cost considerations?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Discord","MongoDB","Snap"]},{"id":"q-4599","question":"Design an AWS-native per-tenant CMEK strategy for a multi-tenant SaaS: each tenant's data resides in their own AWS account and must be encrypted with a customer-managed KMS key (CMEK) owned by the tenant. The service in a central account should be able to perform encryption/decryption on behalf of the tenant using cross-account grants, while no data can be decrypted without explicit tenant authorization. Outline the key policy, IAM roles, cross-account trust, how you rotate keys, and a test plan to verify isolation and revocation?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Google","Instacart","OpenAI"]},{"id":"q-4625","question":"Design a minimal cross-account data access pattern for a beginner SaaS: a partner account must read only their data stored in an S3 bucket in your account, with no public Internet access. Propose the architecture using a cross-account IAM role, bucket policy, and optional VPC endpoint; specify rotation, revocation, and a simple validation test plan?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Slack","Stripe","Zoom"]},{"id":"q-4802","question":"You run a private data ingestion API consumed by tenants across many AWS accounts; all tenant traffic must stay private and not traverse the public Internet. Propose a concrete PrivateLink-based design with per-tenant interface endpoints, DNS, and per-tenant auth; include onboarding/offboarding, revocation timing, and a test plan that proves no public egress?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["MongoDB","Oracle"]},{"id":"q-4822","question":"Your app runs in a private VPC and must access DynamoDB without any Internet access. Outline a minimal setup: (1) create a gateway VPC endpoint for DynamoDB with private DNS, (2) update route tables in private subnets, (3) attach a least-privilege IAM role to the Lambda to read the DynamoDB table, and (4) a simple test plan to prove no Internet egress and validate access. Include monitoring steps?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["NVIDIA","PayPal","Twitter"]},{"id":"q-672","question":"Design a scalable, cost-aware data ingestion and processing pipeline on AWS for 1 TB/day of log data arriving from multiple on-prem and cloud sources. The pipeline must deliver raw data immutably for 90 days, provide near-real-time enrichment within 5 seconds, and support cross-region failover. Specify services, data flows, constraints, and trade-offs?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Hashicorp","IBM"]},{"id":"q-930","question":"An existing web service runs on EC2 in a single VPC with an ALB. Traffic surges cause latency spikes and occasional outages during AZ failures. Propose a beginner-friendly, cost-conscious HA setup using an Application Load Balancer, Auto Scaling across at least two AZs, and a relational database option. Include networking, health checks, a scaling policy, and a basic DB deployment choice with trade-offs. What would you implement first and why?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Amazon","MongoDB"]},{"id":"q-944","question":"Design a multi-tenant SaaS on AWS that ingests telemetry from thousands of tenants daily, enforces strict data isolation, and provides cross-region DR. Outline data partitioning, access control, encryption strategy, immutable logging, and DR failover. Include services, trade-offs, and how you test isolation?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Anthropic","Meta","Snap"]},{"id":"q-989","question":"Design a centralized security telemetry pipeline for 1,000 AWS accounts to detect anomalies in near real-time. Ingest VPC Flow Logs, CloudTrail, and GuardDuty findings into a central security account using AWS Organizations, implement least-privilege cross-account roles, normalize data into a common schema in S3, partition by source account and region, apply Lake Formation permissions, and set up alerting with EventBridge and security findings. Include scaling, cost, DR, and testing?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Google","Plaid"]},{"id":"q-1005","question":"You operate SAP S/4HANA on AWS with analytics in Snowflake and must implement a data masking/tokenization pipeline so analytics do not expose PII. Design end-to-end data flow, masking rules by field, latency (<5 minutes), and governance using KMS/IAM. Include auditing, rollback, and failover considerations?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Oracle","Snap","Snowflake"]},{"id":"q-1011","question":"How would you implement an automated cross-region DR for SAP HANA on AWS? Use SAP HANA System Replication with Region A as primary and Region B as hot standby, orchestrated by AWS Step Functions; back up to EBS/S3 with Data Lifecycle Manager and cross-region KMS keys; ensure automated DR tests, rollback playbooks, and meet RPO <5 minutes, RTO <15 minutes?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["PayPal","Uber"]},{"id":"q-1059","question":"Design an advanced SAP S/4HANA on AWS architecture using SAP HANA MDC on EC2 across 3 AZs, with analytics separated into a data lake. Propose a rolling OS/kernel/SAP patching and upgrade strategy that preserves near-zero downtime, enables automated cross-region DR testing, and guarantees RPO <2 minutes and RTO <5 minutes; specify automation, services, failure modes, and rollback?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Airbnb","Hashicorp","Stripe"]},{"id":"q-1179","question":"Design an automated, auditable patching workflow for SAP S/4HANA on EC2 across multiple AWS accounts and regions. Use AWS Systems Manager Patch Manager to patch OS and SAP kernel updates with rolling upgrades, pre/post checks, and automated rollback if SLA drift occurs. Include governance, approvals, testing, and validation of success before go-live?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Apple","Citadel","Plaid"]},{"id":"q-1186","question":"Design a multi-account SAP S/4HANA on AWS with SAP HANA MDC across 3 AZs and a cross-region DR setup. Route SAP system and security logs to a centralized, cross-account S3 data lake with Object Lock (WORM) and cross-region replication. Use AWS Glue/Data Catalog and Lake Formation for lineage and access control; enforce least privilege with SCPs. Automate DR tests and integrity checks?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Instacart","Lyft","Plaid"]},{"id":"q-1328","question":"Design a cross-account SAP S/4HANA MDC deployment where production data sits in Account A and an analytics MDC mirrors in Account B. Use near real-time CDC data replication (e.g., DMS) into a centralized S3 data lake and Glue catalog, with strict IAM governance, private networking, and automated drift checks. Include rollback and DR testing plan; target RPO <60s, RTO <5m. How would you implement it?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Microsoft","Netflix"]},{"id":"q-1404","question":"Design an advanced SAP S/4HANA on AWS architecture for a global user base focused on latency and data residency. Use SAP HANA MDC on EC2 across 2 AZs in Region A with a cross-region MDC replica in Region B for DR; enforce EU data residency with local S3 buckets and KMS keys; implement IAM/SCP least privilege; outline automated DR tests, failover/failback, and rollback?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Bloomberg","Discord","Slack"]},{"id":"q-1434","question":"Scenario: You operate SAP S/4HANA on SAP HANA MDC on EC2 across two AWS regions (Region A as primary, Region B as hot standby). Design an automated DR test plan that uses SAP HANA System Replication, AWS Step Functions, and AWS Fault Injection Simulator to perform regular, reportable failover tests without impacting production. Define test cadence, success criteria (RPO <2m, RTO <10m), rollback procedures, and how to prove continuity to stakeholders with logs, metrics, and cross-region backups?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Google","MongoDB"]},{"id":"q-1494","question":"You manage SAP HANA MDC on EC2 in a single AWS region. Design a beginner-friendly, automated backup strategy using only AWS native services (EBS snapshots, S3, AWS Backup, KMS) with cross-region replication to a warm standby. Include backup frequency, retention, encryption, and a reproducible restore test plan with objective to meet RPO 15 minutes and RTO 1 hour?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Adobe","Hashicorp"]},{"id":"q-1559","question":"In a single-region AWS SAP deployment (SAP NetWeaver + SAP HANA on EC2), propose a beginner-friendly automated health-check using only AWS native services. The check runs daily, verifies SAP instance status, HANA availability, and key OS metrics, writes a JSON report to S3, and triggers an SNS alert on any failure. Include data flow, AWS services used, and a minimal script snippet the Lambda would run via SSM Run Command?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Adobe","Airbnb","Hugging Face"]},{"id":"q-1606","question":"In a single-region SAP HANA on EC2 deployment, design a beginner-friendly incident-response automation using only AWS-native services to detect an outage within 5 minutes, isolate the SAP subnet, perform a warm-standby failover, and notify stakeholders via SNS. Include data flow, services used, and a minimal Lambda snippet that checks SAP status via SSM Run Command?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Hashicorp","Two Sigma"]},{"id":"q-1709","question":"You operate SAP HANA on EC2 with a parallel SAP ABAP stack in a multi-AZ VPC. Propose a concrete AWS-native auto-scaling and DR plan to handle quarterly peak workloads with zero SAP downtime. Include (1) app-tier scaling strategy (ASG, launch templates), (2) HANA scale-out readiness and data protection, (3) storage IOPS and sizing choices, (4) cross-region DR with replication and failover testing cadence, (5) rollback criteria and metrics?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Discord","Google","PayPal"]},{"id":"q-1779","question":"In an SAP HANA on EC2 landscape spanning three AWS environments (dev, stage, prod), design a policy-driven security hardening and continuous compliance workflow using only AWS-native services. Include OS baselines, SAP user and kernel parameters, and file permissions; enable drift detection, automated remediation via SSM Run Command, and audit reports to S3 + Glue catalog. Provide architecture and a runnable Lambda snippet to enforce the baseline?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["IBM","Meta","Snowflake"]},{"id":"q-1814","question":"In a two-region AWS SAP HANA deployment (NetWeaver + S/4HANA on EC2), design an observability-driven auto-remediation plan to detect SAP performance anomalies (e.g., high ABAP work process waits, HANA stalls, long SAP responses) using only AWS-native services. Define metrics, thresholds, data flow, remediation actions (auto-scale, restart SAP components via SSM Run Command), and how you validate success?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Adobe","Snowflake"]},{"id":"q-1838","question":"Design a beginner-friendly cost governance workflow for SAP HANA on EC2 in a single AWS region that uses only native AWS services: enforce SAP-resource tagging with Environment and Owner, set a monthly spend budget, and trigger an SNS alert if forecasted spend exceeds 80% of the budget. Describe data flow, services used, and provide a minimal CloudFormation snippet to create the budget and a Lambda skeleton to respond to forecast notifications?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Bloomberg","MongoDB"]},{"id":"q-1905","question":"In a multi-region SAP HANA on EC2 deployment, design an automated upgrade path for SAP NetWeaver stack from 7.50 to 7.52 with zero downtime using only AWS-native services. Include blue/green deployment via CodePipeline/CodeDeploy and Route 53, SSM Automation for upgrade steps, rollback triggers, and post-cutover SAP health verification?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Airbnb","Anthropic","OpenAI"]},{"id":"q-1942","question":"You manage an SAP HANA MDC deployment on EC2 across two AWS regions and two accounts. Design a policy-driven, automated patch rollout for SAP kernel and HANA patches using only AWS-native tools, with zero-downtime, predefined maintenance windows, and automated rollback. Include staging, validation, auditing, and a runnable Step Functions workflow that triggers SSM Run Command patches and reports success?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Hashicorp","Scale Ai","Snowflake"]},{"id":"q-2067","question":"Design a daily cost-performance monitoring solution for SAP HANA on EC2 using AWS-native services that alerts when savings potential exceeds 20% or utilization thresholds are breached for consecutive days?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Anthropic","Apple","Oracle"]},{"id":"q-2071","question":"Scenario: You operate a single-region SAP NetWeaver + HANA on EC2. A quarterly kernel upgrade and patch cycle must be performed with minimal downtime. Design a blue/green rollout using only AWS-native services: two identical stacks (Blue/Green) across the same VPC/AZs, Route 53 weighted routing, SSM automation for kernel patching, CloudFormation templates for reproducibility, and automated health checks (SAP status, HANA availability, sample ABAP batch). Include rollback steps and metrics?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Meta","Microsoft"]},{"id":"q-2148","question":"Design a beginner-friendly, AWS-native license-compliance workflow for SAP HANA on EC2 in a single region. Use AWS License Manager to model SAP licenses, collect SAP and OS license data via a daily SSM Run Command, compare against entitlements, and publish a JSON report to S3 with an SNS alert on non-compliance. Include data flow, services, and a minimal Lambda snippet to fetch License Manager data and SAP status?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Adobe","Microsoft","Snowflake"]},{"id":"q-2230","question":"Design a beginner-friendly, AWS-native transport validation workflow for SAP NetWeaver on EC2 in a single region. Outline a minimal CI/CD pipeline (CodeCommit/CodePipeline) that builds transports, stores artifacts in S3, uses a Lambda to validate transport naming, sequence, and signature, requires a manual approval before import to QA, and triggers a CloudWatch alert on failure?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["LinkedIn","Meta","NVIDIA"]},{"id":"q-2439","question":"Design a multi-tenant backup strategy for SAP HANA on EC2 across AWS accounts and regions, ensuring tenant data isolation. Outline AWS Backup usage with per-tenant vaults, cross-account roles, and cross-region replication to S3 encrypted with KMS CMKs. Define per-tenant retention, automated restore tests via Lambda, and a checksum verification step to confirm data integrity. Address RPO/RTO?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Apple","Coinbase"]},{"id":"q-2446","question":"Design an AWS-native cross-region SAP HANA MDC deployment with strict network segmentation. Explain how to implement per-SAP-module security groups, Transit Gateway topology, and DR failover readiness using only AWS-native services. Include trade-offs and a minimal IaC snippet to enforce module boundaries?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["IBM","Netflix","Tesla"]},{"id":"q-2492","question":"In an SAP HANA MDC cluster on AWS EC2 spanning three AZs, design a zero-downtime patching workflow for SAP kernel, HANA, and OS, using rolling upgrades with one node online as primary while others are secondary. Include prechecks, quiesce/resume steps, data-consistency validation, automated rollback, and monitoring with CloudWatch/SNS. Provide an IaC outline and a minimal Lambda snippet to trigger prechecks and start patching?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Citadel","Netflix","Tesla"]},{"id":"q-2527","question":"In a two-region AWS SAP HANA MDC deployment (prod in us-east-1, DR in eu-west-1), design a zero-downtime module upgrade strategy using only AWS-native services. Include per-module traffic isolation, rolling upgrades, cross-region replication, automated DR tests, and a minimal IaC snippet to enforce the module boundaries?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Lyft","Snowflake","Tesla"]},{"id":"q-2554","question":"In a single-region SAP HANA on EC2 deployment, design a beginner-friendly backup/restore workflow using only AWS-native services. Use AWS Backup to schedule daily SAP HANA backups, store backups in S3 with versioning, verify integrity via a Lambda triggered over SSM that runs an SAP backup verification command, and publish a JSON report to S3 with an SNS alert on failures. Include data flow and a minimal Lambda snippet?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Apple","Lyft","Oracle"]},{"id":"q-2578","question":"Design a blue/green upgrade workflow for SAP NetWeaver + SAP HANA on EC2 across prod and a DR AWS account using only AWS-native services. Include traffic cutover with an ALB, ABAP transport isolation, RFC connectivity, <2 minutes downtime, and rollback. Provide data flow and a minimal IaC snippet to implement the switchover?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Citadel","Salesforce","Uber"]},{"id":"q-2689","question":"In an AWS SAP deployment (SAP HANA on EC2 + SAP NetWeaver), design an advanced observability and cost-optimization strategy that ties SAP-layer metrics to AWS resource usage. Include data flow, services used, alerting, and a minimal snippet showing how to emit a custom CloudWatch metric from a Lambda that pings SAP and reports latency?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Bloomberg","Databricks","Snowflake"]},{"id":"q-2828","question":"In a multi-account AWS setup hosting SAP HANA on EC2 with SAP NetWeaver and SAP Fiori, design a secure, AWS-native patch and upgrade workflow for OS and SAP components that minimizes downtime. Include maintenance windows, canary deployments, rollback, and automated pre/post checks; provide a minimal Step Functions + SSM snippet?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Google","Hashicorp","IBM"]},{"id":"q-3026","question":"In a single-region AWS SAP deployment (SAP NetWeaver + SAP HANA on EC2), design a beginner-friendly patch management workflow using AWS Systems Manager Patch Manager to apply SAP and OS patches during a maintenance window. Include how patches are selected, how SAP health is validated post-patch, and how a JSON patch report is stored in S3 with an SNS alert on failures. Provide a minimal Lambda snippet to initiate patch runs and report status?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Instacart","Stripe","Uber"]},{"id":"q-3084","question":"In a multi-account AWS SAP deployment (SAP HANA on EC2 + SAP NetWeaver across two regions), design an automated SBOM workflow for software compliance. You must collect SAP component versions, OS patches, and third‑party libraries; generate SPDX/CycloneDX; publish to a versioned S3 bucket; and trigger a governance alert if non‑compliant. Include a minimal Lambda snippet that assembles SBOM data from SAP metadata and pushes it to S3?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Airbnb","Bloomberg","Citadel"]},{"id":"q-3185","question":"Design a hybrid SAP on AWS architecture to minimize latency for a global user base while central SAP HANA runs in a primary region. Propose a split where SAP HANA on EC2 handles core processing in Region A, and edge-facing SAP NetWeaver/UI runs on Local Zones/Outposts for proximity; replicate HANA to a DR region with automated failover. Include data flow, DR testing approach, backups to S3, security model (IAM, SCP/KMS), and a minimal IaC snippet to bootstrap the DR workflow?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Airbnb","Cloudflare","Discord"]},{"id":"q-3203","question":"Design a policy-driven, multi-account SAP on AWS deployment with automated right-sizing and scale-out. How would you implement end-to-end using Compute Optimizer and Cost Explorer for instance selection, Application Auto Scaling for App Servers, SAP policies for HANA scale-out, and a latency-driven rebalancing workflow via a Lambda-driven CloudWatch metric and Step Functions?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Google","Stripe"]},{"id":"q-3335","question":"In a single-region AWS SAP deployment (SAP NetWeaver + SAP HANA on EC2), design a beginner-friendly automated backup strategy using AWS Backup to manage SAP HANA and OS backups. Specify daily incremental and weekly full backups, retention, S3 lifecycle to Glacier, and a monthly restore test. Include IAM/policy basics, a minimal IaC snippet to create the backup plan, and how to monitor success?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Databricks","IBM"]},{"id":"q-3385","question":"In a single-region SAP NetWeaver + SAP HANA on EC2 deployment, design a beginner-friendly IAM model where the SAP EC2 instance reads DB credentials from Secrets Manager, publishes health metrics to CloudWatch, and runs maintenance via SSM with least privilege. Include a concrete IAM policy for the SAP role, trust relationship, and a minimal IaC snippet to attach it?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Salesforce","Uber"]},{"id":"q-3438","question":"In a multi-account AWS SAP deployment with SAP HANA on EC2 and SAP modules deployed across accounts, design an automated governance workflow that enforces per-SAP-module IAM roles and SCPs using only AWS-native services. Describe how you map SAP modules to IAM roles, implement least-privilege via SCPs, monitor drift with AWS Config, and generate a daily JSON compliance report in S3; include a minimal CDK snippet to create the module-role bindings and a restrictive SCP?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Google","Snap"]},{"id":"q-3497","question":"In a single-region AWS SAP deployment (SAP HANA on EC2 + SAP NetWeaver), design a beginner-friendly security baseline that enforces least privilege for SAP operators and AWS access. Include 1) IAM roles and policies, 2) security groups and network controls, 3) VPC endpoints, and 4) how SAP authorizations map to these controls. Provide a minimal CloudFormation snippet to create the baseline IAM role and policy?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Cloudflare","Goldman Sachs","Robinhood"]},{"id":"q-3575","question":"In an AWS SAP deployment (SAP HANA on EC2 + SAP ABAP/NetWeaver) that requires a compliant software supply chain for ABAP add-ons, design an AWS-native process to build, sign, and deploy ABAP add-ons across environments. Include identity and access boundaries, artifact storage, signing process, deployment gating, and rollback strategy. Provide a minimal IaC snippet to create the pipeline role?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["MongoDB","Robinhood","Tesla"]},{"id":"q-3665","question":"In a single-region AWS SAP deployment (SAP HANA on EC2 + SAP NetWeaver), implement a beginner-friendly cost-control and tagging strategy. Describe (1) a tag schema for SAP resources (Environment, SAPInstance, CostCenter, AppOwner), (2) how to enable cost allocation and generate a monthly S3 cost report, and (3) a minimal CloudFormation snippet to create a basic AWS::Budgets Budget alert and IAM policy for exporting reports. Include one sample tag set and justification for thresholds?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Amazon","Anthropic","Apple"]},{"id":"q-3720","question":"Design an advanced DR strategy for a multi-region AWS SAP deployment (SAP HANA on EC2 + SAP modules) spanning two AWS regions and multiple accounts. Achieve RPO in minutes and RTO under 60 minutes. Include SAP HANA replication topology, cross-region backups, Route 53 failover, IAM roles, and a DR test plan with automated validation. Provide a minimal CloudFormation snippet to create the cross-region failover IAM role?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Anthropic","Microsoft","Twitter"]},{"id":"q-3931","question":"Design an advanced, cross‑account AWS SAP security model for SAP HANA on EC2 in a multi‑account environment. Scenario: Tesla consolidates SAP ops in a Shared Services account; Nvidia uses per‑BU accounts. Implement Just‑In‑Time (JIT) access for SAP operators while enforcing least privilege for AWS principals. Map SAP authorizations to AWS roles and SCPs, and secure network boundaries with VPC endpoints and SGs. Include a minimal CloudFormation snippet to create the cross‑account role?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["NVIDIA","Tesla"]},{"id":"q-3995","question":"In a multi-account AWS SAP HANA on EC2 deployment with EU data residency requirements, design an end-to-end data egress and access model that enforces private data paths, uses PrivateLink and VPC endpoints, and maps SAP authorizations to IAM roles with SCPs. Include a minimal IaC snippet to provision PrivateLink endpoints and a compliance gate to detect public egress?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Airbnb","Apple"]},{"id":"q-4041","question":"For a two-region AWS SAP HANA on EC2 deployment, design a beginner-friendly DR plan that provides cross-region resilience. Include (1) SAP HANA System Replication across regions with a clear failover process, (2) DNS failover with Route 53, (3) cross-region backups to S3 and lifecycle to Glacier, (4) a minimal CloudFormation snippet to create the DR IAM role and the S3 bucket, and (5) defined RPO/RTO targets with a simple restore workflow?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Adobe","Google","Meta"]},{"id":"q-4063","question":"In a multi-account AWS SAP HANA on EC2 deployment with data residency constraints, design a cost-aware backup and archival strategy using AWS Backup, S3 lifecycle, and Glacier Deep Archive. Specify backup frequency, retention, cross-region DR triggers via EventBridge, and how to enforce tagging, encryption, and IAM boundaries. Include a minimal CloudFormation snippet to create the backup vault, a plan, and a cross-region recovery setup?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Google","Snowflake"]},{"id":"q-4085","question":"In a multi-account AWS SAP HANA on EC2 deployment used by multiple BU lines, design an automated reconciliation pipeline that ensures SAP authorization mappings from SAP GRC align with IAM roles and SCPs. On SAP authorization changes, the system should update IAM policies within 5 minutes, log drift, and alert on mismatch. Outline components, data model, events, and a minimal CloudFormation snippet to provision the reconciliation role and an EventBridge rule that triggers on GRC change events?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Apple","Coinbase","Stripe"]},{"id":"q-4329","question":"Design an automated, auditable incident response pipeline for a multi-account AWS SAP deployment (SAP HANA on EC2 + SAP NetWeaver) that detects SAP service degradation in production and automatically remediates by restarting SAP components, selecting a safe rollback, and optionally failing over to a DR region. Use EventBridge, Step Functions, AWS SSM Automation, and CloudWatch logs; ensure cross-account IAM permissions, least privilege, and provide a minimal IaC snippet to deploy the cross-region Role and a basic Step Functions state machine?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Apple","Hugging Face"]},{"id":"q-4349","question":"Design a patch management and blue/green upgrade strategy for SAP HANA on EC2 in a multi-account, multi-AZ AWS SAP deployment. Include how patches are validated in a dedicated Patch Lab VPC, promotion via canary, coordination with SAP SWPM upgrade, rollback plan, and compliance/audit tracing. Outline the IaC required to bootstrap the patch-lab environment (SSM roles, IAM policies, VPC peering, and cross-account access)?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Bloomberg","Lyft","Robinhood"]},{"id":"q-4451","question":"In a two-region AWS SAP landscape (SAP HANA on EC2 + SAP NetWeaver) spanning us-east-1 and eu-west-1, design a real-time drift-detection framework for SAP components that watches IAM roles, security groups, and PrivateLink endpoints. Outline automated remediation with SSM Automation and Step Functions, cross‑account permissions, and provide a minimal CloudFormation snippet to deploy a cross-region Config rule and remediation role?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Cloudflare","Google","Tesla"]},{"id":"q-4475","question":"In a single-region AWS SAP HANA on EC2 deployment, design a beginner-friendly backup and disaster recovery plan using AWS-native services. Include 1) an AWS Backup vault and daily backup plan for SAP HANA data and logs, 2) a lifecycle rule to move older backups to S3 Glacier Deep Archive, 3) a simple cross-region DR option, and 4) a minimal CloudFormation snippet to create the vault and plan?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Instacart","Meta","Snap"]},{"id":"q-4564","question":"In a single-region AWS SAP deployment (SAP HANA on EC2 + SAP NetWeaver), design a beginner-friendly daily health-check pipeline that validates SAP instance status and basic OS metrics; the pipeline should: 1) run SAPControl checks on all SAP instances, 2) fetch CPU/memory from CloudWatch for the host, 3) publish a JSON report to S3 and trigger SNS alerts on any non-green result. Include a minimal CloudFormation snippet to create the Lambda execution role?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Discord","Instacart","Square"]},{"id":"q-4696","question":"In a multi-region SAP HANA on EC2 + SAP NetWeaver deployment, design an automated, cross-region SAP backup and restore verification pipeline that guarantees RPO <= 15 minutes and RTO <= 1 hour. Use AWS Backup vaults, cross-region replication, immutable S3 backups, and SAP HANA backint for data/log backups. Include security controls, cross-account access, and a minimal IaC snippet to create the vaults and roles?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Cloudflare","Google","MongoDB"]},{"id":"q-4740","question":"Design a beginner-friendly disaster-recovery (DR) workflow for SAP HANA on EC2 across two availability zones within a single AWS region. Outline: encrypted SAP backups to S3 with versioning, AWS Backup retention, a Lambda-driven restore path triggered by a CloudWatch alarm, and Route 53 failover. Include a minimal CloudFormation snippet to create the DR execution role and a backup policy?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Cloudflare","Microsoft","Netflix"]},{"id":"q-673","question":"You manage a small SAP NetWeaver footprint on AWS using EC2 for the app tier and a separate DB tier on HANA. Describe a practical single-region HA and backup plan to meet an RPO of 15 minutes and an RTO of 60 minutes. Include chosen services, EC2 sizing approach, storage strategy (EBS/S3), backup schedule, and a cost-conscious trade-off you’d consider?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Microsoft","Robinhood"]},{"id":"q-900","question":"Scenario: You manage a single-region SAP NetWeaver deployment on AWS with a separate SAP HANA DB on EC2. You need a beginner-friendly, cost-conscious maintenance workflow that automates OS patching and SAP kernel upgrades using only AWS native services, with minimal downtime. Outline the steps, services, and a sample two-hour weekly maintenance window, including how you validate success and perform rollback?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Cloudflare","Hashicorp","Zoom"]},{"id":"q-1302","question":"A data pipeline in Account A must read daily compressed data from a bucket in Account B, with no Internet access and least privilege. Propose a practical cross-account access pattern using a role in Account B that can be assumed by a service role in Account A, a bucket policy, and a CMK policy. Include how you would validate that only authorized principals can access and that encryption keys are used and rotated?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Hashicorp","NVIDIA","Uber"]},{"id":"q-1345","question":"In a multi-account setup, a Lambda in Account B must read a secure string from Parameter Store in Account A, encrypted with a CMK in Account A. The environment has no Internet access. Outline a concrete cross-account pattern: (1) IAM role trust, (2) Parameter Store policy, (3) CMK policy, and (4) plan to verify access and CMK rotation?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Citadel","Cloudflare"]},{"id":"q-1428","question":"Scenario: A SaaS app serves 100 tenants. Each tenant stores data in its own S3 bucket encrypted with a per-tenant CMK (SSE-KMS). The backend uses a per-tenant role to access both the bucket and key. Design the IAM roles, bucket policies, and KMS key policies to enforce strict per-tenant isolation, automatic CMK rotation, and auditable access. Include how you’d validate no cross-tenant access and rotate status?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Google","Hugging Face","Snowflake"]},{"id":"q-1574","question":"In a multi-account, multi-region data lake, design a zero-trust cross-account analytics boundary that ensures only approved principals can read S3 data via private endpoints, while data at rest is encrypted with a CMK in a centralized security account that rotates automatically. Include CMK and bucket policies, Lake Formation permissions, VPC endpoints/PrivateLink use, cross-account roles, and how you would validate no privilege creep and immutable logs?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Lyft","Two Sigma"]},{"id":"q-1593","question":"In a cross-account ETL job: a service in Account A needs to copy only files tagged ETL=Allowed from a bucket in Account B, over a private VPC endpoint, with data at rest encrypted by a central CMK. Design the least-privilege pattern: trust policy for a cross-account role, bucket and KMS policies, and how you would enforce per-object tag-based access. Include how you’d validate no privilege creep and that logs are immutable?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Google","NVIDIA","Uber"]},{"id":"q-1612","question":"In a multi-account setup, a worker in Account A must read API credentials stored in Secrets Manager in Account B, with automatic rotation and centralized auditing. Propose a concrete pattern using a cross-account Secrets Manager resource policy, an IAM role in Account A, a CMK in a Security account with rotation, and a least-privilege policy for the worker. Describe validation steps to detect privilege creep and ensure immutable logs?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Bloomberg","Microsoft","Uber"]},{"id":"q-1792","question":"Scenario: A data processor in Account A must read encrypted inputs from bucket 'data-inbound' in Account B, decrypt with a CMK in the Security account, and write results to 'data-out' in Account B. No Internet access. Propose exact cross-account roles, trust policies, bucket policies, and KMS key policy to grant least privilege; show how you would test that only the intended principals can access, and how to ensure immutable auditing logs?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Bloomberg","Discord","Microsoft"]},{"id":"q-1870","question":"Design a federation pattern for querying a shared S3 data lake from multiple AWS accounts using Athena workgroups, cross-account IAM roles, and Lake Formation with ABAC-based dynamic data masking. Ensure least privilege, private connectivity (VPC endpoints), central CMK encryption with rotation in the Security account, immutable audit logs, and automated drift/privilege creep detection. Include IAM roles, bucket policies, Lake Formation permissions, KMS key policy, and a test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Databricks","Lyft","Meta"]},{"id":"q-1897","question":"Scenario: An ETL job in Account A reads from a shared data lake in Account B and writes results to a bucket in Account C. Design an ephemeral, least-privilege cross-account access pattern using STS AssumeRole, ABAC tags, Lake Formation permissions, VPC endpoints, and a centralized CMK in the Security account with rotation. Include trust, session policies, encryption, audit, and a test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Databricks","PayPal"]},{"id":"q-1929","question":"You need to share a subset of an S3 data lake between two AWS accounts. Design a least-privilege cross-account pattern that grants read access to specific prefixes via a dedicated role, enforces access only over S3 VPC Endpoints, and logs access with immutable logs; include bucket policy, IAM role trust, and a test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Apple","Cloudflare","Hugging Face"]},{"id":"q-2012","question":"Design a secure cross-account data-processing pattern for a multi-account AWS setup where data flows from a private S3 data lake in Account A to Account B for processing, with a central Security account handling encryption keys. Include ABAC-based access via STS AssumeRole, Lake Formation permissions, VPC endpoints, and immutable logs. Consider an EventBridge trigger from a partner account and private connectivity, with least privilege and automatic key rotation?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Coinbase","Microsoft","MongoDB"]},{"id":"q-2043","question":"A Lambda-backed API in Account A reads DB credentials from AWS Secrets Manager. Design a minimal, auditable approach to rotate that secret automatically with zero downtime. Include cross-account access if the DB is in another account, the required IAM roles/policies, and a concrete test plan to validate rotation without API downtime?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Hashicorp","Oracle","Stripe"]},{"id":"q-2068","question":"Design a beginner-friendly secure CI/CD pipeline for deploying Lambda functions across three AWS accounts (dev, stage, prod) using GitHub Actions. Enforce least privilege with per-environment execution roles, separate cross-account deployment roles, and approval gates. Include IAM trust policies, role permissions boundaries, artifact handling, Secrets rotation, and a basic test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Adobe","Microsoft","Tesla"]},{"id":"q-2112","question":"Design an on-demand data science workspace using AWS SageMaker notebooks for researchers in a multi-account data lake. Enforce ABAC with data-tag constraints, Lake Formation permissions, per-notebook IAM roles, and cross-account trusts. Require private connectivity via VPC endpoints, centralized CMK rotation, immutable audit logs, and automated drift/privilege creep detection. Provide IAM roles, KMS, Lake Formation, and a test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Adobe","Instacart","LinkedIn"]},{"id":"q-2233","question":"A new project runs a REST API in an AWS account using API Gateway + Lambda, which must be consumed by a partner in a different AWS account. The partner should be able to read a specific S3 data bucket and invoke one Lambda function, with no other access. Design a minimal, auditable cross-account access pattern using a dedicated role in your account that the partner can assume. Include the trust policy, the permissions policy, least-privilege scoping, logging via CloudTrail, and a simple test plan to validate it?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Google","IBM","Robinhood"]},{"id":"q-2270","question":"Design a beginner-friendly cross-account access pattern: analysts in accounts B and C need time-limited access to s3://data-lake/teams/alpha/ for a day. Use a central role in account A named DataLakeAnalyst, MFA, a 1‑day STS session, and a session policy granting s3:GetObject and s3:ListBucket on that prefix plus kms:Decrypt for the data key. What are the roles, policies, and checks?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Databricks","Lyft","Meta"]},{"id":"q-2380","question":"Design a beginner-friendly security baseline for a web app using API Gateway, Lambda, S3, and DynamoDB in a single AWS account. Ensure private connectivity via VPC endpoints, enforce least-privilege IAM roles for compute, rotate DB credentials with Secrets Manager, enable at-rest encryption, and provide an auditable test plan with immutable logs?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Cloudflare","MongoDB","Snowflake"]},{"id":"q-2417","question":"Scenario: A production S3 bucket 'company-reports-prod' stores customer reports. A trusted vendor in a separate AWS account needs temporary, read-only access to objects under the 2025/Q4 prefix for 48 hours. Design a minimal cross-account access pattern using STS AssumeRole, a restricted trust policy, a least-privilege role policy, and a bucket policy. Include revocation, logging, and a basic test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Apple","Discord","PayPal"]},{"id":"q-2461","question":"You manage a web app with API Gateway + Lambda uploading files to a private S3 bucket. Design a secure, beginner-friendly flow using pre-signed URLs (short TTL) to allow client uploads without public S3 access. Include IAM roles, bucket policy restricting the signer, SSE-KMS, VPC endpoints, and an automated test plan that validates access control and immutable logs. What steps would you implement?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Apple","Databricks","Uber"]},{"id":"q-2474","question":"Scenario: You manage a shared data lake across accounts A, B, and C. A partner account D needs to run quarterly Athena queries on a restricted S3 subset, with **private connectivity**, CMK encryption in a central Security account with **monthly rotation**, and immutable audit logs. Design cross-account access (roles, policies, Lake Formation permissions, KMS key policy), ABAC tags, VPC endpoints, and an automated test plan to detect drift and privilege creep; include revocation and incident response steps?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Bloomberg","Robinhood","Twitter"]},{"id":"q-2498","question":"Design a beginner-friendly secure image upload workflow in AWS: a Lambda generates a presigned PUT URL for an S3 bucket. Enforce Content-Type image/* and max 5 MB on upload, bucket policy restricts PutObject to that Lambda with SSE-KMS encryption, and rotate CMK automatically. Send CloudTrail logs to a separate audit bucket with immutable Object Lock. Include a simple test plan and IAM policy sketches?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Discord","Goldman Sachs","Snowflake"]},{"id":"q-2633","question":"In a 3-account AWS Organization SaaS setup, require every new S3 bucket to: (1) default-encrypt with a CMK in the Security account, (2) Block Public Access, (3) bucket policy enforcing x-amz-server-side-encryption and TLS 1.2, and (4) allow access only from VPC endpoints. Propose an automated enforcement pattern using SCPs, IAM boundaries, KMS key policy, and a CloudFormation/CDK template plus a comprehensive test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Cloudflare","DoorDash","Lyft"]},{"id":"q-2654","question":"You run a cross-account data-processor: a Lambda in Account A reads a DB credential stored in Secrets Manager in Account S and writes results to a central S3 bucket in Account A. Design a beginner-friendly pattern using a cross-account IAM role, a resource-based policy on the secret, a CMK in the security account, and private connectivity via VPC endpoints. Include a concrete test plan to verify least privilege and immutable logs. What steps would you implement?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["DoorDash","Google","Oracle"]},{"id":"q-2825","question":"In a multi-account AWS data lake, design an automated breach containment pattern triggered by GuardDuty findings that revokes cross-account trust, rotates the central KMS CMK, refreshes Lake Formation and bucket policies, and forces re-authentication for principals, while routing immutable logs to a dedicated S3/CloudWatch pipeline. Include playbooks and rollback?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Databricks","Lyft"]},{"id":"q-2853","question":"Design a private, cross-account API surface for a data-access service using API Gateway and Lambda, where external clients authenticate via mTLS and STS-based short‑lived credentials grant access to data stored in S3 across accounts. Enforce ABAC-based Lake Formation data masking, central KMS CMK rotation in the Security account, private connectivity via VPC endpoints and PrivateLink, immutable audit logs, and automatic drift detection. Include IAM roles, KMS and Lake Formation policies, and a concrete test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Google","MongoDB"]},{"id":"q-2868","question":"In a two-account AWS setup, a Lambda function running in Account B must read objects from s3://lake-A/data/raw in Account A and write results to a DynamoDB table in Account A. Design a minimal, auditable cross-account pattern with: (1) explicit IAM trust for the Lambda's role, (2) least-privilege S3 and DynamoDB policies, (3) bucket and KMS controls, and (4) an automated test plan that validates privilege boundaries and immutable audit logs. What steps would you implement?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Anthropic","Lyft"]},{"id":"q-3063","question":"In a multi-account AWS org serving multiple units, an attacker is suspected of creating cross-account IAM roles and an abnormal trust policy to access data across accounts. Design an automated incident response workflow that detects, quarantines, and recovers from this compromise. Include detection signals (GuardDuty, CloudTrail), credential revocation, SCP/IAM isolation, secret rotation, CMK rotation, immutable log preservation, and a post-incident audit plan - how would you test and validate this workflow?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Microsoft","PayPal","Uber"]},{"id":"q-3101","question":"In a multi-account AWS data lake, design an automated exfiltration guardrail that detects anomalous data egress via Macie findings and GuardDuty, immediately blocks the offending cross‑account principal by updating cross‑account trust and revoking tokens, rotates the central CMK in the Security account and re-encrypts affected data, and preserves immutable audit logs. Include trust changes, session policies, Lake Formation permissions, and a test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Discord","Scale Ai"]},{"id":"q-3124","question":"Design a beginner-friendly secure data ingestion path for a partner feed into S3 using API Gateway, Lambda, and DynamoDB for metadata. Use Secrets Manager for an API key, per-tenant IAM roles, S3 server-side encryption with a CMK, and an immutable audit trail via CloudTrail and S3 Object Lock. Include a concrete test plan and failure scenarios?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Adobe","MongoDB","Snowflake"]},{"id":"q-3151","question":"Design a tenant-scoped access model for a multi-tenant analytics service on AWS: store each tenant’s data under a single S3 bucket with per-tenant prefixes; a central Lambda processor in a separate account ingests and writes results back to each tenant. Enforce least privilege with ABAC tags, cross-account roles, private connectivity via VPC endpoints, a CMK in the Security account with automatic rotation, and S3 Object Lock for immutability. Include IAM trusts, bucket policies, KMS policy, and a practical test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["MongoDB","Snap","Tesla"]},{"id":"q-3172","question":"Design an automated incident response workflow for credential-based compromise in a multi-account AWS environment (Production, Security, Logging). Leverage CloudTrail, GuardDuty, Detective, EventBridge, and Step Functions with cross-account IAM roles and AWS SSO. Include least-privilege controls, S3 log immutability via Object Lock, and centralized KMS policy. Provide the event flow, roles, and a test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Airbnb","Goldman Sachs","LinkedIn"]},{"id":"q-3343","question":"In a three-account org (Data, Analytics, Security), partner data is ingested into a Kinesis stream in Data. Analytics consumes the stream and writes results to Redshift in Analytics. Design a least-privilege cross-account pattern: cross-account role trust, ABAC-based access by partnerId, strict action scoped to the stream and cluster, central CMK with rotation, private connectivity via VPC endpoints, immutable logs, and drift detection with tests. Include IAM roles, policies, KMS, and a test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Hashicorp","Slack","Tesla"]},{"id":"q-3364","question":"Design a just‑in‑time cross‑account data access pattern for a shared analytics lake: contractors from a partner org must be granted time-limited, session-based read access to S3 via Athena across Accounts A (data), B (compute), and C (security). Implement Lake Formation ABAC masking, private connectivity via VPC Endpoints, central KMS CMK rotation, and immutable audit logs. Describe trust setup, session policies, access revocation, and validation steps?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["NVIDIA","Snowflake"]},{"id":"q-3416","question":"Scenario: A Lambda function in Account A must securely read a DynamoDB table located in Account B using a cross-account role; it also fetches a database credential from Secrets Manager in Account A and uses it to call an external API before writing results back to DynamoDB in Account B. Design the least-privilege pattern with private connectivity via a DynamoDB VPC endpoint, a CMK in the Security account, and a test plan for immutable auditing?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Citadel","MongoDB"]},{"id":"q-3549","question":"In a multi-account AWS environment, describe an automated incident response pattern that detects a compromised IAM role via suspicious STS AssumeRole activity and automatically quarantines affected EC2 instances, rotates Secrets Manager credentials, revokes session tokens, and alerts IR teams. Include services used, timing, and audit considerations?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Anthropic","Twitter"]},{"id":"q-3564","question":"Design a beginner-friendly security baseline for a multi-tenant serverless API (API Gateway + Lambda) in one AWS account. Each tenant has a namespace in DynamoDB. Explain per-tenant isolation using IAM conditions with tenant IDs, constrain Lambda to only necessary DynamoDB actions, enable CMK-based per-tenant encryption, and set up an automated audit with CloudTrail immutable logs. Include policy fragments and a test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Airbnb","Citadel","Robinhood"]},{"id":"q-3585","question":"Design a secure, cross-account, event-driven data pipeline: Kinesis Data Streams in Account A feed events to a Lambda in Account B, which writes to an S3 bucket in Account C. Provide a concrete cross‑account trust model, least‑privilege IAM policies, private connectivity via VPC endpoints and PrivateLink, and an encryption strategy with a CMK in a dedicated Security account (rotation enabled). Include a baseline test plan and audit controls?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["NVIDIA","PayPal","Salesforce"]},{"id":"q-3660","question":"**New Angle: Tenant isolation in a single AWS account**: How would you implement per-tenant isolation for a mobile-backend API using API Gateway, Lambda, S3, and DynamoDB, with two Lambdas (auth-service and data-service), Cognito validation, and Secrets Manager DB creds? Provide concrete IAM policies, cross-service permissions, and a repeatable test plan for tenant boundary checks and audit logging?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Google","Snowflake","Twitter"]},{"id":"q-3704","question":"Across three AWS accounts, a contractor portal must grant rare, time‑limited admin access without broad privileges. Design a just‑in‑time access model using STS, per‑session policies, and a permission boundary, with MFA and an automated approval workflow. Enforce private connectivity, immutable auditing, and KMS rotation; include trust policies, session policy examples, and a concrete test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Apple","Google","Netflix"]},{"id":"q-3794","question":"You run a multi-tenant web API on a single AWS account using API Gateway, Lambda, and DynamoDB. Propose a practical, beginner-friendly tenant isolation pattern that uses API Gateway authorizers, Lambda to inject tenant context, and DynamoDB leading keys, with per-tenant IAM conditions, ensuring no cross-tenant access. Outline IAM roles/policies for the Lambda, DynamoDB table design, and a minimal test plan and logging?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Lyft","Slack","Tesla"]},{"id":"q-3899","question":"In a data lake on AWS storing customer PII in S3, design a beginner-friendly security baseline: enable Macie data classification, require SSE-KMS with a rotating CMK, enforce least-privilege IAM roles for data engineers, block public S3 access, and enable CloudTrail, GuardDuty, and Config with a simple test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Meta","Oracle","Salesforce"]},{"id":"q-4007","question":"Design a secure, just-in-time admin access workflow for a multi-account AWS setup (prod/stage/security). Use IAM Roles Anywhere or SSO with MFA to grant temporary admin privileges via STS AssumeRole, enforce least privilege with conditional policies, auto-rotate credentials, and store immutable audit logs (CloudTrail, S3 Object Lock). Include concrete roles, policies, and a test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Apple","Google"]},{"id":"q-4070","question":"Design a secure cross-account event-driven workflow: Account A publishes user activity to a private EventBridge bus; Account B consumes via a restricted cross-account role to process and store results in Account C’s S3. Enforce least privilege, bus policies, and SSE-KMS with a CMK in Security account rotated quarterly; private connectivity (VPC endpoints/PrivateLink), immutable logs, drift checks. Include a test plan with sample events?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Adobe","Google","Snap"]},{"id":"q-4134","question":"A data-processing Lambda in AWS must read only tenant-A data from a single S3 bucket (prefix tenant-A/) and write logs to CloudWatch in the same account. Outline the IAM role policy, the S3 bucket policy, and a test plan to prove per-tenant isolation, encryption, and no public access while ensuring private connectivity?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Airbnb","Goldman Sachs","Hashicorp"]},{"id":"q-4270","question":"Design a cross-account, event-driven access pattern for a private data lake where a partner publishes events to EventBridge in Account A and a Lambda in Account B processes them to write to S3 in Account C. Enforce strict least privilege, private connectivity (VPC endpoints, PrivateLink), ephemeral credentials via STS, and automatic rotation of KMS keys; include a test plan, drift detection, and immutable audit logging?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Discord","Meta","Scale Ai"]},{"id":"q-4331","question":"In a multi-account AWS environment, design and implement a break-glass emergency access workflow for IAM role elevation triggered only via an approved incident channel. Use Step Functions to orchestrate, STS AssumeRole for a time-bound elevated Prod role, MFA, and a restricted action set (EC2/S3/RDS). Ensure auditing, auto-revocation after 4 hours, and anomaly alerts?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Anthropic","Discord","Twitter"]},{"id":"q-4571","question":"In a multi-account AWS setup for a regulated data science platform, data resides in Account A (S3), compute in Account B, and artifacts in Account C. Design a secure private cross-account data path with least privilege, using VPC Endpoints, PrivateLink, cross-account IAM roles, and a CMK in a Security account with rotation. Include IAM trusts, bucket policies, KMS key policy, drift/audit checks and a test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["MongoDB","Scale Ai"]},{"id":"q-4637","question":"For a single AWS account hosting a serverless app (API Gateway, Lambda, S3, DynamoDB), design a beginner-friendly security baseline that minimizes blast radius and ensures auditability. Include (1) a Lambda execution role with least privileges, (2) a private API access pattern (VPC Endpoint or PrivateLink), (3) an S3 bucket policy allowing access only from the Lambda path, (4) DynamoDB encryption with a CMK and rotation policy, (5) enable CloudTrail, Config, and GuardDuty with immutable logs, (6) a simple test plan for validation. Also show how you'd verify no public buckets?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Microsoft","Plaid"]},{"id":"q-4764","question":"Scenario: In a multi-account AWS environment (Prod, Security, Data), a cross-account role shows anomalous activity indicating compromise. Design an incident response workflow to detect, contain, eradicate, and recover while preserving evidence, including trust-policy edits, CMK rotation, and post‑incident validation. What steps and artifacts would you implement?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Apple","Netflix","Salesforce"]},{"id":"q-4796","question":"In a multi-account data lake, a trusted vendor needs access to a subset of encrypted analytics data for a 30-day testing window. Design a secure sharing pattern using AWS RAM, cross-account roles, Lake Formation ABAC data masking, VPC endpoints for private connectivity, and a centralized CMK with rotation. Include IAM trust, bucket/Lake permissions, revocation and a test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Apple","Twitter"]},{"id":"q-853","question":"In a multi-account AWS setup, centralize a logs bucket in Account A that stores CloudTrail and VPC Flow Logs from Accounts B and C. All objects must be encrypted at rest with a CMK in Account A that rotates automatically. Design the KMS key policy, bucket policy, and cross-account IAM roles to allow Account B/C services to encrypt, while preventing decryption except via a centralized IAM role in Account A. Include how you would validate encryption, rotation status, and auditability?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Citadel","IBM"]},{"id":"q-867","question":"Design cross-account data sharing using Lake Formation and S3 that lets Account B write to a shared data lake in Account A and Account C read it via a service role, with a CMK in A that rotates automatically and multi-region, tamper-evident audit logs. Provide IAM/Lake Formation/KMS policies, cross-account trust, and a validation plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Bloomberg","MongoDB","Snap"]},{"id":"q-938","question":"In a two-account data lake, enforce immutable data retention with S3 Object Lock across regions. Buckets in Account A replicate to Account B via CRR. Design the retention policy (COMPLIANCE vs GOVERNANCE), enable automatic CMK rotation, and set cross-account IAM trust for replication. Explain how you'd validate retention, replication integrity, and rotation status?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["DoorDash","MongoDB","Scale Ai"]},{"id":"q-1004","question":"Scenario: A serverless API stack (API Gateway, Lambda, DynamoDB, S3, CloudFront) runs in us-east-1 with a DR region eu-west-1. Propose an automated DR plan using AWS Global Accelerator and Route 53 health checks to failover within 15 minutes. Include data replication choices, Lambda versioning strategy, S3 replication mode, and a safe rollback/verification approach that avoids production impact during tests?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Databricks","Google"]},{"id":"q-1291","question":"In a 12-account AWS Organization, you must implement near real-time central audit logging for VPC Flow Logs, Lambda logs, and RDS logs in a dedicated Logging account. Design the end-to-end mechanism to ship logs from all member accounts to the central account, ensuring secure cross-account access, encryption, and resilience across Regions. What is your approach?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["MongoDB","Salesforce","Scale Ai"]},{"id":"q-1481","question":"In a multi-account AWS Organization for a global SaaS app, mandate data residency: S3 buckets and DynamoDB tables used by customer data must not store or replicate data outside the designated region per account. Design an automated governance solution using SCPs, AWS Config, IAM Roles, EventBridge, and Lambda to enforce this, with testing and alerting?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Google","IBM","Snowflake"]},{"id":"q-1536","question":"Design a DR orchestration for a global app with primary S3 (with Object Lock) and DynamoDB in us-east-1 and a DR site in us-west-2. Require 15 min RTO and 5 min RPO, data residency, automated backups via AWS Backup, cross-region replication that respects residency, and Route 53 failover. Describe a Step Functions workflow to automate failover, validation, and rollback without production impact, with testing cadence and success criteria?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Cloudflare","Hashicorp","Tesla"]},{"id":"q-1542","question":"In a three-account AWS Organization (prod, staging, dev) for a global SaaS app, enforce cost governance by requiring that all new resources carry the tags Owner and Environment in prod, with automated remediation for noncompliant resources and separate budgets/alerts per account. Design an end-to-end process using SCPs, AWS Config rules, EventBridge, Lambda, and AWS Budgets to detect, remediate, and alert. Include testing steps?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Citadel","DoorDash","Stripe"]},{"id":"q-1591","question":"In a multi-account AWS Organization hosting a global service, a misconfigured VPC peering or transit gateway leaks a route to the internet, risking data exposure and egress costs. Design an automated, end-to-end remediation and validation workflow (detection, isolation, rollback) using AWS Config and Config Rules, EventBridge, Lambda, IAM roles, and CloudWatch alarms, plus a controlled traffic test before rollback. What steps and artifacts would you implement?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Amazon","Instacart","Netflix"]},{"id":"q-1634","question":"How would you implement a guardrail so every new EC2 instance in a single AWS account for a mobile backend launches only in Prod-Subnet, carries Owner and Environment tags, and uses IAM role MobileBackendRole, with automated remediation and alerts via AWS Config, Lambda, EventBridge, and SNS while providing a testing plan?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Cloudflare","DoorDash"]},{"id":"q-1672","question":"Across a three-account AWS Org (prod, staging, dev) hosting a Databricks-powered data lake on S3, design an automated end-to-end remediation to ensure all compute resources access S3 only via VPC Endpoints, enforce private DNS, and block public S3 access. Include SCPs, AWS Config rules, EventBridge, Lambda, and GuardDuty findings with testing steps and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Databricks","Hashicorp"]},{"id":"q-1765","question":"Within a multi-account AWS environment across six regions hosting a critical financial service, implement automated detection and remediation of IAM role trust policy misconfigurations that could expose cross-account access. Use AWS Config, Lambda, EventBridge, IAM Access Analyzer, and SCPs in a central Governance account. Outline controls, testing, and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Apple","Goldman Sachs","Lyft"]},{"id":"q-1886","question":"In a 3-account AWS Organization (prod, staging, dev) for a global SaaS app, enforce a global IAM password policy across all accounts: min length 14, require uppercase, lowercase, number, symbol, password age 90 days, and forbid reuse of last 5. Design an end-to-end automation using SCPs, an AWS Config custom rule, EventBridge, Lambda, and AWS Budgets to detect drift, remediate, and alert. Include testing steps?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Hugging Face","Oracle"]},{"id":"q-1934","question":"In a 5-account, multi-region AWS setup (prod, prod-sec, staging, dev, shared) spanning 3 regions, design an automated disaster recovery plan for a critical app running on **EKS** and **RDS**. The DR must auto‑failover **RDS** to cross‑region replicas, switch **Route 53** DNS, re‑sync **EKS** state with **ArgoCD**, rotate encryption keys, and enforce **SCPs** for DR accounts. Include testing, rollback, and post‑drill validation?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["OpenAI","Snowflake"]},{"id":"q-1949","question":"In a 3-account AWS Organization (prod, staging, dev) for a global SaaS app, configure automatic idle-instance remediation. If an EC2 instance in prod has the tag AutoStop=true and CPUUtilization < 5% for 24h, stop it at 02:00 local time. Use AWS Config to detect noncompliance, EventBridge/Lambda to stop, and a Config rule to enforce tag presence. Include testing steps?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Cloudflare","Two Sigma"]},{"id":"q-2009","question":"In a single-region AWS setup hosting a small SaaS app, design a weekly automated DR test that validates restoring an RDS snapshot and at least one EBS volume, then runs a basic end-to-end check against the app. Outline practical steps using AWS Config, EventBridge, Lambda, and a separate DR bucket/account for test artifacts, including rollback steps and how you verify success?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Google","Hashicorp","Square"]},{"id":"q-2046","question":"In a multi-region AWS deployment (prod across 4 regions) with accounts prod, security, infra, and audit, design an automated containment workflow triggered by GuardDuty findings of UnauthorizedAccess or PrivilegeEscalation. Automatically quarantine affected EC2s by swapping in a deny-all security group and routing traffic to a quarantine subnet, while persisting original SGs for rollback. Restoration requires security approval. Include cross-region EventBridge routing, Lambda orchestration, IAM permissions, rollback, and testing?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Coinbase","Meta","Two Sigma"]},{"id":"q-2136","question":"In a 3-account AWS Organization (prod, staging, dev) for a global SaaS app, enforce a secure S3 baseline in prod: no public access, SSE-KMS with a central CMK, and versioning enabled on all buckets. Use AWS Config rules (e.g., s3-bucket-public-access-prohibited, s3-bucket-server-side-encryption-enabled, s3-bucket-versioning-enabled), EventBridge, Lambda, and SCPs to detect, remediate, and alert. Include testing steps and rollback plan?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Instacart","Oracle","Uber"]},{"id":"q-2196","question":"In a 4-account Organization across 3 regions hosting a real-time analytics stack (Kinesis Data Streams, S3 data lake, DynamoDB Global Tables), design an automated DR test and remediation plan that validates cross-region replication, IAM role trust policies, and automated failover with minimal downtime. Include tooling (AWS Config rules, EventBridge, Lambda, SSM Automation), rollback strategy, and testing steps?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Google","Netflix","Oracle"]},{"id":"q-2241","question":"In a multi-account, multi-region production setup hosting a real-time analytics microservice, design an automated DR failover/fallback that shifts traffic to a hot standby region within 5 minutes of regional outage while preserving data consistency. Specify the wiring of Route 53 failover, AWS Global Accelerator, DynamoDB global tables, S3 cross-region replication, and IAM boundaries, plus orchestration via Step Functions/Lambda, post-failover checks, testing, and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Bloomberg","Hashicorp","Stripe"]},{"id":"q-2323","question":"Beginner scenario: A SaaS app stores per-tenant data in a single S3 bucket and serves tenants from a shared app tier. Design a cost- and security-friendly, auditable isolation model using per-tenant S3 Access Points and IAM roles, plus an Org SCP to prevent cross-account bucket listing. Outline required policies, Access Point config, testing, and rollback steps?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Hugging Face","IBM","Tesla"]},{"id":"q-2359","question":"Single-region web app on EC2 behind an Application Load Balancer experiences daily traffic spikes. Outline a beginner-friendly Auto Scaling setup using a Launch Template, an ASG with Target Tracking on CPU, a CloudWatch alarm, and ALB health checks. Include how you would test scaling in and out and rollback if metrics misbehave?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Anthropic","Hugging Face","IBM"]},{"id":"q-2576","question":"In a two-region real-time trading data platform, design an automated security incident response workflow triggered by a GuardDuty finding that an EC2 is compromised. Outline the end-to-end flow: finding → EventBridge → Lambda to (a) attach a restrictive security group, (b) snapshot/quarantine EBS volumes, (c) pause real-time Kinesis streams, (d) copy forensic data to a dedicated S3 bucket, (e) alert on-call via SNS. Include rollback, cross-region considerations, and a test plan?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Google","IBM","Robinhood"]},{"id":"q-2597","question":"In a three-account, multi-region AWS setup (prod, security, shared) spanning two regions, design an automated S3 governance workflow: require production buckets to enable default encryption with a CMK, block public access, and enforce versioning, with centralized cross-account access controls. Outline an end-to-end solution using AWS Config custom rules, Lambda remediation, EventBridge alerts, and Organizations SCPs; include testing steps and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Databricks","Google","NVIDIA"]},{"id":"q-2645","question":"Across a 6-account AWS Organization (prod, staging, dev) spanning three regions, design an automated cross-account S3 security posture that blocks public access and enforces encryption at rest for all buckets. Include automated remediation, drift detection, and rollback using AWS Config, Lambda, EventBridge, IAM Access Analyzer, and SCPs. How would you test and verify?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Google","LinkedIn","Robinhood"]},{"id":"q-2652","question":"Across six AWS accounts in three regions, design a centralized, immutable audit-log pipeline: have all accounts deliver CloudTrail, VPC Flow Logs, and Config logs to a single S3 bucket in an Audit account with Object Lock (Compliance) for 7 years; enforce encryption via a shared KMS CMK and SCPs; use EventBridge and Lambda for delivery health checks and auto-remediation; outline validation, testing, and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Airbnb","Uber"]},{"id":"q-2676","question":"In a single AWS account hosting a regional web app, require that every new resource carries Owner and Environment tags; propose an end-to-end remediation flow using AWS Config Custom Rules, Lambda remediation, EventBridge, and SNS alerts. How would you test it, and what rollback steps would you include? Provide a minimal CloudFormation snippet for the Config rule and remediation Lambda, please?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Google","Hugging Face","Netflix"]},{"id":"q-2718","question":"In a 3-region, multi-account SaaS deployment, enforce customer data locality so S3 data stays in the region defined by each customer and is not replicated cross-region unless explicitly approved. Design an automated policy engine using AWS Config custom rules, EventBridge, Lambda, and SCPs to prevent unauthorized replication, enforce region-bound KMS keys, and route alerts to a centralized security sink. Include testing and rollback steps with a simulated cross-region copy attempt?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Amazon","Cloudflare","Google"]},{"id":"q-2905","question":"Design a lightweight log aggregation pipeline for a single-region AWS deployment (EC2 behind ALB, RDS) that ships application logs and VPC flow logs to a centralized S3 bucket, with structured naming, encryption, and a 30-day retention policy. Include how CloudWatch Logs, Kinesis Data Firehose, and Lambda would be wired, how to monitor for unusual spikes, and basic testing steps?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Anthropic","Meta","Snowflake"]},{"id":"q-2924","question":"Scenario: In a global SaaS with 3 AWS accounts across 2 regions, design an automated cost anomaly detection and remediation workflow that halts non-production resources when daily spend grows by more than 2x over a 7-day rolling window, while preserving prod SLAs. Specify integration of Cost Anomaly Detection, CloudWatch, Lambda, Step Functions, AWS Budgets, and guardrails and testing?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Amazon","Google","Salesforce"]},{"id":"q-2981","question":"In a multi-account AWS environment hosting a global SaaS app, you must quickly detect and isolate a misconfigured OpenSearch domain that is publicly accessible. Design an end-to-end, beginner-friendly workflow using AWS Config rules, EventBridge, Lambda, and Organizations SCPs to detect, auto-remediate (restrict access and enable encryption), alert on-call, and provide a rollback plan. Include testing steps?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Coinbase","Stripe","Uber"]},{"id":"q-3005","question":"In a six-region, multi-account AWS setup hosting a critical data platform, design an automated secrets lifecycle that rotates database credentials and API keys stored in AWS Secrets Manager, revokes stale IAM principals, and enforces cross-account access controls via IAM and resource policies. Include rotation schedules, cross-region replication, audit trails, testing, and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Databricks","Instacart"]},{"id":"q-3049","question":"Across six AWS accounts spanning four regions, design an automated failover strategy for SageMaker endpoints and associated data pipelines to achieve near-zero RTO during regional outages. Detail how Route 53 health checks, Global Accelerator, cross-region S3 replication, DynamoDB Global Tables, IAM trust models, and Step Functions/Lambda orchestration would work. Include testing, rollback, and observability?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Apple","Databricks","Hugging Face"]},{"id":"q-3097","question":"In a 3-region SaaS deployment (prod us-east-1, prod eu-west-1, staging) implement automated containment for a newly detected public S3 bucket containing customer data. Use a Config rule to flag public buckets, EventBridge+Lambda to apply a DenyPublicAccess policy, and SCPs in prod to block exposure. Include testing steps and rollback plan?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Adobe","DoorDash","Hashicorp"]},{"id":"q-3160","question":"Scenario: In a single AWS region, a small app uses an RDS MySQL primary with one read replica to handle read traffic. Design a beginner-friendly auto-scaling approach that creates a second read replica during peak hours and tears it down after. Outline the AWS components, thresholds, cost guardrails, and testing steps?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Bloomberg","Lyft","OpenAI"]},{"id":"q-3197","question":"In a single-region AWS setup for a small web app, design a lightweight log-collection and alerting pipeline. Use CloudWatch Logs as source, a Kinesis Data Firehose delivery stream to S3, and a Lambda function scheduled hourly to compute errors/requests. If hourly error rate > 5%, publish to SNS. Describe components, thresholds, IAM permissions, and a basic test plan?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Coinbase","Hashicorp","Plaid"]},{"id":"q-3284","question":"In a globally distributed 4-region SaaS backend (us-east-1, eu-west-1, ap-southeast-1, ap-northeast-1) with Lambda, DynamoDB, and S3, you must guarantee an RPO of 15 minutes and an RTO of 60 minutes for customer data, with cross-account backups that survive region failure. Design a cross-region backup strategy using AWS Backup, DynamoDB backups, S3 replication, KMS with rotation, and cross-account restoration via IAM roles and SCPs. Include testing plan and cost considerations?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Apple","IBM","Two Sigma"]},{"id":"q-3406","question":"Scenario: A prod IAM key leaks across three AWS regions in a microservices app. Propose an automated, end-to-end incident response that detects, rotates credentials, revokes sessions, isolates the affected account with SCPs, and validates service health using EventBridge, Lambda, IAM, Secrets Manager, and Route 53 health checks. Include testing and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["IBM","LinkedIn","Lyft"]},{"id":"q-3411","question":"In a single-region AWS setup, a data-processing pipeline uses an SQS standard queue triggering a Lambda function that writes results to DynamoDB. Design a beginner-friendly reliability improvement: (1) configure a Dead-Letter Queue for messages that fail processing after N receives; (2) tune Lambda retry behavior and visibility timeout; (3) add CloudWatch alarms and an SNS alert for DLQ growth and high processing latency; (4) provide a minimal runbook and testing steps?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Coinbase","Meta","Netflix"]},{"id":"q-3521","question":"In a three-region AWS multi-account data pipeline using S3 data lake, Glue ETL, and Redshift, design an automated incident response that detects data inconsistency or SLA breach, isolates the affected region, promotes a hot standby copy, and validates end-to-end data integrity before resuming writes. Outline the orchestration with EventBridge, Step Functions, Lambda, IAM roles, cross-region replication, and rollback procedures?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Amazon","Google"]},{"id":"q-3597","question":"In a two-region deployment hosting a 24x7 financial service behind an ALB and an RDS primary with cross-region replication, design an automated DR workflow that fails over from us-east-1 to eu-west-1. Include data-sync approach (RDS cross-region replica vs DynamoDB Global Tables), Route 53 failover, a Lambda control plane for promoting failover, IAM rotation, ALB target switching, S3 replication, and a deterministic rollback. Outline RTO/RPO targets and a test plan?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Databricks","Meta","Salesforce"]},{"id":"q-3676","question":"In a multi-account AWS deployment across two regions, design an automated disaster recovery (DR) plan for a data processing pipeline (S3 data lake, AWS Glue, Lambda, DynamoDB). Primary region us-east-1, standby us-west-2. Achieve RPO <= 5 minutes and RTO <= 15 minutes with automated failover using Route 53, S3 cross-region replication, DynamoDB Global Tables, and a Lambda-driven reconfiguration. Include failover sequence, data synchronization strategy, testing plan, and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["NVIDIA","OpenAI","Snowflake"]},{"id":"q-3732","question":"In a three-region, two-account deployment hosting a real-time financial telemetry pipeline with tight latency, implement fully automated disaster recovery using Aurora Global Database, cross-region S3 replication, Route 53 failover, and Lambda-driven telemetry validation. Outline data integrity checks, failover criteria and rollback, automated DR tests, and cost considerations?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Cloudflare","Coinbase","Robinhood"]},{"id":"q-3793","question":"In a single AWS region, a static React SPA is hosted on S3 and delivered through CloudFront; a Node API runs on EC2 behind an ALB. Design a beginner-friendly guardrail to improve security and cost control: (1) enable a rate-based AWS WAF rule to throttle abusive IPs, (2) enable CloudFront access logs to an S3 bucket with lifecycle, (3) set up a simple AWS Budgets alert for API costs, and (4) outline a practical testing plan to verify the controls?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["DoorDash","Lyft","Snowflake"]},{"id":"q-3853","question":"In a 3-account, 2-region production setup (prod in us-east-1, replica in us-west-2) hosting a real-time analytics platform with S3 data lake, Glue Catalog, Athena queries, and a frontend API behind an ALB, design an automated DR plan achieving RPO ~5 minutes and RTO ~15 minutes. Include cross-region S3 replication, DynamoDB metadata, Glue catalog replication, Route 53 failover with health checks, and a Lambda-driven runbook to switch endpoints, update ALB target groups, and rollback steps. Provide testing plan?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Citadel","Robinhood","Zoom"]},{"id":"q-3955","question":"In a multi-region, multi-account AWS environment hosting a global web app with a primary deployment in us-east-1 and a DR site in eu-west-1, design an automated, auditable failover procedure using Route 53 failover routing, S3 cross-region replication for assets, and RDS cross-region disaster recovery. Include promotion steps, timing targets, logging, and testing plan?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Airbnb","Coinbase","Oracle"]},{"id":"q-4082","question":"In a multi-region AWS deployment, a globally distributed app uses Lambda@Edge to personalize content at the edge and CloudFront to cache responses. When deploying a content update, how would you ensure deterministic cache invalidation across all regions, minimize user latency, and validate rollback strategies? Provide concrete steps and trade-offs?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Cloudflare","Discord","Google"]},{"id":"q-4161","question":"In a global AWS deployment across three regions (us-east-1, eu-west-1, ap-southeast-1) hosting a critical API backed by EC2 behind ALB, DynamoDB, and RDS, design automated cross-region disaster recovery that keeps DynamoDB synchronized via Global Tables, uses RDS cross-region read replicas with automated promotion, and employs Route 53 failover to direct traffic to healthy regions. Include a Lambda-driven DR runbook, measurable RPO/RTO targets, failover criteria, testing plan, and rollback strategy?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Cloudflare","Oracle","Scale Ai"]},{"id":"q-4195","question":"In a single AWS region hosting a web app on EC2 behind an ALB, SSH is accidentally exposed via a Security Group rule to 0.0.0.0/0. Propose a beginner-friendly automated guardrail using AWS Config and Lambda to detect and remediate within a few minutes: (1) detect SSH port 22 exposure, (2) auto-remediate to restrict SSH to a corporate CIDR or revoke 0.0.0.0/0, (3) enforce resource tagging for ownership, (4) send alerts to Slack/Email via SNS, and (5) outline testing steps?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Apple","Plaid"]},{"id":"q-4305","question":"In a multi-account, multi-region environment, a centralized log pipeline ingests VPC flow logs, application logs, and security alerts. Compliance requires logs to be encrypted at rest with per-account CMKs, no PII stored, and an immutable audit trail. Design a centralized architecture using CloudWatch Logs, KMS, and S3 for long-term storage, with cross-account roles and a dedicated audit account. Outline validation and key rotation plan, and testing steps?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Databricks","Goldman Sachs","NVIDIA"]},{"id":"q-4375","question":"In a three-region, multi-account deployment for a real-time vehicle telemetry platform (Kinesis, Firehose, S3, Lambda, DynamoDB, API Gateway) used by external partners, design an automated strategy to prevent data exfiltration via misconfigured VPC endpoints and S3 policies. Use SCPs, Config, Access Analyzer, EventBridge, and Lambda remediation with blue/green rollout; include testing and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Salesforce","Tesla","Zoom"]},{"id":"q-4645","question":"Scenario: A multinational SaaS app runs across four regions with separate accounts per region. Design a disaster recovery strategy that enables active-passive failover for the DB tier and stateless services with RTO < 5 minutes and RPO < 1 minute. Use Aurora Global Database, Route 53 failover, DynamoDB Global Tables, S3 backups, and Step Functions for orchestration. Include data consistency, testing plan, and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Adobe","Amazon","Meta"]},{"id":"q-4673","question":"Scenario: A global SaaS stores data in RDS, EFS, and S3 across regions and accounts. Design an automated backup integrity and retention strategy using AWS Backup, AWS Config, EventBridge, and Lambda that (1) validates cross-region backups daily, (2) detects backup drift against policy, and (3) auto-remediates drift with alerts. Include testing steps and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Cloudflare","MongoDB","PayPal"]},{"id":"q-4698","question":"In a two-account AWS setup (prod and shared-services) with ~200 Windows EC2s across multiple AZs, design an automated patch management workflow using AWS Systems Manager Patch Manager. Requirements: monthly maintenance windows, automatic approval for Critical patches, automated remediation for instances missing patches after 7 days, and audit-ready logs in CloudTrail/Audit bucket. Outline the end-to-end flow using SSM, EventBridge, Lambda, and Config; include testing steps and rollback plan?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Discord","DoorDash","Meta"]},{"id":"q-4772","question":"In a single AWS region, a serverless REST API uses API Gateway + Lambda + DynamoDB. Design beginner-friendly cost-controls: (1) enforce API rate quotas via usage plans for each stage, (2) cap Lambda concurrency with per-function reserved concurrency and a shared pool, (3) enable DynamoDB provisioned throughput max with autoscale, (4) set CloudWatch alarms and a daily AWS Budgets alert. Outline the components, thresholds, and a simple test plan?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Anthropic","Databricks","Netflix"]},{"id":"q-675","question":"You manage a two-region AWS deployment (us-east-1, us-west-2) behind an ALB with private subnets, NAT gateway, and RDS in us-east-1. During business hours, us-west-2 exhibits spike in 5xx errors and higher latency. Outline immediate incident triage steps, AWS CLI commands to run, how you’d identify root causes (NAT saturation, DNS routing, cross-region replication lag), and both short- and long-term mitigations with verification steps?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Apple","Snowflake"]},{"id":"q-982","question":"Design an automated cross-region disaster recovery plan for a globally distributed web app currently active in us-east-1 with a DR site in us-west-2, covering RDS, DynamoDB, S3, and ALB-backed frontend. Specify data synchronization, failover steps, testing, and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Anthropic","Google"]},{"id":"q-1036","question":"Scenario: A new external vendor needs read-only access to a single Azure AD-secured web portal (SaaS app) via B2B. Create a guest user, assign them to a dedicated App Role of the portal, enforce MFA and device-compliant access with a Conditional Access policy restricted to the office IP range, and outline post-grant validation and rollback?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Citadel","NVIDIA","PayPal"]},{"id":"q-1109","question":"How would you enable Self-Service Password Reset (SSPR) for a 20-user Azure AD group, enforce MFA during resets, and ensure auditable reset events with a simple rollback plan? Include licensing notes, enrollment flow, verification steps, and failure handling?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["MongoDB","Stripe"]},{"id":"q-1191","question":"Scenario: A data platform CI/CD pipeline deploys to Data Lake Gen2, Synapse, and a storage account across three subscriptions using a non-interactive service principal. Design a secure, rotating, auditable auth model: App Registration with certificate-based OAuth, Key Vault-stored certs rotated every 30 days, per-resource RBAC with least privilege, automatic revocation on build failure, and validation steps. Include how you would test access during runs and how to roll back changes if needed?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Databricks","Google","PayPal"]},{"id":"q-1304","question":"Scenario: a startup with 5–15 users is moving to Azure AD for the first time. You need to enable basic identity services: (a) create users and groups, (b) provide SSO for a SaaS app using SAML, (c) enforce MFA, (d) automatically assign Office 365 licenses via group membership, and (e) enable self-service password reset for all users. Outline an end-to-end setup plan and a minimal validation checklist prior to go-live?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Discord","IBM","Lyft"]},{"id":"q-1363","question":"Design a cross-tenant temporary access workflow for a contractor to a SaaS portal secured by Azure AD, leveraging Azure Lighthouse, B2B guest accounts, PIM with Just-In-Time activation, Conditional Access (MFA, device compliance, IP restrictions), and an access-review auto-revoke policy. Detail validation during activation and rollback?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Anthropic","Apple"]},{"id":"q-1380","question":"Scenario: Implement Just-In-Time elevation for Azure AD roles using Privileged Identity Management (PIM) to grant temporary Global Administrator and Privileged Role Administrator access. Include activation workflows, MFA requirements, approval routing, auto-expiry, and post-activation auditing. How would you validate elevated access usage and ensure revocation, including staging validation prior to production?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Instacart","LinkedIn","Two Sigma"]},{"id":"q-1427","question":"Implement Just-In-Time privileged access for production admin roles using Azure AD PIM. Requirements: MFA, approval workflow, time-bound activation, auto-expiry, and audit trails. Describe end-to-end steps: role setup, activation process, testing, monitoring, and how to safely handle emergency bypasses?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Adobe","Apple","Cloudflare"]},{"id":"q-1448","question":"How would you architect an end-to-end cross-tenant access governance flow that grants a partner contractor temporary access to Snowflake via Azure AD B2B and Entitlement Management, with MFA, sign-in risk conditions, an approvals workflow, auto-revocation after 10 days, and automated access reviews before renewal?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Coinbase","Netflix","Snowflake"]},{"id":"q-1561","question":"Design a beginner-friendly plan to enforce conditional access for a single Azure AD‑secured internal portal used by contractors. Include which policies you would create (MFA, device-compliance, IP ranges), how you would test rollout with a small pilot, and how you would validate access during the pilot and after, with minimal automation?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Airbnb","Discord","DoorDash"]},{"id":"q-1589","question":"Scenario: You run Azure AD for a multinational org with internal staff and external partners. Secure access to a sensitive SharePoint Online library and a CI/CD portal. Design a concrete end-to-end plan using: (a) Conditional Access with device compliance and sign-in risk, (b) Entitlement Management for guest access with time‑boxed access and automated reviews, (c) Privileged Identity Management for service accounts with Just‑In‑Time, approvals, and MFA, and (d) comprehensive auditing and alerting. Include validation steps and rollback?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Google","Meta","Stripe"]},{"id":"q-1753","question":"Scenario: A regulated fintech uses Azure AD Entitlement Management to grant external QA vendors episodic access to five internal SaaS apps. Design an Access Package strategy: define apps/roles, set time-limited assignments, enforce MFA, approval workflows, and auto-expiry with revocation. Outline validation across apps during the window and post-expiry?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Bloomberg","Robinhood","Snap"]},{"id":"q-1784","question":"Design a risk‑aware Azure AD access plan for two high‑risk apps (CI/CD portal and HR app) in a multi‑tenant setup, using Conditional Access with sign‑in risk, device compliance, and trusted IPs; implement auto‑remediation via Access Reviews and PIM for admins; outline testing, rollback, and auditing while minimizing automation?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["DoorDash","Salesforce"]},{"id":"q-1824","question":"Pilot passwordless sign-in for 10 users accessing a single internal Azure AD‑secured portal. Describe exact steps to enable passwordless (FIDO2 or Windows Hello), enroll devices, register credentials, enforce a Conditional Access policy for that app, run a 2-week pilot, and validate success and rollback procedures if issues arise?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Cloudflare","Google"]},{"id":"q-1876","question":"In a Cloudflare/NVIDIA-style Azure AD tenant, a partner needs access to a single Azure AD-secured portal for 30 days. Outline a concrete plan to grant time-bound access using Entitlement Management (Access Packages): define the package scope and roles, configure approvals, enforce auto-expiry, and audit/validate during the window and after expiry with minimal scripting. Include pilot testing steps?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Cloudflare","NVIDIA"]},{"id":"q-1985","question":"Scenario: You need to onboard external vendors to access a single Azure AD‑secured SaaS app via Azure AD B2B. Draft a beginner‑friendly, end‑to‑end plan to (1) grant guest access through a dedicated security group, (2) auto‑assign the required SaaS license via group membership, (3) enforce MFA for guests with Conditional Access, and (4) set a 14‑day expiry with Access Reviews and a rollback path. Include validation steps during the window and after expiry?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Coinbase","Google","Lyft"]},{"id":"q-2042","question":"Scenario: A regulated fintech team requires time-bound admin access to production Azure AD and resources via Privileged Identity Management (PIM) for a 72-hour window each quarter. Outline an end-to-end plan to (1) assign and activate a PIM-eligible role, (2) require MFA and Just-In-Time activation, (3) implement approvals and alerts, (4) auto-expire and enable post-activation reviews, and (5) validate access during and after the window with minimal scripting?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Anthropic","Coinbase","Snowflake"]},{"id":"q-2110","question":"Architect an identity governance pattern to manage admin access to multiple critical apps (both SaaS and internal portals) in Azure AD across a multi-tenant org. Design an integrated solution using Identity Governance features: Entitlement Management with Access Packages, Access Reviews with auto-remediation, Privileged Identity Management for JIT elevation, and Conditional Access signals to enforce device compliance and MFA. Include lifecycle flows, automation touchpoints, testing, and rollback plan?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Databricks","LinkedIn","NVIDIA"]},{"id":"q-2290","question":"Design a robust 60-day access plan for external contractors to a single Azure AD‑secured internal portal. Include (1) an Entitlement Management access package with a dedicated security group, (2) B2B guest provisioning and license assignment, (3) Conditional Access policies enforcing MFA, device compliance, and mapped IP ranges, (4) a validation plan during the window and a rollback path on expiry, with minimal automation?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Amazon","DoorDash","Zoom"]},{"id":"q-2328","question":"Scenario: A legacy on-prem HR web app is published through Azure AD Application Proxy. A contractor needs 7 days of access. Outline a beginner-friendly, end-to-end plan to (1) create a dedicated security group, (2) assign the app’s App Role to that group for the proxy, (3) enforce MFA and device compliance via Conditional Access for the group, (4) set a 7‑day Access Review with auto-revoke, and (5) validate access during the window and after expiry with minimal automation?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Goldman Sachs","Plaid","Uber"]},{"id":"q-2490","question":"Scenario: A consulting contractor requires temporary access to a mix of 5 internal apps and 3 SaaS services for six weeks. Outline an end-to-end plan using Azure AD Entitlement Management to create a catalog, build an access package, define approval workflows, enforce time-bound access, auto-assign licenses and group memberships, and validate access during and after the window with minimal scripting. Include monitoring and post-expiry audit steps?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Databricks","Goldman Sachs","Tesla"]},{"id":"q-2509","question":"Scenario: External auditors require temporary administrative access to a cross‑tenant data portal. Provide an end‑to‑end plan using Azure AD Privileged Identity Management (PIM) for Just‑In‑Time elevation to Privileged Role Administrator scoped to the portal, with manager approval, MFA on activation, and a CA policy enforcing device compliance. Include expiry, auto‑revoke, and a rollback/audit path with minimal automation?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Cloudflare","Coinbase","Meta"]},{"id":"q-2572","question":"Propose an end-to-end plan to grant time-limited admin access to a production Azure subscription using **PIM**. Include (1) just-in-time elevation with formal approval, (2) mandatory **MFA** and device compliance during activation, (3) narrow RBAC scope (least privilege) for the elevated role, (4) auto-expiry plus periodic **Access Reviews**, (5) robust auditing, validation steps, and a rollback path. Minimize automation; no custom scripts?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["DoorDash","Stripe","Zoom"]},{"id":"q-2775","question":"In a new Azure AD tenant, enable passwordless sign-in for a dedicated project group using Microsoft Authenticator and Windows Hello for Business, while the rest uses password-based MFA. Outline a beginner-friendly end-to-end plan: configure passwordless methods, create a group and a CA policy that enforces passwordless for that group, enroll devices with Intune, run a 1-week pilot, validate sign-ins, and specify fallback/rollback steps?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Citadel","Lyft","MongoDB"]},{"id":"q-2818","question":"In an Azure AD tenant integrated with on-prem AD, a critical maintenance window requires elevated permissions to adjust directory settings and network configs. Outline a just-in-time access plan using Privileged Identity Management (PIM) for an eligible Directory Administrator role, including: (1) role setup and activation policy, (2) approval workflow and MFA requirements, (3) session controls and duration, (4) auditing/alerts, and (5) rollback on expiry?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Snap","Twitter","Two Sigma"]},{"id":"q-2858","question":"Scenario: Contractors from three partner tenants access a single Azure AD–secured portal via Entitlement Management. Design an end‑to‑end cross‑tenant **Access Package**: (1) guest groups mapped to the package, (2) conditional access with CAE‑MFA and device compliance, (3) 21‑day auto‑expire with revoke, (4) automatic license provisioning via Graph, (5) validation steps and rollback?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Cloudflare","Google"]},{"id":"q-2896","question":"Scenario: An external consulting firm requires access to a single Azure AD‑secured SaaS app for 6 weeks. Propose using Entitlement Management (Access Packages) to grant time-bound access with MFA, an approvals workflow, and auto-expiry. Outline end-to-end steps: catalog design, access package, owner approvals, license considerations, post-expiry rollback, and how you would monitor/audit during the window and after expiry?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Discord","MongoDB","Twitter"]},{"id":"q-2921","question":"Design a beginner-friendly plan to publish an internal time-tracking web app to Azure AD with app registration, OpenID Connect (OIDC), automatic user provisioning via SCIM, group-based access control, and a 2-week pilot with validation and rollback steps. Include how you would test login, verify access, and revoke access at the end?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["MongoDB","Snap","Stripe"]},{"id":"q-2965","question":"For a new SaaS integration in Azure AD, 8 users require SSO with AppX. Outline a beginner-friendly end-to-end plan to: (1) define an Enterprise Application for AppX, (2) create a dedicated Azure AD group and add the 8 users, (3) grant access via group membership and enforce MFA for the group, (4) validate access during rollout and after, with a clear rollback path?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Cloudflare","PayPal"]},{"id":"q-3038","question":"Scenario: A contractor must access two Azure AD‑secured resources for 14 days—one SaaS data portal and one on‑prem data gateway. Design a concrete end‑to‑end workflow using Entitlement Management (Access Packages), Privileged Identity Management (PIM) Just‑In‑Time, and Conditional Access to grant, review, and auto revoke access. Include provisioning steps, validation checks during and after the window, and a rollback path if problems arise?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Anthropic","Citadel","Two Sigma"]},{"id":"q-3095","question":"Scenario: Contractors require ephemeral, cluster-scoped Databricks access via Azure AD for 4 hours, auto-revoked. Design end-to-end: (a) time-bound entitlement mapping a temporary group to Databricks cluster roles, (b) Conditional Access (MFA, device compliance, IP), (c) restricts to cluster scope only, (d) audit logs and alerts, (e) rollback if revocation fails. Include validation steps?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Databricks","Instacart","Netflix"]},{"id":"q-3118","question":"Design an end-to-end Azure AD Entitlement Management workflow to grant a contractor temporary, least-privilege access to a restricted data lake (ADLS Gen2) via an access package that maps to RBAC roles and data-scoped permissions. Include approvals, MFA/device posture, expiry, auto-revocation, and validation steps, with minimal automation?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Instacart","Lyft","Uber"]},{"id":"q-3208","question":"Design a Just-In-Time privileged access workflow using Azure AD Privileged Identity Management for Global Admin and a sensitive resource group. Include eligibility, activation with MFA, manager approval, automatic expiration, and audit trails; outline validation steps and minimal automation (no custom scripts)?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Meta","Netflix","Snap"]},{"id":"q-3316","question":"Design a Cross-Tenant Access Settings (CTAS) plan to grant external contractors from PartnerA, PartnerB, PartnerC access to a secured Power BI portal in your Azure AD tenant. Include inbound/outbound CTAS configuration, least-privilege guest roles, MFA and device posture, sign-in risk checks, a 30-day expiry with auto-revocation, and a rollback/validation process for onboarding and expiry?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Apple","Goldman Sachs","Square"]},{"id":"q-3450","question":"You run a cross‑cloud CI/CD pipeline where GitHub Actions workflows in a secure tenant deploy to an Azure subscription and fetch secrets from Azure Key Vault, using GitHub OIDC for authentication. Design a least‑privilege, time‑bound access pattern that requires no custom scripts. Include identity federation, app RBAC, Key Vault permissions, and an auditable validation flow from token issuance to deployment, with expiry and revocation?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Discord","IBM"]},{"id":"q-3486","question":"Scenario: You need to grant temporary, Just-In-Time admin access to a privileged Azure AD role that controls a vendor management SaaS portal. Use Azure AD Privileged Identity Management to require two-person approval, time-bound elevation (1–4 hours), and MFA and device-compliance for elevation, plus an automatic Access Review to revoke elevation after expiry. Outline the end-to-end setup, validation steps, and how you would verify SIEM logs capture the elevation events?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Cloudflare","Hugging Face","Zoom"]},{"id":"q-3600","question":"Describe a beginner-friendly, end-to-end plan to enable passwordless sign-in for a small team using Azure AD, combining Windows Hello for Business and FIDO2 security keys. Include registration policy, device prerequisites, fallback MFA, how you validate sign-in on multiple devices, and a simple rollback if issues arise?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Google","MongoDB","Zoom"]},{"id":"q-3741","question":"Design a secure, end-to-end process for a GitHub Actions workflow that provisions partner users in a second Azure AD tenant via Graph API. The workflow uses a service principal with a certificate-based credential stored in Azure Key Vault, rotated automatically, within a defined time window and with least-privilege permissions. Include expiry, auto-revocation, and validation steps with no custom scripts?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Anthropic","OpenAI","Snap"]},{"id":"q-3754","question":"Scenario: A CI/CD pipeline needs Just-In-Time access to a resource group for 8 hours via a service account. Outline an end‑to‑end plan to enable Azure AD Privileged Identity Management (PIM) for this role, configure manager approval, require MFA on activation, enforce a hard expiry, and implement post‑activation auditing and validation steps to confirm access during the window and revoke after expiry?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Amazon","Salesforce","Zoom"]},{"id":"q-3804","question":"Scenario: You want to pilot passwordless sign-in for an Azure AD‑secured SaaS portal used by a team of 10. Outline an end‑to‑end plan to enable passwordless sign‑in using FIDO2 security keys and Windows Hello for Business, with a fallback MFA option, device enrollment via Intune, and a Conditional Access policy scoped to the portal. Include a 2‑week pilot validation plan and a rollback path if issues arise?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Adobe","PayPal"]},{"id":"q-3883","question":"Scenario: Grant temporary access to a single Azure AD‑secured SaaS app for external contractors for 5 days. Outline a beginner plan that (1) creates a dedicated guest group, (2) uses a lightweight Conditional Access policy to require MFA, (3) enables sign-in auditing with basic alerts for unusual activity, and (4) validates access during the window and after expiry with minimal automation?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["DoorDash","Microsoft","MongoDB"]},{"id":"q-3988","question":"Publish an Access Package in Entitlement Management, assign it to a contractor group, and route approvals to a manager. Enforce MFA and device state via conditional access, set a 10-day expiry with auto‑revoke, and require a quick access review on expiry. Validate by signing in and confirming app access during window; after expiry, confirm access is blocked. Rollback: remove the package or revoke group membership?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Cloudflare","IBM"]},{"id":"q-4026","question":"Scenario: hundreds of Azure AD App Registrations (service principals) are used by CI/CD pipelines across multiple subscriptions. Some pipelines still rely on long-lived client secrets. Propose an end-to-end plan to enforce ephemeral, certificate-based authentication with minimal automation. Include per-pipeline app registrations or a scalable alternative, certificate lifecycle in Key Vault, least-privilege RBAC, validation steps, and rollback/continuity considerations?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Bloomberg","Google","Salesforce"]},{"id":"q-4112","question":"Grant brief, auditable admin access to the Global Administrator role for 60 minutes during production deployments. Design an end-to-end approach using Privileged Identity Management (PIM): activation workflow, required approvals, MFA, auto-revocation, and comprehensive auditing. Include validation steps during the window and after deactivation. How would you implement?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Amazon","Snowflake","Tesla"]},{"id":"q-4203","question":"You need Just-in-Time admin access for a vendor to the Cost Management portal for 2 days. Describe configuring Azure AD Privileged Identity Management (PIM) to grant the 'Cost Management Contributor' role temporarily with MFA on activation and an approval step, plus auto-expiry. Include how you would validate access during the window and after expiry with minimal scripting?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Hugging Face","Instacart","Meta"]},{"id":"q-4340","question":"Scenario: A sensitive on-prem app is published via Azure AD Application Proxy. External contractors need time-boxed access with MFA and device posture checks, plus an automatic revocation if posture fails. Outline an end-to-end plan using Conditional Access, App Proxy, PIM/Entitlement Management, and Access Reviews; include validation steps and drift detection within 24h?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Instacart","Lyft","Square"]},{"id":"q-4453","question":"Scenario: A security policy requires time-limited admin access for a DevOps engineer to perform a release across two Azure subscriptions. Use Privileged Identity Management (PIM) to enable Just-In-Time elevation, require MFA, implement an approval workflow, and ensure automatic expiry with audit logs and post-expiry revocation. Outline an end-to-end plan, validation steps during elevation, and rollback if approval is not granted?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["DoorDash","Microsoft","Twitter"]},{"id":"q-4498","question":"Scenario: Global enterprise with data residency needs and a partner ecosystem requires secure Azure AD access to a Synapse analytics workspace containing PII. Design an end‑to‑end plan to grant a vendor temporary, least‑privilege access using Privileged Identity Management and Azure AD Entitlement Management. Include B2B guest setup, role scoping to the Synapse workspace, MFA and device posture in Conditional Access, an access package with expiry, and a post–expiry validation strategy with minimal automation?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["IBM","Instacart","Plaid"]},{"id":"q-4578","question":"Scenario: You are the Azure AD admin for a multinational retailer. A vendor needs access to a single Azure AD-secured SaaS app for 21 days. Outline an end-to-end plan to (1) create a dedicated guest/security group and add the contractor, (2) enforce MFA and device compliance via Conditional Access for the portal, (3) auto-revoke at expiry with Access Reviews, (4) ensure licenses are provisioned automatically if needed, and (5) validate during the window with auditable checks and after expiry?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Instacart","Meta","Tesla"]},{"id":"q-4685","question":"Scenario: An Azure AD tenant with a small IT team needs occasional elevated access. Outline an end‑to‑end plan to grant Just‑In‑Time access to the 'User Access Administrator' role using Privileged Identity Management (PIM) for 1 hour, with approval, MFA on activation, and auto‑expiry/removal. Include how you would validate activation during the window and after expiry with minimal automation?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["MongoDB","Square","Zoom"]},{"id":"q-4803","question":"Design a beginner-friendly onboarding plan for a vendor who needs access to a single Azure AD‑secured internal portal published via Application Proxy for 5 days. Outline how you would (1) create an App Registration with a Read App Role, (2) map it to a dedicated security group, (3) enforce MFA and device/IP requirements with Conditional Access, (4) implement a 5‑day auto-revoke Access Review, and (5) validate access during and after expiry with minimal automation?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Anthropic","Meta","Netflix"]},{"id":"q-4828","question":"Design a Just-In-Time access workflow in Azure AD for a contractor who needs temporary elevated rights to run a data migration script against an Azure SQL Database and perform a one-off data validation job in Snowflake. Use PIM for the SQL DB admin role, a guest path with a Snowflake app role, MFA and approval, and a short expiry with automatic revocation. Include validation steps during the window and after expiry, with minimal automation and no custom scripts?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Coinbase","Oracle","Snowflake"]},{"id":"q-855","question":"Your organization needs a temporary access workflow: grant a contractor read/write access to a single Blob Storage container for 30 days using Azure AD groups and RBAC, and automatically revoke access at day 30. How would you implement this end-to-end?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["DoorDash","IBM","Stripe"]},{"id":"q-897","question":"Scenario: A contractor needs access to a single Azure AD‑secured app (CI/CD portal) for 14 days. Outline an end‑to‑end approach using a dedicated Azure AD security group, app RBAC, and an Access Review to auto‑revoke access at day 14. Include how you would validate access during the window and after expiry, with minimal automation and no custom scripts?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Airbnb","Twitter","Two Sigma"]},{"id":"q-929","question":"Scenario: A multinational bank needs external data scientists to access multiple Azure resources (Data Lake Gen2, Synapse, and a storage account) for a 6-week analytics project. Propose an end-to-end access model using Azure AD Entitlement Management, Access Reviews, B2B collaboration, and Privileged Identity Management (PIM) for Just-In-Time role activations, with cross-subscription considerations, least privilege, and auditability. Include how you enforce MFA, token lifetimes, revocation at end, and validation steps?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Goldman Sachs","Netflix","PayPal"]},{"id":"q-942","question":"Scenario: A vendor must run nightly ingestion pipelines against Data Lake Gen2, a storage account, and Synapse in a shared Azure AD tenant for 10 days using a single service principal. Propose an end-to-end access model using an App Registration with scoped RBAC, Just-In-Time activation (PIM) for the service principal, Entitlement Management or Access Reviews, and automatic revocation at expiry. Include how you would validate access during the window and after expiry with minimal automation and no custom scripts?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Instacart","NVIDIA"]},{"id":"q-1053","question":"You’re deploying a multi-tenant Azure OpenAI-powered customer-support assistant for a global marketplace. Describe an end-to-end plan for runtime isolation and governance: prevent prompt injection, redact PII before OpenAI calls, enforce per-tenant quotas, maintain data lineage in Purview, ensure regional residency, and implement drift alerts via Azure Monitor plus a lightweight detector in Azure ML?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Cloudflare","DoorDash","Snowflake"]},{"id":"q-1134","question":"You're building a beginner-level Azure OpenAI-powered chat assistant for a rideshare service that serves clients in two regions. Outline a concrete data path and a minimal routing implementation that ensures user messages and model outputs stay in-region. Include a TypeScript function that selects the regional OpenAI endpoint based on client region, a latency fallback policy, and basic in-region logging to Azure Monitor. Provide testing approaches?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Citadel","Google","Uber"]},{"id":"q-1330","question":"Design a region-agnostic, tenant-aware GenAI service on Azure OpenAI that updates safety and governance policies at runtime via a central config store (Azure App Configuration) without redeploying prompts. Include how you route queries, enforce per-tenant data residency, and measure latency under 200ms for common tasks?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Hashicorp","Snowflake","Square"]},{"id":"q-1360","question":"You're building a real-time, multi-tenant Azure OpenAI-powered support chatbot for PayPal, MongoDB, and Two Sigma. You want canary deployments of two model variants (v1, v2) with automatic rollback on degradation, regional routing, and per-tenant quotas. Describe a concrete end-to-end approach: how you implement versioned deployments, traffic routing, governance, latency/quality monitoring, and rollback triggers using Azure OpenAI, API Management, Functions, Front Door, and Monitor. Include data flow and concrete metrics?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["MongoDB","PayPal","Two Sigma"]},{"id":"q-1414","question":"Scenario: You're deploying a multilingual, multi-tenant fintech chat assistant on Azure OpenAI Service. Each tenant's data must remain in their region, with per-tenant quotas, prompt-injection defenses, and automatic redaction before OpenAI calls. Outline a concrete deployment and testing plan using Azure API Management, Azure Functions, Text Analytics, Purview, regional OpenAI endpoints, and Azure Monitor. What edge cases exist and how would you verify data residency and drift monitoring across languages?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Meta","Scale Ai","Two Sigma"]},{"id":"q-1468","question":"You’re building an Azure OpenAI-powered support assistant for a payments platform that must isolate tenant data regionally, enforce per-tenant quotas, and attach citations to every answer. Design the end-to-end architecture, data paths, and testing plan using Azure API Management, regional OpenAI deployments, Redis quotas, Azure Cognitive Search with vector store, Purview, and residency checks. What are the key trade-offs?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["PayPal","Twitter"]},{"id":"q-1509","question":"Design a regional, policy-driven retrieval-augmented generation (RAG) pipeline on Azure for a global platform. Enforce per-tenant data residency, block secrets in prompts, redact PII automatically. Implement per-tenant quotas and budget alarms; emit audit trails to Purview. Route via APIM to regional OpenAI endpoints and a regional vector store (Cognitive Search or Cosmos DB) with drift monitoring and rollback?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Airbnb","MongoDB","Twitter"]},{"id":"q-1597","question":"You're building a beginner-friendly SaaS chat API powered by Azure OpenAI. It should be fronted by Azure API Management and implemented with an Azure Function backend. Implement a per-tenant rate limit of 60 requests per minute using an API Management policy and log usage to Azure Monitor. Describe the end-to-end setup, plus a minimal policy snippet and a tiny function wrapper showing the data flow. How would you validate it?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Instacart","NVIDIA"]},{"id":"q-1641","question":"Scenario: A Meta/Anthropic-scale social platform needs an Azure-hosted, multi-tenant content moderation bot. It must support multilingual queries, preserve tenant data residency in their regions, harden against prompt injection, redact PII before any OpenAI calls, enforce per-tenant quotas, and provide drift alerts. Outline an end-to-end pipeline using Azure API Management, Azure Functions, Azure OpenAI Service (regional endpoints), Text Analytics for PII, Purview for data lineage, and Azure Monitor plus a lightweight detector in Azure ML. Include a minimal policy snippet and a tiny function wrapper to illustrate data flow?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Anthropic","Meta"]},{"id":"q-1650","question":"Scenario: You are building a beginner-level multi-tenant Azure OpenAI chat assistant that uses a versioned prompt catalog stored in Azure Blob. API Management selects the tenant’s prompt version, enforces a per-tenant quota, redacts PII before calling OpenAI, and falls back to a default prompt if the catalog fetch fails. Outline the end-to-end flow, a minimal policy snippet, a simple function wrapper, and a test/failover plan?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Coinbase","Databricks"]},{"id":"q-1762","question":"Design a private, per-tenant Azure OpenAI-powered assistant exposed via API Management with strict data residency and governance. Describe end-to-end architecture using Private Link to OpenAI, tenant-scoped API Management gateways, per-tenant Managed Identities to call the private endpoint, regional OpenAI endpoints, Key Vault for secret rotation, Purview auditing, per-tenant quotas, and a drift detector in Azure ML. Include deployment, data flow, failure modes, and testing?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Amazon","Hashicorp"]},{"id":"q-1777","question":"Design an Azure OpenAI code-assistant workflow for a SaaS platform where developers paste private repository snippets to generate patches. Implement hard secret redaction before OpenAI calls, integrate Azure Key Vault for secret lookups with RBAC, enable per-tenant data isolation, and log all prompts/responses to Purview. How would you route, guard, and verify outputs end-to-end?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Cloudflare","Microsoft"]},{"id":"q-1833","question":"Design a real-time, search-augmented chat assistant on Azure OpenAI Service for a video conferencing platform. It must query a region-bound Azure Cognitive Search index, redact PII before OpenAI calls, enforce per-tenant quotas, and provide per-tenant explainability of retrieved docs. Outline the end-to-end data path (ingestion, vector search, redaction, prompt assembly, streaming response), a minimal API Management policy snippet, and a tiny backend snippet illustrating the flow and versioning strategy?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Cloudflare","Hugging Face","Zoom"]},{"id":"q-1891","question":"You're deploying an Azure OpenAI-powered enterprise assistant for a multi-tenant SaaS app. Each tenant's data must never mix, residency must be region-bound, and prompts must be safeguarded against leakage via memory or external tools. Outline a concrete data path and governance using Azure OpenAI Service (private endpoint), Azure Functions, API Management, per-tenant Cosmos DB for embeddings, and per-tenant quotas, with a lightweight detector for leakage. Include an example policy snippet and a test plan?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Apple","Goldman Sachs","Meta"]},{"id":"q-1898","question":"Design a privacy-preserving, multi-tenant AI data enrichment pipeline on Azure that uses Azure OpenAI Service for generation and embeddings to augment customer data with external sources, while enforcing per-tenant data residency, PII redaction, drift detection, and auditable governance via Purview; outline architecture, data flows, and testing edge cases for latency and cost under burst traffic?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Citadel","Plaid","Uber"]},{"id":"q-1933","question":"You're deploying a privacy-preserving Azure OpenAI-powered analytics assistant for a fleet of autonomous vehicles with edge-to-cloud hybrids. Tenants must keep data in their own stores, even at the edge, and APIs are exposed via APIM. Propose a concrete pipeline using Azure Arc-enabled OpenAI, Private Endpoints, Functions, Purview, and Monitor, detailing data flow, tenancy isolation, data residency, offline mode and drift monitoring. Include testing and edge-case notes?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Slack","Snowflake","Tesla"]},{"id":"q-1958","question":"Design a multilingual customer feedback analytics pipeline that runs on Nvidia edge GPUs at retail kiosks and pairs with Azure OpenAI for summarization. Ingests local text, translates to English, analyzes sentiment, flags PII locally, then pushes aggregated metrics to Azure for long-term storage. Describe the data path, services involved, governance, and testing approach?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Goldman Sachs","NVIDIA","Snowflake"]},{"id":"q-1994","question":"Beginner-level: You have a multi-service Azure OpenAI chat pipeline: APIM -> Function wrapper -> OpenAI. Latency is variable. Describe a minimal end-to-end tracing plan using Application Insights, OpenTelemetry, and correlation IDs. Show how you would propagate a trace across APIM policy, Function, and OpenAI call, and outline a simple test to reproduce and measure end-to-end latency?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Coinbase","MongoDB","Salesforce"]},{"id":"q-2066","question":"Scenario: You run a multi-tenant Azure OpenAI-powered assistant where each tenant supplies per-tenant safety policies and tone guides that must be enforced before any OpenAI call. Design an end-to-end path using Azure API Management, Azure Functions, a lightweight policy engine, regional OpenAI endpoints, and Purview for data lineage. Explain policy evaluation, tenant isolation, drift handling, and testing with realistic workloads?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Coinbase","IBM","Lyft"]},{"id":"q-2116","question":"Design a multi-tenant Azure OpenAI-powered support bot for fintech apps with per-tenant regional residency, live moderation gating, and PII redaction before OpenAI calls. Outline the end-to-end data path, components, and a testing plan; include edge cases and how residency and drift are validated?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["PayPal","Stripe","Uber"]},{"id":"q-2162","question":"You're building a multi-tenant Azure OpenAI-powered analytics assistant that ingests user-uploaded CSVs and returns AI-generated insights via chat. Each tenant requires data isolation, per-tenant model versions, and privacy controls enforcing in-region processing and PII redaction in outputs. Design an end-to-end pipeline: user → API gateway → function orchestrator → per-tenant data store in Data Lake (Purview-tagged) → OpenAI inference with tenant-specific prompts and versioned deployments. Include canary rollouts, tests for data leakage, and drift monitoring?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Amazon","Google","MongoDB"]},{"id":"q-2179","question":"Design a multi-tenant, region-resident compliance assistant API that uses Azure OpenAI for responses, a retrieval store (Azure Cognitive Search) for context, and per-tenant guardrails: data residency, content filtering, rate limits, and a canary model rollout. Show end-to-end data path and governance, and outline testing strategies for prompt injection, drift, and data leakage?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Meta","Oracle","Robinhood"]},{"id":"q-2325","question":"You're building a beginner-level Azure OpenAI-powered FAQ bot for a multinational retailer with regional tenants. Describe a practical flow that enforces per-tenant data isolation and brand voice using a versioned prompt catalog in Azure Blob, APIM, and Functions. Include fallbacks, PII redaction, and a simple test harness that validates outputs against tenant-specific rules before returning to users?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["IBM","Snowflake"]},{"id":"q-2375","question":"Scenario: You’re building a beginner-level Azure OpenAI-powered internal helpdesk bot for a multinational. It must auto-detect user language (English, Spanish, Chinese) via Azure Text Analytics, route prompts to a regional OpenAI deployment (US/EU/APAC), and keep data residency by region. Outline the end-to-end data path, components, and a simple test plan; include fallback translation path if language isn’t supported?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Apple","Microsoft"]},{"id":"q-2424","question":"Scenario: You’re building a real-time fraud-analytics assistant using Azure OpenAI Service to process streaming transactions from multiple financial partners. You must achieve sub-100 ms per-query latency, enforce strict per-tenant data isolation, redact PII before hitting OpenAI, and produce auditable decisions. Design the data path and architecture using Azure API Management, Event Hubs, Functions, a fast cache, Key Vault for keys, regional OpenAI endpoints, Purview for lineage, and ADR/audit logging. Include a concrete data flow and at least three concrete controls?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Citadel","Hashicorp","Snowflake"]},{"id":"q-2559","question":"Scenario: Build a real-time analytics assistant that ingests streaming telemetry from thousands of tenant apps and uses Azure OpenAI Service to summarize incidents. Data includes secrets and PII; ensure per-tenant residency, data redaction before OpenAI calls, and strict isolation in a pay-as-you-go cost model. Outline the end-to-end data path, gating, and testing plan using Event Hubs, Functions, Text Analytics, regional OpenAI endpoints, and Purview?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Instacart","Robinhood","Two Sigma"]},{"id":"q-2589","question":"You’re deploying a real-time Azure OpenAI-powered field-support assistant for multiple automaker tenants. Outline a concrete, region-aware prompt-routing gateway that guarantees per-tenant isolation and data residency using API Management, Azure Front Door, and versioned OpenAI deployments. Include the data path, model routing rules, rollback plan, and a testing strategy for latency and drift?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Databricks","Google","Tesla"]},{"id":"q-2703","question":"Design a beginner-level Azure OpenAI-powered FAQ bot for a software product. The bot must (1) run behind Azure OpenAI Service and API Management, (2) use a simple prompt template with a versioned FAQ catalog stored in Azure Blob, (3) implement a minimal guardrail: redact PII in user questions and apply a content filter to block disallowed topics in the response before it reaches the user. Outline the end-to-end data path, components, and a basic test plan. Include a small code snippet for the redaction function and for the post-filter?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Tesla","Zoom"]},{"id":"q-2857","question":"Design a beginner-level Azure OpenAI-powered chat assistant for a retail app hosted behind API Management. Implement per-tenant cost visibility by estimating tokens client-side (rough estimate) and returning a 'X-OpenAI-Cost' header for each response; store daily costs in Azure Table Storage; ensure no PII leaks. Outline the end-to-end data path, components, and a minimal test plan. Include a small code snippet for a local token estimator and an APIM policy snippet to append the header?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Bloomberg","MongoDB","Robinhood"]},{"id":"q-2913","question":"Design a beginner-level Azure OpenAI-powered IT helpdesk bot that runs behind Azure OpenAI Service and API Management. It should pull answers from a simple FAQ catalog stored in Azure Table Storage, use a minimal prompt to steer tone, redact PII in user input, and cache the top 100 queries in Azure Cache for Redis with a TTL of 5 minutes. Outline the end-to-end data path, components, and a basic test plan. Include code snippets for redaction and Redis cache lookup. How would you implement and validate this setup?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Google","Meta"]},{"id":"q-3154","question":"You're deploying an Azure OpenAI-powered code-assistant for a monorepo at scale. Design a scalable data path and guardrails to ensure secrets and PII never reach OpenAI or logs, fetch per-repo context via a retrieval layer, support versioned prompts, run a sandboxed test harness before returning results, and manage latency and cost. Outline end-to-end architecture and data flow?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Snowflake","Tesla"]},{"id":"q-3173","question":"Design an Azure OpenAI-powered, tenant-aware governance proxy that routes user requests to OpenAI or a regional on-prem fallback based on per-tenant safety policies, data residency rules, and cost budgets. Exposed via API Management; policy packs live in Azure Blob; implement a PDP function to decide to redact, route, or block; outline data path, components, and a minimal PDP policy example with tests?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Meta","Netflix","Stripe"]},{"id":"q-3359","question":"Design a retrieval-augmented code search assistant for internal docs on Azure. It must run behind API Management, use Azure OpenAI Service for responses, index Git/Docs in Azure Cognitive Search with vector search, support per-tenant data residency and access control, redact secrets from prompts and outputs, and include monitoring. Outline the data path, components, and a test plan; include a small redaction snippet and a guardrail example?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["IBM","Slack"]},{"id":"q-3487","question":"Scenario: You deploy an Azure OpenAI-powered analytics assistant to answer questions about real-time IoT telemetry from customers' devices. The system must enforce per-tenant data residency and quotas, ingest data via Azure IoT Hub, store logs in Data Lake Gen2, and refresh embeddings from telemetry on a scheduled cadence. Outline the end-to-end data path, components, and a practical test plan. Include how you'd implement per-tenant quotas in API Management and a small code snippet for quota checks in Azure Functions?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Amazon","Microsoft","Plaid"]},{"id":"q-3510","question":"Design a beginner-level Azure OpenAI-powered chat assistant that helps field technicians troubleshoot IoT gateway issues for an IoT product. It runs behind API Management and uses a guided prompt template plus a knowledge base stored in Azure Blob; it uses Azure Cognitive Search for grounding with embeddings; ensure PII redaction in user input and per-tenant rate limiting; outline end-to-end data path, components, and a simple test plan?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Snap","Tesla","Zoom"]},{"id":"q-3563","question":"You’re building a real-time financial insights assistant on Azure OpenAI for a trading desk. Ingest streaming earnings call transcripts via Event Hubs, summarize with OpenAI, ground with Azure Cognitive Search against a policy KB in Blob storage, and enforce compliance with a per-tenant policy. Achieve sub-200ms tail latency, strict data residency, and redact PII before OpenAI. Outline end-to-end data path, components, and a minimal test plan?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Bloomberg","Coinbase","Robinhood"]},{"id":"q-3577","question":"Design a beginner-level Azure OpenAI-powered multilingual support bot that runs behind API Management. The bot should automatically detect user language, route prompts to a regional OpenAI deployment that supports the language, fallback to English if unsupported, and store last 5 interactions per user in Azure Cosmos DB with a 24-hour TTL. Outline end-to-end data path, components, and a simple test plan. Include a sample language-detection function and a prompt template snippet?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Amazon","Cloudflare","Stripe"]},{"id":"q-3619","question":"Design a beginner-level Azure OpenAI-powered chat assistant for a Databricks onboarding scenario. It runs behind Azure API Management and uses a guided prompt plus a knowledge base stored in Azure Blob; grounding via Azure Cognitive Search. Implement PII redaction in user input and a per-tenant token-budget gate: if the next OpenAI call would exceed budget, reply with a cost-saving checklist instead of querying OpenAI. Outline end-to-end data path, components, and a basic test plan?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Databricks","Microsoft"]},{"id":"q-3680","question":"Design an Azure OpenAI-powered, multi-tenant support bot for a large gaming platform that also assists moderation. The system must isolate tenants by region, redact PII before OpenAI calls, ground answers with Azure Cognitive Search embeddings from a knowledge base stored in Azure Blob, and enforce per-tenant content gating with explainable flags. Outline the data path, components, testing plan, and how you would monitor drift and cost?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Cloudflare","Discord","Snowflake"]},{"id":"q-3788","question":"Design a multi-tenant retrieval-augmented chat assistant for policy questions using Azure OpenAI Service. Ground answers against a versioned knowledge base stored in Azure Blob and indexed by Azure Cognitive Search. Requirements: per-tenant model specialization, PII redaction prior to grounding, a per-tenant budget gate, a target latency for the retrieval step, and an end-to-end data path. Outline components, data flow, and a practical test plan?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Lyft","Meta","PayPal"]},{"id":"q-3934","question":"You need an Azure OpenAI-powered assistant used by multiple business units in a global bank. Each unit has strict data residency, PII handling, and auditability requirements. Describe an end-to-end architecture that enforces per-tenant residency, masks PII before OpenAI calls, tracks data lineage with Azure Purview, and detects model drift to trigger automatic safe fallback or tenant-specific guardrails. Include data path, components, testing plan, and a policy snippet?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Goldman Sachs","Meta","Uber"]},{"id":"q-3947","question":"Design a multi-tenant Azure OpenAI-powered code-completion service for a cloud IDE plugin. Each tenant has its own prompt template, safety rules, and per-minute quotas. Outline the end-to-end data path (API Management, Functions, OpenAI, embeddings), per-tenant routing, and a cost-aware fallback when a tenant nears quota. Include a practical test plan and observability strategy?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Adobe","Cloudflare","Google"]},{"id":"q-4025","question":"For an Azure OpenAI-powered customer-service bot in fintech, design a multi-tenant solution that translates customer inquiries into policy-compliant responses. It runs behind API Management, uses a guided prompt, grounds with a knowledge base in Azure Blob and embeddings via Azure Cognitive Search. Include per-tenant data residency, logging, and a post-processing filter to redact financial PII before delivery. Outline data path, components, and a minimal test plan?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Amazon","Cloudflare"]},{"id":"q-4061","question":"In a high-traffic, multi-tenant Azure OpenAI deployment, design a dynamic routing layer that selects among multiple OpenAI deployments (GPT-4, GPT-4o, GPT-3.5) based on input features (text length, code vs plain text, sentiment, PII risk) and real-time per-tenant budgets. Outline end-to-end data path, decision logic, caching, and a robust test plan?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Amazon","Google"]},{"id":"q-4091","question":"Design a multi-tenant Azure OpenAI-powered contract-review assistant for a SaaS legal product. Requirements: enforce data residency by processing data in tenant-region, apply per-tenant content-filter policies to both prompts and outputs, implement retrieval-augmented generation using Azure Blob storage for contracts and Azure Cognitive Search for embeddings, and load per-tenant policies from Cosmos DB with versioned prompts. Sketch end-to-end data path, components, and a test plan?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Citadel","Instacart"]},{"id":"q-4201","question":"Design a beginner-level Azure OpenAI-powered onboarding assistant for a fintech compliance workflow. It runs behind API Management, uses a guided prompt and a per-tenant knowledge base stored in Azure Blob. Add: 1) redact PII in inputs, 2) log prompts and responses to Azure Data Lake for audit, 3) generate a one-sentence justification for each answer to aid compliance explainability, 4) a basic outage fallback serving static policy answers from a local cache. Outline end-to-end data path, components, and a simple test plan; include minimal redaction and fallback code?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Goldman Sachs","NVIDIA","Scale Ai"]},{"id":"q-4297","question":"How would you architect a multi-tenant Azure OpenAI-powered assistant for enterprise finance use cases (Bloomberg/PayPal/Salesforce) that enforces per-tenant grounding with fresh embeddings, a policy gate, and strict data isolation while controlling costs? Describe data paths, components, and a minimal test plan?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Bloomberg","PayPal","Salesforce"]},{"id":"q-4386","question":"Design a multi-tenant Azure OpenAI inference broker that routes each prompt to a tenant-scoped OpenAI deployment (region, residency, budget gates). Implement per-tenant templates, PII redaction, and a policy engine; never send PII to OpenAI. Cache recent prompts, log to Data Lake, and provide regional fallbacks to ensure 99.95% availability and sub-200ms routing. Outline data flow, components, and a test plan; include a minimal code snippet for tenant template lookup?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Adobe","Anthropic","Apple"]},{"id":"q-4456","question":"Design a multi-tenant, Azure OpenAI-powered assistant for a corporate compliance team that drafts policy answers using per-tenant knowledge bases in Azure Blob and grounding via Azure Cognitive Search. It must redact PII, attach governance metadata (tenant, model version, provenance) to every reply, run safety/bias checks, log history to Data Lake, and gracefully fall back to static policy replies if confidence is low or OpenAI is unavailable. Outline end-to-end data path, components, and a test plan?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Adobe","Microsoft","Salesforce"]},{"id":"q-4570","question":"Design a real-time AI-assisted security incident response advisor that runs on Azure OpenAI Service. It must ingest Defender for Cloud telemetry via Event Hubs, redact PII and secrets before sending to OpenAI, apply per-tenant policy gating, and propose remediation steps with auditable logs and cost controls. Include an end-to-end data path, components, and a practical test plan?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Apple","MongoDB","Twitter"]},{"id":"q-4699","question":"Design a beginner-level Azure OpenAI-powered support bot for a global SaaS product that honors per-tenant data residency. Each tenant maps to a region; inputs and embeddings must be routed to the tenant's regional OpenAI deployment via API Management, with redaction and grounding from a region-scoped Azure Cognitive Search over blob data. If the region OpenAI is down, serve a static policy-based fallback. Outline end-to-end data flow, components, and a basic test plan?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Hashicorp","LinkedIn","Plaid"]},{"id":"q-866","question":"You're deploying a multi-tenant chat assistant on Azure OpenAI Service for a rideshare company. PII must never be sent to OpenAI and responses must redact sensitive data before delivery. Outline a practical, beginner-friendly data path using Azure API Management, Azure Functions, Text Analytics for PII detection, and a regional OpenAI deployment. Include a simple data flow?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["MongoDB","Tesla","Uber"]},{"id":"q-963","question":"You're building a beginner-friendly customer support bot on Azure OpenAI Service. How would you design a lightweight API boundary policy (Azure API Management) to rate-limit per user, cap monthly spend, and gracefully fall back to a rule-based reply if OpenAI is unavailable? Describe the data flow from API call through OpenAI or fallback, and include a minimal policy snippet?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Apple","Bloomberg","Square"]},{"id":"q-1029","question":"In a global IoT telemetry pipeline ingesting 100k events/s per region into ADLS Gen2 and Databricks Delta Lake, implement schema drift tolerant ingestion, end-to-end data lineage via Purview, RBAC, and cross-region DR replication. How would you architect the data contracts, partitioning, watermarking, retention, and governance to meet data residency and fault-tolerance requirements?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Cloudflare","Twitter","Uber"]},{"id":"q-1043","question":"Design an end-to-end Azure data-ecosystem pipeline that ingests incremental changes from a MongoDB Atlas collection into Delta Lake on ADLS Gen2, preserving SCD Type 2 history for a customers dimension, and handling schema drift. Include data lineage to Azure Purview, late-arriving updates, and on-demand reprocessing without data loss. Which components and config would you choose, and why?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["MongoDB","NVIDIA","Robinhood"]},{"id":"q-1311","question":"Design an end-to-end incremental ELT pipeline on Azure that ingests 2 TB/day of nested JSON telemetry from Event Hubs into a Delta Lake on ADLS Gen2, then loads a star schema in Azure Synapse. Requirements: (1) schema drift tolerant writes and schema evolution, (2) upserts by a composite key (tenant_id, event_id), (3) end-to-end data lineage to Purview, (4) late-arriving data support via watermarking, (5) partitioning by region and day with file-size/compaction strategies. Which components and patterns would you use, and why? Compare Delta Lake MERGE vs Delete+Insert for upserts?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["Bloomberg","Google","Snowflake"]},{"id":"q-1326","question":"Design an end-to-end pipeline that streams user activity from Azure Event Hubs into Delta Lake on ADLS Gen2, supports schema evolution, and updates an SCD Type 2 customer dimension in Synapse. Include exactly-once guarantees, late-arriving data handling, idempotent sinks, and end-to-end data lineage via Purview in a hybrid estate. List components, data flows, and governance approach?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["IBM","Snap"]},{"id":"q-1453","question":"You are starting a beginner-friendly Azure data engineer task: daily telemetry logs arrive in ADLS Gen2 under /raw/telemetry/ in mixed formats (CSV and nested JSON). Design an event-driven pipeline using Azure Data Factory that fires on blob creation, ingests files, flattens nested structures to a canonical schema, handles optional fields and few schema drift cases, and loads into a simple star schema in Azure Synapse Analytics. Include incremental loading with a watermark and basic data quality checks. What components and steps would you implement?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Airbnb","Google"]},{"id":"q-1474","question":"Design a nightly Azure data pipeline that ingests delta updates from a SaaS source into ADLS Gen2, handles schema drift with automatic inference, and materializes a star schema in Azure Synapse. Ensure end-to-end data lineage, idempotent upserts, and reliable rollback after failures. Which services and patterns would you deploy, and how would you verify correctness and lineage end-to-end?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["Oracle","Plaid","Two Sigma"]},{"id":"q-1524","question":"You operate a global event lake: multiple producers push JSON events into Event Hubs. Ingest to ADLS Gen2, apply drift-tolerant transformations, and publish a canonical star schema in Azure Synapse with incremental loads. Propose a production-ready architecture leveraging Data Factory, Databricks Delta Lake, and Purview; detail schema evolution, late-arriving data handling, data quality gates, and privacy masking?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["LinkedIn","Scale Ai","Square"]},{"id":"q-1563","question":"Daily JSON feed arrives in ADLS Gen2. Design a beginner Data Factory pipeline to stage, flatten nested fields, validate data quality, and write a partitioned Parquet table in Azure Synapse. Handle minor schema drift by defaults and ignoring new fields. Which components and steps would you use?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Bloomberg","DoorDash","Twitter"]},{"id":"q-1638","question":"Design a streaming ingestion from an on-prem ERP into ADLS Gen2 for real-time analytics. Use Delta Lake with Auto Loader-like ingestion, bronze-silver-gold data path, exact-once delivery, and schema-drift tolerant processing. The silver layer feeds a star schema in Azure Synapse. Ensure end-to-end lineage in Purview, data quality gates, and robust late-arriving data handling. Describe architecture, contracts, and rollback strategy?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["IBM","Snowflake","Two Sigma"]},{"id":"q-1682","question":"Design an end-to-end near-real-time data pipeline for a global adtech dataset with mixed sources (on-prem SQL Server, Azure SQL, and SaaS APIs). Implement CDC, incremental loads, and schema drift tolerance, load into Delta Lake on Synapse, and enforce end-to-end lineage in Purview. Include data masking for PII, data quality gates, and a GitOps workflow for pipelines?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Amazon","Scale Ai"]},{"id":"q-1782","question":"Design a beginner-level Azure data ingestion task: In a fintech scenario, ingest daily transaction rows from an on-premises SQL Server into Azure Data Lake Storage Gen2 as Parquet files using Azure Data Factory. Include steps to handle date partitioning, security, and reliable runs. What would your pipeline look like, and what minimal code or configuration would you provide?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Citadel","Coinbase"]},{"id":"q-1916","question":"You manage a multi-region analytics platform with data from Salesforce, Workday, and IoT devices. Data lands in ADLS Gen2; you need to apply policy-driven redaction for PII, support incremental loads, enforce schema drift tolerance, and publish end-to-end lineage to Azure Purview and downstream consumers (Power BI, Synapse). Propose an Azure-native pipeline design using Data Factory, Databricks Delta Live Tables, Purview, and Synapse; highlight design decisions, governance, and potential pitfalls?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["LinkedIn","MongoDB","Snap"]},{"id":"q-1979","question":"In Azure, design a production-grade data pipeline for streaming ecommerce telemetry from Azure Event Hubs into ADLS Gen2, with downstream star schema in Synapse, enabling incremental loads, automatic schema drift handling, and end-to-end data lineage via Purview. Include details on Delta Lake usage, MERGE-based upserts, late-arriving data handling, and a minimal Spark code snippet for a MERGE with schema drift tolerance?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Databricks","OpenAI"]},{"id":"q-2102","question":"A batch of daily JSON click events arrives in ADLS Gen2 under /raw/events/ from an on‑prem source via Self-Hosted IR. Build a beginner Data Factory pipeline that (1) copies raw JSON to /staging/events/, (2) uses a schema-drift-tolerant mapping to normalize fields and cast ts to timestamp, (3) writes to a daily-partitioned Parquet table in Azure Synapse, and (4) loads incrementally by tracking max ts in a control table and filtering ts > last_ts. Include a basic data quality check for userId?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Airbnb","NVIDIA","Uber"]},{"id":"q-2378","question":"Design a scalable pipeline to ingest daily customer event logs from an on-prem Kafka cluster into Azure Data Lake Gen2, apply schema drift tolerant transformations, and maintain an SCD Type 2 star schema in Azure Synapse, while providing end-to-end data lineage via Purview. Which Azure services and patterns would you use, and how would you implement incremental loads?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["DoorDash","Lyft","Snowflake"]},{"id":"q-2495","question":"Ingest JSON telemetry from multiple partners into ADLS Gen2 and upsert into a Delta Lake on Synapse while preserving end-to-end lineage and data quality. Outline an Azure-based architecture using IoT Hub/Event Hubs, Data Factory or Synapse pipelines, Databricks, Purview, and Delta Lake features to handle schema drift, late data, and incremental upserts. Include security, cost considerations, and observability?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["OpenAI","Plaid","Snap"]},{"id":"q-2529","question":"Design an Azure data engineering pipeline for streaming IoT telemetry from store sensors into ADLS Gen2 and Synapse, with schema drift tolerant transformations, incremental loads, and end-to-end data lineage. Use Event Hubs to ADLS Gen2, Delta Lake in Synapse, and Purview for lineage. Address late data, schema evolution, security, and cross-region replication. What services and data contracts would you implement, and how would you validate correctness?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["Airbnb","Hashicorp","Meta"]},{"id":"q-2605","question":"Design an end-to-end real-time ingestion and analytics pipeline for a live market tick feed (timestamp, symbol, price, volume) arriving from multiple brokers at high throughput. Ingest to ADLS Gen2 as Delta Lake, with schema drift tolerance and automatic schema evolution, and deliver upserts to a central ticks table. Ensure exactly-once streaming, late-arriving data handling, end-to-end data lineage via Purview, and scalable partitioning by date and symbol. Outline components (Event Hubs, Databricks, Delta Lake, Purview) and provide a workable configuration sketch?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Apple","Slack","Two Sigma"]},{"id":"q-2625","question":"Daily on-prem JSON event files arrive; design a beginner ELT pipeline using Data Factory that ingests JSON, enforces a basic schema, converts to Parquet, partitions by event_date, and loads a simple star schema into Azure Synapse. Explain how you’d implement basic data lineage and a 7-day retention policy, without advanced schema drift handling?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Amazon","Goldman Sachs","Slack"]},{"id":"q-2669","question":"Design an Azure streaming data pipeline for a global fleet of IoT devices sending JSON telemetry to Event Hubs, with data landing in ADLS Gen2 and a Delta Lake lakehouse. Requirements: sub-5 second ingestion latency, end-to-end latency <15 seconds for dashboards, automatic schema evolution, idempotent upserts, and end-to-end lineage with Purview. Outline components, dataflow, and a minimal Delta table evolution example?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["Citadel","Discord","Tesla"]},{"id":"q-2680","question":"Design an end-to-end Azure data platform to ingest per-tenant e-commerce clickstream data from an on-prem SFTP into ADLS Gen2, handle schema drift, and deliver per-tenant analytics in Synapse while preserving end-to-end lineage in Purview and enforcing RBAC/column masking. Implement incremental loads, late-data handling, and cost/perf tradeoffs between serverless vs dedicated pools?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Amazon","Coinbase","Snowflake"]},{"id":"q-2712","question":"Design a global Azure data lakehouse for real-time fraud analytics. Ingest streaming transaction events from on-prem ERP into ADLS Gen2, apply schema-drift tolerant transformations, and store as Delta Lake tables with governance via Unity Catalog. Implement incremental loads with CDC and watermark, ensure end-to-end lineage via Purview, and support multi-region reads with geo-replication. Which components and patterns would you choose, and how would you configure them to balance latency, cost, and compliance?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Bloomberg","Google","Microsoft"]},{"id":"q-2741","question":"Scenario: A REST API provides hourly weather data as JSON. Ingest to ADLS Gen2 and load a simple star schema in Synapse. Outline a beginner Azure Data Factory pipeline that calls the API hourly, lands raw JSON under /data/raw/weather/YYYY/MM/DD/HH/, normalizes to fixed fields (date, location, temp, humidity, wind, precip) with nulls for missing, writes Parquet partitioned by date, and validates via a nightly row-count check. Include components and steps?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Meta","NVIDIA","Scale Ai"]},{"id":"q-2915","question":"Scenario: Ingest IoT JSON logs stored in ADLS Gen2 under /iot/logs/ into a clean, queryable table in Azure Synapse. The schema evolves as new fields are added over time. Design a beginner-friendly pipeline using Synapse Pipelines or Data Factory that: 1) handles schema drift, 2) writes normalized data to Parquet in /processed/iot/, 3) loads incrementally via watermark strategy into a small fact table (device_id, event_date, event_type, duration), and 4) logs data quality metrics and lineage. Which services and steps would you implement, and how would you validate success?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["NVIDIA","Scale Ai"]},{"id":"q-2961","question":"Design an end-to-end data pipeline for a global fintech platform (Coinbase/Airbnb). Ingest real-time trade ticks from an on-prem SFTP feed and batch customer events into ADLS Gen2, leverage Delta Lake on Databricks, and load a star schema into Azure Synapse. Ensure end-to-end data lineage, automatic schema drift handling, incremental loads, and row-level security for PII. Outline components, Purview governance, and testing/rollback strategy?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Airbnb","Coinbase"]},{"id":"q-3019","question":"Design an end-to-end Azure data pipeline for streaming IoT telemetry from on-prem JSON feeds into ADLS Gen2 and a star schema in Synapse. Use Delta Live Tables or Databricks to handle schema drift, implement idempotent upserts, ensure exactly-once semantics, and surface data lineage to Purview. Include data contracts, quality checks, alerting, and recovery from late data?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Hashicorp","IBM","Tesla"]},{"id":"q-3107","question":"Design a streaming ELT pipeline that ingests customer events from Azure Event Hubs and a SQL Server CDC feed, normalizes to a Delta Lake star schema on ADLS Gen2, supports schema evolution, idempotent upserts, and end-to-end lineage via Azure Purview. Outline components, dataflow, and quality checks?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["Microsoft","Robinhood","Two Sigma"]},{"id":"q-3120","question":"In a multinational fintech, ingest both daily batch and streaming micro-batch events into Delta Lake on ADLS Gen2, enforce schema evolution, and capture end-to-end data lineage across on-prem, Event Hubs, and Azure Synapse analytics. Which Azure components would you choose, what patterns ensure correctness and rollback, and how would you validate lineage end-to-end at scale?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Cloudflare","Coinbase","Instacart"]},{"id":"q-3175","question":"You receive daily JSON event files from three SaaS apps landing in ADLS Gen2 at /raw/events/{saas}/{date}. Each file shape differs slightly. Design a beginner Data Factory pipeline to ingest, normalize to a common schema, and write to Parquet in /processed/events/{date}/. Include: (1) a schema-drift tolerant mapping or inference approach, (2) a simple incremental load using a date watermark, (3) basic data quality checks (not-null on key fields, duplicates), (4) automatic partitioning by date, (5) basic monitoring/logging. Which Azure components and steps would you use?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["MongoDB","Snap","Zoom"]},{"id":"q-3229","question":"Scenario: A global retailer streams daily sales events and batch inventory feeds from on-prem sources into ADLS Gen2. You must implement end-to-end data lineage, incremental loads with watermark, schema drift tolerant transforms, and PII masking at ingestion, while enabling analytics in Synapse. Which Azure components would you choose to ensure idempotency and auditability across batch and streaming modes?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["Amazon","Apple"]},{"id":"q-3444","question":"You run a multi-tenant data lake on ADLS Gen2 for three business units. Each unit provides JSON with evolving schemas; you must produce a per-tenant SCD Type 2 customer dimension in Delta Lake, with automatic schema drift handling, tenant-level access control, and end-to-end lineage to Purview. Outline architecture, data contracts, and how you guarantee idempotent upserts and secure access?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["Anthropic","Meta","Snowflake"]},{"id":"q-3467","question":"NEW ANGLE: Daily NDJSON invoices land in ADLS Gen2 under /raw/payments/. Build a beginner Data Factory pipeline that stages to /staging/payments, flattens nested fields (payment_id, amount, currency, customer.id, customer.name, ts), runs basic data quality checks (payment_id not null, amount > 0, ts valid), and upserts into a partitioned Parquet table in Azure Synapse (Payments by date(ts)). Explain incremental loading using a watermark and describe datasets/activities?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Adobe","Airbnb","Meta"]},{"id":"q-3498","question":"You receive daily JSON logs from a SaaS app stored in ADLS Gen2 under /logs/yyyy/MM/dd/. Each file contains an array of events with nestedUser and nestedItems arrays. Design a beginner ELT using Data Factory and Spark (Synapse) to flatten to a users dimension and a facts table, implement an incremental load based on a date watermark, and ensure idempotent upserts with simple checks. What components, data flow, and validations would you implement?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Amazon","Citadel","Databricks"]},{"id":"q-3572","question":"Daily JSON logs arrive in ADLS Gen2 with nested fields and occasional missing keys. Design a beginner Data Factory pipeline that uses a Mapping Data Flow to flatten and unify to a flat schema, load incrementally into Azure Synapse (or Azure SQL DB) with a watermark on event_time, and route invalid records to /rejects. Include a simple schema-drift tolerant approach and a basic test plan?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Cloudflare","Uber"]},{"id":"q-3623","question":"In a globally distributed analytics platform, you must implement a multi-geo data mesh on Azure with data products exposed via Delta Lake tables on ADLS Gen2, governed by Purview, and queried through Synapse. How would you ensure discoverability, end-to-end lineage, contract-driven access, and schema evolution across regions while controlling cost and latency?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Adobe","Coinbase","Snowflake"]},{"id":"q-3643","question":"Design a real-time data lakehouse pipeline that ingests high-velocity financial events from an on-prem Oracle database into ADLS Gen2, processes with Spark Structured Streaming, and upserts into a Delta table consumed by Azure Synapse Analytics. Requirements: end-to-end lineage via Purview, automatic schema drift tolerance, incremental loads, tenant isolation, and robust observability. Which components and data contracts would you choose, and why?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Anthropic","Goldman Sachs","IBM"]},{"id":"q-3656","question":"You receive daily JSONL logs in ADLS Gen2 under /logs/appX/. Each file may add new fields over time. Build a beginner Data Factory pipeline that ingests these logs into a Delta Lake table in ADLS Gen2, using Spark in Synapse or Databricks. Implement schema drift handling, partition by ingest_date, and a simple data-quality check. Describe components and steps?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Amazon","Robinhood","Twitter"]},{"id":"q-3683","question":"Design a data ingestion and governance pattern for a mixed batch/streaming pipeline into ADLS Gen2 and Synapse: landing, silver Delta Lake, and curated views. Propose concrete Azure components (ADF, Databricks, Purview, Synapse), and describe how you enforce data contracts, enable schema evolution, and implement idempotent upserts across regions while controlling costs. How would you handle a breaking source schema change?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["Discord","Google","Snowflake"]},{"id":"q-3705","question":"You're given daily JSON event files arriving in ADLS Gen2 under /raw/events/. Files contain arrays of events with optional fields; you must flatten to a simple star schema (DimUser: user_id, country; FactEvents: event_id, timestamp, user_id, event_type). Design a beginner end-to-end pipeline using Azure Data Factory and Synapse to stage raw JSON, handle schema drift, perform incremental loads with a watermark, and publish lineage to Purview. Which components, steps, and dataflow logic would you implement?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Meta","NVIDIA","Twitter"]},{"id":"q-3728","question":"Design a real-time streaming ELT: ingest JSON events from Azure Event Hubs into Delta Lake on ADLS Gen2 via Databricks Structured Streaming; apply schema drift tolerant transforms; upsert into a Star schema in Azure Synapse using MERGE-based SCD2. How would you ensure end-to-end data lineage (Purview), late-data handling, and automatic schema evolution while controlling latency and costs?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Robinhood","Salesforce","Snowflake"]},{"id":"q-3781","question":"Design an end-to-end data observability solution for a multi-source Azure data lakehouse. Ingested data lands in ADLS Gen2 and is consumed by Synapse; require automatic lineage, schema drift alerts, data freshness checks, and auto-remediation that re-runs failed pipelines or blocks downstream jobs. Describe architecture, thresholds, data quality checks, and how you would implement with Purview, Delta Lake, Data Factory, and Synapse?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Amazon","Goldman Sachs","Google"]},{"id":"q-3888","question":"You get daily JSON event logs from multiple sources placed under /events/campaigns/ in ADLS Gen2. Schema drift is possible with missing fields and occasional new fields. Build a beginner Azure Data Factory pipeline that ingests, flattens, and loads into a simple star schema in Azure Synapse Analytics; partition by eventDate; store details as JSON for drift tolerance; include basic data quality checks (not-null userId and parseable timestamp) and a minimal lineage/logging. What components and steps would you implement?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Citadel","Meta","Uber"]},{"id":"q-4212","question":"You are given daily Parquet files containing user activity logs arriving into ADLS Gen2 at /data/activity/. The schema drifts as new fields appear over time. Design a beginner Data Factory pipeline that: 1) copies raw Parquet to a landing zone preserving data; 2) transforms to a simple star schema in Azure Synapse (DimUsers and FactEvents); 3) handles drift by storing extra fields as a JSON blob in a drift column; 4) enforces basic data quality (not-null user_id, parseable event_time) and logs lineage. Which Azure components and steps would you implement?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Coinbase","Scale Ai","Zoom"]},{"id":"q-4228","question":"You're building a cross-region retail analytics data product in Azure. Ingest on-prem SQL Server CDC into ADLS Gen2 in each region, publish Delta Lake tables to Databricks and Synapse with incremental loads, end-to-end lineage, and contract-based access. Which Azure components and patterns would you use to guarantee incremental loads, schema drift tolerance, cross-region discovery via Purview, and secure data sharing with minimal latency and cost?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Databricks","Google","Microsoft"]},{"id":"q-4278","question":"Design a real-time feature store on Azure for a globally distributed ride-hailing platform: ingest streaming vehicle telemetry from Azure IoT Hub into ADLS Gen2, compute offline features with Databricks Delta Lake, and serve online features from Cosmos DB. Ensure feature versioning, schema evolution, data quality checks, end-to-end lineage via Purview, and multi-region consistency with latency constraints. What would you implement and why?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["NVIDIA","Snowflake","Uber"]},{"id":"q-4439","question":"You receive daily CSV and JSON logs from two partners via SFTP and REST, landing into ADLS Gen2 under /landing. Schemas drift over time. Design a beginner Data Factory pipeline that stages raw data, handles drift with Parquet schema evolution, loads to a simple star schema in Azure Synapse, and captures lineage in Purview plus basic data quality checks. What components and steps would you implement?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Citadel","Hugging Face","Stripe"]},{"id":"q-4542","question":"You operate a global data lake on Azure hosting per-tenant telemetry from multiple SaaS apps via Event Hubs into ADLS Gen2. Each tenant must have isolated data, incremental loads with late-arriving events, and fast analytics in Power BI/Synapse. Propose an end-to-end design using Delta Lake on Databricks, per-tenant namespaces, RBAC, data drift handling, and lineage with Purview. What components, schemas, and processes would you implement to satisfy isolation, consistency, and cost constraints?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["OpenAI","Slack"]},{"id":"q-4583","question":"Design an Azure data pipeline to ingest 100k events/sec from Azure Event Hubs into a Delta Lake on ADLS Gen2, with schema drift handled via Azure Schema Registry, end-to-end lineage in Purview, and upserts into Delta Lake using Spark (Synapse or Databricks). Address late events, out-of-order data, and cost with auto-scaling. What components and steps would you implement?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["Google","Instacart"]},{"id":"q-4629","question":"Context: You’re building an Azure data platform for a global ad-tech customer. Data arrives from partners as CSV via SFTP, JSON via REST, and a streaming source via Event Hubs. Ingest into ADLS Gen2, publish a Delta Lake lakehouse, and expose data products with end-to-end lineage in Purview. You must enforce per‑product data contracts, handle schema drift, and keep upserts into a star schema in Synapse. Design the pipeline, contracts, and monitoring strategy?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["Google","LinkedIn","Netflix"]},{"id":"q-4749","question":"Design an end-to-end pipeline to ingest real-time CDC from an on-prem SQL Server into ADLS Gen2, then populate a star schema in Azure Synapse. The solution must handle schema drift, late-arriving data, and ensure end-to-end lineage in Purview. Specify components, data formats, upsert strategy, partitioning, and validation/monitoring?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Google","Snap"]},{"id":"q-4768","question":"Scenario: design a real-time analytics pipeline for a global retailer. Ingest events from Event Hubs into ADLS Gen2 as JSON; auto-evolve schema; enforce per-tenant isolation and dynamic masking; capture end-to-end lineage in Purview; feed near real-time dashboards in Synapse within 5 seconds. Describe dataflow, components, schema governance, masking, latency monitoring, and cost/trade-offs of Parquet vs Delta with streaming?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Databricks","Microsoft","Netflix"]},{"id":"q-940","question":"Daily CSV exports arrive in an ADLS Gen2 container at /incoming/sales/. Files may evolve over time as columns drift. Build a beginner pipeline using Data Factory to stage raw data, infer/handle schema changes, and load a simple star schema in Azure Synapse Analytics. Include a watermark-based incremental load and basic scheduling. Which services and steps would you implement?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Adobe","Amazon","Zoom"]},{"id":"q-980","question":"Design a global streaming-to-batch data pipeline for a PayPal/Adobe/Netflix-scale analytics platform: ingest real-time clickstream from Event Hubs into ADLS Gen2, maintain a Delta Lake with drift-tolerant schema, mask PII at ingestion, expose masked aggregates via serverless SQL pool, and enforce end-to-end lineage with Purview while supporting cross-region DR and data residency. Which Azure components and patterns would you use, and how would you handle schema evolution and failure modes?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["Adobe","Netflix","PayPal"]},{"id":"q-1006","question":"Design a real-time telemetry ingestion pipeline for a fleet of autonomous vehicles on Azure. Events arrive at high volume per region; you must store compact per-vehicle summaries in Cosmos DB and archive raw events to Data Lake Gen2. How would you achieve exactly-once processing for aggregates, sub-200 ms latency, and zero data loss on transient failures? Propose architecture using Event Hubs, Functions, Databricks, and cross-region replication; justify idempotency and retry strategies?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Lyft","Slack","Tesla"]},{"id":"q-1136","question":"Design an end-to-end telemetry ingestion pipeline for 1M devices/min delivering messages {vehicleId, ts, lat, lon, speed}. Ingest via HTTPS into Event Hubs with vehicleId as partition key, process with a Function app (Event Hubs trigger) using batchSize=100; deduplicate per vehicle with Durable Entity and upsert to Cosmos DB multi-region. Explain data model, idempotency, Change Feed, backpressure, and monitoring?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Airbnb","Amazon","Tesla"]},{"id":"q-1248","question":"Design an end-to-end Azure ingestion pipeline for multi-tenant IoT events: thousands of devices per region send JSON to a gateway, per-tenant aggregates stored in Cosmos DB, raw data archived to Data Lake Gen2. Explain chosen services (Event Hub, Function/Durable Function, Cosmos DB with TTL, Data Lake), how you enforce per-tenant isolation and auditability, and how you achieve exactly-once processing and retry semantics?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Amazon","Citadel","Google"]},{"id":"q-1277","question":"Design a real-time multi-tenant feature-store pipeline on Azure for a high-velocity AI platform. Ingest telemetry events via Event Hubs (tenantId, featureName, value, ts). Build end-to-end streaming with exactly-once semantics, isolation by tenant, and low-latency online reads. Specify concrete components (Event Hubs, Spark Structured Streaming, Cosmos DB with tenantId partition, Redis online store), auditability, TTL, and testing strategy?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Google","Hugging Face","OpenAI"]},{"id":"q-1359","question":"A small API running on Azure Functions must securely retrieve a database connection string from Azure Key Vault at startup and refresh it periodically without restarting the function. Propose a beginner-friendly, low-latency approach using Managed Identity and Key Vault, including caching strategy, rotation handling, and error fallback?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["OpenAI","Salesforce","Snowflake"]},{"id":"q-1397","question":"Design a multi-region, per-tenant Azure API with data residency, deterministic retries, and exactly-once semantics at scale. Propose services (APIM, Functions + Durable Functions, Cosmos DB per-tenant, Front Door, Private Endpoints) and explain how per-tenant isolation, data residency, and retry determinism are achieved?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Goldman Sachs","Oracle","Plaid"]},{"id":"q-1539","question":"Design a real-time, multi-tenant analytics pipeline for a chat platform: events arrive at 5-10k msgs/sec total via Azure Event Hubs; process with Azure Databricks on Delta Lake stored in ADLS Gen2; governance via Unity Catalog; ensure strict per-tenant isolation, auditability, and exactly-once processing; TTL/retention; cross-region read; cost constraints. Describe architecture decisions, data layouts, and failure scenarios?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Databricks","Discord","Microsoft"]},{"id":"q-1565","question":"Design a zero-trust, per-tenant access model to a private Azure SQL Database from a microservices mesh. Use Azure AD, Managed Identities, Row-Level Security, and dynamic data masking. Explain how you enforce least privilege, tenant isolation, auditing, and how you validate access policies in CI/CD?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Discord","Google","Plaid"]},{"id":"q-1594","question":"You're building a real-time, multi-tenant feature-flag platform on Azure to serve traffic across three regions with sub-50ms evaluation latency. Each tenant has per-flag rules, canary/A/B experiments, and strict audit requirements. Outline end-to-end design: data model for flags and experiments, evaluation path, storage choices (Cosmos DB vs SQL), caching, event sourcing, cross-region synchronization, security (Managed Identities, Key Vault, RBAC), rollback strategy, and failure modes. Include how you'd test canary safety and ensure tenant isolation?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Citadel","DoorDash"]},{"id":"q-1620","question":"You're architecting a **multi-region telemetry ingestion** pipeline for a real-time fraud-detection service. Edge devices per region push JSON events to **Azure IoT Hub**; you must ingest, partition by tenant, store raw and processed results with exact-once semantics, and enforce strict per-tenant isolation and auditability within tight cost constraints. Propose architecture choices (IoT Hub, **Event Hubs**, **Delta Lake**, **Unity Catalog**, **Cosmos DB**), data layout, and failure modes; how would you test end-to-end dedup and cross-region replayability?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Google","Hugging Face"]},{"id":"q-1668","question":"You’re building a beginner Azure Function (HTTP trigger) that calls a 3rd‑party API. Do not store API keys in code or app settings. How would you securely fetch the API key at runtime from Azure Key Vault using a managed identity? Outline the steps to enable the identity, grant access, and provide a minimal code snippet to read the secret?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["LinkedIn","Slack"]},{"id":"q-1727","question":"You're building a polyglot Azure-based service with HTTP APIs, background workers, and a data lake. You need end-to-end tracing across Functions, AKS, and Databricks jobs using OpenTelemetry and a single trace across services. Describe how you'd implement tracing, propagate context, and collect/export to Azure Monitor, including sampling strategy and validation steps?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Databricks","Meta","Zoom"]},{"id":"q-1791","question":"You're building an Azure-native, multi-tenant data platform for real-time payments used by PayPal and Tesla. Ingest via Event Hubs, process with AKS and Functions, store in Delta Lake on ADLS Gen2, expose a data API. How would you enforce strict per-tenant isolation, achieve end-to-end exactly-once semantics across services, and implement a shadow-traffic ML model deployment with safe rollback and audit trails? Include architecture choices, data layouts, and failure modes?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["PayPal","Tesla"]},{"id":"q-1839","question":"You're building an Azure IoT telemetry platform for a global fleet. Devices send 20–30k events/sec to IoT Hub. You need per-tenant isolation, real-time enrichment (geolocation, device type), and fan-out to three sinks: Delta Lake on ADLS Gen2, Azure Data Explorer dashboards, and an AI inference service on AKS. Must guarantee at-least-once semantics, handle out-of-order data, and support cross-region DR with measurable RPO. Compare two architectures: (A) serverless micro-batch: IoT Hub → Event Hubs → Functions for lightweight enrichment; Spark Structured Streaming writes to Delta Lake on ADLS Gen2; sinks feed Data Explorer and AKS inference. (B) pure streaming: Event Hubs → Spark Structured Streaming with larger cluster, stricter SLAs, and end-to-end exactly-once semantics. Explain data models, failure modes, and governance?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Amazon","Google","Netflix"]},{"id":"q-1962","question":"You’re building a beginner Azure Function (HTTP trigger) that receives event payloads and stores them in a data container. Design a lightweight, auditable approach to log every write without slowing latency. Include how you would generate an immutable audit trail and what storage pattern you’d use. Provide a minimal code snippet to append an audit line with timestamp, eventId, and userId derived from Authorization header?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["Google","Tesla"]},{"id":"q-1995","question":"Describe how you would implement a timer-triggered Azure Function that runs every 15 minutes to poll an on‑prem REST endpoint and write a daily aggregation blob to Azure Blob Storage. How would you guarantee idempotent writes to avoid duplicates across retries, including blob naming strategy and a minimal check-then-write code pattern?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["Cloudflare","Plaid","Uber"]},{"id":"q-2059","question":"Design an Azure-native data export service for a fintech that must export per-customer data on demand and on a schedule, with strict data residency, consent checks, and an audit trail. Use ADLS Gen2, Cosmos DB, Event Grid, Durable Functions or Functions, and Key Vault. Explain data partitioning, encryption, access control, idempotency, and failure modes including retries, outages, and rollback. Provide a concrete data model and flow?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Goldman Sachs","Robinhood","Stripe"]},{"id":"q-2109","question":"You're building a real-time moderation pipeline for a global social app on Azure. Ingest flows via Azure Event Hubs at 20-40k msgs/sec, then you apply NLP classification in a chain of Functions (or Durable Functions) to flag policy-violating messages, store results in Cosmos DB with per-tenant isolation, and index metadata in Azure Cognitive Search for fast queries. How would you design for latency under 150ms per event, strict per-tenant data isolation, idempotent retries, and auditable trails across services? Include error handling and rollback strategies?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Airbnb","Amazon","Plaid"]},{"id":"q-2155","question":"You're building a tenant-aware API gateway on Azure that serves dozens of microservices for hundreds of tenants. Implement per-tenant quotas, latency budgets, and canary rollouts. Describe a concrete architecture using Azure API Management, Azure Front Door, Redis for rate-limiting, and per-tenant state in Cosmos DB or Redis; explain failure modes, rollback, and testing strategies under traffic spikes?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["IBM","Uber"]},{"id":"q-2176","question":"You're building a beginner Azure-based image processing workflow: a user uploads a photo to Azure Blob Storage. A Function App should trigger, resize the image into three thumbnails, store metadata in Cosmos DB with per-user partition keys, and publish a status message to Azure Service Bus. How would you implement idempotent processing, retries, and per-user isolation, with a minimal code sketch showing how to deduplicate on blob name and initiate the three resizes?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["Adobe","Cloudflare"]},{"id":"q-2203","question":"Design a privacy-preserving, auditable streaming pipeline for real-time telemetry in a multi-tenant SaaS on Azure. Ingest via Azure Event Hubs at ~60k msgs/sec; run per-tenant aggregation and anomaly detection in Durable Functions; store per-tenant data in Cosmos DB with strict isolation; enforce data residency per tenant region and provide rollback for feature-flag changes. Describe architecture, data model, exactly-once guarantees, audit trails, and failure modes?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Anthropic","Plaid","Slack"]},{"id":"q-2235","question":"Design a real-time, multi-tenant feature-flag platform on Azure for a geo-distributed microservices stack. Tenant isolation must be enforced; flag evaluation latency < 200 ms at peak. Use API Management, Functions, Cosmos DB (partitioned by tenant), Redis (near the API layer), and Event Grid. Describe data model, cache strategy, canary rollout, rollback plan, auditing, and failure handling?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Stripe","Uber","Zoom"]},{"id":"q-2307","question":"You're building a privacy-preserving analytics marketplace on Azure: ingest telemetry via Event Hubs; anonymize with differential privacy during ingestion or enrichment; store tenant-scoped data in Delta Lake on ADLS Gen2 with strict per-tenant partitioning; catalog lineage in Azure Purview; and share results through per-tenant REST APIs with RBAC and data-sharing controls. Outline the architecture, data flow, and trade-offs to meet privacy, latency, and cost targets?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Bloomberg","Oracle","Scale Ai"]},{"id":"q-2379","question":"Build a beginner Azure REST API using Azure Functions (HTTP trigger) to manage product data for multiple tenants. Each tenant must be isolated, configs sourced from Azure App Configuration, and telemetry sent to Application Insights. Describe the authentication model, per-tenant data isolation strategy, and a minimal test plan, plus a small code sketch showing API key validation and tenant extraction?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["Airbnb","Hashicorp","PayPal"]},{"id":"q-2484","question":"Design a multi-tenant telemetry pipeline on Azure: events arrive through Event Hubs per-tenant, processed by Durable Functions, stored in per-tenant Cosmos DB, and surfaced via Azure Data Explorer dashboards. How would you implement end-to-end tracing with OpenTelemetry across all components, ensure per-tenant correlation, minimize overhead, handle retries idempotently, and preserve privacy controls (pseudonymization, access controls)?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Adobe","Apple","Meta"]},{"id":"q-2679","question":"Build a real-time fraud scoring pipeline on Azure for a multi-tenant fintech. Ingest 60k msgs/sec via Event Hubs, run scoring in Azure ML, store per-tenant results in Cosmos DB with region-bound writes, and publish to downstream systems. How would you enforce per-tenant data residency, hit sub-200ms latency, support hot model updates, and ensure end-to-end auditability and replay safety? Include regional failover and RBAC?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Coinbase","OpenAI","PayPal"]},{"id":"q-2724","question":"You're building a cross-cloud data onboarding and sharing pipeline on Azure: per-tenant data lands in ADLS Gen2, governance is enforced via Purview with per-tenant contracts, then data is published to Snowflake (external stage) and Databricks (Delta Sharing) for analytics. How would you implement data contracts, isolation, lineage, access control, and cost accounting end-to-end, including failure modes?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Databricks","Snowflake","Tesla"]},{"id":"q-2796","question":"You're designing a globe-spanning telemetry pipeline for an industrial platform: devices push MQTT data to IoT Hub, events feed into Event Hubs at up to 200k msgs/min, Durable Functions compute windowed aggregates, and results are written to Cosmos DB with tenantId partition keys. How would you meet sub-200ms tail latency, strict per-tenant isolation, idempotent retries, cross-region reads, private endpoints, auditable traces, plus DR and cost controls?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["LinkedIn","Tesla"]},{"id":"q-2877","question":"You're building a real-time fraud-detection pipeline for a global fintech app on Azure. Ingest at 100k msgs/sec via Event Hubs; orchestrate with Durable Functions; call a low-latency Azure ML scoring endpoint; persist per-tenant results in Cosmos DB; classify data with Purview; ensure backpressure, idempotent retries, DLQ, and cross-region audit trails. How would you design and what are the key failure modes and mitigations?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Databricks","Scale Ai","Tesla"]},{"id":"q-2932","question":"You're building a real-time, multi-tenant order routing pipeline on Azure. Ingest ~60k–120k events/sec from mobile/web clients via Azure Event Hubs; apply per-tenant business rules in Durable Functions; route to multiple fulfillment providers; persist per-tenant order state in Cosmos DB; expose a low-latency query API. How would you design to guarantee exactly-once processing, strict per-tenant isolation, auditable lineage, and resilience to provider outages, including error handling and rollback strategies?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["DoorDash","IBM","Zoom"]},{"id":"q-2949","question":"Design a beginner Azure data pipeline: land per-tenant CSVs in Azure Blob via Data Factory, create per-tenant Delta tables under Unity Catalog for isolation, and implement MERGE-based upserts in PySpark (using event_id as the key). Include simple validation, audit logging, and retry handling; outline data model, steps, and provide a minimal PySpark MERGE snippet?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["Databricks","NVIDIA","Snowflake"]},{"id":"q-3021","question":"You're building a real-time fraud detection pipeline for a global rideshare app on Azure. Ingest 50-100k events/sec via Event Hubs; perform per-tenant feature extraction and ML scoring in a per-tenant isolated runtime (Durable Functions or AKS); persist scores in Cosmos DB with tenant isolation; expose a low-latency REST API; ensure exact-once delivery, drift-aware model updates, and auditable trails. How would you design and test this end-to-end?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Amazon","DoorDash","Google"]},{"id":"q-3212","question":"In a multi-tenant telemetry platform on Azure, events arrive at 60-100k msgs/sec via Event Hubs. Design an end-to-end pipeline that preserves strict per-tenant isolation, supports idempotent retries, and provides auditable trails for audits. Use a Durable Functions orchestrator for per-tenant enrichment, Cosmos DB with tenant partition keys, and surface analytics via Cognitive Search. Include disaster recovery, cost controls, and governance considerations?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Discord","Tesla"]},{"id":"q-3279","question":"Privacy-first cross-tenant data export service on Azure: tenants configure export jobs to move selected telemetry from a central data lake to their own storage destinations (Azure Blob or S3-compatible) in real time or near-real-time. Data ingested through Azure Event Hubs, processed by a chain of Functions and Durable Functions to apply redaction/pseudonymization rules stored per-tenant in Cosmos DB, then written to the destination with per-tenant encryption keys from Key Vault. How would you design end-to-end flow, guarantees (idempotence, exactly-once), security boundaries, data residency, and auditability, including failure handling and SLA targets? Include concrete components and data formats?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Adobe","MongoDB","Salesforce"]},{"id":"q-3360","question":"You’re building a beginner Azure Function HTTP webhook receiver that ingests events from multiple partner vendors. Each vendor has a tenantId and a shared secret stored in Key Vault. Outline a secure flow: signature validation via Key Vault secrets (Managed Identity), per-tenant rate limiting using Redis, idempotent Cosmos DB writes with tenantId as partitionKey and eventId as id, and App Insights telemetry. Provide a minimal code sketch for tenant extraction and signature check?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["Anthropic","OpenAI","Snap"]},{"id":"q-3580","question":"Design a multi-tenant telemetry analytics stack on Azure for devices generating 30k events/sec. Ingest via Azure Event Hubs, enrich with Durable Functions for dedup and schema normalization, and store per-tenant data in Azure Data Explorer with fast queries. Outline data model, exactly-once processing, per-tenant RBAC, TTL retention, cross-region reads, and cost controls; include failure modes and audit trails?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Coinbase","Tesla","Zoom"]},{"id":"q-3630","question":"Design a fintech telemetry pipeline on Azure to ingest 50k msgs/sec of client telemetry via Event Hubs, process with Spark Structured Streaming on Databricks, redact PII per-tenant, and store redacted data in ADLS Gen2 Parquet partitions by tenant/date. Expose per-tenant aggregates in Cosmos DB for dashboards, ensure exactly-once processing, cross-region replication, retention, audit trails, and security (Managed Identity + Key Vault). What is your implementation plan and key trade-offs?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Citadel","LinkedIn","Uber"]},{"id":"q-3752","question":"You're building a beginner Azure Functions HTTP API to manage a tenant-scoped product catalog. Data lives in Azure Table Storage with PartitionKey=tenantId. To reduce latency, add a read-through cache using Azure Cache for Redis. Describe the end-to-end flow and TTL strategy, and provide a minimal code snippet showing GET /catalog/{tenantId}/{productId} that checks Redis first, then Table Storage, and caches the result. How would you implement this?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["Citadel","Square","Tesla"]},{"id":"q-3823","question":"Design a compliant, multi-tenant data ingestion path on Azure for a fintech app: Ingest 100k msgs/sec via Event Hubs, orchestrate with Durable Functions, use per-tenant keys from Key Vault, store isolated data in Cosmos DB, and emit auditable logs to immutable blob storage with tracing. How would you ensure data isolation, idempotence, auditability, and deletion requests?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Amazon","Goldman Sachs","Meta"]},{"id":"q-3825","question":"You're designing a global, multi-tenant fintech data pipeline on Azure. Ingest 100k events/sec from mobile and web via Azure Event Hubs, process with Azure Databricks Spark into Delta Lake on ADLS Gen2, with strict per-tenant isolation. Implement data masking for PII, audit trails via Azure Purview, TTL retention, and cost-aware autoscaling. How would you structure data layouts, governance, failure modes, and testing?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Databricks","Two Sigma"]},{"id":"q-3929","question":"You're building a real-time, multi-tenant ML inference stack on Azure for 100k device events/sec. Ingest via Azure Event Hubs, batch via Durable Functions per tenant, call a single Azure ML endpoint for inference, and store per-tenant results in Cosmos DB with TTL. How would you ensure end-to-end latency under 200ms, strict isolation, idempotent retries, and auditable trails, plus testing strategies?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Amazon","Google","Two Sigma"]},{"id":"q-3987","question":"You're designing a secure, scalable data integration between Salesforce and an Azure Databricks lakehouse. Enable CDC for accounts and opportunities, stream changes through Event Hubs into Databricks, and upsert into Delta Lake with per-tenant isolation. How would you implement incremental upserts, PII masking, GDPR deletions, data cataloging/audit via Purview, and robust replay/error handling? Include testing and rollback plan?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Databricks","Salesforce"]},{"id":"q-3999","question":"You're operating a global multi-tenant event pipeline on Azure: Event Hubs ingest, Azure Functions processing, and Cosmos DB storage with per-tenant isolation. How would you implement end-to-end observability using OpenTelemetry and Azure Monitor to achieve tenant-aware tracing across components, with correlation IDs, dynamic sampling, and no cross-tenant leakage? Describe instrumentation points, data retention, and costs?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Bloomberg","Stripe"]},{"id":"q-4034","question":"You're building a beginner Azure Functions HTTP API to serve per-tenant feature flags. Flags live in Azure App Configuration, with per-tenant overrides stored in Azure Cosmos DB; the API must resolve a tenant's flags, cache results in Azure Redis Cache for 60s, and gracefully fallback to defaults if a tenant is missing. Implement GET /flags/{tenantId}?name={flagName} and provide a minimal Node.js snippet that reads from Redis, then App Configuration, with a default if not present?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["Databricks","Google","OpenAI"]},{"id":"q-4104","question":"You're building a beginner Azure per-tenant image-processing pipeline. A user uploads an image to a tenant-scoped blob container; an Event Grid BlobCreated event triggers an Azure Function to generate a 300x300 thumbnail, store it in a per-tenant thumbnails container, and log an audit entry in Azure Table Storage. Enforce per-tenant isolation, idempotent retries, and dead-lettering via a Storage Queue. Outline the end-to-end flow and provide a minimal Node.js blob-trigger function using sharp to resize and save the thumbnail?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["Coinbase","Google"]},{"id":"q-4243","question":"You're building a beginner Azure Functions HTTP API to accept per-tenant document uploads (PDFs/images) for Instacart partners. Store files in Blob Storage under tenantId/docs/, generate thumbnails, and keep metadata in Cosmos DB with partitionKey=tenantId. How would you implement secure uploads, per-tenant isolation, simple virus scanning, and cost-aware scaling?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["Instacart","NVIDIA"]},{"id":"q-4461","question":"Describe an end-to-end per-tenant data path: ingest via Event Hubs, tenant-scoped routing using message headers, per-tenant durable functions orchestration, and ADLS Gen2 folders /tenants/{tenantId}/. Store to Delta tables with per-tenant partitions, govern with Purview, enforce retention, and tag costs. Include isolation, retries, audit trails, and testing strategies?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Salesforce","Zoom"]},{"id":"q-4522","question":"You’re building a global multi-tenant analytics pipeline on Azure for a SaaS product. Ingest via Event Hubs, processing with Synapse Spark, data stored per-tenant in ADLS Gen2, queried via serverless SQL. How would you ensure strict per-tenant isolation, dynamic masking at query time, and auditable lineage across Purview, while controlling costs and handling late data? Include testing and rollback strategies?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Airbnb","Snowflake","Two Sigma"]},{"id":"q-4567","question":"You're building a global, real-time feature-flag evaluation service on Azure. End users require a response in ~40ms; flags are stored in Azure App Configuration with per-tenant labels and dynamic rules. How would you design data flow, caching, eviction, audit logging, and failover to meet latency and correctness under partial outages?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Apple","Netflix"]},{"id":"q-4611","question":"You’re building a global edge-accelerated API platform for AI features across tenants. Use Azure API Management in front of a multi-region AKS model-service stack; store per-tenant state in Cosmos DB with tenantId as the partition key and enforce per-tenant quotas with Redis; ensure data residency by region pinning; describe end-to-end latency targets, fault-tolerance, testing, and rollback plans?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Cloudflare","OpenAI","Scale Ai"]},{"id":"q-4622","question":"You're building a global, multi-tenant AI feature store on Azure for tenants across major platforms (e.g., Meta, DoorDash, Hugging Face). Ingest telemetry from mobile/web via Azure Event Hubs, process in Azure Databricks to derive features, store raw data in ADLS Gen2, and publish derived features to a real-time feature store in Azure SQL/Delta Lake with per-tenant isolation. How would you design data contracts, enforce tenant-level access, support cross-region replication, ensure privacy controls, and validate disaster recovery and cost constraints?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["DoorDash","Hugging Face","Meta"]},{"id":"q-4726","question":"Design a per-tenant API gateway pattern on Azure for a multi-tenant SaaS: API Management fronts microservices, routes requests by tenant, enforces per-tenant quotas, provides isolation, and logs governance data. How would you implement routing, rate limiting, authentication, and observability to support 5k+ rps with low latency?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Airbnb","Citadel","Google"]},{"id":"q-891","question":"Design a beginner-friendly serverless image-upload API on Azure: clients POST JPEGs to an HTTP-triggered Function, which saves the file to Blob Storage, enqueues a processing job, and stores a metadata record in Cosmos DB. How would you ensure idempotency, handle retries with back-off, and keep costs predictable on a Consumption plan?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["Robinhood","Salesforce","Tesla"]},{"id":"q-973","question":"Case: You’re building a beginner-friendly Azure API that accepts events from mobile apps. Each event includes userId, eventType, and timestamp. The API should write a compact summary to Cosmos DB and stream raw events to Event Hubs for analytics. On a Consumption plan, outline the minimal architecture, bindings, and error handling to ensure low latency, safe retries, and no data loss during transient outages?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["PayPal","Scale Ai"]},{"id":"q-1063","question":"With a 60-service monorepo deployed to Azure subscriptions across three regions, design an end-to-end release strategy that builds per-service artifacts, promotes to dev/stage/prod environments, gates each promotion with environment approvals tied to region-owner groups, and enforces that PRs link to an Azure Boards item. Include a minimal YAML template and gating approach?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Airbnb","Cloudflare"]},{"id":"q-1135","question":"Design an Azure DevOps multi-tenant canary deployment pipeline for a SaaS service that promotes per-tenant changes to prod only after a staged rollout window, uses tenant-scoped feature flags, enforces per-tenant approvals before prod, and rolls back automatically if telemetry thresholds are exceeded; outline the pipeline structure, environment gates, and auditing approach?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-devops-engineer"],"companies":["LinkedIn","Stripe"]},{"id":"q-1255","question":"Scenario: You’re configuring a new Azure DevOps project with Repos and Boards for a small service. How would you implement beginner-friendly PR governance to ensure PRs into main are linked to a Boards work item, the PR title includes the work item ID, a PR validation build runs and passes, and an automatic staging deployment with a manual prod gate? Outline exact steps and considerations?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Microsoft","Robinhood"]},{"id":"q-1329","question":"In Azure DevOps, design an end-to-end pattern to provide per-PR ephemeral environments in AKS for a microservices app. Use a single AKS cluster with namespace-per-PR, Helm for deployments, Azure Key Vault for per-PR secrets, and a TTL-based teardown. Describe the pipeline steps, gating, security, costs, and how to isolate test data and telemetry. End with ?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["DoorDash","LinkedIn"]},{"id":"q-1355","question":"Configure a PR-only security gate for a Node.js service: trigger on PRs to main (trigger: none; pr: branches include: main) and skip pushes. Add a security job that runs npm ci, npm audit --json, and fail if any advisories of severity high or critical exist, printing a concise summary. This blocks the PR until issues are addressed?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Google","Robinhood"]},{"id":"q-1423","question":"In Azure DevOps for a monorepo with multiple npm packages, design a PR validation that runs tests only for changed packages, builds per-package artifacts, and gates PR merge until all relevant tests pass; include a minimal YAML snippet showing path-based triggers, npm workspaces, and artifact publishing?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Databricks","MongoDB","Twitter"]},{"id":"q-1463","question":"How would you implement an Azure Pipelines YAML that tests a Node.js app against Node.js 14 and 16 using a matrix, caches npm dependencies with Cache@2 to speed up PR builds, and invalidates the cache when package-lock.json changes? Provide the YAML snippet and explain the key derivation?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["IBM","PayPal"]},{"id":"q-1486","question":"Design a blue/green canary rollout in AKS for a critical microservice using Azure Pipelines. Use Istio for traffic splitting, separate namespaces for canary and stable, and Azure Key Vault for per-environment secrets. Describe pipeline structure, gating with approvals, health checks, telemetry correlation across versions, auto-rollback, and cost controls?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Databricks","Microsoft","Scale Ai"]},{"id":"q-1513","question":"Design a scalable Azure Pipelines pattern for a multi-tenant SaaS app where each tenant has its own Azure Key Vault and database shard. Outline a per-tenant environment bootstrap, secret injection into deployments, governance checks (Azure Policy, approvals), telemetry tagging to prevent cross-tenant leakage, TTL-based teardown, and a robust rollback path with canary moves. How would you implement this end-to-end?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Apple","Bloomberg"]},{"id":"q-1535","question":"You have a small Python service in Azure Repos. Create a beginner CI pipeline in Azure Pipelines that: (1) runs on push/PR, (2) installs Python 3.11, (3) runs pytest with coverage, (4) builds a wheel and publishes it as a build artifact, and (5) gates release to Prod with a manual approval and a built-in security scan before promotion. How would you implement this?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Discord","MongoDB","Snap"]},{"id":"q-1644","question":"In Azure DevOps, you manage a beginner Node.js microservice stored in Azure Repos. Create a practical CI/CD flow (YAML) that: 1) triggers on push and PR, 2) uses Node.js 18, 3) runs npm ci and npm test -- --coverage, 4) builds a Docker image and pushes to a private Azure Container Registry, 5) updates a Helm chart in the repo and promotes to a staging environment with a manual approval gate and a basic readiness check. What files and steps would you implement?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Google","LinkedIn","Oracle"]},{"id":"q-1670","question":"Design a robust multi-region release strategy in Azure DevOps for a real-time collaboration service deployed to AKS. The pipeline must support per-region canary rollouts using Istio, region-scoped feature flags in Azure App Configuration, per-region secrets in Azure Key Vault, auto-rollback on SLA deviations, and cost controls via HPA and budget alerts. Describe the YAML structure, gating approvals, health checks, telemetry correlation across regions, and rollback triggers. What files and steps would you implement?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-devops-engineer"],"companies":["Microsoft","Netflix","Zoom"]},{"id":"q-1770","question":"Design a beginner CI/CD workflow in Azure DevOps for a Go microservice stored in Azure Repos, containerized and deployed to AKS. Requirements: 1) Triggers on push and PR; 2) go test ./... -cover and build the binary; 3) multi-stage Dockerfile to build and publish image to ACR; 4) Helm upgrade dev with values.dev.yaml; 5) manual approval gate before prod upgrade with values.prod.yaml; 6) readinessProbe and livenessProbe in deployment; 7) health endpoint for readiness and rollout status check in pipeline. What files and steps would you implement?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Hashicorp","Lyft","Oracle"]},{"id":"q-1850","question":"Design an auditable, region-aware release workflow in Azure DevOps for a global payment service deployed to AKS. The pipeline must promote a single immutable container image across regions (EU, US, APAC) in sequence, generate SBOMs, run security scans, enforce region-specific approvals, implement per-region health checks and rollback triggers, and record full provenance in Azure Artifacts. Describe the YAML structure, gating, and rollback strategy?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["IBM","Meta","Stripe"]},{"id":"q-1911","question":"Design a cross-region release pipeline in Azure DevOps for a high-availability service deployed to AKS across two regions. Use Istio for traffic splitting, per-region Key Vault secrets, and region-scoped feature flags via Azure App Configuration. Implement a single reusable YAML template shared by regions, with region-specific approvals, health checks, telemetry correlation, auto-rollback on cross-region latency SLA breach, and cost controls via HPA and budget alerts. What files and steps would you implement?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Lyft","Meta"]},{"id":"q-1927","question":"Design a tenant-aware release pipeline for a multi-tenant SaaS service deployed to AKS in two regions. Include per-tenant RBAC with Azure AD, per-tenant secrets in Key Vault, per-tenant feature flags in App Configuration, and per-tenant canary traffic routed via Front Door. Architect a policy-driven gate that rejects a release if any tenant SLA is missed, and implement auto-rollback on regional drift. Provide files, stages, and gates you would implement?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Airbnb","Cloudflare"]},{"id":"q-1955","question":"Design an Azure Pipelines release for an AKS-hosted data ingestion service that reads from MongoDB Atlas and writes to Snowflake. Roll out a new transformer to three regions (US, EU, APAC) with per-region approvals, blue/green deployment, and a runtime feature flag to switch transformers. Include data-quality gates, idempotent Snowflake upserts, telemetry lineage, and auto-rollback on ingestion errors within 24 hours. Provide YAML structure and gating strategy?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-devops-engineer"],"companies":["MongoDB","Snowflake"]},{"id":"q-1977","question":"Design an incident-driven rollback for a real-time analytics service deployed on AKS via Azure DevOps. When production alerts breach SLA (latency > 500ms or error rate > 2%), automatically rollback to the previous stable revision, shift traffic back using Istio, pause the Canary, and run post-rollback health checks and synthetic tests before resuming promotion. Describe pipeline changes, alert integrations, gating, rollback criteria, and telemetry correlation?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Citadel","Cloudflare"]},{"id":"q-2103","question":"Design a PCI-DSS compliant CI/CD workflow in Azure DevOps for a payments microservice deployed to AKS, with per-tenant data isolation, image signing, SBOM generation, policy gates, and attestation-based deployments. Describe YAML structure, Key Vault secrets management, runtime scanning, rollback criteria, and how you’d prove compliance. What files and steps would you implement?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Lyft","Tesla"]},{"id":"q-2138","question":"Design a zero-downtime fleet upgrade for 5,000 Azure IoT Edge devices across regions using **Azure Pipelines** to build and sign Edge modules, publish to a registry, and use **Device Update for IoT Hub** to orchestrate per-group rollouts. Include per-group approvals, region-specific configuration, telemetry correlation, and automatic rollback if update failures exceed threshold. How would you structure the pipeline, gating, and rollback criteria?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Apple","Plaid"]},{"id":"q-2186","question":"Design a beginner Azure DevOps YAML pipeline for a Python FastAPI app stored in Azure Repos and containerized for Azure App Service. The pipeline must trigger on pushes and PRs to main, run unit tests with coverage, build and push a Docker image to ACR, deploy to the App Service staging slot, run a smoke test on /health, and swap to production if the smoke test passes. Use Key Vault/App Configuration for per-environment secrets and propagate a correlation-id header for telemetry?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Coinbase","Lyft"]},{"id":"q-2239","question":"Design a beginner-friendly YAML pipeline in Azure Pipelines for a Python Flask app in Azure Repos deployed to Azure App Service (Linux). Include triggers on push and PR, test with pytest/coverage, lint, publish artifacts, and multi-stage deployments to dev, stage, prod with per-environment secrets from Azure Key Vault, approvals on Stage/Prod, and a post-deploy health check plus rollback path?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Bloomberg","Microsoft","MongoDB"]},{"id":"q-2266","question":"Design a real-world release approach in Azure DevOps for a globally distributed identity service on AKS. Implement progressive rollout by percentage with Azure Front Door routing; per-region namespaces; per-tenant isolation via RBAC; automated rollback on latency >300ms or error rate >1%; enforce per-env budgets via Azure Cost Management; telemetry correlation with Application Insights. Describe YAML structure, gating, health checks, and rollback criteria?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Apple","LinkedIn","Microsoft"]},{"id":"q-2327","question":"Create a beginner CI/CD YAML in Azure DevOps for a Python Flask app stored in Azure Repos, deploying to Azure App Service with two deployment slots: staging and production. Requirements: triggers on push and PR; install dependencies with pip, run pytest with coverage; deploy to staging slot; perform smoke tests against /health and /ready; if tests pass, require manual approval before swapping to production; on failure, rollback or revert swap; configure per-environment settings via deployment slots and Key Vault integration?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Databricks","PayPal"]},{"id":"q-2374","question":"Design an Azure DevOps pipeline strategy for a multi-tenant SaaS on AKS with per-tenant namespaces. Create a reusable YAML template that provisions per-tenant environments, pulls secrets from Azure Key Vault, assigns per-tenant RBAC, and enforces per-tenant budgets. Include dynamic environment creation, approvals, canary deployment with traffic routing, health checks, and automatic rollback on SLA breaches. Provide gating and rollback criteria?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Amazon","Apple","Meta"]},{"id":"q-2456","question":"In a monorepo with 50 microservices (AKS deployments and Azure Functions) and Azure Pipelines, design a YAML-driven CI for PRs that builds and tests only the services touched by the PR, produces per-service Docker images pushed to Azure Container Registry, uses per-service templates, runs unit/integration tests, caches dependencies, and gates deployment with per-service approvals. Describe structure, triggers, and rollback strategy?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-devops-engineer"],"companies":["Bloomberg","Goldman Sachs","Slack"]},{"id":"q-2482","question":"You are scaling a globally distributed real-time messaging microservice on AKS across 6 regions. You must implement a per-region progressive rollout using Istio traffic shifting, region-scoped feature flags from Azure App Configuration, per-tenant RBAC in Kubernetes, and cost controls via Azure Cost Management. Outline a concrete Azure DevOps YAML pipeline that does region-wise canary, approvals, readiness and synthetic health checks, telemetry correlation in Application Insights, and automatic rollback if p95 latency or error rate exceed thresholds?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Google","MongoDB","Twitter"]},{"id":"q-2556","question":"Design a beginner YAML pipeline in Azure DevOps for a Python FastAPI app stored in Azure Repos. Trigger on push/PR, install dependencies, run pytest with coverage, build a Docker image, push to ACR, deploy with Helm to AKS in dev/stage/prod namespaces using per-env values. Add a manual prod approval gate and a cost-forecast check (az costmanagement forecast); fail when forecast exceeds a threshold; include health checks and a rollback path on failure?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Google","Microsoft","Oracle"]},{"id":"q-2607","question":"Design a beginner Azure DevOps YAML pipeline for a Java Spring Boot REST API stored in Azure Repos, containerized and deployed to Azure App Service (Linux) with two deployment slots: staging and production. Include: triggers on push and PR; mvn -B -DskipTests=false test; build Docker image and push to ACR; deploy to staging; run a lightweight load test with k6 against /health and /metrics; if thresholds pass, require manual approval before swapping to production; on failure, rollback?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Cloudflare","Hugging Face","Snap"]},{"id":"q-2850","question":"Design a single Azure DevOps YAML pipeline to deploy a multi-tenant microservice to AKS in two Azure regions. Implement per-tenant RBAC in Kubernetes, Istio-based region-aware traffic shifting, and per-tenant feature flags from Azure App Configuration. Include gating with approvals, synthetic health checks, telemetry correlation in Application Insights, and an automatic rollback on SLA breach with traffic shift back and cost budget enforcement?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Adobe","Tesla"]},{"id":"q-2942","question":"Create a beginner Azure DevOps YAML pipeline for a Node.js REST API stored in Azure Repos. The pipeline must: trigger on pushes and PRs; cache npm modules; npm ci; lint with ESLint; tests with NYC coverage; build a Docker image and push to Azure Container Registry; deploy to a dev AKS namespace; smoke test /health; require manual approval before deploying to prod; deploy to prod after approval and run a second smoke test?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Meta","MongoDB","Salesforce"]},{"id":"q-3045","question":"Design an Azure DevOps pipeline to deploy Databricks notebooks across two workspaces (dev and prod) using REST API, sourcing environment-specific secrets from Azure Key Vault, and validating with a data sanity notebook before production. Include a gated approval step and a rollback plan that restores the previous notebook version and pauses prod jobs on failure. What would your pipeline look like?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-devops-engineer"],"companies":["Amazon","Databricks"]},{"id":"q-3146","question":"Design a beginner Azure DevOps YAML pipeline for a Node.js API in Azure Repos deployed to Azure App Service (containers). Trigger on pushes to main and PRs; run npm ci; npm test -- --coverage; build and push a Docker image to ACR; deploy to staging slot; run smoke tests at /health and /ready; on success, swap to production; on failure, rollback. Include per-slot config via Azure Key Vault secrets?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["MongoDB","PayPal"]},{"id":"q-3249","question":"Design an Azure DevOps YAML pipeline to deploy a multi-region service on AKS with Istio traffic shifting. Each region uses its own namespace and AKS context; fetch region secrets from Azure Key Vault; apply per-tenant feature flags from Azure App Configuration; implement progressive rollout with approvals, health checks, and telemetry correlation in Application Insights; and auto-rollback on latency or error thresholds with cost controls. What would your pipeline look like?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Apple","Square"]},{"id":"q-3356","question":"Create a beginner YAML pipeline in Azure DevOps for a Python FastAPI app stored in Azure Repos, deployed to Azure App Service via a Docker container. Include: build & test (pytest with coverage), publish an artifact, deploy to the staging slot, run smoke tests at /health and /ready, require a manual approval to swap to production, and load per-env settings from Azure App Configuration with secrets from Key Vault?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Anthropic","Google","Snap"]},{"id":"q-3418","question":"Design a beginner Azure DevOps YAML pipeline for a .NET Core API stored in Azure Repos. The pipeline must (1) restore, build, run tests with coverage, and run a lint/format check; (2) publish a build artifact; (3) deploy to Azure App Service with two slots: staging and production; (4) fetch per-slot secrets from Azure Key Vault via Managed Identity and apply slot-specific app settings; (5) run a health check against /health on staging; (6) swap to production only after explicit approval; if health fails or swap fails, rollback by re-swapping or re-deploying staging. Provide the YAML skeleton and gating considerations?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Citadel","Lyft","Zoom"]},{"id":"q-3442","question":"Design an Azure DevOps YAML pipeline to deploy a data-ingestion microservice to AKS in a multi-tenant environment. Each tenant uses a separate namespace and stores per-tenant config in Azure App Configuration with secrets in Azure Key Vault. Implement Istio-based canary traffic shifting, region-aware deployments, per-tenant RBAC, and data-quality validation before prod. Include gating with approvals and a rollback that restores the previous container image and pauses ingestion if validation fails. What would your YAML look like?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Airbnb","IBM","Snowflake"]},{"id":"q-3566","question":"Design an Azure DevOps YAML pipeline for a mono-repo with services A, B, C deployed to two Azure regions. The pipeline must build and test only the services that changed, pull per-environment secrets from Key Vault, enforce per-service environment approvals, and implement region-aware canary deployments via Azure Front Door. How would you implement this?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-devops-engineer"],"companies":["Hugging Face","Lyft","Tesla"]},{"id":"q-3594","question":"Design an Azure DevOps pipeline to automate per-tenant namespace provisioning across two AKS clusters (dev-sandbox and prod) using a GitOps workflow with ArgoCD. Pull per-tenant config from Azure Key Vault, deploy manifests via a Git repo watched by ArgoCD, perform Istio canary traffic shifts with progressive weights, gate with approvals, monitor health and latency, and auto-rollback while enforcing per-tenant cost ceilings in Azure Cost Management?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-devops-engineer"],"companies":["Amazon","Google","OpenAI"]},{"id":"q-3805","question":"Design an Azure DevOps pipeline to build and securely deploy container images to AKS using a GitOps flow. Build multi-arch images, generate a CycloneDX SBOM, scan with Trivy, sign with Cosign, push to ACR, and deploy via ArgoCD to prod with per-environment secrets from Azure Key Vault. Include gated approvals and automatic rollback to the previous image tag if health or latency thresholds are violated?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["NVIDIA","Oracle","Snap"]},{"id":"q-3831","question":"Design an Azure DevOps pipeline that on a tenant onboarding request provisions a new AKS namespace, pulls per-tenant manifests from a GitOps repo, and applies quotas via Kubernetes ResourceQuota and LimitRange. Source region and secrets from Azure Key Vault, gate with approvals and a budget cap in Cost Management, and include rollback to the previous sandbox state if deployment fails?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Amazon","Oracle"]},{"id":"q-3918","question":"Design an Azure DevOps pipeline that provisions a temporary feature-branch data environment in Snowflake, using Terraform to create a dedicated database/schema, deploys a Python ETL job, validates with data quality checks, and uses a gated approval to promote to prod. Include per-environment secrets from Azure Key Vault, time travel safe rollback by cloning prod to a rollback schema and re-pointing jobs, with cleanup?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-devops-engineer"],"companies":["MongoDB","Salesforce","Snowflake"]},{"id":"q-3942","question":"Design an Azure DevOps pipeline to deploy a microservices app to two AKS clusters in EastUS and WestUS using a single GitOps repo watched by ArgoCD. Configure region-specific secrets in Azure Key Vault, and expose the service via Azure Front Door with health-based routing and weighted failover. Include blue/green production promotion, automated smoke/integration tests, automatic rollback by re-syncing ArgoCD to the previous revision and pausing traffic, and a one-click disaster-recovery drill validating failover within 15 minutes?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Airbnb","Apple","LinkedIn"]},{"id":"q-3967","question":"Design an Azure DevOps pipeline for a multi-tenant SaaS app that builds and pushes a container to ACR, provisions per-tenant namespaces on two AKS clusters (dev and prod) via a GitOps workflow (ArgoCD), sources per-tenant feature flags from Azure App Configuration (with keys stored in Azure Key Vault), performs progressive canary rollouts with Istio, gates promotions with approvals, monitors per-tenant health, enforces per-tenant cost ceilings via Azure Cost Management, and auto-rolls back on failure (restoring previous image tag and pausing prod traffic). Include cleanup steps?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["DoorDash","NVIDIA","Snap"]},{"id":"q-4023","question":"Design an Azure DevOps pipeline to deploy a .NET microservice to two AKS clusters (East US and West Europe) using Helm charts in a single repo. Integrate per-environment secrets from Azure Key Vault, enforce Kubernetes policies via Azure Policy, and perform post-deploy drift checks with kubectl diff against the last release. If policy fails or drift is detected, automatically rollback to the previous Helm release and pause traffic; include an automated disaster-recovery drill and cost controls?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Goldman Sachs","Meta","Salesforce"]},{"id":"q-4050","question":"Design an Azure DevOps pipeline to deploy a containerized Node.js API to an AKS cluster using Helm into two namespaces (staging and prod) in one cluster. Use Azure Key Vault for per-env secrets, Istio canary for prod traffic shifts, gate promotion with policy approvals, and auto-rollback on health or diff failures with a post-deploy audit to Log Analytics. What does your pipeline look like?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-devops-engineer"],"companies":["Meta","Snap","Tesla"]},{"id":"q-4131","question":"Design an Azure DevOps pipeline to promote a new ML model version from Azure ML Model Registry to production by deploying a canary endpoint and splitting traffic (10/90). Pull per-env secrets from Azure Key Vault; gate the promotion with policy approvals; perform post-deploy drift validation using Log Analytics; auto-rollback to the previous model version on failure; audit all steps to Log Analytics?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Instacart","Microsoft","Uber"]},{"id":"q-4238","question":"Design an Azure DevOps pipeline to promote a streaming analytics service (Flink on AKS) across two AKS clusters in East US and West Europe. Use Helm charts, Istio canary traffic with a 10/90 split, and pull per-env secrets from Azure Key Vault. Gate with policy approvals; post-deploy validation via synthetic transactions and Prometheus checks; auto-rollback to the previous release if latency or error rate breaches thresholds; quarterly disaster-recovery drills; audit to Log Analytics; enforce cost ceilings via Cost Management alerts. What does your pipeline look like?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Adobe","Plaid"]},{"id":"q-4452","question":"Design an Azure DevOps pipeline to promote a Go-based API deployed to a single AKS cluster with two Helm releases (blue/green). Traffic shifted 15% to green via Istio over 10 minutes, then ramped to 85% if healthy. Secrets sourced per-environment from Azure Key Vault; gate promotion with policy approvals; post-deploy checks include synthetic latency and error-rate verification and drift diff against prod. Auto-rollback to blue on any failure; audit all steps to Log Analytics. What does your pipeline look like?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-devops-engineer"],"companies":["Microsoft","Plaid"]},{"id":"q-4525","question":"Design a beginner Azure DevOps YAML pipeline to deploy a Python Flask API stored in Azure Repos as a Docker image to a single AKS cluster with staging and production promotion. Build and push to ACR; pull per env settings from Azure Key Vault; gate with manual approval before production; run a health check after deploy; auto-rollback to the previous image if the check fails; audit all steps to Log Analytics?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Snowflake","Square"]},{"id":"q-4580","question":"Design an Azure DevOps pipeline for a multi-region SaaS web app that promotes a feature-flag release from dev to prod. The pipeline must (1) fetch per-env secrets from Azure Key Vault, (2) apply infrastructure changes with Terraform targeting two Azure regions, (3) gate promotions with policy approvals, (4) implement blue/green or canary traffic shifts via Azure Front Door for PROD, with 10/90 or similar, (5) perform post-deploy drift validation using Terraform state and Log Analytics, and (6) auto-rollback to the previous Terraform state and pause traffic on failure, while auditing all steps to Log Analytics. Provide the pipeline steps, artifacts, and rollback plan?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Apple","Salesforce"]},{"id":"q-4612","question":"Design an Azure DevOps pipeline to promote a new model version to production by deploying to two AKS clusters with Istio canary traffic (10/90), and enable a per-environment transform via Azure App Configuration flags. Gate with approvals and policy checks, validate drift with kubectl diff, auto-rollback on failure, and log all steps to Log Analytics. What steps and artifacts would you include?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Citadel","Hugging Face","IBM"]},{"id":"q-4701","question":"Design an Azure DevOps pipeline to promote a multi-tenant frontend app from staging to production with per-tenant feature flags. Use Azure Front Door for progressive rollout (10/90 first), per-env secrets from Azure Key Vault, and traffic routing between staging and prod. Gate promotion with policy approvals. Validate post-deploy with Application Insights and Log Analytics drift checks; auto-rollback to previous frontend build if latency or error rate exceed threshold for a 15-minute window. Provide artifacts, rollback plan, and audit?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-devops-engineer"],"companies":["Microsoft","NVIDIA","Two Sigma"]},{"id":"q-4773","question":"Design a tenant-scoped blue/green pipeline for a multi-tenant SaaS API across two regions. Pull per-tenant feature flags from Azure App Configuration and secrets from Key Vault. Implement tenant-level canaries (e.g., 10%) with auto-rollback on failure, gate promotions with policy approvals, and audit events to Log Analytics. Provide the pipeline outline and rollback plan?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-devops-engineer"],"companies":["Microsoft","Slack","Square"]},{"id":"q-1007","question":"A web app hosted in Azure App Service must securely call a private API hosted in an Azure Function behind a VNet. How would you implement private connectivity so traffic never leaves the Azure backbone? Include which resources you’d use, how to create a Private Endpoint for the API, DNS configuration, and how to restrict public access?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["DoorDash","Lyft","Salesforce"]},{"id":"q-1083","question":"An enterprise web app must enforce data residency per region, minimize egress costs, and meet real-time latency targets. Design an Azure-native, region-aware architecture that enforces residency, uses regional storage and a global entry point, and supports auditing and DR. Outline services, data replication, access control, and failover strategy?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Coinbase","Oracle","Scale Ai"]},{"id":"q-1110","question":"Scenario: A web API running in Azure App Service must persist user uploads to an Azure Storage account. Public access to the storage is disabled. How would you implement a Private Endpoint so the App Service talks to Storage over a private network, including enabling VNet integration, creating the Private Endpoint in the storage subnet, DNS configuration for privatelink, and any trade-offs?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Citadel","Coinbase","Goldman Sachs"]},{"id":"q-1212","question":"Scenario: a new web app stores and serves user-uploaded images globally. To keep costs predictable and delivery fast, which Azure services would you pair to store, serve, and secure access to images, and what trade-offs would you consider for storage tiering, CDN option, and access control?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Adobe","Hugging Face"]},{"id":"q-1242","question":"A startup runs a web app that stores user-uploaded images in Azure Blob Storage and serves them globally via CDN. Describe a practical storage design to minimize costs and latency: container layout, default and lifecycle tiers, lifecycle rules to auto-tier/delete, access control (RBAC vs SAS), and how you’d wire in CDN caching. Include concrete steps and example commands?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Cloudflare","IBM","Two Sigma"]},{"id":"q-1276","question":"Design an end-to-end Azure architecture for a globally distributed analytics platform servicing EU customers with strict data residency. Your plan should cover: data at rest with customer-managed keys, cross-region disaster recovery, private networking (Private Link/Endpoints), least-privilege access, encryption key rotation, and continuous policy compliance checks. Explain key services and trade-offs?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Hugging Face","Oracle"]},{"id":"q-1386","question":"You're building a real-time telemetry pipeline in Azure. Ingest devices via IoT Hub, route events to a Function App for normalization, and persist to an ADLS Gen2 data lake. Compare Event Grid vs Event Hubs for this pub-sub pattern, and justify your choice, including at-least-once delivery, backoff strategy, and observability requirements?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Microsoft","Square"]},{"id":"q-1491","question":"In a global fleet telemetry use-case, events from devices arrive across regions at high volume. Propose an end-to-end Azure pipeline that achieves sub-200 ms latency, ingests 200k events/sec per region, and writes to Delta Lake on ADLS Gen2 with exactly-once semantics and cross-region DR. Which components would you choose (Event Hubs, Functions/Durable Functions, Databricks or Synapse, Delta Lake, identity/security), and how would you implement idempotent sinks, retries, and cross-region failover?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Airbnb","Databricks","Discord"]},{"id":"q-1516","question":"A startup hosts a static frontend in Azure Blob Storage and a small API in Azure Functions. They want low costs, automatic scaling, and simple CI/CD using GitHub Actions. Design a beginner-level end-to-end setup: storage for static site, enable static website hosting, configure Functions with a Consumption plan, set up GitHub Actions for deployment, and outline basic security considerations?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["DoorDash","NVIDIA","OpenAI"]},{"id":"q-1580","question":"You have a REST API backend hosted in a private subnet (AKS/VM) that must be exposed to external partners. Requirements: IP allowlisting, DDoS protection, WAF, token-based authentication via Azure AD, and per-client rate limiting. Which Azure services would you assemble, and what specific configurations would you apply to meet these requirements while keeping backend private?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Coinbase","PayPal"]},{"id":"q-1710","question":"For a new Azure project with a static site on Blob Storage and a REST API on Azure Functions (Consumption), outline a beginner-friendly, least-privilege deployment setup: create an Azure AD service principal for GitHub Actions, assign minimal roles to deploy both resources, and use Azure Key Vault to store API keys. Include concrete roles and example CLI commands?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Citadel","Meta","Netflix"]},{"id":"q-1743","question":"Design a beginner-friendly, low-cost Azure webhook receiver that ingests JSON POSTs, runs in a Consumption-plan Function, validates an HMAC signature with a shared secret, stores each event in Azure Table Storage with PartitionKey as source and RowKey as GUID, and emits a basic success/failure metric in Application Insights. Outline steps and provide a minimal code snippet for signature verification in JavaScript?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Google","Microsoft","Snap"]},{"id":"q-1818","question":"A multi-tenant SaaS app must enforce strict per-tenant data isolation while serving a global user base with sub-100ms latencies. Propose an Azure-based storage and caching design (e.g., Cosmos DB with per-tenant partitioning vs relational sharding, caching strategy) and justify data residency, consistency, and failover choices. Include how you'd scale and monitor?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["DoorDash","Google","Hugging Face"]},{"id":"q-1829","question":"Advanced Azure Fundamentals: Design a cross-region real-time telemetry pipeline for a global IoT fleet (300k events/sec, 5 regions). Ingest with Event Hubs, process with Databricks Structured Streaming, and write to Delta Lake on ADLS Gen2 with exactly-once semantics. Implement geo-replication to a DR region, enforce data residency via Azure Policy and Purview, and ensure idempotent sinks with a tested failover process. What components and trade-offs would you choose, and how would you validate DR?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["IBM","MongoDB","NVIDIA"]},{"id":"q-1921","question":"In a multinational SaaS app where data residency is user-controlled, design an end-to-end Azure architecture that auto-scales with demand, minimizes latency across regions, and enforces per-tenant data isolation. Include data stores, messaging, compute, routing, security, DR, and a plan for idempotent processing with exact-once semantics?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Google","Netflix","Slack"]},{"id":"q-1983","question":"Design a beginner-level, end-to-end global web API deployment using two Azure regions. The API serves mobile clients worldwide, requires low latency, automatic regional failover, and basic security. Which Azure services would you use (Front Door, Traffic Manager, App Service, WAF), and outline the minimal wiring: two regional API endpoints, Front Door with a backend pool and health probes, WAF policy, and DNS configuration?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Citadel","MongoDB","Tesla"]},{"id":"q-2192","question":"Design a scalable, multi-tenant analytics pipeline on Azure for a SaaS app with globally distributed customers. Each tenant's data must be isolated, while the system ingests hundreds of thousands of events per second in real time. Propose an end-to-end architecture using Event Hubs (region-per-tenant or per-tenant partitions), Functions/Durable Functions, and a lakehouse on Delta Lake in ADLS Gen2 or Synapse. Explain per-tenant isolation, exactly-once semantics, CMEK with Key Vault, RBAC, cross-region DR, and cost governance; include a concrete rationale and trade-offs?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Bloomberg","Citadel","Discord"]},{"id":"q-2207","question":"In a globally distributed, multi-tenant BI platform, tenant data sovereignty and masked PII are required. Outline an Azure-native end-to-end architecture using Azure Purview for data catalog and classification, region-scoped ADLS Gen2 for raw data, Synapse Analytics with dynamic data masking and Row-Level Security, and Azure Key Vault for per-tenant keys. Explain how you enforce per-tenant data residency, masking, auditing, and cross-region governance?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Instacart","NVIDIA"]},{"id":"q-2232","question":"A beginner-level interview question: You have a web app storing user images in Azure Blob Storage. How would you implement a cost-conscious lifecycle strategy to automatically move infrequently accessed items to a cheaper tier and delete after retention, while preserving recoverability and basic governance?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Apple","Snowflake"]},{"id":"q-2274","question":"You're building a multi-tenant SaaS on Azure with tenants worldwide and strict data residency. You need low-latency reads globally, per-tenant data isolation, and predictable costs. Design a practical data layer using Cosmos DB with multi-region writes, throttling, and governance. How would you handle partitioning, consistency, backups, and DR while meeting residency constraints?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Bloomberg","Google","Microsoft"]},{"id":"q-2278","question":"Design a compliant, scalable governance baseline for a multi-tenant analytics data lake on Azure (ADLS Gen2, Databricks, Synapse). Customers require BYOK with rotation, per-tenant isolation, private endpoints, and auditable RBAC. Outline the architecture, enforcement using Azure Policy, key management with Key Vault CMK, data cataloging with Purview, and a reproducible deployment blueprint?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Databricks","LinkedIn","MongoDB"]},{"id":"q-2399","question":"In a multi-tenant Azure SaaS platform, describe how you would implement ephemeral administrator access to a production resource group using Privileged Identity Management (PIM). Include configuring eligible roles, approval workflows, maximum activation duration, MFA requirements, access reviews, and end-to-end auditing. How would you validate that least privilege is enforced and that there are no standing admin privileges outside the workflow?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Airbnb","LinkedIn"]},{"id":"q-2449","question":"Global e-commerce platform with two primary regions and a DR region. You must ensure low latency, strict data residency, and automated failover with per-tenant data isolation. Design the data layer and DR plan using Azure services. Compare Cosmos DB with multi-region writes vs SQL DB for per-tenant isolation, routing with Front Door, and storage with ADLS Gen2. Include governance controls (Policy/RBAC) and a practical DR test plan with RPO/RTO targets?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Lyft","NVIDIA","PayPal"]},{"id":"q-2551","question":"Design an Azure-native, cost-conscious, multi-region, multi-tenant log-aggregation pipeline for a regulated finance app. Ingest public API logs into Event Hubs, route to per-tenant landing zones in Delta Lake on ADLS Gen2, apply per-tenant schema enforcement, and implement exactly-once semantics with retries, plus cross-region DR. Choose components (Event Hubs, Functions/ Durable Functions, Synapse or Databricks, Delta Lake, Key Vault, Private Link) and outline governance, security, and cost-control measures (RBAC, PIM, CMK, budgets)?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Citadel","Databricks","Twitter"]},{"id":"q-2678","question":"Design a compliant data lake ingestion pipeline for a multi-tenant fintech using Azure Data Lake Gen2. Each tenant must have strict data isolation and immutability, fixed retention with legal holds, and auditable access trails. Ingest from diverse sources with Data Factory, catalog with Purview, process with Synapse or Databricks, expose a curated layer for BI, and implement cross-region DR. Include per-tenant RBAC and row-level security, plus cost considerations?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Bloomberg","IBM","Robinhood"]},{"id":"q-2706","question":"Scenario: You manage a three-subscription Azure environment (prod, staging, dev) for a global SaaS. You must enforce that no resource in prod or staging has a public IP, and automatically remediate exposures within 15 minutes. Describe the governance approach, policy initiative, remediation workflow, exemptions, and testing plan. Include integration with Private Endpoints and VNet peering to preserve connectivity while enforcing privacy?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Amazon","Bloomberg","Google"]},{"id":"q-2792","question":"Design a per-tenant isolation pattern for a SaaS platform hosted in Azure that serves 100+ customers with strict EU data residency. Allocate each tenant a separate AKS namespace with NetworkPolicies, per-tenant data stores in Azure SQL (one DB or per-tenant schema) encrypted with CMK in Key Vault, and private endpoints for data access. Propose deployment using ARM/Bicep templates, governance via Azure Policy, and DR strategy. Compare separate DB vs per-tenant schema in terms of cost, latency, and isolation. Include validation tests?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Cloudflare","Square"]},{"id":"q-2894","question":"You're building a photo-sharing app that stores user uploads in Azure Blob Storage and serves them via a CDN. Outline a beginner-end setup to minimize storage costs while keeping recent images fast: choose storage account type and tiers, enable a lifecycle policy to move old blobs to Cool and Archive, and describe how you would monitor costs?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Cloudflare","Twitter"]},{"id":"q-2968","question":"Design a global, multi-tenant log analytics pipeline on Azure where each tenant's logs are isolated, encrypted with customer-managed keys, and data never leaves its region. Specify ingestion, storage, governance, access control, and cost controls using per-tenant Event Hubs, per-tenant ADLS Gen2 with Delta Lake, Key Vault CMK, Lighthouse RBAC, Purview, Private Endpoints, and region-limited replication. Include rollback plan for schema changes and guardrails for quotas?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Airbnb","Hugging Face","Tesla"]},{"id":"q-3009","question":"Scenario: You have an Azure Function (Consumption plan) that ingests JSON payloads and writes to Cosmos DB. How would you securely store and access the Cosmos DB connection string using Azure Key Vault and a Managed Identity? Include enabling the identity, granting access, retrieving the secret in code, and handling rotation?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["NVIDIA","Stripe"]},{"id":"q-3123","question":"Scenario: A SaaS API platform must serve multiple tenants with strict data isolation. External clients hit a public API endpoint, but tenant data must be isolated at the data layer. Propose an Azure architecture using API Management, Front Door, Private Link, Key Vault, and Managed Identities to enforce per-tenant quotas, auditability, and compliance. Include authentication/authorization strategy, deployment blueprint, and trade-offs between per-tenant VNet isolation vs shared networking. How would you test security and DR?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Google","Instacart","NVIDIA"]},{"id":"q-3218","question":"Design a global, latency‑sensitive bidding API on Azure to meet sub‑50ms p95 in multiple regions while preserving data residency. Propose regional API layers (AKS or App Service) behind Front Door, private endpoints to Cosmos DB and Storage, regional data replication, and a global cache/CDN. Outline telemetry, DR testing, and cost trade‑offs?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Google","MongoDB","Zoom"]},{"id":"q-3262","question":"Global telemetry ingestion with data residency constraints: ingest regional events into a central analytics lake while raw payloads stay in-region; dashboards must access cross-region data with minimal egress and strict cost controls. Design a concrete Azure-based pipeline that provides regional ingestion with exactly-once semantics, cross-region analytics, data cataloging/auditing, and security/compliance controls. Name services, data formats, idempotent sinks, and disaster recovery plan?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Citadel","Snowflake","Twitter"]},{"id":"q-3398","question":"Scenario: A new feature stores user uploads in Azure Blob Storage. To minimize costs and meet retention policy, how would you implement a storage lifecycle policy to move blobs older than 60 days to Archive and delete blobs older than 365 days? Include steps to enable lifecycle management and verify with logs?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Airbnb","Instacart"]},{"id":"q-3493","question":"Design a cross-tenant management pattern for a SaaS platform on Azure using Azure Lighthouse. How would you onboard a new customer, grant least-privilege access to your service across their subscription, enforce per-tenant RBAC, and maintain auditability and compliance across all tenants? Include onboarding steps, boundary definitions, and monitoring approaches?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Adobe","Meta","Snap"]},{"id":"q-3622","question":"Scenario: A multi-tenant telemetry pipeline on Azure ingests events per tenant via Event Grid and processes them in isolated per-tenant Function Apps. You must enforce per-tenant concurrency quotas, prevent budget overruns, and provide observable SLAs. Propose a practical end-to-end architecture and deployment plan using Event Grid, Service Bus (or Storage Queues), per-tenant Function Apps, and Application Insights. Include quota enforcement, scaling, failure modes, and onboarding?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Snap","Square"]},{"id":"q-3787","question":"Scenario: a SaaS platform ingests tenant data into a central Azure Data Lake Gen2. Tenants demand strict isolation, customer-controlled keys, and full auditability. Design an end-to-end ingestion and governance pattern using Azure Data Factory (or Synapse pipelines), ADLS Gen2, Event Grid, Azure Key Vault CMK, and per-tenant RBAC. Include data ingress paths, retry semantics, idempotency, and monitoring?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Amazon","Google"]},{"id":"q-3828","question":"You’re provisioning a beginner Azure Fundamentals scenario: a small web app hosted on Azure App Service (Linux) connected to Azure Database for MySQL. Provide a minimal end-to-end setup a new developer can implement: (1) region/resource group choices, (2) GitHub Actions workflow to deploy on push to main, (3) secure connection strings via App Settings or Key Vault, (4) HTTPS enforcement and a custom domain, (5) a daily cost budget alert to keep spend in check. Keep it Azure-native and practical?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Google","Instacart","Meta"]},{"id":"q-3859","question":"You’re building a small web app with a static frontend in Azure Blob Storage and a serverless API in Azure Functions (Consumption). You must track costs by environment (dev/test/prod) and enforce tag requirements via policy. Describe the end-to-end setup: RGs per env, tagging strategy, per-env budget alerts, and a GitHub Actions workflow to deploy with environment-specific settings, all Azure-native?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Citadel","DoorDash"]},{"id":"q-3954","question":"Design a globally distributed SaaS API on Azure that serves many tenants with strict data isolation. Propose a cost-aware pattern using Azure Functions, Cosmos DB, and Azure AD. Explain tenant scoping, data partitioning, per-tenant RLS, onboarding, least-privilege access, and observability at scale, including failure modes and DR options?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Meta","Snap","Zoom"]},{"id":"q-3968","question":"Design a minimal end-to-end Azure Fundamentals setup for a small web app using App Service for Linux with an Azure SQL Database backend. Ensure connectivity is private (Private Endpoint in a VNet), specify region/resource group strategy, include a GitHub Actions workflow to deploy on push to main, secure the connection string via Key Vault/App Settings, enforce HTTPS with a custom domain, and add a daily cost alert?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Apple","MongoDB"]},{"id":"q-4039","question":"Design a geographically-distributed telemetry ingestion pipeline that keeps data residency per region, auto-scales with high throughput, and minimizes cross-region egress. Specify data ingress, storage, processing, and governance across Azure regions, justify Azure services (Event Hubs, Data Lake Storage, Synapse/Databricks), and show region-anchored processing, region-scoped RBAC, and cost controls?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Microsoft","Snowflake","Uber"]},{"id":"q-4130","question":"Design a scalable, multi-tenant telemetry ingestion path on Azure for a SaaS product. How would you ensure per-tenant data isolation, dynamic throughput, and auditable access while keeping costs predictable? Propose concrete services (IoT Hub vs Event Hubs, Functions, per-tenant storage, CMK) and onboarding, RBAC, and monitoring?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Coinbase","Google","Tesla"]},{"id":"q-4320","question":"Scenario: A small web app runs on Azure App Service (Linux) and calls a 3rd-party API. Design a beginner-end-to-end setup to (a) place App Service into a dedicated VNet and enable outbound access control to a single trusted path, (b) securely manage the API key using Azure Key Vault with a managed identity, (c) enable HTTPS with a custom domain, and (d) set a daily budget alert. Include concrete steps and sample commands?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Apple","Hashicorp","Twitter"]},{"id":"q-4389","question":"Design an Azure data pipeline for a multinational retailer requiring data sovereignty: ingest from Event Hub in Region1, process with Functions (Managed Identity) masking PII, write to ADLS Gen2 Region1, replicate to Region2 for analytics, enforce per-tenant isolation with RBAC and separate storage, catalog data with Purview, encrypt with CMK in Key Vault, enable Private Link, and implement cross-region DR and cost governance. Provide the architecture, security controls, and operations plan?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Adobe","LinkedIn","Lyft"]},{"id":"q-4416","question":"Design a multi-tenant telemetry pipeline in Azure that isolates data, scales for thousands of devices, and stays cost-efficient across two regions. Propose an end-to-end architecture using a central ingest (IoT Hub or Event Hubs), per-tenant RBAC, cross-region DR, and monitoring; describe data flow, security controls, and a minimal CI/CD path for infra-as-code?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Discord","Netflix"]},{"id":"q-4543","question":"You need a real-time GPU-accelerated AI inference service in Azure for multiple tenants, with automatic scaling and strict per-tenant isolation. Propose a concrete architecture using AKS with GPU node pools, namespaces per tenant, RBAC via Azure AD groups, NetworkPolicy, and an API gateway (Azure API Management) with rate limiting and auth. Include region choice, model registry (Azure ML/ACR), CI/CD (GitHub Actions), data egress controls, and monitoring/cost alerts?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Hugging Face","NVIDIA"]},{"id":"q-4665","question":"Design a global IoT telemetry pipeline on Azure that minimizes egress, enforces per-tenant quotas, and meets data residency rules. Specify edge gateway, ingestion (IoT Hub/Event Hub), storage (ADLS Gen2), compute (Functions/Databricks), and governance (Purview). Describe tenant isolation, RBAC with Lighthouse, cost controls, and DR. What steps and trade-offs would you choose?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Cloudflare","NVIDIA","Netflix"]},{"id":"q-4804","question":"You're building a multi-tenant analytics portal in Azure for three regional customers. Each tenant must have isolated data, identity-based access, and data residency. Propose a practical Azure-native architecture using per-tenant Resource Groups and databases, API Management, Azure Functions, and App Service; include onboarding, least-privilege RBAC, auto-scaling, and auditing/compliance. How would you implement?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Databricks","Goldman Sachs","Hashicorp"]},{"id":"q-859","question":"You have a REST API hosted on Azure App Service that processes user uploads and stores them in a separate Azure Blob Storage account. To avoid embedding secrets, how would you securely grant the API read/write access to the blob container using a managed identity? Outline the exact steps and roles you would apply, and mention any network considerations?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Amazon","MongoDB","Two Sigma"]},{"id":"q-865","question":"An IoT platform deployed in Azure collects about 5 million device events per minute from global regions. You must build an ingestion and processing pipeline that guarantees at-least-once delivery with idempotent writes to Cosmos DB, tolerates regional outages, and minimizes cost. Describe the end-to-end architecture, data flow, dedup strategy, and monitoring?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Discord","Scale Ai"]},{"id":"q-932","question":"Scenario: a multi-tenant SaaS app runs on Azure App Service and stores per-tenant files in separate containers in Azure Blob Storage. To avoid hard-coded keys, describe a secure pattern using managed identities and RBAC to grant the app the correct container access while ensuring tenant isolation and minimal permission surface. Outline steps, roles, and any network considerations (e.g., Private Endpoint)?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["DoorDash","Oracle"]},{"id":"q-984","question":"In a globally distributed API using Azure Functions and Cosmos DB, implement per-tenant data isolation with minimal cross-region data transfer. How would you design the architecture, enforce least-privilege access via managed identities and RBAC, and ensure data residency with Private Endpoints and cross-region replication policies?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Bloomberg","MongoDB","Square"]},{"id":"q-1322","question":"Scenario: You operate a multi-tenant Azure landing zone with AKS clusters and storage across three tenants. Implement scalable tenant isolation and policy-driven security without per-VM NSG churn, while enabling selective cross-tenant analytics via Private Link. Propose a concrete design using Azure Firewall Manager, Private Endpoints, managed identities, and Azure Policy, plus a scale-test plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Google","Snap"]},{"id":"q-1358","question":"In a three-tenant Azure deployment, implement Just-In-Time privileged access for security admins across tenants using Lighthouse. Outline how you would provision roles, enforce time-bound activations, integrate approval workflows, and ensure continuous auditing and cross-tenant access reviews. Include a concrete rollout plan and rollback safety checks?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Apple","DoorDash","Oracle"]},{"id":"q-1411","question":"In a three-tenant Azure deployment hosting critical workloads (AKS and serverless), design a cross-tenant threat-hunting workflow that federates Defender for Cloud and Azure Sentinel telemetry into a shared analyst workspace using Lighthouse. Specify data schema, access controls, data residency, rollback steps if ingestion fails, rollout milestones, and success criteria?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Cloudflare","Lyft","Uber"]},{"id":"q-1421","question":"Design an end-to-end security model for a serverless data ingestion pipeline in Azure: per-tenant telemetry from IoT devices arrives at IoT Hub, then lands in a Gen2 Data Lake with per-tenant partitions. Outline tenant isolation, identity (RBAC, PIM), data encryption at rest (CMK), network controls (Private Link), and auditing. Include rollout steps and rollback checks?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Coinbase","LinkedIn","Meta"]},{"id":"q-1496","question":"In a three-tenant Azure deployment hosting microservices in AKS and tenant storage, design a scalable zero-trust inter-service access model that avoids NSG churn on every VM. Use Azure AD workload identity, Istio mTLS, Private Endpoints, and ABAC with policy-driven access. Explain how you would enforce least privilege, audit access, and plan rollout with rollback steps?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Amazon","Snap"]},{"id":"q-1538","question":"Scenario: in a three-tenant Azure deployment with shared automation pipelines and cross-tenant data access, design a scalable, auditable model to grant and revoke time-bound automation access without leaking credentials. Leverage Lighthouse as a central RBAC anchor, Azure AD app roles, PIM for Just-In-Time elevation, ABAC with claims, and CI/CD policy gates. Include rollout, rollback, and telemetry strategy?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Microsoft","Netflix"]},{"id":"q-1582","question":"Scenario: A single Azure subscription hosts an App Service and an Azure SQL Database. Harden security with Defender for Cloud, Private Endpoints for Storage, RBAC via two Azure AD groups (Developers and Admins), and basic access reviews and alerts. Provide concrete steps, a minimal RBAC mapping, and a rollout plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["IBM","Instacart"]},{"id":"q-1616","question":"In a tri-tenant Azure deployment (tenants A, B, C) hosting microservices in AKS and serverless functions, design a cross-tenant secret management strategy using Azure Key Vault. Include per-tenant vaults plus a central SharedKV, implement 90‑day secret rotations with automatic versioning, enforce cross-tenant access via Azure AD B2B and access reviews, and specify auditing, rollout, and rollback steps?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Google","Microsoft","Oracle"]},{"id":"q-1647","question":"In a two-tenant Azure deployment, Tenant Alpha hosts a data lake (ADLS Gen2) and Synapse pipelines; Tenant Beta hosts identity and apps. Design a concrete, auditable cross-tenant threat containment strategy to prevent data exfiltration and lateral movement during peak load. Include per-tenant Private Endpoints, Conditional Access, cross-tenant RBAC, Defender for Cloud alerts, Key Vault key rotation (90 days), and an automated rollback plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["IBM","Oracle","Twitter"]},{"id":"q-1656","question":"In a single Azure subscription with dev/test/prod resource groups, a CI/CD pipeline currently uses secrets in code. Propose a beginner-friendly, concrete strategy to manage secrets and identities using Azure Key Vault, managed identities, and least-privilege RBAC. Include steps for retrieval at deploy time, rotation readiness, and rollback checks?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Adobe","LinkedIn","Meta"]},{"id":"q-1689","question":"In a beginner Azure Security Engineer scenario, you manage a single subscription hosting an App Service and a SQL Database. Outline a concrete, auditable process to grant a developer time-limited access to both resources using Azure RBAC and Privileged Identity Management (PIM). Include onboarding, role provisioning, activation policy, approval workflow, logging, and a rollback plan if access is no longer needed?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Apple","Cloudflare","DoorDash"]},{"id":"q-1726","question":"In a three-tenant Azure deployment hosting microservices in AKS across tenants A, B, and C, design a scalable, zero-trust API security model where a central API gateway (in Tenant A) issues short-lived access tokens for per-tenant services, using OAuth2/OIDC with Azure AD, cross-tenant service principals, and mTLS between gateway and services. Include token lifecycle, per-tenant scopes, automatic revocation, cross-tenant auditing with Azure Monitor/Sentinel, and a rollback plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Databricks","Salesforce","Uber"]},{"id":"q-1773","question":"In a two-tenant Azure deployment hosting analytics workloads across TenantA and TenantB (ADLS Gen2 and Synapse), design a zero-trust data access model with per-tenant data boundaries and a central broker in TenantA. Explain how you would implement RBAC and CMK, cross-tenant Data Share approvals, managed identities, Private Endpoints, and centralized monitoring with Sentinel/ Defender, plus automated access reviews and anomaly alerts. Include rollout and rollback steps?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Airbnb","Citadel"]},{"id":"q-1857","question":"In a three-tenant Azure landing zone, implement centralized drift detection for security baselines across tenants using Azure Policy, Azure Monitor, and Azure Arc-enabled resources. Propose per-tenant policy scopes, remediation tasks, cross-tenant alerting, and a rollback plan including safe rollback for policy changes and resource remediation. Include a concrete rollout plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Microsoft","Tesla"]},{"id":"q-1931","question":"In a tri-tenant Azure landing zone hosting workloads on AKS and App Services, design a cross-tenant incident response workflow leveraging Azure Sentinel, Azure Lighthouse, and Defender for Cloud. Include roles, cross-tenant playbooks, containment steps, and rollback tests; specify automated evidence collection, cross-tenant alert correlation, and post-incident reporting. Outline rollout plan and rollback safety checks?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Netflix","Snap","Twitter"]},{"id":"q-2074","question":"In a two-subscription Azure environment (AppInfra and DataInfra) with CI/CD pipelines deploying to both, how would you implement service principal credential hygiene to prevent credential leakage? Outline steps to (1) create a dedicated SPN with least privilege, (2) rotate credentials automatically, (3) enforce ephemeral credentials for pipelines via Azure DevOps service connections with Managed Identity, (4) implement auditing via Defender for Cloud and Azure AD sign-in logs, and (5) a rollback plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Databricks","NVIDIA","Tesla"]},{"id":"q-2111","question":"In a tri-tenant Azure deployment (Tenants A, B, C) hosting AKS and serverless services, design a cross-tenant software supply chain security workflow that enforces artifact signing, image provenance, and runtime integrity. Specify the integration points across Azure DevOps, Azure Container Registry with content trust, Azure Policy, and Lighthouse for cross-tenant approvals; include per-tenant gates, rollback safety checks, and a concrete rollout plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Instacart","Lyft","Tesla"]},{"id":"q-2298","question":"In a tri-tenant Azure deployment hosting CI/CD pipelines and production workloads, design a cross-tenant incident isolation and rollback plan that automatically quarantines affected resources across tenants using Lighthouse, Defender for Cloud, and Azure Arc-enabled resources. Include per-tenant triggers, cross-tenant approval gates, rollback safety checks, and a concrete rollout plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Amazon","Bloomberg","Meta"]},{"id":"q-2300","question":"In a four-tenant Azure deployment (Tenants A–D) hosting AKS microservices and serverless backends, design a cross-tenant breach containment workflow triggered by Defender for Cloud alerts. Include per-tenant isolation steps (networks, identities), revocation of tokens, rotation of keys in Key Vault, traffic redirection, Lighthouse approvals, and a concrete rollout plan with rollback safety checks?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["IBM","Plaid","Tesla"]},{"id":"q-2334","question":"In a single Azure tenant hosting a web app and its containerized worker across two regions, implement a lightweight secure CI/CD gate that enforces artifact signing and image provenance before deployments to AKS, using Azure DevOps, ACR content trust, and a policy gate. Provide concrete steps, rollback, and testing plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Google","Snowflake","Uber"]},{"id":"q-2586","question":"In a multi-region Azure deployment hosting a global Data Lake (ADLS Gen2) and Synapse workspaces across three regions, design an automated data access governance workflow that enforces per-user, per-dataset, and per-column access with dynamic data masking and query-time enforcement. Use Azure Purview for classification/lineage, Synapse RBAC, Azure AD Conditional Access, and Privileged Identity Management; include rollout, rollback, and testing plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Apple","Oracle","Snowflake"]},{"id":"q-2648","question":"Across a single Azure tenant with three environments (dev, test, prod) spanning multiple subscriptions, design a CMK-backed storage encryption baseline. Use Azure Policy to require customer-managed keys in Azure Key Vault for all storage accounts, enforce key rotation, and block cleartext backups. Include per-subscription scopes, remediation tasks, alerting, and a rollback plan. Provide concrete rollout steps and testing criteria?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Apple","Oracle"]},{"id":"q-2780","question":"In a tri-tenant Azure deployment using Lighthouse (Tenants A, B, C), design an automated cross-tenant incident response workflow that ingests Defender for Cloud and Sentinel alerts, correlates by resource ID, and enacts containment (temporary NSG rules, cross-tenant access revocation via PIM, and service isolation) within 10 minutes. Describe data flow, RBAC, cross-tenant approvals, rollback safeguards, and rollout steps?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Discord","Square","Two Sigma"]},{"id":"q-2833","question":"In a two-tenant Azure deployment feeding a shared data lake via ADLS Gen2 and Event Hubs, design a private API access and provenance workflow that enforces per-tenant tokens, automatic rotation, and client attestation for data ingestion. Integrate with Azure API Management, Key Vault, ACR content trust, Defender for Cloud, and Lighthouse for cross-tenant approvals; include rollout and rollback steps?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Bloomberg","Citadel","Salesforce"]},{"id":"q-2872","question":"Design a per-tenant encryption key strategy for a three-tenant Azure deployment hosting AKS and Cosmos DB: Each tenant uses its own customer-managed key (CMEK) in its dedicated Key Vault to encrypt Cosmos DB data at rest and to sign inter-service tokens. Explain provisioning, access control per tenant, rotation every 30 days, rollback, and how to verify rollout?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Adobe","Anthropic","Apple"]},{"id":"q-2923","question":"In a tri-tenant Azure landing zone hosting security-critical workloads, design a beginner-friendly, end-to-end secrets management strategy for CI/CD across tenants. Each tenant should own a Key Vault, while deployment agents pull credentials via managed identities; enforce scanning to prevent secrets in code, and use Lighthouse for cross-tenant approvals. Include rollout steps and rollback checks?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Amazon","Goldman Sachs","NVIDIA"]},{"id":"q-2971","question":"In a single-tenant Azure environment hosting a web app and a background worker, implement a beginner security baseline to prevent data exfiltration from storage and secrets. Enforce storage firewall with private endpoints, use Key Vault with managed identity for secrets, require TLS 1.2 on App Service, and add a Monitor alert for any public access attempts. Provide concrete steps, rollback, and a two-week rollout plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Discord","DoorDash","Google"]},{"id":"q-3094","question":"In a single Azure tenant hosting a SaaS app across several resource groups, design a lightweight guardrail to prevent unsafe changes and enable rollback. Implement Azure Resource Locks for critical assets, a minimal Azure Policy enforcing tag requirements and allowed resource types, and a change-control workflow with PR gates for policy changes. Provide a concrete rollout plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Apple","Hugging Face","Robinhood"]},{"id":"q-3135","question":"In a multi-tenant Azure SaaS used by Scale AI and Stripe, hosting AKS and Key Vault across tenants, design a just-in-time elevated access workflow using Azure AD PIM, Privileged Access Groups, and per-resource scoping. Define time-bound activations, approval gates, MFA, and automatic credential rotation. Include rollback, audit, and Defender for Cloud alerting; provide concrete rollout steps?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Scale Ai","Stripe"]},{"id":"q-3235","question":"In a two-tenant Azure deployment, Tenant Alpha hosts an API gateway and a shared data plane consumed by Tenant Beta's SaaS app. Design a defense-in-depth workflow to enforce tenant isolation, per-tenant data access, and cross-tenant auditing. Include identity (Azure AD B2B and managed identities), encryption (CMK), private endpoints, Row-Level Security, and a rollback/test plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Coinbase","MongoDB","OpenAI"]},{"id":"q-3315","question":"In a single Azure tenant running AKS, implement a beginner-friendly secret management baseline: deploy Azure Key Vault and the AKV2K8s injector, configure IAM so the cluster's service account can read KV secrets, migrate one app to consume a KV secret as an environment variable, and add an Azure Policy to prevent creating Kubernetes Secrets. Include a rollback plan and validation steps?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Airbnb","Discord","Microsoft"]},{"id":"q-3390","question":"In a three-tenant Azure data platform (Tenants A, B, C) sharing a centralized Data Lake Gen2 for telemetry and customer data, design a cross-tenant data governance model that enforces per-tenant data residency, data classification, and secure data sharing. Detail how you would use Azure Purview, Lighthouse, AAD managed identities, per-tenant Key Vault CMKs, ABAC, Private Endpoints, and cross-tenant approvals; include a concrete rollout plan and rollback safety checks?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Citadel","DoorDash","Goldman Sachs"]},{"id":"q-3500","question":"In a two-subscription, multi-region app with a centralized API gateway and per-tenant data stores, design a zero-trust API security model: enforce mTLS, short-lived tokens scoped per tenant, and private endpoints. Outline components, data flows, cert rotation, and monitoring with Defender for Cloud; include a concrete rollback and testing plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["DoorDash","Scale Ai","Snap"]},{"id":"q-3508","question":"In a single Azure tenant hosting a PayPal-like payment API (AKS in two regions and Azure SQL) with a separate analytics workspace, design an automated breach containment workflow. It should 1) quarantine impacted resources with Azure Locks and policy gates, 2) rotate keys/secrets in Key Vault and refresh SPN credentials, 3) validate rollback with canary tests, and 4) provide a concrete end-to-end rollback plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["PayPal","Stripe"]},{"id":"q-3645","question":"Design a zero-trust admin access model for a two-tenant Azure deployment where Tenant A hosts identity and management, and Tenant B runs sensitive workloads (AKS, SQL). Describe how Just-In-Time elevation, cross-tenant approvals via Lighthouse, per-action gating, device posture checks, and audit trails are wired across both tenants. Include rollout milestones and rollback safety?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Cloudflare","Databricks","Tesla"]},{"id":"q-3697","question":"In a three-tenant Azure deployment hosting an API gateway connected to per-tenant data stores across regions, design an automated incident response workflow that detects anomalous secrets access and cross-tenant data exfiltration, contains the compromised tenant, and preserves forensics. Specify how you would integrate Azure Sentinel, Defender for Cloud, Azure Arc-enabled servers, and Lighthouse; include cross-tenant approvals, rollback safety checks, and a concrete rollout plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Coinbase","Instacart","Uber"]},{"id":"q-3722","question":"In a fintech SaaS deployed across two Azure tenants with AKS and App Service, design an automated incident-response workflow that detects a compromised container (Defender for Containers), quarantines the workload, rotates secrets in Key Vault, revokes tokens, and rolls back to a known-good image with provenance. Include per-tenant gates, rollback safety checks, and concrete rollout steps?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Coinbase","Google"]},{"id":"q-3749","question":"In a multi-region Azure environment hosting a SaaS app with two AKS clusters and Azure Sentinel, design an automated incident response playbook that quarantines a suspected host, rotates credentials, and validates rollback. Specify signals, actions, and integration points across Azure Monitor, Azure Sentinel, Key Vault, AKS, and Private Endpoints; include rollback/testing plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Hugging Face","Instacart","Microsoft"]},{"id":"q-3776","question":"In a single Azure tenant hosting a web API in AKS across two resource groups, design a beginner guardrail to prevent unsafe changes. Implement a minimal Azure Policy enforcing TLS on public endpoints, require mandatory resource tagging, enable Defender for Cloud baseline, and configure a simple Azure Monitor alert for failed sign-ins and anomalous API calls; include a concrete rollback and rollout plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Adobe","Snap","Tesla"]},{"id":"q-3841","question":"In a single Azure tenant hosting a web app on App Service and a blob storage account, design a beginner guardrail to prevent secret leakage: require that all secrets are stored in Azure Key Vault, enforce access via managed identities, and add a basic log alert for failed access attempts. Provide concrete steps and a minimal rollout plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Databricks","Meta","Plaid"]},{"id":"q-3898","question":"In a single Azure tenant hosting an App Service web app and an Azure SQL Database, design a beginner guardrail to prevent misconfigurations: (1) ensure no public network access on the SQL server and that App Service uses Private Endpoint or regional VNet integration; (2) require diagnostic logs to be sent to a Log Analytics workspace; (3) implement a minimal Azure Policy gate enforcing tag requirements and allowed resource types for new resources. Provide concrete steps, rollback plan, and rollout timeline?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["LinkedIn","NVIDIA","Tesla"]},{"id":"q-4226","question":"Design a cross-tenant data-plane security workflow to prevent data exfiltration from a centralized data lake across Tenants A, B, and C. Include per-tenant RBAC on the data lake, dynamic data masking in Synapse, Purview DLP classifications with policy enforcement, and cross-tenant approvals via Lighthouse. Include per-tenant gates, egress controls at the API gateway, and a concrete rollout plan with rollback safety checks?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Apple","Meta","Tesla"]},{"id":"q-4252","question":"In a single Azure tenant hosting a multi-tenant SaaS API surface exposed via API Management with private endpoints in two regions, design a security hardening plan that enforces per-tenant data isolation, controls bot/abuse traffic, and enables safe rollback of policy changes. Explain how you would implement: (1) tenant-scoped API policies and RBAC, (2) WAF bot protection and rate limiting, (3) Private Endpoint + Private DNS with per-tenant DNS zones, (4) monitoring/incident response with Sentinel and Defender for Cloud, and (5) automated GitOps-based rollback with staged rollout and rollback checks. Include a concrete rollout plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Amazon","Lyft","Slack"]},{"id":"q-4280","question":"In a three-tenant Azure deployment (Identity tenant, Data plane tenant, Audit tenant) facing a suspected cross-tenant security incident, design a coordinated IR workflow that detects, contains, and recovers within 60 minutes. Use Defender for Cloud, Azure Sentinel, Lighthouse delegation, and per-tenant runbooks. Include cross-tenant evidence collection, rollback-safe containment, and post-incident reporting. Provide a concrete rollout plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["DoorDash","IBM","Lyft"]},{"id":"q-4318","question":"In a three-tenant Azure deployment offering a shared data service, design tenant-aware data isolation for Cosmos DB and blob storage. Use per-tenant containers/shops, envelope encryption with CMK in Key Vault, and runtime attestation for the AKS compute. Include Lighthouse cross-tenant approvals, per-tenant access gates, and a concrete rollout with rollback safety checks?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Citadel","Lyft","Stripe"]},{"id":"q-4370","question":"Design a scalable, tenant-isolated key management and service-to-service auth layer for a multi-tenant Azure SaaS platform where each tenant runs its own AD, region, and Key Vault. Describe per-tenant data encryption keys in HSM-backed Key Vaults, cross-tenant token exchange using OAuth 2.0 On-Behalf-Of and Azure Lighthouse, key rotation/revocation, and auditable telemetry. Include rollout steps and rollback safety checks?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Bloomberg","Google","Salesforce"]},{"id":"q-4393","question":"In a single Azure tenant hosting a SaaS app across two regions, implement a beginner guardrail to prevent public endpoints by default and force private access. Create Azure Policy definitions to require Private Endpoints on App Service, Storage accounts, and Cosmos DB; specify per-resource scopes; outline remediation tasks and a rollback plan with a rollout strategy?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Coinbase","NVIDIA","Salesforce"]},{"id":"q-4533","question":"In a multi-tenant SaaS on Azure with a data-export API feeding external BI tools, design a per-tenant, secure data-export guardrail. Outline how you enforce tenant consent, secure the export path via API Management, encrypt exports with a per-tenant CMK in Key Vault, store in a Private Endpoint storage, issue short-lived, tenant-scoped SAS tokens that rotate every 15 minutes, and how you roll this out with canaries and rollback?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Adobe","Coinbase","Scale Ai"]},{"id":"q-4657","question":"In a single Azure tenant hosting an ADLS Gen2 data lake and a BI analytics workspace, design a beginner guardrail to prevent accidental data exposure. Implement an Azure Policy to block public blob access, enforce private endpoints, require HTTPS-only, and restrict firewall rules to VNets. Describe drift detection, lightweight remediation, and rollback steps with rollout testing?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Adobe","Cloudflare","Twitter"]},{"id":"q-4671","question":"Design a zero-trust data access model for a three-tenant Azure landing zone hosting a data lake (ADLS Gen2) and a real-time stream pipeline. Enforce per-tenant ABAC with Azure RBAC, use customer-managed keys in Key Vault for at-rest encryption, and ensure strict data vault separation. Include cross-tenant auditing via Lighthouse, data discovery with Purview, key rotation, rollback safety, and a concrete rollout plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["DoorDash","Oracle","Snowflake"]},{"id":"q-4756","question":"In an Azure multi-tenant SaaS environment leveraging Arc-enabled Kubernetes and Windows servers across three tenants, design a unified runtime security model using Defender for Cloud, Defender for Containers, Lighthouse, and Sentinel playbooks. Outline per-tenant RBAC, runtime attestation, automated containment (pod kill/host isolation), cross-tenant audits, data residency safeguards, and a rollback approach?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Google","MongoDB","PayPal"]},{"id":"q-898","question":"In a multi-cluster AKS deployment with rapid scale-out, design a workload-based segmentation pattern that avoids per-VM NSGs. Use Kubernetes NetworkPolicy/Calico, central allowlists, and Azure Policy for drift remediation. Include how to enforce per-service identity, automate policy checks, and detect violations with Defender for Cloud?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Bloomberg","Google","NVIDIA"]},{"id":"q-925","question":"In a multi-subscription Azure deployment with ephemeral, autoscaled workloads across AKS and VMs, how would you achieve scalable, workload-oriented segmentation without updating NSG rules on every VM? Propose a concrete design using Azure Firewall Manager, managed identities, Private Link, and policy-driven groupings, plus a plan to test at scale?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Discord","Microsoft"]},{"id":"q-968","question":"In a global Azure deployment with microservices spread across AKS clusters and VM Scale Sets, design a workload-based segmentation that avoids touching NSG rules on every VM. Outline a concrete end-to-end approach using Azure Firewall Manager, policy-based groups, Private Endpoints, workload tags, and Managed Identities. Include deployment steps, scale testing, and violation monitoring?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["MongoDB","Snap","Uber"]},{"id":"q-1046","question":"You're building a regulated, multi-tenant analytics platform on Azure that ingests IoT and application logs from customers across three continents. Customers demand regional data residency while analytics must be global for cross-tenant benchmarks. Propose a practical, cost-conscious architecture that enforces per-tenant data isolation (at rest and in transit), regional ingestion, geo-redundant storage, cross-region analytics, and auditable access control using Azure native services. Include data plane vs control plane separation, and show how you'd satisfy RPO/RTO targets and regulatory requirements?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Databricks","Google"]},{"id":"q-1181","question":"You operate a fintech SaaS platform serving tenants across US, EU, and APAC. Each tenant's data must reside regionally at rest, yet global analytics require anonymized cross-tenant insights. Describe an Azure-native architecture that (1) enforces per-tenant data isolation in storage and processing, (2) supports real-time ingestion of fraud/transaction events, (3) enables cross-region analytics without tenant leakage, (4) meets DR targets with RPO <15 minutes and RTO <5 minutes, and (5) provides end-to-end auditing and governance. Include components, data flows, trade-offs, and a concrete failover test plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Plaid","Snap"]},{"id":"q-1305","question":"You manage a global healthcare analytics platform on Azure. Regulations require that **PHI** stays in-country while **non-PHI** can aggregate regionally. Propose an end-to-end data pipeline using **Azure Data Lake Storage Gen2**, **Data Factory**/Synapse, and **Purview** to enforce residency, enable regional analytics, and provide auditable data lineage and masking. Include encryption, governance, and failover strategies across regions?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Google","Instacart","OpenAI"]},{"id":"q-1365","question":"Design an Azure-based, EU-resident real-time trading analytics pipeline for a regulated fintech platform that must achieve sub-100 ms end-to-end latency, robust multi-region DR, and strict auditability. Outline the services, data flows, data residency, encryption (BYOK), governance, and cost controls you would implement?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Coinbase","Netflix","Robinhood"]},{"id":"q-1391","question":"You are tasked with building a EU-resident, real-time analytics platform with strict data residency and disaster recovery. Ingest events via Azure Event Hubs in the EU, land in a Data Lake Gen2 in the EU, process with Azure Synapse and Azure Databricks, and expose global analytics through external tables or Synapse Link. Include governance with Purview, BYOK via Key Vault, and private connectivity via Private Link/ExpressRoute. Design DR: RPO <5m, RTO <15m, and cost controls; outline testing plan (blue/green, chaos) and tradeoffs?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Hugging Face","Slack","Square"]},{"id":"q-1432","question":"For a new EU-resident SaaS app serving multiple tenants, you choose data isolation in Azure SQL Database. Compare using a single database with Row-Level Security (RLS) vs separate contained databases per tenant, focusing on cost, governance, backup/restore, and scale. Propose a concrete decision and outline the basic migration steps, including how you’d implement BYOK with Key Vault for per-tenant encryption and Azure AD authentication?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Google","Instacart"]},{"id":"q-1477","question":"For a EU-resident, multi-tenant SaaS app deployed in Azure, data residency requires tenant data to remain in EU while global analytics runs from a separate region. Design an end-to-end architecture that enforces per-tenant residency, enables cross-geo analytics, uses Arc-enabled data services, Cosmos DB, Synapse, and Purview, implements BYOK and private connectivity, and achieves DR with RPO<5m and RTO<15m. Include data flows, governance model, and testing plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Salesforce","Tesla"]},{"id":"q-1541","question":"Design a two-region, Azure-native DR for a beginner-friendly SaaS API with EU residency; propose a minimal architecture using Azure Front Door, App Service, and Azure SQL with geo-replication to achieve sub-minute RTO and RPO under 15 minutes; outline the failover process and a practical testing plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Hugging Face","Uber"]},{"id":"q-1642","question":"Design a cost-conscious EU-resident SaaS API hosted in Azure: propose a minimal architecture using Azure App Service, Azure SQL Database, and a public endpoint with autoscale and a regional failover to a secondary Azure region; detail traffic routing, RTO/RPO targets, and a practical testing plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Apple","Google","Netflix"]},{"id":"q-1652","question":"Design a beginner-friendly, Azure-native, event-driven ingestion path for a multi-tenant SaaS that streams user actions to analytics. Use a single Azure Event Hub, a Function with Event Hub trigger, and ADLS Gen2 as the sink. Include idempotent processing, dedup, backoff retries, and a simple tenant-scoped data schema. Outline validation steps to prove end-to-end latency under 2 minutes and zero data loss?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["LinkedIn","Oracle","Salesforce"]},{"id":"q-1696","question":"You’re designing a EU-resident, multi-tenant analytics platform that ingests in near real-time and serves tenant-isolated dashboards. Propose a cost-conscious Azure-based lakehouse architecture using ADLS Gen2, Event Hubs, Data Factory, and Synapse, detailing data isolation, per-tenant encryption with BYOK in Key Vault, and Azure AD-based access control; include a pragmatic migration path from a single-tenant baseline?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Adobe","Robinhood","Salesforce"]},{"id":"q-1730","question":"As the Azure Solutions Architect for a Zoom-scale platform, design an Azure-native, EU-resident data pipeline that ingests telemetry from MongoDB Atlas (Change Streams), streams it with minimal data loss, processes it in near real-time, and serves dashboards without EU data leaving the region. Outline data flow, services, DR, encryption (BYOK), and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["MongoDB","Zoom"]},{"id":"q-1781","question":"Design a beginner-friendly EU-resident telemetry pipeline for a gaming platform: ingest per-session data via Azure Event Hubs, process with Functions, store per-tenant data in Cosmos DB with Row-Level Security, and surface per-tenant dashboards in Power BI. Ensure data stays in the EU, implement BYOK, and outline DR and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Google","NVIDIA"]},{"id":"q-1797","question":"Design an Azure-native, EU-resident, cross-tenant data platform that ingests telemetry from an on-prem gateway fleet into Azure, processes it in near real-time while guaranteeing data sovereignty (EU only), uses BYOK with Key Vault, and supports tenant-level dashboards with RBAC. Outline data model, services, DR, and cost controls?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Adobe","Apple"]},{"id":"q-1841","question":"Design a beginner-friendly Azure-native telemetry pipeline for a fleet of IoT devices used by a mobile app, with EU residency constraints. Ingest device telemetry via Azure IoT Hub, preprocess at the edge with IoT Edge (sampling and filtering), route to Azure Functions for enrichment, and store per-tenant aggregates in Azure SQL with Row-Level Security. Expose dashboards in Power BI; include BYOK with Key Vault, DR planning, and cost levers; keep data in the EU region?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Airbnb","MongoDB","Robinhood"]},{"id":"q-1860","question":"Design an EU-resident Azure-native architecture for a real-time streaming recommendations engine serving EU users; telemetry is generated globally and must be processed entirely within the EU with no data leaving the region. Propose ingestion, real-time processing (sub-second latency), state storage, and serving path, tenant isolation, BYOK with Key Vault, private endpoints, and a regional DR plan with automated failover. Include concrete services and trade-offs?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["MongoDB","Netflix","PayPal"]},{"id":"q-1919","question":"EU-resident, regulated fintech SaaS: design a fully Azure-native, event-driven analytics platform that ingests on-prem transaction streams from a gateway into EU-region data plane; ensure tenants are isolated, data never leaves the EU, with BYOK in Key Vault, and automatic regional failover to a secondary EU region. Choose services (Event Hubs, Functions/Stream Analytics, Cosmos DB multi-tenant with per-tenant containers or databases, ADLS Gen2, Synapse), network controls (Private Link, VNets), security, cost levers, and migration steps. Provide data flow, DR plan, testing plan, and governance considerations?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Goldman Sachs","Hugging Face","Meta"]},{"id":"q-1943","question":"You're building an EU-resident telemetry observability layer for a real-time game platform. Telemetry ingested through EU-based Event Hubs is processed by Functions and stored per-tenant in Cosmos DB; dashboards display in Power BI. Propose an end-to-end observability design that enables per-tenant debugging without exporting data outside the EU. Include logs, tracing, retention, and testing?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["NVIDIA","Plaid"]},{"id":"q-1947","question":"Scenario: design a beginner-friendly **EU-resident** analytics pipeline for an Instacart-like app focusing on **support tickets** and **in-app events**. Ingest via **Azure Event Hubs** in the EU, enrich with **Functions**, store per-tenant metrics in **Cosmos DB** with **RLS**, and visualize in **Power BI**. Keep data EU-only, enforce **BYOK** via **Key Vault**, propose a 30-day retention and a purge workflow, plus a minimal **DR** plan and cost levers. Provide end-to-end data path and governance touches?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Instacart","Lyft"]},{"id":"q-2035","question":"You're building a EU-resident, multi-tenant fintech SaaS with strict data isolation. Propose a 2-region EU Azure-native data fabric that keeps each tenant's data isolated, supports real-time analytics and ML scoring, and enforces BYOK. Include data flow, services, governance, DR, and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Google","Hashicorp","Robinhood"]},{"id":"q-2180","question":"Design an Azure-native, EU-resident, edge-enabled telemetry platform for a real-time messaging app used by enterprise customers (Snap, Discord, Goldman Sachs). The system must ingest client telemetry at the edge (IoT Edge or SDKs), perform regional near-real-time aggregation, keep raw data within EU borders, implement BYOK with Key Vault, and serve dashboards with sub-second latency. Compare IoT Hub vs Event Hubs, edge compute placement, DR across two EU regions, and cost levers. Include a concise migration/testing plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Discord","Goldman Sachs","Snap"]},{"id":"q-2326","question":"You’re the Azure Solutions Architect for a beginner-friendly EU-resident ride-hailing platform onboarding cities as tenants. Propose a minimal, Azure-native onboarding pipeline that creates isolated per-city data stores, uses per-tenant encryption keys in Key Vault, and provides per-city dashboards via Power BI with Row-Level Security. Include data flow, services, cost levers, and a lightweight disaster-recovery plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Amazon","Lyft"]},{"id":"q-2344","question":"As the Azure Solutions Architect for a multi-tenant fintech platform with EU residency requirements, design a cross-tenant data-sharing and analytics pattern that preserves data sovereignty, minimizes data movement, and supports real‑time dashboards. Include governance with Azure Purview, data sharing mechanisms, encryption (BYOK via Key Vault), per-tenant isolation (RBAC/AD), streaming ingestion (Event Hubs), and analytics (Synapse or Databricks). Outline data flow, DR, and testing plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Netflix","PayPal","Snowflake"]},{"id":"q-2376","question":"Design an Azure-native, multi-region architecture for a real-time ad-bidding platform with per-tenant isolation for advertisers, ensuring data residency in chosen regions. Use Event Hubs, ADLS Gen2, Synapse or Databricks, and Confidential Computing. Include data flow, cross-region replication, BYOK, access controls, cost levers, and testing strategy?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Meta","Twitter"]},{"id":"q-2469","question":"As Azure Solutions Architect for a multi-tenant SaaS platform serving EU residents, design a secure, real-time analytics stack that ingests events from diverse SaaS apps via Event Hubs, stores per-tenant data in Delta Lake on Synapse, enforces per-tenant access with Azure AD RBAC, uses BYOK with Key Vault for at-rest keys, and applies cost controls and EU-region DR. What is the end-to-end data flow and governance plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Cloudflare","LinkedIn","Salesforce"]},{"id":"q-2550","question":"As Azure Solutions Architect for a high-velocity ride-hailing platform with EU residency, design a real-time fraud detection data fabric that ingests telemetry from mobile apps via Event Hubs, enriches with user risk signals from Cosmos DB, processes in near-real-time with Synapse Spark, stores a lakehouse in ADLS Gen2, and serves risk signals to dashboards and a monitoring API with per-tenant RBAC, BYOK via Key Vault, and active-active DR across regions. Compare architecture options: Synapse Spark vs Databricks. Outline data flow, governance, testing plan, and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Snowflake","Uber"]},{"id":"q-2568","question":"As Azure Solutions Architect for a EU-resident, multi-tenant SaaS, design a beginner-friendly data governance and catalog workflow that automatically discovers, classifies, and catalogs per-tenant data assets using Azure Purview, with tenant-scoped RBAC and BYOK, keeping data in EU, and delivering compliant lineage from Event Hubs through Delta Lake in Synapse to dashboards. Outline end-to-end data flows, services, and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Databricks","Meta"]},{"id":"q-2593","question":"As Azure Solutions Architect for a global streaming platform with stringent data sovereignty, design an end-to-end cross-cloud analytics stack that ingests telemetry from on-prem and multi-cloud edge devices, streams into Azure Event Hubs, writes per-tenant data into Delta Lake on Synapse, and serves dashboards with real-time insights via Databricks model scoring. Include BYOK, per-tenant RBAC, data contracts, governance with Purview, cross-region DR, and cost controls. Outline data flows, services, and failure modes?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Databricks","Netflix"]},{"id":"q-2719","question":"Design an Azure-native, per-tenant ML governance and real-time inference stack for a global, multi-tenant platform that keeps data regionally resident, isolates tenants, and supports model versioning, drift detection, and cost controls. Include data ingress, feature store, model registry, per-tenant endpoints, monitoring, DR, BYOK, RBAC, and governance tooling; justify services and trade-offs?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Databricks","Hugging Face"]},{"id":"q-2795","question":"As Azure Solutions Architect for a Robinhood-like fintech with strict EU data residency and Oracle migration considerations, design an end-to-end Azure-native architecture to ingest tick data and user actions from multiple venues via Event Hubs, process near-real-time risk scoring with Spark on Azure Synapse, store per-tenant data in Delta Lake with strict isolation, and surface dashboards without data leaving the EU. Include data governance with Purview, BYOK with Key Vault, Private Link, DR, and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Oracle","Robinhood"]},{"id":"q-2993","question":"As Azure Solutions Architect for a EU-resident, multi-tenant SaaS platform about to enable partner data sharing, design a beginner-friendly Azure-native architecture that lets tenants securely publish a subset of their data to partner tenants within the EU. Use Data Lake Gen2 for raw/storage, Synapse for join/transform, Purview for data catalog and lineage, managed identities and tenant-scoped RBAC, and BYOK in Key Vault. Describe end-to-end data flow, isolation, data-sharing contracts, monitoring, and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Bloomberg","Plaid"]},{"id":"q-3032","question":"As EU-resident SaaS analytics platform, design a minimal Azure-native data pipeline that ingests per-tenant telemetry via HTTP and serves dashboards without EU data leaving the region. Outline end-to-end data flow, services, data isolation, security, retention, and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Airbnb","Apple","OpenAI"]},{"id":"q-3036","question":"As Azure Solutions Architect for a EU-resident, privacy-focused analytics startup with external partners, design a beginner-friendly Azure-native data processing and access-control stack that ingests user telemetry from a mobile app via Event Hubs, processes it in near real-time with Spark on Azure Synapse, stores per-tenant data in Delta Lake with strict isolation, and surfaces dashboards without data leaving the EU. Outline end-to-end data flows, services, and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Bloomberg","Google","Snap"]},{"id":"q-3139","question":"As Azure Solutions Architect for a global industrial IoT platform with strict EU data residency, design a cross-region edge-to-cloud telemetry pipeline that ingests device data from regional gateways into EU data stores, streams near real-time analytics, and serves dashboards without data leaving the EU. Include per-tenant isolation, BYOK, Private Endpoints, DR, and cost levers. How would you implement observability and fault-tolerance?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Anthropic","Meta","Microsoft"]},{"id":"q-3295","question":"As Azure Solutions Architect for a global fintech requiring EU data residency, design an Azure-native real-time data pipeline using Event Hubs and Azure Arc-enabled Postgres to keep sovereign data on-premise/other cloud, with a lakehouse in Synapse for analytics, per-tenant isolation, BYOK, and cross-region DR. Explain data flow, governance, and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Goldman Sachs","Instacart","Oracle"]},{"id":"q-3428","question":"As Azure Solutions Architect for a globally distributed, crypto-asset exchange, design an Azure-native, event-driven data plane that ingests tick data and user activity from multiple venues via Event Hubs and Kafka Connect, partitions by tenant, stores per-tenant data in Delta Lake on Synapse with strict isolation, applies BYOK with Key Vault and HSM, includes data masking, and supports cross-region DR with active-active replication and cost governance. Provide end-to-end data flows, governance components, and trade-offs?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Cloudflare","Coinbase","MongoDB"]},{"id":"q-3446","question":"For a globally available chat feature with EU data residency, design a beginner-friendly Azure-native deployment that scales to millions of users. Propose components (Azure Front Door, Functions with Durable Functions, Cosmos DB per-tenant partition, Redis for presence, Storage for media) and describe data flow, DR, and cost controls. Explain latency, consistency, and isolation trade-offs?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Discord","Instacart","Netflix"]},{"id":"q-3476","question":"As an Azure Solutions Architect for a Stripe-like payments platform, design a PCI-DSS compliant, Azure-native end-to-end architecture for real-time fraud detection and payment routing with strict data residency (EU/US), per-tenant isolation, and cross-region DR? Outline data flow, services, security controls, governance, and cost levers (BYOK, Private Link, autoscaling, data retention)?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["DoorDash","Stripe"]},{"id":"q-3618","question":"As Azure Solutions Architect for a EU-resident, multi-tenant marketplace handling real-time bidding data, design an Azure-native architecture that ingests per-tenant events from multiple vendors via Event Hubs, processes near-real-time analytics, and serves dashboards without EU data leaving the region. Include data isolation, BYOK, Private Link, DR, and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Airbnb","Google","Plaid"]},{"id":"q-3663","question":"As Azure Solutions Architect for a EU-resident, multi-tenant SaaS, design an Azure-native observability stack that provides per-tenant dashboards and SLA validation. Instrument services with OpenTelemetry, send traces to per-tenant EU App Insights, ship logs to per-tenant EU Log Analytics, and expose cost-aware dashboards. Outline data flow, services, and governance?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Google","Zoom"]},{"id":"q-3724","question":"As Azure Solutions Architect for a EU-resident, multi-tenant data marketplace, design a fully Azure-native streaming and analytics stack that ingests per-tenant event data via Event Hubs, enforces per-tenant data contracts using Azure Schema Registry, stores raw/enriched data in a tenant-scoped Delta Lake on Synapse in the EU, runs real-time scoring with Spark, uses BYOK via Key Vault, and guarantees data never leaves the EU while enabling compliant cross-tenant sharing via consent rules. Outline end-to-end data flow, governance, DR, and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Google","Instacart","Two Sigma"]},{"id":"q-3910","question":"As Azure Solutions Architect for a Zoom-scale platform, design an edge-to-cloud telemetry pipeline where millions of EU endpoints run edge modules that redact PII, sending only anonymized metrics to EU data planes. Ingest via MQTT to EU IoT Hub, stream to Event Hubs, near-real-time processing with Spark on Azure Synapse, store per-tenant data in Delta Lake in EU, enforce RBAC and BYOK, use Private Link, DR, and cost levers. What is the end-to-end data flow and governance plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Google","Scale Ai","Zoom"]},{"id":"q-3993","question":"As Azure Solutions Architect for a EU-resident, privacy-first streaming platform serving 200+ tenants, design an end-to-end Azure-native analytics stack ingested from edge encoders via Event Hubs, with dynamic per-tenant data masking/tokenization at ingress, Delta Lake partitions per tenant on Synapse, BYOK with Key Vault, tenant-scoped RBAC, EU-region DR, and automated Purview-based cross-tenant data sharing approvals. Describe data flows, isolation boundaries, governance, and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Netflix","Snap","Snowflake"]},{"id":"q-4014","question":"As Azure Solutions Architect for a global fintech platform with Square and Cloudflare integration, design an Azure-native edge-to-core architecture that processes payments at the edge, enforces data residency in EU and APAC regions, uses Azure Arc-enabled data services and confidential computing, integrates Cloudflare for edge security and traffic routing, and guarantees per-tenant isolation with BYOK. Outline data flows, services, DR, and cost levers, plus latency and availability targets?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Cloudflare","Square"]},{"id":"q-4141","question":"As Azure Solutions Architect for a Discord-like real-time chat platform, design an Azure-native, multi-tenant ingestion and search stack that ingests messages from a third-party chat API via Event Hubs, surfaces near-real-time search with Azure Cognitive Search, stores per-tenant data in Cosmos DB with dedicated throughput and strict isolation, leverages Private Link for data planes, and implements cost-aware autoscaling and data lifecycle policies. Explain data flow, isolation, security, and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Cloudflare","Discord"]},{"id":"q-4260","question":"As Azure Solutions Architect for a cross-brand data collaboration program across Two Sigma-like, Twitter-like, and Snap-like platforms, design an Azure-native multi-party data clean room that enables joint analytics without sharing raw data. Include data sources, governance, BYOK, data contracts, privacy-preserving compute, and DR strategy; explain how you ensure consent and regulatory compliance?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Snap","Twitter","Two Sigma"]},{"id":"q-4342","question":"As an Azure Solutions Architect for a global IoT telemetry platform with 100k tenants, design an Azure-native pipeline that ingests device telemetry from on-prem edge gateways, guarantees sub-500ms end-to-end latency for near-real-time dashboards, isolates tenant data in Delta Lake per-tenant, uses Event Hubs for ingest and Spark Structured Streaming for processing, stores in Synapse, enforces per-tenant RBAC with Azure AD, applies BYOK for data at rest, implements cross-region DR with active-active replicas, and provides a cost-optimization plan (auto-scaling, retention, quotas). What is the end-to-end data flow, components, and trade-offs?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Adobe","DoorDash","Oracle"]},{"id":"q-4350","question":"As Azure Solutions Architect for a EU-resident, multi-tenant SaaS, design a beginner-friendly tenant onboarding pattern that programmatically provisions isolated resources (App Service, Cosmos DB per-tenant container, and Blob storage) under a single management project using Bicep; implement per-tenant RBAC with Azure AD, BYOK via Key Vault, and enforce EU data residency. Describe data flow, isolation, DR (region pair), and basic cost controls?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Apple","Stripe"]},{"id":"q-4435","question":"As an Azure Solutions Architect for a beginner-friendly SaaS startup, design a single-region backend using Azure Functions and Cosmos DB to support 10k MAU with per-tenant isolation; ensure data at rest/in transit, use customer-managed keys in Key Vault, implement simple DR to a secondary region with manual failover, and cost controls via autoscale. Provide data model, security, and failover approach?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Amazon","Google","Uber"]},{"id":"q-4566","question":"As Azure Solutions Architect for a beginner-friendly global SaaS, design a cost-aware, Azure-native hosting and data pipeline for a new product with 10k MAU in a single region, scalable to multi-region later; use Azure Functions or App Service for the API, Cosmos DB for per-tenant data isolation, an event-driven analytics path with Event Grid to an ADLS Gen2 lake, and implement CMK in Key Vault, basic DR planning, and cost levers. Explain deployment, data flow, security, and trade-offs?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Cloudflare","Tesla","Twitter"]},{"id":"q-4663","question":"Design an Azure-native, multi-tenant streaming pipeline for a EU-resident analytics platform: ingest telemetry from mobile clients via Event Hubs, process with Databricks Spark, store tenant-isolated Delta Lakes in Synapse, and provide end-to-end data lineage and observability (OpenTelemetry trace propagation, Purview lineage, dashboards). What concrete steps, trade-offs, and components would you implement?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Apple","Databricks","Slack"]},{"id":"q-4674","question":"As an Azure Solutions Architect for a beginner-friendly SaaS, design an automated tenant-onboarding workflow that provisions per-tenant resources (Cosmos DB containers, Function app, API Management) in the EU region, implements BYOK via Key Vault, and uses ARM/Bicep with Azure Policy to enforce guardrails; describe data flow, RBAC, and a simple DR strategy?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Amazon","Citadel","Square"]},{"id":"q-4790","question":"As Azure Solutions Architect for a multinational media analytics platform, design a cost-aware, Azure-native data fabric that ingests regional CDN edge telemetry, performs edge aggregation with Azure IoT Edge, streams to EU Event Hubs, stores per-region data in Delta Lake with cross-region DR, enforces tenant isolation, applies privacy-preserving transforms (k-anonymity, data minimization) before storage, and uses BYOK and Private Link. Detail end-to-end data flow, governance, and trade-offs?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Apple","IBM"]},{"id":"q-887","question":"You’re building a multi-tenant analytics platform on Azure for a consumer-brand SaaS product. Each tenant must have isolated data processing, with per-tenant data lake isolation, on-demand Spark/notebook compute that auto-suspends, and cost governance at the tenant level. Propose an architecture using Azure Data Lake Storage Gen2, Unity Catalog or RBAC, Synapse or Databricks, private endpoints, and auditing. How do you ensure data isolation, prevent cross-tenant leakage, and meet compliance while keeping ops simple?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Instacart","Microsoft","Snap"]},{"id":"q-606","question":"How would you implement a rate limiter for a REST API to prevent abuse while ensuring legitimate users aren't blocked? Describe the algorithm and data structures you would use.","channel":"backend","subChannel":"api-design","difficulty":"intermediate","tags":["rate-limiting","api-design","redis","distributed-systems","backend"],"companies":["Google","Amazon","Twitter","Stripe","GitHub"]},{"id":"q-614","question":"How would you implement API rate limiting for a high-traffic service that needs to handle millions of requests per minute? Discuss the trade-offs between different algorithms and your approach for distributed systems.","channel":"backend","subChannel":"api-gateway","difficulty":"intermediate","tags":["rate-limiting","api-design","distributed-systems","redis","token-bucket","scalability"],"companies":["Google","Meta","Twitter","Stripe","Amazon","Netflix"]},{"id":"q-624","question":"How would you implement API rate limiting in a distributed system to prevent abuse while ensuring fair usage across multiple servers?","channel":"backend","subChannel":"api-infrastructure","difficulty":"intermediate","tags":["rate-limiting","redis","distributed-systems","api-design","scalability"],"companies":["Stripe","Twitter","GitHub","Google","Amazon"]},{"id":"q-611","question":"How would you implement API rate limiting to prevent abuse while ensuring fair usage for legitimate clients?","channel":"backend","subChannel":"api-middleware","difficulty":"intermediate","tags":["rate-limiting","api-design","middleware","redis","security"],"companies":["Twitter","GitHub","Stripe","Google","Amazon"]},{"id":"gh-46","question":"How would you design comprehensive API documentation that ensures smooth developer integration and reduces support overhead?","channel":"backend","subChannel":"apis","difficulty":"beginner","tags":["api","service-mesh"],"companies":["GitHub","LinkedIn","Microsoft","Postman","Stripe"]},{"id":"q-267","question":"Compare REST, GraphQL, and gRPC performance characteristics and identify optimal use cases for each protocol in modern microservices architecture?","channel":"backend","subChannel":"apis","difficulty":"beginner","tags":["rest","graphql","grpc","openapi"],"companies":["Amazon","Google","Microsoft","Netflix","Square","Stripe"]},{"id":"q-3259","question":"Design a contract-first API gateway strategy for a platform exposing REST, GraphQL, and gRPC endpoints. How would you enforce a unified auth, error payload, and tracing across protocols, while maintaining separate OpenAPI specs, GraphQL SDL, and protobufs, including versioning, translation layers, and cross-protocol contract testing?","channel":"backend","subChannel":"apis","difficulty":"advanced","tags":["rest","graphql","grpc","openapi"],"companies":["Citadel","Microsoft"]},{"id":"q-396","question":"You're building a microservice that needs to expose both REST and GraphQL endpoints for the same data model. How would you design the architecture to avoid code duplication while maintaining optimal performance for each query type?","channel":"backend","subChannel":"apis","difficulty":"intermediate","tags":["rest","graphql","grpc","openapi"],"companies":["Amazon","Booking.com","Citadel"]},{"id":"q-4518","question":"Design a beginner-friendly API gateway that serves REST at /api/v1/*, GraphQL at /graphql, and forwards to internal gRPC services. Explain how to auto-generate an OpenAPI spec from the gRPC interface, map REST/GraphQL fields to protobuf messages, and propagate JWT authentication across REST, GraphQL, and gRPC. Include a minimal Go snippet showing REST-to-gRPC translation via grpc-gateway?","channel":"backend","subChannel":"apis","difficulty":"beginner","tags":["rest","graphql","grpc","openapi"],"companies":["DoorDash","Snap","Uber"]},{"id":"q-515","question":"You're building a REST API for a payment service. How would you design the endpoint for processing a payment, and what HTTP status codes would you return for different scenarios?","channel":"backend","subChannel":"apis","difficulty":"beginner","tags":["rest","graphql","grpc","openapi"],"companies":["PayPal","Twitter"]},{"id":"q-539","question":"What is dependency injection in Spring and how does it improve application design?","channel":"backend","subChannel":"apis","difficulty":"intermediate","tags":["spring","dependency-injection","ioc","design-patterns","java"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-1196","question":"In a production backend with multiple IdPs (OIDC providers and a SAML bridge), design a token validation strategy to prevent replay and bind tokens to a device/session. Outline how you would implement: (a) JWKS caching and per-provider key rotation, (b) replay protection using jti stored in a distributed cache with TTL, (c) token binding via mTLS or client certificate binding, and (d) cross-provider revocation propagation and token lifecycle (short-lived access tokens with refresh tokens)?","channel":"backend","subChannel":"authentication","difficulty":"intermediate","tags":["jwt","oauth2","oidc","saml"],"companies":["Goldman Sachs","Hashicorp","Twitter"]},{"id":"q-1240","question":"Within an enterprise multi-IdP setup (OIDC and SAML), you run an API gateway that issues short-lived JWTs for a service mesh. Propose a concrete bridge design that: (a) supports converting SAML assertions and OIDC tokens into token-bound JWTs with audience and scope constraints, (b) binds tokens to a device fingerprint and a one-time nonce, (c) supports PKCE-backed mobile/native flows and refresh token rotation, (d) provides revocation and token introspection across regions, and (e) prevents token replay in a globally distributed environment. Explain data flows, token formats, and security checks?","channel":"backend","subChannel":"authentication","difficulty":"advanced","tags":["jwt","oauth2","oidc","saml"],"companies":["Anthropic","MongoDB"]},{"id":"q-2803","question":"In a microservice-backed ML platform gateway that supports OIDC and a SAML bridge, design a practical flow that issues a short-lived access token and a rotate-on-use refresh token bound to a device fingerprint. Describe data formats, Redis schema for jti and fingerprint binding, cross-region revocation, introspection, and how you handle failure modes like token theft or replay?","channel":"backend","subChannel":"authentication","difficulty":"beginner","tags":["jwt","oauth2","oidc","saml"],"companies":["Hugging Face","MongoDB","Snap"]},{"id":"q-3006","question":"You operate a gateway that supports both SAML and OIDC IdPs and issues short-lived JWTs for services. Propose a concrete migration path from SAML to OIDC that avoids downtime. Include (a) how to map SAML attributes to OIDC claims while preserving access controls, (b) token rotation strategy and PKCE support for mobile clients, (c) cross-region logout and token revocation, (d) user provisioning and immutable audit logs, (e) testing with canaries and rollback criteria?","channel":"backend","subChannel":"authentication","difficulty":"intermediate","tags":["jwt","oauth2","oidc","saml"],"companies":["IBM","LinkedIn"]},{"id":"q-342","question":"You're implementing OAuth2 for a SaaS product. A user reports their access token works but refresh token fails. What are the top 3 causes and how would you debug each?","channel":"backend","subChannel":"authentication","difficulty":"intermediate","tags":["jwt","oauth2","oidc","saml"],"companies":["Cohere","Hulu","Spotify"]},{"id":"q-3712","question":"You're building a backend SSO gateway that accepts SAML assertions from a partner IdP and issues internal JWTs for microservices. Describe a beginner-friendly, concrete plan to: (1) map SAML attributes to JWT claims, (2) verify the SAML signature with a simple metadata cache, (3) enforce audience/issuer per tenant, and (4) implement a minimal token revocation using a TTL cache and a /revoke endpoint. Include data structures and a small code snippet showing token issuance and revocation checks?","channel":"backend","subChannel":"authentication","difficulty":"beginner","tags":["jwt","oauth2","oidc","saml"],"companies":["Apple","Tesla","Zoom"]},{"id":"q-455","question":"Design a secure authentication system for a microservices architecture that supports JWT, OAuth2, and SAML. How would you handle token rotation, session management, and prevent token replay attacks across multiple services?","channel":"backend","subChannel":"authentication","difficulty":"advanced","tags":["jwt","oauth2","oidc","saml"],"companies":["OpenAI","Twitter"]},{"id":"q-4667","question":"You're building a multi-tenant auth service that issues short-lived access tokens (JWT) and rotating refresh tokens for mobile and web apps using OAuth2 with PKCE. Design a concrete, scalable strategy to prevent refresh token leakage across tenants: (a) token rotation policy and how to store/validate old vs new tokens, (b) device-bound/PKCE binding strategy, (c) per-tenant revocation and auditing, (d) cross-region replication and latency considerations, (e) failure modes and monitoring. Include data models and high-level pseudo-code for refresh handling?","channel":"backend","subChannel":"authentication","difficulty":"intermediate","tags":["jwt","oauth2","oidc","saml"],"companies":["Databricks","Hugging Face","LinkedIn"]},{"id":"q-544","question":"You're implementing SSO for an enterprise application using SAML 2.0. The IdP sends signed assertions but you're seeing intermittent 'Invalid Signature' errors. What are the most common causes and how would you debug them?","channel":"backend","subChannel":"authentication","difficulty":"intermediate","tags":["jwt","oauth2","oidc","saml"],"companies":["Adobe","Hashicorp"]},{"id":"q-1340","question":"How would you implement a tiered rate limiting system that provides different limits for free, premium, and enterprise customers while preventing users from bypassing limits by creating multiple accounts?","channel":"backend","subChannel":"backend","difficulty":"intermediate","tags":["rate-limiting","authentication","abuse-prevention","distributed-systems"],"companies":[]},{"id":"q-4124","question":"How would you implement adaptive rate limiting that dynamically adjusts limits based on system load, user behavior patterns, and time-of-day traffic patterns to optimize resource utilization while preventing abuse?","channel":"backend","subChannel":"backend","difficulty":"intermediate","tags":["adaptive-rate-limiting","dynamic-throttling","system-monitoring","distributed-systems","performance-optimization"],"companies":[]},{"id":"q-1116","question":"You're building a highly cached backend for a social app. **Redis** stores user profiles and feeds; **Memcached** caches post details. On a user profile edit, describe a precise, scalable strategy for **cache invalidation** that prevents stampedes, maintains consistency, and minimizes stale reads. Include data structures, TTLs, invalidation triggers, and atomic operations across **Redis** and **Memcached**, with concrete commands or pseudo-code?","channel":"backend","subChannel":"caching","difficulty":"advanced","tags":["redis","memcached","cache-invalidation"],"companies":["Apple","Google","Snap"]},{"id":"q-2663","question":"You're building a read-heavy analytics API that caches per-user daily summaries in Redis and Memcached. When a user performs an action that changes their summary, the cached entry must be invalidated and rebuilt on the next read. Design a practical cache invalidation approach that minimizes drift and avoids cache stampede. Include (a) the read path and data structures you would use, (b) how you trigger invalidation on writes, (c) handling bulk invalidation for many users in a single event, and (d) a basic testing approach to verify correctness?","channel":"backend","subChannel":"caching","difficulty":"beginner","tags":["redis","memcached","cache-invalidation"],"companies":["Netflix","Snap"]},{"id":"q-2938","question":"In a real-time pricing and inventory cache layer used by a high-traffic marketplace, design a robust cross-cache invalidation strategy for Redis (pricing/inventory) and Memcached, when updates come from multiple services (pricing, inventory, promotions). How do you keep reads fresh, avoid thundering herd, and tolerate partial failures? Include per-key invalidation, versioned keys, and a publish/subscribe pathway?","channel":"backend","subChannel":"caching","difficulty":"intermediate","tags":["redis","memcached","cache-invalidation"],"companies":["DoorDash","Scale Ai","Twitter"]},{"id":"q-3744","question":"Context: A read-heavy product catalog API used by Uber-scale apps. Data lives in PostgreSQL and is cached in Redis or Memcached. Propose a beginner-friendly plan to keep the cache correct when a product updates, avoiding stale reads. Cover TTL/eviction, invalidation signals, stampede protection, Redis vs Memcached trade-offs, and a small implementation sketch?","channel":"backend","subChannel":"caching","difficulty":"beginner","tags":["redis","memcached","cache-invalidation"],"companies":["Salesforce","Uber"]},{"id":"q-427","question":"You're building a user profile service that caches frequently accessed profiles. How would you implement cache invalidation when a user updates their profile, and what trade-offs would you consider between Redis and Memcached?","channel":"backend","subChannel":"caching","difficulty":"beginner","tags":["redis","memcached","cache-invalidation"],"companies":["Airbnb","Amazon","Google","Microsoft","Netflix","Snowflake","Stripe","Zoom"]},{"id":"q-443","question":"You're building a user profile API that caches user data in Redis. How would you implement cache invalidation when a user updates their profile, and what's the difference between using TTL vs explicit invalidation?","channel":"backend","subChannel":"caching","difficulty":"beginner","tags":["redis","memcached","cache-invalidation"],"companies":["Meta","MongoDB","NVIDIA"]},{"id":"q-4527","question":"You're caching per-tenant user profiles in Redis. Keys: tenant:{tid}:user:{uid}:profile with TTL 3600s. On a profile update, describe a beginner-friendly plan to invalidate caches efficiently using Redis Pub/Sub or keyspace notifications, how to structure keys/channels, implement stampede protection, and testing. Provide a concrete flow and a small code snippet?","channel":"backend","subChannel":"caching","difficulty":"beginner","tags":["redis","memcached","cache-invalidation"],"companies":["Databricks","LinkedIn","OpenAI"]},{"id":"q-330","question":"You're building a collaborative whiteboard app like Miro. When a user drags a shape, you need to update the UI immediately and persist the change. How would you implement this using CQRS?","channel":"backend","subChannel":"microservices","difficulty":"beginner","tags":["saga","cqrs","event-sourcing"],"companies":["Miro","Slack","Snowflake"]},{"id":"q-3349","question":"Design a beginner-friendly exercise for a microservices backend role: You have an Order service (creates orders), an Inventory service (manages stock), and a Payment service. Implement a simple saga using events and an orchestrator: when an order is placed, reserve inventory, then process payment; if payment fails, release inventory; if inventory is insufficient, cancel order. How would you model the event stream and state transitions?","channel":"backend","subChannel":"microservices","difficulty":"beginner","tags":["saga","cqrs","event-sourcing"],"companies":["Robinhood","Snowflake","Twitter"]},{"id":"q-3404","question":"You're building a multi-warehouse order fulfilment system using CQRS and event sourcing. An order may span two warehouses; a Saga coordinates InventoryA, InventoryB, and Shipping. If InventoryA reserves successfully but InventoryB times out, how would you implement compensating actions and idempotent handlers to guarantee consistency? Include event schemas, orchestration vs choreography choices, and your testing approach?","channel":"backend","subChannel":"microservices","difficulty":"intermediate","tags":["saga","cqrs","event-sourcing"],"companies":["Citadel","Google","Stripe"]},{"id":"q-364","question":"You're building an order management system using CQRS with microservices architecture. How would you ensure data consistency between the write and read models when a command to create an order is processed, considering network partitions and potential service failures?","channel":"backend","subChannel":"microservices","difficulty":"beginner","tags":["saga","cqrs","event-sourcing"],"companies":null},{"id":"q-379","question":"You're building a distributed order processing system using the Saga pattern. How would you handle compensation when a payment service fails after inventory has been reserved?","channel":"backend","subChannel":"microservices","difficulty":"beginner","tags":["saga","cqrs","event-sourcing"],"companies":["Elastic","Epic Systems","Oscar Health"]},{"id":"q-3876","question":"You're building a cross-service checkout in a microservices architecture for a high-traffic retailer. Use CQRS and event sourcing with a Saga orchestrator to coordinate Inventory, Payments, and Shipping. Describe event models, how to implement a long-running saga without distributed transactions, including compensating actions, idempotency and deduplication, plus snapshotting and read-model strategies. Provide a concrete sequence and a minimal saga state machine in pseudocode?","channel":"backend","subChannel":"microservices","difficulty":"advanced","tags":["saga","cqrs","event-sourcing"],"companies":["Cloudflare","Meta"]},{"id":"q-4624","question":"You're adding a voucher-aware checkout in a microservices stack using Saga, CQRS, and event sourcing. Design an orchestrated saga coordinating Inventory, Voucher, and Payment services to reserve stock, apply a voucher, and capture payment, ensuring idempotent replay, compensations, and correct read-model updates. Describe data models, event schemas, sequence, and a concise code pattern for one step and its compensation?","channel":"backend","subChannel":"microservices","difficulty":"intermediate","tags":["saga","cqrs","event-sourcing"],"companies":["Amazon","PayPal","Two Sigma"]},{"id":"q-666","question":"How would you implement a **saga**-driven checkout across services using **CQRS** and **event-sourcing**? Provide a concrete flow for an order touching Inventory, Payment, and Shipping: what commands and events you define, orchestration vs choreography, idempotency, compensating actions, and how read models are projected and kept consistent. Include reliability patterns like outbox and retries to ensure at-least-once delivery?","channel":"backend","subChannel":"microservices","difficulty":"intermediate","tags":["saga","cqrs","event-sourcing"],"companies":["DoorDash","OpenAI","Oracle"]},{"id":"q-667","question":"In a microservices backend for a retail platform, design a saga-driven workflow using CQRS and event sourcing across Order, Inventory, Payment, and Shipping. When an order is created, reserve inventory and authorize payment; on success, create shipping and complete the order. If inventory or payment fails, apply compensations (InventoryRelease, RefundPayment). Detail the event/command sequence, data in the event store, idempotency strategy, and orchestration vs choreography trade-offs?","channel":"backend","subChannel":"microservices","difficulty":"advanced","tags":["saga","cqrs","event-sourcing"],"companies":["Meta","Snowflake"]},{"id":"q-1095","question":"Design a globally distributed event store for a chat app where user_id determines the shard via consistent hashing. Each shard has 3 replicas in distinct regions; ingestion writes go to a leader replica and durably commit to all replicas using a 2-of-3 quorum. Reads are served from any replica with read-your-writes guarantees. Explain shard rebalancing without downtime, hot shard mitigation, cross-region replication lag, and failure recovery strategies?","channel":"backend","subChannel":"server-architecture","difficulty":"advanced","tags":["scaling","sharding","replication"],"companies":["Discord","MongoDB","Tesla"]},{"id":"q-249","question":"How would you implement a connection pool manager for aiohttp that handles graceful degradation under high load and connection timeouts?","channel":"backend","subChannel":"server-architecture","difficulty":"advanced","tags":["asyncio","aiohttp","concurrency"],"companies":["Airbnb","Amazon","Google","Meta","Microsoft","Netflix","Stripe","Uber"]},{"id":"q-2555","question":"Design a globally distributed event store for real-time analytics that shards by tenant_id and supports multi-region writes, cross-region replication, and per-tenant SLA guarantees. How would you choose shard keys, handle hot shards, implement idempotent writes, resolve conflicts, and perform resharding with minimal downtime?","channel":"backend","subChannel":"server-architecture","difficulty":"intermediate","tags":["scaling","sharding","replication"],"companies":["Databricks","PayPal","Zoom"]},{"id":"q-2970","question":"Design a scalable backend for a social feed that stores user posts in a sharded key-space by user_id, with cross-region replication and hotspot mitigation for celebrities. Describe shard strategy, online re-sharding, replication topology, consistency guarantees, and failure handling. Include concrete trade-offs and a minimal traffic-splitting plan?","channel":"backend","subChannel":"server-architecture","difficulty":"intermediate","tags":["scaling","sharding","replication"],"companies":["Airbnb","LinkedIn"]},{"id":"q-3319","question":"Design a globally distributed data plane for a marketplace with multi-region deployments that uses sharding and replication to scale writes for orders and inventory; specify shard keys, replication model, cross-region consistency, and how you rebalance shards without downtime while maintaining correctness under failures?","channel":"backend","subChannel":"server-architecture","difficulty":"advanced","tags":["scaling","sharding","replication"],"companies":["Airbnb","Citadel","Square"]},{"id":"q-4589","question":"You’re designing a globally scaled social feed where user timelines are partitioned by user_id across shards with replication for durability. Explain your approach to: a) shard key choice and routing for reads/writes, b) dynamic shard rebalancing to handle hotspots with minimal downtime, c) read-after-write guarantees vs. eventual consistency, and d) failure handling with replica promotion. End with ?","channel":"backend","subChannel":"server-architecture","difficulty":"beginner","tags":["scaling","sharding","replication"],"companies":["IBM","Plaid","Twitter"]},{"id":"q-485","question":"You're designing a distributed database for a fintech platform handling 10M transactions/day. How would you implement sharding and replication to ensure strong consistency while maintaining 99.99% availability?","channel":"backend","subChannel":"server-architecture","difficulty":"advanced","tags":["scaling","sharding","replication"],"companies":["Amazon","Coinbase","Plaid"]},{"id":"q-568","question":"How would you design a database schema for a user authentication system that needs to handle 1 million users with proper indexing and sharding considerations?","channel":"backend","subChannel":"server-architecture","difficulty":"beginner","tags":["scaling","sharding","replication"],"companies":["Citadel","LinkedIn","Tesla"]},{"id":"q-1648","question":"In a scenario with a two-week deadline for a critical feature, you mediate between a senior engineer pushing for scope expansion and a PM pressing to cut scope to meet the deadline. Describe your mediation steps, the decision framework you use, and how you communicate the final plan and follow-ups?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"advanced","tags":["negotiation","mediation","feedback"],"companies":["Bloomberg","Meta","Twitter"]},{"id":"q-185","question":"Describe a specific situation where you had to resolve a technical disagreement with a difficult team member. What conflict resolution techniques did you use, and what was the measurable outcome?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"intermediate","tags":["communication","collaboration"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-2636","question":"How would you mediate a cross-functional negotiation where shipping a feature on a tight deadline clashes with potential technical debt? Outline your data collection, trade-off framing (impact vs risk), stakeholder involvement, and how you document decisions and gather feedback to close the loop?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"intermediate","tags":["negotiation","mediation","feedback"],"companies":["LinkedIn","Meta","PayPal"]},{"id":"q-2832","question":"Explain how you would facilitate a 60-minute mediation between a product-management group and a research/ML team when a tight release deadline creates conflict over scope, quality gates, and risk appetite. What concrete steps, artifacts, and feedback loops would you use to reach a documented plan with commitments from both sides?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"advanced","tags":["negotiation","mediation","feedback"],"companies":["Hugging Face","Oracle"]},{"id":"q-312","question":"Tell me about a time you had to negotiate a solution between two team members with conflicting approaches?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"beginner","tags":["negotiation","mediation","feedback"],"companies":["Amazon","Google","Meta"]},{"id":"q-326","question":"Tell me about a time you had a conflict with a team member. How did you handle it and what was the outcome?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"beginner","tags":["negotiation","mediation","feedback"],"companies":["Crowdstrike","Salesforce","Tesla"]},{"id":"q-3569","question":"Describe a scenario: You are mediating a conflict between Product and Platform teams over a backend refactor that risks both deadline and reliability. A key stakeholder demands a rushed release while QA flags growing risk. What concrete, repeatable steps would you take to negotiate scope and deadline, deliver candid feedback to leadership about trade-offs, and restore collaboration without sacrificing reliability?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"advanced","tags":["negotiation","mediation","feedback"],"companies":["Cloudflare","Google"]},{"id":"q-3699","question":"You’re a program manager facilitating a cross-functional mediation between hardware and software engineers on a sprint disagreement. One side pushes a feature now, the other warns of risk. Outline a concrete 30-minute facilitation plan to surface concerns, negotiate a plan, and deliver feedback that preserves relationships and meets project priorities?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"beginner","tags":["negotiation","mediation","feedback"],"companies":["Apple","Tesla","Two Sigma"]},{"id":"q-3769","question":"In a cross-functional project, frontend, backend, and product clash on an API contract and delivery timeline. How would you conduct a mediation session to reach a tangible, trade-off based agreement? Outline your prep, facilitation steps, decision criteria, and how you document and measure success?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"intermediate","tags":["negotiation","mediation","feedback"],"companies":["IBM","Instacart","Oracle"]},{"id":"q-4299","question":"In a scenario where priorities clash between a product manager and a designer under a tight deadline, outline a concrete 60-minute process to surface priorities, negotiate scope, and land on a measurable plan. Include how you frame feedback, document decisions, and validate success with clear acceptance criteria and a checkpoint?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"beginner","tags":["negotiation","mediation","feedback"],"companies":["Apple","Meta"]},{"id":"q-439","question":"Tell me about a time you had to mediate a conflict between two senior engineers who disagreed on a critical technical approach for a high-stakes project with a tight deadline?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"advanced","tags":["negotiation","mediation","feedback"],"companies":["Amazon","Apple","Google","Instacart","Meta","Microsoft","Netflix"]},{"id":"q-446","question":"Tell me about a time you had a disagreement with a teammate about how to approach a project. How did you handle it?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"beginner","tags":["negotiation","mediation","feedback"],"companies":["IBM","OpenAI"]},{"id":"q-4481","question":"Describe a time you mediated a disagreement over a shared feature between frontend and backend engineers under a tight deadline. What steps did you take to surface concerns, negotiate concrete trade-offs (security, latency, UX), and deliver actionable feedback plus a written decision record?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"beginner","tags":["negotiation","mediation","feedback"],"companies":["Goldman Sachs","Hugging Face","Plaid"]},{"id":"q-4758","question":"Describe a time you mediated a negotiation between two internal stakeholders with conflicting priorities on a platform change that would impact SLAs (for example indexing strategy or data-model modification). Outline your preparation, mediation steps, trade-offs you proposed, and how you measured success, including the final outcome and any accountability actions?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"advanced","tags":["negotiation","mediation","feedback"],"companies":["Apple","Citadel","MongoDB"]},{"id":"q-486","question":"Tell me about a time you had to mediate a conflict between two senior engineers with opposing technical approaches. How did you handle it?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"advanced","tags":["negotiation","mediation","feedback"],"companies":["Microsoft","OpenAI","Square"]},{"id":"q-1039","question":"Describe a real incident in a fintech app where a release caused a customer-visible outage during market hours. Outline the explicit ownership and the immediate bias-for-action decision (hotfix vs rollback) with severity criteria, and how to ensure customer-obsessed restitution (transparent status updates, potential compensation, postmortem, and preventive steps)?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"beginner","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Google","Robinhood"]},{"id":"q-1243","question":"You're inheriting a mission-critical data pipeline that powers dashboards for a global customer; a nightly ETL job is failing in one region. Describe exactly how you would take ownership, act with urgency, and prioritize fixes while keeping customers informed. What trade-offs would you make and how would you measure success?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"advanced","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Airbnb","NVIDIA","Snowflake"]},{"id":"q-1969","question":"How would you own a cross‑functional initiative to ship a high‑impact feature under a tight deadline when early signals favor a different direction than customer feedback suggests? Describe your approach to ownership, bias-for-action, and customer obsession, including decision criteria, quick experiments, and stakeholder communication?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"intermediate","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Coinbase","Databricks"]},{"id":"q-2063","question":"In a Netflix-like app, a low-severity but high-visibility startup latency bug is reported by many users. Describe a concrete, end-to-end plan showing ownership, bias-for-action, and customer obsession: triage steps, quick hotfix, monitored rollout, rollback criteria, and a plan for a permanent fix with clear metrics and communication?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"beginner","tags":["ownership","bias-for-action","customer-obsession"],"companies":["NVIDIA","Netflix"]},{"id":"q-3099","question":"Tell me about a time you owned a failing customer-facing feature rollout. What concrete actions did you take to diagnose the root cause, decide a fast remediation, and communicate with customers and stakeholders? How did you balance ownership and bias-for-action with customer obsession, and what changes did you implement to prevent recurrence?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"advanced","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Apple","NVIDIA","Zoom"]},{"id":"q-3328","question":"Scenario: A production bug in a real-time data pipeline spikes latency and skews dashboards for customers during market hours. Describe exactly how you would demonstrate **ownership**, **bias-for-action**, and **customer-obsession**: define immediate triage steps, decision criteria (rollback vs patch), and a communication plan to stakeholders. Include specific time-bound actions for 0–60 min, 6 h, and 24 h?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"advanced","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Goldman Sachs","NVIDIA"]},{"id":"q-3345","question":"Leading a high-visibility feature release with a hard deadline, you discover a 0.7% increase in payment failures due to a new routing config. How do you own the issue, decide between ship or rollback, and keep customers and business goals aligned while minimizing risk? Provide concrete steps, metrics, and communication plan?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"intermediate","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Hashicorp","Scale Ai","Stripe"]},{"id":"q-3730","question":"You own a real-time recommendations service used by millions; a sudden bug skews results during peak load, and a full model retrain would take weeks. How do you decide between a reversible patch, a rollback, or a staged rollout with a quick fix, and how do you own the outcome while keeping customer impact front and center?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"intermediate","tags":["ownership","bias-for-action","customer-obsession"],"companies":["MongoDB","NVIDIA","Uber"]},{"id":"q-4224","question":"Describe a time you owned a high-stakes decision with incomplete telemetry that affected customers. How did you balance bias-for-action with customer-obsession, what concrete steps did you take to validate before shipping, and what was the outcome?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"intermediate","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Hugging Face","Square","Tesla"]},{"id":"q-431","question":"Tell me about a time when you noticed a small issue that others overlooked. How did you take ownership and what was the impact?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"beginner","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Hugging Face","IBM","Tesla"]},{"id":"q-4556","question":"You own a high-impact feature with uncertain customer impact and noisy metrics. Walk through how you would decide what to ship, what experiments you’d run to validate hypotheses with customers, how you guard against bias-for-action, and how you measure success before a regional rollout?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"intermediate","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Airbnb","Lyft","Slack"]},{"id":"q-4607","question":"Describe a time you owned a project with a high customer impact where a critical feasibility assumption proved wrong. How did you take ownership, decide quickly on the path forward, and communicate tradeoffs to stakeholders while keeping customer outcomes at the center? What was the result and what did you learn?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"intermediate","tags":["ownership","bias-for-action","customer-obsession"],"companies":["NVIDIA","Two Sigma"]},{"id":"q-516","question":"Tell me about a time you had to make a critical decision with incomplete data. How did you balance speed vs accuracy?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"intermediate","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Cloudflare","Coinbase","PayPal"]},{"id":"q-1273","question":"Describe a time you faced a technical disagreement in a cross-functional project and successfully influenced a decision without direct authority. What was the conflict, what steps did you take to communicate your view, what data or patterns supported you, and what was the outcome?","channel":"behavioral","subChannel":"soft-skills","difficulty":"beginner","tags":["communication","collaboration","influence"],"companies":["NVIDIA","Tesla"]},{"id":"q-1282","question":"Describe a time when influence changed a project's approach after data showed the initial plan underperformed. What data did you present, what communication channels did you use, how did you secure collaboration, and what was the outcome and learning?","channel":"behavioral","subChannel":"soft-skills","difficulty":"beginner","tags":["communication","collaboration","influence"],"companies":["Google","Tesla","Twitter"]},{"id":"q-2082","question":"Describe a time when a project failed due to miscommunication across cross-functional teams (engineering, product, and QA). What happened, how did you identify the root cause, what concrete steps did you take to restore alignment, and what would you do differently to prevent recurrence?","channel":"behavioral","subChannel":"soft-skills","difficulty":"beginner","tags":["communication","collaboration","influence"],"companies":["Citadel","Google","Oracle"]},{"id":"q-2154","question":"Describe a time when a project faced scope drift due to a miscommunication across teams. What steps were taken to clarify priorities, align stakeholders, and avoid future breakdowns? Include the channels you used, artifacts (like a decision log or a RACI matrix), and the outcome?","channel":"behavioral","subChannel":"soft-skills","difficulty":"beginner","tags":["communication","collaboration","influence"],"companies":["Google","IBM","Snap"]},{"id":"q-2421","question":"Describe a time you faced a tight deadline and needed to align multiple functions on a trade‑off decision. How did you map stakeholders, justify scope changes, communicate the rationale, and secure buy‑in to move forward?","channel":"behavioral","subChannel":"soft-skills","difficulty":"intermediate","tags":["communication","collaboration","influence"],"companies":["Apple","LinkedIn","NVIDIA"]},{"id":"q-2569","question":"Explain a situation where you had to persuade a cross-functional team to adopt a technical approach you believed was best. How did you diagnose concerns, tailor your communication, obtain buy-in, and ensure follow-through across teams?","channel":"behavioral","subChannel":"soft-skills","difficulty":"advanced","tags":["communication","collaboration","influence"],"companies":["IBM","Meta","Tesla"]},{"id":"q-2892","question":"Tell me about a cross-functional data-platform rollout that missed its Q1 targets. How did you surface the issue, take ownership, drive a postmortem, identify root causes, and implement corrective actions with measurable impact? What would you change for next time?","channel":"behavioral","subChannel":"soft-skills","difficulty":"advanced","tags":["communication","collaboration","influence"],"companies":["Databricks","Meta","Netflix"]},{"id":"q-297","question":"Tell me about a time you had to influence a senior stakeholder who disagreed with your technical approach. How did you handle it?","channel":"behavioral","subChannel":"soft-skills","difficulty":"advanced","tags":["communication","collaboration","influence"],"companies":["Amazon","Google","Meta"]},{"id":"q-3051","question":"Describe a time when a critical project faced a major setback due to miscommunication among stakeholders. How did you surface the issue, align competing priorities, and influence a path forward? What concrete steps did you take, what metrics did you track, and how did you ensure accountability across parties?","channel":"behavioral","subChannel":"soft-skills","difficulty":"advanced","tags":["communication","collaboration","influence"],"companies":["Instacart","Meta"]},{"id":"q-3198","question":"In a cross-functional project at Hugging Face, Nvidia, or Robinhood, you strongly believe a model evaluation should emphasize fairness metrics, while product stakeholders favor throughput. How would you communicate your stance, build consensus, and influence the final decision while maintaining trust and collaboration?","channel":"behavioral","subChannel":"soft-skills","difficulty":"beginner","tags":["communication","collaboration","influence"],"companies":["Hugging Face","NVIDIA","Robinhood"]},{"id":"q-3851","question":"Explain how you influenced multiple teams to adopt a user-facing change with conflicting priorities and tight deadline. What actions did you take to communicate, who did you engage, and what was the impact?","channel":"behavioral","subChannel":"soft-skills","difficulty":"beginner","tags":["communication","collaboration","influence"],"companies":["Discord","Google"]},{"id":"q-4781","question":"In a high-stakes data platform migration that derails at go-live, how would you diagnose the root cause, coordinate cross-functional teams (data engineers, ML engineers, product, security), and influence timely decisions to minimize business impact while preserving trust and future reliability?","channel":"behavioral","subChannel":"soft-skills","difficulty":"advanced","tags":["communication","collaboration","influence"],"companies":["Databricks","Snowflake"]},{"id":"q-913","question":"Tell me about a time you persuaded a cross-functional team to adopt a non-obvious technical approach. What was the situation, how did you influence without direct authority, what data or experiments did you rely on, what trade-offs did you communicate, and what was the result?","channel":"behavioral","subChannel":"soft-skills","difficulty":"intermediate","tags":["communication","collaboration","influence"],"companies":["Bloomberg","Google","Salesforce"]},{"id":"q-212","question":"How would you structure a STAR method response when describing a time you resolved a technical conflict between frontend and backend teams over API design, and what specific communication strategies did you employ?","channel":"behavioral","subChannel":"star-method","difficulty":"beginner","tags":["situation","task","action","result"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Postman"]},{"id":"q-2493","question":"Describe a time you shipped a small feature under a tight deadline. What was the situation and your task, what actions did you take to deliver on time, and what was the result? Include how you defined the MVP scope, handled trade-offs, and validated the outcome?","channel":"behavioral","subChannel":"star-method","difficulty":"beginner","tags":["situation","task","action","result"],"companies":["Robinhood","Square"]},{"id":"q-2732","question":"Describe a time you faced a tight deadline to migrate a MongoDB replica set or perform a critical data operation under production load. Outline the Situation, Task, Action, and Result, including how you scoped work, mitigated downtime (e.g., rolling upgrades, backups, feature flags), and how you validated data integrity post-change?","channel":"behavioral","subChannel":"star-method","difficulty":"beginner","tags":["situation","task","action","result"],"companies":["LinkedIn","MongoDB"]},{"id":"q-2945","question":"Describe a time you resolved a high-stakes architectural disagreement about a distributed system (e.g., multi-region deployment, eventual vs strong consistency) while delivering a critical milestone. What was the situation, the task, your actions to justify and implement the decision, and the measurable results?","channel":"behavioral","subChannel":"star-method","difficulty":"advanced","tags":["situation","task","action","result"],"companies":["Hashicorp","Microsoft","MongoDB"]},{"id":"q-3639","question":"Describe a time when you discovered a critical usability or safety risk in a new feature near launch. You must decide whether to ship with mitigations or delay. Outline the Situation, Task, Action, and Result, including how you quantify risk, who you involve, what trade-offs you propose, and how you communicate the final decision to leadership?","channel":"behavioral","subChannel":"star-method","difficulty":"beginner","tags":["situation","task","action","result"],"companies":["Apple","NVIDIA","Tesla"]},{"id":"q-375","question":"Tell me about a time when you had to make a critical technical decision with incomplete data that impacted production systems. What was the situation, what data did you have, what decision did you make, and what was the result?","channel":"behavioral","subChannel":"star-method","difficulty":"advanced","tags":["situation","task","action","result"],"companies":["Amazon","Apple","Coinbase","Google","Meta","Microsoft","Netflix"]},{"id":"q-3878","question":"Scenario: You’re a junior engineer at HashiCorp assigned to a flaky CI pipeline that sometimes marks PRs as failing during Terraform module tests. A PM pressures a release of a v1.1 feature; QA demands more isolated tests. Describe concrete, repeatable steps to diagnose, decide scope, implement a minimal fix, and verify impact before re-opening the PR?","channel":"behavioral","subChannel":"star-method","difficulty":"beginner","tags":["situation","task","action","result"],"companies":["Hashicorp","Microsoft"]},{"id":"q-389","question":"Tell me about a time when you had to convince your team to adopt a new technology or approach that they were initially resistant to. What was the situation, what did you do, and what was the outcome?","channel":"behavioral","subChannel":"star-method","difficulty":"intermediate","tags":["situation","task","action","result"],"companies":["Expedia","IBM","Shopify"]},{"id":"q-4678","question":"During market hours, a data ingestion pipeline (Kafka → Spark Streaming → Delta Lake) intermittently drops a subset of tick data, rendering dashboards stale. As incident lead, coordinate SRE, data engineering, and product to surface impact, decide between rollback, hotfix, or degraded mode, and document a durable remediation plan with ownership and metrics. Answer in STAR format?","channel":"behavioral","subChannel":"star-method","difficulty":"intermediate","tags":["situation","task","action","result"],"companies":["Bloomberg","NVIDIA","Stripe"]},{"id":"q-4723","question":"Intermittent cross-service checkout failures occur in a multi-service payment flow under latency spikes, affecting PayPal/Discord users. With a 48-hour patch window, outline a STAR-based plan: how you observe the issue, reproduce it, isolate root cause, decide between a quick workaround (circuit breaker/feature flag) and a permanent fix, implement and verify, and document decisions with rollback plans. What would you report to stakeholders and how would you measure success?","channel":"behavioral","subChannel":"star-method","difficulty":"beginner","tags":["situation","task","action","result"],"companies":["Discord","PayPal"]},{"id":"q-569","question":"Tell me about a time you had to make a difficult technical decision with incomplete information under extreme pressure?","channel":"behavioral","subChannel":"star-method","difficulty":"advanced","tags":["situation","task","action","result"],"companies":["Citadel","Netflix","Tesla"]},{"id":"q-1073","question":"Implement transpose64(uint64_t x) by treating x as an 8x8 bit matrix in row-major order (bits 0-7 are row 0, 8-15 row 1, etc.). Return the transposed matrix (bit (r,c) moves to (c,r)). Use only bitwise ops and shifts; no loops or conditionals. Provide signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Cloudflare","Coinbase","Hashicorp"]},{"id":"q-1216","question":"Implement reverse128 for a 128-bit value stored as two 64-bit halves hi and lo. Provide a function:\n\nvoid reverse128(uint64_t hi, uint64_t lo, uint64_t *out_hi, uint64_t *out_lo);\n\nThe reversal should map bit i to bit 127 - i, using only bitwise operations and shifts (no loops or conditionals). Include a brief justification and 1-2 quick test examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Google","Snowflake","Stripe"]},{"id":"q-2040","question":"Implement countTrailingZeros64(uint64_t x) using only bitwise operations and shifts (no loops or conditionals). Return 64 if x == 0. Use a De Bruijn sequence with a 64-entry lookup table. Provide the function signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Oracle","Twitter","Uber"]},{"id":"q-2221","question":"In a 32-bit word handling module, implement rotateRight32(uint32_t n, uint32_t k) that returns n rotated right by k positions. Use only bitwise operations and shifts; no loops or conditionals. Assume 0 <= k < 32. Provide the function signature and a brief justification, plus 1–2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Google","MongoDB","Twitter"]},{"id":"q-2271","question":"Given a 64-bit bitboard representing an 8x8 grid (bit 0 = A1, bit 63 = H8), implement neighborMask64(uint64_t board) that returns a mask with a 1 where a bit has any of its eight neighboring cells set in the original board. Use only bitwise operations and shifts; no loops or conditionals. Provide the function signature and a brief justification, plus 1-2 quick tests?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Snap","Snowflake"]},{"id":"q-2354","question":"In bit-manipulation interviews, implement rotateRight64(uint64_t n, unsigned k) that rotates n right by k bits (0 <= k < 64). Use only bitwise operations and shifts; no loops or conditionals. Provide the function signature and a brief justification, plus 1-2 quick examples demonstrating the behavior?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Amazon","Google","Instacart"]},{"id":"q-2644","question":"Given a 64-bit unsigned integer n with k set bits, implement nextHigherWithSameBitCount(n) that returns the smallest y > n with popcount(y) == k; if no such y exists, return 0. Provide a function signature and 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Airbnb","Netflix"]},{"id":"q-2665","question":"Treat n as an 8x8 bit matrix in row-major order. Implement rotate90Clockwise64(uint64_t n) that returns the matrix rotated 90 degrees clockwise, using only bitwise operations and shifts (no loops or conditionals). Provide the signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["LinkedIn","Snap"]},{"id":"q-2723","question":"Implement a 32-bit Morton code generator morton2D(uint16_t x, uint16_t y) that interleaves bits of x and y (x bit 0 -> code bit 0, y bit 0 -> bit 1, etc.). Use only bitwise operations and shifts (no loops). Provide the signature and a brief justification, plus 1-2 examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Goldman Sachs","IBM"]},{"id":"q-3186","question":"Implement a 32-bit header packer. Write a function packHeader(uint8_t version, uint8_t flags, uint16_t length) that places version in bits 31-28, flags in bits 27-20, and length in bits 19-0. Use only bitwise operations and shifts, no loops or conditionals. Provide the signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Amazon","Instacart"]},{"id":"q-3354","question":"In a 64-bit word, four 16-bit channels are packed as R (bits 0-15), G (16-31), B (32-47), A (48-63). Implement a function swapR_B(uint64_t v) that returns a new 64-bit value with R and B swapped using only bitwise operations and shifts (no loops/conditionals). Provide the signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Bloomberg","NVIDIA","Oracle"]},{"id":"q-3372","question":"Implement nextGen64(uint64_t board) that applies one Life tick on an 8x8 toroidal grid. Bit i maps to cell (row=i/8, col=i%8) in row-major order. Return the next board as uint64_t. Use only bitwise operations and shifts; no loops or conditionals?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Discord","Hugging Face","Oracle"]},{"id":"q-3507","question":"Implement a function isPowerOfTwo128(uint64_t hi, uint64_t lo) that returns 1 if the 128-bit value formed by hi:lo has exactly one bit set (is a power of two), else 0. Use only bitwise operations and shifts (no loops or conditionals). Provide the signature and a brief justification, plus 1–2 quick test examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Adobe","Apple","NVIDIA"]},{"id":"q-3562","question":"Given a 128-bit value split into hi and lo, implement a rotate-left by k bits (0 <= k < 128) that returns new hi/lo using only bitwise operations and shifts, with no loops or conditionals. Provide the function signature and a brief justification, and include 1-2 quick edge-case examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Cloudflare","OpenAI","Tesla"]},{"id":"q-3602","question":"Implement parity128 for a 128-bit value stored as hi and lo: the function returns 1 if the total number of set bits across hi and lo is odd, otherwise 0. Use only bitwise operations and shifts; no loops or conditionals. Provide the function signature and a brief justification. Include 1–2 quick sanity checks (e.g., 0x0/0x0 -> 0; 0x1/0x0 -> 1)?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Google","Salesforce"]},{"id":"q-3864","question":"Given a 64-bit unsigned integer n, implement swapAdjacent64 that returns a value with each bit pair swapped (bit 0 <-> bit 1, bit 2 <-> bit 3, ..., bit 62 <-> bit 63) using only bitwise operations and shifts, no loops or conditionals. Provide the function signature, a short justification, and 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Adobe","IBM","Meta"]},{"id":"q-3966","question":"Implement rotate-left by k bits for a 32-bit unsigned integer using only bitwise operations and shifts. Provide the function signature and a branch-free solution that handles k in 0..31. Explain why it works and show 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Anthropic","Goldman Sachs"]},{"id":"q-3983","question":"Implement a 3D Morton code function morton3D(uint32_t x, uint32_t y, uint32_t z) -> uint64_t that interleaves the bits of x,y,z so that bit i of x goes to bit 3i, bit i of y to 3i+1, and bit i of z to 3i+2 for i=0..20. Use only bitwise operations and shifts (no loops or conditionals). Assume each input uses only the lower 21 bits. Context: spatial indexing for voxel data in a 3D engine?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Meta","MongoDB","Salesforce"]},{"id":"q-4116","question":"Implement a 32-bit rotate-left function rotl32(uint32_t n, unsigned int k) that rotates n left by k bits (i.e., bits shifted out on the left wrap around to the right). Use only bitwise operations and shifts, no loops or conditionals. Provide the function signature, a short justification, and 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Cloudflare","Instacart","Meta"]},{"id":"q-4136","question":"Implement rotateLeft32(n, k) that rotates a 32-bit value n left by k positions using only bitwise operations and shifts (no loops or conditionals). Use a 32-bit unsigned interpretation and ensure k is treated modulo 32. Provide the function signature, a brief justification, and 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Microsoft","Twitter"]},{"id":"q-4148","question":"Context: In a high-performance network filter, a 64-bit mask denotes enabled features. Implement lsbIndex64(uint64_t n) that returns the index (0-63) of the least significant set bit in n, assuming n != 0. Use only bitwise operations and shifts; no loops or conditional branches. Provide the function signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Cloudflare","Coinbase"]},{"id":"q-4193","question":"Implement rotateLeft32(uint32_t n, uint32_t k) that rotates n left by k positions. Use only bitwise operations and shifts, no loops or conditionals. Normalize k to 0..31 and return (n << kk) | (n >> ((32 - kk) & 31)). Provide the function signature and brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Microsoft","Snowflake"]},{"id":"q-4436","question":"Write a 64-bit function nextBitPermutation64(uint64_t x) that returns the next higher number with the same popcount as x, or 0 if none; implement using only bitwise operations and shifts, with no loops or conditionals. Provide the signature and a brief rationale, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["DoorDash","Google"]},{"id":"q-4455","question":"In a 32-bit bitmap of flags, implement a function uint32_t popcount32(uint32_t n) that returns the number of 1-bits using only bitwise operations and shifts, with no loops or conditionals. Provide the function signature, a brief justification, and 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Coinbase","Goldman Sachs","NVIDIA"]},{"id":"q-4575","question":"In a 32-bit unsigned integer context, implement a rotate-left function rotl32(n, k) that shifts n left by k bits and wraps the high bits around to the low end. Do not use loops or conditionals. Provide the signature, a brief justification, and 1-2 concrete examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Databricks","Discord","MongoDB"]},{"id":"q-4829","question":"Implement bitpack64(uint64_t v, uint64_t mask) that compresses bits from v selected by mask into the least-significant bits, preserving the relative order of those bits (i.e., the i-th set bit in mask maps to bit i in the result). Use only bitwise operations and shifts; no loops or conditionals. Provide the signature and a brief justification, plus 1-2 examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Cloudflare","Discord","Hugging Face"]},{"id":"q-680","question":"Given a 32-bit unsigned int n, implement a function hasAdjacentOnes(n) that returns true if n contains any two consecutive 1 bits (for example 0b1100100 has adjacent ones). Use only bitwise operations, no loops or lookups. Explain the core trick in a sentence?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Cloudflare","Oracle","Two Sigma"]},{"id":"q-689","question":"Given a 32-bit unsigned integer n, implement a function isSingleEvenBitSet(n) that returns true if exactly one bit is set and that bit lies at an even index (0, 2, 4, ...). Use only bitwise operations, no loops or built-in helpers. Provide the expression and a brief justification, plus a couple of quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["LinkedIn","MongoDB","Snowflake"]},{"id":"q-698","question":"Implement a 32-bit unsigned integer function popcount32(n) that returns the number of set bits in n using only bitwise operations and shifts, with no loops or built-ins. Use the SWAR (SIMD Within A Register) technique: apply a sequence of masks and shifts (0x55555555, 0x33333333, 0x0F0F0F0F) and a final multiply to consolidate counts. Provide the function and a brief justification, plus a couple of quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Google","Two Sigma"]},{"id":"q-705","question":"Implement reverseBits32(n) that reverses all 32 bits in a 32-bit unsigned integer using only a fixed sequence of bitwise operations (no loops or conditionals). Use a SWAR-style approach with masks 0x55555555, 0x33333333, 0x0F0F0F0F and 0x00FF00FF, finishing with a 16-bit half-swap. Provide the function signature and a brief justification, plus a couple of quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Coinbase","Snap"]},{"id":"q-711","question":"In a network packet parser, a 32-bit field uses trailing-zero count to encode the length of a value. Given a 32-bit unsigned n, implement countTrailingZeros32(n) that returns the number of trailing zero bits (0-32). If n==0, return 32. Use only bitwise operations, shifts, and basic arithmetic, no loops or built-ins. Provide signature, brief justification, and 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Amazon","Google","Uber"]},{"id":"q-724","question":"In memory allocation, implement nextPowerOfTwo32(uint32_t n) that returns the smallest 32-bit unsigned power-of-two >= n, given n > 0, using only bitwise operations with no loops or conditionals. If the result would overflow 32 bits, return 0. What is the correct implementation signature and approach?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Apple","IBM","Uber"]},{"id":"q-730","question":"Given a 32-bit unsigned integer n, implement swapAdjacentBits32(n) that returns a new 32-bit value with every adjacent bit pair swapped (bits 0-1, 2-3, ..., 30-31). Use only bitwise operations, no loops or conditionals. Provide the function signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Amazon","Plaid","Twitter"]},{"id":"q-738","question":"Implement a 64-bit bit reversal function reverse64(uint64_t n) that returns the bitwise reversal of n (bit 0 becomes bit 63, bit 63 becomes bit 0). Use only bitwise operations and shifts, no loops or conditionals. Provide the function signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Lyft","MongoDB","PayPal"]},{"id":"q-747","question":"Implement isBinaryPalindrome32(uint32_t n) that returns 1 if the 32-bit binary representation of n is a palindrome (bit0 equals bit31, bit1 equals bit30, etc.), and 0 otherwise. Use only bitwise operations and shifts; no loops or conditional branches. Provide the function signature and a brief justification, plus 1-2 quick examples.\n\nExamples:\n- 0x80000001 is a palindrome\n- 0xA5A5A5A5 is not a palindrome?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Coinbase","Google","Netflix"]},{"id":"q-751","question":"Implement popcount64(uint64_t x) that returns the number of set bits in x using only bitwise operations and shifts, with no loops and no built-in popcount. Use a fixed SWAR approach with masks 0x5555555555555555ULL, 0x3333333333333333ULL, 0x0F0F0F0F0F0F0F0FULL and a final multiply/shift step. Provide the function signature and a brief justification, plus 1-2 quick examples of inputs and outputs?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Lyft","MongoDB","Snap"]},{"id":"q-757","question":"Given a 32-bit unsigned integer n, implement maskBelowLSB32(n) that returns a 32-bit mask with all bits at positions <= the least significant set bit of n turned on; if n is 0 return 0. Use bitwise operations and shifts (and optional arithmetic). Provide the signature, justification, and 1-2 examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Meta","Microsoft","Zoom"]},{"id":"q-776","question":"Implement a 64-bit bitboard function knightAttacks64(n) that returns a 64-bit mask of all squares attacked by knights, given a 64-bit bitboard n where 1s indicate knight positions. Use only bitwise operations and shifts, no loops or conditionals. Provide the signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Bloomberg","Netflix","Plaid"]},{"id":"q-783","question":"Given a 32-bit unsigned n, implement hasPattern101(n) that returns true if there exists any i such that bits i, i+1, i+2 form 101 (n_i=1, n_{i+1}=0, n_{i+2}=1). Use only bitwise operations and shifts, no loops or conditionals. Provide the function signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Apple","Plaid","Slack"]},{"id":"q-791","question":"Implement parity32(n) that returns 1 if the number of set bits in a 32-bit unsigned n is odd, otherwise 0. Do not use loops or built-ins; only bitwise ops and shifts. Provide function signature parity32(uint32_t n) and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Databricks","DoorDash"]},{"id":"q-798","question":"Implement nextHigherWithSamePopcount64(uint64_t n) that returns the smallest integer greater than n with the same number of 1-bits. Use only bitwise operations and shifts; no loops or conditionals. If no such number exists, return 0. Provide the function signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Google","Two Sigma"]},{"id":"q-807","question":"Implement rotateLeft128 by k on a 128-bit value stored as two 64-bit words (hi, lo). The function rotates within the 128-bit boundary by k bits (0 <= k < 128) using only bitwise operations and shifts, no loops or conditionals. Provide the signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Google","Meta","Twitter"]},{"id":"q-814","question":"Implement interleave16(uint16_t a, uint16_t b) that returns a 32-bit value with bits interleaved as a0 b0 a1 b1 ... a15 b15, where a0 is LSB of a and b0 is LSB of b. Use only bitwise operations and shifts (no loops or conditionals). Provide the function signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Adobe","Snowflake","Zoom"]},{"id":"q-823","question":"Implement rotateLeft32(uint32_t n, unsigned int k) that returns the 32-bit value formed by rotating n left by k bits. Constraints: no loops or conditionals; handle k >= 32 by using k % 32. Provide the function signature and a brief justification, and give 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Citadel","LinkedIn","Stripe"]},{"id":"q-830","question":"Implement xor32(uint32_t a, uint32_t b) that returns a ^ b without using the ^ operator. Use only &, |, ~ and shifts. Provide the function signature and a brief justification, plus 1-2 quick examples to verify correctness?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Instacart","Netflix"]},{"id":"q-1091","question":"Scenario: An OTA firmware update causes GPS altitude drift in a subset of IoT devices across regions. Design a CAPA program to detect, collect evidence, and prevent recurrence. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API for CAPAs with device logs, 4) region/device-type metrics (recurrence rate, MTTR, false positives), 5) an RCA template and a canary rollback plan for OTA updates?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Plaid","Snowflake","Tesla"]},{"id":"q-1111","question":"Scenario: Production ML feature store drift after a data refresh degrades latency and CTR across regions due to stale features and late streaming data. Propose a CAPA program: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPAs, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region-aware metrics to prove containment and recurrence, 5) an RCA template and a canary rollout plan to prevent recurrence during future refreshes?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["LinkedIn","Tesla"]},{"id":"q-1132","question":"Scenario: batch ingestion misses PII masking in two tenants across regions, raising privacy risk. Design a CAPA program: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) tenant/region-aware metrics to prove containment and recurrence (MTTD, MTTR, false positives), 5) an RCA template and a canary rollout plan to validate policy enforcement before global deployment?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Coinbase","MongoDB","Salesforce"]},{"id":"q-1156","question":"Scenario: A distributed streaming analytics pipeline processes click events for an online marketplace. A recently deployed shard rebalancing causes out-of-order events in two regions, leading to incorrect revenue attribution and fraud alerts. Design a CAPA program that covers: 1) a CAPA data model that captures evidence (events, traces, timestamps, orderings) and artifacts (config, manifest, canary results), 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces and event metadata, 4) region- and shard-aware metrics to prove containment (recurrence rate, MTTR, misattribution rate), 5) an RCA template and a canary-rollback plan for shard rebalancing?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Amazon","Apple"]},{"id":"q-1200","question":"Scenario: A global real-time telemetry platform for autonomous vehicles experiences intermittent PII exposure due to a log-redaction misconfiguration after a software update in two regions. Design a CAPA program to detect, document, and prevent recurrence. Include: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) tenant- and region-aware metrics (recurrence rate, MTTR, false positives), 5) an RCA template and a canary-based rollback/patch plan, 6) a policy gate preventing deployment until redact coverage is above threshold?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["MongoDB","Tesla"]},{"id":"q-1426","question":"Scenario: A multi-tenant SaaS billing system release unintentionally changes pricing rules for a subset of tenants, triggering incorrect invoices and revenue churn. Design a beginner CAPA program to address this. Your task: 1) propose a CAPA data model capturing evidence (invoices, logs, traces) and artifacts (config, feature flags, pricing rules); 2) define a lifecycle state machine for CAPA progression; 3) outline a minimal REST API to create/update CAPAs with linked billing events; 4) specify tenant- and plan-level metrics to prove containment (recurrence rate, MTTR, false positives); 5) provide an RCA template and a canary-based patch plan before full rollout; 6) draft a simple policy gate that prevents deployment until pricing rules pass a preflight check?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Hashicorp","Zoom"]},{"id":"q-1450","question":"Scenario: A global multi-tenant data platform implements a new consent-logging feature. After rollout, a bug in the consent service causes PII redaction failures in two regions, risking data exposure and compliance violations. Design a CAPA program to detect, collect evidence, and prevent recurrence. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region- and tenant-aware metrics (recurrence rate, MTTR, false positives), 5) an RCA template and a canary rollout plan to validate fixes before global deployment?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Citadel","LinkedIn","Stripe"]},{"id":"q-1485","question":"**Scenario**: A cross-tenant data export feature in a multi-tenant SaaS app accidentally exposes data from unrelated tenants during a canary rollout in two regions. Design a beginner **CAPA** program to detect, document, and prevent recurrence. Your task: 1) propose a CAPA data model framing evidence and artifacts, 2) define a lifecycle state machine, 3) outline a minimal REST API to create/update CAPAs with linked logs/traces, 4) specify tenant-scoped metrics for containment, 5) provide an RCA template and a canary-based fix plan to validate before broader rollout?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Salesforce","Twitter"]},{"id":"q-1502","question":"Scenario: A new analytics feature ingests raw customer IDs into a 3rd-party BI dataset due to a masking policy misconfiguration in the data pipeline. Design a CAPA program to detect, document, and prevent recurrence. Your tasks: 1) propose a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) tenant-scoped metrics for containment and recurrence, 5) an RCA template and a canary-based remediation plan to validate before rollout?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Oracle","PayPal","Tesla"]},{"id":"q-1528","question":"Scenario: A vendor data feed for pricing and product metadata experiences occasional delays and duplicates during a quarterly refresh, causing price mismatches in two regions. Design a beginner CAPA program to address this. Your tasks: 1) propose a CAPA data model capturing evidence and artifacts, 2) define a lifecycle state machine, 3) outline a minimal REST API to create/update CAPAs with linked feed records, 4) specify region- and tenant-aware metrics to prove containment, 5) provide an RCA template and a canary-based preventive action plan to validate before broader rollout?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Adobe","Coinbase","Microsoft"]},{"id":"q-1556","question":"Two regions saw bids inflated after a cache invalidation caused an expired pricing model to be applied in an ad-bidding pipeline. Design a CAPA program to detect, document, and prevent recurrence. Your task: 1) CAPA data model for evidence and artifacts, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with logs/traces, 4) region-asset metrics for containment (recurrence rate, MTTR, false positives), 5) RCA template and a canary cache-invalidation plan before rollout?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Discord","DoorDash","LinkedIn"]},{"id":"q-1635","question":"A distributed cache layer across two regions intermittently serves stale reads after a deployment; design a beginner CAPA program to detect, document, and prevent recurrence?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Microsoft","MongoDB"]},{"id":"q-1745","question":"Scenario: In a multi-tenant payments platform serving PayPal, Lyft, and Coinbase, a policy change to transaction masking and logging fails to propagate to streaming and analytics pipelines in two regions, raising privacy risk and audit gaps. Design a CAPA program to address this: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces and policy versions, 4) region-aware metrics to prove containment (recurrence rate, MTTR, false positives), 5) an RCA template and a canary rollback plan for policy changes?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Coinbase","Lyft","PayPal"]},{"id":"q-1848","question":"Design a CAPA for drift in an ML fraud detector across regions: 1) CAPA data model with driftIndex, affectedFeatures, modelVersion, evidence and artifact links; 2) lifecycle: detected → investigating → containment → RCA → closed; 3) minimal REST API to create/update CAPAs with logs and model snapshots; 4) region metrics: drift magnitude, MTTR, FPR/TPR changes; 5) RCA and canary rollback plan?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Cloudflare","Netflix","Uber"]},{"id":"q-1899","question":"Scenario: A real-time analytics service relies on a 3rd-party enrichment API. Under peak load, enrichment latency spikes cause backpressure and data loss in two regions. Design a beginner CAPA program to address this. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with evidence, 4) region-aware metrics to prove containment, 5) an RCA template and a canary-based fallback plan?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Meta","Netflix","Twitter"]},{"id":"q-1952","question":"Scenario: A multi-tenant data platform powering market data feeds experiences cross-tenant data leakage under peak load. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model capturing tenantId, policyId, incidentId, evidence and artifact links; 2) a lifecycle state machine; 3) a minimal REST API to create/update CAPAs with tenant-scoped logs and traces; 4) tenant-aware metrics (recurrence, MTTR, spillover rate); 5) RCA template and a canary policy rollout plan?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Meta","MongoDB","Robinhood"]},{"id":"q-2132","question":"Scenario: A multi-region real-time bidding system processes ad events via Kafka across us-east-1 and eu-west-1. A recently deployed bid normalization microservice causes timeouts and mispricing, leading to revenue variance and increased false positives in two regions. Design a CAPA program that covers: 1) a CAPA data model capturing evidence, artifacts, and cross-region traces; 2) a lifecycle state machine for CAPA progression; 3) a minimal REST API to create/update CAPAs with linked logs/traces; 4) region-aware metrics to prove containment (recurrence rate, MTTR, revenue delta, false-positive rate); 5) an RCA template and a canary rollout plan for the normalization service?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Amazon","Salesforce","Slack"]},{"id":"q-2174","question":"Scenario: A data-access policy violation occurs when a misconfigured feature-flag rollout temporarily allows cross-tenant data access in two regions. Design a CAPA program to detect, document, and prevent recurrence. Include: 1) a CAPA data model capturing lineage, access changes, and remediation artifacts; 2) a lifecycle state machine; 3) a minimal REST API to create/update CAPAs with linked logs/traces and policy changes; 4) region- and tenant-scoped metrics; 5) an RCA template and a canary-based remediation plan?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Microsoft","Twitter"]},{"id":"q-2228","question":"Scenario: A webhook-driven notification service intermittently loses messages in two regions after deploying a new retry-backoff policy. Design a beginner CAPA program to address this. Your task: 1) propose a CAPA data model, 2) define a lifecycle state machine, 3) outline a minimal REST API to create/update CAPAs with evidence, 4) specify practical, region-aware metrics to prove containment, 5) provide an RCA template and a canary-based rollback/test plan before full rollout?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Bloomberg","Instacart","LinkedIn"]},{"id":"q-2363","question":"In a multi-region streaming analytics platform, a surge triggers backpressure and late events in two regions. Design a CAPA program that focuses on data-plane backlogs affecting downstream attribution. Include: 1) a CAPA data model capturing evidence and artifacts, 2) a region/shard-aware lifecycle, 3) a minimal REST API to create/update CAPAs with logs/traces, 4) metrics proving containment (latency drift, backlog depth, MTTR), 5) RCA template and a canary rollout for a dynamic circuit-breaker in the ingestion path?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["LinkedIn","Snap"]},{"id":"q-2393","question":"Scenario: A cost spike in cloud spend occurs due to a runaway batch job in two regions; design a beginner CAPA that addresses this. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with evidence, 4) region-aware metrics to prove containment, 5) an RCA template and a canary-based remediation plan?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Instacart","Snowflake","Twitter"]},{"id":"q-2430","question":"Scenario: A multi-region analytics platform begins exporting customer PII to a BI partner after a schema change, risking regulatory non-compliance. Design a CAPA program to address this: 1) CAPA data model for evidence and artifacts, 2) lifecycle state machine, 3) minimal REST API to create/update CAPAs with linked logs and data-access policy checks, 4) region-aware metrics (policy-violation rate, MTTR, false positives), 5) RCA template and a canary rollout plan with automated data redaction?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Tesla","Two Sigma"]},{"id":"q-2611","question":"Scenario: A global data-analytics pipeline intermittently drops events in two regions during peak load due to a misconfigured ETL window. Design a beginner CAPA program: 1) CAPA data model, 2) lifecycle state machine, 3) a minimal REST API to create/update CAPAs with evidence, 4) region-aware metrics to prove containment and prevent recurrence, 5) RCA template and a canary-based remediation plan. Keep it implementable with simple tooling?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["LinkedIn","MongoDB","Square"]},{"id":"q-2731","question":"Scenario: A real-time telemetry pipeline logs PII after a schema update in two regions, triggering a privacy incident. Design a CAPA program to prevent recurrence: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with redacted evidence (logs/traces), 4) region-aware metrics to prove containment (PII_exposure_rate, MTTR, false_positives), 5) an RCA template and a canary rollout plan for patching the logging/path?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Discord","NVIDIA","Uber"]},{"id":"q-2753","question":"In a global platform handling encrypted customer data, a key-rotation facility misdeploy causes decrypt failures in two regions, blocking access to data. Design a CAPA program to detect, contain, and prevent recurrence. Your task: 1) propose a CAPA data model capturing evidence and artifacts, 2) define a region-aware lifecycle state machine, 3) outline a minimal REST API to create/update CAPAs with linked logs/traces, 4) specify region-aware metrics to prove containment (downtime, MTTR, data-access success rate, false positives), 5) provide an RCA template and a canary-based remediation plan for key-rotation changes?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Anthropic","NVIDIA","Netflix"]},{"id":"q-2785","question":"In a multi-tenant analytics platform, a schema evolution causes cross-tenant data leakage and inconsistent joins during peak load. Design a CAPA program that mitigates these risks: 1) a CAPA data model capturing evidence and lineage, 2) a cross-tenant lifecycle state machine, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) tenant-aware metrics (recurrence rate, MTTR, false positives), 5) an RCA template and a canary rollout plan with rollback?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["MongoDB","Snowflake","Square"]},{"id":"q-2888","question":"A multi-tenant policy engine used for feature-flag entitlements in a globally distributed SaaS shows inconsistent user permissions after a policy rollout in two regions, causing sporadic authorization errors (429/403) and revenue leakage. Design a CAPA program focused on data-feed integrity and policy-vs-data latency. Include: 1) a CAPA data model, 2) a region-aware lifecycle, 3) a minimal REST API for CAPAs with linked logs, 4) metrics to prove containment, 5) RCA template and a canary rollout plan for policy updates?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Hashicorp","NVIDIA","Uber"]},{"id":"q-2912","question":"Scenario: An ML labeling pipeline for ad quality causes bias in two regions after a data source change. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model with incident_id, timestamps, regions, data_source, bias_metric, and evidence artifacts; 2) region-aware lifecycle: detected, containment, investigation, remediation, verification, closed; 3) REST API to create/update CAPAs with logs/traces; 4) metrics: bias_delta, MTTR, FP_rate by region; 5) RCA template and a canary remediation plan?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Goldman Sachs","LinkedIn","Meta"]},{"id":"q-2962","question":"Scenario: A two-region rollout of a new ML-powered recommender model causes drift in user-item interactions and CTR drop within the first hour. Design a CAPA program to prevent recurrence. Include: 1) a CAPA data model capturing evidence and artifacts (drift stats, feature distributions, logs, and model/version with region), 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region-aware metrics (drift decay, MTTR, false positives), 5) an RCA template and a canary-based rollback plan?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Anthropic","Apple","Twitter"]},{"id":"q-3157","question":"Scenario: A cross-tenant data-sharing microservice was extended to support dynamic data scoping, and a release leaked data across two regions. Design a CAPA program to detect, triage, and prevent recurrence. Include: 1) a CAPA data model, 2) a region-aware lifecycle state machine, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region-level metrics proving containment and recurrence, 5) an RCA template and a canary rollout plan for the data-scoping change?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Anthropic","DoorDash","Meta"]},{"id":"q-3177","question":"Scenario: An ML inference service deployed globally leaks user features after a feature-store refresh and model rollback. Design a CAPA program to detect, capture evidence, contain the leak, and prevent recurrence. Include: 1) a CAPA data model capturing evidence, artifacts, and model/feature lineage, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region- and data-class metrics (containment time, false negatives, drift delta), 5) an RCA template and a canary-based remediation plan (retrain on a subset, gated rollout, rollback if KPI fail)?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Apple","Meta"]},{"id":"q-3287","question":"In a real-time feature-store pipeline, a schema evolution causes feature drift and model drift across regions, impacting revenue. Design a CAPA program with: 1) a CAPA data model capturing evidence and artifacts, 2) a region-aware lifecycle, 3) a minimal REST API to create/update CAPAs with logs/traces, 4) region metrics (driftScore, featureAge, MTTR, FP rate), 5) an RCA template and a canary rollout plan for schema changes?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Meta","Netflix","Salesforce"]},{"id":"q-3333","question":"Scenario: A real-time fraud-detection service shows a surge in false positives after a deployment across three regions. Design a CAPA program focusing on model and data drift. Include: 1) a CAPA data model capturing evidence, signals, and data lineage; 2) a lifecycle state machine for CAPA progression; 3) a minimal REST API to create/update CAPAs with logs/traces; 4) region-aware metrics (precision, recall, F1, drift_score, MTTR); 5) RCA template and a canary retraining/feature rollout plan?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Databricks","Goldman Sachs","Two Sigma"]},{"id":"q-3541","question":"In a multi-tenant data platform used by Zoom and Databricks, a sudden ingestion spike from one tenant causes backpressure that delays processing for others across regions. Design a CAPA program to detect, capture evidence, contain, and prevent recurrence. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API for CAPAs with linked logs and traces, 4) tenant-/region-aware metrics (backlog depth, MTTR, recurrence rate), 5) RCA template and a canary rollback plan for throttle-based remediation?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Databricks","Zoom"]},{"id":"q-3568","question":"Scenario: A data ingestion pipeline across two regions experiences data skew during the daily batch window, causing delayed events and stale dashboards. Design a beginner CAPA to address this. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with evidence, 4) region-aware metrics to prove containment and time-to-containment, 5) an RCA template and a canary-based remediation plan?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Cloudflare","Netflix","Tesla"]},{"id":"q-3616","question":"Scenario: After a feature-store schema evolution, a real-time fraud-detection model begins misclassifying events, causing elevated false positives in two regions. Design a CAPA program to address this: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region-aware metrics to prove effectiveness (precision, recall drift, MTTR, false-positive rate), 5) an RCA template and a canary rollout plan for feature window validation and model rollback?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Anthropic","Databricks","Instacart"]},{"id":"q-3687","question":"In a multi-region Databricks lakehouse, a misconfigured data-classification rule briefly exposes PII in two regions. Design a beginner CAPA to prevent recurrence. Include: 1) a CAPA data model to capture evidence, root cause, corrective and preventive actions, 2) a simple lifecycle state machine, 3) a minimal REST API to create/update CAPAs with linked logs/evidence, 4) region-aware containment metrics, 5) an RCA template and a canary-based remediation plan before full rollout?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Databricks","OpenAI"]},{"id":"q-3780","question":"Scenario: An identity service misrouting leads to token leakage under load, exposing user data in two regions. Design a CAPA program that addresses security risk at scale. Deliverables: 1) a CAPA data model capturing evidence, tenants, and config changes, 2) a region-aware lifecycle state machine, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) metrics for containment (leakage_rate, MTTR, recurrence, false_positives), 5) an RCA template and a canary rollout plan to validate remediation before global rollout, including key-rotation strategy?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Microsoft","OpenAI","Twitter"]},{"id":"q-3822","question":"Scenario: A multi-tenant AI model hosting platform uncovers cross-tenant data leakage after a deployment that altered data partitioning. Design a CAPA program to detect, capture evidence, and prevent recurrence. Include: 1) a CAPA data model for evidence, lineage, and remediation; 2) a tenant-aware lifecycle; 3) a minimal REST API to create/update CAPAs with linked logs; 4) tenant-scoped metrics (leak rate, time-to-detection, MTTR); 5) RCA template and canary rollback plan targeting partitioning code?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["IBM","OpenAI"]},{"id":"q-3869","question":"Scenario: In a multi-tenant data platform used by enterprise customers, an ingestion job drift triggers potential exposure of PII across two regions due to a schema evolution. Design a CAPA program focused on privacy and regulatory compliance: 1) a CAPA data model capturing privacy artifacts (PII fields, masking rules, data lineage, access controls), 2) a region-aware lifecycle state machine (discovery, containment, remediation, verification, closure), 3) a minimal REST API to create/update CAPAs with linked logs and lineage, 4) privacy metrics (time-to-containment for sensitive data drift, PIIs discovered, false positives), 5) an RCA template and a canary plan to validate masking and access controls before global rollout?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Salesforce","Snowflake"]},{"id":"q-3952","question":"Scenario: A globally deployed fraud-detection ML model starts showing elevated false positives in two regions after a retrain. Design a CAPA program to detect model drift, contain impact, and prevent recurrence. Include: 1) a CAPA data model capturing evidence (drift metrics, feature distributions, FP rate by region, logs, dataset snapshots) and artifacts (model version, data lineage), 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked traces, 4) region-aware metrics proving containment (drift score, FP rate, MTTR), 5) RCA template and a canary-based remediation plan (retrain with backfill, rollback, feature-store checks)?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Airbnb","Hashicorp","MongoDB"]},{"id":"q-4036","question":"Scenario: A cross-region ML content-safety classifier drift after a rollout causes increased unsafe-content passes in Region A while Region B remains fine. Design a CAPA program to detect, collect evidence, and prevent recurrence. Include: 1) a CAPA data model; 2) a region-aware lifecycle; 3) a minimal REST API to create/update CAPAs with evidence; 4) region-based metrics to prove containment; 5) RCA template and a canary/rollback plan?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Cloudflare","Hugging Face","Meta"]},{"id":"q-4140","question":"Scenario: A multi-region EV telemetry pipeline relies on a third‑party enrichment service. An outage causes inconsistent vehicle trip durations and location matching in Regions North and East. Design a CAPA program that covers: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for cross‑vendor CAPAs, 3) a minimal REST API to create/update CAPAs with logs/traces and vendor links, 4) region-aware metrics to prove containment (recurrence rate, MTTR, false positive rate), 5) an RCA template and a canary rollback plan for vendor changes?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Google","Tesla"]},{"id":"q-4187","question":"Scenario: A dynamic ride-pricing model experiences unexplained underpricing after a feature-store schema evolution, leading to regional revenue drop. Design a CAPA program to detect drift, collect evidence (model version, feature lineage, data distributions, logs), and prevent recurrence. Include: 1) a CAPA data model, 2) a region-aware lifecycle, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) drift-aware metrics to prove containment (drift rate, MTTR, FP rate), 5) an RCA template and a canary remediation plan (rollback to prior feature/version, retraining)?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Google","Lyft","Uber"]},{"id":"q-4302","question":"In a real-time model serving pipeline for a high-frequency trading platform, a schema evolution in the feature store causes online scores to be computed with mismatched features across three regions, leading to incorrect risk estimates. Design a CAPA program to address this: 1) a CAPA data model capturing evidence and artifacts, 2) a region-aware lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with logs/traces, 4) region/feature-type metrics to prove containment and avoid recurrence, 5) an RCA template and a canary-based rollback plan for feature changes?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Goldman Sachs","Hugging Face","NVIDIA"]},{"id":"q-4434","question":"Scenario: In a multi-tenant data lake used by a regulated enterprise, one tenant's ingestion triggers data skew that contaminates downstream analytics while costing spikes. Design a CAPA program focusing on tenant isolation, data provenance, and cost control. Include: 1) a CAPA data model capturing evidence and artifacts, 2) a tenant-scoped lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) tenant-aware metrics to prove containment and recurrence, 5) an RCA template and a safe rollback plan for data ingestion changes. What would you implement and why?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["IBM","LinkedIn","Slack"]},{"id":"q-4526","question":"In a cross-region data-aggregation pipeline for fraud detection, a misconfigured regional join flag breaks data lineage and risks PII exposure in one region. Design a CAPA program with: 1) a CAPA data model for evidence and artifacts, 2) a cross-region lifecycle state machine, 3) a minimal REST API to create/update CAPAs with logs/traces, 4) region-specific privacy metrics (lineage completeness, PII access, MTTR), 5) an RCA template and a canary rollout plan for flag changes?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Coinbase","Meta"]},{"id":"q-4601","question":"Scenario: In a fleet of autonomous vehicles operating across multiple regions, a recently rolled OTA firmware update causes sporadic misdetections in sensor fusion, leading to unsafe behavior in Region A and Region B. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model capturing evidence, telemetry, firmware/OTA versions, and logs; 2) a region-aware lifecycle state machine for CAPA progression; 3) a minimal REST API to create/update CAPAs with linked logs/traces; 4) region- and sensor-type metrics to prove containment (recurrence rate, MTTR, false positives, drift); 5) an RCA template and a staged canary rollout plan for firmware/feature changes?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["NVIDIA","Tesla"]},{"id":"q-4722","question":"Scenario: After a deployment, a misconfigured ETL sink writes PII to a non-prod data lake in Region-2, triggering a data-governance alert. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model with evidence, lineage, and owners; 2) a region-aware lifecycle with gates; 3) a minimal REST API to create/update CAPAs with logs/traces and lineage links; 4) region metrics for containment and leakage; 5) an RCA template and a canary rollback plan for sink changes?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Goldman Sachs","Oracle","Uber"]},{"id":"q-4750","question":"Scenario: A multi-tenant streaming platform experiences sudden, per-tenant drift in sessionization in Region-2, causing incorrect churn metrics for several tenants. Design a CAPA program that addresses drift without impacting others. Include: 1) a CAPA data model capturing tenant scope, evidence, and lineage; 2) a region-tenant gated lifecycle; 3) a minimal REST API to create/update CAPAs with linked logs/traces; 4) tenant-scoped metrics to prove containment and recurrence; 5) an RCA template and a canary rollout plan for the sessionization logic updates?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Netflix","Twitter"]},{"id":"q-841","question":"Design a CAPA workflow for a high-volume platform (Airbnb/LinkedIn scale). The system must log incidents, perform RCA, implement corrective and preventive actions, and verify outcomes before closing. Provide: 1) a CAPA data model, 2) a lifecycle state machine, 3) an API surface to create/update CAPAs, 4) metrics to prove effectiveness (recurrence rate, time-to-close)?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Airbnb","LinkedIn"]},{"id":"q-937","question":"Scenario: A post-rollout incident caused latency spikes and higher error rates for a subset of regions when a new feature flag was enabled. Design a beginner-friendly CAPA to address this. Your task: 1) propose a CAPA data model, 2) define a lifecycle state machine, 3) outline a minimal REST API to create/update CAPAs with evidence, 4) specify practical metrics to prove effectiveness, 5) provide a simple RCA template and a canary-based preventive action you would test before full rollout?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["DoorDash","Twitter"]},{"id":"q-965","question":"Scenario: A multilingual moderation model update causes spikes in unsafe content in two locales. Design a beginner CAPA plan focusing on locale-scoped evidence, drift checks, and a safe rollback with feature flags. Include: 1) a CAPA data model, 2) a lifecycle machine, 3) a minimal REST API to capture CAPAs with evidence, 4) locale-specific success metrics, 5) an RCA template and a locale-specific canary plan for preview before global rollout?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Apple","Hugging Face"]},{"id":"q-986","question":"Scenario: After a schema evolution in the event ingestion pipeline, latency spikes and incorrect bids appear in two regions. Design a CAPA program with: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region-aware metrics to prove effectiveness (recurrence rate, mean time to containment, false-positive rate), 5) an RCA template and a canary rollout plan to validate before global deployment?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Citadel","Oracle","Uber"]},{"id":"q-1020","question":"You’re evaluating a beginner feature: a daily market digest in a Robinhood-like app. Do a practical cost‑benefit analysis: state assumptions, estimate data/API costs, storage and engineering time, quantify benefits (retention lift, ARPU), and compute break-even time. Provide a rough calculation and rationale?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["MongoDB","NVIDIA","Robinhood"]},{"id":"q-1052","question":"Scenario: Real-time ingestion service receives JSON events from edge devices. Guarantee per-user throughput while allowing bursts in a distributed cluster. Design and implement a practical throttling mechanism, specify data structures, atomicity (e.g., Redis Lua script), failure modes, testing strategy, and observability?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["NVIDIA","OpenAI","Tesla"]},{"id":"q-1057","question":"In a real-time feed system using a contextual bandit with attention weighting (CBA), design a policy that balances short-term CTR and long-term engagement. Explain your reward decomposition, exploration strategy, and handling of non-stationarity. How would you validate offline with CPE and ramp online safely? Provide a concise update rule?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Hugging Face","Microsoft","Uber"]},{"id":"q-1084","question":"Given a large social network planning to adopt a real-time feature flag evaluation service that runs on a hybrid stream/batch pipeline. Current pipeline: 1.2M events/sec, median latency 50 ms, 5% tail. New service promises 20–30% latency reduction and 25% cost increase, plus migration risk. Perform a cost-benefit analysis: quantify costs, benefits, risks, horizon (12 months), and decision rule with sensitivity ranges?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["LinkedIn","Meta","Snowflake"]},{"id":"q-1114","question":"You’re migrating a real-time feature flag engine (multi-tenant SaaS) to reduce tail latency and cost. Propose a three-phase migration: centralized eval, regional edge gateway, then hybrid routing with per-tenant caches. Specify success metrics, rollback criteria, and a 12-month plan?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["DoorDash","Hashicorp","Two Sigma"]},{"id":"q-1151","question":"In a real-time analytics system for engagement on a large social app, design a privacy-preserving cohort analytics pipeline that ingests ~3M events/sec with sub-100 ms latency per region. Requirements: data residency, differential privacy for cohort counts, delta- or exact-once streaming state, drift detection, and cost-conscious multi-region deployment. Outline architecture, data contracts, and trade-offs?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Coinbase","Meta","Snap"]},{"id":"q-1209","question":"You’re building an offline-first version of a daily digest app for low-connectivity users. Outline a minimal data model, eviction policy, and a practical plan to quantify cost savings from reduced network usage versus increased storage and complexity; provide a concrete example with rough numbers?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Adobe","DoorDash","Goldman Sachs"]},{"id":"q-1238","question":"You're evaluating a beginner feature: a daily 'Portfolio Health Snap' panel that assigns a risk score to each user based on volatility of top holdings. Do a practical cost-benefit analysis: state assumptions, data/API costs, storage, and engineering time; quantify benefits (retention lift, ARPU) and compute break-even time. Provide rough calculations and rationale?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Adobe","Square","Two Sigma"]},{"id":"q-1274","question":"You're assessing migrating a high-volume telemetry pipeline from a centralized data warehouse to a lakehouse with streaming ingestion and on-demand materialized views. Current throughput 5M events/sec, latency 5–7 min; target latency 2–3 min, 25% cost increase. Build a 12-month cost-benefit model: incremental storage/compute, streaming infra, data egress, drift/rollback costs; specify decision rules and sensitivity ranges?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Bloomberg","Citadel","Slack"]},{"id":"q-1403","question":"Design a real-time, fault-tolerant order processing pipeline for a high-traffic ecommerce system. Partition Kafka by user_id, use transactional producers for exactly-once ingestion, and idempotent consumers keyed by event_id. Maintain a durable log, emit to a warehouse via an idempotent sink, and support replay via event sourcing and schema evolution. Include chaos and backpressure tests?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Goldman Sachs","Instacart","Square"]},{"id":"q-1472","question":"Design and implement a cost-benefit analysis module for a feature-flag rollout in a global streaming platform. Given 1000 edge regions, 1M users, latency budget 50ms, and known incremental costs, specify a data model, metrics to collect, uplift estimation, and a core function that returns whether to roll out. Include rollout policy and plan for validation via A/B tests in production?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Bloomberg","Microsoft","Netflix"]},{"id":"q-1508","question":"You're deploying a real-time personalization pipeline where a new ML model runs behind a feature flag with a 1% canary. If live drift is detected against offline benchmarks and latency deviates beyond a threshold, outline a concrete plan for rollout control, rollback, and data integrity checks that minimizes user impact while preserving auditability?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Airbnb","Apple","Hashicorp"]},{"id":"q-1531","question":"Context: A streaming feature store for real-time personalization must decide between two deployment models under GDPR/CCPA constraints: (A) cloud-region-centric with EU data replicated to a central US region for global analytics, delivering tail latency 30–70 ms; (B) EU-first on-prem data plane with a lightweight cloud cache for global lookup, targeting 50–120 ms tail latency. Provide a 12–18 month cost model including data residency compliance, data transfer, storage, compute, tooling, outage risk, and a decision rule with sensitivity ranges?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Databricks","Google","Two Sigma"]},{"id":"q-1724","question":"You operate a real-time feature store for a global e-commerce platform. Ingested events arrive from multiple regions with clock skew and occasional late arrivals. You must deliver online feature values with tail latency under 100 ms while ensuring correctness when late data retroactively updates aggregates (e.g., cart_value_last_24h). Propose the architecture, covering data versioning, watermarking strategy, exactly-once processing, per-tenant isolation, and testing approach. Include a concrete late-event example and the system’s expected behavior?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Coinbase","IBM","Meta"]},{"id":"q-1817","question":"Design and describe a scalable, fault-tolerant real-time collaboration pipeline: ingest per-document operations via a durable queue (Kafka), assign strict sequence numbers, and ensure exactly-once processing with idempotent workers. Explain data model, ordering guarantees, CRDTs vs OT, cross-region replication, failure modes, tests, and rollback strategy. How would you implement end-to-end?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Adobe","Google","Zoom"]},{"id":"q-1874","question":"You’re evaluating a beginner feature: an in-app guided onboarding widget for a CRM product. Do a practical cost-benefit analysis: state assumptions, estimate data/hosting costs, content creation time, estimate uplift in activation and paid conversion, and compute break-even time. Provide a rough calculation and rationale?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["PayPal","Salesforce"]},{"id":"q-1963","question":"How would you design a practical 1-day task to ingest daily payments from an Oracle source CSV into Snowflake, ensuring idempotent loads, proper schema mapping, and data quality checks, with a minimal Python MERGE-based load skeleton by transaction_id and a quick test plan?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Oracle","Snowflake","Square"]},{"id":"q-1989","question":"You're evaluating replacing a centralized analytics pipeline with a federated learning-based recommendation model deployed on-device across 3 regions for 5M MAU; build a 2-year cost-benefit model including capex, opex, data-transfer savings, regulatory risk penalties, and uplift in engagement/ARPU, and specify the break-even horizon and go/no-go criteria?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Google","Twitter"]},{"id":"q-2005","question":"In a 3-site edge anomaly-detection rollout for 10k sensors per site (roughly 100 GB/day), design a 2-year cost-benefit analysis for an on-device autoencoder with federated updates to a central model. Include capex for edge hardware, opex for cloud training and data transfer, and quantified benefits from reduced downtime, maintenance savings, and MTBF uplift. Provide break-even horizon and go/no-go criteria?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Amazon","PayPal","Tesla"]},{"id":"q-2057","question":"You’re evaluating a starter feature named 'Retention Policy Engine' for a Slack/HashiCorp-like chat app that auto-purges messages older than a per-channel window, with per-user holds and legal holds; design a 2-year cost-benefit model, including storage savings, indexing/compute overhead, engineering time, potential compliance penalties avoided, and a go/no-go break-even horizon?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Hashicorp","Slack"]},{"id":"q-2086","question":"You're evaluating a privacy-preserving on-device NLP summarizer that runs on user devices with federated updates to a central model via secure aggregation. Design a 2-year cost-benefit model for deploying across Android and iOS in 4 regions with ~25M MAU. Include capex for secure enclaves and on-device storage, opex for federated aggregation and governance, update bandwidth, regulatory penalties for leakage, and uplift in engagement. Provide break-even and go/no-go criteria?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Meta","MongoDB","OpenAI"]},{"id":"q-2218","question":"You're evaluating a beginner feature: an in-app, offline-first guided onboarding using a localized FAQ (5 topics) and 3 short tutorial videos to cut first-week support tickets. Build a 2-year cost-benefit model: content creation time, local storage impact per user, update/hosting costs, uplift in activation, and reduction in first-week tickets; compute break-even horizon?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Airbnb","Amazon","Plaid"]},{"id":"q-2256","question":"You're evaluating a privacy-preserving on-device recommender for a mobile wallet app to surface merchant offers. Use federated learning with differential privacy across 3 regions and 5M MAU. Build a 2-year cost-benefit: edge hardware/storage, server aggregation, data transfer, content updates, regulatory penalties, activation uplift, ARPU uplift, and support costs. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Apple","OpenAI","Stripe"]},{"id":"q-2429","question":"You're deploying an on-device, privacy-preserving content moderation assistant for real-time group chats at Microsoft/Discord scale. It runs a distilled transformer on user devices to flag policy-violating messages, with periodic federated updates to keep the model aligned. Build a 2-year cost-benefit model: device storage and energy, on-device latency, update bandwidth, cloud aggregation costs, regulatory risk penalties, and expected safety/retention uplift. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Discord","Microsoft"]},{"id":"q-2690","question":"You’re evaluating a new Data Quality as a Service (DQaaS) layer for a lakehouse used by Snowflake, Uber, and Databricks. The plan is to run continuous profiling, schema drift detection, and automated remediation across 4 domains (orders, users, payments, events) with 3 prod regions. Build a 12–18 month cost-benefit model detailing profiling compute, metadata storage, alerting, remediation actions, and data egress; include break-even horizon, go/no-go criteria, and 3–5 concrete quality rules with thresholds?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Databricks","Snowflake","Uber"]},{"id":"q-2717","question":"You're evaluating moving from a centralized real-time alerting system to edge-first, on-device anomaly detection for fraud in a mobile shopping app. Three regions (US, EU, APAC) total 5M MAU. Build a 2-year cost-benefit model including device storage, on-device inference costs, data transfer, cloud orchestrator costs, regulatory penalties, and uplift in fraud detection and retention. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Google","Instacart"]},{"id":"q-2735","question":"You’re evaluating a global fraud-detection feature for a ride-hailing app. Compare a hybrid deployment: (A) on-device anomaly detectors with federated updates across regions, (B) regional edge gateways with centralized analytics, under data residency constraints. Build a 2-year cost-benefit model including training, device energy, data transfer, hosting, false positives costs, and regulatory penalties. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Cloudflare","DoorDash","Slack"]},{"id":"q-2779","question":"Assess migrating a centralized image moderation pipeline to on-device edge moderation across 4 regions handling 2M requests/day. Build a 2-year cost-benefit analysis including edge hardware amortization, on-device model updates, quarterly bandwidth for diffs, cloud moderation backend costs, data-transfer savings, regulatory penalties, and accuracy/false-positive gains affecting retention. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Adobe","Cloudflare","Google"]},{"id":"q-2806","question":"You're evaluating a beginner feature: on-logger data masking in analytics to redact PII before BI ingestion. Create a 12-month CBA: dev time (2 engineers for 3 weeks), hosting/storage overhead, tooling costs, privacy penalties avoided, uplift in dashboard adoption, and incident reductions; compute break-even and go/no-go criteria?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Databricks","PayPal"]},{"id":"q-2922","question":"You're evaluating a privacy-preserving fraud-detection feature for a mobile payments app. Compare (A) on-device risk scoring with secure enclaves and periodic federated updates vs (B) centralized scoring with differential privacy. Build a 2-year cost-benefit model including hardware/edge costs, data transfer, maintenance, latency, FP/FN costs, uplift in fraud detection and activation, regulatory penalties avoided, break-even horizon, and go/no-go criteria with mitigations?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Bloomberg","Coinbase","Meta"]},{"id":"q-2936","question":"You're evaluating a policy-driven, offline-first config-management engine for a distributed platform (like Consul/Terraform). The feature enables regional policy modules (JSON/YAML) to be evaluated locally on agents, with periodic delta updates from a central policy service when connectivity exists, and automatic conflict resolution when policies conflict. Build a 2-year cost-benefit model including authoring time, runtime overhead, update costs, data-transfer, compliance penalties avoided, uplift in deployment velocity, and break-even horizon; specify go/no-go criteria and edge-case mitigations?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Hashicorp","IBM"]},{"id":"q-2990","question":"You’re evaluating a beginner feature: a local-first search index for a mobile knowledge-base app used by field workers. The index caches documents and metadata on-device and syncs updates when online. Build a 12-month CBA: index size per user, CPU/memory impact, delta-sync costs, content update cadence, and projected uplift in offline task completion. Include go/no-go criteria and risk mitigations?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Microsoft","Uber"]},{"id":"q-3182","question":"You're evaluating a beginner feature: a client-side, offline-capable, autosave-enabled note editor for a CRM. Design and implement a debounced autosave to localStorage, restore on load, and handle cross-tab edits with a versioning strategy. Include a minimal code sketch and validation steps?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Bloomberg","Goldman Sachs","Snap"]},{"id":"q-3210","question":"You're evaluating migrating a centralized discounting and pricing engine for a marketplace app to a peer-to-peer edge-cached policy, with on-device decision rules and occasional server fallbacks due to latency and data locality. Build a 2-year cost-benefit model including compute, storage, data transfer, policy updates, fairness penalties, uplift in conversions, and risk of price wars; define break-even horizon and go/no-go criteria, plus fallback strategies?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Amazon","Meta","Uber"]},{"id":"q-3263","question":"You’re evaluating a beginner feature: a privacy-preserving onboarding chatbot that operates offline with a local FAQ and optional server calls for telemetry only when the user opts in. Build a 12‑month cost-benefit model: content creation, on-device storage impact, minimal server costs, regulatory/privacy overhead, expected uplift in activation and paid conversions, and go/no-go criteria?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Google","IBM","Zoom"]},{"id":"q-3401","question":"You're evaluating privacy-preserving mobile content moderation for a live video app: implement on-device real-time classification with weekly federated updates to improve policy rules. Build a 2-year cost-benefit model capturing on-device compute, memory, energy, uplink costs, server aggregation, DP noise, regulatory penalties avoided, and uplift in safe engagement; define break-even horizon and go/no-go criteria, including failure modes and mitigations?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Meta","NVIDIA","Netflix"]},{"id":"q-3477","question":"You're evaluating migrating a centralized on-call search feature for financial content to an edge-based offline index with federated updates across 3 regions. Build a 2-year cost-benefit model including edge hardware/storage per user, content update cadence, bandwidth savings, latency improvements, regulatory penalties, and user experience uplift; define break-even horizon and go/no-go criteria; include failure modes and mitigations?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["LinkedIn","Robinhood"]},{"id":"q-3551","question":"You're evaluating a beginner feature: an adaptive animation and UI fidelity controller that reduces effects on devices with low battery or CPU to improve perceived responsiveness across iOS and Android. Build a 12-month CBA: estimate detection overhead, telemetry/data costs, maintenance, expected uplift in retention and satisfaction, and risk of degraded UX; provide go/no-go criteria and mitigations?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Adobe","Apple"]},{"id":"q-3646","question":"You're evaluating a privacy-preserving on-device dispatcher optimization feature for a rideshare platform that runs per-driver route-choice inference inside secure enclaves, with federated model updates across 3 regions; build a 2-year cost-benefit analysis including edge hardware, secure enclave licensing, data transfer, key management, latency impact on dispatch, regulatory penalties, and uplift in on-time arrivals and rider satisfaction. Define break-even horizon and go/no-go criteria?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Instacart","Uber"]},{"id":"q-3837","question":"You're evaluating adding a real-time data quality monitoring microservice to a multi-region Lakehouse (Databricks-based) that tracks completeness, schema drift, and anomaly detection for streaming events (billions of events per day). Build a 2-year cost-benefit model including ingestion/processing costs, metadata storage, governance penalties avoided, uplift in downstream analytics accuracy, and operational overhead. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Databricks","Lyft","Netflix"]},{"id":"q-3907","question":"You're evaluating privacy-preserving on-device demand forecasting for a grocery-delivery platform. Compare a federated-learning with differential privacy setup across 4 regions (4M MAU) to a centralized cloud forecast. Build a 2-year cost-benefit model including edge hardware amortization, DP-noise overhead, orchestration and bandwidth, content updates, and regulatory penalties, plus uplift in SLA adherence and GMV. Provide break-even horizon and go/no-go criteria; specify failure modes and mitigations?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["DoorDash","Instacart","Tesla"]},{"id":"q-3985","question":"Design a privacy-preserving cross-border attribution system for a delivery/booking app that measures campaign impact across 4 regions without sharing raw user signals. Use on-device buffering, secure aggregation, and periodic federated updates; enforce data residency and differential privacy on aggregates. Provide a 2-year cost model, break-even, and go/no-go criteria; include failure modes and mitigations?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Airbnb","DoorDash"]},{"id":"q-4012","question":"You're evaluating a beginner feature: an offline-first stock widget for a mobile checkout app used by field reps. The widget caches product data locally, syncs when online, and shows real-time stock/ETA. Build a 12-month CBA: per-SKU cache size, total SKUs cached, device storage constraints, delta-sync costs, ongoing maintenance, and projected uplifts in conversion and reductions in stock-out support tickets; specify break-even horizon and go/no-go criteria?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Adobe","Scale Ai","Square"]},{"id":"q-4054","question":"Evaluate a feature: an offline-first, on-device search index for a team messaging app (Slack-like) that indexes messages locally and syncs diffs when online. Build a 18–24 month CBA covering per-device storage, CPU cost, delta-sync traffic, on-device encryption overhead, regulatory/privacy risks, expected uplift in retention/usage, and a go/no-go criteria with break-even horizon?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Meta","Slack","Snowflake"]},{"id":"q-4097","question":"You're evaluating a privacy-preserving on-device cohort analytics engine that uses secure aggregation across regions to generate user-segment insights without exposing raw data. Build a 2-year CBA: include per-device compute/memory, MPC costs, data transfer, content updates, regulatory penalties, uplift in decision quality, and support costs. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Meta","Oracle"]},{"id":"q-4322","question":"You're evaluating a cost-aware, tiered real-time event processing pipeline for a SaaS analytics product that ingests 100M events per day across 3 regions. Propose a 2-year CBA for implementing a tiered processing model (gold/silver/bronze) with per-tenant cost attribution, dynamic sampling for bronze, and regulatory data-retention controls. Include CAPEX, OPEX, data-transfer costs, penalties, and expected uplift in retention and ARPU; specify break-even horizon and go/no-go criteria?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Microsoft","Netflix","Salesforce"]},{"id":"q-4332","question":"You're adding a lightweight, on-device offline translation feature to a mobile customer-support chat app used by field technicians. It caches phrase bundles for 20 languages, uses a small neural MT model, and syncs updates when online. Build a 12-month CBA: storage per language, total cache size, on-device latency, delta-update costs, maintenance, and projected uplift in first-contact resolution. Include go/no-go criteria?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Amazon","DoorDash","IBM"]},{"id":"q-4672","question":"You're evaluating a beginner feature: an offline-first customer support chat assistant that caches last 60 days of chat transcripts locally and suggests canned responses with NLP on-device. Build a 12-month CBA: per-user storage, CPU overhead, delta-sync costs, model update cadence, privacy/opt-in costs, and go/no-go criteria?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Airbnb","Square"]},{"id":"q-4693","question":"You're evaluating migrating a centralized analytics stack to a data mesh with domain-owned data products across 4 regions. Build a 2-year CBA for Kafka streaming, Delta Lake, and dbt-based pipelines; consider domain/platform costs, data duplication, governance, licensing/infra, regulatory penalties, uplift in decision velocity/quality, and MTTR. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Coinbase","Uber"]},{"id":"q-4743","question":"You're evaluating an edge-first fraud-detection feature that runs entirely on mobile devices, flagging suspicious activity in real time without sending raw data to servers. Build an 18-month CBA: model size, on-device CPU and energy impact, incremental telemetry/storage, false-positive rate targets, regulatory/compliance costs, and expected business impact (revenue protection, user trust). Include go/no-go criteria and break-even horizon?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Amazon","IBM","Uber"]},{"id":"q-4837","question":"You're evaluating a privacy-preserving on-device content moderation model for a real-time chat feature across iOS, Android, and web. Build a 24-month CBA: model size, on-device CPU cycles, energy impact, delta telemetry/storage, regulatory/compliance costs, and expected business impact (reputation, reduced moderation incidents, uplift in trusted usage); include go/no-go criteria and break-even horizon?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Discord","Microsoft","Twitter"]},{"id":"q-843","question":"Given a CSV file with columns user_id, action, timestamp, write a Python function using only the standard library that returns the most recent action per user by deduplicating on user_id and keeping the latest timestamp; describe time complexity and edge cases?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["OpenAI","PayPal","Twitter"]},{"id":"q-872","question":"You are evaluating two deployment options for a new AI inference service under budget and latency constraints. Outline a practical, end-to-end cost-benefit analysis framework to decide which to deploy in production. Include: data you would collect (traffic, latency, SLA penalties, accuracy), metrics (NPV, ROI, payback), horizons, discount rate, handling uncertainty (scenarios), and a concrete calculation workflow you would run?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Meta","Scale Ai"]},{"id":"q-1010","question":"You're building a GPU-accelerated graph analytics pipeline that streams 1e9 edges. Design a cache coherence protocol (CCA) to keep per-vertex state consistent across 8 GPUs via a central directory. Choose directory vs snooping, invalidation vs update, and granularity. Describe data layout, coherence transitions, and a minimal update protocol with atomic operations; include trade-offs and performance tips?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Amazon","LinkedIn","NVIDIA"]},{"id":"q-1102","question":"You’re building a real-time 'cca' analytics service ingesting 10k events/sec from multiple services; it must provide low latency, deduplicate, and support backfill. Describe the architecture, data model, and exactly-once strategy, including how you’d implement dedup, transactional writes, and testing under node failures. What trade-offs do you consider?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Lyft","Netflix"]},{"id":"q-1106","question":"Design an end-to-end CDC pipeline that ingests change events from Salesforce and MongoDB and publishes to downstream consumers with at-least-once delivery. Explain your transport choice, deduplication, ordering across partitions, schema evolution, and strategies for backfills, replay, and rollbacks. Include monitoring, testing, and failover plans?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["MongoDB","Salesforce"]},{"id":"q-1190","question":"You're building a real-time collaborative whiteboard for a chat/video platform at Discord/Airbnb/Netflix scale. Each of 5–10k rooms can have up to 200 concurrent editors and must stay highly available with <100 ms latency. Explain your stack decisions: transport (WebSocket vs gRPC streaming), per-room state partitioning, operation encoding, and conflict resolution (CRDT vs OT). How would you handle exactly-once delivery and failure recovery?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Airbnb","Discord","Netflix"]},{"id":"q-1211","question":"In a multi-region service, each region maintains a local L1 cache and a shared L2 cache. How would you implement a robust cache coherence protocol to prevent stale reads while keeping latency low during write-heavy workloads? Include data paths, an invalidation strategy (push vs TTL), race-condition handling, and testing approaches?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Bloomberg","Microsoft"]},{"id":"q-1260","question":"You're building a real-time cca analytics service that ingests 20k-50k events/sec from multiple microservices and external partners. It must deliver per-user engagement scores with sub-second latency, handle out-of-order and late data, deduplicate events, and support backfill. Describe the end-to-end architecture, data model, and exactly-once strategy, including how you'd implement dedup, transactional writes, watermarking, and backfill testing under network partitions and clock skew?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Google","Uber","Zoom"]},{"id":"q-1296","question":"Design a privacy-conscious extension of a real-time cca analytics pipeline that computes per-user engagement scores across multiple geo-regions for three partner firms (Lyft, NVIDIA, Instacart). The extension must minimize PII exposure, support synthetic data feeds for testing without leaking real PII, provide auditable data events for compliance, and preserve correctness under backpressure, partition rebalancing, and clock skew. Describe architecture, data model changes, masking strategies, and how you’d validate with synthetic data?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Instacart","Lyft","NVIDIA"]},{"id":"q-1333","question":"Design a beginner-friendly data quality and observability pattern for a cca event ingestion pipeline. Ingest 1000–2000 events/sec from mobile and web sources. Specify a lightweight schema: user_id, event_type, ts, event_id. Implement at ingest: schema validation, DLQ for invalid records, per-field quality metrics, and a 60s watermark for late data. Describe implementation details and a concrete test plan with synthetic late and malformed events?","channel":"cca","subChannel":"general","difficulty":"beginner","tags":["cca"],"companies":["Apple","Google"]},{"id":"q-1344","question":"You're building a privacy-preserving, cross-tenant event ingestion and analytics service for a media analytics platform. Ingest 40k-120k events/sec from partner APIs and mobile SDKs, including PII fields. Design the end-to-end pipeline to enforce per-tenant isolation, field-level consent-based access, and auditability while preserving low-latency analytics. Include data model, masking, consent revocation handling, schema evolution, and testing strategy?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Anthropic","Meta","Snowflake"]},{"id":"q-1372","question":"Design a beginner-friendly, end-to-end pipeline to generate a daily per-user cca_score from 20-50k events/day across services. Each event has event_id, user_id, type (view, click, purchase), ts. Weights: view 0.2, click 0.5, purchase 2.0. Handle out-of-order data with a 4-hour watermark, deduplicate by event_id, and support day-level corrections (if a repair event arrives, recompute that day and upsert). Describe data schema, ETL steps, dedup strategy, and a minimal test plan?","channel":"cca","subChannel":"general","difficulty":"beginner","tags":["cca"],"companies":["NVIDIA","Stripe","Two Sigma"]},{"id":"q-1464","question":"You’re building a real-time cca scoring pipeline ingesting 30k–80k events/sec from multiple vendors and regions. A feature update changes the engagement formula and must be reproducible for historical backfills without mutating past results. Design end-to-end data lineage, feature versioning, and deterministic backfill workflows: how to version features, catalog definitions, replay with identical inputs, handle non-determinism, and test under clock skew and partitions?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Citadel","Microsoft"]},{"id":"q-1489","question":"You're building an advanced real-time cca analytics platform for a multi-tenant product used by Zoom and Microsoft. The pipeline must respect per-tenant data residency, apply on-the-fly PII masking, and support per-user consent states while still delivering sub-second per-user scores. Explain the end-to-end architecture, privacy controls, and test strategy for backfill and audit trails?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Microsoft","Zoom"]},{"id":"q-1585","question":"You're running a real-time cca analytics pipeline ingesting 20k-50k events/sec from multiple partners. Beyond latency and dedup, design a GDPR/CCPA-compliant data erasure and retention mechanism: when a user requests deletion, purge all derived scores and raw events across online stores, backfills, and audit logs within sub-second latency. Describe architecture, data model, and guarantees, plus testing under partitions and clock skew?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Netflix","Oracle","PayPal"]},{"id":"q-1607","question":"You're operating an advanced streaming cca scoring pipeline where a new feature-weighting model must be rolled out with minimal disruption. Design a canary rollout strategy that guarantees deterministic routing, comparability of scores, and safe rollback under drift or latency spikes. What data-plane changes, testing plans, and rollback criteria would you implement?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Google","Two Sigma"]},{"id":"q-1662","question":"You're operating a privacy-preserving, real-time cca scoring service for a multi-region delivery platform with strict tenant isolation. Design an architecture that guarantees per-tenant data isolation, deterministic routing for canary vs production, and safe rollback under drift or latency spikes. Include data partitioning, encryption, feature gating, testing plan, and rollback criteria?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["DoorDash","Meta"]},{"id":"q-1684","question":"You're building a production-grade per-user cca_score service that spans multiple regions and partner integrations. Design a hybrid real-time/batch pipeline to compute and refresh cca_score with model versioning, ensuring region-local data processing, strict tenant isolation, dedup and exactly-once semantics, and safe backfill testing under clock skew and partitions. Include data model, streaming windowing, incremental scoring, canary rollout, rollback criteria, and test plan?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["DoorDash","Twitter","Uber"]},{"id":"q-1717","question":"You're building a production-grade real-time cca scoring service with tenant data residency rules. Some tenants require EU-only storage and compute, others are global. Design an architecture that (a) deterministically routes requests by tenant and user, (b) version-controls per-tenant models, and (c) supports zero-downtime hot-swapping with safe rollback under drift. Include data model, isolation, testing, and rollback criteria?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Instacart","Lyft","Twitter"]},{"id":"q-1769","question":"You're building a region-scoped, multi-tenant cca_score streaming system with regional data residency guarantees for each tenant. The pipeline ingests 15k–60k events/sec from partner feeds, computes per-tenant cca_scores with model_versioning, and must support cross-region dedup, exactly-once semantics, and safe backfill during partitions. Describe end-to-end architecture, data model, and deployment strategy to meet residency, isolation, and rollback guarantees?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Meta","Plaid"]},{"id":"q-1991","question":"Design a privacy- and compliance-focused real-time cca_score pipeline for a multi-tenant enterprise. The system must enforce per-tenant data isolation, immutable audit logs, and an on-demand privacy mode that halts ingestion and model updates for regulatory checks. Describe architecture, data lineage, deletion policies (Right-to-Deletion), drift detection, and rollback criteria under partitions and clock skew. How would you implement this?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Anthropic","Instacart","Microsoft"]},{"id":"q-2062","question":"You're building a beginner-friendly per-user cca_score API for a mobile app used worldwide. Design a minimal, region-aware scoring service with a 2-feature linear model, cache results in Redis (TTL 15m), and deduplicate requests via a request_id. Use Postgres for user features, gate features by region to meet privacy rules, and keep writes idempotent. Provide endpoints, data schemas, and a simple test plan with canary rollout, clock skew, and backfill considerations?","channel":"cca","subChannel":"general","difficulty":"beginner","tags":["cca"],"companies":["Apple","IBM","NVIDIA"]},{"id":"q-2267","question":"You're adding beginner-friendly observability to a cca_score microservice used by multiple regions. Design a minimal OpenTelemetry tracing plan that captures per-request latency, propagates trace context across API gateway, auth, feature-store fetch, scoring, and response. Show sample spans and how you'd link traces to centralized logs and metrics. Propose Grafana dashboards and alert thresholds for regional latency spikes. Include a reproducible test to verify trace propagation and a region-failover scenario?","channel":"cca","subChannel":"general","difficulty":"beginner","tags":["cca"],"companies":["Amazon","Google","Hashicorp"]},{"id":"q-2390","question":"You're designing a multi-tenant cca_score service with SLA-aware routing. Some tenants require sub-200 ms online scoring; others tolerate batch refresh. Design a dual-path architecture: hot path with per-tenant in-memory cache for latency-critical tenants, and a cold path for others. Include routing rules, data model, cache key schema, backpressure, degraded scoring modes, and a testing plan?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Apple","Hugging Face","Salesforce"]},{"id":"q-2413","question":"You're building a privacy-preserving, multi-tenant cca_score service with partners and regulatory constraints. Detail a design that enforces strict tenant isolation, per-tenant key management for features and models, encrypted data in transit, and explainability traces that redact PII. Include data flow, threat model, testing (simulated breaches, leakage tests), and rollback criteria?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Goldman Sachs","Meta"]},{"id":"q-2455","question":"You’re building a privacy-preserving, real-time cca scoring service that runs in TEEs across multiple regions. Tenant data is encrypted in transit and at rest, and tenants supply per-tenant feature pipelines and keys. Design deterministic tenant routing, per-tenant model/versioning, TEEs attestation and key management, and a testing strategy to prove no data leakage, handle drift, and rollback safely under partitions?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Cloudflare","Microsoft","Netflix"]},{"id":"q-2599","question":"You're rolling out a region-aware cca scoring model that weights features by user segment. Design a deterministic, hash-based routing canary with region-level rollout, ensuring score comparability and fast rollback on drift or latency spikes. Include versioning, testing plan, metrics, and rollback criteria?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Snap","Tesla"]},{"id":"q-2650","question":"Design a privacy-preserving, multi-tenant cca scoring pipeline that evaluates per-user scores inside confidential compute enclaves. Features and raw data must remain encrypted at rest and in transit. Describe data flows, tenant isolation, key management, audit trails, and methods for drift detection and rollback if enclave compromise is suspected. Include concrete components (KMS, envelope encryption, SGX/TEE, HSM), testing plan, and deployment strategy?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Airbnb","DoorDash"]},{"id":"q-2671","question":"You're operating a real-time cca scoring service with per-tenant model_versioning and drift-control. Design an automated drift-detection and canary rollout framework that (a) compares live feature distributions against a stable reference per tenant, (b) promotes per-tenant canaries when drift crosses a threshold, (c) enables zero-downtime rollback with defined criteria during latency spikes or model degradation. Include data path, metrics, and rollback plan?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["OpenAI","Oracle","Two Sigma"]},{"id":"q-2772","question":"You're designing a privacy-preserving cca_score service under GDPR/CCPA with cross-border data flows. Propose an architecture that guarantees data residency, minimizes cross-jurisdiction data sharing, and enables deterministic scoring for A/B tests. Include encryption, key management, access controls, and compliance testing plans?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Adobe","Google","Instacart"]},{"id":"q-2988","question":"You're integrating a privacy-preserving cca scoring system across multi-tenant apps (Discord-like and Lyft-like). Design an end-to-end pipeline that enforces per-tenant data access controls, differential privacy budgets for features, and auditable lineage. Include feature store versioning, drift detection with automated safe rollback, and a test plan for backfill and latency under partition/latency spikes. Provide concrete data models, metrics, and rollback criteria?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Discord","Lyft"]},{"id":"q-3062","question":"You're adding a new optional feature weight in a per-user cca_score service (2-feature linear model). Design a beginner-friendly, region-aware rollout using a model_version flag and an API that accepts model_version. Outline data model changes, a zero-downtime migration, and a basic test plan with canary rollout and rollback criteria?","channel":"cca","subChannel":"general","difficulty":"beginner","tags":["cca"],"companies":["Google","Hugging Face","Snowflake"]},{"id":"q-3257","question":"You're deploying a new edge-scored cca feature at a CDN edge (e.g., Cloudflare Workers) to cut mobile latency. Design a rollout plan that guarantees deterministic user routing across edge locations, preserves score consistency during model_version changes, and enforces strict tenant isolation with safe rollback. Include data-plane changes, cache invalidation strategy, testing plan, and rollback criteria?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Cloudflare","Lyft"]},{"id":"q-3366","question":"You're building a privacy-aware, multi-tenant cca_score platform where tenants enforce different data retention and PII-masking rules. Design an end-to-end pipeline that (1) applies per-tenant masking without leaking raw data, (2) preserves analytic utility under masking, (3) supports real-time scoring with strict isolation, (4) provides an auditable data lineage and rollback path, and (5) includes a robust backfill strategy. Describe architecture, data model changes, and testing approach?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Goldman Sachs","Scale Ai","Stripe"]},{"id":"q-3384","question":"You're building a multi-tenant cca scoring service with strict data isolation and per-tenant privacy budgets. Propose an architecture that enables cross-tenant feature reuse without leakage, implementing per-tenant differential privacy controls, per-feature epsilon budgeting, and real-time scoring with exactly-once semantics. Describe data model, streaming windowing, testing under partitions, and canary rollout?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Coinbase","Google","OpenAI"]},{"id":"q-3454","question":"You're adding a beginner-friendly cca_score API with strict tenant isolation and auditability. Design a security-first flow where each request includes tenant_id, user_id, and a signed token; describe how you enforce per-tenant data boundaries, implement an immutable audit log, and expose a minimal endpoint model_version. Include data schemas and a basic test plan?","channel":"cca","subChannel":"general","difficulty":"beginner","tags":["cca"],"companies":["Apple","Snowflake","Square"]},{"id":"q-3528","question":"You're adding an A/B testing layer to a beginner-friendly per-user cca_score API that uses a 2-feature linear model. The API accepts user_id, region, and an optional experiment_id. Describe how to deterministically assign users to variants, store per-experiment feature deltas, ensure consistent variant assignment per user, and design a safe canary rollout with rollback criteria. Include data models and a minimal test plan?","channel":"cca","subChannel":"general","difficulty":"beginner","tags":["cca"],"companies":["Databricks","MongoDB"]},{"id":"q-3593","question":"You're releasing an offline-first mobile app that uses a per-user cca_score API with a simple 2-feature model. Design a beginner-friendly offline sync flow for intermittent connectivity: client caches locally, applies optimistic updates, and reconciles with the server on reconnect. Outline the server data model and a minimal endpoint (POST /cca_score/sync) with fields device_id, user_id, model_version, client_revision, local_changes, server_time, and a simple conflict rule (latest revision wins). Include a basic test plan?","channel":"cca","subChannel":"general","difficulty":"beginner","tags":["cca"],"companies":["Instacart","MongoDB","Twitter"]},{"id":"q-3615","question":"Design a privacy-preserving, multi-tenant cca_score pipeline with per-tenant isolation and configurable DP noise. Provide end-to-end flow, a data model sketch, and an API spec for POST /cca_score/compute with tenant_id, user_id, features, model_version, epsilon, timestamp. Include a canary rollout plan and testing strategy for privacy guarantees, accuracy, latency, and auditability?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Square","Two Sigma","Uber"]},{"id":"q-3695","question":"You're designing a real-time per-user cca_score service for Snowflake, Uber, and MongoDB where tenants enforce data-retention policies and deletion requests must purge data across streaming and batch layers. Outline an end-to-end flow: tombstone propagation (tenant_id, user_id, before_ts, model_version), purge semantics in state stores, auditability, and rollback. Propose a minimal API for deletions, a data model sketch, and a test plan for late data and clock skew?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["MongoDB","Snowflake","Uber"]},{"id":"q-3709","question":"You're migrating a per-user cca_score service to a multi-tenant, privacy-preserving store with tenant-scoped masking and encryption. Design a concrete rollout plan: (1) data model changes (tenant_id, user_id, model_version, features, score, audit_id) with per-tenant masking, (2) API changes for model_version-aware scoring and privacy settings, (3) zero-downtime migration with canary rollout and rollback criteria, (4) testing plan including privacy validation and performance under high load. Provide concrete steps and milestones?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Anthropic","Plaid","Stripe"]},{"id":"q-3785","question":"You’re building a privacy-preserving, explainable cca_score API for a multi-tenant app where some tenants require differential privacy and strict data residency. The API must return a per-user score plus a concise explanation that does not reveal training data or model internals, while enforcing tenant isolation and model_versioning. Describe the API surface (endpoints and payloads), the data model, privacy controls (masking, differential privacy), and a testing plan including explainability fidelity and privacy risk assessment. Include a simple flow diagram and a sample canary test scenario?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Amazon","Discord","Microsoft"]},{"id":"q-3922","question":"You're adding a beginner-friendly cca_score API variant where explanations are provided through a short-lived token instead of containing training data. Design the API flow for POST /cca_score/compute with user_id, model_version, locale, device_id, and explain=true. Server returns score and an explanation_token encoding model_version, user_id, and expiry. Describe the minimal data model, token format (JWT HS256), and a basic test plan validating token issuance, expiry, and revocation?","channel":"cca","subChannel":"general","difficulty":"beginner","tags":["cca"],"companies":["Discord","Google","Plaid"]},{"id":"q-3948","question":"You're building a privacy-conscious cca_score API for a multi-tenant platform with data residency and regulatory constraints. Design a policy-driven routing layer that, per request, deterministically selects region and model_version based on tenant SLA, current load, and compliance constraints, ensuring tenant isolation and reproducible scoring. Describe the routing logic, data paths, and rollback tests?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Goldman Sachs","Lyft"]},{"id":"q-4108","question":"You're integrating a per-user cca_score API used by partner apps. Design a beginner-friendly rate-limiting and auditing layer to prevent abuse while preserving latency. Outline (1) per-tenant quotas and a simple token-bucket or fixed-window policy, (2) an API contract for POST /cca_score including fields tenant_id, user_id, model_version, timestamp, and quota_token, and (3) a minimal observable audit log and testing approach?","channel":"cca","subChannel":"general","difficulty":"beginner","tags":["cca"],"companies":["Apple","Discord","Tesla"]},{"id":"q-4156","question":"You're running a multi-tenant cca_score API with tenants having different data distributions and privacy budgets. Design a drift-detection and retraining workflow that updates models per-tenant without impacting others. Describe telemetry, metrics, data versioning, and a canary rollout plan for retraining; include endpoints and data schema changes?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Databricks","Google","OpenAI"]},{"id":"q-4306","question":"You're running a real-time per-user cca_score service across tenants with varied data residency rules. When a new model_version is deployed, design a drift-detection and observability workflow that can detect per-tenant degradation and trigger safe rollbacks. Specify: API surface to trigger drift checks and fetch per-tenant reports; data model for drift metrics; how to synthesize and test drift; and the canary rollout strategy with rollback criteria. Include a simple diagram and a sample canary test scenario?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Google","Tesla"]},{"id":"q-4358","question":"You're adding drift detection and per-tenant calibration to a multi-tenant cca_score service. Design a practical experiment framework to detect score drift across tenants, enforce per-tenant calibration, and provide explainable drift reports without exposing training data. Specify metrics (AUC, calibration, PSI), data lineage, and a dedicated API to fetch drift reports; outline a test plan for clock skew and late data?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["IBM","OpenAI","Robinhood"]},{"id":"q-4423","question":"You're adding lightweight telemetry for a beginner-friendly cca_score API to monitor health while avoiding PII. Design the telemetry contract and a minimal ingest endpoint (POST /cca_score/telemetry) with fields: anonymous_id, model_version, region, latency_ms, status_code, timestamp, sample_rate, and an obfuscated user_id field. Propose 1% sampling, retention, masking rules, and a basic test plan including privacy checks and canary rollout?","channel":"cca","subChannel":"general","difficulty":"beginner","tags":["cca"],"companies":["Adobe","Robinhood","Tesla"]},{"id":"q-4473","question":"You're building a multi-tenant cca_score service where tenants cannot share user data but want a globally calibrated score. Design an API surface and data model for a federated calibration flow (e.g., POST /cca_score/calibrate) that returns a per-tenant calibration factor and a privacy-preserving explanation. Explain how to aggregate without leaking tenant data, version calibration, and a test plan including privacy audits, drift detection, canary rollout, and rollback?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Bloomberg","Lyft","Square"]},{"id":"q-4778","question":"You're deploying an edge-first cca_score pipeline: a 2-feature model runs on-device with on-device differential privacy and signed model manifests. Describe the end-to-end data flow, data retention, and how the device fetches a signed manifest, computes the score, and posts an attestation back for verification. Include endpoints, data formats, and a testing plan for offline and tamper-resistance scenarios?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Google","Oracle","Tesla"]},{"id":"q-4810","question":"You're building a beginner-friendly cca_score API for a privacy-conscious fintech app that must comply with data residency: all tenant data stays in a single region. Design an endpoint POST /cca_score/explain that returns a per-user score, a concise explanation, and a data_hash for lineage. Explanations must not reveal training data or internals, and must be deterministic per user using a per-tenant salt. Include data models, routing strategy, and a minimal test plan?","channel":"cca","subChannel":"general","difficulty":"beginner","tags":["cca"],"companies":["Apple","Coinbase","Plaid"]},{"id":"q-4843","question":"You're migrating a high-traffic cca_score service to a multi-tenant SaaS with a shared feature store. Some tenants require strict data residency and complete cross-tenant isolation to prevent leakage via timing/side channels. Design a per-tenant feature namespace, API surface (endpoints and payloads), data model, privacy controls, and a testing plan that proves no leakage, includes differential privacy, auditing, and a rollback plan with deterministic canary checks?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Netflix","Snowflake","Two Sigma"]},{"id":"q-840","question":"In a secure messaging service, ciphertexts are decrypted by a server with a decryption oracle exposed to clients, creating a potential CCA risk. Design a practical IND-CCA secure scheme for message confidentiality using existing primitives (e.g., OAEP, AES-GCM, MACs). Explain how you prevent chosen-ciphertext attacks, outline a concrete protocol, and discuss trade-offs?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Microsoft","Oracle","Stripe"]},{"id":"q-879","question":"Describe a cross-region user preferences syncing protocol using MongoDB that tolerates regional partitions. Specify the data model (per-field version stamps), conflict resolution policy, and read/write configurations. Provide a concrete merge approach and an example conflict scenario?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Coinbase","MongoDB","Uber"]},{"id":"q-970","question":"You're shipping an E2E chat feature for an internal Meta–Microsoft product. An attacker may access a decryption oracle. Outline a concrete IND-CCA2 secure scheme for message exchange using public-key crypto, detailing padding (OAEP), a KEM/DEM split or AEAD wrapper, ephemeral keys, and how you bind metadata (timestamps, sender IDs) to prevent malleability. What are the failure modes and mitigations?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Meta","Microsoft"]},{"id":"q-1026","question":"Design a CGOA wrapper for a C library that provides two symbols: int* generate_seq(int n) which allocates an int array of length n via malloc filled with 0..n-1, and void free_seq(int* p) to free it. Provide the C header, the Go binding using CGO, and a small Go program that concurrently requests arrays of sizes 4 and 8, validates contents, and frees them. Include exact build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Bloomberg","Coinbase","Oracle"]},{"id":"q-1099","question":"Given a C library with an asynchronous API: \n\n- void run_async(const char* input, void (*cb)(int status, const char* data, void* user), void* user);\n\nwhere the callback runs on a worker thread and data is malloc-allocated or NULL on error. Design a CGO-based Go wrapper that exposes RunAsync(input string, cb func(status int, data string, err error)). The wrapper must: manage the Go callback safely across C boundaries, free C data, map non-zero status to errors, and handle thread attachment; provide header + binding + minimal test harness to demonstrate safety?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Google","LinkedIn","PayPal"]},{"id":"q-1207","question":"Design a CGO bridge for a C EventLib that exposes a function: void register_event_source(int stream_id, void (*cb)(int, const char*)); void start_event_loop(); Build a Go binding that lets two independent streams subscribe and receive events via a single exported Go callback, using a C shim to bridge into Go. Describe memory management and thread-safety; provide header, Go binding, and a small Go program demonstrating two streams; include build steps?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Hashicorp","Twitter","Two Sigma"]},{"id":"q-1235","question":"Implement a CGOA binding for an opaque C Counter handle. API: typedef struct Counter Counter; Counter* CounterNew(int); void CounterInc(Counter*, int); int CounterValue(Counter*); void CounterFree(Counter*); In Go wrap as type Counter with NewCounter, Inc, Value, Close. Show two goroutines each creating its own Counter, incrementing independently, and printing values to verify isolation and no data races. Include header, binding, and a minimal program with build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Adobe","Instacart","Tesla"]},{"id":"q-1294","question":"Implement a CGOA bridge that lets C trigger a Go callback asynchronously when an external sensor fires events. Provide a C header and stub, a CGO binding in Go that registers a Go callback and routes events through an exported Go function, and a safe cleanup mechanism to release resources when producers stop?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Anthropic","Lyft","NVIDIA"]},{"id":"q-1371","question":"Implement a CGO binding for a C function that computes sum and mean of an int array. Provide header and C source with int compute_stats(const int* values, size_t n, long long* sum, double* mean); 0 on success, -1 if n==0. In Go, implement ComputeStats(values []int) (int64, float64, error) calling the C function via CGO, converting types. Add a small main.go that runs two goroutines, each calling ComputeStats on its own slice. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Lyft","Two Sigma"]},{"id":"q-1396","question":"Implement a CGO bridge for a C event source that runs callbacks on its own threads and delivers events to Go via a callback of the form void cb(int code, const char* msg, void* ctx). Expose StartEventSource(cb, ctx) -> handle and StopEventSource(handle). Provide a C header, the Go binding using CGO (with a Go-exported callback and pointer-ownership strategy), and a small Go program that starts two sources and validates receipt of events through a single Go channel. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Instacart","PayPal","Slack"]},{"id":"q-1548","question":"Implement a CGO wrapper for a C library that registers a Go callback and invokes it from a C event loop. Provide a C header and implementation for: - void register_callback(void (*cb)(int)); - void trigger(int value); Write Go bindings using CGO to pass a Go function as the callback, ensuring safe cross-language invocation, correct Go runtime considerations, and demonstrate with two goroutines each registering and triggering events. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Databricks","Meta","NVIDIA"]},{"id":"q-1659","question":"Design a CGOA wrapper to let a Go function be registered as a callback for a C API that emits events asynchronously on external threads. The C API exposes: typedef void (*cb_t)(int eventCode, void* context); void register_cb(cb_t cb, void* ctx); void emit_event(int code); Implement header, CGO binding, and a minimal program that registers a Go callback, triggers an event, and exits safely. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Google","Scale Ai","Twitter"]},{"id":"q-1758","question":"You have a C library exposing void start_event_loop(void (*cb)(int event, void* ctx), void* ctx) and void stop_event_loop(). Implement a CGO bridge in Go that safely delivers events to per-stream Go handlers without passing Go pointers to C, supports multiple concurrent streams, and clean teardown. Provide C header, a thin C wrapper to register a per-stream callback, and a Go binding plus a small program that starts two streams and stops them?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Lyft","Netflix","Snap"]},{"id":"q-1815","question":"Design and implement a CGOA bridge for a C streaming library that emits integers via a callback. Expose a Go API StartStream() (<-chan int, func Stop()) using cgo.Handle to pass the Go-side context. Ensure thread-safe delivery of values to Go, proper cleanup with finalizers, and robust error reporting across the boundary. Provide C header, Go binding, and a minimal caller?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Cloudflare","Google","Plaid"]},{"id":"q-1845","question":"Implement a CGOA wrapper around a C function that returns a heap-allocated string: char* greet(const char* name). Provide a header with greet and void free_string(char*). Write a Go binding using CGO to call greet from multiple goroutines and free the result with free_string to avoid leaks. Include the header, a Go binding file, and a small Go program that calls greet(\"Alice\"), greet(\"Bob\"), greet(\"Carol\") concurrently and prints results. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["MongoDB","NVIDIA","Snowflake"]},{"id":"q-1910","question":"Design a CGOA exercise: Create a C header exposing void scale_and_offset(const float* input, int n, float* output, float scale, float offset); Implement a Go binding using CGO that wraps ScaleAndOffset and returns a slice of float results. Provide a minimal C implementation, a Go binding, and a short Go program that launches two goroutines, each feeding distinct input arrays to scale_and_offset concurrently? Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Bloomberg","Instacart","PayPal"]},{"id":"q-1993","question":"Design a CGO bridge for a C API that uses a progress callback and supports cancelable work. The C API exposes: typedef void (*progress_cb)(int percent, void* user); void register_progress(progress_cb cb, void* user); int do_work(int steps); The callback may be invoked on worker threads. Implement: (1) a C header that defines the callback type and registration; (2) a Go binding using CGO that exposes a Go channel-based Progress stream; (3) a small Go program that starts do_work in a goroutine and prints progress, with a context-based cancellation. Ensure memory and lifecycle safety across boundaries?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Cloudflare","Google","Stripe"]},{"id":"q-2011","question":"Implement a beginner CGO binding that exposes a C API void compute_stats(const double* data, size_t n, double* mean, double* stddev) to Go. Provide a C header, a Go binding using CGO, and a small Go program that launches 4 goroutines, each calling ComputeStats on its own data slice concurrently. Ensure memory safety, zero-copy access where possible, and that data races are avoided. Include explicit build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Airbnb","Meta","Square"]},{"id":"q-2036","question":"Your C library exposes a function int start_task(int task_id, void (*progress_cb)(int, const char*)); void stop_task(int task_id); The callback is invoked from worker threads and passes a progress value and a C string message. Design a CGO-based Go binding that allows registering a Go callback per task, safely translates C strings to Go strings, guarantees thread-safe delivery of progress events to Go, and supports cancellation via stop_task while ensuring resources are freed. Include a header, the Go binding, and a minimal Go program launching two concurrent tasks with per-task callbacks. Provide build steps?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Google","NVIDIA","Robinhood"]},{"id":"q-2185","question":"Design a CGOA binding for a C library exposing a thread-safe ring buffer with multiple producers and a single consumer. Provide a C header with:\n- typedef struct ring_buffer ring_buffer;\n- ring_buffer* rb_new(size_t capacity);\n- void rb_free(ring_buffer*);\n- int rb_push(ring_buffer*, int value);\n- int rb_pop(ring_buffer*, int* value); // 0 on success, -1 if empty\nImplement a Go binding using CGO that wraps rb_push/rb_pop, exposing a Go type RingBuffer with New, Push, Pop. Include a small demo with 4 producers and 2 consumers and build steps?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Anthropic","Apple","Discord"]},{"id":"q-2263","question":"Design a CGOA binding for a C library function that registers a callback and a context pointer. Create a Go binding using CGO that lets Go pass a Go function as the callback, supports multiple concurrent subscriptions, and is memory-safe with proper cleanup. Provide the C API, the Go binding, and a short Go snippet showing two subscriptions and callbacks concurrently?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Apple","Lyft","Netflix"]},{"id":"q-2305","question":"## Question\n\n**CGO Binding: In-Place String Reversal with Concurrency**\n\nYou have a C function:\n```c\nvoid reverse_inplace(char* s);\n```\nProvide:\n- A C header exposing the signature\n- A Go binding using CGO implementing ReverseGo(s string) string\n- A short Go program that launches two goroutines calling ReverseGo on different inputs\n- Build steps\n\nEnd with a question mark?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Netflix","Snowflake","Square"]},{"id":"q-2373","question":"Implement a CGO binding for a C API that processes an int array and reports results via a callback. The C API: typedef void (*cb_t)(int result, void* ctx); void sum_with_callback(const int* data, size_t n, cb_t cb, void* ctx); Provide a C header exposing the signature, a Go binding using CGO that offers func SumWithCallback(data []int, cb func(int)) error, and a short Go program that concurrently calls SumWithCallback from multiple goroutines. Include build steps and CGO thread-safety and memory-management considerations. End with a question mark?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["DoorDash","Hugging Face","NVIDIA"]},{"id":"q-2478","question":"Implement a CGOA binding for a C matrix-vector multiply. Provide matvec.h and a minimal matvec.c implementing void matvec(const double* A, const double* x, double* y, int rows, int cols) that computes y[i] = sum_j A[i*cols + j] * x[j]. Write a CGO Go binding exposing MatVec(A []float64, x []float64, rows, cols int) ([]float64, error). Include a small Go program that runs two goroutines, each calling MatVec with different inputs to prove concurrency safety, and build steps to compile and run?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Coinbase","NVIDIA","Scale Ai"]},{"id":"q-2757","question":"Design a CGOA wrapper for a C streaming function that reports encrypted chunks via a callback: void stream_xor(const uint8_t* input, size_t n, void (*chunk_cb)(const uint8_t* chunk, size_t len, void* user), void* user); Implement a Go binding (CGO) that exposes a function StreamXor(src []byte) (<-chan []byte, func()) which starts the stream in a goroutine, pipes chunks to a channel, and returns a cancel function to terminate and free resources. Provide a header, a minimal C implementation, the Go binding, and a small program that runs two streams concurrently?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Adobe","Hugging Face","Twitter"]},{"id":"q-2830","question":"Design a CGOA wrapper around a C asynchronous task API: C header exposes Task* start_task(const uint8_t* input, size_t n, void (*cb)(const char* status, void* user), void* user); void cancel_task(Task*); void free_task(Task*); Implement a Go binding that exposes RunTask(input []byte) (<-chan string, func()) which starts the task, streams status updates to the channel, and returns a cancellation function that frees resources. Include header, minimal C impl, Go binding, and a small Go program that runs two tasks concurrently and prints progress. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Cloudflare","Netflix","Twitter"]},{"id":"q-2839","question":"CGO callback bridging: Given a C library that invokes a callback of the form void (*event)(int id, const char* payload) from internal threads, design a CGO wrapper so Go can safely register a Go callback and receive events. Provide a C header exposing the registration, a Go binding using CGO that registers and forwards events to Go, and a minimal Go program that demonstrates two concurrent event streams. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Goldman Sachs","Netflix","Uber"]},{"id":"q-3000","question":"Implement a CGO wrapper for a C library function that performs a one-shot transform: void transform(const uint8_t* data, size_t n, uint8_t* out, size_t* out_n, int* err); The wrapper should expose a Go function Transform(src []byte) ([]byte, error). Provide a header, a minimal C implementation (e.g., XOR each byte with 0x5A, allocate out via malloc, fill out_n, set err), and a Go program that concurrently calls Transform from two goroutines and prints results. Ensure memory is freed?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Square","Tesla"]},{"id":"q-3046","question":"CGO Callback Bridge: Implement an async C API that schedules a job and reports results via a callback. Provide a C header and a Go binding using CGO so that Go passes a Go function as a callback by using an opaque handle and a C bridge wrapper; ensure thread-safety, lifecycle, and memory cleanup. How would you design and implement this?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Discord","Instacart","Uber"]},{"id":"q-3152","question":"CGO Callback Bridge: Given a C API that streams integers via a callback 'void cb(int value, void* user)' registered with 'void register_stream(cb_t cb, void* user);', implement a Go binding that lets Go supply a Go function to handle values. Provide a C header and a minimal C trampoline, and a Go API StartStream(process func(int)) (<-chan int). Show two goroutines consuming from distinct streams. How will you manage the Go callback lifetime and memory across CGO boundaries?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Adobe","Databricks","Meta"]},{"id":"q-3244","question":"Design a CGOA wrapper for a C library that returns a heap-allocated greeting string: const char* greet(const char* name, int* err); The function allocates memory for the greeting and returns NULL on error; The Go binding should expose a function GetGreeting(name string) (string, error) which calls greet, converts the C string to Go, and frees the C buffer. Demonstrate concurrency by launching two goroutines requesting greetings concurrently. Include a C header, a minimal C implementation, the Go binding, and a small program that runs in parallel?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Instacart","NVIDIA","Tesla"]},{"id":"q-3266","question":"Design a CGOA wrapper for a C library function that formats a message into a pre-allocated buffer: int format_message(const char* name, char* out, size_t out_size); It should return the number of bytes written or -1 on error with errno set to ERANGE when the buffer is too small. Expose a Go binding: func FormatMessage(name string) (string, error) that starts with 128 bytes and auto-resizes to 256, 512, then 1024 on ERANGE. Ensure concurrent calls are safe. Include header, C impl, Go binding, and a small multi-goroutine test?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Google","Lyft","OpenAI"]},{"id":"q-3347","question":"Interfacing asynchronous C callbacks with Go via CGOA. Given a C API:\n\ntypedef void (*ev_cb)(const char* event, void* user);\nvoid register_evt_source(ev_cb cb, void* user);\nvoid stop_evt_source(void);\n\nDesign a CGOA wrapper that exposes in Go:\n\nfunc RegisterEvents(h EventHandler) error\n\n- Header and minimal C that starts a thread emitting events every 100ms and stops on stop_evt_source.\n- Safe callback from C to Go (exported function), with string lifetime and memory cleanup.\n- Demonstrate two independent handlers concurrently and discuss CGO threading rules?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Amazon","Oracle","Snowflake"]},{"id":"q-3378","question":"Implement a CGOA wrapper for a C thread-safe queue API. The C API (queue.h) exposes: queue_t* queue_create(); void queue_destroy(queue_t*); int queue_push(queue_t* q, item* it); int queue_pop(queue_t* q, item** out, int timeout_ms); void queue_free_item(item*); typedef struct item { int id; const char* data; } item; Design a Go binding that provides:\n- type Item struct { ID int; Data string }\n- type Queue struct { p *C.queue_t }\n- func NewQueue() *Queue\n- func (q *Queue) Push(it Item) error\n- func (q *Queue) Pop(timeout time.Duration) (Item, error)\n\nRequirements: convert Go string to C string for pushes; allocate item for C, copy data, free data after push; on Pop, convert C item to Go Item and call queue_free_item. Ensure thread-safety across goroutines, zero memory leaks, and proper error semantics. Include a minimal C glue (wrapper header and c go glue), a Go binding file, and a small Go program that runs 4 producers and 2 consumers for 2 seconds?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Goldman Sachs","Google","MongoDB"]},{"id":"q-3494","question":"Design a CGOA wrapper for a C library that streams partial results via a callback: a process_start function takes data and a callback with signature chunk_cb(chunk, len, user, is_final), and is_final signals completion. The Go binding should expose: func ProcessData(input []byte) ([]byte, error) that aggregates all chunks into a single result. Provide a C header, a minimal C implementation, the Go binding, and a small Go program that calls ProcessData on two inputs concurrently to prove thread-safety and correct aggregation?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Coinbase","Scale Ai","Snap"]},{"id":"q-3514","question":"Design a CGOA binding for a C API that creates and frees a Book object and returns its title. Given C:\\ntypedef struct Book Book;\\nBook* create_book(const char* title);\\nconst char* book_title(Book*);\\nvoid free_book(Book*);\\nImplement a CGO Go wrapper exposing:\\n\\n type BookHandle struct { ptr *C.Book }\\n\\nfunc NewBook(title string) (*BookHandle, error)\\nfunc (b *BookHandle) Title() (string, error)\\nfunc (b *BookHandle) Close() error\\n\\nEnsure ownership is explicit (Close frees) or a finalizer, no leaks, and demonstrate two concurrent book creations and title fetches?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Meta","Oracle"]},{"id":"q-3544","question":"Design a CGOA wrapper for a C API that streams a file's bytes via a callback: void stream_file(const char* path, void (*chunk_cb)(const uint8_t*, size_t, void*), void* user). The C function streams 4KB chunks and signals completion with a zero-length chunk. Expose in Go: func ReadFileStream(path string) (<-chan []byte, func()) that streams in a goroutine and returns a cancel function. Provide a header, a minimal C impl, the Go binding, and a small program that runs two streams concurrently?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Cloudflare","Meta","Microsoft"]},{"id":"q-3666","question":"Design a CGOA wrapper for a C library exposing a multi-threaded async job API: typedef void (*job_cb)(int job_id, const unsigned char* result, size_t len, void* user); void submit_job(int job_id, const unsigned char* payload, size_t len, job_cb cb, void* user); void shutdown_jobs(void); Expose in Go a function: func SubmitJob(payload []byte) (<-chan []byte, func()) - Each call assigns a unique job_id internally; the C side calls back with the result asynchronously. The Go binding should convert the C callback into Go via a //export wrapper, dispatch into a channel, and provide a cancel function that shuts down all pending jobs safely. Ensure there are no Go pointers passed through CGO into C, handle lifetimes, and explain how to avoid data races across goroutines. Provide a minimal header and C implementation, Go binding, and a test program that submits 2-3 jobs concurrently and reads results from the channel. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Lyft","Meta"]},{"id":"q-3751","question":"CGOA Binding: Safe Go callback bridge for a C API with user context?\n\nDesign a CGOA binding for a C API that registers a callback and emits events from multiple threads. Provide a C header and minimal C implementation, plus a Go binding using CGO that exposes BindEventHandler(handler EventHandler) (unbind func(), error). The wrapper must deliver events to Go safely (via a channel), support multiple concurrent handlers, and clean up on unbind. Include a small Go program that registers two handlers and triggers events concurrently. End with a question mark?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Adobe","DoorDash"]},{"id":"q-4013","question":"CGOA Binding: Implement a Go binding for a C streaming transform API that reports progress via a callback. C API: typedef void (*progress_cb)(int percent, void* user); int stream_transform(const uint8_t* in, size_t in_n, uint8_t** out, size_t* out_n, progress_cb cb, void* user); Expose StreamTransform(src []byte) ([]byte, error) and stream progress to a Go channel via a callback, using a stable handle to pass Go data to C. Provide header, minimal C impl (reverse bytes, call cb 0..100, malloc), the Go binding, and a short program spawning two concurrent transforms. Build steps?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Apple","Bloomberg","Cloudflare"]},{"id":"q-4067","question":"Design a CGOA wrapper for a stateful C library exposing ip_init, ip_process, ip_destroy. Build Go bindings providing NewImageProcessor(config string) (*ImageProcessor, error), (p *ImageProcessor) Process(data []byte) ([]byte, error), and (p *ImageProcessor) Close(). Demonstrate two instances in parallel with different inputs to prove per-instance isolation. Include header and minimal C, Go binding, and a small program that runs two goroutines in parallel; how would you implement and verify this binding?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Amazon","Apple","Google"]},{"id":"q-4149","question":"CGO Callback Bridge: Implement a C library that exposes subscribe(void (*cb)(int event, const char* payload, void* user), void* user) and emit events asynchronously. Build a Go binding that lets Go register a Go callback via //export and exposes StartStream() (<-chan Event) that yields parsed events. Show header, minimal C wrapper, and a Go program with two goroutines consuming events concurrently. End with a question mark?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Cloudflare","Snowflake"]},{"id":"q-4262","question":"Design a CGOA wrapper for a C event system that streams events on a background thread: \n\nC API:\ntypedef void (*event_cb)(int id, const char* payload, void* user);\nvoid start_events(void);\nvoid stop_events(void);\nvoid register_event_cb(event_cb cb, void* user);\n\nRequirements:\n- In Go, expose func RegisterHandlers(hs []Handler) (cleanup func(), err error) where Handler has id int and Callback func(string)\n- Use cgo.Handle to pass Go callbacks to C safely; ensure callbacks from C invoke the correct Go handler and do not poison GC\n- Support registering multiple independent handlers concurrently; events must route to the right callback; stop_events cleans resources and prevents further callbacks\n- Provide a minimal header and a C implementation that emits events every 50ms on a worker thread\n- Provide a Go binding and a small program that registers three handlers, prints received payloads, then calls the cleanup when done\n- Discuss lifecycle and CGO threading caveats?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Airbnb","Bloomberg","Snap"]},{"id":"q-4336","question":"Design a CGOA wrapper for a C library that provides an asynchronous API using a completion callback. Expose a Go function AsyncCompute(src []byte) ([]byte, error) that can run concurrently from multiple goroutines. Provide a C header and minimal C implementation that schedules a callback on a new thread; show a Go binding using CGO and a short program launching two concurrent calls. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Anthropic","Robinhood"]},{"id":"q-4445","question":"Design a CGOA wrapper for a C streaming JSON tokenizer: void json_stream_parse(const char* data, size_t len, void (*tok_cb)(int type, const char* value, size_t value_len, void* user), void* user); Tokens types: STRING, NUMBER, TRUE, FALSE, NULL, BEGIN_OBJECT, END_OBJECT, BEGIN_ARRAY, END_ARRAY. Expose Go API: type JsonToken struct { Type TokenType; Value string }; func JsonStreamParse(src []byte) (<-chan JsonToken, func()) . Provide header, minimal C implementation, Go binding, and a test program that feeds two streams concurrently and cancels one stream mid-flight?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Cloudflare","Instacart","Netflix"]},{"id":"q-4487","question":"CGOA Binding: Implement a C API with an asynchronous progress callback, and provide a Go binding that allows Go code to supply a callback function to receive progress updates. The C API should be: typedef void (*progress_cb)(int progress, void* user); int start_task_async(progress_cb cb, void* user); The C implementation should call cb with progress every 25% and then return 0 on success. Expose a Go wrapper StartTaskWithCallback(cb func(int) error) error and demonstrate concurrent usage by starting two tasks in parallel with distinct callbacks. Provide header, C implementation, Go binding, and a minimal Go program that runs two tasks concurrently. End with a question?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Anthropic","Cloudflare"]},{"id":"q-4500","question":"CGO Callback Bridge: Design a CGOA wrapper so C can invoke a Go closure on a background thread. Provide: a C header with typedef void (*cb_t)(int, const char*); void register_cb(cb_t, void*); and a Go binding exposing RegisterCallback(fn func(int, string)) (handle, error) and Unregister(handle). Implement a registry mapping handles to Go closures, export a Go callback entry, dispatch via a channel, and demonstrate with a concurrent test?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Citadel","Twitter"]},{"id":"q-4531","question":"CGOA Binding: Given a C library API:\n\nvoid register_listener(void* ctx, void (*cb)(int event, const char* data));\nvoid unregister_listener(void* ctx);\n\nDesign a CGOA wrapper that lets Go code subscribe to events via a Go channel. Implement:\n- A C header exposing the callback type and registration functions\n- Go binding using //export to route C callbacks to Go; use cgo.Handle to map Go channels\n- A small Go program that starts two independent listeners and prints incoming events\n- Build steps?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Citadel","Microsoft","Scale Ai"]},{"id":"q-4653","question":"Design a CGOA wrapper for a C library that provides asynchronous computation via a callback: void compute_async(const uint8_t* data, size_t n, void (*cb)(const uint8_t* result, size_t m, void* user), void* user); Expose a Go API ComputeAsync(src []byte) (<-chan []byte, <-chan error) that streams results back via channel; implement memory safety, cancellation, and cross-GO thread-safety; include a header, minimal C wrapper, Go binding, and a small program that runs ComputeAsync in two goroutines concurrently?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Discord","MongoDB"]},{"id":"q-4716","question":"Wrap a C API that manages a configuration as an opaque handle. The C API provides:\n\ntypedef struct Config Config;\nConfig* config_new(const char* json);\nconst char* config_to_json(const Config* cfg);\nvoid config_free(Config* cfg);\n\nThe string returned by config_to_json is allocated with malloc and must be freed with free. Build a CGO wrapper in Go exposing:\n\ntype ConfigHandle struct { ptr *C.Config }\nfunc CreateConfig(json string) (*ConfigHandle, error)\nfunc ConfigToJSON(h *ConfigHandle) (string, error)\nfunc (h *ConfigHandle) Close() error\n\nRequirements:\n- Allocate and free memory correctly, translate C strings to Go strings, and free buffers.\n- Ensure safe usage across two goroutines by calling CreateConfig in two goroutines, then pass to ConfigToJSON in parallel, and free the handles?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Cloudflare","Google","Snowflake"]},{"id":"q-4737","question":"Design a CGOA wrapper for a C API: void get_message(char* buf, size_t* len); if buf is NULL, return required len in *len; otherwise copy up to *len-1 bytes, NUL-terminate, and set *len to actual length. Expose in Go a function GetMessage() (string, error) that performs the two-step call, converts the result to a Go string, and frees any allocated memory if needed. Provide header, minimal C, the Go binding, and a small program that launches two goroutines calling GetMessage concurrently to validate thread-safety?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Coinbase","Oracle"]},{"id":"q-842","question":"You have a Go service using cgo to wrap a C API. A C function 'char* fetch_data(int id)' returns a malloc-allocated string or NULL on error. Design a safe Go wrapper that converts the result to a Go string, ensures the C allocation is freed, handles NULL with a meaningful error, and notes CGO thread-safety considerations. What implementation would you write?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Meta","PayPal","Snowflake"]},{"id":"q-870","question":"Design a Go wrapper for a C API with malloc’d results that must be freed, exploring a new angle: ensure thread-safe, single-point ownership transfer for each call and robust error handling when the C call returns a non-zero code or NULL. API: typedef struct { int code; const char* msg; } ScanResult; ScanResult* perform_scan(const char* query); void free_scan_result(ScanResult*); Implement function: func Scan(query string) (string, error)?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Instacart","Netflix"]},{"id":"q-914","question":"Using cgo, how would you wrap a C function that allocates a string (char*) and returns it, ensuring memory is freed by Go code without leaks, and provide a minimal working example?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Instacart","Snap","Twitter"]},{"id":"q-950","question":"Implement a minimal CGOA wrapper that exposes a C function int add(int a, int b) to Go. Provide the C header, the Go binding using CGO, and a small Go program that concurrently calls Add from two goroutines to demonstrate thread safety. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Google","Snap","Twitter"]},{"id":"q-978","question":"Design a CGO bridge for a C streaming API that delivers chunks via a callback: void stream_data(int id, void (*chunk_cb)(const char* data, size_t len, void*), void* ctx). Implement a Go wrapper StreamFromC(id int) (io.Reader, error) that buffers chunks into an io.Pipe and exposes a safe reader, supporting concurrent streams and proper backpressure. Include a C header, a Go CGO binding, and a simple consumer showing two goroutines reading from the reader?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Goldman Sachs","Microsoft","NVIDIA"]},{"id":"q-987","question":"Implement a CGO binding for a C function char* greet(const char* name) that returns a newly allocated string. Provide the C header and implementation, a Go binding using CGO that wraps greet in a safe Go function returning (string, error), and a small Go program that concurrently calls the binding from multiple goroutines and frees the allocated memory. How would you handle NULL returns and memory deallocation robustly?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Google","MongoDB","Snap"]},{"id":"cissp-communication-network-1768227886993-3","question":"An organization wants to ensure the integrity and authenticity of DNS responses to prevent cache poisoning, while not necessarily encrypting DNS query payloads. Which mechanism provides this guarantee?","channel":"cissp","subChannel":"communication-network","difficulty":"intermediate","tags":["AWS","DNS","Terraform","certification-mcq","domain-weight-13"],"companies":null},{"id":"q-1097","question":"An enterprise with microservices deployed across AWS, Azure, and GCP stores API keys and credentials in Kubernetes secrets and environment variables. After an audit finds stale keys and overly broad service accounts, design a defense‑in‑depth credential management strategy: architecture changes, tooling choices (Vault, AWS Secrets Manager, KMS, Kubernetes CSI or equivalent), rotation cadence, access controls, and how you would validate effectiveness?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Apple","Discord","Meta"]},{"id":"q-1113","question":"In a multi-tenant ML feature store on AWS, a compromised notebook can access all tenants due to broad IAM policy. Propose a concrete mitigation plan to enforce tenant isolation and data protection: separate namespaces, least-privilege roles with explicit ARNs, ABAC tags (TenantID/DataClass), SCPs to block cross-tenant actions, VPC isolation with PrivateLink, per-tenant KMS keys, and centralized logging with CloudTrail, Config, and GuardDuty. Include testing steps?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Scale Ai","Twitter"]},{"id":"q-1373","question":"In a global organization with Kubernetes clusters on AWS and GCP, CI/CD pipelines currently pull secrets from plaintext files. Design a CISSP-aligned secrets management solution: vault choice (HashiCorp Vault or cloud-native), access controls, dynamic Secrets, short‑lived tokens, rotation cadence, audit logging, break-glass, and CI/CD integration with GitHub Actions/Jenkins. Include trade-offs?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Adobe","Bloomberg"]},{"id":"q-1407","question":"Scenario: A data platform uses MongoDB Atlas for operational data and Snowflake for analytics across three regions; admins use SSO, but access is excessive and logs are incomplete. Design a defense-in-depth plan that enforces least privilege, KMS-backed encryption, and robust auditing. Include concrete steps for MongoDB Atlas and Snowflake: provisioning, key management, logging, data masking for PII, and verification?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["MongoDB","Snowflake"]},{"id":"q-1480","question":"In a multinational fintech operating across Google Cloud, AWS, and on-prem environments, PCI data flows through a centralized data lake used by data scientists for analytics. Propose a data-centric security design that ensures PCI data never leaves regulated regions while enabling cross-cloud analytics. Include data tagging, tokenization or masking strategy, key management, and telemetry/audit hooks. What pattern do you choose and why, and outline concrete steps to implement in the next sprint?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Bloomberg","Google","PayPal"]},{"id":"q-1493","question":"An international company runs CI/CD pipelines in GitHub Actions that assume cross‑cloud roles across AWS and GCP. Secrets are stored in plaintext, and pipeline logs may expose credentials. Propose a CISSP-aligned remediation: pick an access model (e.g., short‑lived credentials + Just‑In‑Time access), specify token lifetimes, rotation, logging/auditing, and CI/CD integration checks. How would you validate fixes in production-like staging and what trade-offs exist?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Airbnb","Discord","Tesla"]},{"id":"q-1558","question":"Design a CISSP-aligned access-control plan for a multinational org running data science workloads across AWS and GCP. After a contractor's credentials were compromised, enforce Just-In-Time access, short‑lived tokens, and auditable trails. Specify tooling (AWS STS, Vault, GCP IAM, OPA, Cloud Audit Logs), break-glass process, and how you validate the policy before production rollout?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["LinkedIn","NVIDIA","OpenAI"]},{"id":"q-1619","question":"A multinational fintech runs microservices on AWS EKS and Azure AKS with a shared Snowflake data lake. Credential theft enables pivot via cross-cloud service accounts. Design a CISSP-aligned control set: (1) cross-cloud Just-In-Time access with tokens from AWS STS, Azure AD, and GCP IAM; (2) ABAC/RBAC policy-as-code with OPA; (3) service-mesh mTLS and micro-segmentation; (4) audit trails; (5) break-glass and staging purple-team validation. Specify acceptance criteria and evidence for production rollout?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["IBM","Meta"]},{"id":"q-1633","question":"A global retailer stores customer data in AWS S3, Google Cloud Storage, and an on-prem Hadoop cluster. Data is classified as public, internal, or restricted. Design a CISSP-aligned data-classification and access-control policy using ABAC with tags to enforce least privilege and enable auditable access across all platforms. Include example tags, roles, token lifetimes, and a minimal policy snippet?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Hashicorp","Two Sigma"]},{"id":"q-1660","question":"Scenario: A multinational runs workloads in AWS and IBM Cloud with Cloudflare Zero Trust. An insider exfiltrates data using DNS tunneling to a rogue domain. Propose a concrete, CISSP‑level detection and containment plan that preserves legitimate traffic. Include monitoring sources, detection signatures, tools, and IR steps across the three platforms?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Amazon","Cloudflare","IBM"]},{"id":"q-1704","question":"Design a CISSP-aligned zero-trust admin access model for remote production sessions across a hybrid cloud environment (AWS/Azure) with Windows/Linux endpoints. Include device posture checks, MFA, ephemeral credentials, just-in-time roles, jump-host/RDP/SSH brokering, session recording, break-glass controls, and auditable logging. Provide a concrete deployment plan and a safe red-team validation approach?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Meta","Tesla"]},{"id":"q-1798","question":"Design a CISSP-aligned admin-access governance model for a global platform with multi-cloud assets (AWS, GCP, Azure) and microservices. A contractor needs short-term admin rights across prod and staging; outline controls, tooling (IdP, Vault, OPA, Cloud Audit/Logging, Kubernetes RBAC), break-glass, and how you would validate the policy before production rollout?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Google","Slack","Tesla"]},{"id":"q-1892","question":"An organization runs production apps across AWS, on‑prem, and Azure. A compromised developer workstation exposed a long‑lived API key used by a microservice to access production data. Outline a CISSP‑aligned incident response plan: containment, eradication, recovery; specify evidence sources (CloudTrail, VPC flow logs, EDR), credential rotation (short‑lived tokens), revocation (IAM/Vault), network segmentation, and post‑incident hardening. Include a Vault runbook snippet to revoke the compromised token and issue a new one?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Microsoft","Tesla"]},{"id":"q-1904","question":"Describe a CISSP-aligned secure software supply chain plan for a new product shipped via CI/CD pipelines across AWS and GitHub Actions. Include SBOM generation/verification, code signing and binary attestation, secrets management in pipelines, and auditable logs plus incident response. Provide concrete steps and tooling?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["NVIDIA","PayPal","Scale Ai"]},{"id":"q-1980","question":"A multi-tenant delivery platform serving Microsoft and DoorDash customers processes sensitive order data across microservices in AWS. Propose a CISSP-aligned data protection plan covering data-in-transit and data-at-rest, key management (rotation, separation of duties, access controls), and auditing/incident response. Include concrete tools and steps?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["DoorDash","Microsoft"]},{"id":"q-2051","question":"Scenario: a junior developer accidentally commits a cloud service key to a public repo, exposing AWS and GCP access. Outline a CISSP-aligned, beginner-friendly containment and recovery plan: immediate revocation of exposed keys, rotate credentials, enforce MFA, apply least-privilege IAM changes, enable cross-cloud audit logs, and run a quick tabletop exercise to validate readiness. Which steps and tools would you use?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Amazon","Google","IBM"]},{"id":"q-2083","question":"In a global fintech with services on AWS and GCP plus on‑prem, a contractor's workstation is compromised and used to harvest ephemeral credentials authenticating to production apps. Propose a CISSP‑aligned containment, eradication, and recovery plan covering cross‑cloud token revocation, key rotation, Just‑In‑Time access, vault integration, break‑glass, and auditable logging. Include tool‑specific steps and a runbook snippet to revoke and reissue tokens across AWS IAM, GCP IAM, and Vault?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["LinkedIn","Robinhood","Snap"]},{"id":"q-2118","question":"In a multi-cloud dev sandbox spanning AWS, Azure, and GCP, service accounts for CI deployments have overly broad admin rights, risking secret exposure in production. Provide a CISSP-aligned, beginner-friendly containment and remediation plan: inventory affected identities, revoke excessive privileges, implement least-privilege roles per cloud, enable just-in-time access, rotate keys, enable cross-cloud audit logs, and validate with a tabletop exercise. Which steps and tools would you use?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Apple","Coinbase","Databricks"]},{"id":"q-2137","question":"Scenario: a multinational platform operates across AWS, Azure, and on‑prem storage. A CSV containing customer PII was exposed publicly due to a drift in IAM/RBAC policies and a misconfigured data catalog. Outline a CISSP‑aligned incident response playbook: detection signals, containment (isolate storage, revoke tokens), eradication (remediate drift, enforce ABAC), recovery, and post‑incident audit across clouds; include tooling, roles, and timing?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["LinkedIn","MongoDB","Tesla"]},{"id":"q-2172","question":"In a data lake spanning AWS S3, GCP Cloud Storage, and on-prem HDFS, design a CISSP-aligned, beginner-level plan to enforce least privilege for 120 analysts. Describe an RBAC/ABAC approach, just-in-time elevation, access revocation, and auditable logging, plus onboarding and withdrawal workflows and pre-deployment validation?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Apple","Meta","Two Sigma"]},{"id":"q-2209","question":"Scenario: a contractor with multi-cloud access (AWS, GCP, on‑prem Kubernetes) is compromised, and customer PII starts exfiltrating via a rogue data pipeline. Design a CISSP‑aligned containment, eradication, and recovery plan that emphasizes data-centric security, cross‑cloud audit consolidation, and Just‑In‑Time access. Include runbook steps to revoke service accounts, rotate keys, re-encrypt data, and validate containment before returning to production. What specific controls and tools would you invoke, and in what order?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Bloomberg","Lyft","MongoDB"]},{"id":"q-2476","question":"Design a CISSP-aligned, beginner-level plan to ensure data privacy and access auditing for a multi-tenant analytics dashboard used by Robinhood, Discord, and Meta. The plan must cover data minimization, ABAC vs RBAC decisions, just-in-time elevation, data tagging and lineage, auditable logging, and onboarding/offboarding workflows; include concrete steps and tooling?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Discord","Meta","Robinhood"]},{"id":"q-2532","question":"You manage a serverless data pipeline on AWS Lambda and GCP Cloud Functions that reads from S3 and GCS and writes to a data warehouse. For 120 analysts triggering jobs via an internal portal, design a CISSP-aligned, beginner-level plan to enforce least privilege using ABAC with just-in-time elevation, ephemeral credentials, and strict auditing. Include onboarding/withdrawal workflows, credential lifecycle, and pre-deployment validation?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Google","Meta","Oracle"]},{"id":"q-2561","question":"In a multi-cloud setup with CI/CD pipelines building images for AWS EKS and Google GKE, a leaked credential could enable a backdoored image. Design a CISSP-aligned, practical supply-chain plan covering: code signing and provenance, SBOM, image scanning, policy enforcement (OPA), runtime controls, ephemeral credentials for CI/CD, least-privilege IAM, cross-cloud auditing, and an incident runbook. Include concrete steps and tools?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Google","Microsoft","MongoDB"]},{"id":"q-2646","question":"In a global streaming platform distributed across AWS, GCP, and on-prem, design a CISSP-aligned CI/CD security plan to protect the software supply chain. Include SBOM enforcement, artifact signing with an HSM, provenance checks in GitOps, release access controls, and an incident playbook for a compromised signing key. Provide concrete tooling and steps?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["IBM","NVIDIA","Netflix"]},{"id":"q-2693","question":"Scenario: A multinational uses a third‑party market data feed delivered via gRPC over mutual TLS into on‑prem Kubernetes and AWS analytics services. Downstream policies are weak and vendor attestations are sparse. Design a CISSP‑aligned security plan to ensure integrity, confidentiality and availability. Specifically address: **identity and access model**, **secret management** and rotation, **logging/audit** of data events, and **vendor risk/contract** controls; include concrete steps and tooling?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Bloomberg","Salesforce"]},{"id":"q-2711","question":"Multi-cloud CI/CD compromise: a fintech platform runs services on AWS EKS, GCP GKE, and on‑prem OpenShift. A GitHub Actions runner was compromised, pushing a malicious image that escalates permissions for a service account across clouds. Provide a CISSP-aligned containment and recovery plan: isolate artifacts, rotate credentials, revoke access, enable cross‑cloud audit logs, verify SBOM and image signatures, and run a tabletop to validate runbooks. Which steps and tools would you prioritize and why?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Citadel","Square","Twitter"]},{"id":"q-3055","question":"For a real-time video collaboration platform used by enterprise customers (Snap, DoorDash, Zoom), a third‑party payments library has a critical supply-chain flaw in a transitive dependency. Provide a CISSP‑aligned strategy to detect, contain, and recover within 24 hours. Include SBOM generation (CycloneDX), trusted build provenance (SLSA), artifact signing, a rollback plan, and post‑incident reporting?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["DoorDash","Snap","Zoom"]},{"id":"q-3121","question":"In a simplified ML platform used by three partners (Hugging Face, Lyft, Uber), each partner uploads datasets and runs model experiments in isolated Kubernetes namespaces on AWS. Design a CISSP-aligned beginner-level plan to enforce least privilege and data separation: RBAC, ABAC labels, namespace quotas, network policies, ephemeral credentials, and auditable logs; include onboarding/offboarding workflows and pre-deploy validation checks?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Hugging Face","Lyft","Uber"]},{"id":"q-3174","question":"In a global SaaS with multi-cloud data lakes (AWS and Azure) and a central data science workspace, a data scientist needs ephemeral access to production datasets for a week. Design a CISSP-aligned least-privilege workflow using ABAC, Just-In-Time elevation, and automated revocation, plus data masking and auditable logs. Include lifecycle, tooling, and validation steps?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Cloudflare","LinkedIn","MongoDB"]},{"id":"q-3215","question":"In a multi-cloud SaaS used by Airbnb, Databricks, and Tesla, design a CISSP-aligned beginner plan to enforce least privilege for API access to encrypted datasets stored across AWS KMS, GCP KMS, and Azure Key Vault. Include per-tenant RBAC/ABAC, ephemeral credentials, key rotation, access revocation, and auditable logging; provide onboarding/offboarding workflows and pre-deployment validation checks?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Airbnb","Databricks","Tesla"]},{"id":"q-3241","question":"In a multi‑cloud pipeline bridging Adobe (AWS/Azure) and Citadel (GCP), a private CI artifact registry token is leaked and a malicious dependency is introduced via a pull request, impacting a critical production service. Outline a CISSP‑aligned containment and recovery plan focusing on software supply chain security, SBOM validation, code signing/attestation, dependency vetting, break‑glass, and cross‑cloud audit visibility. Which steps and tooling would you prioritize and why?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Adobe","Citadel"]},{"id":"q-3264","question":"In a CISSP‑intermediate interview, describe a runtime access plan for a data moderation service deployed across on‑prem, AWS, and Kubernetes. Moderators from multiple regions access a central portal; data access must follow ABAC with just‑in‑time elevation and ephemeral credentials. Include identity, secrets, policy‑as‑code, and an incident playbook with tooling examples?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Apple","Oracle"]},{"id":"q-3376","question":"In a cross-cloud analytics platform with AWS Lambda, GCP Cloud Functions, and Azure Functions deployed via a single CI/CD pipeline, a bug secretly injects a short-lived credential into all function environments granting access to every tenant's data lake. Design a CISSP-aligned containment and recovery plan: immediate credential revocation, least-privilege policy, multi-cloud audit logging, secret-rotation gates, and a preprod tabletop that validates runbooks. Which steps and tools would you prioritize, and why?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Cloudflare","Goldman Sachs","MongoDB"]},{"id":"q-3413","question":"In a global enterprise with a shared data lake across AWS S3 and Google Cloud Storage, governed by a central IAM and SSO, a junior analyst needs 72 hours access to a restricted dataset. Design a CISSP-aligned beginner plan to enforce least privilege using RBAC for roles, ABAC data labels (PII/financial), just-in-time elevation via cross-cloud credentials, audit logging, onboarding/offboarding, and pre-deployment checks across clouds?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Adobe","Goldman Sachs","Google"]},{"id":"q-3516","question":"Design a CISSP-aligned privacy and data governance program for a multinational social media platform handling cross-border user data. Include data mapping across jurisdictions, DPIAs, data retention, encryption (at rest/in transit), access control strategy (RBAC/ABAC with just-in-time elevation), third‑party risk management, incident response, and an audit/testing plan with concrete tooling and metrics?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["IBM","Meta"]},{"id":"q-3591","question":"In a unified ML platform spanning Databricks Unity Catalog, AWS and GCP data assets, and OpenAI endpoints, a CI pipeline leaks API keys enabling cross-cloud data exfiltration from a private data lake containing PII. Outline a CISSP-aligned containment and recovery plan emphasizing: immediate credential revocation, workspace isolation, fine-grained Unity Catalog ABAC/row-level controls, Just-In-Time access, cross-cloud credential rotation, Cloud Audit Logs, data provenance, and a tabletop runbook validation. Which steps and tools would you use?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Databricks","OpenAI","Snap"]},{"id":"q-3786","question":"A fintech platform runs services in AWS, Azure, and GCP. An attacker tampers with the centralized logging pipeline, hiding CloudTrail, Azure Activity Logs, and Cloud Logging. Provide a CISSP-aligned containment and recovery plan that preserves log integrity, enforces immutability, and enables rapid detection across clouds. Specify concrete steps and tools you’d deploy, and how you’d validate recoverability before production?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["NVIDIA","Robinhood","Square"]},{"id":"q-3809","question":"Scenario: A distributed ML platform spans AWS data lake, GCP feature store, and an on-prem model registry. A developer workstation compromise threatens data and artifact integrity. Propose a CISSP-aligned, practical plan to enforce least privilege using ABAC with just-in-time elevation, artifact signing and provenance, automated access reviews, and an incident response playbook. Include tooling and validation steps?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Coinbase","Instacart","Snap"]},{"id":"q-3843","question":"A fintech firm runs CI/CD in GitHub Actions, deploys to AWS and a Kubernetes cluster, with roles for developers, release engineers, and SREs. Design a CISSP-aligned, beginner-level access plan that enforces least privilege and just-in-time elevation, using RBAC/ABAC, ephemeral credentials, secret management, onboarding/offboarding workflows, and auditable logging. Specify tooling and validation steps?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Bloomberg","Square","Stripe"]},{"id":"q-3902","question":"In a production, multi-region, multi-cloud Kubernetes deployment delivering a fintech service, a CI/CD runner was compromised and leaked short-lived access tokens and container signing keys stored in a secret store. Outline a CISSP-aligned containment and recovery plan that minimizes blast radius, including immediate credential revocation, automated rotation across AWS and GCP, enforcement of least privilege with dynamic access controls (OPA/ABAC), ephemeral credentials (OIDC), audit/log review, and a validation tabletop before rollout. Which concrete steps and tools would you use and why?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Coinbase","Zoom"]},{"id":"q-4167","question":"In a multi-cloud, containerized service with SSO to Google Workspace, AWS, and Azure AD, a contractor needs two days access to CI/CD and prod secrets. Design a CISSP-aligned beginner-level access plan using ABAC/RBAC, JIT credentials, ephemeral secrets, break-glass, and auditable logs. Include onboarding/offboarding and validation steps?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Amazon","Apple","Robinhood"]},{"id":"q-4190","question":"You’re deploying an AI/ML platform for real-time equities analytics across multiple clouds (GCP and AWS) with on-prem data; outline a CISSP-aligned end-to-end security architecture that enforces policy-as-code (OPA), model provenance signing, least-privilege access with ABAC, and auditable cross-cloud logs, plus an incident response plan for model poisoning and supply-chain risk. Include validation before production?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Goldman Sachs","Google","Hugging Face"]},{"id":"q-4197","question":"A fintech uses AWS for compute, Azure for identity, and an on-prem data gateway. Design a CISSP-aligned, beginner-level access-control plan to enforce least privilege and zero-trust across 20 microservices. Include RBAC/ABAC, just-in-time elevation, ephemeral credentials, SSO, secret management, onboarding/offboarding, access reviews, and auditable logging. Specify tooling and validation steps?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Airbnb","Google","Square"]},{"id":"q-4242","question":"In a cross-cloud secure development environment spanning AWS, GCP, and on-prem, a central secrets vault stores API keys and SSH certificates for 40 microservices deployed in Kubernetes. Propose a CISSP-aligned, beginner-level access plan that enforces least privilege and just-in-time elevation using RBAC/ABAC, ephemeral credentials, artifact signing, and auditable logs. Specify tooling and validation steps?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Apple","Bloomberg","NVIDIA"]},{"id":"q-4460","question":"In a cross-platform mobile app used by 3 partners, you must vet a new 3rd-party analytics SDK before release. As a CISSP beginner, design a practical, auditable supply-chain control plan: what artifacts to require, how to model threats, how to enforce least privilege data access (RBAC/ABAC, feature flags), runtime policies, and incident response triggers?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Meta","MongoDB","Tesla"]},{"id":"q-4517","question":"In a multi-cloud CI/CD pipeline for a ride-hailing platform deployed to AWS, Google Cloud, and on-prem, a vulnerability is found in a base image. Outline a CISSP-advanced, practical plan to secure the software supply chain: SBOM, image signing, attestation, policy-as-code gates, ephemeral credentials for runners, secret rotation, and cross-cloud logging. Include validation steps and rollback procedures?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Adobe","Google","Lyft"]},{"id":"q-4662","question":"In a 3-cloud fintech platform (AWS for services, Azure AD for identity, and on-prem Kubernetes) with 120 microservices deployed via a GitOps pipeline, outline a CISSP-aligned, end-to-end security plan focusing on policy-as-code, ephemeral credentials, and cross-cloud auditing. Include identity federation, least privilege, supply-chain attestation, runtime enforcement, and a validation plan with metrics?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Apple","IBM","Lyft"]},{"id":"q-4707","question":"You manage a multi-tenant data platform deployed on Kubernetes in AWS with Kafka as the event backbone and a separate on-prem data classifier. A tenant's data is exposed due to a misconfigured RBAC on a data-ingest service. Outline a CISSP-aligned, practical plan to enforce strict data isolation, implement just-in-time access with ABAC, ensure artifact provenance and signing for data exports, and define a detection/response workflow?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Goldman Sachs","Snap","Snowflake"]},{"id":"q-4724","question":"Across Uber, Square, and Citadel, design a CISSP-aligned, practical plan to secure workload identity across multi-cloud microservices (Kubernetes clusters on AWS, GCP, and Azure) using SPIFFE/SPIRE with mTLS. Include service-to-service credential management, short-lived cert rotation, policy-as-code with OPA, least privilege, drift detection, cross-cloud attestation, and a multi-tenant incident tabletop focusing on credential compromise and supply-chain risk?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Citadel","Square","Uber"]},{"id":"q-958","question":"Scenario: A fintech startup uses cloud IAM for multi-cloud apps. Admins use weak passwords and MFA is not enforced; API keys are shared in chat and stored insecurely. You’re asked to pick one first CISSP-aligned control to reduce risk while enabling operations. Which baseline control should be implemented first and why?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Coinbase","Google","Meta"]},{"id":"q-1087","question":"You're running a 5-node HA Kubernetes control plane (3 in AZ-a, 2 in AZ-b) with a 3-member etcd cluster. After a regional outage, etcd loses quorum. Describe exact, command-level steps to restore quorum, rejoin the third member, and validate API availability across both AZs, including risk notes and DR readiness checks?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Cloudflare","Scale Ai","Snap"]},{"id":"q-1368","question":"In a 3-node etcd-backed Kubernetes cluster, one node loses network connectivity and becomes partitioned while the other two remain healthy. How do you preserve availability and data integrity, avoid split-brain, and recover the partitioned member? Outline health checks, how you isolate the partition, recovery steps, and verification?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Netflix","Tesla"]},{"id":"q-1395","question":"In a 3-node Kubernetes cluster, deploy a stateless web app using a Deployment with 3 replicas and a ClusterIP Service. Include readiness and liveness probes, CPU/memory requests and limits, and populate APP_MODE via a ConfigMap and DB_PASSWORD from a Secret. Describe the steps to perform a rolling update to 4 replicas with zero downtime and how you would verify the rollout across nodes?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Adobe","Snap"]},{"id":"q-1418","question":"Enable at-rest encryption for Kubernetes Secrets using a KMS (envelope) provider on an existing 3-control-plane cluster. Describe exact steps to configure the EncryptionConfig, rotate KEKs without downtime, trigger re-encryption of existing Secrets, and verify that new and existing Secrets are stored encrypted at rest (without decrypting at-rest data). Include concrete commands and caveats?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Google","Robinhood","Scale Ai"]},{"id":"q-1452","question":"You operate a 3-node Kubernetes cluster with a StatefulSet of 3 replicas backed by PVs. A critical fix requires upgrading to image app:v2 with zero downtime. Outline a precise upgrade plan: set a PodDisruptionBudget to minAvailable 2, apply a RollingUpdate with partition=2, ensure readiness probes tolerate brief pod restarts, and verify data integrity during rollout with concrete kubectl commands?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Google","Oracle","Uber"]},{"id":"q-1562","question":"How would you design a Kubernetes Job named app-init-seed that seeds app data on first run? The Job should use an Alpine-based image, mount a PVC at /data, load a script from a ConfigMap at /scripts/seed.sh, skip if /data/.seeded exists, optionally fetch seed.json from https://example.com using API key from a Secret, and write a log and the marker file upon success; include full YAML and apply steps?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Adobe","Salesforce"]},{"id":"q-1737","question":"You're deploying a 3-replica Deployment in Kubernetes for a payment app. Suddenly 2 pods crash with CrashLoopBackOff. Describe a practical debugging workflow to identify if the issue is container startup, config, resources, or image, and outline the exact kubectl commands and file checks you would perform. Include how you would propose a minimal fix and a rollback plan?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Apple","Robinhood"]},{"id":"q-1764","question":"Advanced: In a 5-node Kubernetes cluster where etcd memory usage has spiked after a noisy deployment, outline a production-safe remediation plan: verify health with etcdctl, diagnose via metrics, perform defrag/compact, take a snapshot, and adjust API watch load while preserving API server availability. What steps would you take?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Google","Microsoft","Snowflake"]},{"id":"q-1810","question":"You're operating a 5-node Kubernetes cluster with a validating webhook named cfg.example.com enforcing a label requirement on Deployments in all namespaces. A critical patch must be applied in prod-adv while the webhook service is temporarily unavailable. Outline a safe, auditable plan to bypass the webhook with exact kubectl commands to identify, disable, apply, and revert?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["DoorDash","Google","Meta"]},{"id":"q-1885","question":"In a high-traffic delivery platform, you deploy a Kubernetes-based worker pool for async order processing. Explain how you would implement a robust, rate-limited queue with backpressure, idempotent workers, and at-least-once delivery, using Kubernetes primitives and common tools. Include how you handle spikes, retries, and data-store consistency?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["DoorDash","Two Sigma"]},{"id":"q-1932","question":"Describe a **concrete, production-ready plan** for zero-downtime deployments in a Lyft-scale Kubernetes cluster with ~40 microservices, multiple data stores, and strict uptime. How would you implement **blue/green or canary releases**, traffic shaping, and automated rollbacks? Include rollout strategy, readiness checks, RBAC, DR, and a compact manifest example comparing Istio and Linkerd approaches?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Lyft","Tesla","Twitter"]},{"id":"q-1946","question":"On a 3-node Kubernetes cluster, a Deployment’s Pods stay Pending while 2 nodes are Ready. The scheduler shows a NoSchedule taint on node-1. Explain exact commands to identify the taint, check the pod’s tolerations, and either remove the taint or add a matching toleration so the workload schedules. How do you validate after changes?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Amazon","Apple","Uber"]},{"id":"q-1972","question":"You maintain a Node.js app deployed as a 3-replica Kubernetes Deployment behind a service. A security patch requires updating the container image from v1.0.1 to v1.0.2 with zero downtime. List exact kubectl steps to perform a safe rolling update, how you confirm readiness, and how you rollback if the rollout fails?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Microsoft","Netflix"]},{"id":"q-2027","question":"You're managing a shared Kubernetes cluster with namespaces prod, analytics, and dev. A nightly analytics batch job sometimes starves frontend pods during peak hours, triggering evictions. Design an end-to-end remediation plan: tune pod requests/limits, enforce defaults with a LimitRange, cap total usage with a ResourceQuota, ensure guaranteed QoS, evaluate VerticalPodAutoscaler vs HorizontalPodAutoscaler, and outline validation steps to prevent regressions?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["DoorDash","Netflix"]},{"id":"q-2134","question":"You manage a Databricks/OpenAI-like platform on Kubernetes. A Spark streaming job processes 1TB/day and faces sporadic node preemption causing data loss without checkpoints. Design a fault-tolerant deployment (manifests and configurations) and explain trade-offs between Kubernetes Job vs. StatefulSet, backoff strategies, taints/tolerations, and Spark dynamic allocation. What would you implement and why?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Databricks","OpenAI"]},{"id":"q-2152","question":"On a Kubernetes cluster running Spark jobs via Spark on Kubernetes, a 12-pod job reports intermittent OOM errors during shuffle-heavy stages on medium-sized datasets. Provide a concrete debugging plan: how you verify memory limits, adjust executor/driver memory and overhead, tune JVM GC, and validate changes with metrics and a controlled benchmark run?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Databricks","Oracle"]},{"id":"q-2197","question":"In a Kubernetes cluster running mixed workloads, a critical data-processing job occasionally starves QoS pods during peak load. Provide a concrete plan to enforce SLA guarantees: specify resource requests/limits with appropriate QoS, enable Pod Priority and Preemption, configure HorizontalPodAutoscaler and ClusterAutoscaler, and outline validation steps to prove SLAs hold under load?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["LinkedIn","Two Sigma"]},{"id":"q-2282","question":"Implement namespace-scoped network isolation for namespace 'team-a' using Kubernetes NetworkPolicy. Ensure pods in team-a can reach only the DNS service and a single internal log sink at 'log-sink.logging.svc.cluster.local:9200', and cannot reach other namespaces or external hosts. Provide the manifests (default-deny and allow rules), kubectl commands to apply, and verify using a tester pod with targeted tests for DNS, log sink, and external access?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Databricks","Scale Ai"]},{"id":"q-2304","question":"You're administering a 3-namespace Kubernetes cluster hosting two teams. There are no NetworkPolicies today. Draft and implement a precise plan to enforce per-namespace default-deny, permit intra-namespace traffic for all pods, and allow outbound HTTPS to api.external.com:443, while blocking cross-namespace pod communication. Provide exact YAMLs for the policies, commands to verify with curl from representative pods, and rollback steps?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Hugging Face","Netflix","Twitter"]},{"id":"q-2338","question":"How would you identify memory pressure causing OOMKilled pods in a 3-node Kubernetes cluster (kubectl top, pod events, and logs), compare usage to requests/limits, profile memory if possible, and implement a fix with proper requests/limits, ResourceQuota, and a canary rolling update (maxUnavailable:1) plus a LivenessProbe, then validate with a soak test?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Microsoft","Netflix"]},{"id":"q-2395","question":"In a shared Kubernetes cluster serving teams at Slack, IBM, and Salesforce, design a namespace-per-tenant isolation strategy with quotas and security controls. Include ResourceQuota, LimitRange, NetworkPolicy, PodSecurity admission, and RBAC. Propose a GitOps deployment flow per-namespace and an observability plan. Compare against a single-namespace approach and justify your choice?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["IBM","Salesforce","Slack"]},{"id":"q-2519","question":"In a Kubernetes cluster, a Deployment named web-app with 3 replicas in namespace prod suddenly fails after a ConfigMap update that injects environment variables. Pods crash due to a missing DB_PASSWORD from the ConfigMap. Describe exact steps and commands to diagnose, patch the ConfigMap, and roll out a fix with minimal downtime while ensuring the deployment restarts cleanly?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Databricks","Hashicorp","Oracle"]},{"id":"q-2553","question":"In a two-region Kubernetes cluster with 3 nodes per region and a PostgreSQL StatefulSet backed by PVs, deploy a new service image with zero downtime using canary or blue-green. Detail exact rollout steps, traffic shaping, health checks, and data-consistency verification during rollout?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Google","IBM","Square"]},{"id":"q-2584","question":"**Advanced Kubernetes Debugger**: You manage a 12-node cluster on AWS EKS running three microservices. During a surge, a critical deployment experiences OOMKilled. Provide a concrete, end-to-end debugging plan and rollback strategy: include metrics you’d capture, commands you’d run, and exact changes (requests/limits, HPA, PDB, rollout)?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Hashicorp","Meta","Twitter"]},{"id":"q-2656","question":"In a three-AZ Kubernetes cluster, design an operator to keep replicas of Deployments labeled app=svc spread across zones, even during node drains. Describe the CRD you would add, the reconcile logic, and how you would validate distribution with tests. Provide a minimal manifest example?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Databricks","LinkedIn","Netflix"]},{"id":"q-2721","question":"In a 6-node Kubernetes cluster, a critical stateless service is upgraded via RollingUpdate. Ensure availability never drops below 80% of replicas during the upgrade. Outline a concrete rollout plan with specific values for maxUnavailable, maxSurge, and a PodDisruptionBudget, plus readiness probes, preStop hooks, and a rollback strategy. Include how you'd verify rollout status and handle failure?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Amazon","Slack","Tesla"]},{"id":"q-2939","question":"Design a production-grade Kubernetes cluster with a three-node HA control plane across AZs. Explain how you ensure etcd backups and retention, implement per-tenant RBAC, enforce NetworkPolicy isolation, and deploy canaries with automated rollback. Include concrete commands, monitoring checks, and rollback criteria?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Amazon","Apple","DoorDash"]},{"id":"q-3024","question":"You're operating a multi-tenant Kubernetes cluster with DoorDash-like traffic. A new service must scale from 2 to 40 pods at peak while minimizing waste. Describe how you would configure ResourceQuota, LimitRange, HPA, and PodDisruptionBudget, plus how you would implement canary deployments and observability to ensure zero-downtime during node maintenance?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Anthropic","DoorDash","Goldman Sachs"]},{"id":"q-3117","question":"A Deployment in namespace default has 3 replicas, and one pod is CrashLoopBackOff after a rollout. List exact commands and checks you would perform to diagnose, including how you view logs, events, and the container state, and how you verify the fix with a safe rollout?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Discord","Oracle","Scale Ai"]},{"id":"q-3141","question":"Implement namespace-scoped RBAC in a 5-namespace cluster to run a batch Job in tenant-a that must not read Secrets unless explicitly allowed. Outline concrete Roles, RoleBindings, and test steps with exact kubectl commands and YAML to verify denial and controlled grant, including can-i checks and a sample Job manifest?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Discord","Goldman Sachs","Meta"]},{"id":"q-3299","question":"In a 5-node cluster with 3 control-plane nodes and 2 workers, upgrading from v1.26 to v1.28 with zero downtime and CRD migrations. Provide a concrete, command-driven upgrade plan: pre-checks, etcd backup, upgrade order, drain strategy, CRD handling, and post-upgrade validation. What exact steps ensure zero-downtime during the upgrade?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["IBM","LinkedIn","NVIDIA"]},{"id":"q-3351","question":"Explain a concrete, production‑readiness plan to run a 3‑node **etcd** cluster across **AZs** with a primary region and disaster recovery in a separate region. Include deployment topology, TLS/mTLS, backup/restore (etcdctl snapshot), upgrade strategy, monitoring, and a tested failover process. What commands and checks would you rely on?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Hugging Face","Lyft","Microsoft"]},{"id":"q-3374","question":"Scenario: In a 5-node cluster (3 workers, 2 control-planes) with many namespaces, introduce a new requirement to strictly enforce tenant-level network egress controls using Calico without downtime. Outline a concrete rollout plan to implement namespace-scoped egress policies with default deny, ensuring existing pods can still reach DNS, API server, and private registry. Include policy snippets, rollout steps, and verification methods?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Google","Twitter"]},{"id":"q-3400","question":"You're running a 3-member MongoDB replica set as a StatefulSet on Kubernetes. A long-running backup is required to be consistent and zero-downtime, using Kubernetes VolumeSnapshots for the PVCs. Outline a precise, end-to-end plan to produce a crash-consistent snapshot: coordinate a primary fsyncLock, snapshot all MongoDB PVCs, and fsyncUnlock; handle possible primary failover during the window; and validate the snapshot and readiness for DR?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Meta","MongoDB"]},{"id":"q-3408","question":"You're given a 1-node Kubernetes cluster with a Deployment named web-app that reports ImagePullBackOff. Describe the exact, practical steps you would take to diagnose the image pull error, configure registry authentication with imagePullSecrets, update the Deployment manifest, and verify the pod becomes Ready?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Amazon","Google","NVIDIA"]},{"id":"q-3571","question":"You're operating a 5-node Kubernetes cluster (3 control-plane, 2 workers). A suspected abuse of cluster privileges is detected via noisy API activity. Outline a concrete plan to enable rigorous auditing, contain the risk, and verify no unauthorized actions occur without disrupting legitimate workloads. Include exact commands or file samples for audit policy, apiserver flags, log forwarding, and validation steps?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Citadel","Meta","Snowflake"]},{"id":"q-3606","question":"Scenario: A 5-node Kubernetes cluster experiences intermittent DNS failures after a rolling CoreDNS upgrade that disrupts frontend service resolution. Describe concrete steps to diagnose and fix, including how to inspect CoreDNS pods and ConfigMap, how to test DNS from a pod, how to adjust the Corefile (cache TTL, forwarders), and how to validate under load?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Bloomberg","Lyft","Snap"]},{"id":"q-3700","question":"You manage a 3-node Kubernetes cluster with a StatefulSet named 'db' consisting of 3 replicas, each using PVCs provisioned by the CSI storage class 'fast-ssd'. A data-intensive resize from 100Gi to 150Gi must be done with zero downtime. Outline a concrete plan: confirm expansion support, resize all PVCs, grow the filesystem inside pods, perform a rolling restart with a PodDisruptionBudget, verify data integrity, and specify rollback steps if expansion fails?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Goldman Sachs","Oracle","Slack"]},{"id":"q-3703","question":"In a 4-node Kubernetes cluster, enable mutual TLS across all namespaces using Linkerd, enforce mTLS by default, and verify that only services with Linkerd sidecars can communicate with a backend API on port 8080 while a plain pod cannot. Provide exact commands to install Linkerd, inject sidecars, deploy apps, and perform tests?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Apple","Lyft","Tesla"]},{"id":"q-3779","question":"You're given a Kubernetes cluster with Deployment web-api (3 replicas). One pod shows CrashLoopBackOff; describe the exact diagnostic steps (commands, log files, and configs) you would perform to identify the root cause, verify the fix, and restore the deployment to healthy. Include how you'd inspect events, container exit codes, and readiness probes?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Hugging Face","MongoDB","Stripe"]},{"id":"q-3856","question":"In a Kubernetes cluster with a StatefulSet running 3 PostgreSQL pods across three AZs, you observe intermittent 10–20s startup delays and rollout stalls during updates. Describe exact, actionable steps you would take to diagnose and remediate while preserving availability?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["IBM","Slack"]},{"id":"q-3950","question":"Scenario: A 3-replica Deployment behind a ClusterIP Service experiences sporadic pod restarts with OOMKilled during peak load. Provide a concrete debugging plan and implement fixes: add memory requests/limits, readinessProbe and livenessProbe, and a horizontal autoscaler. Include exact kubectl commands and YAML fragments you would apply?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Databricks","Tesla"]},{"id":"q-4035","question":"You deploy a 3-replica Deployment for a Node.js app on Kubernetes; pods crash during startup and show CrashLoopBackOff. Describe a practical debugging workflow using kubectl and provide a minimal readinessProbe and livenessProbe for port 3000 /health to ensure traffic only reaches healthy pods?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Amazon","Snap"]},{"id":"q-4163","question":"Scenario: In production, a Deployment named order-processor in namespace prod uses a ConfigMap named app-config with LOG_LEVEL and a Secret app-secret with API_KEY. The Pod fails to start because API_KEY is missing. Provide a practical fix: 1) ensure Secret exists and mounted, 2) wire LOG_LEVEL and API_KEY into env vars from ConfigMap/Secret, 3) add readinessProbe on port 8080 and a livenessProbe, 4) outline a 0-downtime rollout approach and verification steps?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Netflix","Stripe"]},{"id":"q-4183","question":"You have a Kubernetes Deployment running in a single-node cluster. A pod restarts with OOMKilled after a few minutes. Describe exact, concrete steps you would take to diagnose and fix the issue, including commands, YAML changes, and how you would verify the fix?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Google","OpenAI","Snowflake"]},{"id":"q-4288","question":"A 4-node Kubernetes cluster serves a stateless web app behind a LoadBalancer Service. After applying a hotfix, 25–30% of pods crash with CrashLoopBackOff and requests show 5xx errors. Outline a real-world diagnosis and rollback plan, including exact kubectl commands to inspect rollout status, pods, events, ConfigMaps/Secrets, verify readiness/liveness probes, adjust resource requests/limits, and implement a canary rollout to shift traffic safely?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Instacart","LinkedIn","Oracle"]},{"id":"q-4388","question":"Scenario: A 3-node, etcd-backed Kubernetes control plane on bare metal managed by kubeadm has suffered a regional outage that left only one master up. Provide a precise disaster-recovery plan to restore a healthy 3-member etcd quorum and API control plane within 20 minutes, including etcd snapshot restoration, rejoining control-plane nodes, updating manifests, and verification steps with exact commands?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Apple","LinkedIn","Tesla"]},{"id":"q-4450","question":"CRD Migration in a 3-AZ cluster with a StatefulSet of 5 PV-backed pods: migrate CRDs from v1alpha1 to v1 without downtime or data loss. Provide a concrete end-to-end plan: conversion webhook strategy, canary rollout per CRD, etcd backup/restore, upgrade order (APIServer, controller-manager, operator), and rollback criteria with exact kubectl commands?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Apple","Oracle","Uber"]},{"id":"q-4620","question":"In a 3-node Kubernetes cluster, a frontend Deployment that calls an external API experiences intermittent DNS failures after a cluster upgrade. Outline a concrete troubleshooting and fix plan focusing on CoreDNS health, DNS policy, and a safe rollout that preserves uptime. Include exact commands to verify CoreDNS, modify CoreDNS config to enable caching or upstreams, and verify end-to-end DNS resolution from a pod?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Coinbase","Google","Microsoft"]},{"id":"q-4755","question":"Scenario: A 5-node Kubernetes control-plane cluster (3 masters, 2 workers) experiences an unhealthy etcd member. You must perform disaster recovery by restoring from the latest snapshot to a new member and rejoining with minimal downtime. Outline a concrete, command-driven runbook: health checks, member add/remove, etcd snapshot restore, updating the etcd manifest, restarting API server, and post-restore validation?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Google","PayPal"]},{"id":"q-874","question":"In a Kubernetes cluster used by Salesforce/Cloudflare/Snap engineers, a Deployment's startup latency rose from 1–2s to 6–8s after introducing an initContainer that runs a health check before the application starts. Describe how you would diagnose, what metrics/logs to collect, and concrete fixes (e.g., moving checks to readiness, caching, parallel init, or canary rollout). Include rollback and validation steps?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Cloudflare","Salesforce","Snap"]},{"id":"q-893","question":"You manage a 3-node Kubernetes control plane backed by an etcd cluster. After a power outage, one etcd member reports corruption. Describe the exact steps to detect the corrupted member, restore from a known-good snapshot, rejoin the cluster, and validate API availability. Include concrete commands, risk notes, and how you would verify DR readiness?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Cloudflare","IBM","Netflix"]},{"id":"q-936","question":"A 3-control-plane Kubernetes cluster on AWS experiences API server latency spikes after a webhook deployment. The admission webhook is malfunctioning and causing slow requests; outline precise steps to identify the failing webhook, safely disable it to restore API responsiveness, validate cluster availability, and prepare a rollback plan with minimal downtime?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Airbnb","Databricks","Google"]},{"id":"q-1066","question":"Inside namespace analytics, schedule a daily batch to process a CSV: use a CronJob (02:00 UTC) to start a Job that runs a Python script from a ConfigMap, reads input from a ConfigMap, uses a Secret for DB credentials to insert results into Postgres service, writes output to a PVC, runs as non-root with a readOnlyRootFilesystem, with resource limits, a backoffLimit of 3, and a 15-minute activeDeadlineSeconds; ensure proper probes?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Goldman Sachs","Microsoft","Snowflake"]},{"id":"q-1312","question":"In a Kubernetes CKAD scenario, you have a Deployment named 'web-server' in namespace 'prod' running a Node.js app. During rolling updates, some pods terminate and restart slowly, causing request timeouts. Outline concrete changes to implement startupProbe, adjust readiness and liveness probes, and add a preStop hook to drain existing connections. Provide a minimal manifest patch showing startupProbe, a readinessProbe, a livenessProbe, and a preStop lifecycle hook that ensures graceful shutdown. How would you approach this?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Discord","MongoDB","Snap"]},{"id":"q-1352","question":"You have a small Python API app to run in Kubernetes. Write a minimal manifest that creates a Deployment with 3 replicas using image myregistry/api:1.0, a readinessProbe httpGet /health on port 8080, a livenessProbe with initialDelaySeconds 15 and periodSeconds 10, and a ClusterIP Service exposing port 8080. Inject config via ConfigMap app-config with LOG_LEVEL. How would you verify and how would you scale to 5 replicas?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Google","Meta","OpenAI"]},{"id":"q-1393","question":"Design a Kubernetes manifest that deploys a stateless app with 3 replicas, uses a ConfigMap and a Secret, attaches a 1Gi PVC for data, includes a HorizontalPodAutoscaler, exposes via ClusterIP, and enforces a NetworkPolicy restricting egress to api.internal.example.com:443. Provide YAML fragments and discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["OpenAI","Two Sigma"]},{"id":"q-1459","question":"Create Kubernetes manifests for a simple API: Deployment using image 'my-api:1.0' that reads PORT from a ConfigMap via env, a ConfigMap with PORT and APP_MODE, readinessProbe and livenessProbe for /healthz on that port, and resource requests/limits. Expose with a Service on port 80 targeting 3000. Describe a rolling update plan?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["IBM","Meta","NVIDIA"]},{"id":"q-1488","question":"Given a Deployment named 'image-processor' in namespace 'prod' with 6 replicas processing images from a Redis queue, design a practical patch to ensure graceful shutdown of in-flight tasks during rollouts, prevent simultaneous pod terminations, and maintain availability during node drains; include a minimal manifest patch adding a preStop script, terminationGracePeriodSeconds, readiness and liveness probes, and a PodDisruptionBudget targeting the deployment. What steps would you take to validate under load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Adobe","Airbnb","Snap"]},{"id":"q-1547","question":"Configure a NetworkPolicy to restrict egress from prod/checkout-service to only reach payments/payment-processor in the payments namespace on port 443, blocking all other egress. Provide a minimal manifest patch and a test strategy to validate allowed and blocked traffic inside the cluster?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Netflix","Oracle"]},{"id":"q-1609","question":"You're deploying inventory-service on Kubernetes. Write YAML to (1) deploy 3 replicas with a ConfigMap for API_ENDPOINT and a Secret for DB_PASSWORD, (2) an InitContainer that runs migrations, (3) readiness and liveness probes, resource requests/limits, and a RollingUpdate strategy with maxUnavailable: 1, (4) a Job to run migrations before the first pod starts, (5) a sidecar log-shipper. Include the key fragments and rationale?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["DoorDash","NVIDIA","Slack"]},{"id":"q-1626","question":"Blue/Green rollout for a CKAD production web service: in namespace 'prod', a Deployment 'web-app' with 3 replicas serves traffic via the Service 'web-app'. Introduce a canary path with a second Deployment 'web-app-canary' and a canary route (via canary Ingress or a second Service) to validate with 10–20% traffic before full switch. Provide a minimal manifest patch showing resource requests/limits, readinessProbe, and a livenessProbe for both deployments, plus how to switch traffic and rollback. Include validation steps under load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["NVIDIA","OpenAI","Oracle"]},{"id":"q-1707","question":"Explain a progressive canary rollout for a 4-replica frontend behind an NGINX Ingress. Use a stable Deployment and a canary Deployment (image frontend:1.2-canary, replicas:1). Patch Ingress to route 10% to canary; later 50% and 100%. Include minimal YAML patches for deployments and a canary Ingress with canary-weight; how would you monitor health and rollback?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Coinbase","Discord"]},{"id":"q-1767","question":"Scenario: In a CKAD scenario, you must roll out a new image for a stateless API 'inventory-api' in namespace 'prod' with 2% traffic to the canary, using vanilla Kubernetes (no service mesh). Outline a practical canary strategy with two Deployments and two Services, explain how you split traffic without a mesh, and provide minimal manifests for the canary Deployment and a canary-facing Service, plus a rollback plan and how you'd validate under load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Google","NVIDIA"]},{"id":"q-1776","question":"Design and implement a CKAD deployment for a 3-replica web app: use a ConfigMap for env vars, define resource requests/limits, add readiness and liveness probes, and configure an HPA with min 3, max 10 and targetCPUUtilizationPercentage 50. Provide the YAML and show verification steps?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Meta","Oracle"]},{"id":"q-1879","question":"You are deploying a Node API behind a Service in Kubernetes with Redis and PostgreSQL. Provide manifests to run API with 4 replicas, Secret for DB creds, ConfigMap for flags, readiness/liveness probes, resource requests/limits, and a RollingUpdate with maxUnavailable=25%, maxSurge=25%. Include how you would test zero-downtime and how HPA would be wired?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Amazon","Discord","Google"]},{"id":"q-1922","question":"CKAD intermediate: In a prod namespace, a Deployment named 'orders-api' with 3 replicas experiences brief outages during image upgrades. Provide a concrete patch for zero-downtime upgrades: set rollingUpdate strategy (maxUnavailable: 0, maxSurge: 1), add a PreStop hook for graceful shutdown, and create a PodDisruptionBudget to protect at least 2 healthy pods during maintenance. Include minimal Deployment and PDB manifests and describe validation under maintenance-like load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Google","Square"]},{"id":"q-1959","question":"In a Kubernetes CKAD scenario, design a real-time log-processor that consumes from Kafka, writes results to Cassandra, restarts gracefully on failure, and preserves at-least-once delivery. Provide a concrete deployment design with InitContainer, ConfigMap, Secret, Probes, HPA, DLQ strategy, and a minimal YAML skeleton. Explain trade-offs and monitoring hooks?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Meta","NVIDIA","Two Sigma"]},{"id":"q-1992","question":"Scenario: A 3-replica Deployment for a Kubernetes CKAD exercise uses a ConfigMap for APP_PORT and LOG_LEVEL and a Secret for DB_PASSWORD. How would you structure manifests to ensure zero-downtime updates, proper health checks, and correct secret/config usage? Provide concrete manifest fragments and a rollout strategy?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Apple","IBM","Meta"]},{"id":"q-2015","question":"Design a CKAD-grade, multi-tenant API gateway canary rollout: implement two Deployments (stable and canary) for api-gateway, share a Service, and use an Ingress canary annotation to route 20% traffic to canary. Use a ConfigMap flag newFeature to toggle the new code path; store TLS certs in a Secret; include readiness/liveness probes, resource requests/limits, and a minimal YAML skeleton. Explain how you would observe traffic split and rollback?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Meta","Uber"]},{"id":"q-2093","question":"You deploy a Kubernetes Deployment named web-app using image registry.example.com/web-app:v1.2 and expose port 8080. Provide a YAML manifest snippet that adds: livenessProbe for /health on 8080 with initialDelaySeconds: 5 and periodSeconds: 10; readinessProbe for /ready with timeoutSeconds: 3; resources: requests cpu: 250m memory: 256Mi; limits cpu: 500m memory: 512Mi; a ConfigMap mounted at /etc/config providing APP_MODE and an env var APP_MODE sourced from that ConfigMap?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Discord","Salesforce"]},{"id":"q-2145","question":"CKAD intermediate: In namespace retail, a Deployment 'inventory' (3 replicas) talks to Postgres service 'inventory-db'. You need a one-time seed of lookup data at Pod startup without delaying traffic. Provide a minimal patch that uses an InitContainer to run a seed SQL script against inventory-db with idempotent INSERTs (ON CONFLICT DO NOTHING), and add resource requests/limits, a readiness probe, and a liveness probe. Explain how you'd validate under load and re-seed behavior on restarts?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Discord","Microsoft","PayPal"]},{"id":"q-2205","question":"CKAD intermediate: A 3-rep StatefulSet named 'cache' in namespace 'prod' must be upgraded with zero downtime. How would you implement a partitioned, rolling update (one pod at a time) using spec.updateStrategy.RollingUpdate with partition, and set readiness/startup probes plus CPU/memory limits? Also provide a minimal PodDisruptionBudget to keep at least 2 pods healthy during maintenance, and outline validation under load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Discord","Salesforce"]},{"id":"q-2237","question":"Design a Kubernetes manifest suite for image registry.example/ckad-demo:1.0 that uses a ConfigMap and a Secret. Include a Deployment with 3 replicas, a Service on port 80, a ConfigMap app-config with APP_MODE=production, and a Secret app-secret with API_KEY=secret123. Mount the ConfigMap at /etc/app and the Secret at /etc/secret, export APP_MODE as an env, include readiness and liveness probes on /healthz, and trigger rolling updates on ConfigMap changes via a checksum/config annotation?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Anthropic","Hashicorp","Oracle"]},{"id":"q-2276","question":"In a CKAD scenario, namespace 'analytics' hosts Deployment 'data-collector' (5 replicas) behind service 'collector-svc' on port 9000. Implement a NetworkPolicy that (1) allows ingress to data-collector only from pods in the analytics namespace labeled app=ingest on port 9000, (2) denies all other inbound traffic, and (3) allows egress to 10.0.0.50:5140. Provide the YAML manifest and a patch to the Deployment to apply the policy. Describe how you'd validate under simulated load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Citadel","Discord","Snowflake"]},{"id":"q-2306","question":"In CKAD terms, design a per-namespace streaming transformer that consumes Redis Stream 'events', computes a rolling 60s window average latency per user, and writes aggregates to PostgreSQL. Replays failed messages to a Redis DLQ. Use an InitContainer to preload an ML model from a Secret, a ConfigMap for thresholds, readiness/liveness probes, and an HPA based on latency. Provide a minimal YAML skeleton and discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Coinbase","Hugging Face","Snap"]},{"id":"q-2366","question":"Design a Kubernetes manifest for a beginner CKAD task: deploy a static site with nginx where index.html is served from a ConfigMap named app-content. Mount TLS certs from Secret tls at /etc/tls. Requirements: Deployment with 2 replicas, runAsNonRoot and readOnlyRootFilesystem, readiness/liveness probes on /healthz, INDEX_FILE env from ConfigMap, and an HPA with CPU target 50%. Provide a minimal YAML skeleton and discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Bloomberg","Tesla"]},{"id":"q-2427","question":"Design a CKAD-focused per-namespace file-processing service: it watches a PVC-mounted directory for new text files, processes each file, and submits results to a REST API. Use a ConfigMap for FILE_EXT and BATCH_SIZE, a Secret for API creds, an InitContainer to install deps, and a marker-based idempotence scheme to achieve at-least-once with near-exactly-once semantics. Include readiness/liveness probes and a minimal YAML skeleton; discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Adobe","Anthropic","Snap"]},{"id":"q-2453","question":"In a CKAD scenario, design a per-namespace real-time image-resize service that consumes from a Kafka topic, uses an InitContainer to fetch a resize model from a Secret, writes results to PostgreSQL, and preserves at-least-once delivery with idempotent DB writes. Include a ConfigMap for RESIZE_WIDTH/HEIGHT and BATCH_SIZE, readiness/liveness probes, an HPA based on CPU, and a per-namespace NetworkPolicy restricting Kafka ingress and DB egress. Provide a minimal YAML skeleton and discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Apple","Hugging Face"]},{"id":"q-2541","question":"In CKAD terms, design a new, per-namespace data-collector that subscribes to an in-cluster message bus (NATS) and writes results to a time-series store, ensuring at-least-once delivery with a DLQ. Include an InitContainer to install runtime deps, a ConfigMap for BATCH_SIZE, a Secret for creds, a Sidecar for TLS cert rotation, readiness/liveness probes, and a NetworkPolicy restricting egress to the TSDB, a DLQ path on a PVC, and a minimal YAML skeleton. Explain trade-offs?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Databricks","Discord","Meta"]},{"id":"q-2614","question":"CKAD advanced: design a per-namespace data-pipeline controller using a CRD NamespaceDataPipeline. Each instance subscribes to a namespaced NATS subject (metrics.{namespace}) and writes batched data to a TSDB. Guarantee at-least-once via a DLQ on a PVC and idempotent writes. Include InitContainer for deps, a TLS-rotation sidecar, a ConfigMap for BATCH_SIZE, a Secret for creds, readiness/liveness probes, and a NetworkPolicy restricting egress to the TSDB. Provide a minimal YAML skeleton and trade-offs?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Apple","Oracle","Robinhood"]},{"id":"q-2767","question":"CKAD advanced: Design a per-namespace policy-enforcement flow using a MutatingAdmissionWebhook that auto-injects resource requests/limits into Pods lacking them. The webhook reads defaults from a ConfigMap (e.g., default_cpu: 125m, default_memory: 128Mi) with per-namespace overrides; a Secret stores a token to fetch policy updates from an in-cluster policy store. Include a minimal webhook Deployment, MutatingWebhookConfiguration, and a per-namespace test namespace. Provide a minimal YAML skeleton and discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Coinbase","Databricks"]},{"id":"q-2782","question":"CKAD intermediate: In namespace 'prod', a Deployment 'payments-api' with multiple replicas risks resource contention under peak load. You must enforce per-namespace resource discipline. Create a ResourceQuota and a LimitRange; provide minimal manifests and describe validation under load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Goldman Sachs","Scale Ai","Snap"]},{"id":"q-2881","question":"In a CKAD scenario, you need to isolate a payment microservice from other pods while allowing only approved traffic within namespace 'finance'. The Deployment 'payment' has 3 replicas. Write a NetworkPolicy that (a) allows Ingress to payment only from pods labeled app=frontend in the same namespace, (b) allows Egress from payment only to pods labeled app=db on port 5432 and to pods labeled app=cache on port 6379. Provide the minimal policy manifest and describe how to validate under load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Adobe","Airbnb","Amazon"]},{"id":"q-2950","question":"Design a per-namespace Kubernetes CronJob in CKAD terms that runs every 10 minutes and pings a TARGET_URL from a ConfigMap, using an API_TOKEN from a Secret to authenticate, posting a heartbeat payload with a timestamp. Persist the last status in a namespace ConfigMap heartbeat-status and write run logs to a PVC at /logs. Use an InitContainer to install curl, mount the logs volume, add readiness/liveness probes on the Job's Pod, and provide a minimal YAML skeleton. What trade-offs exist?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Adobe","Salesforce","Stripe"]},{"id":"q-3044","question":"In CKAD terms, design a per-namespace CronJob-based worker that watches a PVC-backed /data directory for new CSV files, processes them in batches (configurable via a ConfigMap named app-config with BATCH_SIZE), converts to JSON, and POSTS to a cluster-internal endpoint at http://data-portal:8080/api/v1/ingest. Use an InitContainer to install csvkit, a Secret for API credentials, a Sidecar for TLS cert rotation, readiness/liveness probes, a Namespace-scoped NetworkPolicy restricting egress to the internal endpoint, and a minimal YAML skeleton. Discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Databricks","Discord","Twitter"]},{"id":"q-3158","question":"Design a per-namespace CKAD microservice that exposes a REST endpoint and supports dynamic feature flags controlled by a ConfigMap. The ConfigMap contains JSON features; a Secret stores per-namespace API keys. Include an InitContainer to install runtime dependencies, a sidecar that watches the ConfigMap for changes and signals the main container to reload config, readiness/liveness probes, an HPA based on 95th percentile latency, and a PVC-backed log store. Provide a minimal YAML skeleton and discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["IBM","Meta","Scale Ai"]},{"id":"q-3246","question":"In namespace analytics, Deployment 'reporter' with 4 replicas is failing to fetch tokens because a restrictive NetworkPolicy blocks egress to api.analytics.internal:443. Provide a minimal patch to allow egress to that endpoint, preserving other rules, and describe how you would verify connectivity under load and rollback if needed?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Discord","OpenAI"]},{"id":"q-3338","question":"In CKAD terms, design a per-namespace Deployment that runs a small REST API (example Flask app) with config from a ConfigMap named app-config and an API key from Secret named api-credentials. Include an InitContainer to install dependencies, a sidecar to rotate TLS certs from a Secret named tls-certs, readiness/liveness probes, a Namespace NetworkPolicy restricting egress to DEST_ENDPOINT, and a minimal YAML skeleton. Discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["LinkedIn","Tesla"]},{"id":"q-3410","question":"Design a per-namespace log-forwarder in CKAD terms: a Deployment that tails logs from a PVC-mounted /logs directory produced by apps in the namespace, aggregates log counts by severity for the last minute, and POSTs a heartbeat payload plus the count to a REST endpoint defined in a per-namespace ConfigMap ENDPOINT using API_TOKEN from a Secret. On failure, write payloads to a DLQ directory in the same PVC. Include an InitContainer to install curl and jq, readiness/liveness probes, and a minimal YAML skeleton; discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Databricks","Meta"]},{"id":"q-3480","question":"In CKAD terms, design a per-namespace Deployment that serves a static landing page with Nginx. The site content lives in a PVC mounted at /usr/share/nginx/html. Use a ConfigMap named site-config with SITE_TITLE and FOOTER, and a Secret named site-auth for htpasswd-based basic authentication. Include an InitContainer to pre-populate index.html from /config data, a readiness/liveness probe for /. Provide a Namespace NetworkPolicy restricting egress to an internal CDN at 10.0.0.0/16:443. Include a minimal YAML skeleton and discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Adobe","Oracle","Stripe"]},{"id":"q-3548","question":"In CKAD terms, design a per-namespace Deployment that serves a tiny REST API (Flask) backed by a local SQLite DB stored on a PVC. Use a ConfigMap app-config to toggle ENABLE_FEATURE and set API_TIMEOUT, and a Secret api-credentials for basic-auth. Include an InitContainer to install deps, a TLS cert rotation sidecar, readiness/liveness probes, and a NamespaceNetworkPolicy restricting egress to the internal auth-service host. Provide a minimal YAML skeleton and discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Discord","Snap","Two Sigma"]},{"id":"q-3637","question":"In CKAD terms, design a per-namespace Deployment that handles file-based ingestion from a PVC mounted /input: it should validate each JSON file against a schema stored in a ConfigMap named schema-config, normalize valid records to a consistent JSON, and POST them to http://data-portal:8080/api/v1/ingest using a Bearer token sourced from a Secret named api-tokens. Include an InitContainer to install jq and a validator, a Sidecar for TLS cert rotation, readiness/liveness probes, a Namespace NetworkPolicy restricting egress to the internal endpoint, and a minimal YAML skeleton. Discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Adobe","Uber"]},{"id":"q-3702","question":"In namespace data, create a CronJob named daily-analytics that runs at 02:00 daily using image analytics-runner:2.0. It must mount a ConfigMap analytics-config (DB_HOST, DB_NAME, QUERY_TIMEOUT) and a Secret analytics-creds (DB_USER, DB_PASSWORD). Enforce no overlaps (concurrencyPolicy: Forbid), backoffLimit: 3, TTLSecondsAfterFinished: 86400, successfulJobsHistoryLimit: 3, and annotate the PodTemplate with a checksum/config to reload on changes. How would you verify this under simulated load and when the config changes?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Bloomberg","Snap","Tesla"]},{"id":"q-3798","question":"In CKAD terms, design a per-namespace log-collector Deployment that mounts /logs from a PVC, tails app.log for ERROR lines, batches them (configurable via a ConfigMap named app-config with BATCH_SIZE), and POSTs JSON payloads to an internal endpoint at http://analytics.local:9000/ingest using credentials from a Secret named api-credentials. Use an InitContainer to install a minimal Python runtime + requests, a Sidecar for TLS cert rotation, readiness/liveness probes, and a Namespace NetworkPolicy restricting egress to the internal endpoint. Provide a minimal YAML skeleton and discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Coinbase","IBM"]},{"id":"q-3915","question":"In CKAD terms, design a per-namespace Deployment that exposes a tiny HTTP API to store and retrieve JSON blobs using a PVC-backed path. Endpoints: POST /store with {id, blob}, GET /store/{id}. Use a ConfigMap named app-config with MAX_BLOB_SIZE, and a Secret named api-credentials. Include an InitContainer to install a minimal store tool, a Sidecar for TLS cert rotation, readiness/liveness probes, and a Namespace NetworkPolicy restricting egress to an internal auth service. Provide a minimal YAML skeleton and discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Hashicorp","PayPal"]},{"id":"q-4031","question":"In a CKAD scenario, in namespace dataops, create a CronJob named db-backup that runs daily at 02:00 UTC to back up a PostgreSQL database. Use a ConfigMap backup-config for BACKUP_DIR and DB_HOST and a Secret db-credentials for DB_USER and DB_PASSWORD. Mount PVC backup-pvc at /backups and write backups as /backups/<dbname>-YYYYMMDD.sql. Enforce concurrencyPolicy: Forbid, keep 7 successful jobs, and set activeDeadlineSeconds: 7200. Provide manifests and a straightforward validation and rollback plan?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["LinkedIn","Netflix"]},{"id":"q-4049","question":"In CKAD terms, implement a per-namespace BackupPlan pattern (CRD) that triggers backups of a StatefulSet's PVC to a per-namespace S3-like bucket. Define CRD fields for targetStatefulSet, sourcePVC, destBucket, destPrefix; show a minimal Job-based workflow with an InitContainer to install awscli, a Secret for AWS creds, a ConfigMap for BUCKET/PREFIX, and a PVC for logs. Include readiness/liveness probes and a minimal YAML skeleton; discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Hugging Face","Meta","MongoDB"]},{"id":"q-4184","question":"Design a per-namespace synthetic-traffic generator Deployment in CKAD terms: it reads TARGET_URL, PAYLOAD, and HEADERS from a Namespace ConfigMap, and a TOKEN from a Secret; an InitContainer installs curl. It should generate requests at a configurable rate, log each attempt to a PVC at /logs, and on failure move the payload to a DLQ on the same PVC. Include readiness/liveness probes and a CPU-based HPA; provide a minimal YAML skeleton and discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["DoorDash","Twitter","Zoom"]},{"id":"q-4196","question":"CKAD intermediate: In namespace prod, the 4-replica Deployment 'payments-api' must talk to an upstream processor at payments.proc.svc:443 using mTLS. Create minimal Kubernetes manifests to mount a Secret tls-secret containing client cert/key and CA, configure the container to use those certs, and restrict egress to only that endpoint via a NetworkPolicy. Show the patches and explain how you'd verify TLS and cert rotation?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Coinbase","IBM","Scale Ai"]},{"id":"q-4282","question":"In CKAD terms, design a per-namespace Job that migrates data from a PVC /data/migrate by reading CSVs with csvkit (InitContainer installs tools), sending batched JSON to http://data-portal:8080/api/v1/migrate. Configure MIG_BATCH_SIZE in mig-config ConfigMap and API token in mig-creds Secret. Use a PVC-stored marker for idempotence, set backoffLimit, and add readiness/liveness probes. Provide a minimal YAML skeleton and discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Instacart","LinkedIn"]},{"id":"q-4418","question":"In CKAD terms, design a per-namespace Deployment that runs a rate-limited API proxy inside the cluster. It should read its rate limit from a ConfigMap named app-config, authenticate upstream calls with a Secret named api-credentials, and initialize dependencies in an InitContainer. Include a TLS cert-rotation sidecar using a Secret named tls-certs, readiness/liveness probes, and a Namespace NetworkPolicy restricting egress to upstream.internal:8080. Provide a minimal YAML skeleton and discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["NVIDIA","Uber","Zoom"]},{"id":"q-4509","question":"CKAD intermediate: In namespace 'prod', a Deployment named 'image-processor' with 4 replicas experiences spikes under load. Propose and implement a minimal patch to: (a) add resource requests/limits and health probes; (b) configure an HPA targeting 75% CPU (min 3, max 10 replicas); (c) add a PodDisruptionBudget to keep at least 3 healthy pods; (d) tune rolling updates (maxUnavailable:1). Then describe how you'd validate under simulated load and rollback if needed?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["DoorDash","MongoDB","Tesla"]},{"id":"q-4635","question":"In CKAD terms, design a per-namespace Deployment that serves a static site via nginx. Use a PVC-backed volume mounted at /usr/share/nginx/html, an InitContainer to fetch assets from an internal artifact registry based on a ConfigMap site-config (INDEX_FILE, THEME), and a Secret site-basic-auth for HTTP basic auth. Include readiness/liveness probes, and a Namespace NetworkPolicy that restricts egress to the asset registry and internal DNS. Provide a minimal YAML skeleton and discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Cloudflare","DoorDash","Google"]},{"id":"q-4791","question":"CKAD intermediate: In namespace prod, Deployment payments-api with 4 replicas uses a vendor-base image that runs as root. Provide a minimal patch to enforce a strict security posture: container securityContext (runAsNonRoot: true, runAsUser: 1000, readOnlyRootFilesystem: true, allowPrivilegeEscalation: false, capabilities: drop: ['ALL']); PodSecurityContext (fsGroup: 2000); and include imagePullSecrets if the registry is private. Explain how you would validate under load and rollback if issues arise?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Meta","Scale Ai","Stripe"]},{"id":"q-858","question":"In a Kubernetes CKAD scenario, you have a Deployment named 'web-app' in namespace 'prod' with 3 replicas; pods frequently OOMKilled under load. Describe a practical debugging plan and provide a minimal manifest patch showing resource requests/limits, a readiness probe, and a liveness probe. Include scaling considerations and how you'd validate the fix under load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Hugging Face","LinkedIn","Snowflake"]},{"id":"q-1180","question":"Design a CKNE-aware per-tenant admission control for a multi-tenant real-time analytics gateway. Downstream CKNE health signals (queue depth, latency, error rate) are exposed via metadata. Propose a per-tenant health score, a dynamic token-bucket policy, and a cross-tenant shedding strategy that preserves fairness and SLA compliance. Include payload schemas, a minute-by-minute control loop, and a minimal sample payload?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Goldman Sachs","Meta","Snap"]},{"id":"q-1198","question":"Design a CKNE-aware per-tenant traffic shaping policy for a real-time collaboration platform (gateway -> engine -> persistence) servicing thousands of tenants with different SLAs. Edge CKNE health signals drive a minute-by-minute token-bucket shedding policy that prioritizes high-SLA tenants while gracefully degrading others; specify payload schemas and provide a minimal test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Discord","Snap"]},{"id":"q-1214","question":"Design a CKNE-aware data lineage policy for a three-stage ETL pipeline (ingest → transform → load) servicing thousands of tenants. Each hop attaches CKNE health in trace metadata. Propose a per-tenant degradation policy that preserves auditability for high‑SLA tenants while shedding heavy lineage data during degradation. Include payload schema, a minute-by-minute decision loop, and a minimal payload example?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Anthropic","Google","Scale Ai"]},{"id":"q-1280","question":"Scenario: A serverless workflow (API gateway -> orchestrator -> worker) serves thousands of tenants. Design a CKNE-aware tracing approach where every hop propagates a CKNE health signal in trace metadata and implement a per-tenant adaptive sampling policy that starts at 3% and scales to 25% during degradation. Include payload schemas, per-tenant health aggregation at the orchestrator, strategies to preserve trace fidelity during micro-bursts, and a minute-by-minute loop mapping health to sampling for the next window; provide a minimal payload example and a test plan?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Discord","Microsoft","Netflix"]},{"id":"q-1289","question":"Design a CKNE-aware canary rollout strategy for a multi-tenant image-resize API (ingest -> process -> deliver). Each tenant's requests carry CKNE health in headers. Propose a per-tenant rollout policy that starts at 5% canary, scales to 40% during healthy conditions, and reverts on degradation, with a minute-by-minute control loop. Include payload schemas, edge aggregation, and a minimal payload example?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Amazon","Meta","NVIDIA"]},{"id":"q-1377","question":"Design a CKNE-aware cross-region cache strategy for a multi-tenant real-time feed service (ingest -> compute -> deliver). Each tenant emits a CKNE health signal attached to requests. Propose per-tenant cache admission, TTLs, and prefetching depth that adapt minute-by-minute based on CKNE health, edge burst traffic, and tenant SLAs. Include payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Instacart","Slack"]},{"id":"q-1442","question":"Context: a multi-tenant mobile app with gateway -> dispatcher -> worker. Each tenant emits CKNE health in requests; design a CKNE-aware per-tenant notification dispatcher that throttles messages by a simple score-to-drop policy: score < 0.7 drops 10%, score < 0.5 drops 30%, otherwise none. Provide payload schemas, minute-by-minute decision loop, a minimal payload example, and a unit test to validate the policy?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Google","NVIDIA","Uber"]},{"id":"q-1554","question":"Design a CKNE-aware per-tenant event routing policy for a real-time analytics pipeline (ingest -> stream-processor -> dashboard) servicing thousands of tenants with varying SLAs. Each hop propagates a CKNE health signal. Propose a per-tenant routing policy that uses CKNE health, queue depth, and SLA tier to determine dynamic fan-out limits, with a minute-by-minute control loop and tenant-aware backpressure. Include payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Citadel","Coinbase","Meta"]},{"id":"q-1595","question":"Design a CKNE-aware per-tenant circuit-breaker policy for a real-time order routing system (gateway -> routing -> fulfillment) serving thousands of merchants. Each hop propagates a CKNE health signal. Propose a per-tenant policy that activates a progressive circuit breaker during degradation based on CKNE health, queue depth, and tenant SLA, with a minute-by-minute control loop; include payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["DoorDash","Tesla"]},{"id":"q-1632","question":"Design a CKNE-aware per-tenant cache TTL policy for a multi-tenant CDN path (gateway -> edge-cache -> origin) where each hop propagates CKNE health signals. Propose how TTLs and cache invalidation granularity vary by tenant health and SLA, with a minute-by-minute control loop. Include payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["OpenAI","PayPal","Snap"]},{"id":"q-1663","question":"Design a CKNE-aware multi-tenant ingestion pipeline (ingest → stream-processor → store) where edge CKNE health signals gate per-tenant throughput: batch size, forwardRaw vs enriched, and backpressure to enrichment services. Provide a minute-by-minute decision loop, per-tenant SLA handling, payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Apple","Databricks","Snowflake"]},{"id":"q-1755","question":"Design a CKNE-aware multi-tenant caching layer for a real-time recommendations pipeline (ingest -> rec-service -> storefront). Edge nodes propagate CKNE health; implement adaptive per-tenant TTLs, prefetch, and anti-stampede guards. Provide payload schemas, minute-by-minute decision loop, and a minimal payload example; include a test plan?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Meta","Slack"]},{"id":"q-1843","question":"Design a CKNE-aware privacy-gating policy for a multi-tenant streaming analytics pipeline (ingest -> processor -> store) that handles PII with per-tenant redaction levels tied to CKNE health. When health degrades, progressively increase redaction, throttle nonessential fields, and gate exports to dashboards. Describe payload schemas, a minute-by-minute control loop, and provide a minimal payload example?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["IBM","MongoDB","Oracle"]},{"id":"q-2169","question":"Design a CKNE-aware per-tenant rate-limiter for a real-time multi-tenant ingestion pipeline (ingest -> transform -> store). Edge CKNE health signals drive per-tenant quotas and burst handling. Propose how quotas are computed, payload schemas, a minute-by-minute control loop, and a minimal payload example; include a test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Lyft","Meta","Snowflake"]},{"id":"q-2289","question":"Design a CKNE-aware per-tenant feature-flag evaluation stack for a real-time personalization engine (ingest -> flag-service -> edges). Each hop propagates CKNE health; implement per-tenant fidelity tiers (A full, B approximate with sampling, C default) driven by SLA and CKNE vector. Ensure deterministic minute-level sampling, edge caching, and per-tenant fallbacks while preserving latency. Provide payload schemas, a minute-by-minute decision loop, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Amazon","PayPal","Two Sigma"]},{"id":"q-2319","question":"Design a CKNE-aware per-tenant resource isolation policy for a real-time multi-tenant data-processing pipeline (ingest -> enrich -> analytics) serving tenants with different SLAs. CKNE health signals propagate at each hop; specify dynamic per-tenant CPU/memory quotas, adaptive throttling, and backpressure during degradation. Include payload schemas, a minute-by-minute decision loop, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Amazon","Citadel","Lyft"]},{"id":"q-2352","question":"Design a CKNE-aware per-tenant deduplication and backpressure policy for a real-time log ingestion pipeline (agents -> collector -> indexer) servicing thousands of tenants. Each hop propagates a CKNE health signal. Propose a per-tenant dedup window, dynamic backpressure thresholds based on CKNE, and a minute-by-minute health-to-throttle loop that preserves high-SLA data while shedding during degradation. Include payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Apple","Bloomberg","IBM"]},{"id":"q-2384","question":"Design a CKNE-aware per-tenant retry and idempotency policy for a streaming ingestion path (ingest -> processor -> warehouse) serving thousands of tenants. Each hop propagates CKNE health. Propose: 1) how to encode CKNE into message headers, 2) per-tenant retry budgets and adaptive backoff with jitter responsive to CKNE and queue depth, 3) a robust idempotency strategy with a tenant-scoped dedupe cache and TTLs aligned to SLA, 4) a minute-by-minute control loop for budget and TTL adjustments, 5) concrete payload schemas and a minimal payload example, 6) a test plan and observability hooks?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Instacart","LinkedIn","Oracle"]},{"id":"q-2422","question":"Design a CKNE-aware cross-tenant telemetry sharing layer: publishers -> broker -> consumers in a real-time multi-tenant pipeline. Each hop adds CKNE health; implement a degradation policy that prioritizes high-SLA tenants, drops low-priority tenants, and tunes per-tenant sampling and backpressure minute-by-minute. Include payload schemas, tenant priorities, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Apple","Meta","Snap"]},{"id":"q-2475","question":"Design a CKNE-aware per-tenant job scheduler for a multi-tenant data pipeline (ingest -> queue -> worker). Each job carries a CKNE health score; implement a minute-by-minute policy to deprioritize high-CKNE tenants, pause new jobs for degraded tenants, and preempt long-running low-priority jobs when total queue depth exceeds a threshold. Include payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Anthropic","Google","MongoDB"]},{"id":"q-2504","question":"In a CKNE-enabled multi-tenant data ingestion pipeline (Ingest -> Processor -> Store), design a per-tenant backpressure mechanism that uses CKNE health to throttle high-SLA tenants vs. low-priority tenants. Edge CKNE signals propagate upstream; implement a minute-by-minute control loop that adjusts per-tenant request rates and queue depths. Include payload schemas, a minimal payload example, per-tenant SLA map, and a concrete test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Google","IBM","OpenAI"]},{"id":"q-2540","question":"Design a CKNE-aware per-tenant autoscaling policy for a multi-tenant streaming pipeline (ingest -> stream-processor -> analytics). Edge and processing nodes propagate CKNE health; specify metrics, scaling rules, throttling, fault isolation, and how you'd test it in a production-like environment?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Airbnb","Salesforce","Zoom"]},{"id":"q-2604","question":"Design a CKNE-aware per-tenant feature-flag controller for a real-time analytics cockpit (ingest -> processor -> dashboard). Each tenant has features with SLAs. Build a policy: when a tenant’s CKNE health drops, automatically disable non‑critical features, throttle telemetry sampling, and hide non-essential widgets while keeping baseline latency under 150 ms. Include payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Google","Instacart"]},{"id":"q-2659","question":"Design a CKNE-aware multi-tenant batch scheduler for a queue (ingest -> scheduler -> workers). Each job carries tenant_id and a CKNE health score (0–1). Explain how to map CKNE to per-tenant priority, implement a minute-by-minute degradation policy prioritizing high-SLA tenants, and provide a minimal payload example plus a basic test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Airbnb","Hashicorp","Stripe"]},{"id":"q-2673","question":"Design a CKNE-aware per-tenant feature rollout controller for an API gateway serving multiple tenants. Each tenant has a CKNE health score (0–1). Explain how to map CKNE to per-tenant feature exposure (0–100%), implement a minute-by-minute degradation loop that lowers exposure for low-CKNE tenants while keeping high-CKNE tenants fully exposed, and provide payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Airbnb","MongoDB"]},{"id":"q-2698","question":"Design a CKNE-aware per-tenant API rate limiter at an edge gateway that enforces per-tenant QoS using CKNE scores to scale capacity. How would you map CKNE to per-tenant capacity, implement a minute-by-minute degradation loop, and provide a minimal payload example plus a test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Databricks","Meta","Robinhood"]},{"id":"q-2895","question":"Design a CKNE-aware training runner for a shared GPU cluster servicing multiple tenants. Each tenant has a CKNE health score (0–1). Explain how to map CKNE to per-tenant training priority, implement a minute-by-minute degradation loop that throttles low-CKNE tenants (fewer workers, smaller concurrent trials) while preserving high-CKNE tenants, and specify fairness guarantees, preemption rules, and telemetry. Include a minimal payload example and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Databricks","Google"]},{"id":"q-2928","question":"Design a CKNE-aware per-tenant traffic router in a service mesh (Istio/Linkerd). Each tenant has a CKNE score (0–1). Describe a minute-by-minute degradation loop that reduces traffic to low-CKNE tenants while preserving high-CKNE tenants, with a concrete routing payload example and a minimal test plan?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["LinkedIn","Plaid"]},{"id":"q-2977","question":"Design a CKNE-aware ingestion gateway for a multi-tenant streaming pipeline where each tenant publishes events with a CKNE health score (0–1). How would you map CKNE to per-tenant ingest caps, implement a minute-by-minute degradation loop that trims low-CKNE tenants and rebalances capacity, ensure fairness and preemption, and what would payloads and tests look like?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Airbnb","Anthropic","DoorDash"]},{"id":"q-3056","question":"Design a CKNE-aware per-tenant API quota limiter for a shared REST gateway serving multiple tenants. Each tenant has a CKNE health score (0–1). Explain how to map CKNE to per-tenant QPS caps, implement a minute-by-minute degradation loop that reduces low-CKNE tenants' quotas and reallocates capacity to high-CKNE tenants, plus a minimal payload example and a basic test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["OpenAI","Plaid"]},{"id":"q-3171","question":"Design a CKNE-aware multi-tenant analytics query scheduler for a shared data lake engine. Each tenant has a CKNE score (0–1). Explain mapping CKNE to per-tenant concurrency and memory budgets, implement a minute-by-minute degradation loop throttling low-CKNE tenants and rebinding to high-CKNE tenants, specify fairness guarantees, preemption rules, telemetry, a minimal query payload, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Amazon","Instacart","Meta"]},{"id":"q-3194","question":"Design a CKNE-aware notebook runner for a shared JupyterHub-like environment serving multiple tenants. Each tenant has a CKNE score (0–1). Explain how to map CKNE to per-tenant CPU/memory quotas, implement a minute-by-minute degradation loop that throttles low-CKNE tenants (e.g., pause idle kernels, reduce concurrent notebooks) and rebinding to high-CKNE tenants, specify fairness guarantees, preemption rules, telemetry, and a minimal notebook payload example plus a test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Netflix","Snowflake"]},{"id":"q-3285","question":"Design a CKNE-aware streaming ingestion service for a multi-tenant data lakehouse. Each tenant has a CKNE score (0–1) representing reliability and urgency. Specify how CKNE maps to per-tenant ingress rate and shard assignment, and implement a minute-by-minute degradation loop that throttles low-CKNE tenants (e.g., pause non-critical streams, shrink per-tenant concurrency) while preserving progress for high-CKNE tenants. Include fairness guarantees, preemption rules, telemetry, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Coinbase","Discord","Snowflake"]},{"id":"q-3505","question":"Design a CKNE-aware real-time stream processing pipeline for a shared platform (e.g., Flink) serving multiple tenants. Each tenant has a CKNE score 0–1. Explain how CKNE maps to per-tenant parallelism, windowing, and checkpoint cadence, then implement a minute-by-minute degradation loop throttling low-CKNE tenants and rebinding to high-CKNE tenants while preserving a baseline SLA. Include fairness guarantees, preemption rules, telemetry, a minimal payload example: payload: {tenant:'A',stream:'events',ckne:0.25} and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["DoorDash","Netflix"]},{"id":"q-3631","question":"Design a CKNE-aware multi-tenant model-serving gateway for a shared inference cluster hosting LLM-style models. Tenants have CKNE scores 0–1. Explain mapping CKNE to per-tenant concurrent inferences, token budgets, and model choice; implement a minute-by-minute degradation loop that reallocates capacity toward high-CKNE tenants while meeting baseline SLA, with fairness guarantees, preemption rules, telemetry, and a minimal payload example?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["IBM","Oracle","Plaid"]},{"id":"q-3686","question":"Design a CKNE-aware multi-tenant CI/CD runner fleet on Kubernetes. Each tenant has a CKNE score (0–1). Explain how CKNE maps to per-tenant concurrent jobs, runner isolation, and retry budgets, then implement a minute-by-minute degradation loop that throttles low-CKNE tenants and rebinds runners to high-CKNE tenants while preserving a baseline SLA. Include a minimal payload example and a test plan?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Citadel","Hashicorp","Slack"]},{"id":"q-3923","question":"Design a CKNE-aware shared feature store for real-time model scoring across multiple tenants. Each tenant has a CKNE score (0–1). Explain how CKNE maps to per-tenant cache sizing and prefetching, then implement a minute-by-minute rebalance that shifts cached features from low-CKNE tenants to high-CKNE tenants while preserving a baseline latency SLA, with fairness guarantees, preemption rules, telemetry, and a minimal payload example?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Citadel","Google"]},{"id":"q-3990","question":"Design a CKNE-aware multi-tenant in-memory cache layer (e.g., Redis-like) for a high-traffic service mesh. Each tenant has a CKNE score in [0,1]. Specify how CKNE maps to per-tenant cache quotas (bytes), eviction priorities, and burst protection. Propose a minute-by-minute degradation loop that reclaims cache from low-CKNE tenants during spikes and rebinds to high-CKNE tenants while preserving baseline latency. Include telemetry, fairness guarantees, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Meta","OpenAI","Tesla"]},{"id":"q-4010","question":"Design a CKNE-aware multi-tenant model serving platform for online inference. Each tenant has CKNE score 0–1. Explain how CKNE maps to per-tenant latency budgets, max concurrency, and batch sizing in a shared GPU cluster; implement a minute-by-minute degradation loop: throttle low-CKNE tenants by shrinking batch size and request rate, rebinding to high-CKNE tenants while preserving SLA. Include telemetry, fairness, preemption rules, minimal payload, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Amazon","PayPal"]},{"id":"q-4152","question":"Design a CKNE-aware per-tenant cache tiering policy for a shared in-memory cache (e.g., Redis cluster) serving multiple tenants. Each tenant has a CKNE score (0–1). Explain how CKNE maps to per-tenant cache memory budgets, eviction strategies, and shard allocation, then implement a minute-by-minute degradation loop that throttles low-CKNE tenants and rebinds capacity to high-CKNE tenants while preserving a baseline SLA. Include a minimal payload example and a test plan?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Airbnb","Google","Snowflake"]},{"id":"q-4174","question":"Design a CKNE-aware cache+planner for a shared feature store (Redis-backed). Map CKNE to per-tenant cache quotas and concurrent query slots. Implement a minute-by-minute loop: evict low-CKNE tenants’ cache and trim concurrency, rebinding capacity to high-CKNE tenants while preserving a baseline SLA. Include fairness, preemption, telemetry; payload: {tenant:'T25',feature:'user_last_seen',ckne:0.18,query:'SELECT ...'}. Test plan: unit eviction tests and SLA-focused integration with mixed load?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Microsoft","Snowflake","Two Sigma"]},{"id":"q-4347","question":"Design a CKNE-aware serverless function scheduler for a shared FaaS platform serving multiple tenants. Each tenant has a CKNE score (0–1). Explain how CKNE maps to per-tenant invocation budgets, max concurrency, and cold-start allowances, then describe a minute-by-minute degradation loop that throttles low-CKNE tenants (delay/drop invocations) and reallocates capacity to high-CKNE tenants while preserving a baseline SLA. Include fairness guarantees, preemption rules, telemetry, a minimal payload example like {tenant:'T1',function:'resize',ckne:0.35} and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Discord","Microsoft","Slack"]},{"id":"q-4443","question":"Design a CKNE-aware multi-tenant CI/CD runner scheduler for a shared CI pool backing hundreds of pipelines. Each tenant has a CKNE score (0–1). Explain how CKNE maps to per-tenant build parallelism, cache usage, artifact retention, and per-job quotas. Propose a minute-by-minute degradation loop that throttles low-CKNE tenants and rebinds slots to high-CKNE tenants while preserving a baseline SLA. Include fairness guarantees, preemption rules, telemetry, a minimal payload example: payload: {tenant:'A',job:'build',ckne:0.25} and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Bloomberg","Cloudflare","Plaid"]},{"id":"q-4523","question":"Design a CKNE-aware rate limiter for a shared API gateway servicing hundreds of tenants. Each tenant has a CKNE score in [0,1]. Explain how CKNE maps to per-tenant QPS, token bucket size, and burst handling, and propose a minute-by-minute degradation loop that throttles low-CKNE tenants and reallocates tokens to high-CKNE tenants while preserving a baseline SLA. Include fairness guarantees, preemption rules, telemetry, a minimal payload example: payload: {tenant:'A',endpoint:'/orders',ckne:0.25} and a test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Airbnb","Lyft","NVIDIA"]},{"id":"q-4552","question":"Design a CKNE-aware shared feature store serving multiple ML models across tenants. Each tenant has CKNE 0–1. Explain mapping CKNE to per-tenant feature-cache budgets, prefetch depth, and TTLs. Propose a minute-by-minute degradation loop that throttles low-CKNE tenants by reducing prefetch depth and cache residency, while rebinding capacity to high-CKNE tenants, ensuring SLA, fairness, and safe preemption rules. Include telemetry, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Apple","Databricks"]},{"id":"q-4675","question":"Design a CKNE-aware multi-tenant FaaS scheduler for a shared edge region (serverless platform powering responsive microservices). Each tenant has a CKNE score 0–1. Explain how CKNE maps to per-tenant function concurrency, memory budgets, and cold-start behavior. Propose a minute-by-minute degradation loop that throttles low-CKNE tenants (e.g., cap concurrent invocations, extend cold-start latency) and rebinding capacity to high-CKNE tenants while preserving a baseline SLA. Include a minimal payload example: payload: {tenant:'A',fn:'checkout',ckne:0.25} and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Amazon","Bloomberg","PayPal"]},{"id":"q-4807","question":"Design a CKNE-aware API gateway for a multi-tenant SaaS platform (Robinhood, Bloomberg) handling REST/GraphQL. Each tenant has a CKNE score 0–1. Explain how CKNE maps to per-tenant rate limits, queueing, and circuit-breakers, then outline a minute-by-minute degradation loop that throttles low-CKNE tenants and rebinds traffic to high-CKNE tenants while preserving a baseline SLA. Include fairness guarantees, preemption rules, telemetry, and a minimal payload example: payload: {tenant:'A',endpoint:'/accounts',ckne:0.25,method:'GET'} and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Bloomberg","Robinhood"]},{"id":"q-846","question":"Design a real-time CKNE failure detector for a distributed microservice mesh. Specify the data pipeline, latency budget, how you compute p95 latency and error rate, and how you implement replay and backpressure for fault tolerance. Include testing strategies and production validation to demonstrate correctness and resilience?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Coinbase","Meta","NVIDIA"]},{"id":"q-861","question":"Design an adaptive CKNE-aware tracing and sampling strategy for a real-time order-processing pipeline in a multi-tenant mesh. Explain how CKNE health signals influence sampling decisions, how you preserve trace fidelity under bursts, and how you quantify the overhead and impact on latency. Include concrete data structures and an example workflow?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["PayPal","Snap","Tesla"]},{"id":"q-954","question":"Scenario: A three-service order flow (API gateway -> inventory -> payment) runs in one region. Design a CKNE-aware tracing approach where each service propagates a CKNE health signal in trace metadata and employs an adaptive sampling policy: base 10% with a health-adjusted factor that can raise sampling to 50% during degradation. Specify data structures for per-service health, the trace metadata payload, and a minute-by-minute workflow to compute health and adjust sampling for the next window. Provide a minimal code snippet showing the payload and health update logic?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Goldman Sachs","IBM"]},{"id":"q-991","question":"Design a CKNE-aware tracing strategy for a real-time ad bidding pipeline (gateway -> bidding service -> settlement) servicing multi-tenant advertisers. Each leg propagates a CKNE health signal; implement an adaptive sampling policy that scales from 5% baseline to 60% during degradation, with per-tenant health aggregation at the edge. Specify payload schemas, how to preserve trace fidelity under micro-burst traffic, and a minute-by-minute workflow for health-to-sampling decisions; provide a minimal payload example and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["NVIDIA","Netflix","Stripe"]},{"id":"q-1121","question":"Scenario: You operate a shared Kubernetes cluster serving multiple product teams. You must prevent cross-namespace data leakage and enforce least-privilege access while remaining auditable and scalable. Describe a concrete strategy using either OPA Gatekeeper or Kyverno for admission control (with at least two constraints), implement namespace RBAC boundaries, apply Calico NetworkPolicy for namespace isolation, and outline a monitoring/audit plan with tests and runbooks. Include example policies and a minimal test commands snippet?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Google","IBM","Snap"]},{"id":"q-1130","question":"You're running a Kubernetes cluster for a web app. A Pod mounting hostPath and running as root was detected in dev. Outline a practical plan to enforce least privilege across namespaces (Baseline/Restricted) using a policy engine (Kyverno or OPA Gatekeeper) and show how you would validate enforcement without disrupting workloads. What steps and files would you use?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["Amazon","Google"]},{"id":"q-1167","question":"Scenario: You operate a multi-cluster Kubernetes data platform (cloud+on‑prem) where a Spark job can access customer data. Design an end-to-end approach to detect, prevent, and respond to data exfiltration attempts from pods across clusters. Include policy design, telemetry signals, enforcement, and incident runbooks; discuss trade-offs?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["Bloomberg","Instacart"]},{"id":"q-1278","question":"Scenario: A fintech data platform runs a multi-tenant data lake on Kubernetes. Each data job uses per-job ServiceAccounts to access restricted cloud storage. A rogue pod tries to exfiltrate data via the bucket. Propose a security approach that binds each pod to a dedicated cloud IAM role (workload identity), enforces namespace-scoped permissions, and provides tamper-evident audit trails. Include detection and response for abnormal egress and a safe rotation plan. What trade-offs?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["DoorDash","Robinhood"]},{"id":"q-1301","question":"You're debugging a Kubernetes deployment in a multi-tenant environment where one namespace's pods delay startup by several minutes. Provide a practical, beginner-friendly diagnostic flow focusing on pod events, init containers, image pulls, and config maps. List concrete kubectl commands you would run and how you’d determine the root cause?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["Anthropic","Netflix","Twitter"]},{"id":"q-1323","question":"In a Kubernetes cluster deploying an ML inference service, models and weights live in a private registry. Outline a practical plan to sign models with cosign, publish attestations, and enforce runtime verification so only attested models can be deployed via GitOps (Argo CD) and an enforcement policy (OPA Gatekeeper or Kyverno). Include concrete commands and sample configuration?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Hugging Face","PayPal","Uber"]},{"id":"q-1356","question":"You manage a Kubernetes cluster hosting regulated data across tenants. Design a practical end-to-end plan to enable at-rest encryption with envelope encryption using a cloud KMS, protect both API server data and etcd data, rotate keys safely, and prove compliance via CI checks. Include concrete commands and sample manifests?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Amazon","Google","Uber"]},{"id":"q-1374","question":"Scenario: A multi-cloud platform runs Kubernetes on EKS and serverless runtimes; ensure only cosign-signed images with verifiable attestations can be deployed across all targets. Describe an end-to-end plan to enforce cross-registry provenance, automate Rekor attestations, and integrate with a GitOps workflow (Argo CD), including concrete commands and sample policy rules?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Discord","Meta","Tesla"]},{"id":"q-1398","question":"In a multi-tenant Kubernetes cluster with images stored in a private OCI registry, design an end-to-end workflow to sign images with cosign, publish attestations, and enforce that only attested images are deployed. Include concrete commands, CI hints, and an admission control policy snippet?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Cloudflare","Discord","Google"]},{"id":"q-1444","question":"Scenario: A poly-cloud serverless stack runs AWS Lambda, GCP Cloud Functions, and Azure Functions. CI/CD signs each function package and its dependencies with cosign and publishes SLSA attestations to a central registry. Deployments must be allowed only if attested across all clouds. Describe an end-to-end plan, including concrete commands and sample configs for signing, attesting, and cross-cloud enforcement?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Amazon","LinkedIn","Netflix"]},{"id":"q-1522","question":"Scenario: In a multi-tenant SaaS, a central migration service applies Flyway SQL scripts to dozens of PostgreSQL instances. How would you implement a concrete plan to sign each migration with cosign in CI, publish attestations to a registry, and enforce at runtime that only attested migrations are executed? Include concrete signing commands, attestation storage approach, and a sample enforcement policy (Kyverno/OPA) plus integration steps with Flyway?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Databricks","Slack","Uber"]},{"id":"q-1551","question":"In a Kubernetes-based data platform hosting a multi-tenant ML feature store exposed via a high-volume API, design a privacy-preserving, end-to-end audit trail to support forensics without exposing PII. Specify architecture, RBAC/ABAC controls, OpenTelemetry instrumentation, log pipeline (Fluentd/Loki), encryption, data redaction, retention, and how you’d run production-scale incident drills. What would you monitor first and why?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["Cloudflare","LinkedIn","Two Sigma"]},{"id":"q-1691","question":"Scenario: A Terraform-driven multi-tenant cloud platform provisions resources across clouds. You must sign every Terraform plan in CI with cosign, publish an attestation to a central registry, and enforce at runtime that only attested plans are applied by the GitOps flow. Describe an end-to-end approach with concrete signing commands, attestation storage layout, an sample OPA policy, and integration steps with the deployment pipeline?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Netflix","Stripe"]},{"id":"q-1734","question":"In a multi-cloud Terraform deployment across AWS, GCP, and Azure, a central registry hosts official modules. Propose a concrete plan to: - sign every module and its dependencies with cosign during CI, - publish attestations to a provenance registry, - enforce at plan/apply time that only attested modules are used via an OPA policy or equivalent gate, including concrete signing commands, storage layout for attestations, and a sample enforcement policy with integration steps?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Airbnb","Apple","Salesforce"]},{"id":"q-1846","question":"In a Tesla-scale Databricks lakehouse on AWS, with Unity Catalog and a Kubernetes data plane, contractors routinely export aggregated datasets to external S3 buckets. Design a detection and response mechanism that distinguishes legitimate exports from exfiltration attempts. Include telemetry (Unity Catalog audit logs, IAM activity, Delta table operations), thresholds, alerting, and an incident playbook?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["Databricks","Tesla"]},{"id":"q-1964","question":"In a multi-tenant notebook service on Kubernetes, each notebook runs in a transient pod and pulls dependencies from a private registry. Design a concrete plan to sign the notebook artifact (notebook.ipynb) and its dependencies with cosign, publish SLSA attestations to a central registry, and enforce at runtime that only attested notebooks and dependencies are allowed to run. Include concrete signing commands, attestations storage, and a sample Kyverno/OPA policy plus integration steps with the notebook runner?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Anthropic","Citadel","Microsoft"]},{"id":"q-2092","question":"In a Kubernetes-based data-analytics platform with two namespaces, dev and prod, design a practical RBAC and Pod Security setup for a new microservice 'data-processor' to ensure it can only read its own ConfigMaps and Secrets, runs as a non-root user, and cannot escalate privileges. Provide concrete manifest fragments (RBAC, ServiceAccount, RoleBinding, PodSecurityContext) and describe how to validate at admission time that all pods comply?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["Adobe","Databricks","DoorDash"]},{"id":"q-2128","question":"In a high-sensitivity environment spanning on-prem and cloud, design a tamper-evident Kubernetes audit logging and incident-response pipeline that preserves evidence from the API server, etcd, and kubelets while enabling automated triage and containment during a breach. Specify data formats, forwarding targets, non-repudiation measures, and failure modes?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["Adobe","PayPal","Tesla"]},{"id":"q-2188","question":"In a monorepo-managed Kubernetes fleet with per-service Helm charts and shared base images, design an attestation model where base image attestations are linked to derived service images via SBOM, and enforcement ensures every deployed chart references an attested base. Describe signing commands, SBOM workflow, attestation storage, and a gating policy (OPA/Kyverno) with CI/CD integration?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Citadel","Zoom"]},{"id":"q-2418","question":"A data ingestion pipeline uses a private artifact registry hosting Spark JARs and Python wheels. Each artifact is cosign-signed with a SBOM attestation; runtime must guarantee that only attested artifacts execute in Spark jobs managed by Airflow on Kubernetes. Describe an end-to-end plan to sign, publish attestations, and enforce runtime checks across the data plane, including concrete cosign commands, attestation storage approach, and a sample Kyverno/OPA policy plus integration steps with Airflow and the registry?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Bloomberg","OpenAI"]},{"id":"q-2508","question":"In a Knative-based FaaS layer on Kubernetes, functions are built as OCI images stored in a private registry and loaded by the function runtime at cold start. Design an end-to-end plan to sign and attest function images, publish attestations to Rekor, and enforce at runtime that only attested functions execute in the Knative namespace. Include concrete cosign commands, attestation storage approach, and a sample Kyverno or OPA policy plus integration steps with the registry and the function loader?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["Google","LinkedIn","Plaid"]},{"id":"q-2537","question":"In a policy-driven edge compute fleet, WASM modules are built per customer and pushed to a private OCI registry. You must ensure only attested modules run on edge devices. Design a concrete plan to sign modules with cosign, publish SLSA attestations, and enforce at runtime that only attested modules are loaded via an admission controller (Kyverno/OPA). Include concrete commands and sample policies?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Anthropic","Google","Instacart"]},{"id":"q-2639","question":"In an AWS EKS cluster with Istio, an attacker attempts DNS tunneling to exfiltrate data from the dev namespace. Design a practical detection pipeline using: (a) eBPF-based DNS egress monitoring, (b) CoreDNS query analytics, (c) Istio telemetry + network policies, and (d) alerting aligned to MITRE techniques. Explain data flow, signals to watch, and how you would validate alerts?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["Hashicorp","Netflix","Salesforce"]},{"id":"q-2687","question":"Scenario: A Kafka Connect cluster loads dozens of custom connectors from a shared registry. Describe a concrete plan to sign each connector JAR and its JSON configs with cosign, publish attestations to Rekor, and enforce at deployment and runtime that only attested connectors are loaded. Include concrete signing commands, attestation storage layout, and a sample Kyverno/OPA policy and integration steps with Kafka Connect?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["IBM","Instacart","Zoom"]},{"id":"q-2747","question":"Scenario: An edge compute platform runs user-provided WASM modules on 100 edge nodes. A compromised module could exfiltrate data or break isolation. Describe a concrete plan to sign each WASM module and its metadata with cosign in CI, publish attestations to Rekor, and enforce at deployment and runtime that only attested WASMs are loaded, including exact signing commands, attestation layout, and a sample Kyverno/OPA policy plus integration steps with the edge runtime?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Discord","Slack","Twitter"]},{"id":"q-2837","question":"In a multi-tenant Kubernetes cluster used by several large services, you must build a scalable, privacy-conscious audit-logging and anomaly-detection pipeline that ingests API server audit events from multiple masters, normalizes per-tenant identity, and emits near-real-time alerts for suspicious access patterns (e.g., sudden spikes in secret fetches, impersonation). Outline the design, components, and trade-offs?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["DoorDash","Google","Zoom"]},{"id":"q-2843","question":"Scenario: A data platform ingests dozens of datasets into a lakehouse from multiple teams. Design a concrete plan to sign each dataset file with cosign, publish attestations to Rekor, and enforce at read-time that only attested datasets are processed by downstream Airflow pipelines and Looker dashboards. Include concrete signing commands, attestation storage layout, and a sample Kyverno/OPA policy integration with Airflow?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Bloomberg","Citadel","Square"]},{"id":"q-2911","question":"In a private Python package index used by Apple and MongoDB, design a beginner CI workflow to generate CycloneDX SBOMs for wheels, cosign-sign artifacts, publish attestations to Rekor, and enforce that only attested wheels can be uploaded. Include concrete cosign commands, attestation storage layout, and a sample Kyverno/OPA policy?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["Apple","MongoDB"]},{"id":"q-3034","question":"In a Kubernetes-based edge compute platform, workers pull WebAssembly plugins from a registry to extend behavior. Describe a concrete plan to sign WASM plugins with cosign, publish attestations to Rekor, and enforce at runtime that only attested WASM plugins are loaded by the plugin loader. Include concrete signing commands, attestation storage layout, and a sample Kyverno/OPA policy plus integration steps with the plugin loader?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Instacart","Microsoft","NVIDIA"]},{"id":"q-3076","question":"In a Kubernetes cluster with Kyverno installed, draft a beginner policy to ensure all pods in namespace secure cannot run as root, require runAsNonRoot true and runAsUser 1000, enforce readOnlyRootFilesystem, disallow privileged containers and added capabilities, require imagePullPolicy Always, and images from registry example.com/secure/*. Provide the Kyverno policy YAML, an example compliant Pod spec and a noncompliant one, and the validation steps?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["DoorDash","LinkedIn"]},{"id":"q-3103","question":"You operate a multi-tenant data science platform on Kubernetes used by multiple teams across Uber and Robinhood; design a concrete, automated control plane to enforce per-tenant data access, runtime isolation, and secrets management. Include: (1) a policy framework using Open Policy Agent (OPA) or Kyverno, (2) secret rotation and access controls with Kubernetes Secrets and CSI Secrets Store, (3) namespace network egress controls and audit logging to Rekor, (4) a test plan with compliant vs noncompliant examples, and (5) an integration with CI to attest policy conformance. How would you implement this end-to-end?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["Robinhood","Uber"]},{"id":"q-3222","question":"Scenario: A Kubernetes-based plugin host loads third‑party data processors at runtime. Design a concrete plan to cosign sign each plugin binary and its manifest, publish attestations to Rekor, and enforce at load time that only attested plugins are admitted by a loader gatekeeper with OPA/Kyverno. Include concrete signing/verification commands, attestation layout, and sample policy?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["NVIDIA","Salesforce","Twitter"]},{"id":"q-3298","question":"In a security-focused edge compute platform with 50k devices, function bundles are delivered as WebAssembly modules from a private registry. Propose a concrete plan to sign WASM bundles and their JSON manifests with cosign, publish Rekor attestations, and enforce at load time that only attested bundles are executed. Include exact signing commands, an attestation storage layout, and a sample Kyverno/OPA policy plus integration steps with the edge runtime?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Apple","Coinbase","Meta"]},{"id":"q-3423","question":"Scenario: A multi-tenant data platform deploys user-defined data processing plugins as containerized extensions loaded by a central registry. Design a concrete plan to sign each plugin image and its JSON config with cosign, publish attestations to Rekor, and enforce at runtime that only attested plugins are loaded by the plugin host. Include concrete signing commands, attestation storage layout, and a sample Kyverno/OPA policy and integration steps with the plugin loader?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Hugging Face","IBM","Snap"]},{"id":"q-3474","question":"In a data lake, design a practical plan to sign each ingested dataset with cosign, publish Rekor attestations, and enforce at read time that only attested data is processed by Spark/Presto, including integration with a data catalog and an OPA policy. Include concrete commands and an artifact layout?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Amazon","Plaid"]},{"id":"q-3561","question":"Design a lightweight runtime security check in Kubernetes using Falco to alert on containers running as root or spawning a shell; provide two concrete Falco rules (compliant vs noncompliant), a minimal test Pod manifest to trigger the alert, and a verification plan to confirm alerts route to the chosen sink?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["Hugging Face","MongoDB","PayPal"]},{"id":"q-3674","question":"Scenario: An edge platform runs WebAssembly (WASM) modules pushed from a central registry to thousands of devices. Devs want to ensure only attested modules load. Describe a concrete plan to cosign-sign each WASM module and its metadata, publish attestations to Rekor, and enforce at the loader level that only attested modules execute. Include concrete signing commands, attestation storage layout, and a sample policy (Kyverno/OPA) plus edge-loader integration steps?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Coinbase","DoorDash","Hashicorp"]},{"id":"q-3768","question":"Advanced interview: In a production ML platform used by Tesla, Databricks, and PayPal, ensure every ML model artifact (weights, config) and its data dependencies are signed, attested, and verifiable before deployment to inference endpoints. Design a policy-driven end-to-end pipeline: 1) signing workflow with Cosign, 2) attestation storage with Rekor, 3) pre-deployment checks enforcing Rekor attestations and an SBOM, and 4) a test plan that detects tampering and blocks deployment. Include concrete Cosign commands and a sample Rekor layout?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["Databricks","PayPal","Tesla"]},{"id":"q-3949","question":"In a multi-tenant SaaS, Terraform modules are published to a private registry. Propose a practical plan to cosign-sign each module package and its accompanying values.json, publish attestations to Rekor, and enforce at plan/apply that only attested modules are used. Include concrete signing commands, attestation storage layout, and a sample Kyverno/OPA policy plus Terraform integration steps?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["MongoDB","Netflix"]},{"id":"q-3961","question":"Scenario: An extensible web server loads dynamic plugins (shared objects) from a private registry at runtime. Design a concrete plan to sign each plugin artifact and its manifest with cosign, publish attestations to Rekor, and enforce at runtime that only attested plugins are loaded. Include concrete signing commands, attestation storage layout, and a sample Kyverno/OPA policy and integration steps with the plugin loader?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Google","Robinhood","Slack"]},{"id":"q-4062","question":"Scenario: A data platform spanning Snowflake and IBM workloads runs Spark jobs in Kubernetes and must enforce that only pods with SPIRE-backed identities can call the Kubernetes API. Design a runtime access-control solution using OPA Gatekeeper with RBAC, SPIRE, and a mutating webhook to inject auditing sidecars. Provide a concrete constraint template, an example compliant Pod, a noncompliant Pod, and a validation plan?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["IBM","Snowflake"]},{"id":"q-4109","question":"In a Kubernetes cluster, implement a Gatekeeper policy that enforces container image provenance: images must come from an allowlisted registry and include a digest; provide YAML for ConstraintTemplate and Constraint, a compliant Pod manifest, a noncompliant Pod manifest, and a validation plan using kubectl?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["Lyft","Plaid"]},{"id":"q-4165","question":"In a Kubernetes-based fintech platform, dozens of webhook-handler microservices are deployed from a private registry. Propose a concrete plan to sign both container images and their Kubernetes manifests with cosign, publish attestations to Rekor, and enforce at admission and runtime that only attested artifacts are deployed and loaded. Include concrete signing commands, attestation storage layout, and a sample Kyverno/OPA policy plus CI steps for the registry and admission controls?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Plaid","Square"]},{"id":"q-4281","question":"Scenario: A multi-tenant SaaS platform hosts user-contributed ML model plugins that load at runtime in isolation. Design a concrete plan to sign each model artifact (e.g., .onnx, .pt) and its metadata with cosign, publish attestations to Rekor, and enforce at load time that only attested models are accepted by the model loader. Include concrete signing commands, attestation storage layout, and a sample OPA policy and loader integration steps?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Discord","Lyft","Twitter"]},{"id":"q-4368","question":"In a multi-tenant Kubernetes cluster, design a defence-in-depth strategy to prevent API abuse and runtime policy violations that bypass admission controls. Outline how you would combine: 1) OPA constraints enforcing image provenance via Rekor cosign attestations; 2) Kyverno mutation/validation to enforce non-root, readOnlyRootFilesystem, and to block unsigned images; 3) eBPF/Falco runtime checks for anomalous API usage; 4) a verification plan with a minimal test manifest. Provide concrete policy fragments and test steps?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["Databricks","Hashicorp","Snap"]},{"id":"q-4397","question":"Scenario: A shared Kubernetes cluster has test and prod namespaces. Draft a minimal, beginner-friendly NetworkPolicy in the test namespace that isolates it from prod by default, allowing egress only to a logging endpoint at logs.internal:443 and to DNS, while denying all other egress. Provide the YAML, example compliant/noncompliant Pod specs, and verification steps?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["Oracle","Robinhood","Salesforce"]},{"id":"q-4449","question":"In a multi-tenant Kubernetes cluster with sensitive data, design an end-to-end defense that enables rapid containment and IR. Include (a) Cilium network policies for namespace isolation with restricted egress to trusted endpoints, (b) admission controls via OPA Gatekeeper and Kyverno for image provenance and security contexts, (c) Falco runtime rules for API abuse and container escapes, and (d) an incident response workflow with SIEM and Rekor/Cosign attestations. What would you implement first and why?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["Amazon","Coinbase","OpenAI"]},{"id":"q-4591","question":"In a multi-tenant Kubernetes cluster used by IBM, Tesla, and Cloudflare, implement an admission-control policy to prevent pod service accounts from invoking the Kubernetes API outside a restricted set of verbs in all namespaces, except for a dedicated monitoring namespace and a set of approved SPIFFE IDs. Provide the OPA Gatekeeper ConstraintTemplate, a corresponding Constraint, a compliant API call example, a noncompliant one, and a validation plan?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["Cloudflare","IBM","Tesla"]},{"id":"q-4680","question":"In a multi-tenant data science platform used by Apple and Goldman Sachs, each user runs notebooks against a shared data lake. Design a concrete plan to prevent cross-tenant data access by implementing per-tenant data tag ACLs at the storage layer, notebook kernel isolation, and policy‑driven data lineage validation with OPA/Kyverno. Include a sample policy, a validation workflow, and a minimal end-to-end example?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Apple","Goldman Sachs"]},{"id":"q-4759","question":"In a centralized data processing platform used by IBM, Databricks, and Google, design a policy that guarantees Spark batch jobs can only read datasets tagged with the tenant’s label and can only write to a per-tenant sandbox path, regardless of cluster or namespace. Provide a concrete OPA Gatekeeper ConstraintTemplate and Constraint to enforce dataset-level RBAC for Spark jobs, a compliant API submission example, a noncompliant example, and a verification plan with test scenarios?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Databricks","Google","IBM"]},{"id":"q-4823","question":"In a shared ML model registry used by multiple teams at Meta and Nvidia, design a complete plan to enforce that every model artifact (weights, config, tokenizer) is attested with provenance including dataset version, training code hash, and environment, using Sigstore cosign, Rekor, and a Kyverno/OPA policy to gate pushes and deployments to inference clusters. Provide signing commands, attestation storage layout, a policy snippet, and an end-to-end validation plan with a compliant and a noncompliant push?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Meta","NVIDIA"]},{"id":"q-920","question":"In a real-time chat service like Discord, deployed on Kubernetes with NVIDIA GPUs for video processing, you introduce a third-party plugin system that runs as WebAssembly modules to apply custom video filters. How would you design a secure plugin sandbox and runtime attestation to prevent leakage of streams or keys, ensure isolation from other plugins, and enable rapid rollback if a plugin behaves unexpectedly in production? Provide concrete approaches and trade-offs?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Discord","NVIDIA","Zoom"]},{"id":"q-959","question":"Scenario: A service executes user-provided Python plugins inside a container. Design a concrete runtime hardening plan using Linux namespaces, a minimal seccomp profile, and capability bounding, ensuring plugins cannot access host files or network directly while preserving IPC with a controlled channel. Outline exact steps and validation tests?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["PayPal","Tesla"]},{"id":"q-967","question":"Scenario: You manage a microservice app deployed to Kubernetes with CI/CD; you need to prevent tampered container images. **Describe a practical, beginner-friendly plan** to implement image signing and verification using **cosign**, integrate it into a GitHub Actions workflow, and enforce verification at deployment (registry or admission webhook). Include concrete commands?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["Apple","NVIDIA","Stripe"]},{"id":"q-994","question":"Scenario: A Kubernetes-based ML platform serves multiple teams; outbound data exfiltration is a breach risk. Propose a concrete, end-to-end control plane approach to prevent unauthorized data egress using policy-as-code, Kubernetes NetworkPolicy, and a centralized egress gateway. Include a sample Rego policy for Gatekeeper that enforces a namespace label data-export=allowed and an annotation egress-proxy=https://proxy.internal, and outline testing and GitOps integration?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["OpenAI","Zoom"]},{"id":"q-1019","question":"You're operating a CNF-based API gateway cluster that terminates TLS for thousands of tenants across 3 regions. A mandate requires migrating all TLS to post-quantum algorithms with per-tenant keys sourced from an HSM, while delivering zero-downtime upgrades, per-tenant key isolation, and a rollback plan. Outline an end-to-end rollout including (a) inventory and compatibility checks, (b) PQC algorithm and certificate strategy, (c) HSM PKCS#11 integration and key rotation, (d) canary/traffic-mirror rollout and drift detection, (e) observability and rollback criteria?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Goldman Sachs","LinkedIn","PayPal"]},{"id":"q-1040","question":"You're deploying a CNF-based UDP gateway across four data centers. A CVE requires hardware-backed attestation before image execution. Outline a concrete end-to-end rollout plan that (a) signs CNF images with Cosign and SBOMs, (b) enforces attestation via TPM 2.0-based attestation bundles and ImagePolicyWebhook, (c) rolls out region-by-region with traffic mirroring and per-tenant quotas, (d) implements drift detection and automated rollback on attestation failure, and (e) provides observability for attestation metrics and rollback triggers?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Adobe","OpenAI"]},{"id":"q-1071","question":"You're deploying a CNF-based API gateway across 2 Kubernetes clusters. A recent upgrade causes a cross-tenant data bleed under load due to a shared in-memory cache. Outline a beginner-friendly, end-to-end plan to fix and roll out safely: (a) reproduce in staging with two tenants and isolated traffic, (b) implement tenant-scoped cache keys and per-tenant isolation checks, (c) add unit/integration tests for isolation, (d) perform blue/green canary rollout with tenant-based traffic splits, (e) observability: per-tenant SLA metrics and automatic rollback if bleed is detected. Include a minimal code snippet for tenant-scoped cache key?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Discord","Stripe","Two Sigma"]},{"id":"q-1081","question":"You're deploying a CNF-based API gateway that uses workload identities for tenants. Describe an end-to-end plan to enforce identity attestation and per-tenant isolation using SPIRE for SPIFFE IDs, OPA Gatekeeper for policy decisions, and a local Kind testbed. Include (a) issuing and rotating SVIDs, (b) encoding tenant permissions in policies, (c) testing isolation with two tenants, and (d) observability hooks for attestation and policy evaluations?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Lyft","Meta"]},{"id":"q-1107","question":"You're deploying a CNF-based API gateway serving two tenants with strict data-retention and per-tenant log-redaction requirements. Outline an end-to-end plan using SPIRE for workload identities and OPA Gatekeeper for policy decisions to enforce data handling rules while enabling zero-downtime upgrades in a local Kind testbed. Include (a) SVID issuance/rotation tied to tenant IDs, (b) tenant-scoped policies for retention windows and redaction, (c) runtime redaction checks on logs/traces, (d) cross-tenant isolation testing under load, and (e) observability hooks for attestation, policy decisions, and redaction misses?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Google","Meta","Twitter"]},{"id":"q-1187","question":"A beginner CNF certification scenario: you implement a gated CI/CD pipeline for a CNF gateway image. Outline an end-to-end workflow to ensure image provenance before deployment: (a) sign CNF images with Cosign using a KMS-backed key, (b) generate and publish SBOMs, (c) enforce signatures via ImagePolicyWebhook, and (d) surface observability in the monitoring stack for signing success/failure and rollback signals. Provide concrete steps and minimal config snippets?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Instacart","LinkedIn","Tesla"]},{"id":"q-1221","question":"You're operating a CNF-based API gateway deployed across three regions behind a service mesh. Propose a practical upgrade workflow that enforces runtime integrity along with image attestations: (a) sign images with Cosign using a KMS-backed key and publish SBOMs, (b) require TPM/measured-boot attestation plus runtime integrity checks for CNFs, (c) roll out region-by-region with per-tenant canaries and live connection migration, (d) implement drift detection and automatic rollback on attestation/runtime mismatch, (e) surface observability for sign-off, attestation, and rollback triggers. Provide minimal config references?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Netflix","Plaid"]},{"id":"q-1347","question":"Advanced CNF certification scenario: deploy a CNF gateway across three air‑gapped data centers. Design an end‑to‑end plan to ensure image provenance and runtime integrity for updates, covering (a) offline Cosign signing with a TPM‑backed key, (b) SBOM generation and private catalog, (c) per‑tenant policy enforcement via OPA/Gatekeeper, (d) edge‑site offline attestation broker, (e) drift detection with automated rollback, and (f) observability hooks. Include concrete config examples?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Apple","Citadel","Oracle"]},{"id":"q-1511","question":"Beginner CNF certification: You manage a CNF API gateway in a single Kubernetes namespace with a private registry and no external access. Outline an end-to-end plan to enforce SLSA provenance for each image before deployment, including (1) build and sign with Cosign using a KMS-backed key, (2) generate and attach SPDX SBOMs, (3) verify SLSA provenance via ImagePolicyWebhook, (4) canary upgrades with rollback on provenance failure, (5) observability for signing events, SBOM validity, and rollback triggers?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Databricks","Hashicorp","Twitter"]},{"id":"q-1526","question":"In a CNF gateway spanning multiple tenants, implement runtime **per-tenant feature flags** controlled by a central policy server. Describe a concrete plan using SPIRE for tenant identities, OPA for policy evaluation, and a sidecar that hot-reloads policies from a central repo. Include how you detect flag revocation, perform zero-downtime updates, and observability hooks (SBOM provenance, traces, metrics)?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Netflix","Oracle"]},{"id":"q-1544","question":"You're managing a CNF-based API gateway deployed in two regions, serving three tenants with a single global CI/CD pipeline. Propose an end-to-end upgrade and rollback strategy that delivers zero downtime while guaranteeing strict tenant isolation during updates. Include deployment approach (canary vs blue-green), tenant-aware routing, per-tenant metrics and logging, rollback triggers, and how you would validate the plan in staging that mirrors production. Cite concrete tooling choices (Argo Rollouts, Istio/Envoy, OPA, Prometheus) and touchpoints?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["DoorDash","Snowflake"]},{"id":"q-1571","question":"You’re deploying a CNF API gateway across 6 regions and need strict per-tenant isolation with dynamic rate limits and auditable policy changes. Propose an end-to-end plan that uses SPIRE for per-tenant identities, OPA for policy decisions, Envoy with hot xDS updates, and a shared policy catalog. Include how you push canary upgrades, rollback on drift, and observability for quota hits and policy eval latency?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["IBM","Stripe","Uber"]},{"id":"q-1599","question":"Outline a beginner-friendly, end-to-end workflow to enforce image provenance and runtime integrity per-tenant before deployment, using Cosign signing, SPDX SBOMs, and admission controls (ImagePolicyWebhook + Gatekeeper/OPA). Provide concrete steps and minimal config examples?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Apple","IBM"]},{"id":"q-1674","question":"Advanced CNF scenario: deploy a CNF API gateway across three geographies with a private registry and multi-tenancy. Propose an end-to-end plan to enforce per-tenant image provenance and runtime integrity, including (i) per-tenant Cosign signing keys stored in KMS/HSM, (ii) per-tenant SBOMs, (iii) admission controls using ImagePolicyWebhook/OPA, (iv) tenant-aware drift detection and rollback, and (v) observability for signing, attestation, and rollback signals. Provide concrete steps and minimal config examples?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Hashicorp","IBM","Snowflake"]},{"id":"q-1735","question":"Beginner CNF certification: In a multi-tenant Kubernetes cluster hosting CNF gateways in separate namespaces, design an end-to-end image provenance gate: (1) sign images with Cosign using a KMS-backed key and attach SPDX SBOMs, (2) enforce with an ImagePolicyWebhook that requires both signature and SBOM presence, (3) surface a per-tenant visibility badge on deployments, (4) provide minimal config snippets and commands to validate end-to-end in one namespace before rolling out to others?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Google","Hashicorp","Snowflake"]},{"id":"q-1793","question":"In a multi-tenant cluster hosting CNF gateway images, design a beginner end-to-end flow to enforce per-tenant image provenance using Cosign signing with tenant keys, SPDX SBOMs, and a Gatekeeper constraint that reads a per-tenant policy from a ConfigMap. Include minimal YAML for the ConstraintTemplate and Constraint, plus compliant vs noncompliant Deployment manifests, and the exact kubectl steps to validate end-to-end in a single namespace first?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Coinbase","Snap"]},{"id":"q-1812","question":"In a beginner CNF certification scenario, you operate a multi-tenant CNF gateway platform on Kubernetes with a private image registry. Describe a concrete end-to-end workflow to enforce provenance and runtime integrity for every pull: (1) embed build provenance into image labels (commit SHA, CI job, build ID), (2) sign with Cosign using a KMS-backed key and attach SPDX SBOMs, (3) enforce per-tenant policy via Gatekeeper/OPA that rejects images missing provenance or SBOM, and (4) provide a minimal test plan and config snippets to validate in a single namespace before rolling out to others?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Cloudflare","Coinbase","Square"]},{"id":"q-1914","question":"In a multi-tenant CNF gateway managed by a central control plane, design an end-to-end upgrade workflow that uses per-tenant feature flags and a canary rollout with Istio, while enforcing runtime policy with OPA Gatekeeper and SPIRE-based identity. Include: (a) how CNF versions and flags are modeled in CRDs, (b) traffic splitting and health checks for safe canaries, (c) per-tenant policy evaluation, and (d) drift detection with automatic rollback and observability?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Cloudflare","NVIDIA","Twitter"]},{"id":"q-2030","question":"Beginner CNF certification: How would you design an end-to-end workflow in a multi-tenant Kubernetes cluster to enforce SBOM freshness and key-rotation awareness for CNF images, so that when a Cosign key rotates SBOMs are re-generated and re-attested, and a Kyverno policy rejects images signed with old keys or missing SBOMs, with a minimal namespace-scoped test?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Anthropic","Cloudflare","NVIDIA"]},{"id":"q-2189","question":"Advanced CNF Certification: Design an end-to-end runtime policy and rollout flow to ensure that only CNFs with verifiable provenance and hardware compatibility SBOM attestations can allocate SR-IOV NICs across regions; describe data sources, policy versioning, canary rollout, and rollback, plus a minimal policy snippet?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Apple","OpenAI"]},{"id":"q-2262","question":"How would you implement an end-to-end runtime-provenance strategy for upgrades in a three-region, air-gapped CNF platform hosting stateful gateways across Kubernetes clusters, without pulling new images from the registry? Include image provenance checks, TPM-backed attestation via Sigstore, in-cluster drift detection with eBPF, and a safe, low-downtime rollback plan with minimal manifests?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Discord","NVIDIA","Slack"]},{"id":"q-2362","question":"Beginner CNF certification: In a multi-tenant Kubernetes cluster with a shared private registry, design an end-to-end image provenance gate that adds a freshness check: require Cosign-signed images with SPDX SBOMs, and enforce via Open Policy Agent that SBOM timestamp is within 7 days. Provide minimal constraint template, example annotation, and a one-namespace test plan?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["MongoDB","Snowflake","Uber"]},{"id":"q-2486","question":"Advanced CNF certification: In a four region CNF gateway fleet with a private image registry, design a scalable end-to-end workflow for per tenant image provenance and runtime attestation. Include tenant-specific signing keys, SBOMs, admission controls, and a rollback plan for regional outages. Provide concrete steps and minimal config snippets you would actually use in practice?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Apple","Netflix","Salesforce"]},{"id":"q-2560","question":"Design an end-to-end CNF image provenance and runtime attestation workflow for a globally deployed API gateway across x86_64 and ARM edge nodes with intermittent connectivity. Include per-arch signing and SBOMs, cross-arch attestation claims, an OPA policy enforcing SBOM presence and trusted architecture, and a testing plan with canary rollouts and rollback criteria?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Goldman Sachs","IBM","NVIDIA"]},{"id":"q-2638","question":"In a CNF gateway deployed across multi-cloud Kubernetes clusters, outline an end-to-end plan to guarantee runtime integrity and tenant isolation during updates, by combining hardware-backed attestation (TPM/SEV), SBOM provenance, a private attestation service, SPIRE identities, and policy enforcement with OPA Gatekeeper. Include upgrade testing (canary and rollback) and observability hooks?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Amazon","Oracle"]},{"id":"q-2745","question":"In a multi-tenant CNF gateway namespace, design a beginner-end-to-end image provenance gate that: (1) embeds per-tenant labels cnf/tenant and commit in the image, (2) signs images with Cosign using a KMS-backed key and attaches an SPDX SBOM, (3) enforces with a Kyverno policy that validates both the signature and the presence of the SBOM and tenant label, (4) includes a minimal test plan and config snippets to verify in one namespace before rollout?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Meta","Scale Ai"]},{"id":"q-2814","question":"Describe a scalable, tenant-aware CNF provenance and runtime integrity flow: tenants supply per-tenant Cosign root keys and SBOM policies; images signed with tenant key and SBOM attached; admission controls enforce signature+SBOM before deployment; a runtime sidecar attests the image via TPM-backed quote and digest, blocking traffic on failure; include minimal config and test plan?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Amazon","Twitter","Two Sigma"]},{"id":"q-3156","question":"In a multi-tenant CNF platform running on Kubernetes across edge sites, design an end-to-end runtime attestation workflow that prevents unauthorized CNF updates from taking effect without a full image rebuild. The workflow should (1) collect hardware-backed startup attestations (TPM2.0/IMA) and publish to a central verifier, (2) publish a per-patch SBOM delta and attach a trust score, (3) enforce tenant-scoped runtime policy via OPA/Kyverno that rejects changes without valid attestation and SBOM delta, and (4) include a minimal in-namespace test plan to validate end-to-end before rollout. Provide concrete steps and minimal config snippets?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Cloudflare","NVIDIA","Tesla"]},{"id":"q-3205","question":"In a multi-tenant CNF API gateway cluster on Kubernetes, a CVE hits a shared library in the gateway control plane. Describe a concrete end-to-end patch and rollout plan that preserves per-tenant SLAs while meeting CNF governance: (1) regenerate and attach SBOMs for the patched image, (2) re-sign with Cosign and rotate KMS-backed keys if required, (3) implement per-tenant canary deployments using Argo Rollouts and Istio routing with per-tenant traffic shaping, (4) validate tenant isolation and rollback criteria with automated tests, (5) include minimal config snippets and concrete commands to validate end to end?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Oracle","Plaid","Tesla"]},{"id":"q-3230","question":"In an air-gapped edge CNF deployment, outline a practical end-to-end provenance workflow that signs an image with Cosign using an offline KMS key, attaches an SPDX SBOM, and transfers a portable provenance manifest to edge sites for offline verification before pull. Include steps for signing, SBOM generation, manifest format, offline verification, and a minimal in-namespace check?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Coinbase","Google","MongoDB"]},{"id":"q-3294","question":"In a multi-tenant CNF-based firewall deployed across four regional clusters, design an end-to-end provenance workflow that (1) signs CNF images with Cosign using a region-scoped KMS key, (2) attaches a tenant-bound SPDX SBOM and a site-binding annotation, (3) enforces via a Kyverno policy that both the signature and SBOM exist and that the tenant annotation matches, and (4) supports automated cross-region key rotation and revocation. Include a one-namespace test plan and minimal config snippets?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Cloudflare","DoorDash","Google"]},{"id":"q-3379","question":"Design an **external attestation API** for a Kubernetes-based multi-tenant CNF mesh (Slack/Google/OpenAI). Given an image digest and tenant, return a signed provenance bundle containing: imageDigest, sbomHash, signerKeyID, tenant, verdict, and timestamp. Describe data model, signing flow with **Sigstore** using a per-tenant KMS key, Rekor publication, and a minimal in-namespace test exercising the API end-to-end via a ServiceAccount?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Google","OpenAI","Slack"]},{"id":"q-3399","question":"In a globally distributed CNF platform spanning three regions, design an end-to-end governance gate for CNF updates that ensures provenance and safe rollback. Requirements: (1) per-tenant image SBOMs and Cosign signatures; (2) host attestation (TPM2.0/IMA) at update time to verify base hardware; (3) config-diff provenance by signing and attaching SBOM delta for Kubernetes manifests; (4) cross-region policy enforcement with Kyverno/OPA and a central verifier; (5) a minimal in-namespace test plan and config snippets. Provide concrete steps and minimal config?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Apple","Netflix"]},{"id":"q-3452","question":"Beginner CNF certification: In a multi-tenant Kubernetes cluster, implement end-to-end config drift protection for CNF gateway deployments. Design a flow to (1) sign gateway configs with Cosign using a KMS-backed key and publish SBOMs, (2) mount a signed config as a per-tenant ConfigMap via a controller, (3) enforce at admission time that runtime config matches the signed artifact, and (4) surface per-tenant drift alerts with minimal in-namespace tests and sample manifests. What steps and minimal snippets would you provide?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Airbnb","Instacart","Microsoft"]},{"id":"q-3457","question":"In a shared Kubernetes CNF certification setup with multi-tenant Gatekeeper policies, design an end-to-end flow to validate and enforce per-tenant feature flags before deployment: 1) define a tenant-scoped FeaturePolicy CRD, 2) implement a ConstraintTemplate (Rego) that reads Deployment.annotations['cnf/feature'] and checks against the tenant policy, 3) enforce with Gatekeeper so disallowed flags fail admission, 4) wire a minimal CI gate to publish tenant policies and test manifests, 5) add in-namespace tests that verify allowed vs disallowed cases. What would you implement and why?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Amazon","Google"]},{"id":"q-3536","question":"Beginner CNF certification: In a multi-tenant CNF gateway platform on Kubernetes, design a minimal end-to-end post-deployment image revocation flow. Include (1) a signed revocation artifact published to the central registry (pointing to revoked image digests), (2) an ImagePolicyWebhook rule that denies pods whose image digest appears in the revocation list, (3) per-tenant rollback flags surfaced per-namespace via a ConfigMap, and (4) a one-namespace validation test with minimal manifests before rollout?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Airbnb","Google","Two Sigma"]},{"id":"q-3689","question":"In a CNF gateway deployed across multiple clouds, design a runtime attestation workflow using Linux IMA to produce a signed attestation report for each running CNF container, verify it against a central policy server, and enforce tenant-scoped access based on attestation results. Include how to collect measurements, sign with an HSM, verify at admission, and a minimal one-namespace validation before rollout?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Coinbase","Microsoft","Uber"]},{"id":"q-3852","question":"Scenario: in a multi-tenant CNF platform on Kubernetes, design an end-to-end upgrade gate that enforces tenant-specific policies at admission time. Requirements: CNF images and SPDX SBOMs must be Cosign-signed; upgrades allowed only when provenance and SBOM delta satisfy a tenant policy stored in a KMS-backed store; a TPM2.0/IMA attestation proves a trusted baseline before upgrade; include a minimal in-namespace manifest to validate in one namespace and discuss trade-offs and fallbacks?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Google","Hugging Face","Salesforce"]},{"id":"q-3980","question":"In a multi-tenant CNF gateway platform on Kubernetes across two clouds, design an end-to-end image provenance gate for per-tenant upgrades. Requirements: (1) Cosign-sign images with a tenant-scoped KMS key and attach an SPDX SBOM, (2) a central policy (Kyverno/OPA) enforcing presence of both signature and SBOM before admission, (3) canary rollout with per-tenant rollback on SBOM or signature drift, (4) provide minimal in-namespace manifests and commands to validate end-to-end in one tenant namespace before broader rollout?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Instacart","Slack","Two Sigma"]},{"id":"q-4065","question":"Scenario: You run a multi-tenant CNF platform on Kubernetes delivering real-time video processing at edge. Design an end-to-end update validation workflow that (1) signs new CNF images with Cosign and attaches SBOMs, (2) performs runtime attestation (TPM/IMA) and extracts a lightweight anomaly score from telemetry, (3) enforces via OPA/Kyverno that upgrades are blocked if attestation or SBOM is invalid or anomaly score is high, and (4) includes a minimal in-namespace test to verify rollout and rollback before full deployment?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["MongoDB","Netflix","Snap"]},{"id":"q-4153","question":"In a multi-tenant CNF platform on Kubernetes delivering a real-time data plane CNF across regions, design an end-to-end upgrade validation flow that ensures zero-downtime upgrades per tenant. Include: (1) per-tenant compatibility manifests with API/ABI versions and SBOM attestations, (2) a canary rollout strategy with traffic splitting and progressive health checks, (3) cross-tenant attestation checks with a policy engine, (4) rollback plan and auditability, and (5) a one-namespace pre-check before full rollout?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Google","Instacart","Two Sigma"]},{"id":"q-4272","question":"In a CNF platform spanning on-prem, AWS, and Azure, design an end-to-end CNF image signing and key-rotation workflow using per-cloud KMS (AWS KMS, Azure Key Vault, and in-house HSM) with Cosign, SPDX SBOMs, and a cross-cloud provenance store. Ensure zero-downtime key rotation, tenant isolation, and automatic rollback if attestation fails. Provide steps, minimal manifests, and verification plan?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Plaid","Snowflake"]},{"id":"q-4379","question":"Design an end-to-end workflow to verify integrity of dynamic CNF configuration updates (CRDs, ConfigMaps, policy) in a multi-tenant CNF gateway on Kubernetes. Include signing of config payloads, SBOM generation, admission-time enforcement, per-tenant rollback, and a minimal in-namespace test plan; detail how you would validate in a canary namespace before rollout?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Citadel","Coinbase"]},{"id":"q-4427","question":"Design an end-to-end CNF upgrade gate for a multi-tenant Kubernetes platform. Requirements: (1) sign upgrade images with Cosign using a KMS-backed key and attach SPDX SBOMs; (2) compute per-tenant SBOM deltas and enforce via Kyverno to permit upgrades only with valid attestation and delta approval; (3) implement canary rollout with per-tenant rollout windows and automatic rollback; (4) include minimal manifests and commands for in-namespace validation?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Amazon","Discord","Slack"]},{"id":"q-4446","question":"CNF certification: In a multi-tenant CNF gateway platform on Kubernetes, design an end-to-end upgrade safety workflow for CNF images that guarantees API contract compatibility, tenant isolation, and safe rollback. Requirements: per-tenant upgrade windows; canary rollout; API surface contract tests; in-cluster image provenance (Cosign) with SBOM; automatic rollback on contract/test failures, latency spikes, or SBOM mismatches; include minimal manifests and testing steps?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["DoorDash","Google","Zoom"]},{"id":"q-4480","question":"In a multi-tenant CNF platform on Kubernetes serving data-plane proxies for a fintech partner, design a runtime attestation workflow that enforces per-tenant trust before a CNF is allowed to handle traffic. The solution should extend prior provenance (image signing + SBOM) to include a small in-guest attestation agent that reports a runtime hash to a per-tenant attestation service. Describe: (1) how tenants produce and publish SBOMs and runtime attestations; (2) how an admission controller enforces both image signature and runtime attestation; (3) how keys are rotated with KMS; (4) a minimal one-namespace test to validate the end-to-end; (5) failure modes and mitigations?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["MongoDB","Robinhood","Scale Ai"]},{"id":"q-4524","question":"Beginner CNF certification: In a multi-tenant CNF gateway platform, add end-to-end provenance for runtime plugins loaded by gateways (e.g., TLS, rate-limiting, IDS) packaged as separate images. Outline an end-to-end flow to (1) sign plugin images with Cosign and attach SBOMs, (2) enforce admission-time loading of only signed plugins per-tenant, (3) surface per-tenant plugin provenance in gateway status, (4) validate in one namespace with minimal manifests before rollout?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Discord","Scale Ai","Twitter"]},{"id":"q-4559","question":"Design a tenant-scoped CNF image trust workflow for a multi-tenant Kubernetes CNF platform. Each tenant has a dedicated Cosign signing key stored in a KMS. Build a per-tenant admission policy (Kyverno) that: (a) validates the image digest is signed with the tenant’s key, (b) requires an SPDX SBOM, (c) enforces a tenant-specific trust score before deployment, and (d) surfaces per-tenant test status prior to rollout. Provide concrete steps and minimal in-namespace manifests to validate one tenant before broader rollout?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["LinkedIn","Scale Ai","Twitter"]},{"id":"q-4636","question":"Design an intermediate CNF certification scenario for a hybrid cloud service mesh: implement an end-to-end runtime attestation and upgrade gate for CNFs deployed across namespaces and clouds. Include (1) a lightweight in-pod sidecar that signs an attestation report with a KMS-backed key and posts to a central attestation service, (2) a trust policy validating image provenance and attestation against a root of trust, (3) a canary upgrade strategy with per-CNF rollback based on attestation and traffic signals, and (4) minimal config snippets to validate in one namespace before wider rollout?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Airbnb","Cloudflare","Twitter"]},{"id":"q-4833","question":"Advanced CNF Certification: In a CNF fleet deployed across three regions with intermittent WAN, design a fault-tolerant provenance and upgrade governance workflow. Requirements: (1) replicate SBOMs and Sigstore signatures to a central verifier using quorum-based replication, (2) per-tenant upgrade gating via OPA/Kyverno that only allows upgrades when a current attestation + SBOM digest exists, (3) partition-safe rollout with local policy persistence and automatic backoff, (4) provide minimal in-namespace tests and commands to validate a single-region upgrade before global rollout. Include data flow and minimal config examples?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["IBM","Stripe"]},{"id":"q-847","question":"You're deploying a CNF gateway (e.g., NGINX CNF) on a 50-node Kubernetes cluster handling streaming traffic. A node eviction hits during peak load. Outline a concrete plan to maintain streaming availability, focusing on (1) graceful drain with preStop, (2) health checks/readiness/liveness, and (3) traffic affinity and pod topology, and (4) observability and rollout strategy?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Netflix","Plaid","Tesla"]},{"id":"q-918","question":"You're rolling out a CNF-based NAT gateway across a 3-region multi-cluster Kubernetes setup. A policy change must be applied without disrupting live traffic. Outline a concrete, end-to-end rollout plan emphasizing (a) shadow canaries with traffic mirroring, (b) region-by-region rollout with per-region SLI targets, (c) policy-state reconciliation and drift detection, (d) rollback conditions and observability instrumentation?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Citadel","Coinbase"]},{"id":"q-935","question":"You manage CNF-based gateways across 4 regions. A suspected supply-chain compromise requires enforcing in-cluster image attestations before rollout without downtime. Outline an end-to-end plan to (a) sign CNF images with Cosign and SBOMs, (b) enforce signatures via ImagePolicyWebhook, (c) roll out region-by-region with traffic mirroring, (d) implement drift detection and automatic rollback on attestation failure. Include observability?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["IBM","Tesla","Two Sigma"]},{"id":"q-985","question":"Design a zero-downtime, CNF-based API gateway rollout where per-tenant routing rules update live without dropping connections. Outline end-to-end steps: (a) safe rule distribution, (b) canary-ingress slicing with weighted traffic, (c) drift detection between desired and active routes, (d) observability and rollback?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Goldman Sachs","Meta","Tesla"]},{"id":"q-1051","question":"CNPA stack: HTTP API writes to PostgreSQL and emits Kafka events. A hot, large users table needs a non-blocking schema change (e.g., adding a new NOT NULL column with default). Propose a production-grade online migration plan that minimizes downtime, keeps Kafka in sync, handles backfill, and describes rollback and validation steps?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Amazon","Instacart","Plaid"]},{"id":"q-1150","question":"In a CNPA stack: HTTP API ingests events, writes to PostgreSQL, and publishes to Kafka; during spikes, retries duplicate processed events. Propose a concrete, end-to-end plan to guarantee idempotent processing and prevent duplicates under retry storms. Include idempotency key strategy, dedup enforcement point, Kafka/DB coordination, and validation?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Netflix","Snap","Twitter"]},{"id":"q-1157","question":"CNPA pipeline uses an HTTP API -> PostgreSQL -> Kafka with Avro schemas in a Schema Registry. A new optional field is added to the events, and some consumers crash when they see older versions. Provide a concrete, zero-downtime plan for schema evolution, including compatibility mode, rollout strategy, topic/consumer changes, backfill approach, rollback, and validation?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Anthropic","Hugging Face","Salesforce"]},{"id":"q-1188","question":"CNPA stack: an HTTP API writes to PostgreSQL and publishes to Kafka. During peak load, duplicate events may be produced due to retries and at-least-once semantics. Describe a concrete end-to-end plan to enforce idempotent processing across HTTP, DB, and Kafka, including dedupe strategy, upsert/constraints, transactional writes, offset tracking, and how you’d validate correctness under load?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Meta","PayPal","Tesla"]},{"id":"q-1283","question":"CNPA stack with HTTP API, PostgreSQL, Kafka, and Redis: a Redis-based rate limiter fronting the API causes legitimate bursts to be 429-throttled during a promo, despite normal traffic. Provide a concrete debugging plan to isolate whether latency or errors come from Redis Lua script, Redis network, the HTTP handler, or downstream services, with exact metrics and concrete fixes and how you’d validate impact?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["DoorDash","PayPal"]},{"id":"q-1383","question":"CNPA stack with HTTP API writing to PostgreSQL, publishing to Kafka, and Elasticsearch dashboards. A nightly backfill misses events, causing dashboards to report incorrect counts. Describe a concrete debugging plan to isolate whether loss occurs in HTTP write/transaction, Postgres-to-Kafka CDC, or Kafka-to-Elasticsearch sink, with exact metrics, sampling, and concrete fixes (idempotent sinks, transactional writes, producer retries, dedup IDs) and how you would verify end-to-end?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Databricks","Robinhood"]},{"id":"q-1401","question":"CNPA stack: HTTP API writes events to PostgreSQL, publishes to Kafka with event_time metadata, and a downstream analytics service reads from Kafka to produce 5-minute windowed counts. After a release, a dashboard shows both late counts and misaligned windows. Provide a concrete debugging plan to determine if the issue is event-time timestamps, Kafka timestamps, consumer windowing, or clock skew across services, including exact metrics, sampling, and fixes (re-timestamp, watermarking, idempotent sinks) and how you would validate end-to-end correctness?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Amazon","Apple"]},{"id":"q-1440","question":"In a CNPA stack with an HTTP API writing to PostgreSQL and publishing to Kafka, add an optional field customer_segment; design a zero-downtime schema evolution and payload versioning plan, including DB changes, Kafka message formats, consumer upgrades, backfill, and validation?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Apple","NVIDIA","Zoom"]},{"id":"q-1586","question":"In a CNPA stack with an HTTP API writing to PostgreSQL, publishing to Kafka, and Elasticsearch/Redis downstream, a burst causes duplicate rows in Postgres and delayed dashboard freshness. Propose a concrete end-to-end exactly-once plan: transactional Kafka producers, Postgres outbox, idempotent Elasticsearch sinks, Redis invalidation, and verification steps, plus rollback if needed?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Airbnb","IBM","Snowflake"]},{"id":"q-1630","question":"In a CNPA stack with an HTTP API, PostgreSQL, Kafka, and Redis, dashboards display stale data for a cohort after a deployment; propose a concrete debugging plan to determine whether drift originates from writes to Postgres, the Kafka sink, or Redis caching, including exact metrics to collect, sampling strategy, and concrete fixes (transactional outbox, idempotent sinks, Redis invalidation, cache-warming) and how you would verify impact?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["NVIDIA","Plaid"]},{"id":"q-1716","question":"CNPA stack: an HTTP API writes to PostgreSQL and publishes to Kafka. A schema evolution adds a new optional field to the event payload stored in Postgres and emitted to Kafka. Propose a concrete migration plan that preserves compatibility, uses a versioned envelope, coordinates writes and reads, validates with end-to-end tests, and provides a safe rollback. Include concrete steps, metrics, and rollback strategy?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Databricks","Google"]},{"id":"q-1732","question":"CNPA stack: HTTP API writes to PostgreSQL, emits to Kafka, and a Redis-backed read cache. A schema change adds a new optional field to the event payload; rollout under peak load leads to some consumers crashing due to compatibility. Provide a concrete, practical rollout and debugging plan to ensure no data loss or outages, including steps, metrics, and concrete changes (schema registry, backward/forward compatibility tests, dual-write, feature flags, cache invalidation) and how you would verify success?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Adobe","Coinbase","MongoDB"]},{"id":"q-1751","question":"CNPA stack: HTTP API → PostgreSQL → Kafka → stream processor → Redis dashboards. A new audit requires end-to-end latency visibility for late events during windows; latency spikes 2–3 minutes. Provide a concrete debugging plan to pinpoint whether the delay lies in HTTP ingress, DB writes, Kafka publish, stream windowing, or Redis caching. Include exact metrics, sampling, and fixes (idempotent sinks, transactional outbox, watermark tuning, checkpointing) and how you’d verify impact?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Goldman Sachs","Instacart","Snowflake"]},{"id":"q-1794","question":"In a CNPA stack—HTTP API, PostgreSQL, Kafka, Redis—latency spikes appear on a new endpoint that touches all components. Outline a practical plan to implement end-to-end tracing with a correlation_id: where to instrument, which metrics to collect (end-to-end latency, per-service latency, queue time, DB time, cache misses), and concrete changes (propagate correlation_id in HTTP, persist it in DB, attach it to Kafka messages, emit trace spans). How would you validate in staging before production?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Bloomberg","Google","Hugging Face"]},{"id":"q-1851","question":"In a CNPA stack: HTTP API ingests events, writes to PostgreSQL, and publishes to Kafka. A new enrichment step guarded by a feature flag calls a 3rd-party service. Design a concrete, end-to-end rollout plan that minimizes risk: per-tenant flag rollout, fallback behavior when the service is down, tracing with correlation IDs, circuit breaker, and backpressure; metrics to monitor (p50/p95 latency, error rate, backlog), rollback criteria, and validation steps?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Discord","Salesforce"]},{"id":"q-1875","question":"CNPA stack: HTTP API writes to PostgreSQL and publishes to Kafka; downstream analytics reads from Kafka and loads into a data warehouse. During deployment, dashboards drift and latency tails widen. Create a concrete, end-to-end debugging plan to isolate whether the root cause is HTTP serialization, DB write latency, Kafka publish, consumer, or ETL/warehouse load, with exact metrics, sampling, and concrete fixes (outbox pattern, idempotent sinks, backpressure, staged deploy) and verification steps?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Hashicorp","Slack","Snowflake"]},{"id":"q-1956","question":"In a CNPA stack: HTTP API ingests events, writes to PostgreSQL, and publishes to Kafka. A new optional field customer_region must be added without downtime or data loss. Describe a concrete, end-to-end migration plan: schema changes, producer/consumer compatibility, backfill strategy, validation checks, and rollback. Include metrics and canary signals to verify success?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["DoorDash","Oracle","Two Sigma"]},{"id":"q-2041","question":"In a CNPA stack (HTTP API -> Postgres -> Kafka -> Redis) you notice tail latency spikes during peak hours. Outline a concrete, beginner-friendly plan to diagnose end-to-end latency using distributed tracing. Include what to instrument, where to insert spans (HTTP handler, DB query, Kafka producer/consumer, Redis ops), how to propagate trace context, minimal metrics, and a simple verification checklist to confirm fixes?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Hashicorp","Lyft","Snap"]},{"id":"q-2351","question":"CNPA pipeline: HTTP API writes to PostgreSQL and publishes events to Kafka. A rollout requires strict per-user ordering across a high-throughput, multi-partition topic; out-of-order deliveries break dashboards. Describe a concrete plan to guarantee per-user ordering while preserving throughput: data-path changes (outbox with transactional writes, per-user partitioning), producer settings (acks=all, enable.idempotence, max.in.flight=1, batch/linger), idempotent sinks, testing (replay, verifications per user), and rollback. Include concrete metrics and verification steps?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Anthropic","Plaid","Twitter"]},{"id":"q-2414","question":"In a CNPA stack where an HTTP API ingests JSON events, writes to PostgreSQL, and publishes to Kafka, a deployment adds a required field 'customerTier' to the payload. Older clients omit it. Outline a concrete, practical debugging plan to locate the failure point and implement a safe rollout using a backward-compatible schema, optional field, or a schema registry with compatibility settings, plus a minimal migration and a feature flag. Include exact steps, data checks, and how you would verify no data loss?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Google","IBM"]},{"id":"q-2450","question":"CNPA stack: HTTP API writes to PostgreSQL and publishes to Kafka. A feature flag routes writes through a shadow sink and a shadow Kafka topic. When enabled, dashboards show delayed data and downstream consumers occasionally duplicate messages due to retries. Provide a concrete debugging plan to verify data correctness and latency impact, including idempotency, outbox, transactional boundaries, and rollback procedures; specify metrics, sampling, and verification steps?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Amazon","Netflix","Robinhood"]},{"id":"q-2489","question":"CNPA stack: HTTP API writes to PostgreSQL and publishes events to Kafka using Avro; a schema migration is requested while dashboards rely on Redis caches updated by a Kafka consumer. Outline a concrete, end-to-end rollout plan that guarantees backward/forward compatibility, zero downtime, and data correctness. Include schema registry strategy, compatibility modes, dual-write/outbox techniques, canary rollout, monitoring, and rollback criteria with concrete change sets?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Amazon","Anthropic","Databricks"]},{"id":"q-2539","question":"CNPA stack with an HTTP API, PostgreSQL, and Kafka: a new schema migration on a hot table runs during peak, causing write latency spikes and intermittent 5xx errors. Provide a concrete debugging plan to isolate whether the bottleneck is the HTTP handler, the migration's locks in PostgreSQL, or the Kafka sink, with exact SQLs, metrics, and concrete fixes (online migrations, lock avoidance, prepared statements, idempotent sinks) and a validation strategy?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["DoorDash","Netflix","Tesla"]},{"id":"q-2952","question":"CNPA stack with HTTP API, Postgres, Kafka, Redis. A new canary feature behind a dynamic flag shows tail latency spikes only for flagged users. Provide a concrete debugging plan to isolate whether latency stems from flag evaluation, HTTP handler, DB, Kafka, or Redis. Include exact metrics to collect, sampling strategy, and concrete fixes (hybrid flag eval, caching, inlining, canary routing) and how you’d verify impact?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Cloudflare","MongoDB","Netflix"]},{"id":"q-3018","question":"In a CNPA stack with an HTTP API, PostgreSQL, and Kafka, a recent feature rollout triples tail latency under load. Propose a concrete, end-to-end debugging plan to identify whether the bottleneck is the HTTP server, the DB query, or the Kafka producer/consumer, including exact metrics to collect, a sampling strategy, and concrete fixes (blocking vs non-blocking I/O, prepared statements, pooling, idempotent sinks, and backpressure). Also describe how you'd validate improvement?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Discord","Instacart","Zoom"]},{"id":"q-3043","question":"CNPA stack with HTTP API, PostgreSQL, Kafka, and Redis. During a JWT rotation, tail latency spikes for authenticated requests. Provide a concrete debugging plan to isolate whether latency stems from JWT middleware, HTTP handler, DB, Kafka, or Redis, including exact metrics to collect, tracing steps, and concrete fixes (JWKS caching with TTL, per-user auth cache, pool tuning, Kafka acks, Redis warming) and how you’d verify impact?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Robinhood","Salesforce","Slack"]},{"id":"q-3073","question":"CNPA stack: HTTP API writes to Postgres and publishes to Kafka; downstream analytics consumes from Kafka and loads into a data warehouse. A schema evolution adds an optional field to the event payload used by dashboards. Describe a concrete plan to roll this out without downtime, covering: versioned schemas and topic management, writer/reader compatibility rules in a schema registry, backfill strategy, feature flags to toggle new field usage, and validation metrics?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Databricks","DoorDash","MongoDB"]},{"id":"q-3188","question":"In a CNPA stack: an HTTP API ingests events, writes to PostgreSQL, and publishes to Kafka. You must deploy a non-breaking schema migration adding a nullable region field with zero downtime. Provide a concrete, beginner-friendly plan covering: (a) migration approach, (b) feature-flag strategy for reads/writes, (c) backfill and data consistency checks, and (d) validation that latency remains within acceptable bounds. Include steps, checks, and sample commands?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Meta","Twitter"]},{"id":"q-3277","question":"In a CNPA stack with an HTTP API, PostgreSQL, Kafka, and a Redis-backed dashboard, design a beginner-friendly observability plan to trace one event end-to-end. Describe correlation_id propagation, required log fields at each boundary, and sample log formats; include how to validate cross-service correlation in practice. End with the concrete steps you would take to implement this?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["LinkedIn","PayPal"]},{"id":"q-3318","question":"In a CNPA stack with an HTTP API, PostgreSQL, and Kafka, a new tenant-isolation requirement demands that data for one tenant is inaccessible to others. Propose a concrete beginner-friendly plan to implement per-tenant isolation using PostgreSQL Row-Level Security (RLS) and a tenant_id header, including schema changes, API adjustments, tests, and end-to-end verification?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Hashicorp","Meta","Two Sigma"]},{"id":"q-3340","question":"You run a CNPA stack where an HTTP API ingests events, writes to PostgreSQL, and publishes to Kafka with Avro schemas; analytics consume the Kafka topics. After a schema change, new events fail to publish while old consumers parse v1. Outline a concrete, end-to-end migration plan to keep services live with no downtime. Include steps, metrics, and concrete changes (schema registry usage, topic versions, dual writing, canary) and how you would verify success?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Citadel","Cloudflare","Snap"]},{"id":"q-3481","question":"In a CNPA stack with an HTTP API, PostgreSQL, Kafka, and a Redis-backed dashboard, you need to add a feature flag that enables a new data field (tenant_id) only for a subset of users. Describe a beginner-friendly rollout plan to gate writes to Postgres and publishes to Kafka based on the flag, including how to thread the flag through services, handling backward compatibility, and how you would verify correctness and rollback if issues arise?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["IBM","Two Sigma"]},{"id":"q-3506","question":"CNPA stack with HTTP API, Postgres, Kafka, Redis. After enabling OpenTelemetry tracing across services for observability, tail latency spikes appear under load. Design a concrete debugging plan to determine if tracing overhead is causing the latency (sampling rate, exporter bottlenecks, or instrumentation choices) and propose concrete mitigations (adaptive tail sampling, attribute trimming, batch exporters) and how you would validate impact?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Apple","Meta","Scale Ai"]},{"id":"q-3598","question":"CNPA stack with an HTTP API, Postgres, Kafka, and a cross-region MirrorMaker setup. After a regional failover, consumer lag spikes in region B and dashboards drift. Provide a concrete debugging plan to isolate whether bottlenecks are the HTTP producer, DB writer, cross-region Kafka replication, or downstream consumers. Include metrics to collect, tracing steps, and concrete fixes (batching, acks, backpressure, idempotence, failover testing)?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Apple","Databricks"]},{"id":"q-3601","question":"CNPA stack: HTTP API writes to PostgreSQL and publishes to Kafka; a Redis cache serves reads and a downstream warehouse ingests Kafka topics. You need to roll out a non-breaking Avro schema change and a concurrent DB index rewrite with zero downtime. Outline a concrete end-to-end rollout, covering schema compatibility, online migrations, cache invalidation, backfill strategy, and verification steps?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Goldman Sachs","Oracle","Tesla"]},{"id":"q-3636","question":"In a CNPA stack with an HTTP API, PostgreSQL, Kafka, and Redis, describe a concrete plan to implement and test schema evolution for user events to ensure backward and forward compatibility. Include versioning strategy, envelope format, optional fields, canary deployment, and validation steps?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Hugging Face","Oracle"]},{"id":"q-3713","question":"CNPA stack: HTTP API writes to Postgres, publishes Kafka events with Avro, and Redis caches hot reads. A deployment introduces a new event schema version, causing intermittent consumer deserialization errors and stale Redis data. Provide a concrete debugging plan to isolate whether the issue is the HTTP producer, Kafka schema evolution, consumer deserialization, or Redis caching. Include metrics to collect, sampling, and fixes (backward-compatible fields or separate topics, schema registry flags, canary rollout, event replay). How would you verify impact?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Bloomberg","Cloudflare","Oracle"]},{"id":"q-3830","question":"In a CNPA stack with an HTTP API, PostgreSQL, Kafka, and Redis, you want to enforce strict JSON payload validation at the HTTP boundary using versioned JSON schemas. How would you implement this, handle schema evolution (backward compatibility, defaults, deprecation), and ensure invalid payloads are rejected with clear errors without blocking throughput? Include concrete steps, tool suggestions (Ajv, schema registry), and a minimal code snippet?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Microsoft","Tesla"]},{"id":"q-3865","question":"CNPA stack where an HTTP API writes to Postgres, publishes to Kafka, and a Redis cache backs a downstream dashboard. Latency telemetry sometimes misses spans across Kafka and Redis, breaking traceability. Design a concrete debugging plan to verify trace propagation across HTTP, Kafka, and Redis, pinpoint where spans disappear, and implement fixes with concrete metrics and validation steps?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Plaid","Stripe"]},{"id":"q-3943","question":"CNPA stack: HTTP API, Postgres, Kafka, Redis. A non breaking DB schema change and a new Kafka event schema must be rolled out with zero downtime during peak load. Describe a concrete end to end plan including migration design (backward/forward compatibility), canary rollout, feature flags, dual writes, rollback criteria, and verification steps?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["DoorDash","Slack"]},{"id":"q-4053","question":"In a CNPA stack with an HTTP API, PostgreSQL, and Kafka, a 15-minute spike doubles event rate while tail latency remains stable. Provide a beginner-friendly, concrete debugging plan to determine whether bottleneck is HTTP, DB write, or Kafka publish. Include exact metrics to collect, sampling approach, and minimal instrumentation changes (request_id propagation, per-stage timers, lightweight traces) and how you’d verify impact?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Cloudflare","Goldman Sachs"]},{"id":"q-4106","question":"CNPA stack in production spans two regions (us-east-1 and eu-west-1). A CNPA API writes to PostgreSQL in the primary region, which asynchronously replicates to read replicas and to a Kafka topic for downstream consumers. A BI dashboard across regions shows inconsistent cohort counts after deployments. Describe a concrete debugging plan to determine whether drift is due to replication lag, cross-region consistency, Kafka sinks, or data pipeline caching, including exact metrics to collect, sampling strategy, and concrete fixes (synchronous commit for critical writes, cross-region reads, idempotent sinks, cache invalidation). How would you verify impact?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["IBM","Oracle","Two Sigma"]},{"id":"q-4353","question":"A CNPA stack aggregates per-user daily metrics in a separate batch service. Describe a concrete, beginner-friendly plan to ensure correct windowed counts despite late-arriving events, including event_time usage, watermarking with a defined lateness bound, idempotent upserts in Postgres, durable window state, and end-to-end tests that inject late data to verify convergence in the dashboard?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Instacart","Slack"]},{"id":"q-4394","question":"Describe a concrete, beginner-friendly plan to migrate a CNPA stack from a single-tenant events table to multi-tenant isolation by adding a tenant_id, without downtime. Include step-by-step migration, backfill strategy, indexing, and validation with canaries on 5-10% traffic. How would you verify cross-tenant data isolation and performance during the rollout?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Amazon","LinkedIn"]},{"id":"q-4537","question":"CNPA stack includes an HTTP API, PostgreSQL, and Kafka. A new optional field device_model is added to the event envelope for future analytics. Describe a concrete, beginner-friendly plan to evolve the schema without downtime: version the payload, keep v1 compatible, add a nullable column in Postgres, adapt upserts and downstream consumers, and validate with mixed v1/v2 events in end-to-end tests?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Meta","Twitter","Two Sigma"]},{"id":"q-4562","question":"CNPA stack with HTTP API, PostgreSQL, and Kafka: you plan to add an optional 'source' field to every ingested event, controlled by a feature flag. Describe a concrete, beginner-friendly rollout plan that guarantees backward compatibility, no data loss, and safe rollback. Include payload schema decisions, DB changes, Kafka behavior, canary strategy, observability, and validation steps?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Citadel","Cloudflare","Discord"]},{"id":"q-4684","question":"In a CNPA stack with an HTTP API, PostgreSQL, Kafka (Avro via Schema Registry), and a downstream Elasticsearch-backed dashboard, a recent Avro schema upgrade triggers more deserialization errors and downstream lag during peak load. Provide a concrete debugging plan to isolate whether the fault is compatibility, producer/consumer, or backfill, with exact metrics, sampling, and fixes (compat checks, controlled replay, idempotent sinks) and how you would verify impact?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Amazon","MongoDB","Uber"]},{"id":"q-4780","question":"CNPA stack: HTTP API writes to PostgreSQL, publishes events to Kafka with Avro, and a downstream BI pipeline consumes from Kafka. A new event version adds optional fields and changes a field type; describe a concrete plan to implement schema evolution with backward/forward compatibility, including staging readers with dual schemas, feature flags, and a rollback strategy, plus how you would validate no data loss during rollout?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["IBM","LinkedIn","Meta"]},{"id":"q-4816","question":"CNPA stack: HTTP API writes Postgres, publishes to Kafka; Redis-backed read model. A staged feature flag rollout causes intermittent tail latency under load and occasional stale reads. Provide a concrete debugging plan to isolate whether bottlenecks are DB locks, Kafka backpressure, or Redis cache invalidation, with exact metrics, tracing steps, and fixes (row locks, partitioned topics, per-tenant warming) and verification steps?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Adobe","DoorDash","Google"]},{"id":"q-4847","question":"A CNPA stack with an HTTP API, PostgreSQL, and Kafka experiences intermittent 5xx errors during peak load. Design a beginner-friendly plan to determine whether the bottleneck is the HTTP handler, the DB connection pool, or the Kafka producer, including exact steps, metrics to collect, and concrete changes (pool sizing, timeouts, prepared statements, idempotent producer) and how you would verify impact?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Airbnb","MongoDB","Twitter"]},{"id":"q-845","question":"In a MongoDB-backed service, read latency tails spike during peak hours. Provide a concrete, practical debugging plan to determine whether the bottleneck is network, driver, query plan, or index design. Include exact steps, metrics to collect, and concrete changes (indexes, readConcern, pooling) you would apply, plus how you would verify impact?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Anthropic","Lyft","MongoDB"]},{"id":"q-873","question":"A CNPA service receives events over HTTP, writes to PostgreSQL, and publishes to Kafka. During peak hours, read latency spikes. Describe a concrete debugging plan to determine whether the bottleneck is the HTTP handler, the DB query, or the Kafka producer, including exact steps, metrics to collect, and concrete changes (indexes, pooling, prepared statements, idempotent producer) and how you would verify impact?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Databricks","Instacart","NVIDIA"]},{"id":"q-903","question":"In a CNPA stack, an HTTP API writes to Postgres, publishes to Kafka, and a Redis-backed dashboard consumes the stream. During peak load, HTTP latency tails spike. Provide a concrete debugging plan to isolate whether the bottleneck is HTTP, DB, Kafka, producer, consumer, or Redis, with exact metrics, sampling, and concrete changes (pooling, prepared statements, acks, batch sizes, caching strategies) and how you would verify impact?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Bloomberg","Meta","Tesla"]},{"id":"q-928","question":"In a CNPA stack consisting of an HTTP API, PostgreSQL, Kafka, and Redis, latency tails spike under peak load. Provide a concrete, beginner-friendly plan to enable end-to-end tracing with OpenTelemetry to pinpoint the bottleneck. Include trace propagation, spans for HTTP handler, DB query, Kafka publish/consume, Redis access, and verification steps with a sample end-to-end trace?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Discord","Hashicorp","Netflix"]},{"id":"q-957","question":"CNPA stack with HTTP API, PostgreSQL, Kafka, and Redis dashboards requires a schema evolution: add a new optional field region_id to event records without downtime or breaking producers/consumers. Describe a practical, step-by-step migration plan: DB changes, schema registry versioning, producer/consumer updates, data backfill, testing, and rollback strategies, ensuring end-to-end consistency?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["PayPal","Scale Ai"]},{"id":"q-1193","question":"You have an array A of n positive integers and a threshold T. For a given window length L, define ok(L) as: there exists a subarray of length L with sum <= T. Design an O(n) check for ok(L) using a sliding window, then outline how to find the maximum L with binary search over [1..n], and analyze total time and space. Include edge-case handling and practical optimizations?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Airbnb","Snowflake","Tesla"]},{"id":"q-1254","question":"You're given a DAG G=(V,E) with N nodes and M edges. Edges can be inserted online in batches of size B. Design a dynamic transitive-closure using bitsets to answer reachability queries in O(1). Provide initialization, amortized per-batch update time, and memory. Include two practical optimizations and compare to recomputing the closure after each batch?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Goldman Sachs","Square","Two Sigma"]},{"id":"q-2159","question":"Scenario: In a weighted directed graph with nonnegative weights, you must maintain approximate betweenness centrality for all nodes under batches of edge insertions of size B. Propose a concrete, implementable scheme that (i) initializes estimates, (ii) updates after each batch with amortized cost, (iii) bounds memory, and (iv) provides guaranteed error bounds ε. Include two practical optimizations and compare to re-running Brandes' algorithm after every batch?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["LinkedIn","Snowflake","Two Sigma"]},{"id":"q-2497","question":"Maintain exact single-source shortest paths from a fixed hub h under online edge insertions (batches). Propose a practical scheme that (i) initializes with Dijkstra, (ii) after each batch re-relaxes only affected nodes using a min-heap, (iii) bounds memory, and (iv) gives amortized update time. Include two optimizations and compare to rerunning Dijkstra after every batch. Provide asymptotics in n, m, B?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Meta","Oracle","PayPal"]},{"id":"q-2635","question":"Scenario: A real-time analytics service ingests event messages with a 32-bit integer key. Events arrive in batches of size B every second, and the system must report the number of distinct keys seen so far (cardinality) with a configurable error ε. Propose a practical streaming algorithm and data structure (no deletions) to maintain an ε-approximate cardinality. After each batch, specify (i) per-element insertion time and total batch time, (ii) space usage, (iii) query time for the current estimate, (iv) two optimizations, and (v) how this compares to re-scanning all seen events after every batch. Use asymptotics in n (seen events), B, and ε?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Hashicorp","PayPal","Two Sigma"]},{"id":"q-2655","question":"Design a dynamic (1+ε)-spanner for an undirected weighted graph under online edge insertions in batches of size B. Build a (k,ε)-spanner with O(n^{1+1/k}) edges. After each batch, update only edges that improve distances beyond ε; amortized O(B log n). Query dist via the spanner in O(1). Memory O(n^{1+1/k}). Optimizations: lazy rebuild; reuse tight edges. Compare to rebuilding: faster updates; worst-case near O(n^{1+1/k}) per batch?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Robinhood","Zoom"]},{"id":"q-2726","question":"Scenario: A weighted directed graph with nonnegative weights tracks network latencies. Edges can be inserted or have their weights decreased in online batches of size B. After each batch, you must maintain exact distances from a fixed source S to all nodes. Propose a practical hybrid scheme that combines (i) an incremental update pass limited to affected nodes, (ii) periodic full re-computation every T batches, and (iii) a lazy rebuild trigger when incremental cost exceeds a threshold. Provide initial preprocessing, amortized update time per batch, memory bounds, two optimizations, and a comparison to re-running Dijkstra from scratch after every batch?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Databricks","Meta","Uber"]},{"id":"q-3064","question":"In a directed graph with nonnegative weights, updates arrive in batches of B edge insertions or weight decreases. After each batch, maintain P = { (u,v) | dist(u,v) ≤ D } for a fixed threshold D. Propose a practical scheme that (i) precomputes a compact structure, (ii) updates P after each batch with amortized time, (iii) bounds memory, and (iv) provides two optimizations. Compare to recomputing all-pairs distances up to D after every batch?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Databricks","Goldman Sachs","Tesla"]},{"id":"q-3179","question":"You maintain a growing list of N tasks with integer priorities. Operations arrive in batches of size B, appending new tasks. After each batch you must surface the current minimum priority quickly. Compare two schemes: (A) rebuild a heap from scratch after every batch; (B) insert the B new tasks into a single heap. Provide total time and per-operation costs, and derive the breakeven batch size B* in terms of N. Include memory usage considerations?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Slack","Tesla","Two Sigma"]},{"id":"q-3314","question":"Scenario: A streaming service receives integers in batches of size B. After each batch, output the median of all values seen so far. Propose a practical scheme that (i) uses two heaps to balance lower/upper halves, (ii) inserts each new value with O(log n) cost, (iii) bounds memory, (iv) offers two optimizations, and (v) compares to recomputing from scratch after every batch. Give asymptotics in terms of n and B?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Discord","Microsoft","Robinhood"]},{"id":"q-3412","question":"Write a function countPairs(n) with:\n for i from 1 to n:\n   for j from 1 to floor(n/i):\n     // O(1) work\nWhat is the overall time complexity and space usage? Provide the tight bound, discuss best/worst-case, and compare to a memory-efficient variant that avoids the inner loop entirely?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Google","Twitter"]},{"id":"q-3504","question":"Scenario: Streaming terms from a document corpus arrive in batches of B. Estimate the number of distinct terms seen so far using a probabilistic counter. Design an implementable plan with initial preprocessing, per-batch updates, memory bounds, two optimizations, and a comparison to exact counting. Include formulas in terms of B, U (distinct terms), and target error ε?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Goldman Sachs","Hugging Face","Slack"]},{"id":"q-3589","question":"You maintain a weighted undirected graph G=(V,E) with nonnegative weights. Edges arrive online in batches of size B. You need to answer s shortest-path queries dist(u_i,v_i) for i=1..s after each batch, within a (1+ε) approximation, reusing work across all pairs. Propose a single shared data structure (i) initialization to support all s pairs, (ii) amortized per-batch update cost when a batch arrives, (iii) per-query time, (iv) memory, (v) two practical optimizations, and (vi) a comparison to rebuilding from scratch per batch. Express asymptotics in n=|V|, m=|E|, s, B, ε?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Instacart","Plaid","Scale Ai"]},{"id":"q-3629","question":"In a directed graph with N nodes and M edges, edges arrive online in batches of size B. After each batch, estimate, with high probability, the number of nodes reachable from a fixed set S within at most D hops, within a (1±ε) factor. Propose a streaming sampling scheme: (i) pick k random walks of length D starting from S or uniformly; (ii) specify update rules per batch; (iii) derive variance/error bounds and how to set k for a target failure prob; (iv) amortized time and memory; (v) compare to rerunning a bounded-depth multi-source BFS after each batch. Provide asymptotics in n,m,k,B,ε,D?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Amazon","Discord","NVIDIA"]},{"id":"q-3881","question":"Scenario: In a weighted undirected graph G=(V,E) with nonnegative weights, edges arrive in online batches of size B. From a fixed root r, maintain a (1+ε)-approximate shortest path tree (SPT) to all nodes after each batch. Propose a practical scheme that (i) initializes the SPT, (ii) updates after a batch with amortized time, (iii) bounds memory, (iv) offers two optimizations, and (v) compares to rebuilding the SPT from scratch after every batch. Provide asymptotics in n=|V|, m=|E|, B, ε?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["LinkedIn","NVIDIA","Twitter"]},{"id":"q-3905","question":"Scenario: A ride-hailing network graph G=(V,E) with nonnegative weights receives edge insertions in batches of size B. You must maintain an ε-approximation to the top-r eigenvector centrality for all nodes after each batch, with update time sublinear in n. Propose a concrete scheme: (i) initial computation, (ii) update after batch using low-rank ΔA and a few subspace iterations, (iii) how to answer centralities fast, (iv) memory bounds, (v) two optimizations, (vi) comparison to recomputing from scratch after every batch. Express asymptotics in n, m, B, r, ε?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Plaid","Uber"]},{"id":"q-3920","question":"In a directed weighted graph G=(V,E) with nonnegative weights, edges are inserted online in batches of size B. After each batch you must maintain approximate eccentricities ecc(v)=max_u dist(v,u) for all v within a (1+ε) factor. Propose a practical scheme combining (i) a landmark-based distance substructure from a fixed set L (|L|=k), (ii) selective exact updates for nodes affected by the batch, and (iii) a periodic rebuild. Provide preprocessing, per-batch update time, memory, and a comparison to recomputing all-pairs eccentricities from scratch after each batch. Express asymptotics in n=|V|, m=|E|, k, B, ε?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Apple","Tesla"]},{"id":"q-3971","question":"Scenario: A DAG G=(V,E) with a fixed source S. Each edge e=(u,v) contributes 1 path along that edge; the number of distinct S→v paths is computed by DP over a topological order. Edges can be inserted online in batches of size B. Propose a practical hybrid scheme that maintains exact path counts to all v after each batch using (i) an incremental update pass limited to affected nodes, (ii) a periodic full recomputation every T batches, and (iii) a lazy rebuild trigger when incremental cost exceeds a threshold. Include initial preprocessing, amortized update time per batch, memory bounds, two optimizations, and a comparison to re-running the full DP after every batch?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Lyft","NVIDIA","Netflix"]},{"id":"q-4046","question":"Scenario: A log analytics service processes a high-velocity stream of user IDs and needs the number of unique users in the last W events (sliding window). Design two practical strategies to maintain that count as IDs arrive: (A) recompute from scratch for every window; (B) maintain a hash-map of counts and update incrementally. Provide asymptotics for time and space in terms of N, W, and batch size B, and compare to a full recompute after each batch?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Anthropic","Bloomberg","MongoDB"]},{"id":"q-4223","question":"You have n users as undirected nodes. Edges arrive in batches of size B, each batch adding friendships. After each batch, report the number of connected components in the graph. Propose a practical incremental scheme using a union-find (disjoint-set) structure with path compression and union by rank. Provide amortized per-edge cost, per-batch cost, and memory in terms of n, m, B, alpha(n). Also discuss when a rebuild might be preferable?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Meta","Stripe"]},{"id":"q-4296","question":"Count inversions in an array of length n. Compare the naive approach O(n^2) with a merge-sort based approach that runs in O(n log n). Explain which inputs favor each, and discuss space usage and stability implications. End with ?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Lyft","NVIDIA"]},{"id":"q-4352","question":"You have an integer array a of length n. You receive online batches of B point updates (set a[i] = v). After each batch you must answer: how many elements in a exceed a fixed threshold T? Propose a practical data structure and amortized analysis for updates and queries, including preprocessing time, memory, and a comparison to recomputing counts from scratch after every batch. Assume B ≤ n and T is fixed?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Amazon","Apple","Databricks"]},{"id":"q-4459","question":"You receive a real-time stream of integers in [0, M-1], arriving in batches of size B. After each batch, determine whether value x has appeared at least t times so far. Propose a Count-Min Sketch based solution: set width w and depth d in terms of ε and δ to bound errors, compute update time per batch, per-query time, memory, and compare to exact counting. Assume B, t, ε, δ fixed?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Bloomberg","DoorDash","OpenAI"]},{"id":"q-4598","question":"Dynamic SSSP under batch edge insertions: A directed graph with n nodes; edges are added online in batches of size B. After each batch, maintain approximate distances dist(s,v) from a fixed source s within a (1+ε) factor for all v. Propose a practical scheme using (i) a fixed landmark set L with precomputed dist(s,l) and dist(l,v), (ii) selective exact relaxations for nodes affected by the batch, and (iii) periodic rebuild. Provide preprocessing, per-batch update time, memory, and a comparison to recomputing SSSP from scratch?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Google","Salesforce"]},{"id":"q-4702","question":"Real-time logs: integers in [0, M-1] arrive in batches of size B. After each batch, determine how many distinct values have appeared at least t times in total so far. Propose a practical solution using approximate counting, discuss update/query costs, and memory, and compare to recomputing counts from scratch. Express costs in terms of M, B, t, and number of processed batches?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Bloomberg","Instacart","Netflix"]},{"id":"q-4715","question":"Dynamic graph: edges are inserted in batches of size B in a weighted directed graph. After each batch, design a practical incremental algorithm to maintain approximate betweenness centrality for all nodes within a (1+ε) factor. Use sampling with s sources and incremental SP-tree updates; specify preprocessing time, per-batch update time, memory, and compare to recomputing BC from scratch?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["IBM","Oracle","Snap"]},{"id":"q-4830","question":"You receive a stream of integers in [0, M-1] in batches of B. Maintain a sliding window of the most recent N elements. After each batch, report the current number of distinct values in the window. Propose a practical data structure (e.g., hashmap + queue) to support batch inserts, updates, and a correct count, and derive update time, memory, and a comparison to recomputing from scratch?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["LinkedIn","MongoDB","Snowflake"]},{"id":"q-678","question":"In a directed acyclic graph with N nodes and M edges, all edge costs are nonnegative. Compute the minimum-cost path from S to T. Costs can decrease online; design a strategy to maintain shortest paths with updates, aiming for sublinear re-computation on average. Provide initial complexity and amortized update complexity, plus memory usage and practical optimizations?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Lyft","Meta","Oracle"]},{"id":"q-690","question":"Design a data structure to support two online operations on an integer array A of length N: 1) rangeAdd(l, r, delta) adds delta to A[i] for l <= i <= r, 2) queryMaxSubarray() returns the maximum subarray sum of the current A. Provide a structure that supports both operations in O(log N) time, describe what to store per node, how to merge children, and how to apply a lazy add. Include correctness and complexity considerations?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Goldman Sachs","Microsoft"]},{"id":"q-700","question":"You're building a real-time analytics dashboard that shows the top-k most frequent event types from a high-volume log stream (e.g., clicks, errors). Each event has a string type. Design a data structure and algorithm to maintain the current top-k frequencies with online increments, aiming for roughly O(log k) update time and O(n) memory. Explain how you handle ties and memory growth, and compare with a naive approach that re-sorts after every insert?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Bloomberg","Google"]},{"id":"q-704","question":"Scenario: a directed graph with nonnegative weights and a fixed source S. Each batch updates up to B edges (increases or decreases). Propose a practical dynamic data structure to maintain exact distances from S to all nodes and answer distance queries S→T in polylog time, with sublinear amortized update time. Compare to rerunning Dijkstra after every batch; include expected bounds, memory usage, and practical heuristics?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Salesforce","Snap"]},{"id":"q-709","question":"Given a directed graph with nonnegative weights, a fixed source S, and a stream of online edge weight updates (both increases and decreases), design a dynamic SSSP data structure that maintains exact distances dist(S, v) for all v after each update. Aim for sublinear amortized update time per edge change; specify initial preprocessing, worst-case vs amortized bounds, memory usage, and practical optimizations. Provide a plan for applying this in a traffic-graph scenario with frequent but localized updates?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["OpenAI","PayPal"]},{"id":"q-721","question":"Given a fixed directed graph with nonnegative weights and a single source S, handle a batch of edge-weight decreases (no insertions/deletions). Design a dynamic algorithm to update the exact S→v distances after each batch with sublinear amortized per-edge cost. Specify data structures, provide an amortized bound, and discuss memory and practical optimizations for real-time traffic networks?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Lyft","NVIDIA","Two Sigma"]},{"id":"q-732","question":"Scenario: A data stream yields integers. At each time step, a new value enters a sliding window of fixed size W, and the oldest value leaves. Design a solution to maintain the top-2 most frequent values in the current window with fast updates. Compare a naive O(W) recompute to an augmented structure using a frequency map and a max-heap with lazy deletions. Provide update and query complexities and memory usage?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Meta","Twitter","Two Sigma"]},{"id":"q-740","question":"Scenario: An edge CDN collects response times in milliseconds for every request. Design a beginner-friendly online algorithm to maintain the median latency as new times arrive, using only inserts. Explain the data structure, update steps, and time/space bounds, assuming up to 1e6 entries?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Cloudflare","Coinbase","Oracle"]},{"id":"q-742","question":"In a DAG with N nodes and M edges, nonnegative edge weights. You maintain shortest-path distances from source S to a fixed set of target nodes {T1,...,Tk}. Edge weights can only decrease over time due to updates. After a batch of updates, you should update only the target distances that can improve, avoiding full re-relaxation. Propose a practical algorithm that lazily propagates decreases using the DAG’s topological order, such that total work across updates is sublinear on average. Provide update and query steps, concrete time bounds, and memory usage, plus optimizations?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Microsoft","Robinhood"]},{"id":"q-755","question":"You're maintaining real-time travel times in a citywide road network modeled as a weighted directed graph with nonnegative costs. Costs can only decrease as new data arrives. Design an incremental algorithm to keep the shortest-path distances from a fixed hub S to all nodes up-to-date after each edge-cost decrease, aiming for sublinear amortized update work. Provide initial SSSP complexity, amortized per-decrease update, memory usage, and practical optimization strategies?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Discord","Google","Zoom"]},{"id":"q-766","question":"## Prompt\n\nIn a dynamic directed graph G=(V,E) with nonnegative weights, edge latencies only decrease in batches. Design a practical, production-ready algorithm to maintain a (1+ε)-approximate SSSP tree from a source S under these updates, enabling distance queries dist(S,v) in O(log|V|) time. Target sublinear amortized update in |E| for batch updates, and linear space. Explain data structures, update bounds, and how you bound cascade effects?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Discord","Meta","OpenAI"]},{"id":"q-770","question":"Given a directed graph G=(V,E) with |V|=N and |E|=M, nonnegative weights, support online operations: 1) decreaseWeight(u,v,delta) with delta>0, 2) queryShortestPath(S,T) returning current shortest path length. Updates and queries are interleaved. Propose a data-structure and algorithm that achieves sublinear amortized reprocessing per update, justify amortized bounds, and discuss space and practical optimizations for massive graphs (N up to 1e6, M up to 1e7)?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Citadel","Lyft"]},{"id":"q-779","question":"You're building a real-time analytics component for a fintech platform. You must maintain the number of distinct values in the most recent W events in a streaming fashion. Implement two operations: append(v) to push a new event value, and distinctCount() to return the number of unique values among the last W events. Assume values are integers up to 1e9 and W can be large. Provide a simple approach with its time/memory costs, then describe an amortized-constant-time solution using a hashmap plus a circular buffer, and discuss edge cases (e.g., large W, many duplicates). How would you implement and analyze it?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Databricks","LinkedIn","Robinhood"]},{"id":"q-787","question":"You maintain N players with integer scores in the range 0..10000. You must support two online operations: 1) update(i, s) — set player i's score to s; 2) countLE(X) — return how many players have score <= X. Propose a data structure and algorithm to support both in O(log V) time per operation, where V=10001, and analyze space usage. Include initialization and a brief correctness sketch?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Apple","Cloudflare","Snowflake"]},{"id":"q-796","question":"Given an array A of length N with integers in range [0, R). You implement a function to count distinct values by inserting each A[i] into a hash set, then return its size. 1) What is the time and space complexity in terms of N and D (distinct values)? 2) If R is small, propose a memory-efficient alternative and analyze its tradeoffs?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["DoorDash","Netflix","PayPal"]},{"id":"q-803","question":"In a directed graph G=(V,E) with N nodes, M edges and nonnegative weights, a fixed source S, and an SSSP tree T. Edges can change weight online (increase or decrease), but no edges are added or removed. Propose a concrete, implementable strategy to maintain the SSSP efficiently, including data structures, update rules, and expected time bounds. Provide preprocessing, amortized per-update, and memory usage, plus practical optimizations and a concrete scenario where it shines (e.g., streaming latency updates)?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Instacart","LinkedIn"]},{"id":"q-808","question":"Dynamic path counting in a DAG: maintain the number of S→T paths of length at most L under online edge insertions and deletions. Propose a data structure and amortized update time bound in terms of N, M, L and #updates; discuss memory usage and how to handle large L and modulo arithmetic in practice?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Citadel","Snowflake","Zoom"]},{"id":"q-817","question":"Design a dynamic, multi-source shortest-path maintenance scheme for a directed graph with nonnegative weights. A fixed set of K hub nodes H must always have exact shortest-path distances to all nodes. Edges can be inserted or weights decreased online, in batches of size at most B. Provide initial preprocessing and a full-update algorithm, with (i) initial time, (ii) amortized update time per batch, and (iii) memory usage. Include two practical optimizations and compare to recomputing from scratch after each batch?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Bloomberg","PayPal","Uber"]},{"id":"q-827","question":"Design a dynamic distance labeling scheme for an undirected weighted graph that supports edge insertions and deletions online. Use a fixed hub set H to enable dist(u,v) queries via dist(u,v)=min_h dist(u,h)+dist(h,v) only if H covers all shortest paths. Explain maintenance of hub distances under updates, and bound update/query times and memory usage. Provide two optimizations and a comparison to recomputing from scratch?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Google","Meta","Microsoft"]},{"id":"q-831","question":"In an undirected weighted graph G=(V,E) with nonnegative weights, design a (1+ε)-approximate distance oracle based on a fixed landmark set L (|L|=k). Edges are only inserted online in batches of size B; no deletions. After each batch, specify: (i) preprocessing time and space to build distances from every landmark, (ii) amortized update time per batch to update the oracle, (iii) query time for dist(u,v), (iv) total memory, (v) two practical optimizations, (vi) a comparison to rebuilding all-pairs distances after each batch. Provide concrete asymptotics in terms of n=|V|, m=|E|, k, ε, B?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Instacart","Microsoft","NVIDIA"]},{"id":"q-1028","question":"Your team runs a multi-tenant SaaS analytics platform on Kubernetes in AWS. Each tenant lives in an isolated namespace with 100+ microservices. Explain how you enforce least privilege RBAC, ephemeral credentials with automatic rotation, secrets management, mTLS/workload identity (SPIFFE/SPIRE), policy-driven runtime enforcement, and a playbook for detecting and responding to cross-tenant data exfiltration?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Airbnb","Google","Meta"]},{"id":"q-1034","question":"In a multi-region SaaS platform running 3,000 microservices on Kubernetes in AWS with a shared data lake, design a secure data-access flow that enforces least privilege, uses workload identity (SPIFFE/SPIRE), ephemeral credentials with rotation, and policy-driven runtime checks. Include tenant isolation, secret management, and a playbook for cross-tenant data exfiltration?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Netflix","PayPal","Zoom"]},{"id":"q-1079","question":"In a multi-tenant rideshare analytics platform with per-tenant clusters and a shared data lake, design a zero-trust data-access pattern that enforces least privilege, uses SPIFFE/SPIRE for workload identity, ephemeral credentials with rotation, and policy-driven runtime checks. Include tenant isolation, secret management, cross-tenant data-exfiltration playbooks, and a DR/IR plan?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Lyft","Snap"]},{"id":"q-1315","question":"Scenario: A Databricks-based analytics platform on AWS feeds Slack alerts. Engineers share notebooks and data assets across teams with minimal controls. Propose a practical, beginner-friendly security plan that enforces least privilege using Unity Catalog RBAC, secrets management with rotation, and a CI check to block secrets in code, plus a 48-hour incident playbook for credential leakage or cross-tenant exposure?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Databricks","Slack"]},{"id":"q-1362","question":"Scenario: A multi-tenant data platform serving Apple, Adobe, and Snowflake runs data jobs in a shared Kubernetes cluster with per-tenant namespaces. An operator could pivot roles to access other tenants. Design a practical access-control plan enforcing least privilege via ABAC, dynamic approvals for sensitive actions, policy-drift detection, and a 24–72 hour rollback/incidence playbook. Include tools like OPA, AWS IAM, Kubernetes RBAC, Vault?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Adobe","Apple","Snowflake"]},{"id":"q-1379","question":"Scenario: A multi-tenant SaaS on Kubernetes in AWS uses a shared MongoDB Atlas cluster and Vault-managed secrets; Oracle reports data. Design a practical plan to enforce least privilege, issue per-tenant Vault DB credentials with short lifetimes, bound to tenant service accounts, include CI checks to block secrets in code, and a 48-hour IR playbook for credential leakage?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Hashicorp","MongoDB","Oracle"]},{"id":"q-1406","question":"In a multi-tenant data platform on AWS with a shared data lake and event bus, outline a practical plan to prevent cross-tenant leakage as data and events flow through data-plane and message-plane. Include (1) ABAC with OPA, (2) SPIRE-like identities and mutual TLS, (3) ephemeral credentials with rotation to data stores and topics, (4) drift detection/remediation, and (5) a 72-hour IR playbook for credential leakage or cross-tenant exposure, with concrete tool choices and trade-offs?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Hashicorp","Plaid","Twitter"]},{"id":"q-1501","question":"In a Slack-like multi-tenant platform deployed on Kubernetes in AWS with a central analytics data lake, a leaked tenant admin token is used to attempt cross-tenant data access via the data API. Design a practical, intermediate-level test plan to validate and harden per-tenant isolation. Include concrete abuse vectors to test, enforcement via RBAC/ABAC/OPA, SPIRE/mTLS, ephemeral credentials, and per-tenant secrets; outline CI checks, observability gates, and a concise IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Citadel","Slack","Square"]},{"id":"q-1529","question":"In a Kubernetes-based multi-tenant analytics platform on AWS where data and logs traverse a shared data plane to a central observability stack, design a production-ready plan to prevent cross-tenant data leakage via log/metric pipelines. Include enforcement with RBAC/ABAC/OPA, SPIRE/mTLS, ephemeral credentials, and per-tenant secrets; detail CI checks, observability gates, and an IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Lyft","Meta","MongoDB"]},{"id":"q-1686","question":"Context: A beginner security analyst joins a multi-tenant SaaS deployed on Google Cloud. The platform runs on GKE and a central data lake; tenants are isolated by per-tenant namespaces with restricted data access. Secrets live in Secret Manager and workloads use Cloud IAM + Workload Identity; CI/CD has a secret-scanning step. Design a practical, beginner-friendly security plan to enforce least privilege, implement secrets rotation, add a CI check to block secrets in code, and prepare a 48-hour incident playbook for credential leakage or cross-tenant exposure. Specify concrete steps, tooling, and success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Google","Instacart","Robinhood"]},{"id":"q-1889","question":"In a global multi-tenant analytics platform on Kubernetes with a shared data lake, design an advanced, concrete plan to verify and harden per-tenant isolation against cross-tenant data access, focusing on dynamic ABAC with OPA, SPIRE/mTLS, ephemeral credentials, and per-tenant secrets. Include abuse vectors, CI gates, observability checks, and an IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Cloudflare","Meta","Zoom"]},{"id":"q-1938","question":"In a beginner security role for a multi-tenant delivery platform on AWS, tenants store order data in per-tenant prefixes under a shared data lake. A developer accidentally attaches a Lambda execution role with broad access to all buckets. Propose a practical, beginner-friendly security plan to enforce least privilege with per-tenant IAM roles and bucket policies, implement per-tenant key rotation via Secrets Manager, and add a CI check to block broad policies before merges, plus a 48-hour incident playbook for credential leakage or cross-tenant exposure. Include concrete steps and success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["DoorDash","Hashicorp","Instacart"]},{"id":"q-2000","question":"In a Kubernetes cluster utilizing MIG partitions for shared NVIDIA GPUs, how would you enforce per-tenant GPU isolation and protect models and data from cross-tenant leakage? Outline architecture, policy controls (RBAC/ABAC/OPA), SPIRE/mTLS, ephemeral credentials, per-tenant secrets; include CI tests, observability gates, and a concise IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["NVIDIA","Square"]},{"id":"q-2091","question":"Design a practical per-tenant data-sharing broker for a SaaS analytics platform that must allow approved external collaborators to access only specific tenant datasets for a limited time. Propose the key-management model (per-tenant CMKs and envelope keys), a policy-driven issuance flow (OPA), service identities (SPIRE), and short-lived credentials. Include rotation cadence, audit/logging, and a concise IR/DR plan with success criteria. Give concrete steps and success metrics?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Amazon","Bloomberg","Databricks"]},{"id":"q-2114","question":"In a multi-tenant SaaS on AWS with a Kubernetes cluster and a shared artifact store, how would you harden the CI/CD pipeline to prevent tenant-confusion and supply-chain abuse during builds and releases? Include architecture: per-tenant runners, SPIRE/mTLS, Vault tenant roles, image signing, OPA policy checks, SBOM validation, per-tenant secret rotation, plus concrete test vectors and an IR/DR plan with measurable success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Apple","Snowflake"]},{"id":"q-2141","question":"In a cloud-native platform deployed to Kubernetes with GitOps pipelines, design an intermediate-level test plan to secure the software supply chain end-to-end, including SBOM provenance (SLSA), per-tenant secrets, and policy enforcement with OPA; outline CI gates, runtime observability, and an IR/DR playbook for a compromised dependency or insertions into the build process?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Cloudflare","Microsoft","Tesla"]},{"id":"q-2222","question":"In a cloud-native analytics platform where tenants upload datasets and train models against a shared data lake, a feature store isolates tenant data but allows cross-tenant model export. Design a concrete, intermediate-test plan to prevent leakage via cross-tenant feature reuse or model exports, including policy checks (OPA), RBAC/ABAC, per-tenant secrets, and CI/CD gates; include abuse vectors, observability gates, and an IR/DR playbook?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["IBM","Snap","Two Sigma"]},{"id":"q-2297","question":"In a cloud-native SaaS on Kubernetes with a centralized, multi-tenant logging/observability stack (OpenTelemetry + OpenSearch) and a shared data lake, design an intermediate test plan to prevent tenant data leakage through logs and traces. Specify how to ensure per-tenant isolation in logs, enforce redaction and tenant-scoped indexing, ABAC/OPA gates at ingest, SPIRE/mTLS between agents and collectors, ephemeral credentials for exporters, CI checks, observability gates, and a concise IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["DoorDash","Hashicorp","MongoDB"]},{"id":"q-2400","question":"Scenario: A multi-tenant e-commerce platform (Airbnb-like) processes guest and host data in a centralized data lake on AWS. A new data export feature allows hosts to download their own booking data as CSV; misconfigured IAM/bucket policy could leak other tenants' data. Propose a beginner-friendly security plan to enforce least privilege using IAM roles, S3 bucket policies, and KMS, add a CI gate to block secrets in code, and draft a 48-hour IR playbook for credential leakage or cross-tenant exposure; include concrete steps, tooling, and success metrics?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Airbnb","Amazon"]},{"id":"q-2472","question":"Context: A DoorDash-like platform runs on Kubernetes in AWS with a centralized analytics data lake and edge workloads. A rogue build signs a container image that deploys and tries tenant data access via shared APIs. Design an advanced test plan to prevent supply-chain abuse and enforce tenant isolation from build to runtime. Include Sigstore signing with attestations, OPA-based policies, per-tenant secrets, ephemeral credentials via SPIRE, CI image-sign checks, and a concise 60-minute IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["DoorDash","Lyft","OpenAI"]},{"id":"q-2516","question":"Context: A beginner security analyst at a large tech org reviews a Meta/Adobe‑like SaaS portal with a central data lake. Developers push via GitHub Actions; tenants are isolated by per‑tenant RBAC. A tenant API key was leaked and used to probe other tenants. Design a practical, beginner-friendly security plan to enforce least privilege with per‑tenant RBAC, rotate secrets automatically, add a CI gate to block secrets in code, and a 48‑hour IR/DR playbook for credential leakage or cross‑tenant exposure. Include concrete steps, tooling, and success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Adobe","Meta"]},{"id":"q-2668","question":"In a beginner-friendly AWS multi-tenant SaaS on Scale AI and Amazon, tenants are isolated with per-tenant IAM roles and tenant prefixes in a central data lake. A developer accidentally commits a long‑lived credential to a public repo. Propose concrete steps to enforce least privilege (RBAC), implement automatic secret rotation, add a CI gate to block secrets in code, and craft a 24‑hour IR playbook for credential leakage or cross‑tenant exposure, including tooling?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Amazon","Scale Ai"]},{"id":"q-2727","question":"**Scenario**: A multi-tenant Databricks Lakehouse on AWS uses Unity Catalog for isolation. A rogue tenant’s pipeline can access the shared model registry and feature store, risking cross-tenant leakage. Design a concrete, intermediate-level test plan to validate and harden isolation against leakage via these artifacts. Include per-tenant RBAC/ABAC, OPA policies, ephemeral credentials, per-tenant secrets, CI gates, runtime checks, observability gates, and a 24–48h IR/DR playbook with success criteria; provide a test that fails before hardening and passes after.**?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Databricks","IBM"]},{"id":"q-2810","question":"Context: A Kubernetes-based multi-tenant SaaS platform on AWS with a shared CI/CD pipeline. Design an intermediate-level test plan to secure the software supply chain and enforce per-tenant isolation in builds and deployments. Include SBOM generation, artifact signing with Sigstore/Cosign, dependency auditing, per-tenant build secrets, and runtime policy via OPA or Kubernetes admission controllers; outline abuse vectors, CI checks, observability gates, and an IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Cloudflare","Google","Scale Ai"]},{"id":"q-2821","question":"Design an intermediate-level, practical test plan for a real-time, multi-tenant event bus powering a marketplace platform (Airbnb/Plaid/Google-style). Tenants subscribe to streams using per-tenant credentials. Outline testing for tenant isolation, including per-tenant encryption, event signing, replay protection, ABAC/OPA policy enforcement, ephemeral credentials, CI gates, observability gates, and a concise IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Airbnb","Google","Plaid"]},{"id":"q-2848","question":"In a Google Cloud-hosted multi-tenant SaaS on GKE with a central data lake, a tenant's service account is compromised and can reach other namespaces via the Kubernetes API and access per-tenant secrets. Design a beginner-friendly plan to prevent lateral movement and data exfiltration, covering namespace isolation, ephemeral credentials, a CI gate to block secrets in code, and a 48-hour IR playbook. Include concrete steps, tooling, and success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Google","Hashicorp"]},{"id":"q-2879","question":"Context: A multi-tenant SaaS runs on Kubernetes in AWS; tenant data at rest uses per-tenant envelope encryption with tenant CMKs in KMS and a central data lake. A leaked admin token from one tenant could be used to access data. Design a practical, advanced test plan to validate and harden cryptographic isolation, covering per-tenant keys, envelope encryption, HSM-backed CMKs, ephemeral credentials, ABAC/OPA, and tenant-scoped secrets; outline CI checks, observability gates, and a concise IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Apple","Lyft"]},{"id":"q-2920","question":"In a multi-tenant ML model hosting platform (like Hugging Face) deployed on Kubernetes, ingestion uses CI that pulls vendor models into a shared runtime. Outline a practical intermediate-level interview plan to ensure provenance, tamper-detection, and least privilege for model-serving components. Include artifact signing (Sigstore), SBOMs, reproducible builds, per-tenant RBAC/ABAC, runtime enforcement (SPIRE/mTLS, OPA), observability gates, and a 48-hour IR/DR playbook?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Hugging Face","Snap","Square"]},{"id":"q-3033","question":"In a Kubernetes-based multi-tenant AI inference platform, tenants upload models that could exfiltrate data via shared embeddings. Design an intermediate-level test plan to validate per-tenant isolation at the API and data plane. Include RBAC/ABAC/OPA, SPIRE/mTLS, ephemeral credentials, per-tenant secrets, CI checks, observability, and a 48-hour IR/DR playbook?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["NVIDIA","Plaid"]},{"id":"q-3077","question":"Context: A Cloudflare-like edge network serves multi-tenant real-time collaboration streams. How would you design a concrete test plan to validate and harden per-tenant isolation using edge RBAC/ABAC, OPA policies, SPIRE mTLS, and per-tenant ephemeral credentials, including abuse vectors, CI policy gates, observability checks, and a 72-hour IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Cloudflare","MongoDB","Zoom"]},{"id":"q-3092","question":"Context: Tesla operates a fleet of connected vehicles (IoT edge devices) that stream telemetry to a cloud data lake and run lightweight microservices in Kubernetes. Developers push code via GitHub Actions; secrets sometimes leak to logs or PRs. Propose a beginner-friendly security plan that: 1) enforces least privilege with per-service RBAC and dedicated ServiceAccounts, 2) centralizes secrets in a rotation-enabled secret manager, 3) adds a CI gate to block secrets in code, and 4) outlines a 24-hour incident playbook for credential leakage or cross-component exposure. Include concrete steps, tooling, and success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Scale Ai","Tesla"]},{"id":"q-3133","question":"Context: A multi-tenant SaaS on AWS uses a CI/CD pipeline that pulls NPM/Git dependencies and signs artifacts with Cosign for internal registry. A malicious transitive dependency could compromise builds. Propose a beginner-friendly plan to harden the supply chain: enforce SBOM, sign verification, dependency pinning, least-privilege registry access, and a 48-hour IR/DR playbook with concrete steps and success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["LinkedIn","Square","Two Sigma"]},{"id":"q-3224","question":"Scenario: A cloud-native, multi-tenant platform on Kubernetes in AWS with a central analytics lake and a telemetry pipeline. A tenant’s data encryption keys are rotated automatically, but a compromised key policy could enable cross-tenant decryption. Design an advanced test plan to validate tenant isolation at the data-crypto boundary. Include per-tenant KMS isolation, envelope encryption, key lifecycle, RBAC/ABAC/OPA controls, SPIRE/mTLS between services, ephemeral credentials, per-tenant secrets, CI checks, observability gates, and a 72-hour IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Coinbase","Hugging Face","Uber"]},{"id":"q-3274","question":"Context: A multi-tenant ML platform hosted on AWS/GCP with per-tenant model stores and a shared feature store. Tenants call hosted models via an API gateway. An adversary from one tenant probes a model and attempts to reconstruct sensitive data from outputs. Design a practical, end-to-end test plan to prevent model leakage and enforce data locality. Include differential privacy controls, per-tenant RBAC/ABAC/OPA, per-tenant secrets, ephemeral credentials, a CI gate to block leakage in notebooks, and a 48-hour IR/DR playbook with clear success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Cloudflare","Google"]},{"id":"q-3546","question":"In a multi-tenant platform with a central event bus and per-tenant webhook subscriptions, an attacker subscribes to broad topics to exfiltrate other tenants' events. Design an intermediate test plan to harden event routing and data isolation, including abuse vectors, RBAC/ABAC/OPA enforcement, SPIRE/mTLS, ephemeral credentials, per-tenant secrets, CI gates to block secrets, observability gates, and a 48-hour IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Discord","DoorDash","Meta"]},{"id":"q-3558","question":"Context: A fintech SaaS used by Apple, Snap, and Plaid relies on open-source dependencies. A third-party library update introduces risk. Propose a beginner-friendly plan to reduce supply-chain risk: generate an SBOM, integrate SCA in CI, enforce code signing, gate deployments with OPA policies, and prepare a 24-hour incident response playbook for a suspected compromised dependency. Include steps and success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Apple","Plaid","Snap"]},{"id":"q-3582","question":"Context: A Twitter/Snap-like multi-tenant service on AWS EKS uses per-namespace isolation. Misconfigurations allow service accounts to leak tokens across tenants. Propose a beginner-friendly plan to enforce least privilege with per-namespace RBAC and ABAC via OPA, implement ephemeral credentials rotation, and craft a 48-hour incident playbook. Include concrete steps, tooling, and success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Snap","Twitter"]},{"id":"q-3632","question":"Context: A multi-cloud AI platform used by LinkedIn and Tesla for data ingestion, model training, and a shared feature store. Design a practical security plan focusing on CI/CD and runtime governance to prevent supply-chain attacks and cross-tenant data leakage. Include per-tenant artifact isolation, SBOM/CVE gates, ephemeral credentials, SPIRE/mTLS, ABAC/OPA enforcement, and a 48-hour IR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["LinkedIn","Tesla"]},{"id":"q-3729","question":"Context: A DoorDash-like platform with mobile apps and backend on AWS uses OAuth2; a token leakage risk across tenants via mobile app storage exists. Propose a beginner-friendly security plan to mitigate token theft and cross-tenant access, covering: PKCE-enabled OAuth2 flow for mobile clients; short-lived access tokens with device-bound refresh tokens; API gateway token introspection; a CI gate to block secrets in code; and a 72-hour IR playbook for credential leakage. Provide concrete steps, tooling, and success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Apple","DoorDash","Scale Ai"]},{"id":"q-3972","question":"Context: An AI inference platform uses a shared Redis cache and object store to hold per-tenant embeddings and prompts. A compromised token could access another tenant’s data via the cache. Propose an advanced, practical plan to enforce strict per-tenant isolation in the data path, covering: tenant-scoped cache namespaces, per-tenant encryption keys with rotation, API gateway token-binding, ABAC/OPA checks, and a 24-hour IR playbook with concrete steps?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Anthropic","NVIDIA","OpenAI"]},{"id":"q-4018","question":"Context: A multi-tenant SaaS on a shared Kubernetes cluster uses a CI/CD pipeline to build and deploy images across tenants. A rogue image signed with a compromised key is deployed in a tenant namespace. Design a defense-in-depth plan to prevent, detect, and respond to supply-chain attacks, covering: code signing with key rotation and hardware-backed storage; SBOM + image attestations via Sigstore; CI gates enforcing image provenance; Kubernetes admission controls (OPA/Gatekeeper) enforcing per-tenant provenance; runtime detection (Falco) and a 24-hour IR playbook with containment and eradication steps. End with concrete steps and success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Adobe","Meta","Robinhood"]},{"id":"q-4048","question":"Context: A multi-tenant fintech platform exposes a customer SDK with hard-coded credentials embedded in apps; a leak could grant cross-tenant access. Propose a beginner-friendly security plan to prevent secret leakage and enforce least privilege, covering per-tenant vault-based credentials, short-lived tokens bound to device attestation, a CI gate to block secrets in code, and a 48-hour IR playbook?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Discord","Plaid","Stripe"]},{"id":"q-4099","question":"Context: A multi-tenant SaaS on AWS provides per-tenant data isolation. Customers supply their own KMS keys (BYOK) to encrypt data at rest; an incident reveals misconfigured IAM policies allowed cross-tenant key access. Propose a beginner-friendly security plan to strictly enforce BYOK with per-tenant CMKs, enforce envelope encryption, require automatic key rotation, harden IAM least privilege and access boundaries, and a 48-hour IR playbook for key compromise. Include concrete steps, tooling, and success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Discord","Hashicorp","Scale Ai"]},{"id":"q-4169","question":"Context: A DoorDash-like platform runs microservices on Kubernetes with images pulled from multiple registries. A recently published base image contains a known vulnerability. Propose a beginner-friendly security plan to enforce image provenance via cryptographic signing (Cosign), SBOMs, a CI gate blocking unsigned images, and a 72-hour IR playbook for compromised artifacts, with concrete steps and success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["DoorDash","Twitter"]},{"id":"q-4205","question":"Context: A Citadel/Instacart-like multi-tenant e-commerce platform runs microservices in Kubernetes and a central data lake on AWS. A widely-used open-source library has a compromised release that propagates through transitive dependencies. Design an advanced, production-grade secure software supply chain plan to prevent compromised code from reaching production within 24 hours. Include SBOM generation, dependency pinning, reproducible builds, cryptographic signing, CI/CD verification, runtime attestation, per-tenant secret controls, gated approvals, and a 24-hour incident response/contingency playbook with measurable success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Citadel","Instacart"]},{"id":"q-4421","question":"Context: A Meta/Microsoft/Zoom-like multi-tenant collaboration platform hosting per-tenant serverless functions and live messaging. A compromised function role token is observed being used to enumerate buckets in the central data lake and exfiltrate data from another tenant. Propose an intermediate, production-ready security test plan to validate tenant isolation and least privilege for serverless workloads. Include concrete abuse vectors to test, enforcement via per-tenant IAM with RBAC/ABAC, SPIRE/mTLS between services, ephemeral credentials, per-tenant secrets with rotation, CI gates to block secrets, observability gates, and a 48-hour IR/DR playbook with measurable success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Meta","Microsoft","Zoom"]},{"id":"q-4627","question":"In a Netflix-like streaming platform running on EKS with a central data lake and per-tenant data silos, a rogue microservice gains access to another tenant's analytics bucket via misconfigured RBAC and trust boundary drift. Design a concrete plan to prevent cross-tenant data exfiltration, covering SPIFFE IDs, mTLS, OPA-based per-tenant authorization, per-tenant KMS keys and envelope encryption, and an IR playbook?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Hashicorp","Netflix","Slack"]},{"id":"q-4681","question":"Context: A multi-tenant analytics platform on AWS uses a centralized data lake with per-tenant workspaces. A misconfigured IAM policy could expose another tenant's data via Spark/Presto queries. Design an intermediate, practical plan to enforce strict per-tenant isolation in analytics queries, covering data labeling/partitioning, Lake Formation/ABAC, short-lived credentials, row/column masking, and a 48-hour IR playbook with measurable success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Bloomberg","Databricks","Meta"]},{"id":"q-4736","question":"In a Twitter-like microservice platform deployed on Kubernetes, a compromised image uploader service can be exploited to reach internal endpoints via SSRF and exfiltrate tenant data from object storage. Propose a concrete, production-ready defense-in-depth plan that prevents cross-tenant data access while preserving agility. Include per-tenant isolation, mesh security, identity validation, supply-chain controls, runtime protection, and a 24-hour IR playbook. What steps and success criteria would you implement?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Instacart","Twitter","Zoom"]},{"id":"q-4800","question":"Context: A Snap/Instacart-like multi-tenant SaaS builds Docker images in CI for tenants and deploys to a shared Kubernetes cluster. A secret accidentally ends up baked into an image and could be used to reach other tenants. Propose a beginner-friendly security plan focusing on: 1) per-tenant image signing/verification, 2) SBOM generation and vulnerability scanning in CI, 3) runtime secret management to avoid secrets baked into images, 4) an admission control gating for only signed images, and 5) a 48-hour IR playbook for baked secrets. Include concrete steps, tooling, and success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Instacart","Snap"]},{"id":"q-4821","question":"Context: A multi-tenant SaaS platform runs microservices in Kubernetes across multiple AWS accounts under a single org. A compromised developer workstation begins assuming prod roles via misconfigured trust policies, risking tenant data. Design a practical, intermediate-level plan to prevent lateral movement and rapidly detect/contain it. Include: per-tenant isolation via accounts, role trust boundaries, ephemeral credentials (STS), automated policy gates (OPA), audit/logging, testing plan, and a 48-hour IR playbook with measurable success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Apple","MongoDB","Square"]},{"id":"q-883","question":"Describe a practical, scalable secure software supply-chain workflow for an automotive platform with OTA-enabled ECUs, leveraging SBOMs, code signing, and runtime attestation. Include concrete steps for CI/CD isolation, secret management, artifact signing, and incident response?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Hashicorp","Salesforce","Tesla"]},{"id":"q-974","question":"Given a global fintech platform delivering real-time risk analytics via a fleet of edge appliances and cloud microservices, outline a practical, end-to-end security workflow for secure software and firmware updates that ensures provenance, integrity, and trust. Include: SBOMs, code signing, runtime attestation for both containers and devices; CI/CD isolation and secrets management; artifact signing and key rotation; secure OTA rollout with canarying and rollback; and incident response playbooks?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Goldman Sachs","Meta","Tesla"]},{"id":"q-988","question":"Scenario: A fintech app runs microservices in Kubernetes on AWS and Cloud Run on GCP. You must implement least privilege and dynamic secrets for a data-access service. Outline a practical, beginner-friendly workflow to give the service temporary credentials to a Postgres DB, with a choice between Vault or cloud-native secret managers. Include CI/CD integration, rotation, RBAC, and auditability?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Bloomberg","Citadel","Cloudflare"]},{"id":"q-1018","question":"You’re building a mobile camera app that auto-captures frames from a live video feed. Describe a practical, beginner-friendly pipeline to decide whether a frame is usable by applying two simple checks: (1) sharpness via variance of Laplacian, (2) exposure via histogram-based brightness. Explain how thresholds would be chosen, how you'd adapt them across lighting, and provide a minimal code snippet illustrating the core checks?","channel":"computer-vision","subChannel":"general","difficulty":"beginner","tags":["computer-vision"],"companies":["Goldman Sachs","LinkedIn","NVIDIA"]},{"id":"q-1093","question":"Design a privacy-preserving, real-time hand-gesture recognition system for video calls on consumer laptops (720p camera) that distinguishes a small set of gestures (hand-raise, thumbs-up, peace) without exposing facial details. Must run on-device at 30–60 FPS, handle lighting/occlusion, and support federated fine-tuning with differential privacy. Outline architecture, data strategy, and evaluation plan?","channel":"computer-vision","subChannel":"general","difficulty":"intermediate","tags":["computer-vision"],"companies":["Hashicorp","Tesla","Zoom"]},{"id":"q-1108","question":"Design a real-time, on-device hand pose/gesture system for air-drawing in a video-conferencing app. From a monocular 1080p60 stream, infer 2D/3D hand pose with <40ms latency on a CPU, robust to occlusion and varied skin tones, and support at least 5 gestures (draw, erase, next, previous, pointer). Outline data needs, model architecture, latency optimizations, temporal consistency, and evaluation plan?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Adobe","Plaid","Zoom"]},{"id":"q-1270","question":"Design a real-time monocular 3D detector for an assembly line that estimates 6-DoF pose of tools from a single RGB camera, achieving sub-50ms per frame on embedded hardware. Use a lightweight backbone with self-supervised pretraining plus a small labeled set; recover pose via differentiable PnP from 2D-3D correspondences with a temporal filter and reprojection losses. Monitor drift with streaming per-frame errors?","channel":"computer-vision","subChannel":"general","difficulty":"intermediate","tags":["computer-vision"],"companies":["Goldman Sachs","Hashicorp","MongoDB"]},{"id":"q-1417","question":"Given a single RGB camera mounted on an autonomous delivery drone operating in urban environments, design a real-time system to detect and track pedestrians and other vulnerable actors at 30 fps under nighttime and rain conditions. Propose data strategy (synthetic rain + real), model backbone, temporal fusion and latency targets, and safety/failover mechanisms?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Amazon","Cloudflare","Uber"]},{"id":"q-1435","question":"For aerial inspection of solar farms, design a CV system to detect and grade micro-cracks on solar panels from drone video with limited labeled data. Specify an architecture that detects tiny defects, data-augmentation strategies (synthetic crack overlays, texture randomization), domain adaptation, and an edge-friendly output (box, segmentation mask, and severity score). Include evaluation protocol and latency targets?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Bloomberg","MongoDB","Scale Ai"]},{"id":"q-1861","question":"You're given an overhead RGB image of a table with scattered coins. Design a beginner-friendly pipeline to count the number of coins and estimate their approximate denomination from a single image. Include preprocessing, circle/contour detection, radius-based grouping to separate coin types, handling shadows, and a minimal code sketch using OpenCV to detect circular shapes?","channel":"computer-vision","subChannel":"general","difficulty":"beginner","tags":["computer-vision"],"companies":["Goldman Sachs","Instacart","Snowflake"]},{"id":"q-1945","question":"Design a real-time anomaly-detection pipeline for a single RGB camera monitoring an industrial warehouse. Propose a memory-augmented autoencoder approach that runs on an edge device at 15–20 FPS, uses frame-wise reconstruction error plus optical-flow residuals, maintains a fixed-size normal-pattern memory, and includes a drift-adaptation strategy with minimal labeling?","channel":"computer-vision","subChannel":"general","difficulty":"intermediate","tags":["computer-vision"],"companies":["Hashicorp","Hugging Face"]},{"id":"q-2131","question":"In a factory setting, design a real-time 2-view RGB tool-pose tracking system that achieves sub-60ms per frame on edge hardware. Use two synchronized cameras, a lightweight backbone, and a differentiable PnP with 2D-3D correspondences; include cross-view fusion, a temporal filter, and a self-supervised pretraining strategy with synthetic data. How would you validate drift and occlusion resilience?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Adobe","OpenAI","Two Sigma"]},{"id":"q-2242","question":"In a warehouse setting, design an edge-friendly CV pipeline that counts and localizes pallets from a single moving RGB camera mounted on a forklift, using self-supervised depth cues from motion and a lightweight 3D detector to output per-pallet 3D bounding boxes with uncertainty at 30 Hz. Address occlusion, dynamic workers, and domain shift between day and night?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Lyft","Square"]},{"id":"q-2333","question":"Design a beginner-friendly pipeline to detect and count red boxes moving on a conveyor using a single RGB camera in real time. Include HSV color space selection, dual-range red thresholds, noise removal with morphology, contour filtering by area/shape, and a simple line-crossing tracker. Provide a minimal OpenCV.js code snippet for core steps?","channel":"computer-vision","subChannel":"general","difficulty":"beginner","tags":["computer-vision"],"companies":["Coinbase","Google","Tesla"]},{"id":"q-2377","question":"Scenario: fixed overhead RGB camera watches a single shoebox on a shelf. Design a beginner-friendly, non-deep-learning pipeline to decide whether the box is upright within ±10 degrees using contour-based detection. Explain preprocessing, edge/shape heuristics, thresholds, and how to handle perspective distortion and occlusion. Include a minimal Python OpenCV snippet that outputs 'upright' or 'tilted'?","channel":"computer-vision","subChannel":"general","difficulty":"beginner","tags":["computer-vision"],"companies":["Oracle","Scale Ai"]},{"id":"q-2546","question":"Design a real-time monocular hand pose and gesture recognition system for AR UI on a battery-constrained headset. Propose a lightweight architecture (e.g., two-branch network with 2D heatmaps and a temporal encoder), latency target <25 ms per frame on edge hardware, occlusion handling, and 3D hand pose estimation—describe data strategy, loss terms, and evaluation?","channel":"computer-vision","subChannel":"general","difficulty":"intermediate","tags":["computer-vision"],"companies":["Cloudflare","NVIDIA","Snap"]},{"id":"q-2778","question":"Design a compact on-device system to detect and track pedestrians in urban traffic and estimate their 3D pose (6-DoF) from a monocular video feed. Constraints: sub-40 ms per frame on an embedded GPU, a lightweight backbone with self-supervised pretraining on unlabeled video plus a small labeled set, and robustness to occlusion, motion blur, and adverse weather. Describe architecture, data strategy, losses, and evaluation methodology?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Instacart","Lyft","Uber"]},{"id":"q-3001","question":"Design a real-time multi-view RGB-D fusion system to estimate 6-DoF tool poses and hand interactions on a 4-camera rig. Use a two-branch head: per-view 2D heatmaps plus a depth-voxel fusion module feeding a lightweight 3D pose regressor. Train with synthetic data and self-supervised cross-view consistency; monitor drift via reprojection error and pose-trajectory. Latency <40 ms?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Bloomberg","DoorDash","Tesla"]},{"id":"q-3027","question":"Scenario: A fixed RGB camera watches a horizontal conveyor belt in a packaging line. Bottles pass one by one; you must count each item and verify the cap is present and seated using only lightweight CV techniques (no deep nets). Propose a beginner-friendly pipeline: (1) background subtraction to segment bottles, (2) contour-based separation and simple tracking to avoid double counting, (3) HSV-based cap presence check, (4) a temporal filter to smooth counts across frames. Explain thresholds, occlusion handling, and provide a minimal Python/OpenCV outline?","channel":"computer-vision","subChannel":"general","difficulty":"beginner","tags":["computer-vision"],"companies":["Coinbase","Oracle","Uber"]},{"id":"q-3066","question":"On a fast, fixed conveyor, design a real-time monocular 6-DoF pose estimator for a robotic gripper picking tools with glossy surfaces and changing lighting. Target <=25 ms per frame on embedded edge hardware; use a lightweight backbone (MobileNetV3+FPN), differentiable PnP from 2D-3D correspondences, and a short temporal fusion. Describe data strategy, losses (reprojection, pose, temporal consistency), and how you monitor drift and recover from failures?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Hashicorp","Microsoft","Plaid"]},{"id":"q-3102","question":"Design a privacy-preserving real-time perception pipeline for monocular video on an embedded device that detects cars, pedestrians, and other scene objects while obfuscating faces and license plates in intermediate features. Outline architecture (edge detector, privacy module, differentiable de-identification loss), latency target <50 ms/frame, and evaluation combining object mAP with privacy leakage metrics?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Coinbase","Netflix","Tesla"]},{"id":"q-3228","question":"Design a real-time on-edge computer vision pipeline to identify grocery items on a fast-moving conveyor using a single RGB camera. The catalog resides in a MongoDB collection; new items arrive weekly. Achieve <40 ms/frame inference on embedded GPUs, handle unknown items with open-set recognition, and support rapid adaptation with a small labeled set plus self-supervised pretraining. Outline architecture, data flow, and evaluation?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Instacart","MongoDB"]},{"id":"q-3291","question":"Design a real-time monocular 6-DoF object tracker for a single RGB camera that can track up to 4 tools on an assembly-line in clutter and variable lighting. Propose a lightweight backbone (MobileNetV3) for initial pose, a differentiable renderer-based refinement (e.g., PyTorch3D) with reprojection and silhouette losses, and a streaming UKF to fuse measurements and handle occlusions. Explain data strategy, losses, latency targets (60 FPS), and evaluation protocol?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Coinbase","Instacart","Twitter"]},{"id":"q-3439","question":"Design a robust multi-view CV pipeline to detect tampering in high-volume financial documents scanned from different angles; propose a light 2D-3D fusion model using a differentiable renderer to verify cross-view consistency, augmented with synthetic tamper data and a small real set, plus a temporal consistency module to tolerate scan noise. Include latency targets and evaluation protocol?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Citadel","Snowflake"]},{"id":"q-3574","question":"Design a real-time 3D object tracker for autonomous warehouse robots that fuses stereo cameras, a lightweight LiDAR, and an IMU to track up to 6 moving pallets in clutter with occlusions. Propose a sensor fusion architecture (learned fusion head plus model-based tracker), data association (JPDA or MHT), and a latency target under 40 ms per frame on edge hardware. Include evaluation and failure handling?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Goldman Sachs","Meta","Tesla"]},{"id":"q-3576","question":"You have a fixed RGB camera overlooking a supermarket shelf. Design a beginner-friendly pipeline to detect when two items occlude each other in the frame to help picker guidance. Use only a single RGB stream, lightweight cues (color histograms, edge density, simple optical flow), and optionally a tiny classifier trained on a small labeled set. Outline data flow, thresholding strategy, and a minimal evaluation plan?","channel":"computer-vision","subChannel":"general","difficulty":"beginner","tags":["computer-vision"],"companies":["Cloudflare","Google","Instacart"]},{"id":"q-3635","question":"Scenario: a fixed RGB camera watches a conveyor; detect a missing screw by comparing live frame to a reference image using a lightweight template matching approach (normalized cross-correlation). Describe a real-time pipeline on a consumer CPU at 24 FPS: ROI selection, lighting normalization, pass/fail thresholding, and a simple occlusion guard. Include a minimal OpenCV NCC-based snippet and a plan for evaluation?","channel":"computer-vision","subChannel":"general","difficulty":"beginner","tags":["computer-vision"],"companies":["Goldman Sachs","Meta","Microsoft"]},{"id":"q-3692","question":"With two synchronized RGB cameras observing a factory belt, design a real-time system to detect and estimate 6-DoF poses of articulated tools (e.g., screwdrivers, wrenches) as they move. Use stereo cues, a lightweight backbone, weak supervision (motion, silhouette), and differentiable rendering for pose refinement. Target sub-60 ms per frame on edge hardware, include uncertainty estimates and a practical evaluation protocol?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Google","Slack","Snap"]},{"id":"q-3777","question":"Design a real-time 3D scene reconstruction system for a warehouse robot using a single RGB camera. Build a voxel-based TSDF map that separates static infrastructure from moving objects (people, pallets) and fuses frames with a dynamic mask. Specify voxel resolution, dynamic handling method (per-voxel dynamic score), pose estimation for fusion, latency target (20-30 ms/frame), and an evaluation plan including per-voxel error and dynamic object drift metrics?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["LinkedIn","Meta","Two Sigma"]},{"id":"q-3835","question":"Design a real-time monocular perception pipeline for a Tesla/PayPal-style ADAS/detection task: detect pedestrians and cyclists in adverse weather (rain, fog) using only a single RGB camera on edge hardware. Outline the architecture (backbone, detector head), how monocular depth cues are integrated, a weather-robust data strategy (augmentation, synthetic data, self-supervised tasks), latency targets, and an evaluation plan across IoU, depth accuracy, and safety-critical false negatives?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["PayPal","Tesla"]},{"id":"q-4001","question":"On a fixed RGB camera watching a conveyor belt, design a beginner-friendly CV pipeline to detect when an item is rotated away from upright by more than 15 degrees. Use only a single RGB stream and lightweight cues: color histograms, edge density, simple optical flow, and optionally a tiny classifier trained on a small labeled set. Outline data flow, thresholding strategy, and a minimal evaluation plan?","channel":"computer-vision","subChannel":"general","difficulty":"beginner","tags":["computer-vision"],"companies":["Discord","Square","Tesla"]},{"id":"q-4044","question":"Scenario: A fixed RGB camera watches a 3-row shelf in a convenience store. Each row should contain items from a category: snacks, drinks, or stationery. Design a beginner-friendly CV pipeline to detect when an item is placed in the wrong row (e.g., a drink on the snacks row) using only a single RGB stream. Use lightweight cues (color histograms, edge density, simple optical flow) and optionally a tiny classifier trained on a small labeled set. Outline data flow, thresholding strategy, and a minimal evaluation plan?","channel":"computer-vision","subChannel":"general","difficulty":"beginner","tags":["computer-vision"],"companies":["Citadel","Microsoft","Robinhood"]},{"id":"q-4069","question":"Context: A fixed rooftop RGB camera watches a campus bike lane. Design a beginner-friendly CV pipeline to detect cyclists and estimate their real-time speed using only RGB frames. Include a lightweight detector, a simple calibration-based mapping from pixel motion to m/s, a temporal filter, latency targets, and a minimal evaluation plan?","channel":"computer-vision","subChannel":"general","difficulty":"beginner","tags":["computer-vision"],"companies":["Hugging Face","IBM","Meta"]},{"id":"q-4159","question":"Design a real-time on-device video segmentation system for AR on a mobile phone with a single RGB camera that can learn new object masks from 10–20 labeled frames using active learning and self-supervised pretraining. Target latency <40 ms/frame and memory under 400 MB. Describe architecture (shared encoder + compact decoder), online adaptation (entropy-based frame selection, pseudo-labels, limited fine-tuning), data flow, and evaluation metrics (IoU, mask stability, adaptation speed)?","channel":"computer-vision","subChannel":"general","difficulty":"intermediate","tags":["computer-vision"],"companies":["Adobe","Twitter"]},{"id":"q-4225","question":"Design an edge-device semantic segmentation system for a mobile robot with a single RGB camera that must rapidly adapt to new indoor/outdoor environments using only a tiny labeled seed (<50 frames) and no cloud access. Propose a lightweight model, a self-supervised domain adaptation loop (temporal consistency + augmentations), and a concrete evaluation plan across IoU, boundary accuracy, and inference latency?","channel":"computer-vision","subChannel":"general","difficulty":"intermediate","tags":["computer-vision"],"companies":["Google","LinkedIn","Snowflake"]},{"id":"q-4555","question":"Design a real-time monocular depth and 6-DoF pose estimation pipeline for a robotic pick-and-place task using a single RGB camera in cluttered, occluded environments. Requirements: sub-40 ms latency on an edge GPU, self-supervised training for depth and pose, temporal filtering to handle occlusions, and a robust evaluation plan including depth accuracy, pose drift under occlusion, and failure modes?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Amazon","Google","LinkedIn"]},{"id":"q-456","question":"How would you design a real-time object detection system for a social media platform that processes 10M images/day with 99.9% accuracy and <100ms latency?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Microsoft","Salesforce","Twitter"]},{"id":"q-4582","question":"Design a production-ready monocular 3D object detector for robotic pick-and-place on a new warehouse line. The system must maintain accurate 6-DoF poses for a fixed set of tools under changing intrinsics, lens distortion, and illumination, using only a single RGB camera and streaming data. Propose a real-time adaptation loop with no labeled data, specify data flow, losses (temporal reprojection, photometric consistency, geometric self-supervision), latency targets, and evaluation plan for drift and false negatives?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Anthropic","Hugging Face"]},{"id":"q-4836","question":"Edge CV task: A warehouse robot with a single RGB camera (Jetson-class) must detect a fixed set of 60 SKUs and flag Unknown items for human review in real time. Design a compact on-device detector with an Open-Set component and a memory module for rapid few-shot updates. Outline architecture (backbone, embedding head, novelty scorer), data strategy (synthetic + real), latency targets (<40 ms/frame), and evaluation (per-class mAP, unknown recall/precision)?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Databricks","LinkedIn","Lyft"]},{"id":"q-487","question":"Design a real-time object detection system for DoorDash delivery vehicles that must identify packages, license plates, and traffic signs in varying weather conditions. How would you handle model optimization for edge deployment and ensure 99% accuracy?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["DoorDash","Google"]},{"id":"q-517","question":"Design a real-time object detection system for cryptocurrency trading terminals that must detect and classify multiple monitor types, trading interfaces, and unauthorized screen recording devices with <100ms latency. How would you optimize YOLOv8 for this specific use case?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Amazon","Coinbase","Meta"]},{"id":"q-545","question":"How would you detect if an image contains a face using basic computer vision techniques?","channel":"computer-vision","subChannel":"general","difficulty":"beginner","tags":["computer-vision"],"companies":["Airbnb","NVIDIA","Netflix"]},{"id":"q-570","question":"How would you design a real-time object detection system for Airbnb's property listing photos that can identify amenities and safety violations while processing 10,000 images per hour?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Airbnb","Instacart"]},{"id":"q-274","question":"How would you implement a hybrid CNN architecture combining ResNet residual connections with EfficientNet compound scaling for production image classification?","channel":"computer-vision","subChannel":"image-classification","difficulty":"intermediate","tags":["cnn","resnet","efficientnet"],"companies":["Amazon","Google","Meta","Microsoft"]},{"id":"q-253","question":"How does YOLO implement real-time object detection using grid-based prediction and what are the key components of its architecture?","channel":"computer-vision","subChannel":"object-detection","difficulty":"beginner","tags":["yolo","rcnn","detr"],"companies":["Amazon","Google","Meta","Microsoft","NVIDIA","Tesla"]},{"id":"q-200","question":"How does U-Net's skip connection architecture enable precise medical image segmentation?","channel":"computer-vision","subChannel":"segmentation","difficulty":"beginner","tags":["unet","mask-rcnn","sam"],"companies":["Amazon","Google","Meta"]},{"id":"q-228","question":"How would you optimize a real-time medical image segmentation pipeline using SAM with 100ms latency constraint on edge devices?","channel":"computer-vision","subChannel":"segmentation","difficulty":"advanced","tags":["unet","mask-rcnn","sam"],"companies":["Apple","Google","Meta","Microsoft","NVIDIA"]},{"id":"q-1080","question":"In a log-processing pipeline, multiple producers enqueue log entries into a bounded, rate-limited work queue. Implement a beginner-friendly token-bucket rate limiter that allows enqueuing only when a token is available; a background timer refills tokens at a fixed rate up to a max. Producers block when tokens==0; workers process items from the queue. Provide a concrete Go/Python/Java solution and discuss testing?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Bloomberg","Lyft","NVIDIA"]},{"id":"q-1204","question":"In a real-time notification system with thousands of tenants, each tenant's events must be delivered to their own handler in order, while cross-tenant processing happens in parallel. Design a bounded, multi-queue architecture with per-tenant in-order guarantees, backpressure, and graceful shutdown. Compare two backpressure strategies (per-tenant token buckets vs. global credits) and discuss testing and failure scenarios. Provide runnable sketch in Go or Rust?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Scale Ai","Tesla"]},{"id":"q-2080","question":"In a real-time chat system, multiple producers enqueue messages into per-room bounded queues and multiple consumers deliver to connected clients. Design a beginner-friendly concurrency solution (Go, Java, or Python) that guarantees producers block when a room's queue is full and consumers block when the queue is empty, while preserving per-room isolation and simple backpressure. Compare a mutex/condition-based queue versus a channel-based approach for bounded queues?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Discord","Hashicorp"]},{"id":"q-2329","question":"Design a bounded, concurrent router for a real-time analytics platform ingesting events from thousands of clients. Each client's events must be processed in arrival order, while events from different clients may be processed in parallel. The router should implement per-client backpressure and support clients joining/leaving on the fly. Describe two concrete implementations: (A) per-client queues with a central scheduler using locks, (B) per-client lock-free queues with a shared ready-list. Include correctness, failure handling, and a practical test plan?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Google","Instacart","Stripe"]},{"id":"q-2542","question":"Design a two-stage, bounded-concurrency pipeline for real-time video frames from multiple cameras. Stage 1 decodes frames (CPU-bound) using a fixed thread pool; Stage 2 runs an AI-based enhancer asynchronously. Ensure per-frame ordering across all cameras, backpressure to keep queues bounded, support dynamic worker scaling and clean shutdown with cancellation, and provide a runnable minimal sketch in a language of your choice?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Airbnb","Google","Hugging Face"]},{"id":"q-2606","question":"Design an in-process, bounded-concurrency event bus for a streaming service that ingests events from hundreds of producers (per-video id) and delivers to a pool of workers while guaranteeing per-video in-order processing, minimal latency, and backpressure signaling to producers when queues fill. Include a runnable sketch in Go or Rust and discuss testing?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Airbnb","Amazon","LinkedIn"]},{"id":"q-2658","question":"Design a bounded, fair, multi-producer/multi-consumer system for real-time order matching in a crypto exchange. Many producers submit limit orders; multiple workers attempt matches against an in-memory order book. Guarantee bounded memory, backpressure, and starvation-free fairness across producers (e.g., per-producer quotas or weighted fairness). Describe data structures, synchronization, and a minimal runnable Java snippet showing enqueue and a worker loop?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Coinbase","DoorDash","Robinhood"]},{"id":"q-2804","question":"In a real-time analytics pipeline, millions of event streams (streamId) arrive; each stream must be processed in-order, but streams can be processed in parallel. Design a bounded per-stream queueing layer with a shared worker pool. Include backpressure when a queue fills, handling of late/out-of-order events, and graceful recovery after worker failure. Provide a runnable sketch in Go or Rust?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["DoorDash","Oracle"]},{"id":"q-2834","question":"Three producers generate events tagged by user id and time; a single in-memory sink aggregates per-user activity over a 1-second sliding window and emits a summary to a consumer. Implement a beginner-friendly concurrency solution in Go, Python, or Java that guarantees thread-safe window updates, handles late events (up to 200ms), and ensures the sink doesn't drop events under low backpressure. Provide runnable sketch and tests?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Amazon","PayPal","Tesla"]},{"id":"q-2876","question":"Design a dynamic DAG-based task scheduler with runtime-submitted tasks and dependencies; implement a fixed-size worker pool with local queues, support cross-worker work-stealing, and ensure dependents wake atomically when prerequisites finish; bound the global queue to enforce backpressure; discuss deadlock avoidance and fairness under high contention?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Instacart","Netflix","Scale Ai"]},{"id":"q-2910","question":"Design a concurrent job queue for an image-processing service: tasks arrive with tenant_id, priority, and deadline; implement a bounded, multi-queue system where workers always pull from the currently highest-priority non-empty tenant queue, but aging prevents any tenant from starving. Include backpressure on the global queue, cross-tenant fairness, and a test plan for liveness under bursty load. Provide concrete structures and trade-offs?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["LinkedIn","OpenAI"]},{"id":"q-2995","question":"Design a speculative execution layer for a real-time collaborative editor where user actions arrive as a DAG of edits with dependencies. Implement a multithreaded executor that runs independent edits in parallel, defers dependents until prerequisites finish, and detects conflicts on the same position to rollback. Include a bounded replay queue to cap work and a rollback protocol. Provide a runnable sketch in Go or Rust and discuss guarantees?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Cloudflare","Discord","Tesla"]},{"id":"q-3132","question":"Design an in-memory, concurrent stream join operator for two input streams A and B. Each event has a key and a timestamp. Implement a per-key sliding-window equi-join with multiple worker threads that can process different keys in parallel, but guarantee in-order output per key. Bound memory usage with eviction, and apply backpressure when downstream slows. Compare locking vs lock-free implementations and outline testing strategies and failure scenarios?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Cloudflare","LinkedIn","Snowflake"]},{"id":"q-3217","question":"Design a per-session ordered event dispatcher for a real-time multiplayer game server. Each game session has its own FIFO queue; multiple producers can enqueue events across sessions, and a fixed-size pool of worker threads processes events. Requirements: (1) preserve strict per-session order; (2) allow cross-session work-stealing for load balancing; (3) enforce a global bounded queue with backpressure to producers; (4) support dynamic session creation/destruction with zero deadlock and graceful shutdown; (5) discuss memory reclamation and safety?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Discord","Google","NVIDIA"]},{"id":"q-3306","question":"Design a bounded, deadline-aware task scheduler for a live image-processing pipeline. Producers enqueue tasks labeled CPU-bound or IO-bound with deadlines; implement a two-tier priority with earliest deadline first and aging to prevent starvation. Use per-worker local queues with work-stealing, and a global backpressure mechanism when backlog grows. Ensure deadlines are respected and test bursts with latency metrics?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Adobe","Meta","Two Sigma"]},{"id":"q-3453","question":"Design a beginner-friendly concurrency setup in Go for a real-time notification service where many producers emit payloads that must be delivered to users in per-user order. Implement a global bounded queue and per-user FIFO channels so that each user's messages are processed in order, while a small worker pool handles delivery. Explain how you enforce backpressure, avoid starvation, and test ordering and race conditions?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Meta","Salesforce"]},{"id":"q-3483","question":"In a real-time video analytics service, cameras stream frames that must be processed in per-camera order while frames from different cameras can run concurrently. Design a bounded, backpressured dispatch with a central queue and per-camera FIFO queues, plus a small worker pool. Explain ordering, starvation avoidance, and failure handling. Provide a runnable asyncio Python sketch demonstrating enqueue, dispatcher, queues, and graceful shutdown?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Airbnb","Robinhood","Scale Ai"]},{"id":"q-3770","question":"Scenario: A telemetry inlet ingests events from numerous IoT devices. Each device’s events must be processed in arrival order, while events from different devices are handled concurrently by a small worker pool. Design a beginner-friendly Go solution using a bounded global queue and per-device FIFO channels to preserve per-device order. Add deduplication for repeated (deviceId,seq) pairs, implement backpressure, and ensure fairness when bursts occur. Provide a runnable sketch and brief testing plan?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Tesla","Two Sigma"]},{"id":"q-3797","question":"In a GPU-accelerated image-processing pipeline, dozens of producers generate work units with different priorities and data dependencies, and a pool of workers dispatch kernels. Design a bounded, multi-producer multi-consumer queue with backpressure that supports per-item priority without starving high-priority tasks. Explain data structures, memory ordering, and how you avoid ABA and deadlocks. Include a runnable minimal sketch (Rust or C++) showing enqueue/dequeue semantics and a basic test strategy with synthetic load?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Hashicorp","NVIDIA","Zoom"]},{"id":"q-3855","question":"Design a streaming dataflow for real-time analytics that processes events into fixed 1-second windows. Implement per-window bounded queues, a dynamic worker pool, and a watermark-driven cancellation protocol: late events crossing the watermark cancel the window and abort pending tasks. Guarantee in-window output order and no deadlocks under backpressure. Explain testing strategies and trade-offs?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Apple","Instacart","Tesla"]},{"id":"q-3994","question":"Design a fault-tolerant, multi-region task queue for a real-time data-enrichment service serving many tenants. Each tenant has a cap and can enqueue interdependent tasks. Propose a bounded, distributed queue with backpressure, delivering at least once while achieving exactly-once processing for idempotent handlers, plus per-tenant ordering and a recovery plan. Include a test strategy?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Amazon","Google","Hashicorp"]},{"id":"q-4066","question":"Design a backpressure-aware streaming engine for real-time analytics: streams partitioned by sensor_id; each partition has a bounded queue and a 5s tumbling window; a watermark advances to emit aggregates; late data goes to a late-path buffer for a grace period; exactly-once via incremental checkpointing of partition offsets and window state; discuss latency-throughput trade-offs and testing strategy?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Databricks","DoorDash"]},{"id":"q-4144","question":"Design a beginner-friendly concurrency pattern for a bounded multi-producer, multi-consumer router: many producers publish tasks tagged by resource_id into a shared bounded global queue. Implement per-resource FIFO buffers and a small worker pool that processes tasks from those buffers, while guaranteeing per-resource order and fair progress between resources. Explain backpressure, starvation avoidance, and a simple test plan to verify ordering under bursty loads?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Instacart","Microsoft","Zoom"]},{"id":"q-4222","question":"You're building a real-time analytics ingestion service for a multi-tenant SaaS platform. Each incoming event carries tenant_id and a local sequence number. You must preserve per-tenant in-order semantics across keys while allowing full parallelism across tenants. Design a bounded, multi-queue scheduler with per-tenant queues, a fixed-size worker pool, and backpressure when buffers saturate. Describe data structures, synchronization approach (locks vs lock-free), deadlock avoidance, and how to achieve exactly-once processing on failure. Provide runnable sketches in Go or Rust and outline a testing plan?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Anthropic","Microsoft"]},{"id":"q-4292","question":"You are building a real-time notification system with a bounded, multi-producer, multi-consumer queue. Producers label tasks as critical or normal. The queue capacity is fixed; producers block when full. A worker pool must fetch tasks in a fair, per type round robin to avoid starvation. Provide a runnable sketch in Go (or Python/Java) and discuss backpressure, cancellation, and testing for latency and task ordering?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Lyft","Scale Ai","Snowflake"]},{"id":"q-4482","question":"In a real-time task processing system, multiple producers enqueue jobs into a global bounded queue with capacity C. A fixed pool of workers consumes jobs. Extend a beginner concurrency pattern to support two priority levels: high and normal. High-priority jobs must be served before normal ones, while preserving FIFO within each priority. Producers should block when the queue is full. Implement backpressure signaling and outline a concrete test plan to verify ordering and backpressure?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Hashicorp","Meta"]},{"id":"q-4617","question":"Design a beginner-friendly concurrency pattern for a real-time ingest system where producers submit messages tagged by dynamic topics. Each topic must preserve per-topic in-order delivery, while a bounded global queue and a small worker pool process items across topics. Topics can be created or removed at runtime; ensure backpressure signals to producers when quotas are reached and gracefully drain or archive messages on topic removal. Explain test plan for per-topic ordering under churn and for topic lifecycle events?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Discord","Goldman Sachs","Twitter"]},{"id":"q-4669","question":"In a real-time ingestion service, dozens of producers emit per-tenant events into a fixed-size global queue. Implement a bounded, multi-tenant queue that guarantees per-tenant in-order processing and fair sharing while allowing dynamic partition rebalancing without pausing producers. Provide a runnable minimal sketch in Go or Rust showing enqueue/dequeue and a basic test plan for backpressure, ordering, and rebalancing?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Google","Netflix","Slack"]},{"id":"q-4770","question":"In a real-time video channel encoding pipeline with thousands of channels, producers enqueue per-channel tasks into a bounded global queue and per-channel local queues. Design a backpressure-aware, low-latency scheduler (Rust; crossbeam or Tokio) that preserves per-channel order, avoids starvation, and scales to many cores. Include a runnable sketch showing enqueue, worker pull, and backpressure?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Meta","Netflix"]},{"id":"q-4793","question":"Design a bounded, multi-producer, multi-consumer processing system for streaming analytics. Each event has a topic id; events within the same topic must be processed in order, but topics may run in parallel. Implement a dynamic worker pool that can grow/shrink at runtime and rebalance tasks across topics while preserving per-topic order. Include backpressure signaling when the global queue is full, fairness across topics, and a graceful shutdown that drains in-flight items per topic?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Google","IBM"]},{"id":"q-682","question":"In a service handling image uploads, each file triggers a resize and thumbnail generation pipeline. Design a bounded producer-consumer queue in Python using asyncio. Use a Queue with maxsize, a fixed number of worker coroutines, and backpressure so producers await when full. Include clean shutdown and error handling. Provide a runnable minimal example showing enqueue, worker loop, and cancellation?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Discord","Lyft","Snap"]},{"id":"q-687","question":"Write a small Python snippet: create a shared counter initialized to 0, spawn 4 threads that increment it 1000 times each, using a Lock to protect the increment. After joining, print the final value. Explain what happens if the lock is removed and how atomicity is ensured. What value do you expect and why?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Google","Tesla","Two Sigma"]},{"id":"q-696","question":"Context: in a real-time analytics pipeline for a video-conference system, dozens of producers emit events into a shared queue and multiple workers consume them. Implement a bounded, multi-producer, multi-consumer queue with capacity 1024. It must not drop messages while not full, block producers when full, support concurrent consumers, and support a clean shutdown. Describe API, invariants, and a simple synchronization strategy?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Hugging Face","Zoom"]},{"id":"q-708","question":"Design a bounded, concurrent, multi-priority work scheduler for a rendering service: 3 priority levels (0 highest). Producers enqueue tasks into per-priority queues with backpressure; workers drain from the highest non-empty queue (0→1→2). Include aging to prevent starvation and a graceful shutdown sentinel. Provide a runnable minimal Java example?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Adobe","Salesforce","Snap"]},{"id":"q-713","question":"Implement a thread-safe bounded queue using a fixed circular buffer with mutex protection and condition variables for coordinating producers and consumers. How would you handle enqueue/dequeue operations and implement graceful shutdown when the queue is no longer accepting new tasks?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Adobe","Cloudflare","Tesla"]},{"id":"q-720","question":"Design a bounded worker pool for a high-throughput API gateway that queues work with backpressure. Use a lock-free ring buffer, N workers, and blocking enqueue when full. Include per-task cancellation, timeouts, and metrics. Compare Go, Rust, and Java trade-offs and explain how you’d debug data races and starvation under burst traffic?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Hugging Face","Square"]},{"id":"q-726","question":"Design a concurrent event broker for a live chat service that guarantees per-user in-order delivery while allowing parallel processing across users, using a bounded buffer; describe backpressure handling when the buffer fills, and compare locking versus lock-free approaches in your language of choice?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Meta","Microsoft","Zoom"]},{"id":"q-735","question":"Design a concurrency-safe per-channel message queue for a Discord-like chat service: multiple producers push messages, a single consumer persists them to storage; implement bounded capacity, preserve per-channel order, and handle backpressure. Which synchronization primitives would you use and how would you test edge cases?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Discord","Snowflake"]},{"id":"q-748","question":"You're building a real-time analytics pipeline with many shards. Propose a concurrent, bounded path that preserves per-shard in-order processing while enabling cross-shard parallelism. Design a data structure and protocol (enqueuing, routing, backpressure, and shutdown). Provide a runnable sketch in Go or Rust showing enqueue, per-shard consumer loop, and graceful shutdown?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["NVIDIA","Snap","Twitter"]},{"id":"q-756","question":"Implement a concurrent task-graph executor with dependencies. Tasks form a DAG; a task runs only after all its prerequisites complete. Use a bounded in-flight task pool, per-worker work stealing, and deadlock/backpressure handling. Provide a runnable Go or Rust sketch showing submission, ready-state transitions, worker loop, and graceful shutdown?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Lyft","NVIDIA"]},{"id":"q-759","question":"In a multi-threaded microservice, there is a shared in-memory counter for total processed events. Provide a concrete, beginner-friendly approach to implement a thread-safe increment using language primitives (Java, Go, or Python) and discuss the trade-offs between lock-based vs lock-free solutions when scaling across cores?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Bloomberg","Coinbase","MongoDB"]},{"id":"q-773","question":"In a Go HTTP server, you maintain a per-user quota counter in memory. Multiple requests for different users should advance concurrently, but updates to the same user must be serialized. Design a striped lock (a fixed set of mutexes) to guard a map[string]int, map userID to a bucket via hashing, and explain how you minimize contention, handle hash collisions, and ensure correctness when entries may be evicted?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["MongoDB","NVIDIA","Plaid"]},{"id":"q-780","question":"Implement a lock-free, bounded multi-producer, multi-consumer pipeline with per-symbol partitions and fixed-capacity queues. Producers must acquire credits to enqueue; downstream workers release credits on completion. Explain memory visibility, false sharing avoidance, and a clean shutdown. How do you guarantee in-order per partition and exactly-once processing on failure?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Bloomberg","Microsoft","PayPal"]},{"id":"q-788","question":"In a streaming data pipeline, multiple producers generate frames and a bounded buffer sits between the producer stage and a consumer stage. Implement a backpressure-enabled producer-consumer pair in JavaScript (Node.js) using an async bounded queue that supports multiple producers and multiple consumers. It must guarantee no data loss, bounded memory, and graceful shutdown. How would you test under varying production/consumption rates?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Citadel","Tesla"]},{"id":"q-795","question":"Design a dynamic, concurrency-safe worker pool in Go for a real-time analytics pipeline. Each task type has its own queue; enforce backpressure with bounded channels and implement a central scaler that adjusts worker counts based on observed tail latency and throughput. Provide a runnable skeleton showing enqueue, worker loops, and graceful shutdown?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Databricks","Meta","Microsoft"]},{"id":"q-804","question":"You're building a video-frame ingestion service where frames from thousands of users arrive on a shared input channel. Design a bounded, concurrent dispatcher that routes frames to per-user in-order queues, enabling parallel processing across users. Provide backpressure handling when the global buffer fills, a strategy for reordering out-of-order frames, and a graceful shutdown. Sketch the core data structures and synchronization in Rust or C++ and explain trade-offs?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Hugging Face","Netflix","Snap"]},{"id":"q-811","question":"In a real-time analytics service, multiple producers enqueue data into a fixed-size circular buffer consumed by multiple workers. Design a beginner-friendly concurrency solution in Java, Go, or Python that guarantees producers block when the buffer is full and consumers block when empty, while maintaining correctness under concurrent access; compare lock-based vs channel-based approaches for this bounded buffer?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Databricks","Oracle","Two Sigma"]},{"id":"q-818","question":"In a build pipeline, N compile tasks run in parallel and must all finish before the linker runs. Design a barrier that blocks each task at barrier.wait() until all N tasks have arrived, then releases them to proceed. Implement two beginner-friendly approaches in Go (or Java/Python): (A) locks/condition variables; (B) channels/futures. Ensure the barrier is reusable for repeated builds, avoids deadlocks, and explain correctness and trade-offs?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Discord","Two Sigma"]},{"id":"q-825","question":"In a real-time chat moderation pipeline for a platform like Discord/LinkedIn, design a dynamic Go worker pool that scales from minWorkers to maxWorkers based on a bounded task queue. The queue should backpressure producers, guarantee per-user fairness, support graceful shutdown, and tolerate occasional misbehaving workers (timeouts). Provide runnable code showing enqueue, worker loop, and scaler?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Discord","LinkedIn"]},{"id":"q-835","question":"Design a multi-tenant dispatcher for a streaming event bus. Producers from each tenant push events into per-tenant input channels; a central dispatcher interleaves tenants fairly, enforces per-tenant rate limits using a token-bucket, and enforces a maximum in-flight events per tenant. Implement in Go using channels; ensure backpressure propagates to producers when quotas or in-flight limits are reached, and support graceful shutdown. Include a runnable minimal example with: per-tenant limiter, dispatcher loop, producer(s), and cancellation?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Airbnb","NVIDIA","Slack"]},{"id":"q-1027","question":"You're running a mixed Consul Connect mesh with Kubernetes services in DC1 and VM-based services in DC2. A new API service in DC1 calls a legacy VM backend behind a firewall via a mesh gateway, but TLS handshakes intermittently fail after a CA rotation. Propose a zero-downtime plan to diagnose, implement automatic CA rotation, and validate end-to-end, including config changes, monitoring, and rollback steps?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Cloudflare","LinkedIn","Microsoft"]},{"id":"q-1178","question":"Across a tri-cloud Consul Connect mesh, a new microservice 'payments-api' in namespace 'payments' must reach a legacy data service 'orderdb' in namespace 'legacy' via a mesh gateway. Propose an end-to-end pattern that enforces strict identity via namespace-scoped Intention defaults, per-service tokens, and gateway ACLs, including resource definitions, deployment steps, and rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Anthropic","DoorDash","Tesla"]},{"id":"q-1293","question":"In a hybrid setup with Consul across two Kubernetes clusters (AWS) and VM-based services on-prem, how would you design cross-cluster service authentication and discovery using Consul Connect with mesh gateways, Namespaces, and ACLs to enforce zero-trust policy and automatic token rotation while preserving DNS-based discovery?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Microsoft","Snowflake"]},{"id":"q-1334","question":"Scenario: You operate a mixed environment with Consul Connect across Kubernetes namespaces dev (auth-service) and prod (user-service). How would you implement an Intentions policy that allows auth-service to call user-service only on port 8080, with default deny for all other traffic, and enable audit logging? Describe the commands, tokens, and how you would verify enforcement from a small client pod?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["NVIDIA","Robinhood","Snap"]},{"id":"q-1348","question":"Scenario: A global application runs in three environments (prod, staging, dev) across Kubernetes in GCP and legacy VM services on‑prem, all using Consul Connect. Design a zero-trust mesh that uses Vault for dynamic secrets, namespace-scoped ACL tokens, and mesh gateways for cross-cluster traffic. Explain how you would rotate credentials automatically, enforce service-to-service ACLs, and validate safety during partial outages?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["IBM","MongoDB"]},{"id":"q-1369","question":"Three-datacenter Consul Connect mesh (DC1 Kubernetes, DC2 VM, DC3 on‑prem) needs zero-downtime TLS credential rotation using Vault PKI and Consul CA integration. Describe a concrete plan including Vault roles, TTLs, renewal cadence, root CA distribution, per-service identity, and a rollback procedure with deployment steps and rollback commands?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Google","MongoDB"]},{"id":"q-1408","question":"Design a zero-downtime certificate rotation strategy for a Consul Connect mesh spanning Kubernetes and VM-based services across two clusters. How would you coordinate CA rotation, per-service identity tokens, and mesh gateway trust so that mutual-TLS remains valid during rotation and new services automatically trust the updated CA without downtime?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Google","Twitter","Two Sigma"]},{"id":"q-1429","question":"In a Consul Connect mesh spanning two Kubernetes clusters, a frontend service in cluster A must call a private external billing API behind a VPC. The API requires mTLS and tenant-scoped API keys that rotate every 24 hours. Design a scalable egress pattern using a Mesh Gateway for outbound traffic, per-service Intentions with explicit allow rules, TLS credential rotation via Vault, and a rollback plan. Include sample manifests, deployment steps, and failure recovery?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Discord","Twitter"]},{"id":"q-1465","question":"In a Consul Connect mesh spanning Kubernetes and VM workloads, an external analytics service must fetch data from internal microservices via mesh gateways but cannot run a sidecar. Design a secure bridge pattern: per-request identity, TLS with a bridge certificate minted by Consul, short-lived credentials rotated automatically, and revocation without downtime. Include concrete resource definitions, deployment steps, and rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Citadel","NVIDIA","Two Sigma"]},{"id":"q-1470","question":"In a multi-cluster Consul Connect mesh spanning Kubernetes and VMs, design a tenant-aware, dynamic access control model where new tenants join/leave at runtime without service downtime. How would you implement per-tenant namespaces, ephemeral tokens, and policy synchronization to enforce zero-trust across tenant boundaries? Include concrete steps and constraints?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Goldman Sachs","Square","Tesla"]},{"id":"q-1525","question":"In a Consul Connect mesh, two services communicate over gRPC. One is in Kubernetes, the other is VM-based without a sidecar. Design a secure bridge via a mesh gateway to enable mutual-TLS and per-request identity, including how to configure the gRPC wiring, gateway policy, and a minimal manifest for the gateway and client. Include a rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Airbnb","IBM","Meta"]},{"id":"q-1669","question":"In a single cluster with two namespaces, dev and prod, a frontend service in dev must call a backend service in prod through a Consul mesh gateway. Design a beginner-friendly end-to-end setup: register both services, enable a mesh gateway in prod, add a route, ensure mTLS with short-lived certs, and include a rollback plan if the gateway fails. Include concrete YAML fragments and commands?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Hashicorp","Hugging Face","Robinhood"]},{"id":"q-1693","question":"In a Consul Connect mesh spanning Kubernetes and VM workloads, how would you implement scalable, per-service access with automatic token rotation and zero-downtime revocation for an external analytics app calling internal services via mesh gateways? Include identity model (SPIFFE IDs), Vault-based token lifecycle, per-service ACLs, mesh-gateway configuration, and a rolling update plus rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Apple","Scale Ai","Square"]},{"id":"q-1795","question":"In a Consul Connect-enabled Kubernetes cluster, a new Python gRPC service must call a legacy on-prem TCP service via a mesh gateway using TCP mode (no HTTP). Draft a minimal setup: (1) service definitions and ACLs, (2) mesh gateway TCP config, (3) per-service identity, tokens, and rotation, (4) a rollback plan if handshake fails?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Oracle","Tesla","Twitter"]},{"id":"q-1849","question":"In a Consul Connect mesh spanning Kubernetes in DC1 and VM-based services in DC2, a compromised service identified by SPIFFE ID spiffe://example.org/compromised-frontend.dc1 must be revoked mesh-wide within minutes. Design a reproducible, low-downtime remediation workflow: revoke tokens, rotate TLS certs, refresh CA bundles on mesh gateways, enforce temporary ACL lockdown, and verify no unauthorized calls remain. Include concrete resource definitions, rotation steps, and rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Apple","Two Sigma"]},{"id":"q-2031","question":"Design a three-region Consul Connect mesh where 500+ services span Kubernetes clusters in US/EU and VM-based services in APAC, with a partner-facing API exposed via a mesh gateway. Describe your approach to: (1) identity and ACL strategy across regions, (2) automatic TLS certificate rotation backed by Vault, (3) WAN federation topology, (4) zero-downtime token revocation, and (5) testing/rollout plan with rollback steps?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Two Sigma","Uber"]},{"id":"q-2065","question":"Scenario: a Kubernetes-based checkout calls inventory. A new canary version inventory-v2 is deployed on both Kubernetes and VM-backed services. Implement a 20% canary split using Consul Connect service-router, gate by a health check, and allow quick rollback to 0% canary. Provide minimal service-router config, deployment labels, and a rollback process with commands to reweight or disable v2?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Amazon","DoorDash","Oracle"]},{"id":"q-2126","question":"Design a per-tenant isolation in a mixed Kubernetes/VM Consul Connect mesh with two tenants (alpha, beta). Implement namespace-scoped ACLs and per-tenant tokens so payments-alpha can access ledger-alpha only, and payments-beta ledger-beta only. Provide service-router rules using identity, a short-lived token rotation plan, and an automated test that verifies cross-tenant access is denied and a rollback path to re-allow access if misconfig is detected?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Snowflake","Tesla","Twitter"]},{"id":"q-2236","question":"In a two-DC Consul Connect mesh (DC1 and DC2), DC1 experiences an outage. Design a reproducible DR workflow to promote DC2 as primary within minutes, preserving TLS identity, ACLs, and service discovery. Include concrete resource definitions for ACLs/Intentions, mesh-gateway config, Vault-integrated token rotation, CA bundle refresh, and a rollback plan with tests and canary semantics?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Hashicorp","Snowflake","Tesla"]},{"id":"q-2321","question":"In a Consul Connect mesh spanning Kubernetes in DC1 and VM workloads in DC2, a rolling upgrade to ingest-api triggers intermittent TLS handshake failures with data-warehouse. Propose a concrete, low-downtime remediation workflow that rotates Vault-backed TLS certs, refreshes CA bundles on mesh gateways, updates ACLs/Intentions to permit only existing paths during the window, and provides a rollback plan. Include example resource definitions and step-by-step commands to verify success?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Apple","Microsoft","MongoDB"]},{"id":"q-2335","question":"Design a Consul Connect mesh that spans two Kubernetes clusters (prod and analytics) and an on‑prem VM fleet. A data-ingest service in prod must securely call downstream processors during business hours only, with per‑request identity binding, zero‑trust ACLs, and automatic token rotation via Vault. Explain how you'd implement cross‑environment identity, time‑bound access, credential rotation without downtime, and auditability. Include concrete config excerpts for both clusters and the VM host?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Amazon","Lyft","Snowflake"]},{"id":"q-2458","question":"Three-DC Consul Connect mesh: in DC1 a real-time feed service is exposed via a mesh gateway; in DC2 a Kubernetes data-collector calls it with SPIFFE IDs and Vault-managed TLS rotation. Design a production-ready flow to enforce least privilege, support automatic certificate rotation, and achieve zero-downtime reconfiguration of Intentions and ACLs, plus rollback. Include concrete resource blocks for ACLs, Intents, mesh-gateway, and Vault rotation steps?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Robinhood","Snowflake"]},{"id":"q-2491","question":"In a Consul Connect mesh spanning Kubernetes and VM workloads, a newly registered VM-based service `reporter` cannot call `data-api` in the Kubernetes cluster after an ACL policy revision. Outline a beginner-friendly, end-to-end debugging workflow to diagnose and fix cross-namespace authorization, including minimal commands, policy fragments, and how to verify per-call identity?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Goldman Sachs","Twitter"]},{"id":"q-2507","question":"In a Consul Connect mesh spanning Kubernetes (DC1) and VM workloads (DC2), a policy change must allow a new service ingestor to reach data-warehouse only under canary conditions. Design a practical, automated canary workflow for ACL/Intentions changes: isolate the canary, run end-to-end tests, validate identity via SPIFFE IDs, rotate tokens automatically, and rollback on failure. Include concrete policy fragments, CI steps, and rollback criteria?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Discord","Microsoft","PayPal"]},{"id":"q-2548","question":"In a two-DC Consul Connect mesh, a new ingest-service in DC1 must securely push time-series data to a real-time analytics sink in DC2. Design a scalable, auditable Intentions workflow that uses SPIFFE IDs, per-service ACLs, and dynamic token rotation, with a canary rollout and rollback plan. Include concrete policy fragments (ACL/Intentions), a test harness, and a rollback trigger?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["MongoDB","Twitter"]},{"id":"q-2684","question":"In a two-DC Consul Connect mesh (DC1 Kubernetes, DC2 VM workloads), a new ingest service in DC1 must call a data-sink in DC2 via a mesh gateway. Design a practical, low-downtime migration that enforces per-service SPIFFE IDs, tenant isolation, and automatic TLS cert rotation. Include concrete Intentions/ACL fragments, canary rollout criteria, rollback triggers, and verification steps across both DCs?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Cloudflare","Meta","Slack"]},{"id":"q-2769","question":"In a multi-cloud Consul Connect mesh that spans Kubernetes and VM workloads, how would you implement automatic per-service mTLS certificate rotation using Vault as the CA with short-lived certs, ensuring identity-based ACLs and zero-downtime rotation? Provide concrete config references and rollback considerations?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Airbnb","NVIDIA","Salesforce"]},{"id":"q-2845","question":"In a multi-datacenter Consul Connect mesh (Kubernetes in DC1, VM-based services in DC2), TLS certs are issued and rotated by Vault’s PKI. Design a zero-downtime CA rotation workflow that updates leaf and root certs across all services, propagates updated CA bundles to gateways/sidecars, triggers config reloads, and verifies successful mutual TLS handshakes. Include concrete steps, rollback criteria, and example artifact definitions?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["OpenAI","Square"]},{"id":"q-2865","question":"In a mixed Kubernetes and VM-based Consul Connect mesh spanning multiple tenants, design an identity-federated, zero-trust PKI using Vault roots per tenant to enable per-tenant mTLS with automatic rotation and revocation, while preventing cross-tenant trust. Describe architecture, Vault PKI roles, ACL scoping, and a rollback plan. Include concrete steps to migrate namespaces with zero downtime and how to validate tenant isolation with canary services?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Salesforce","Tesla","Twitter"]},{"id":"q-3040","question":"In a Consul Connect mesh spanning Kubernetes and VM-based services with two Namespaces (staging and production), design a beginner-friendly ACL workflow to permit prod.payments to access prod.pricing but deny any staging access. Explain token lifecycle, per-service intentions, and a safe promotion process from staging to production including revocation without downtime?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Adobe","NVIDIA","Plaid"]},{"id":"q-3067","question":"In a Consul Connect mesh spanning Kubernetes and VM workloads across three regions, you want per-service mTLS with Vault as CA and short-lived certs, but now you must implement application-layer certificate pinning to prevent rogue peers from authenticating after cert rotation. Describe how to configure pinning in client libraries, coordinate rotation with CA, and fallback if pin validation fails, including concrete config references?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Cloudflare","Databricks","Snap"]},{"id":"q-3100","question":"Design an automated drift-detection and remediation workflow for ACLs/Intentions in a Consul Connect mesh spanning Kubernetes and VM workloads, where CI/CD policy-as-code must stay in sync with the live mesh. Include **rollback to the last-good Intentions**, per-call **SPIFFE-ID** end-to-end verification, and concrete policy fragments?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Goldman Sachs","IBM","Two Sigma"]},{"id":"q-3219","question":"In a Consul Connect mesh with multiple namespaces across Kubernetes and VM workloads, Team A's service 'report-processor' in namespace 'team-a' must call Team B's 'data-warehouse' in namespace 'team-b' only through the shared gateway. Design a namespace-scoped policy: per-service ACLs, SPIFFE IDs, and gateway ACLs; include concrete policy fragments, a minimal test to prove whitelisting, and a rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Adobe","Salesforce"]},{"id":"q-3363","question":"In a mixed Consul Connect mesh where 60% of services run in Kubernetes and the rest on VM-based hosts, implement automatic per-service TLS certificate rotation using Vault as the CA with short-lived certs, ensuring zero-downtime rotation for a non-HTTP protocol (e.g., Kafka) and strict identity-based ACLs. Outline the end-to-end workflow, including how sidecars fetch renewed certs, how rolling upgrades are coordinated, and how to rollback if rotation fails?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Google","Instacart","Robinhood"]},{"id":"q-3469","question":"In a Consul Connect mesh spanning Kubernetes (DC1) and VM workloads (DC2), a legacy service named 'legacy-collector' lacks SPIFFE identity and TLS bootstrap. Design a secure bootstrap workflow that grants minimal, time-bound access to a single upstream API 'billing-api' in DC1 for 24 hours, including ephemeral certificate issuance, identity injection, temporary Intentions updates, automatic rotation, and rollback if onboarding fails. Include concrete policy fragments and verification steps?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Google","Hashicorp","Meta"]},{"id":"q-3588","question":"In a Consul Connect mesh spanning Kubernetes in DC1 and VM-based services in DC2, a payment service 'paygate' in DC1 must reach a ledger service 'ledger' in DC2 during a regional outage. Design an automated, low-downtime failover workflow using SPIFFE IDs, ACLs, and token rotation, with canary rollout and rollback criteria. Include concrete policy fragments, metrics, and test steps?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Lyft","Robinhood","Square"]},{"id":"q-3659","question":"In a single Consul Connect mesh that uses Kubernetes namespaces, design a namespace-scoped ACL policy so that a CI runner service in namespace ci can access only the artifact-service in namespace prod, without access to any other service. Include concrete policy ideas, token provisioning, and deployment steps, plus a rollback plan if access must be broadened again?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Amazon","Netflix","Oracle"]},{"id":"q-4057","question":"In a mixed Kubernetes (DC1) and VM (DC2) Consul Connect mesh, a high-throughput stream processor 'streamer' must call an analytics sink 'sink' with strict per-call SPIFFE identity checks while maintaining low tail latency during peak traffic. Propose a practical workflow to minimize policy churn under load: precompute policy variants, stage canary rollouts, implement a dynamic token rotation cadence, and validate with end-to-end tests. Include concrete fragments for Intentions/ACL, metrics, and rollback criteria?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Databricks","Zoom"]},{"id":"q-4071","question":"In a Consul Connect mesh deployed on Kubernetes, a frontend service cannot reach a backend service intermittently due to DNS failures. Describe a beginner-friendly, end-to-end debugging workflow to diagnose whether the issue is DNS, service registration, or the sidecar proxy, and provide concrete commands, config checks (e.g., DNS settings, service definitions, ACLs if used), and rollback steps to restore access quickly?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Google","Tesla","Two Sigma"]},{"id":"q-4102","question":"In a Consul Connect mesh on Kubernetes, a gRPC client in service 'orders-broker' intermittently fails to establish streaming RPCs with 'orders-backend' due to TLS handshake errors. Propose a beginner-friendly end-to-end debugging workflow to determine if the issue is TLS/certs, DNS/service registration, or the sidecar proxy, with concrete commands, config checks (TLS settings, CA, CNs, ALPN), and rollback steps to restore stable gRPC streaming quickly?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Google","Microsoft"]},{"id":"q-4132","question":"In a Consul Connect mesh spanning two Kubernetes clusters in different data centers, a frontend in DC1 must reach a backend in DC2 via a mesh gateway but traffic is flaky. Propose a beginner-friendly end-to-end debugging workflow to verify cross-DC routing, including gateway config checks, service registrations, TLS certs, DNS, and a quick rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Airbnb","Microsoft"]},{"id":"q-4221","question":"Scenario: In a Consul Connect mesh on Kubernetes, a canary of 'payments-frontend' intermittently cannot reach 'currency-service' during peak traffic; DNS lookups sometimes succeed but requests fail. Propose a beginner-friendly end-to-end workflow to determine if the issue is DNS resolution (CoreDNS/Consul DNS), service name aliasing/registration in Consul, or the sidecar proxy. Include concrete commands, config checks, and a rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["DoorDash","Stripe"]},{"id":"q-4233","question":"In a Consul Connect mesh on Kubernetes, after adding a partner tenant and enabling a mesh gateway, the internal service 'gaming-auth' cannot reach 'player-stats' and TLS handshakes fail. Design a beginner-friendly end-to-end debugging workflow to determine whether the issue lies with the mesh gateway, mTLS identity, or DNS. Include concrete commands, config checks, and a rollback plan to restore connectivity quickly?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Discord","Slack"]},{"id":"q-4367","question":"In a multi-tenant Consul Connect deployment on Kubernetes, onboarding a new partner tenant requires strict per-tenant access control via ACLs, a mesh gateway, and automated mTLS cert rotation through Vault. Propose a concrete end-to-end plan that covers namespace scoping, per-tenant intentions, gateway wiring, Vault roles and cert TTLs, rotation strategy, contract-end revocation, and rollback steps. Include concrete config snippets and commands?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Oracle","Plaid","Salesforce"]},{"id":"q-4406","question":"In a Consul Connect mesh on Kubernetes, a mesh gateway upgrade in namespace 'net' coincides with intermittent failures when 'web-portal' calls 'reporting-api'. The issue seems tied to ACL/Intentions not propagating to sidecars during the upgrade window. Design a beginner-friendly end-to-end debugging workflow to determine whether the problem is (a) intentions propagation, (b) mesh gateway policy, or (c) per-namespace ACL caching. Include concrete commands, config checks, and rollback steps?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Oracle","Zoom"]},{"id":"q-4512","question":"In a Consul Connect mesh spanning Kubernetes and VMs, a canary of data-ingest intermittently stalls when calling ingest-processor under peak load. Design an end-to-end debugging workflow to determine whether the bottleneck is Envoy filter latency, mTLS identity binding, or ACL policy evaluation, with concrete commands, config checks, and rollback steps to restore throughput. Include how to collect Envoy stats, inspect ACLs and SPIFFE IDs, and test TLS material rotation?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["NVIDIA","Tesla"]},{"id":"q-4659","question":"In a Consul Connect mesh spanning two Kubernetes namespaces, a service 'payments' calls 'currency' through a mesh gateway. After a CA rotation and gateway policy change, TLS handshakes intermittently fail while DNS remains healthy. Propose a precise, end-to-end debugging workflow to isolate whether the issue is (a) CA propagation, (b) mesh gateway policy, or (c) per-service identity (SPIFFE/CN/SAN). Include concrete commands, config checks, and rollback steps?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Discord","IBM","Microsoft"]},{"id":"q-4762","question":"In a three-DC Consul Connect mesh with cross-DC SPIFFE federation, a frontend in DC1 intermittently fails TLS handshakes with a backend in DC3. Diagnose end-to-end: is it SPIFFE ID mismatch, cross-DC CA trust propagation, or mesh gateway policy? Provide exact commands to inspect identities, trust bundles, and gateway configs, plus rollback steps?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Apple","Oracle","Slack"]},{"id":"q-4794","question":"In a Consul Connect mesh on Kubernetes, the payments service in namespace finance intermittently receives 403s when calling currency after a new intent-based routing rule enabling path-based routing and service-name aliasing was rolled out via the mesh gateway. Outline a precise end-to-end debugging workflow to determine whether the failure is due to (a) ACL/policy evaluation, (b) service alias resolution, or (c) sidecar proxy configuration, with concrete commands, config checks, and rollback steps?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Adobe","Cloudflare","LinkedIn"]},{"id":"q-4846","question":"In a multi-region Consul Connect deployment (**Region A** and **Region B**) with a WAN federation, the service **payments-processor** in **Region A** intermittently fails to call **fraud-detector** in **Region B** during peak traffic. After a CA rotation and a mesh gateway policy change, cross-region TLS handshakes fail while DNS remains healthy. Propose a rigorous end-to-end debugging workflow to isolate whether the issue is (**a**) cross-region **SPIFFE identity propagation**, (**b**) mesh gateway **TLS termination** or **SNI handling**, or (**c**) **CA certificate propagation** across regions. Include concrete commands, config checks, and rollback steps?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Adobe","Plaid","Tesla"]},{"id":"q-880","question":"In a Consul Connect-enabled dev cluster, two services run in namespace dev: 'reviews' and 'ratings'. You want to enforce that only reviews can call ratings via the mesh, with all other cross-service calls denied by default. Describe the minimal service-defaults and service-intentions changes needed, and how you would verify using a small client container in dev?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Amazon","Google","Square"]},{"id":"q-919","question":"You're operating a multi-datacenter Consul Connect mesh. A new API service in DC1 must reach a legacy monolith in DC2 that cannot run a sidecar. Design a cross-datacenter connectivity pattern using a mesh gateway, per-service Intentions with explicit allow rules, and TLS credential rotation. Include resource definitions, deployment steps, and rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Anthropic","Hugging Face","PayPal"]},{"id":"q-999","question":"Design a cross-datacenter Consul Connect pattern in a three-datacenter setup where api-service.dc1 must reach legacy-db.dc3 (no sidecar). Use a MeshGateway to bridge DC1↔DC3, per-service Intentions with explicit allow rules, and TLS credential rotation. Include concrete resource definitions, deployment steps, and a rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["NVIDIA","Salesforce","Two Sigma"]},{"id":"q-1077","question":"Design a data ingestion and processing pipeline for a global ride-hailing platform that ingests 5 TB/day of operational events from multiple regional Kafka topics and batch feeds. Requirements: idempotent upserts into a table (Iceberg/Delta), handle late-arriving events, schema evolution, and partition pruning by country/date. Compare Flink vs Spark for streaming, and outline testing, monitoring, and data quality checks?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Amazon","LinkedIn","Lyft"]},{"id":"q-1140","question":"Given daily 1 GB of web logs in JSON lines stored on S3 with fields user_id, timestamp, path, status, and optional referrer, design a beginner-friendly pipeline (Python or Node.js) that deduplicates by timestamp+user_id+path, validates required fields, normalizes timestamp to UTC, and writes date-partitioned Parquet to a data lake; include basic tests and monitoring?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Discord","NVIDIA","Stripe"]},{"id":"q-1272","question":"Design a data pipeline to ingest 2M GPU telemetry events per minute from a global fleet of AI training clusters into a data lake and a feature store. Events include host_id, region, timestamp, metric_type, and value. Requirements: immutable raw Parquet storage partitioned by region/hour; near-real-time metrics and anomaly alerts (1–2 minute latency) via a streaming engine; idempotent upserts into a feature store; schema evolution handling; late-arriving data; cost-aware storage/compute; monitoring and tests; compare Spark vs Flink for streaming components?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["Apple","NVIDIA"]},{"id":"q-1498","question":"Design a global data ingestion and governance pipeline for a real-time ad-tech platform that processes 200k events/sec from 3 cloud regions into a centralized lakehouse. Each event has event_id, tenant_id, timestamp, event_type, and payload. Requirements: enforce data contracts via a central registry (schema + compatibility rules), support schema evolution with automatic catalog updates, and ensure multi-tenant data isolation and access control. Implement partitioning by tenant/date, handle late-arriving data within a 1–2 minute SLA, ensure data lineage and quality checks, and provide rollback semantics for contracts. Compare Iceberg vs Delta as the storage layer, outline testing/monitoring, and describe concrete example schemas and contract definitions. Include how you'd validate end-to-end?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Scale Ai","Tesla"]},{"id":"q-1692","question":"In a social app generating 100 GB/day of newline-delimited JSON events across 3 regions stored in S3, design a beginner-friendly batch pipeline that validates fields (event_id, user_id, timestamp, event_type), derives date, deduplicates by event_id, hashes user_id for privacy, and writes Parquet partitioned by date to a data lake. Compute a daily data-quality score (0-1) based on missing/invalid fields and store it in a metadata table. Outline tooling, testing, and monitoring?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Anthropic","PayPal","Square"]},{"id":"q-1701","question":"Ingest 3 TB/day of regional event data from Kafka (EU/US/APAC) with user_id, session_id, event_type, timestamp, and attributes. Design a privacy-first analytics pipeline: per-region salt pseudonymization of user_id before cross-region joins, idempotent upserts into Apache Iceberg, event-time processing with 15-minute tumbling windows and 1-hour late data, and immutable audits and data contracts. Compare Spark Structured Streaming vs Flink for the streaming layer, and specify GDPR masking after aggregation?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Google","Lyft"]},{"id":"q-1928","question":"Design a cost-aware real-time ingestion pipeline that processes 50 TB/day of clickstream data from multiple sources (Kafka topics and batch feeds) into a lakehouse. Ensure idempotent upserts into an Iceberg/Delta table, handle late-arriving events, and support schema evolution with partition pruning by date and region. Compare Spark Structured Streaming vs Flink for the path and outline data quality checks, testing, and monitoring strategies?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["NVIDIA","Robinhood","Two Sigma"]},{"id":"q-1965","question":"Design a GDPR/CCPA-compliant, multi-source data pipeline for an e-commerce analytics platform. Ingest CDC from OLTP databases plus batch feeds, preserve raw data immutably, and enable per-tenant data isolation. Implement lineage via a metadata catalog, apply privacy techniques (PII redaction and differential privacy for aggregates), and support schema evolution. Compare Flink vs Spark for streaming, outline tests, monitoring, and cost implications across regions?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Google","LinkedIn","Robinhood"]},{"id":"q-1971","question":"You run a product analytics pipeline for a global iOS/Android app (telemetry: user_id, device_id, ts, event, properties). Data must be ingested from a MongoDB Atlas source into a lakehouse on S3 using Apache Iceberg. Design an end-to-end pipeline that supports idempotent upserts, late-arriving events, and schema evolution, while enforcing PII masking and GDPR data deletion requests. Compare using Flink vs Spark for streaming, outline testing, monitoring, and data-quality checks?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["Apple","MongoDB"]},{"id":"q-2272","question":"Design a global telemetry ingest pipeline for 8 regional Kafka clusters, totaling 20 TB/day of JSON events. Build an end-to-end flow into a lakehouse using Iceberg, ensuring idempotent upserts, late-arriving events, and schema evolution for nested JSON; enforce per-country data sovereignty and GDPR deletion requests. Compare Spark Structured Streaming vs Flink, and outline testing, monitoring, and data-quality checks?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Google","Tesla"]},{"id":"q-2433","question":"Design a multi-tenant data ingestion pipeline that streams 40 TB/day of event data from dozens of publishers, each with its own schema and retention policy. Explain how you enforce per-tenant data contracts, support dynamic schema evolution, guarantee idempotent upserts into a lakehouse, handle late-arriving events and GDPR delete requests, and implement cross-region replication with cost controls. Compare Spark Structured Streaming vs Flink for this workload and outline testing and monitoring?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Bloomberg","Salesforce"]},{"id":"q-2460","question":"Design an end-to-end data pipeline for a multi-tenant ride-hailing platform where city-level feature stores power online fraud scoring and offline model training. Ingest 1) trip events, 2) driver status, 3) external pricing data; ensure per-city isolation, schema evolution, and GDPR deletion. Compare Spark Structured Streaming vs Flink for streaming, outline tests and monitoring?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["LinkedIn","Lyft","Uber"]},{"id":"q-2564","question":"You're building a cross-region analytics platform ingesting 200M events/day into a lakehouse. Design a data-contract–driven pipeline with a central schema registry enforcing compatibility, add automated data quality, lineage, and privacy masking for PII, and ensure BI dashboards and model training never see raw sensitive fields. Compare Spark vs Flink for streaming governance and outline testing and monitoring?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["Amazon","Bloomberg","Instacart"]},{"id":"q-2730","question":"You're building a multi-tenant retail analytics pipeline. Ingest JSON logs of user sessions from regional storefronts: {user_id, session_id, region, items: [{sku, qty, price}], event_ts, revenue, marketing}. Data lands in S3 daily and must feed a lakehouse (Iceberg) with near-real-time revenue per SKU by region. Requirements: per-tenant masking, GDPR deletion, schema evolution, idempotent upserts, late-arriving events, and robust testing/monitoring. Describe the end-to-end design, data contracts, and trade-offs between Flink and Spark for streaming?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["Salesforce","Stripe"]},{"id":"q-2762","question":"You're given a daily 200–500 MB CSV of user events with columns: user_id, event_time, event_type, payload. Design a beginner-friendly pipeline to load it into a Parquet lakehouse, partitioned by event_date, deduplicated on user_id+event_time, and with a small audit log for rows with invalid timestamps. Outline steps and a minimal PySpark snippet to start?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["IBM","LinkedIn","Stripe"]},{"id":"q-2789","question":"You are onboarding a partner providing a daily JSON event feed (fields: user_id, event_ts, event_type, payload). Design a beginner-friendly batch ETL to normalize into a flat table, store Parquet in a lake partitioned by date, and perform idempotent upserts into the warehouse. Include basic data quality: non-null fields, timestamp sanity (within 90 days), and malformed-record handling. Compare batch vs streaming ingestion and outline tests?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Meta","Scale Ai","Twitter"]},{"id":"q-2884","question":"Ingest streaming IoT telemetry from a global fleet where devices send json envelopes with id, ts, region, metrics, and an optional base64-encoded payload. Build an end-to-end pipeline into a lakehouse that decodes payloads, handles schema evolution, late events, and per-region masking, and supports GDPR delete requests. Compare Spark Structured Streaming vs Flink for this workload and outline testing/monitoring strategies?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["Netflix","Oracle"]},{"id":"q-2984","question":"You receive a nightly CSV feed customers.csv with fields customer_id, signup_date, region, tier, and email_verified. A customer can appear on multiple nights with updated fields. Design a beginner-friendly pipeline to upsert into a canonical table customers_dim, ensure late changes are captured, implement basic data-quality checks, and describe how you'd test it?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Discord","Meta","Netflix"]},{"id":"q-3087","question":"You're handed a monthly 5GB JSON Lines file of e-commerce orders: {order_id, user_id, product_id, price, qty, timestamp, status}. Design a beginner-friendly batch ETL to deduplicate by order_id, normalize timestamp, fill missing price with 0 and status with 'new', and store as partitioned Parquet by year-month in a data lake. Explain validation, idempotence, and tests; compare Spark vs Pandas for this workload?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Citadel","MongoDB","NVIDIA"]},{"id":"q-3181","question":"Design a global customer-analytics pipeline for a multi-region marketplace. Region-specific Kafka streams deliver events: user_id, session_id, region, event_type, timestamp, and attributes. Build a unified, near-real-time profile in an Iceberg lakehouse with per-region privacy, GDPR deletion, and schema evolution. Describe data contracts, idempotent upserts, late-arriving events, and trade-offs between Spark Structured Streaming and Flink; include testing and monitoring plan?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["Discord","Instacart","Tesla"]},{"id":"q-3189","question":"You operate a streaming ingestion system that consumes 50k JSON events per second from mobile devices across regions. Events have nested payloads with optional fields and must be transformed into a star schema in a lakehouse (Iceberg) with schema evolution. Data arrives late (up to 2 hours). Describe end-to-end design, including idempotent upserts, per-record data contracts, and per-record lineage. Compare Spark Structured Streaming vs Flink for processing and outline testing/monitoring?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["Coinbase","Discord","Oracle"]},{"id":"q-3312","question":"Design an end-to-end cross-region streaming pipeline ingesting 2.5M events/day from mobile apps into a lakehouse. Build a real-time feature store with time-bounded features, support late arrivals up to 10 minutes, and enforce per-region residency and masking. Compare Spark Structured Streaming vs Flink for the pipeline, and outline testing, monitoring, and data-contract guarantees?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Databricks","Netflix","Zoom"]},{"id":"q-3441","question":"Ingest 2 TB/day mobile app events in JSON with nested optional fields into an Iceberg lakehouse. Timestamps arrive in multiple time zones. Design end-to-end pipeline that normalizes times to UTC, flattens key nested fields into a flat schema, and upserts with per-record lineage. Include schema validation, handling of missing fields, and monitoring; compare Spark Structured Streaming vs Flink for this workload?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Google","Plaid"]},{"id":"q-3545","question":"Design a real-time user-event pipeline ingesting multi-region nested JSON into a lakehouse (Iceberg). Each event includes user_id, event_ts, app_version, and a nested payload with optional fields that can evolve. Build an end-to-end flow with per-record data contracts, idempotent upserts, and per-record lineage; support late data, GDPR deletion, and maintain an online feature store for scoring with sub-100ms reads. Compare Spark Structured Streaming vs Flink for the streaming path, and outline testing and monitoring?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["Netflix","Twitter"]},{"id":"q-3693","question":"Ingest 3 TB/day of JSON telemetry from 30 regional devices into an Iceberg lakehouse. Events include nested payloads with optional fields. Build a streaming pipeline that upserts using an idempotent upsert_id (device_id+event_id), enforces per-record data contracts with a schema registry, and records lineage. Handle late data (12h) and tombstones for deletes. Compare Spark Structured Streaming vs Flink and outline tests/monitoring?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["DoorDash","Meta"]},{"id":"q-3821","question":"You're building a real-time graph analytics platform that ingests edge events from IoT devices: fields include device_id, neighbor_id, ts, tenant_id, and edge_attrs (nested, evolving). Data lands per-tenant in object storage and must feed a lakehouse (Iceberg) with near-real-time graph views. Requirements: per-tenant isolation, late-arriving edges up to 12 hours, idempotent upserts for edge state, schema evolution for edge_attrs, and end-to-end data contracts with lineage. Compare Flink vs Spark for streaming, and outline testing, monitoring, and GDPR-delete handling?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["IBM","Plaid","Snap"]},{"id":"q-3827","question":"Design a global real-time fraud-detection pipeline for a fintech with three regional streams (Kafka and MQTT) producing JSON/Parquet. Build a lakehouse (Iceberg) with bronze/silver/gold layers, ensure per-record lineage and region-based data isolation, and GDPR deletion. Implement idempotent upserts and late-data handling with schema evolution. Compare Spark Structured Streaming vs Flink for the stream path and outline testing, monitoring, and rollback strategies?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Goldman Sachs","IBM","MongoDB"]},{"id":"q-4164","question":"Design a data mesh for ML features: ingest millions of signals from diverse sources, publish stable feature views to a central feature store, enforce per-source data contracts, implement drift detection and lineage to models, and honor GDPR deletions. Explain schema evolution, near-zero downtime feature updates, and a testing/monitoring plan?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Adobe","Meta","Snowflake"]},{"id":"q-4246","question":"Design a policy-driven data masking and lineage system for streaming user data into a data lakehouse. Data arrives from regional sources with PII; implement dynamic masking based on consent and region, propagate masking decisions downstream, and ensure end-to-end lineage and a reproducible testable rollback. Include data contracts, schema evolution, and GDPR delete handling. Compare Spark vs Flink for the streaming leg and outline testing/monitoring?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["Google","Meta","MongoDB"]},{"id":"q-4395","question":"Design a multi-region CDC ingest for a payments platform: region adapters expose fields (payment_id, user_id, region, timestamp, event_type, metadata) with differing schemas; implement a canonical Avro, a changelog for schema evolution, and an end-to-end pipeline that upserts into Iceberg lakehouse via Flink (or Beam on Dataflow). Ensure per-event lineage, GDPR delete handling, and test/monitor strategies; justify trade-offs between Flink and Beam?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Amazon","Square","Stripe"]},{"id":"q-4440","question":"Design and implement a CDC-based data pipeline that captures changes (insert/update/delete) from a PostgreSQL source at 50k rows/sec, streams to a data lake using Apache Iceberg, ensures idempotent upserts, per-record lineage, and automatic schema drift handling; compare Debezium + Spark Structured Streaming vs Flink for sink upserts, and describe testing and monitoring strategies?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["Bloomberg","MongoDB","Uber"]},{"id":"q-4448","question":"You're building a geo-distributed ride-hailing analytics platform across 50 cities. Ingest GPS traces (vehicle_id, rider_id, ts, lat, lon), events (type: trip_start, trip_end), and telemetry. 2 TB/day of JSON. Requirements: per-city data isolation, GDPR deletion, schema evolution, idempotent upserts, late data up to 30 minutes, per-record data contracts, and robust data-quality gates. Propose end-to-end design: streaming ingestion, lakehouse (Iceberg), partitioning by city, and per-record lineage. Compare Spark Structured Streaming vs Flink for processing, and outline testing/monitoring strategies including contract tests?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Airbnb","Scale Ai","Square"]},{"id":"q-4519","question":"You operate nightly ingestion of 30 regional vendor CSV feeds, each with up to 60 columns, some added over time. Data must land in a lakehouse (Iceberg) with per-record lineage and idempotent upserts, while dynamic schema evolution handles new columns. How would you design an end-to-end pipeline that enforces per-record data contracts, buffers late rows (up to 24h), isolates data by region, and validates data quality before load? Include simple monitoring and a concrete MERGE example?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Google","Instacart"]},{"id":"q-457","question":"You need to process 10GB of CSV files daily and load them into a PostgreSQL database. The files contain user activity logs with timestamps, user IDs, and event types. How would you design an efficient ETL pipeline using Python?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Anthropic","Discord","MongoDB"]},{"id":"q-4590","question":"You're building a real-time ad-tech analytics pipeline with impression and click events from multiple CDNs; data must be region-isolated, support late events up to 15 minutes, and GDPR deletions, with Iceberg as the lakehouse. Design end-to-end ingestion, processing, and storage, plus lineage and testing; compare Spark vs Flink trade-offs?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["Bloomberg","Google","NVIDIA"]},{"id":"q-4689","question":"Design a beginner-friendly ingestion pipeline for a mobile app emitting JSON events (open, screen_views) at ~60k/min across regions. Ingest into a lakehouse with per-record data contracts and idempotent upserts. Flatten nested payloads, support late events up to 15 minutes, and produce daily active users by country/version. Compare Airflow vs Dagster for orchestration and outline tests and monitoring?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Coinbase","DoorDash","Salesforce"]},{"id":"q-4718","question":"You're building a geo-distributed rideshare telemetry pipeline across 24 regions. Ingest JSON events with nested metrics; land into Iceberg lakehouse with per-record lineage, and perform idempotent upserts on (vehicle_id, event_ts). Support dynamic schema evolution and late data up to 10 minutes. Describe end-to-end ingestion, region isolation, data contracts, a concrete MERGE example; compare Spark vs Flink for streaming and outline monitoring?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["Bloomberg","Lyft","Twitter"]},{"id":"q-4812","question":"Ingesting 100 TB/day of Parquet into an Iceberg lakehouse across regions, design a cost-aware data lifecycle with per-tenant retention, dynamic tiering to hot/warm/cold storage, and a policy engine that preserves Iceberg time travel; how would you ensure sub-60s dashboard reads, encryption at rest, per-tenant masking, and scalable partition pruning for cost control?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Microsoft","Tesla"]},{"id":"q-488","question":"You're building a real-time analytics pipeline for a food delivery app. How would you design a data pipeline to process 1M events/day with 5-minute latency, considering data quality, schema evolution, and cost optimization?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["Adobe","Google","Instacart"]},{"id":"q-518","question":"You have a 10GB CSV file with user activity logs that needs to be processed daily. The file contains user_id, timestamp, action_type, and metadata. How would you design a data pipeline to efficiently process this file and load it into a data warehouse?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Adobe","Airbnb","Oracle"]},{"id":"q-571","question":"How would you design a data pipeline to process 1M ride events per minute from Uber's real-time streaming system?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Meta","Twitter","Uber"]},{"id":"q-885","question":"You operate a multi-tenant SaaS analytics platform ingesting per-tenant event streams from Kafka into Snowflake. Each tenant has different event schemas that can evolve independently. Design a data pipeline to enforce per-tenant data contracts, support late-arriving events, and minimize schema drift while controlling storage costs. Include schema versioning, validation, and deployment safety steps?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["NVIDIA","Snowflake","Stripe"]},{"id":"q-915","question":"You ingest 200k newline-delimited JSON app events daily into S3. Each event has event_id, user_id, timestamp, event_type, and attributes. Design a beginner-friendly pipeline to deduplicate by event_id, hash user_id for privacy, validate required fields, and write Parquet data partitioned by date in a data lake. Address simple schema drift and testing?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Discord","MongoDB","Snap"]},{"id":"q-993","question":"Design a global, multi-tenant data ingestion system for a ride-hailing platform with streams for billing, trips, safety, and promotions. Each tenant defines a data contract; schemas evolve independently; late-arriving events up to 15 minutes must be accepted. Describe architecture using Apache Kafka, Schema Registry (Avro/JSON), an Iceberg/Delta lake sink, and a streaming processor (Flink/Spark). Include data model, schema evolution, backfill handling, testing, and observability?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Coinbase","DoorDash","Meta"]},{"id":"q-176","question":"How would you design a data pipeline that handles both batch and streaming workloads for real-time analytics?","channel":"data-engineering","subChannel":"streaming","difficulty":"beginner","tags":["streaming","kafka"],"companies":["Amazon","Google","Meta","Netflix","Uber"]},{"id":"q-222","question":"How would you design a Kafka Streams application to handle exactly-once processing with stateful aggregations while maintaining sub-second latency during peak loads of 100K events/sec?","channel":"data-engineering","subChannel":"streaming","difficulty":"advanced","tags":["kafka","flink","kinesis"],"companies":["Amazon","Confluent","Netflix","Stripe","Uber"]},{"id":"q-248","question":"How would you implement exactly-once processing in a data pipeline when both source (Kafka) and sink (database) can fail, ensuring no duplicate data or data loss during network partitions and system crashes?","channel":"data-engineering","subChannel":"streaming","difficulty":"intermediate","tags":["dag","orchestration","scheduling"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Snowflake"]},{"id":"q-1038","question":"Design a stack that supports push(x), pop(), top(), and getMin() in O(1) time per operation. Duplicates allowed. Explain the approach, discuss invariants and space usage, and provide a compact code sketch (Python or Java)?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Apple","Databricks","Tesla"]},{"id":"q-1213","question":"Design a dynamic autocomplete data structure for a code search tool that stores terms with frequencies; implement insertWord(word), eraseWord(word), and querySuggestions(prefix, k) returning up to k completions starting with prefix, ordered by descending frequency then lexicographically. Discuss a Trie-based design with per-node top-k structures and update costs?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Hashicorp","Hugging Face"]},{"id":"q-2010","question":"Design a dynamic session tracker: each session has an id, a score, and a lastActive timestamp. Support insertSession(id, score, ts), updateScore(id, delta), expireSessionsBefore(ts) removing expired sessions, topK(window, k) returning the k highest scores among active sessions (active = ts >= now - window) breaking ties by id, and medianScore(window) returning the median score among active sessions. Target near O(log n) per update and O(k log n) for topK. Explain data structures and tradeoffs?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Anthropic","Tesla"]},{"id":"q-2087","question":"Design a dynamic leaderboard data structure to support a game with many players: addOrUpdatePlayer(playerId, score), removePlayer(playerId), getTopK(k), and getPlayerRank(playerId). Achieve O(log n) updates and O(log n + k) to fetch the top-k. Explain how you handle duplicates (equal scores) and score changes?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Databricks","Lyft","Plaid"]},{"id":"q-2178","question":"Design a dynamic prefix-aware dictionary that stores strings with counts (duplicates allowed). Implement: addWord(s), removeWord(s) removing one occurrence, queryPrefixCount(p) giving how many distinct words start with p, and kthWordWithPrefix(p, k) returning the k-th word in lexicographic order among words beginning with p. Handle up to 2e5 words, optimize memory and updates; propose a data layout (e.g., compressed Trie) and discuss trade-offs and edge cases?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Anthropic","Google","Two Sigma"]},{"id":"q-2324","question":"Design a dynamic bitset over a universe [1, 10^9] that supports set(l, r), reset(l, r), flip(l, r), sum(l, r), and kthOne(l, r, k). Use a run-length encoded interval map to achieve near O(log M) updates, where M is number of stored intervals. Explain data structure, edge cases, and complexity; provide a robust approach for sparse updates in large N?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Citadel","Google","Hugging Face"]},{"id":"q-2356","question":"Design a dynamic string editor data structure that supports insert at position, delete substring, and substring hash queries with O(log n) edits and O(log n) hash lookups. Propose a rope-based solution using an implicit treap, where each node stores size and a double rolling hash (mod1, mod2). Explain split/merge, hash maintenance, collision mitigation, and memory trade-offs with a realistic editor workload?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Adobe","Citadel","Twitter"]},{"id":"q-2503","question":"Design a lightweight 1D histogram data structure for values in [0, 1_000_000] that supports: insertValue(v), eraseValue(v) (one occurrence), rangeCount(l, r) counting how many values lie in [l, r], and kthValueInRange(l, r, k) returning the k-th smallest value within [l, r]. Aim for O(log M) per operation. Compare Fenwick vs segment-tree-of-frequencies and discuss handling of duplicates and memory?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["OpenAI","Robinhood","Twitter"]},{"id":"q-2666","question":"Design a dynamic string-frequency dictionary that stores strings with counts. Implement: addString(s), removeString(s) (one occurrence), and queryTopK(prefix p, k) that returns the k most frequent strings starting with p, ordered by frequency descending and then lexicographically. Up to 2e5 total insertions. Provide a practical approach and discuss time/memory trade-offs; keep it beginner-friendly?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Airbnb","Netflix","Square"]},{"id":"q-2725","question":"Design a stack that supports push(x), pop(), top(), and getMin() in O(1) time. Duplicates allowed. Describe the two-stack approach that maintains the current minimum on each push, how pops update both stacks, and how to handle empty state. Include a short example sequence and expected min values?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Cloudflare","Lyft","Two Sigma"]},{"id":"q-2919","question":"Design a dynamic priority queue for a real-time task scheduler: supports insertJob(id, priority, createdAt), updatePriority(id, newPriority), removeJob(id), popNext(), and pruneOld(now, maxAge). Tie-breaking: oldest createdAt. Aim for near O(log n) per operation. Explain data structures (augmented heap + hashmap), lazy deletion, edge cases, and memory trade-offs. Scenario: high-scale service processing user requests with varying priorities?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Apple","MongoDB"]},{"id":"q-2956","question":"Design a dynamic 3D weighted point set that supports insertPoint(x,y,z,w), erasePoint(x,y,z,w) (duplicates allowed), movePoint(oldX,oldY,oldZ,newX,newY,newZ) to relocate one occurrence, rangeSum3D(x1,x2,y1,y2,z1,z2) for the total weight in the axis-aligned 3D box, and kthLargestIn3DBox(x1,x2,y1,y2,z1,z2,k) for the k-th largest weight inside that box. Target average O(log^3 N) per operation; discuss coordinate compression, memory trade-offs, and handling duplicates and moves?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Coinbase","Google","Salesforce"]},{"id":"q-2969","question":"Design a dynamic histogram for integers in a large domain (e.g., 0..1,000,000). Implement addValue(x, delta), removeValue(x, delta), and queryQuantile(q) returning the smallest x with cumulative frequency ≥ q. Also support mergeHistogram(other). Explain coordinate compression, O(log n) updates, and memory trade-offs between sparse and dense representations. How would you implement this, including data structures and edge cases?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Citadel","Lyft","OpenAI"]},{"id":"q-3061","question":"Design a dynamic data structure for a set of 2D colored points that supports: insertPoint(x,y,color), erasePoint(x,y,color) (duplicates allowed), and countDistinctColorsInRect(x1,y1,x2,y2) returning how many unique colors appear in the rectangle. Optimize for average O(log^2 n) per op; discuss coordinate compression, per-node color maps, and duplicate handling?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Discord","Google","PayPal"]},{"id":"q-3195","question":"Design a persistent 2D weighted point set that supports insertPoint(x,y,w) and erasePoint(x,y,w) (duplicates allowed) creating new versions, rangeSum4D(x1,x2,y1,y2,t) for version t, and kthLargestInRectInVersion(x1,x2,y1,y2,t,k) for that version; aim for O(log^2 N) per op; discuss coordinate compression, persistence strategy, and memory trade-offs?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Airbnb","Apple","Lyft"]},{"id":"q-3282","question":"Design a dynamic 2D dominance index for weighted points. Each point is (x,y,w,id) where id differentiates duplicates. Supports: insertPoint(x,y,w,id), erasePoint(x,y,w,id) removing one occurrence, queryDominatedCount(x0,y0) returning the number of points with x<=x0 and y<=y0, and queryKthLargestDominated(x0,y0,k) returning the k-th largest weight among dominated points. Aim for O(log^2 N) updates and O(log^2 N log W) kth queries. Discuss compression and per-node multisets?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Databricks","Two Sigma","Zoom"]},{"id":"q-3391","question":"Design a dynamic 2D data structure for a multiset of colored points (x, y, color). Implement insertPoint(x,y,color) and erasePoint(x,y,color) (duplicates allowed). Provide rangeCountInRect(x1,x2,y1,y2,color) and rangeModeInRect(x1,x2,y1,y2) that returns the color with the highest frequency in the rectangle (ties broken by smaller color id). Aim for average O(log^2 n) per op; discuss compression, memory trade-offs, and handling duplicates?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Google","Instacart"]},{"id":"q-3495","question":"Design a dynamic forest data structure for rooted trees with labeled edges. Operations: link(u,v,w) attach the root of one tree as a child of another with edge label w; cut(u) detach a subtree rooted at u; pathLabels(u,v) return the multiset of edge labels along the path from u to v; and kthSmallestOnPath(u,v,k) return the k-th smallest label on that path. Achieve near O(log n) per operation by using a Link-Cut Tree augmented with order-statistics; discuss how you handle duplicates and path aggregation?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Databricks","Google","Meta"]},{"id":"q-3644","question":"Design a data structure to process a stream of integers arriving one by one. Implement insert(x) and getMedian() that returns the current median after each insertion. Use two heaps: a max-heap for the lower half and a min-heap for the upper half. Describe balancing rules to keep sizes within one and how to compute the median for odd/even counts. Consider duplicates and streaming constraints?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Databricks","IBM","Microsoft"]},{"id":"q-3773","question":"Design a data structure that supports: addValue(x) to insert an integer x into a multiset, removeValue(x) to remove one occurrence, and getMode() to return the value with the highest frequency (ties broken by smaller value). Aim for average O(log n) updates. Example: after addValue(5), addValue(3), addValue(5), addValue(2), addValue(5), getMode() should return 5?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Discord","Meta"]},{"id":"q-3913","question":"Design a dynamic 2D weighted point set supporting insertPoint(x,y,w) and erasePoint(x,y,w) (duplicates allowed). Add time-aware queries: (a) sumWeightsInRectangle(x1,x2,y1,y2,t) for total weight inside the rectangle as of time t, and (b) kthLargestWeightInRectangle(x1,x2,y1,y2,t,k) for the k-th largest weight among active points in the rectangle at time t. Target avg O(log^2 n) updates and queries; discuss persistent structures and versioning strategies?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Google","Square"]},{"id":"q-3962","question":"Design a queue that supports enqueue(x), dequeue(), and getMin() in O(1) amortized time. The queue must handle duplicates and scale to 1e5 operations. Explain the data structure and show how enqueue, dequeue, and getMin would work in detail?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Apple","DoorDash","Two Sigma"]},{"id":"q-4015","question":"Design a queue data structure that supports: push(x) to back, pop() from front, max() returning the current maximum element, and sum() returning the sum of all elements in the queue. All four operations must be amortized O(1). Provide a concrete implementation plan (two stacks with per-element max, and a running sum) and a short code sketch in a language of your choice?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Amazon","MongoDB"]},{"id":"q-4040","question":"Design a data structure to process a dynamic undirected graph with parallel edges that can appear and disappear over time. Each edge has an active interval [l, r). Support addEdge(u, v, l, r, edgeId) and closeEdge(u, v, edgeId) to end early, plus isConnected(u, v, t) for any time t. Propose an offline segment-tree-over-time solution using a DSU with rollback; analyze time complexity and edge counts, including handling of parallel edges?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Netflix","OpenAI","Scale Ai"]},{"id":"q-4142","question":"Design a data structure to manage a dynamic multiset of circular intervals on a fixed circular timeline with period P. Each interval is represented as [start, end) on the circle; intervals may wrap around and duplicates are allowed. Implement insertInterval(start, end, w), eraseInterval(start, end, w) (one occurrence), rangeSumInArc(a, b) returning the total weight of all intervals intersecting arc [a, b] clockwise, and kthLargestInArc(a, b, k) returning the k-th largest weight among intervals intersecting that arc. Target average O(log n) per operation. Explain handling wrap-around, coordinate compression, memory trade-offs, and duplicates?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Amazon","Oracle","Plaid"]},{"id":"q-4254","question":"Design a dynamic double-ended queue (deque) that supports pushFront(x), pushBack(x), popFront(), popBack(), and returns getMin() and getMax() in O(1) time after each operation. Duplicates allowed. Explain how to maintain min/max with constant-time updates and analyze memory usage?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Databricks","Oracle","Two Sigma"]},{"id":"q-4413","question":"Design a dynamic 2D point set supporting insertPoint(x, y) and erasePoint(x, y) (duplicates allowed). Implement two queries: nearestNeighbor(x0, y0) returning the closest point to (x0, y0), and kNearestNeighbors(x0, y0, k) returning the k closest points. Optimize for practical performance with insertions and deletions; discuss using a dynamic KD-tree with lazy deletions and rebuild strategy, how to handle duplicates with unique IDs, and expected amortized costs in practice?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Square","Stripe","Tesla"]},{"id":"q-4426","question":"Design a dynamic 2D weighted point set supporting insertPoint(x,y,w) and erasePoint(x,y,w) with duplicates allowed, plus a region toggle operation toggleRegion(x1,x2,y1,y2) that flips the active flag of all points inside the rectangle. Implement rangeSumActive2D(x1,x2,y1,y2) for total weight of active points and kthLargestActiveInRect(x1,x2,y1,y2,k) for the k-th largest weight among active points in the rectangle. Aim for average O(log^2 n) per operation; discuss data structure choices, persistence implications, and how to handle duplicates?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Anthropic","OpenAI","Stripe"]},{"id":"q-4468","question":"Design a dynamic graph connectivity data structure that processes a time-ordered sequence of operations on an undirected graph with N nodes. Supported ops: addEdge(u,v), removeEdge(u,v) (edges may be added again later), and queryComponents() to return the current number of connected components. All operations are known in advance. Describe and implement an offline solution using a segment tree over time and a rollback DSU to answer queries in O((N+Q) log Q) time. Explain edge lifespan handling and complexity trade-offs?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Amazon","Meta","Tesla"]},{"id":"q-4577","question":"Design a dynamic multiset of integers supporting insertWeight(v) and eraseWeight(v) (duplicates allowed) and rangeSum(l, r) returning the total of all values in [l, r]. Use coordinate compression and two Fenwick trees (one for counts, one for sums) to support O(log M) updates and queries. Address duplicates, erase of non-existent values, and memory usage. Assume values are integers within a known domain?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Goldman Sachs","Lyft"]},{"id":"q-4817","question":"You're given a graph where edges can be added and removed over time. Design an offline dynamic connectivity solution that answers, at arbitrary times, whether two nodes are connected and the size of their connected component. Use a segment-tree-over-time plus a DSU with rollback; outline edge-lifetime mapping, query handling, and complexity for up to 2e5 operations?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Amazon","Google","IBM"]},{"id":"q-677","question":"Design a data structure to maintain dynamic counters for items, supporting add(key, delta), get(key), and getTopK(k) returning the k keys with highest counts, ties broken by key. In a real-time analytics scenario, such as tracking top active users by event count, describe the structure, updates, and complexity trade-offs?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Bloomberg","Discord","Netflix"]},{"id":"q-686","question":"Design a data structure that maintains a multiset of integers with insert(x), erase(x) for one occurrence, findMedian(), and findKthSmallest(k). Achieve O(log n) time per operation on average. Explain data layout, invariants, and handling duplicates and lazy deletions. For example: insert 1,2,3,4,5; erase 3; what is median and findKthSmallest(2)?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Apple","Oracle","Twitter"]},{"id":"q-697","question":"Design a time-weighted event multiset data structure. Supports insertEvent(ts, w), eraseEvent(ts, w) removing one occurrence, rangeSum(a, b) returning the sum of weights for events with timestamps in [a, b], and findKthWeightInRange(a, b, k) returning the k-th smallest weight among events with timestamps in [a, b]. Target O(log^2 n) per operation; handle duplicate timestamps and weights; discuss memory and trade-offs?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["DoorDash","Plaid","Scale Ai"]},{"id":"q-701","question":"Design a dynamic multiset of 2D points (duplicates allowed) with insertPoint(x,y), erasePoint(x,y) removing one occurrence, rangeCount(x1,y1,x2,y2), and rangeKthX(x1,y1,x2,y2,k) returning the k-th smallest x in the rectangle (tie by y). Target O(log^2 n) per op. Propose a segment tree over x; each node stores an ordered multiset of (y, unique_id). Explain duplicates handling, updates, rangeCount, and rangeKthX via binary search over x?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Apple","Discord","Snap"]},{"id":"q-710","question":"Design a data structure that maintains a dynamic multiset of strings with operations insert(word), erase(word) removing a single occurrence, countWithPrefix(prefix) returning how many words start with the prefix, and mostFrequentWithPrefix(prefix) returning the most frequent word among those starting with the prefix (tie-break lexicographically). What data structure would you implement, and what are the time complexities and invariants?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Apple","Scale Ai","Slack"]},{"id":"q-722","question":"Design a data structure for an array of integers that supports point updates update(i, val) and range maximum subarray sum queries maxSubarray(l, r) in O(log n). Explain the segment tree node data (sum, bestPref, bestSuff, bestSub) and the merge logic, including tie-breaking for leftmost subarray and handling entirely negative ranges. Example: start with [1,-2,3,4,-1], update index 2 to 5, query maxSubarray(0,4)?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Apple","Bloomberg"]},{"id":"q-727","question":"Design a dynamic data structure for weighted 3D points (x,y,t) with weight w. Support insertPoint(x,y,t,w) and erasePoint(x,y,t,w) (duplicates allowed), rangeSum3D(x1,x2,y1,y2,t1,t2) for total weight in the 3D box, and findKthLargestInRange(x1,x2,y1,y2,t1,t2,k) for the k-th largest weight inside the box. Target O(log^3 n) per op; O(log W * log^3 n) for kth; discuss coordinate compression, memory trade-offs, and handling duplicates?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Airbnb","Google","Meta"]},{"id":"q-739","question":"Design a dynamic 2D weighted point data structure that supports insertPoint(x,y,w), erasePoint(x,y,w) (duplicates allowed), rangeSum2D(x1,x2,y1,y2) for the total weight in the rectangle, and kthLargestInRectangle(x1,x2,y1,y2,k) for the k-th largest weight inside the rectangle. Aim for average O(log^2 n) per operation; discuss coordinate compression, memory trade-offs, and duplicates handling?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Citadel","Hugging Face","Snap"]},{"id":"q-746","question":"Design a dynamic multiset of integers that supports insert(x), erase(x) (one occurrence), countInRange(l, r) for the number of elements in [l, r], and kthSmallestInRange(l, r, k) returning the k-th smallest value among elements in [l, r]. Aim for expected O(log n) updates and O(log n * log U) queries. Explain data structure, balance, and duplicates handling?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Airbnb","Google","PayPal"]},{"id":"q-749","question":"Design a dynamic weighted string multiset with insertWord(word, w), eraseWord(word, w) (duplicates allowed), sumByPrefix(prefix) returning total weight of words starting with prefix, and kthLargestWeightInPrefix(prefix, k) returning the k-th largest weight among those words. Outline the data structure, invariants, and expected complexities; discuss handling of long words and memory trade-offs?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Google","IBM"]},{"id":"q-761","question":"Design a dynamic word-frequency histogram for a text stream. Implement addWord(word) to increment frequency, eraseWord(word) to decrement (removing word when count hits zero), getFrequency(word), and topKWords(n) returning the n most frequent distinct words (ties broken lexicographically). Target average O(log m) per update and O(k log m) for topK, where m is the number of distinct words. Explain approach and data structures you would use?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Adobe","Scale Ai"]},{"id":"q-774","question":"Design a data structure to manage a dynamic multiset of weighted intervals on the real line. Each interval is [l, r] with weight w; duplicates allowed. Implement insertInterval(l, r, w), eraseInterval(l, r, w) (one occurrence), rangeSumAt(x) returning the total weight of all intervals covering point x, and kthLargestWeightAt(x, k) returning the k-th largest weight among intervals covering x. Target average O(log n) update and O(log n) query; discuss coordinate compression, memory trade-offs, and handling duplicates?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Hashicorp","Slack"]},{"id":"q-782","question":"Design a fully dynamic data structure to maintain a set of linear cost functions y = m_i x + b_i. Support: addLine(id, m, b), removeLine(id) (one occurrence per id), queryMin(x) returning the minimum cost at x across active lines, and queryKthMin(x, k) returning the k-th smallest cost at x. Domain x in [Xmin, Xmax]. Aim for near O(log X) per operation; discuss how deletions are handled, precision, and memory trade-offs?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Apple","DoorDash","Tesla"]},{"id":"q-790","question":"Design a persistent 2D dynamic weighted point data structure that supports insertPoint(x,y,w) and erasePoint(x,y,w) (duplicates allowed). Extend with rangeSum2D(x1,x2,y1,y2,version) and kthLargestInRectangle(x1,x2,y1,y2,k,version). Each insertion creates a new version; queries run in O(log^2 n) time. Explain coordinate compression, memory management, and how duplicates are handled across versions?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Amazon","Databricks","Google"]},{"id":"q-801","question":"Design a fixed-window streaming data structure: push(x) appends an integer to the window; if the window exceeds size W, remove the oldest value. Implement getKthSmallest(k) to return the k-th smallest value in the current window in O(log W). Propose a concrete structure (e.g., an order-statistics tree with duplicates) and outline push/evict/getKthSmallest, including how duplicates and memory are handled?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Cloudflare","Plaid","Twitter"]},{"id":"q-805","question":"Design a dynamic forest data structure supporting: addNode(id, value), link(childId, parentId) to attach a child under a parent, cut(childId) to detach a subtree, update(id, delta) to adjust a node's value, pathQuery(u, v) for the sum of values along the path from u to v, and subtreeQuery(u) for the sum of values in the subtree rooted at u. Target amortized O(log n) per operation. Which approach would you pick (Link-Cut Tree vs Euler Tour Tree), and how would you handle edge cases like root changes and duplicate node values?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Lyft","Meta","Netflix"]},{"id":"q-815","question":"Design a dynamic data structure for a bipartite graph with left indices [1..N] and right indices [1..M]. Each edge (u,v) has weight w; duplicates allowed. Support: - addEdge(u,v,w), eraseEdge(u,v,w) (one occurrence) - rangeSum(uL,uR,vL,vR) total weight of edges with u in [uL,uR] and v in [vL,vR] - kthLargestEdgeWeight(uL,uR,vL,vR,k) the k-th largest edge weight in that submatrix. Aim for ~O(log N log M) per operation; discuss compression, duplicates, and memory trade-offs?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Apple","NVIDIA","Snowflake"]},{"id":"q-820","question":"Design a dynamic sequence structure 'RopeArray' storing an integer array supporting insertAt(i, val), eraseAt(i), rangeSum(l, r), rangeMax(l, r), and getAt(i). How would you implement it to achieve average O(log n) per operation using a balanced tree with implicit keys and augmented fields (size, sum, max), and how would you test with an example sequence?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Amazon","Google","Microsoft"]},{"id":"q-832","question":"Design a data structure to maintain a dynamic multiset of words with three operations: insertWord(word), eraseWord(word) (one occurrence), and getAnagrams(word) that returns all current words that are anagrams of the given word (including duplicates). Explain how you would store the words, how to compute the canonical signature, and the expected time complexity for updates and queries. For example, after insertWord('listen') and insertWord('silent'), getAnagrams('tinsel') should return ['listen','silent']?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["NVIDIA","Snowflake","Twitter"]},{"id":"q-1160","question":"How would you implement and maintain an indexing strategy for a database that experiences frequent schema changes and evolving query patterns, while ensuring minimal performance degradation during migrations?","channel":"database","subChannel":"database","difficulty":"intermediate","tags":["database-indexing","schema-migration","performance-optimization","index-lifecycle"],"companies":[]},{"id":"q-1161","question":"How would you implement and maintain an indexing strategy for a database that experiences frequent schema changes and evolving query patterns, ensuring optimal performance without requiring constant manual intervention?","channel":"database","subChannel":"database","difficulty":"intermediate","tags":["adaptive-indexing","database-automation","performance-monitoring","schema-evolution"],"companies":[]},{"id":"q-1162","question":"How would you implement and maintain an indexing strategy for a database that experiences frequent schema changes and evolving query patterns, ensuring optimal performance without requiring constant manual intervention?","channel":"database","subChannel":"database","difficulty":"intermediate","tags":["adaptive-indexing","schema-evolution","automated-maintenance","query-optimization"],"companies":[]},{"id":"q-3651","question":"How would you design an indexing strategy for a multi-tenant SaaS application where each tenant has different query patterns and data access requirements, while ensuring data isolation and preventing index bloat?","channel":"database","subChannel":"database","difficulty":"intermediate","tags":["multi-tenancy","index partitioning","partial indexes","tenant isolation","index monitoring"],"companies":[]},{"id":"q-3653","question":"How would you design and implement a multi-level indexing strategy for a hybrid OLTP/OLAP database that needs to support both high-frequency transactional writes and complex analytical queries, while minimizing index maintenance overhead?","channel":"database","subChannel":"database","difficulty":"intermediate","tags":["hybrid-database","oltp-olap","multi-level-indexing","index-maintenance","performance-optimization"],"companies":[]},{"id":"q-643","question":"How would you design an indexing strategy for a time-series database that handles both recent data queries and long-term historical analysis, considering the trade-offs between write performance and query efficiency?","channel":"database","subChannel":"database","difficulty":"intermediate","tags":["time-series","index-optimization","partitioning","performance-tuning"],"companies":[]},{"id":"q-651","question":"How would you design an indexing strategy for a table with 10 million rows that has frequent read queries with multiple WHERE conditions, occasional bulk inserts, and needs to support both exact match and range queries on different columns?","channel":"database","subChannel":"database","difficulty":"intermediate","tags":["database-indexing","query-optimization","performance-tuning","composite-indexes","bulk-operations"],"companies":[]},{"id":"q-765","question":"How would you design an indexing strategy for a multi-tenant SaaS application where each tenant has isolated data but queries frequently need to aggregate across tenants for reporting, while ensuring query performance doesn't degrade as the number of tenants scales to thousands?","channel":"database","subChannel":"database","difficulty":"intermediate","tags":["multi-tenancy","index design","performance optimization","scalability"],"companies":[]},{"id":"q-630","question":"Explain the difference between a B-tree index and a hash index, and when would you choose one over the other?","channel":"database","subChannel":"index-types","difficulty":"intermediate","tags":["database-indexing","b-tree","hash-index","query-optimization"],"companies":["Google","Amazon","Microsoft","Meta","Netflix"]},{"id":"da-125","question":"Explain database indexing and when should you use it?","channel":"database","subChannel":"indexing","difficulty":"intermediate","tags":["sql","indexing"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"db-1","question":"Explain the differences between Clustered and Non-Clustered Indexes, including their performance implications, storage characteristics, and when to choose each type in database design?","channel":"database","subChannel":"indexing","difficulty":"beginner","tags":["sql","indexing","perf"],"companies":null},{"id":"q-170","question":"When would you choose a composite index over multiple single-column indexes in a relational database?","channel":"database","subChannel":"indexing","difficulty":"intermediate","tags":["index","optimization"],"companies":["Amazon","Goldman Sachs","Google","Meta","Microsoft"]},{"id":"q-288","question":"What is the main difference between B-tree and hash index in terms of range query performance?","channel":"database","subChannel":"indexing","difficulty":"beginner","tags":["btree","hash-index","composite"],"companies":["Amazon","Google","Meta"]},{"id":"q-365","question":"You're designing a real-time analytics system for Discord that processes millions of message events per minute. Your PostgreSQL database is experiencing severe write contention on the message_events table. How would you design a partitioning strategy using declarative partitioning, and what specific index optimizations would you implement to handle both time-series queries and user-based lookups efficiently?","channel":"database","subChannel":"indexing","difficulty":"advanced","tags":["joins","indexes","normalization","postgres"],"companies":["Amazon","Discord","Google","Netflix","Palantir","Stripe","Uber"]},{"id":"q-409","question":"You're designing a database for an e-commerce platform with frequent queries on (user_id, order_date) and (product_id, category). How would you choose between B-tree and hash indexes, and what composite index strategy would optimize both query patterns?","channel":"database","subChannel":"indexing","difficulty":"intermediate","tags":["btree","hash-index","composite"],"companies":["Gitlab","Tcs","Tempus"]},{"id":"q-420","question":"You're designing a user database for a chat application with 10M users. When would you choose a B-tree index over a hash index for the 'email' column, and what are the performance implications for login queries, user search, and profile updates?","channel":"database","subChannel":"indexing","difficulty":"beginner","tags":["btree","hash-index","composite"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-489","question":"You're designing a database for LinkedIn's feed system. Posts can be queried by user_id, created_at, and engagement_score. How would you optimize the indexing strategy for high-throughput reads and writes?","channel":"database","subChannel":"indexing","difficulty":"advanced","tags":["btree","hash-index","composite"],"companies":["Anthropic","LinkedIn","NVIDIA"]},{"id":"q-572","question":"You're designing a database for a high-frequency trading system. When would you choose a B-tree index over a hash index for composite queries on (symbol, timestamp, price)? What are the specific performance implications?","channel":"database","subChannel":"indexing","difficulty":"advanced","tags":["btree","hash-index","composite"],"companies":["IBM","NVIDIA"]},{"id":"q-599","question":"When would you choose a composite index over multiple single-column indexes, and what are the trade-offs?","channel":"database","subChannel":"indexing-strategies","difficulty":"intermediate","tags":["indexing","performance","query-optimization","database-design"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-618","question":"Explain the difference between clustered and non-clustered indexes and when you would choose each type. Provide a specific example scenario.","channel":"database","subChannel":"indexing-strategies","difficulty":"intermediate","tags":["database","indexing","performance","sql"],"companies":["Google","Microsoft","Amazon","Meta","Apple"]},{"id":"da-129","question":"What is the main difference between SQL and NoSQL databases in terms of data structure?","channel":"database","subChannel":"nosql","difficulty":"beginner","tags":["nosql","mongodb"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-242","question":"How do MongoDB's document structure and SQL's table rows differ in handling user data with varying attributes, and what are the performance implications for common user operations?","channel":"database","subChannel":"nosql","difficulty":"beginner","tags":["mongodb","dynamodb","cassandra","redis"],"companies":null},{"id":"q-331","question":"You're designing a multi-region e-commerce platform using DynamoDB. Your product catalog needs to support 10M items with eventual consistency across regions, but you must handle hot partitioning during flash sales. How would you design your partition key strategy and what trade-offs would you make between read performance and write throughput?","channel":"database","subChannel":"nosql","difficulty":"advanced","tags":["mongodb","dynamodb","cassandra","redis"],"companies":["Hashicorp","Meta","Oracle"]},{"id":"q-3885","question":"You’re building a product reviews platform using MongoDB as the primary store, Redis for live leaderboards and caching, and DynamoDB for archival, deployed across two regions. In a high-throughput environment (10k writes/sec) with heavy reads, describe data modeling, shard key design, indexing, and lifecycle (TTL/archival) across the systems. Include cross-region consistency and cache invalidation strategies?","channel":"database","subChannel":"nosql","difficulty":"intermediate","tags":["mongodb","dynamodb","cassandra","redis"],"companies":["Instacart","Twitter"]},{"id":"q-4536","question":"Design a cross-datastore pipeline for a global mobile app: Redis for real-time caches, MongoDB for user profiles, Cassandra for event time-series, and DynamoDB for archival. With two-region writes at ~50k events/sec, specify concrete shard/partition keys, indexing and TTL strategies for each store, cache invalidation and idempotent-write handling, and how you would support last-24h analytics with low latency?","channel":"database","subChannel":"nosql","difficulty":"intermediate","tags":["mongodb","dynamodb","cassandra","redis"],"companies":["Apple","Databricks","Snap"]},{"id":"q-268","question":"How would you optimize a time-series analytics query that scans 100M+ rows across multiple date partitions in PostgreSQL when the WHERE clause cannot be pruned effectively due to complex temporal conditions?","channel":"database","subChannel":"query-optimization","difficulty":"advanced","tags":["explain","query-plan","partitioning"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-303","question":"How would you optimize a slow PostgreSQL query that joins 5 tables with millions of rows?","channel":"database","subChannel":"query-optimization","difficulty":"advanced","tags":["joins","indexes","normalization","postgres"],"companies":["Amazon","Google","Meta"]},{"id":"q-343","question":"You have a PostgreSQL table with 100M rows partitioned by date. A query filtering on a specific date range is still slow. What would you check in the EXPLAIN plan and how would you optimize it?","channel":"database","subChannel":"query-optimization","difficulty":"intermediate","tags":["explain","query-plan","partitioning"],"companies":["Affirm","Amazon","Google","Jane Street","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-380","question":"You're optimizing a query that's slow due to a large time-series table. The query filters by timestamp range and device_id. How would you analyze the query plan and what partitioning strategy would you recommend?","channel":"database","subChannel":"query-optimization","difficulty":"intermediate","tags":["explain","query-plan","partitioning"],"companies":["Hrt","Stripe","Tesla"]},{"id":"q-436","question":"You have a 100M row orders table with slow queries. The query plan shows sequential scans despite indexes on customer_id and order_date. How would you diagnose and fix this performance issue?","channel":"database","subChannel":"query-optimization","difficulty":"advanced","tags":["explain","query-plan","partitioning"],"companies":["Bloomberg","Salesforce","Slack"]},{"id":"q-4409","question":"Given a table 'events' with 10B rows (event_time, user_id, event_type, payload) and a small 'users' table, the common query filters a date range and a user_id set and aggregates by event_type. Design a composite partitioning strategy to enable partition pruning and explain how you would validate with EXPLAIN ANALYZE and handle potential maintenance costs?","channel":"database","subChannel":"query-optimization","difficulty":"advanced","tags":["explain","query-plan","partitioning"],"companies":["Anthropic","Databricks","Plaid"]},{"id":"q-4587","question":"In a PostgreSQL cluster storing 3TB of event data, partitioned by month (range partitions) and with a hot path in memory, a common query filters on event_type and a 30-day window, orders by event_value DESC, and LIMIT 100. How would you design the partitioning and indexing to ensure partition pruning and fast top-N results? Describe the layout, index types (BRIN, partial, INCLUDE), plan expectations via EXPLAIN ANALYZE, and how you would adjust if event_type is highly selective vs broad?","channel":"database","subChannel":"query-optimization","difficulty":"advanced","tags":["explain","query-plan","partitioning"],"companies":["Lyft","Netflix","Twitter"]},{"id":"q-546","question":"You're analyzing a slow query on a partitioned table. The EXPLAIN plan shows a full table scan instead of partition pruning. What could cause this and how would you fix it?","channel":"database","subChannel":"query-optimization","difficulty":"intermediate","tags":["explain","query-plan","partitioning"],"companies":["Microsoft","Tesla"]},{"id":"da-156","question":"What are the key differences between DELETE and TRUNCATE commands in SQL, including their impact on identity columns, foreign key constraints, and performance characteristics?","channel":"database","subChannel":"sql","difficulty":"beginner","tags":["sql","indexing"],"companies":["Amazon","Google","Microsoft","Netflix","Oracle","Snowflake"]},{"id":"q-172","question":"Design a database failover strategy for a high-traffic e-commerce platform using primary-replica PostgreSQL. How would you ensure zero-downtime failover while maintaining data consistency during peak traffic of 10,000 requests/second?","channel":"database","subChannel":"sql","difficulty":"intermediate","tags":["chaos","resilience"],"companies":["Amazon","Bloomberg","Microsoft","Netflix","Uber"]},{"id":"q-4333","question":"Given a PostgreSQL schema with tables: customers(id), orders(id, customer_id, product_id, placed_at, status, amount), products(id, category_id), categories(id, name). For every customer, return the latest completed order per product category. How would you write the query, what indexes (including partial or expression indices) would you create, and how would you verify the plan actually uses them under a 1TB data set?","channel":"database","subChannel":"sql","difficulty":"intermediate","tags":["joins","indexes","normalization","postgres"],"companies":["Discord","Goldman Sachs"]},{"id":"q-4360","question":"In PostgreSQL, you have tables: customers(id PK, name), orders(id PK, customer_id FK, placed_at), order_items(id PK, order_id FK, product_id FK, quantity, unit_price). Write a single query to return each customer's name, total_spent (sum of quantity*unit_price across their orders) and last_order_date for customers who placed at least one order in the last 60 days. Then discuss index choices to support this query and why?","channel":"database","subChannel":"sql","difficulty":"beginner","tags":["joins","indexes","normalization","postgres"],"companies":["Instacart","Meta"]},{"id":"q-458","question":"You have a PostgreSQL database with orders (10M rows) and customers (1M rows). A query joining these tables is slow. How would you optimize it?","channel":"database","subChannel":"sql","difficulty":"intermediate","tags":["joins","indexes","normalization","postgres"],"companies":["Apple","Tesla","Uber"]},{"id":"q-4813","question":"You have a PostgreSQL schema with tables: users(id, name, created_at); orders(id, user_id, created_at, status); order_items(id, order_id, product_id, quantity, price); products(id, category, name, price). For a specific user_id and date range, return each product category with total_spent (sum of quantity*price) and order_count. Outline normalization, needed indexes, and provide the SQL to compute the result?","channel":"database","subChannel":"sql","difficulty":"beginner","tags":["joins","indexes","normalization","postgres"],"companies":["Airbnb","Hashicorp","PayPal"]},{"id":"da-128","question":"You have a banking system where users can transfer money between accounts. Design a transaction to handle a transfer of $500 from Account A (balance: $1000) to Account B (balance: $200). What happens if the system crashes after debiting Account A but before crediting Account B? How would you ensure data consistency?","channel":"database","subChannel":"transactions","difficulty":"intermediate","tags":["acid","transactions"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Stripe"]},{"id":"da-134","question":"You have a banking system where Account A transfers $100 to Account B, but during the transaction, Account B gets deleted by another process. The transfer uses READ COMMITTED isolation. What happens to the $100, and how would you prevent data inconsistency?","channel":"database","subChannel":"transactions","difficulty":"advanced","tags":["acid","transactions"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Stripe"]},{"id":"da-170","question":"You're building a banking system where users can transfer money between accounts. How would you design the transaction handling to ensure no money is lost or created during transfers, especially when the system crashes mid-transfer?","channel":"database","subChannel":"transactions","difficulty":"intermediate","tags":["acid","transactions"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Stripe"]},{"id":"da-172","question":"In a distributed database system, how would you implement a two-phase commit protocol to ensure atomicity across multiple nodes, and what are the key failure scenarios and recovery mechanisms you must handle?","channel":"database","subChannel":"transactions","difficulty":"advanced","tags":["acid","transactions"],"companies":null},{"id":"db-2","question":"How do ACID properties ensure data integrity in a banking transaction where $100 is transferred from Account A to Account B?","channel":"database","subChannel":"transactions","difficulty":"intermediate","tags":["acid","transactions","theory"],"companies":["Amazon","Goldman Sachs","Google","PayPal","Stripe"]},{"id":"q-190","question":"What is the difference between READ COMMITTED and REPEATABLE READ isolation levels in database transactions, and how does MVCC implementation affect their behavior?","channel":"database","subChannel":"transactions","difficulty":"beginner","tags":["acid","isolation-levels","mvcc"],"companies":["Amazon","Databricks","Google","Microsoft","Oracle","Snowflake"]},{"id":"q-317","question":"Explain how MVCC (Multi-Version Concurrency Control) works and how it prevents lost updates in a database system?","channel":"database","subChannel":"transactions","difficulty":"intermediate","tags":["acid","isolation-levels","mvcc"],"companies":["Microsoft","Plaid","Warner Bros"]},{"id":"q-353","question":"You're building a collaborative design tool where multiple users can edit the same document simultaneously. How would you use database transactions and isolation levels to prevent conflicts while maintaining good performance?","channel":"database","subChannel":"transactions","difficulty":"beginner","tags":["acid","isolation-levels","mvcc"],"companies":["Adobe","Amazon","Canva","Epic Games","Google","Meta","Microsoft","Netflix"]},{"id":"q-397","question":"In a high-transaction payment system using PostgreSQL, how would you design a transaction isolation strategy to prevent lost updates while maintaining high concurrency for account transfers?","channel":"database","subChannel":"transactions","difficulty":"advanced","tags":["acid","isolation-levels","mvcc"],"companies":["Amazon","Google","Netflix","PayPal","Square","Stripe"]},{"id":"q-428","question":"You're building a booking system for Airbnb where multiple users can reserve the same property simultaneously. How would you design the transaction handling to prevent double bookings while maintaining high availability?","channel":"database","subChannel":"transactions","difficulty":"intermediate","tags":["acid","isolation-levels","mvcc"],"companies":["Airbnb","Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"q-519","question":"You're designing a high-frequency trading system where transactions must see consistent data snapshots. How would you implement MVCC to handle concurrent reads while preventing write skew anomalies, and what isolation level would you choose?","channel":"database","subChannel":"transactions","difficulty":"advanced","tags":["acid","isolation-levels","mvcc"],"companies":["Lyft","Snap","Tesla"]},{"id":"q-1100","question":"In a Databricks data pipeline, ingest JSON events from S3 into Delta Lake with Auto Loader. Each event has a nested user object (id, email) and an optional pages array. Some events miss user.email. How would you design the pipeline to safely flatten nested fields, handle missing values, and upsert the latest user state into a Delta Silver table using an idempotent MERGE?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["MongoDB","Oracle"]},{"id":"q-1112","question":"Scenario: In a Databricks Delta Live Tables pipeline, ingest JSON events from a Kafka source into a Bronze table, where nested fields drift over time (e.g., payload.user.locale is added later, some events miss payload.user). You then transform to a Silver table used by billing and marketing. How would you implement a robust schema-evolution strategy and gating so new fields are captured without breaking existing downstream queries, and how would you test this in a run?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["DoorDash","Microsoft","Snap"]},{"id":"q-1144","question":"In a Databricks streaming pipeline ingesting tenant-scoped events from Kafka into Delta Lake, events may arrive late by up to 10 minutes. Describe a concrete end-to-end approach to upsert the latest state per (tenant_id, user_id) into a Silver table while preserving the full event history. Include: data model, CDC logic, watermarking and late data handling, an idempotent MERGE strategy, schema evolution handling, and governance considerations with Unity Catalog RBAC and data lineage?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Airbnb","Cloudflare","Databricks"]},{"id":"q-1182","question":"In a multi-tenant Databricks lakehouse ingesting per-tenant telemetry from Kafka and S3 into a unified Silver Delta table, describe an end-to-end CDC strategy to implement SCD2-like history per tenant with idempotent MERGE, handle late data with a watermark, and enforce governance through Unity Catalog RBAC and dynamic PII masking. Include data model, CDC logic, testing plan, and how you’d validate lineage?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Discord","Google","Snowflake"]},{"id":"q-1226","question":"In a Databricks Delta Live Tables pipeline that ingests clickstream events from a cloud bucket into a Delta table, data quality checks are defined via expectations. A batch arrives with corrupted event_time values that would fail the run. Outline a concrete approach to quarantine bad data, feed downstream tables only from valid rows, and store invalids for audit, while still handling late data and deduplication?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Hashicorp","Zoom"]},{"id":"q-1264","question":"In a Databricks job ingesting daily Parquet files from S3 into Delta Lake, a new field 'campaign_id' may appear in newer files while older ones do not. Describe a concrete, beginner-friendly approach to ingest without data loss, allowing downstream joins, and handling the schema change gracefully. Include how to enable Delta schema evolution (mergeSchema/autoMerge), define a compatible target schema, and implement a minimal validation to drop rows missing essential fields?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Anthropic","Coinbase","Uber"]},{"id":"q-1320","question":"You're designing a Delta Live Tables workflow ingesting data from Kafka and an S3 landing zone, with a downstream customer dimension in Delta Lake that uses SCD Type 2. How would you implement idempotent MERGE-based upserts, handle schema drift, and preserve late-arriving data while auditing invalid events and ensuring downstream BI reads only current rows?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["NVIDIA","Robinhood","Two Sigma"]},{"id":"q-1375","question":"In a Databricks streaming job, a Kafka topic emits JSON events for many tenants. The payload schema drifts with new fields; you want a stable Silver Delta table with a canonical schema and history. Describe a beginner-friendly approach to map events to the canonical schema, handle new fields without breaking downstream joins, and perform an idempotent MERGE into Silver by (tenant_id, event_id). Include a concrete mapping rule set and a small MERGE example?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["MongoDB","Oracle","Robinhood"]},{"id":"q-1409","question":"Describe an end-to-end approach for a fraud-detection streaming pipeline using two Kafka topics (transactions, account_updates): Bronze ingest, join to a versioned SCD2 customer_dim, compute risk in Silver, upsert via MERGE with a deterministic key, watermark late data, handle schema drift with Delta Lake evolution, and enforce governance with Unity Catalog RBAC and lineage. Include testing with synthetic late-arriving data?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Hugging Face","Scale Ai","Snowflake"]},{"id":"q-1451","question":"In a Databricks notebook, you need to join a 10M-row Delta Lake 'customers' table with a 10k-row 'segments' reference table to produce a daily marketing audience feed. How would you implement an efficient join strategy in Spark/Delta to minimize shuffle and cost, ensure correctness if segments update, and maintain lineage? Include: join type and hints, caching strategy, refresh cadence, and where to store results?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Databricks","Google","Two Sigma"]},{"id":"q-1540","question":"In a Databricks streaming pipeline ingesting order events from Kafka into Delta Lake, implement a scalable SCD Type 2 for a customer_dim table to preserve history while handling late-arriving updates up to 15 minutes. Describe the data model (bronze/silver), CDC logic using MERGE, watermarking, and schema evolution, plus Unity Catalog RBAC and lineage considerations. Include a minimal code sketch of the MERGE closing an old row and inserting a new version?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Cloudflare","Hugging Face"]},{"id":"q-1555","question":"In a Databricks pipeline ingesting 20 TB/day of Parquet logs on S3 into Delta Lake, design a practical optimization plan to improve read latency for near-real-time dashboards using Delta features like OPTIMIZE, ZORDER, Data Skipping, caching, and Photon. Discuss partitioning strategy, trade-offs, and how you'd validate gains with benchmark metrics?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Databricks","IBM","Oracle"]},{"id":"q-1643","question":"In Databricks, ingest streaming data from Kafka into Delta Lake for 10k IoT devices emitting multiple sensor types (temperature, humidity, pressure). Build a Silver table with the latest per-device per-sensor-type state while preserving full history. The source schema will evolve (new sensors added, some removed). Outline a robust end-to-end approach: data model, CDC/Upsert logic, watermarking for late data, schema evolution strategy, idempotent MERGE, and governance with Unity Catalog RBAC and lineage. Include concrete examples of Bronze->Silver handoff and a dynamic per-tenant view?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Lyft","OpenAI"]},{"id":"q-1658","question":"Design a secure external data sharing workflow in a Databricks environment using Unity Catalog and Delta Sharing to expose aggregated metrics derived from production Delta tables to external partners while maintaining tenant isolation and governance. Include data model, masking strategy, per partner quotas, refresh cadence, and how to monitor and revoke access?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Amazon","Oracle","Twitter"]},{"id":"q-1683","question":"Design a multi-tenant, Databricks-based data pipeline ingesting 1 TB/day of JSON events from Kafka into Delta Lake. Tenants share storage but must be completely isolated; dashboards must mask PII fields per-tenant. Propose an end-to-end pattern using Unity Catalog RBAC, dynamic data masking, Delta Live Tables, and Photon-enabled reads. Include data model, masking rules, handling schema evolution, and how you validate governance and lineage?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Lyft","Netflix","Square"]},{"id":"q-1721","question":"Databricks beginner scenario: A streaming pipeline reads 2 TB/month of JSON events from S3 via Autoloader into Delta Lake. A downstream dashboard shows active_users by hour. Late events arrive up to 10 minutes. Design a practical plan to maintain accurate hourly active_user counts with minimal duplication, including: schema/partitioning for streaming, watermark-based late data handling, an idempotent MERGE into a Silver table, and a simple end-to-end test using synthetic late events. Also outline monitoring steps?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Adobe","Instacart","Plaid"]},{"id":"q-1844","question":"In a Databricks streaming pipeline ingesting events from Kafka into Delta Lake for a 50+ TB/day workload, design a CDC-based upsert to a Silver table with per-tenant sharding, late data handling, and schema evolution. Describe the data model, idempotent MERGE keys, watermark latency model, and Unity Catalog RBAC considerations; propose validation metrics and a minimal reproducible code skeleton?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Databricks","Instacart","MongoDB"]},{"id":"q-1853","question":"In a Databricks job ingesting 50 GB/day of Avro logs from S3 into Delta Lake, design a beginner-friendly Delta Live Tables pipeline to produce Bronze (raw) and Silver (flattened) tables. Include how you flatten the nested field user (id and tier) into Silver, enforce a simple data quality gate (NOT NULL user_id, user_tier in {'free','standard','premium'}), and choose a partitioning strategy by file_date. Explain testing and how you'll measure improvements?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Meta","Robinhood","Stripe"]},{"id":"q-1944","question":"Design a Delta Live Tables pipeline that streams per-tenant user activity from Kafka into Bronze, then updates a per-tenant Silver table implementing SCD Type 2 on a tenant-scoped customer_dim, using a deterministic MERGE for upserts. Tenants can emit out-of-order data and schema drift occurs. Propose concrete config for watermarking, per-tenant constraints, and schema evolution, plus RBAC in Unity Catalog. Include synthetic late-data tests and metrics to validate latency and correctness?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Bloomberg","Citadel","Meta"]},{"id":"q-1990","question":"Beginner: Adobe wants a minimal, end-to-end Databricks pipeline to ingest daily JSON event logs from S3 into Delta Lake. Design the workflow using Auto Loader for a Bronze table, then flatten to a date-partitioned Silver table. Include simple schema-evolution handling, a lightweight data-quality check (required fields, duplicates), and a basic unit test that validates the Silver schema and daily row count?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Adobe","Uber"]},{"id":"q-2033","question":"In a Databricks workflow, ingest 50 GB/day of JSON web logs from S3 into a Bronze Delta table. Propose a beginner-friendly pipeline to populate a Silver table that upserts the latest event per session_id, handles 2-minute late data with a watermark, and validates data quality before write. Include partitioning strategy, a MERGE-based CDC, and Unity Catalog RBAC considerations?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Amazon","Bloomberg","Google"]},{"id":"q-2070","question":"Design an end-to-end Databricks pipeline to ingest a daily JSON feed from an on-prem FTP drop into Delta Lake. Use Auto Loader with schema evolution for changing fields (user_id, event_type, event_ts, ip_address, device, etc.). Describe Bronze to Silver upserts via MERGE, data quality tests, IP masking, and Unity Catalog RBAC with lineage. Be concrete about steps and trade-offs?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["MongoDB","Snowflake"]},{"id":"q-2199","question":"How would you implement a compliant, zero-duplication deletion workflow in a Databricks streaming pipeline that ingests user events from Kafka into Delta Lake? A deletions log triggers row-level removals across Bronze/Silver, handling late data with tombstones, ensuring idempotent MERGE, and preserving an auditable deletion history via a separate Delta table. Describe architecture, data flow, and validation?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Adobe","Databricks","Goldman Sachs"]},{"id":"q-2279","question":"In a Databricks job ingesting 10 GB/day of user event JSON from S3 into Bronze Delta Lake, design a beginner-friendly Silver pipeline that upserts latest user state by user_id using MERGE, enforces Delta constraints (NOT NULL user_id, age > 0), and fails the pipeline on any quality violation. How would you implement and test this end-to-end?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Netflix","Salesforce","Twitter"]},{"id":"q-2397","question":"In a Databricks data pipeline, ingest telemetry from two sources: S3 JSONs (Auto Loader) and a Kafka topic, with evolving schema (device_id, ts, metrics...). Build Bronze, Silver (dedupe by event_id using MERGE CDC), Gold. Implement 2-minute late data watermark, PII masking, and Unity Catalog RBAC with lineage across Bronze/Silver/Gold. Provide concrete steps, partitioning, and a lightweight test plan?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Anthropic","Goldman Sachs","MongoDB"]},{"id":"q-2435","question":"Design a Databricks pipeline ingesting telemetry from S3 (Auto Loader) and a Kafka topic with evolving schema. Build Bronze/Silver/Gold with CDC dedupe by event_id; add a drift-detection layer that scores per-field distribution changes and quarantines anomalous records. Mask PII before Silver/Gold, enforce Unity Catalog RBAC with lineage. Provide concrete steps, partitioning, and a lightweight test plan?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Adobe","Microsoft","Tesla"]},{"id":"q-2577","question":"In a Databricks Delta Live Tables (DLT) pipeline, ingest telemetry from two sources: S3 Auto Loader JSON with evolving schema and a Kafka stream. Implement multi-source dedup by a composite key (device_id, event_id), support up to 5-minute late data with a watermark, mask PII fields, and publish lineage via Unity Catalog RBAC across Bronze Silver Gold. Provide concrete steps, test plan, and rollback path?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Citadel","NVIDIA","OpenAI"]},{"id":"q-2662","question":"In a Databricks Delta Lake pipeline ingesting telemetry from two sources (S3 JSONs via Auto Loader with evolving schema and a streaming Kafka feed), design a GDPR-style purge workflow to delete or mask PII across Bronze, Silver, and Gold while preserving aggregates and lineage. Describe concrete steps using MERGE/DELETE, specify retention, audit logging, and how you test downstream dashboards and time-travel reads. Include RBAC considerations?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Databricks","Snap"]},{"id":"q-2743","question":"Design a multi-tenant Databricks pipeline for a SaaS IoT product. Ingest streaming telemetry from an MQTT bridge and a batch Postgres metadata feed. Build Bronze (raw), Silver (device_id normalization, unit standardization, PII masking), Gold (per-tenant aggregates). Implement schema evolution, a 2-minute late-data watermark, RBAC via Unity Catalog with per-tenant lineage, and a lightweight QA suite. Provide concrete steps, partitioning, and validation plan?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Anthropic","Databricks","Discord"]},{"id":"q-2794","question":"In a Databricks environment, you receive a daily JSON feed of fintech transactions from a partner S3 bucket. Build a three-table pipeline: Bronze Delta with Auto Loader and schema evolution, Silver Delta that upserts deduplicating by transaction_id, and Gold that aggregates daily totals by customer_id. The JSON evolves (new fields like merchant_id and currency). Explain how you'd implement schema evolution, a MERGE based upsert, and partitioning by transaction_date. Include a lightweight data quality test plan and PII masking in outputs?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["IBM","Netflix","Robinhood"]},{"id":"q-2987","question":"Design a Databricks data pipeline that ingests telemetry from two sources: a streaming Kafka topic and a batch REST API, both with evolving schemas. Build Bronze (raw), Silver (mask PII and implement a 2-minute watermark), and Gold (Customer SCD2) layers, with CDC merges. Enforce Unity Catalog RBAC with lineage across layers, and outline concrete partitioning, test plans, and recovery strategies?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Adobe","Google"]},{"id":"q-3011","question":"Design a cross-tenant Databricks Delta Live Tables pipeline that shares Gold metrics with external partners via Delta Sharing, while enforcing per-tenant isolation with Unity Catalog. Ingest telemetry from S3 JSON with evolving schema and a high-throughput Kafka topic. Build Bronze (raw), Silver (dedupe by event_id with a 2-minute watermark), and Gold (per-tenant aggregates). Include masking for PII, partition strategy, tests, and RBAC plan?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Lyft","Oracle"]},{"id":"q-3115","question":"Design a Databricks Delta Live Tables pipeline for a drone delivery fleet. Ingest two streams: a) S3-stored JSON drone events (Bronze) with evolving fields (drone_id, ts, lat, lon, battery, sensor), b) PostgreSQL CDC drone metadata (model, region, maint_status). Build Bronze, Silver, Gold with a MERGE CDC to upsert Silver and support schema evolution, and a Gold layer with enriched metrics. Implement a 2-minute late-data watermark, mask operator_id in Bronze, and enforce Unity Catalog RBAC with lineage across layers. Include partitioning strategy, testing plan, and trade-offs?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Bloomberg","MongoDB"]},{"id":"q-3155","question":"Design a production ready Databricks data pipeline for telemetry arriving from S3 Auto Loader and a Kafka topic with an evolving schema. Build Bronze Silver Gold with Silver deduplicating on event_id using MERGE CDC, a 2 minute watermark for late data, and PII masking. Enforce Unity Catalog RBAC with lineage. Propose a Great Expectations CI/CD test harness with synthetic data and a rollback strategy?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["IBM","LinkedIn","MongoDB"]},{"id":"q-3250","question":"Design a Databricks streaming pipeline ingesting S3 Auto Loader JSONs and a Kafka topic with evolving schema. Build Bronze/Silver/Gold with event_id dedupe via MERGE CDC and a 2-minute late data watermark. Add an observability layer: inline data drift detection for numeric fields and schema drift alerts with Slack alerts; enforce Unity Catalog RBAC with lineage. Provide concrete steps and a minimal test plan?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Cloudflare","Robinhood","Snowflake"]},{"id":"q-3325","question":"In a Databricks data-engineer exercise, ingest daily Parquet logs of customer events from S3 into Delta Lake; events originate from several microservices with event_time in different time zones. Describe a beginner-friendly pipeline that uses Auto Loader, Bronze/Silver with a simple MERGE to upsert the latest per customer_id, standardizes timestamps to UTC, and validates with a lightweight data quality check (not null, reasonable ranges). Include a partitioning plan and a quick validation approach?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["MongoDB","Netflix"]},{"id":"q-3468","question":"In a multi-tenant Databricks data platform, design a Delta Live Tables pipeline that ingests telemetry from two sources: S3 Auto Loader JSON drops and a Kafka topic, with per-tenant isolation. Build Bronze, Silver, and Gold layers; deduplicate in Silver by event_id using MERGE CDC; apply per-tenant data masking for PII; enforce Unity Catalog RBAC with lineage; handle per-tenant schema evolution and provide a rollback path via Delta Time Travel. Include synthetic data testing plan?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Apple","Bloomberg","DoorDash"]},{"id":"q-3604","question":"In a Databricks pipeline ingesting 10 GB/day of Parquet user events from S3 into a Bronze Delta table via Auto Loader, design a beginner-friendly Silver layer that deduplicates by event_id, handles 2-minute late data with a watermark, and masks PII (email) at query time using a simple hashing or masking approach. Outline concrete steps including partitioning, a MERGE-based CDC, Unity Catalog RBAC, and a lightweight test plan. How would you implement the masking view?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Discord","Meta","Oracle"]},{"id":"q-3667","question":"Design a Databricks data pipeline that ingests 5 TB/day of semi-structured JSON events from S3 into a Bronze Delta table, then propagates changes to Silver and Gold marts using Delta Change Data Feed (CDF) without re-reading full history. Implement 2-minute late data handling, PII masking at query time, Unity Catalog RBAC, and referential integrity across marts. Outline CDC strategy, partitioning, and a practical test plan?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Goldman Sachs","Hashicorp"]},{"id":"q-3854","question":"In a Databricks data platform for a retail chain, ingest 3 sources: S3 Parquet orders, S3 JSON customers, and Kafka click events, into Bronze, Silver, and Gold using Delta Live Tables. Build a centralized data-contract registry with versioned schemas and field-level expectations, enforce it via contract-aware backfills: bump a contract version, backfill Bronze, then cascade to Silver/Gold. Outline promotion flow, compatibility rules, testing plan, and Unity Catalog RBAC considerations?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["DoorDash","Snowflake"]},{"id":"q-3914","question":"In a beginner Databricks pipeline, ingest 5 GB/day JSON clickstream from S3 into Bronze via Auto Loader; flatten nested payloads into Silver, deduplicate by event_id with MERGE CDC, and apply a 2-minute watermark. Build Gold as daily active users by country. Partition by date; enforce Unity Catalog RBAC. Provide a lightweight test plan and RBAC considerations?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Adobe","Instacart","Microsoft"]},{"id":"q-3991","question":"Design a Databricks ingestion path for streaming clickstream: Bronze from S3 JSON (Auto Loader) and Kafka; Silver computes per-user features (last_seen, total_events, avg_session_len) with a windowed watermark; Gold registers features in the Databricks Feature Store for model training. Include schema evolution, Unity Catalog RBAC, and a CI/CD test harness with synthetic data, drift checks, and Delta time-travel backfills with rollback?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Hugging Face","Square"]},{"id":"q-4073","question":"In a Databricks environment ingest 100 GB/day of Parquet user events from S3 and 5 MB/s real-time orders from Kafka. Design Bronze-Silver-Gold pipelines and add a versioned Feature Store with source lineage and drift monitoring. Include: schema evolution strategy, Unity Catalog RBAC, and a rollback/refresh plan for features. How would you implement feature retrieval for model scoring with time-travel to feature versions?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Adobe","Lyft","Scale Ai"]},{"id":"q-4086","question":"In a Databricks project ingesting 5 GB/day of JSON event data from two APIs into Delta tables on S3, design a beginner-friendly pipeline that unifies the schemas, builds Bronze, Silver, and Gold layers, and handles 2-minute late data with a watermark. Implement simple per-field normalization, a PII masking approach at query time, and Unity Catalog RBAC restricting access to Silver and Gold. Outline concrete steps including partitioning, schema evolution, and a lightweight test plan?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Google","Plaid","Tesla"]},{"id":"q-4118","question":"Scenario: Ingest 200 GB/day of JSON transactional events from two e-commerce APIs into Delta Lake on S3. Design a Bronze→Silver→Gold pipeline with Auto Loader, handle evolving schemas (promo_code, device_type, customer_segment), implement per-field normalization, a 2-minute late-data watermark, and a query-time PII masking view. Enforce Unity Catalog RBAC restricting Silver/Gold, optimize partitioning by date and region, define a schema-evolution policy, and outline a lightweight test plan plus a rollback strategy for failed deploys?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Apple","Lyft","Oracle"]},{"id":"q-4253","question":"Design a multi-tenant Databricks pipeline to ingest 5-20 TB/day of mobile events from S3 via Auto Loader, with per-tenant isolation and evolving schemas. Build Bronze, Silver, Gold with: 2-minute watermark, dedupe by event_id using MERGE CDC, and schema evolution; per-tenant dynamic masking for email via Unity Catalog; data residency by tenant_id/region partitions; RBAC; cost optimization with Photon and caching; and a lightweight test plan. How would you implement and test this end-to-end?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Meta","Robinhood","Twitter"]},{"id":"q-4301","question":"In a Databricks data platform, ingest 50 TB/day of JSON web logs from S3 into a Bronze Delta table using Auto Loader. Build Silver with deduplication by event_id via MERGE CDC, a 24-hour watermark for late data, and a query-time IP masking view. Implement cross-region DR: replicate Bronze and Silver to a secondary region with consistent CDC, schema evolution, and mirrored Unity Catalog RBAC. Outline concrete steps, partitioning, test plan, and DR failover procedure?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["NVIDIA","Salesforce"]},{"id":"q-4330","question":"In a Databricks data lake for a fintech app, ingest 8-12 TB/day JSON events from S3 via Auto Loader and a streaming CDC feed from Postgres. Build Bronze/Silver/Gold with strict per-record RBAC via Unity Catalog, implement field-level masking for SSN at query time, and introduce a progressive data quality gate that halts Silver if latency P95 exceeds threshold. Include schema evolution, partitioning, and a lightweight test plan?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Snowflake","Twitter"]},{"id":"q-4387","question":"Scenario: Databricks Bronze→Silver→Gold pipeline ingesting 200 GB/day of product events from S3 via Auto Loader, with Silver dedupe on event_id and a 5-minute watermark. Propose a scalable observability and drift-detection layer: metrics schema (latency, completeness, duplicates, schema changes), anomaly detection approach, alerting, rollback/backfill plan, and Unity Catalog RBAC for dashboards and lineage. Include test plan?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Robinhood","Zoom"]},{"id":"q-4588","question":"In a Databricks setup, design a Databricks Feature Store pipeline: ingest Kafka real-time events and S3 batch catalog changes, build Bronze and Silver via Delta Live Tables, publish offline features to Delta tables and online features to the Databricks Feature Store, implement versioned feature definitions and drift detection, and enforce Unity Catalog RBAC. Outline the concrete steps and a test plan with synthetic data to validate online/offline parity?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Discord","NVIDIA"]},{"id":"q-4666","question":"Design a Databricks data pipeline for a multi-region fintech app that ingests 1 TB/day JSON transaction events from S3, with strongly evolving schemas and strict data contracts. Build Bronze, Silver, Gold using Auto Loader and Delta Live Tables, but focus on cross-region replication, time-travel correctness, and secure data sharing with Unity Catalog. Explain how you'd enforce row-level access, implement schema drift handling without downtime, and validate with a lightweight test harness?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Anthropic","Coinbase","Google"]},{"id":"q-4694","question":"Scenario: In a Databricks pipeline ingesting 100 GB/day of JSON clickstream from S3 plus nightly REST API enrichment, design a Silver layer that builds user_session features (counts, dwell, funnels) via windowed aggregates, uses a dynamic schema registry to handle evolving event fields (no CDC merges), and enforces Unity Catalog RBAC for multi-tenant access. Include partitioning, clustering, and a light validation/test plan?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Apple","Hugging Face","Lyft"]},{"id":"q-4798","question":"In a Databricks pipeline, you ingest 1 TB/day of Parquet logs into a Delta table via Auto Loader. The table has many small files causing frequent small-file problems, and downstream queries occasionally misreport counts due to late data and churn. Propose a safe, automated nightly compaction strategy that preserves CDC correctness, avoids breaking streaming readers, and uses ZORDER. Outline concrete steps, toggles, and a lightweight test plan?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Apple","Meta"]},{"id":"q-906","question":"Scenario: A daily Delta Live Tables (DLT) pipeline ingests clickstream JSONs into a Bronze Delta table and then enriches with a users_dim to a Silver table. Build a beginner-friendly pipeline that: deduplicates by event_id, validates user_id via users_dim, filters to business hours 08:00–18:00, enriches with user fields, and writes to Silver partitioned by event_date. Outline minimal steps and rationale?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Apple","Netflix","Scale Ai"]},{"id":"q-1125","question":"You're building an incremental dbt model analytics.daily_revenue_by_product sourced from staging.sales_raw. Data can arrive late up to 2 days. How would you implement incremental upserts, reprocess late days without disturbing older data, and validate with tests and a snapshot of product price history? Provide a minimal SQL snippet illustrating the approach?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Apple","Hashicorp"]},{"id":"q-1165","question":"You're designing a dbt analytics pipeline for a ride-sharing platform. The raw events are in `staging.events_raw` with columns like `ride_id`, `city`, `ride_type`, `currency`, `amount`, `occurred_at`, and data can arrive up to 48 hours late. Build a beginner-friendly incremental model `analytics.daily_revenue` that computes USD revenue per day by `date`, `city`, and `ride_type`. Use a daily `pricing.exchange_rates` table to convert from local currency to USD. Describe how you'd implement incremental upserts, late-data handling (2-day window), schema-drift guards, and validation with tests and a snapshot. Also outline tests for late refunds and missing rates?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Google","PayPal"]},{"id":"q-1257","question":"Design a drift-aware dbt workflow across dev/staging/prod in Snowflake. How would you detect schema drift between sources and models using snapshots, tests, and sources, enforce a drift threshold, and automatically fail CI when drift exceeds the threshold? Describe the macro, tests, and alert/rollback mechanism, plus how you version thresholds?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["Goldman Sachs","Google","Meta"]},{"id":"q-1361","question":"Design a beginner-friendly incremental dbt model in Snowflake for a SaaS analytics pipeline: raw events live at staging.event_logs with user_id, event_type (signup, login, purchase), country, occurred_at. Build analytics.daily_metrics that counts distinct active users by day and event_type, enriched by dimensions.countries. Explain incremental logic, late data handling (7 days), schema drift guards, and validation with tests and a snapshot. Also outline an Exposure for the dashboard with lineage?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Snowflake","Zoom"]},{"id":"q-1523","question":"Design an incremental dbt model analytics.daily_engagement for a global multi-tenant app. Raw events sit in staging.user_events with tenant_id, user_id, email, event_type, occurred_at, privacy_flag. Build per-tenant daily distinct-user counts by event_type, redacting emails when privacy_flag is true. Describe incremental strategy, late data window, schema-drift guards, tests and snapshots, and how to expose lineage for dashboards?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Apple","IBM","Tesla"]},{"id":"q-1560","question":"In a dbt project, you ingest raw events into `staging.web_events` with columns: `event_id`, `user_id`, `event_type`, `country`, `occurred_at` (UTC). Build a beginner-friendly incremental model **analytics.daily_active_users** that reports the count of distinct `user_id`s per day, by `country` and `event_type`. Data can arrive up to 2 days late. Describe your incremental logic, how you handle late data with a rolling window, how to guard against schema drift, and how you validate with tests. Also draft an Exposure for a dashboard showing daily active users by country and event_type, including lineage?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Bloomberg","DoorDash","Oracle"]},{"id":"q-1590","question":"Design a two-model incremental dbt flow in Snowflake: 1) analytics.first_session derives first_session_at per user and seeds analytics.users(user_id, first_session_at, country_code, cohort). 2) analytics.daily_metrics counts daily active users by day and event_type, enriched with dimensions.countries. Include late-data handling (3 days), schema-drift guards, tests (not_null, unique), a snapshot, and expose lineage to dashboards?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Amazon","Oracle"]},{"id":"q-1604","question":"In a dbt pipeline powering a fintech analytics platform, raw events arrive at staging.fin_events with customer_id, txn_id, amount, currency, status, event_ts. Design an incremental analytics.transactions model that deduplicates by txn_id across sources, handles late events within a 2-day window, validates currency via a macro-based set per source, guards against schema drift with a dynamic mapping table, and snapshots customers on their first txn for dashboard lineage. Include tests and performance considerations?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["IBM","NVIDIA"]},{"id":"q-1714","question":"In a beginner dbt project for a media site, staging.events_logs(user_id, event_type, occurred_at, region_code) feeds analytics.daily_engagement that counts distinct users per date and event_type, enriched by dimensions.regions on region_code. Build the incremental model with late data window (3 days), include schema-drift guards, and tests (not_null on user_id, occurred_at; unique on (user_id, occurred_at, event_type)). Add a data-contract macro/test ensuring staging.events_logs contains required fields and types, and a snapshot on users to capture region changes. Explain approach, tests, and macro design?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Bloomberg","Cloudflare","Google"]},{"id":"q-1741","question":"You're building a multi-tenant dbt analytics pipeline on Snowflake. Raw events live in staging.events with tenant_id, user_id, event_type, occurred_at. You plan per-tenant analytics schemas (analytics.{tenant}). Design an incremental analytics.daily_metrics per tenant that counts distinct active users by day and event_type, with late data up to 2 days, joining dimensions.tenants and dimensions.countries, including schema-drift guards, tests (not_null, unique), and a snapshot for user_first_seen per tenant. Explain approach for cross-tenant lineage and tenant-scoped materializations?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["IBM","Stripe"]},{"id":"q-1757","question":"Design a dbt analytics layer for three domains: core_events (user actions), memberships (plans), and referrals. Data arrives late (up to 2 days). Propose an architecture that ensures: 1) incremental models with controlled late data backfills, 2) deterministic surrogate keys for cross-domain joins, 3) schema-drift guards via custom tests/macros, 4) automated docs with complete lineage, 5) dashboard-friendly exposure with stable lineage during backfills, and 6) performance considerations for Snowflake or BigQuery (partitioning, clustering). Include a concrete incremental SQL snippet?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["Anthropic","MongoDB","Slack"]},{"id":"q-1808","question":"Design an intermediate dbt workflow for a fintech analytics pipeline on Snowflake. Raw events are in staging.transactions (transaction_id, user_id, amount, currency, occurred_at, status) and staging.users (user_id, country_code, account_status). Build an incremental analytics.daily_finance that sums total_amount and transaction_count by day, currency, country_code, and status, with late data support up to 2 days. Add analytics.users_snapshot as a Type 2 surrogate for changes in country_code/account_status. Expose lineage and discuss a data-contract macro to validate source schemas and auto-generate tests?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Cloudflare","LinkedIn"]},{"id":"q-1823","question":"Scenario: In a beginner dbt project for a gig-economy platform, raw events arrive in staging.event_logs with user_id, session_id, event_type (visit, click, conversion), occurred_at, and region_code. Build an incremental model analytics.daily_events that counts events by day and event_type, enriched by dims.regions on region_code. Use a 2-day late data window, guard against schema drift with not_null and unique tests, and create a data-contract macro to verify staging.event_logs contains the required columns and types before run. What approach would you take?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Amazon","Uber"]},{"id":"q-1909","question":"Design a per-tenant incremental dbt flow in Snowflake that computes analytics.{tenant}.daily_event_summary from staging.events (tenant_id, user_id, event_type, occurred_at, country_code, record_hash). Include enrichment from dimensions.countries, and produce a per-tenant daily count of distinct users by event_type. Implement late data handling (2 days), schema-drift guards, tests (not_null, unique), and a snapshot for analytics.{tenant}.users to capture cohort changes. Explain how you ensure cross-tenant lineage and isolation?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["LinkedIn","Netflix","Twitter"]},{"id":"q-1988","question":"Design an incremental, per-tenant analytics.daily_metrics model in Snowflake with dbt. Staging.events has tenant_id, user_id, event_type, occurred_at, platform, revenue. Build analytics.{tenant}.daily_metrics counting distinct users per day by event_type, with late data tolerance of 2 days. Add a per-tenant feature flag in dimensions.tenants (revenue_enabled) and a macro to include revenue only when true. Enforce isolation via per-tenant schemas, add schema-drift guards, tests (not_null, unique), and a snapshot for analytics.{tenant}.users. Explain cross-tenant lineage and dashboard exposure?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Hugging Face","Snowflake"]},{"id":"q-2013","question":"Design a per-tenant incremental analytics model in Snowflake that materializes analytics.{tenant}.daily_event_engagement from staging.events (tenant_id, user_id, event_type, occurred_at). Enrich with dimensions.regions on country_code. Produce daily counts by event_type and region_name. Add a 3-day late data window, schema-drift guards, tests (not_null, unique), and a snapshot analytics.{tenant}.customers for cohort changes. Explain tenant isolation and cross-tenant lineage via macro-generated per-tenant schemas?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Goldman Sachs","LinkedIn"]},{"id":"q-2097","question":"Design a per-tenant weekly retention pipeline in Snowflake using dbt where raw events live in staging.events with tenant_id, user_id, first_seen_at, occurred_at, and an events.users table. Create analytics.tenant_retention (tenant_id, cohort_week, retention_users, total_users) that computes weekly retention by cohort (first_seen_week) with incremental loading, and late data handling up to 4 days. Also implement analytics.global_retention that aggregates per-tenant retention across tenants with tenant_dim country/plan, ensuring cross-tenant lineage and isolation. Include schema-drift guards, tests (not_null, unique), and a snapshot of analytics.tenants to track cohort definitions. Explain how you ensure isolation and lineage?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Lyft","Snowflake"]},{"id":"q-2133","question":"Scenario: A new multi-tenant event feed named staging.stream_events with columns tenant_id, user_id, event_type, occurred_at, country_code, payload (JSON). Task: implement a dbt incremental model analytics.daily_user_events that, for each day, tenant_id, and event_type, returns the count of distinct users. Enrich with dimensions.countries on country_code. Create a macro to parse payload JSON extracting device and app_version; fallback defaults if missing. Implement late data tolerance of 2 days (i.e., if occurred_at within last 2 days, process in daily batch). Add tests: not_null on tenant_id, user_id, occurred_at; unique on (tenant_id, user_id, occurred_at, event_type). Add a snapshot for analytics.users to capture cohort-country changes. Provide strategy for cross-tenant lineage and isolation in a single schema (no per-tenant schemas)?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["MongoDB","NVIDIA","OpenAI"]},{"id":"q-2173","question":"Design a per-tenant incremental dbt model for a SaaS analytics pipeline on Snowflake. Source staging.user_events (tenant_id, user_id, event_type, event_timestamp, revenue, lifecycle_stage). Build analytics.{tenant}.daily_cohort to compute daily cohorts by lifecycle_stage and event_type with a 2-day late-data window, upserting via MERGE. Add schema-drift guards, tests (not_null, unique), and a per-tenant last_seen_users snapshot. How do you enforce cross-tenant isolation and lineage?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Hashicorp","NVIDIA"]},{"id":"q-2315","question":"Design a beginner-friendly per-tenant daily_session_summary in Snowflake/dbt. Source: staging.sessions(tenant_id, user_id, session_start). Build analytics.{tenant}.daily_session_summary counting distinct users per tenant per day, with enrichment from dimensions.tenants (plan, region). Implement late data tolerance of 1 day, schema-drift guards, tests (not_null, unique). Include a data-contract macro to validate staging.sessions fields/types and a snapshot on analytics.{tenant}.users. Explain how you ensure cross-tenant isolation and lineage in dashboards?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Bloomberg","Microsoft","Salesforce"]},{"id":"q-2403","question":"Beginner dbt task: from staging.events (event_id, user_id, event_type, occurred_at, region_code, delete_flag) and dimensions.regions, implement an incremental model analytics.daily_event_summary that counts distinct users per day, event_type, country_code (via region_code). Exclude deleted events by default; provide a macro to toggle inclusion for audits. Add tests not_null(event_id, occurred_at) and unique(event_id)?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Discord","Netflix"]},{"id":"q-2442","question":"In a dbt project, given staging.events(user_id, event_type, occurred_at, country_code) and staging.users(user_id, created_at, country_code), implement an incremental model analytics.daily_engagement that counts distinct users per day by event_type and country_code, enriched by dimensions.countries. Include 1-day late data tolerance, schema-drift guards, tests (not_null, unique), and a snapshot on analytics.users to track country changes. Describe lineage and isolation notes?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Cloudflare","Uber"]},{"id":"q-2502","question":"Design a contract-validated, multi-tenant revenue analytics flow in dbt on Snowflake. Raw events live in staging.{tenant}.events (tenant_id, user_id, event_type, amount, currency, occurred_at). Build a incremental analytics.{tenant}.daily_revenue that sums revenue per day/tenant/currency, with 2-day late data tolerance. Create contracts.tenants describing required fields and a macro that fails tests when missing fields. Add per-tenant analytics.{tenant}.users snapshot for status changes. Enforce per-tenant schemas, schema-drift guards, and tests (not_null, unique). Explain cross-tenant lineage and performance considerations?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["Meta","Microsoft","Plaid"]},{"id":"q-2685","question":"Design a beginner dbt task to build an incremental per-tenant analytics.daily_events model in Snowflake from staging.events(tenant_id, user_id, event_type, occurred_at). Include enrichment from dimensions.event_types, a 2-day late-data window, per-tenant isolation via schemas, and schema-drift guards. Add tests (not_null, unique) and a snapshot for analytics.{tenant}.users; also implement a simple data-contract macro to validate staging.events columns/types and a test using it?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Citadel","Instacart","Snowflake"]},{"id":"q-2765","question":"In a beginner dbt project for multi-tenant analytics, build an incremental model analytics.{tenant}.hourly_engagement over staging.events(tenant_id, user_id, event_type, occurred_at). Implement a 2-hour late data window, per-tenant schema isolation, and tests not_null (tenant_id, user_id, occurred_at, event_type) plus unique on (tenant_id, occurred_at, user_id, event_type). Include a small macro to provision analytics.{tenant} schemas and a snapshot analytics.{tenant}.users to capture cohort changes. Explain how to surface tenant lineage in dashboards?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Google","Scale Ai","Slack"]},{"id":"q-2797","question":"In a multi-tenant dbt analytics stack on Snowflake, raw events sit in staging.events with tenant_id, user_id, event_type, occurred_at. Propose a per-tenant drift-guard plan: a canary analytics.canary_{tenant} surfacing current schema, fields, and max_version; staged per-tenant views; and an incremental analytics.tenant.daily_metrics by day, event_type, tenant_id using MERGE for upserts. Include tests (not_null, unique, foreign_key), a snapshot analytics.tenants_cohort, and isolation/lineage via dbt sources and Snowflake row-level policies. Provide skeleton SQL and tradeoffs?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Bloomberg","Discord","Google"]},{"id":"q-2840","question":"You’re building a beginner dbt flow for multi-tenant analytics. With staging.events(tenant_id, user_id, event_type, occurred_at) and a tenants.csv seed containing tenant_id and risk_multiplier, design analytics.{tenant}.daily_event_score that sums events per day by type, applies the per-tenant risk_multiplier, tolerates 1 day late data, and includes a per-tenant users snapshot and basic tests. How would you implement this?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Hugging Face","Meta"]},{"id":"q-2882","question":"How would you design a multi-tenant dbt pipeline in Snowflake where staging.events (tenant_id, user_id, event_type, occurred_at, event_properties) feeds analytics.{tenant}.daily_metrics incrementally to count distinct active users by day and event_type, with late data up to 2 days via MERGE upserts and a per-tenant surrogate key? Include enrichment from dimensions.tenants (region, plan), a users cohort snapshot, tenant isolation via namespaced models, and a central metadata table for cross-tenant lineage; outline tests and validations?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Goldman Sachs","Meta","OpenAI"]},{"id":"q-2901","question":"Design a multi-tenant dbt pipeline that creates per-tenant-per-region analytics schemas (analytics.{tenant}_{region}). From staging.events (tenant_id, region_id, user_id, event_type, occurred_at, ingestion_id), implement an incremental analytics.{tenant}_{region}.hourly_metrics counting distinct users by event_type per hour, with late data handling of 1 day. Add a tenant-region users snapshot for cohort changes and a tenancy-isolation macro to block cross-region joins. Include tests (not_null, unique) and cross-tenant lineage validation?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Microsoft","NVIDIA","Netflix"]},{"id":"q-2933","question":"Design a multi-tenant dbt flow in Snowflake that computes analytics.{tenant}.hourly_metrics from staging.events_raw (tenant_id, user_id, event_type, occurred_at, record_hash). Implement an incremental analytics.{tenant}.hourly_metrics by hour_start and event_type with active_users = count(distinct user_id); support 1-day late data; add dynamic schema-drift guards and tests (not_null on hour_start, tenant_id; unique on (tenant_id, hour_start, event_type)); include analytics.{tenant}.users_cohort snapshot for first_session and country changes; ensure per-tenant isolation via schemas and a global analytics.lineage table. Outline macro-driven tests and tenant-scoped materializations?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Microsoft","NVIDIA","Twitter"]},{"id":"q-2940","question":"Design a beginner dbt exercise in Snowflake to build analytics.{tenant}.daily_event_summary from staging.events (tenant_id, user_id, event_type, occurred_at) enriched with dimensions.tenants (retention_group). Implement an incremental model per day that counts distinct users by event_type, with a 2-day late data window. Add tests (not_null on occurred_at,event_type,tenant_id; unique on (tenant_id, occurred_at, event_type)). Create a data-contract macro that validates staging.events fields and types before run and describe how to enforce per-tenant isolation and lineage?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Discord","Microsoft"]},{"id":"q-3016","question":"Design an advanced, privacy-aware per-tenant dbt flow. Tenants table stores privacy_level. Create a macro that SHA256-hashes user_id to analytics.{tenant}.daily_engagement.hashed_user_id only when privacy_level=high; otherwise pass-through. Build analytics.{tenant}.daily_engagement incrementally by date and action with 2-day late data; snapshot analytics.{tenant}.users for cohort and country changes. Enforce per-tenant schemas, schema-drift tests, and not_null/unique constraints; expose a cross-tenant lineage view and privacy-aware dashboards?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["Instacart","Oracle","Scale Ai"]},{"id":"q-3275","question":"In a multi-tenant dbt deployment on Snowflake serving analytics.{tenant}.hourly_metrics from staging.events_raw (tenant_id, user_id, event_type, occurred_at, record_hash), implement a per-tenant, incremental materialization by hour_start and event_type with late data handling of 1 day. Add a tenant-filter macro that enforces isolation by injecting tenant_id filters into every model. Describe isolation, tests, and a macro skeleton with usage and a lineage approach for dashboards?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Adobe","Databricks","LinkedIn"]},{"id":"q-3337","question":"Design an intermediate dbt flow in Snowflake to derive analytics.{tenant}.hourly_metrics from staging.events_json (tenant_id, occurred_at, payload VARIANT) with event_type extracted from payload.eventType (and fallback to payload.type) and indexed by hour_start and event_type. Implement an incremental model that handles 2-day late data, adds schema-drift guards, and tests (not_null on hour_start, tenant_id, event_type; unique on (tenant_id, hour_start, event_type)). Include a per-tenant analytics.{tenant}.users snapshot from payload.user.* for cohort analysis. Outline tenant isolation via per-tenant schemas, a global lineage, and a macro for cross-tenant extraction of event_type?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["DoorDash","Lyft"]},{"id":"q-3377","question":"Design a beginner dbt flow to compute analytics.{tenant}.daily_user_event_count from staging.events (tenant_id, user_id, event_type, occurred_at). Use per-tenant schemas for isolation. Build an incremental model keyed by day and event_type, counting distinct users. Implement 1-day late data tolerance. Create a data-contract macro validating required fields and that event_type values are whitelisted per-tenant via dimensions.tenants.event_whitelist. Add tests: not_null on tenant_id, user_id, occurred_at, event_type; unique on (tenant_id, occurred_at_date, event_type). Include a snapshot analytics.{tenant}.users to track first_seen/last_seen. Explain cross-tenant lineage and how to add a new tenant without touching existing tenants?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Airbnb","NVIDIA","Oracle"]},{"id":"q-3432","question":"Describe an automated, tenant-aware dbt workflow that discovers tenants from a tenants table, creates per-tenant schemas, and materializes analytics.{tenant}.interactions incrementally from staging.events (tenant_id, user_id, interaction_type, occurred_at). Include late data tolerance of 2 days, per-tenant lineage via a global analytics.lineage table, and macro-driven schema-drift guards plus tests (not_null on occurred_at and user_id; unique on (tenant_id, occurred_at, user_id, interaction_type)); explain how isolation is enforced and how you would validate changes across tenants?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["Discord","Plaid","Square"]},{"id":"q-3470","question":"Design a beginner dbt flow to compute per-tenant daily metrics from staging.events that include an is_deleted flag for tombstones. Build analytics.{tenant}.daily_metrics by day_start (date of occurred_at) and event_type for rows where is_deleted = false; also create a per-tenant analytics.{tenant}.users snapshot with last_seen. Ensure incremental processing with tenant isolation (per-tenant schemas), plus not_null and unique tests and a data-contract macro. Explain tombstone handling and lineage?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Citadel","Microsoft","NVIDIA"]},{"id":"q-3627","question":"Design a beginner dbt workflow to build analytics.country_event_summary per tenant. Source: staging.events(tenant_id, user_id, event_type, occurred_at, country_code). Create analytics.{tenant}.country_event_summary incrementally by day, counting distinct users per event_type and country_code. Enrich with dimensions.countries. Implement late data handling for up to 2 days. Add basic tests: not_null on tenant_id, day, country_code, event_type; unique on (tenant_id, day, event_type, country_code, user_id). Include a simple snapshot analytics.{tenant}.users to capture last_known_country and explain how you verify cross-tenant isolation and lineage?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Goldman Sachs","IBM","LinkedIn"]},{"id":"q-3669","question":"Design a multi-tenant dbt pipeline for a Discord/Snap like platform that computes analytics.{tenant}.event_summary from staging.events (tenant_id, user_id, event_type, occurred_at, country). Build an incremental model by hour and event_type with 1 day late data, tenant isolation via per tenant schemas, and a tenant-filter macro; add dynamic schema drift guards and tests (not_null on hour_start, tenant_id, event_type; unique on (tenant_id, hour_start, event_type)); include per-tenant analytics.{tenant}.users snapshot, analytics.lineage, and analytics.{tenant}.alerts for anomalies; outline testing and alerting strategy?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Discord","Snap"]},{"id":"q-3755","question":"Design a beginner-friendly multi-tenant dbt exercise in Snowflake: for each tenant, build analytics.{tenant}.daily_summary from staging.events (tenant_id, user_id, event_type, occurred_at) enriched by dimensions.countries, producing daily counts by event_type per tenant. Implement a 2-day late data window, per-tenant isolation via schemas, and tests: not_null on tenant_id, occurred_at, event_type; unique on (tenant_id, occurred_at, event_type); a per-tenant data_contract that country_code exists in countries; and a cross-tenant leakage test ensuring analytics.{tenant} only reads its own data. Outline approach and tests?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Snowflake","Uber"]},{"id":"q-3815","question":"Design an intermediate dbt workflow on Snowflake for a federated analytics layer with per-tenant schemas (tenant_<id>). Staging.events_raw has (tenant_id, user_id, event_type, occurred_at, payload VARIANT). Build analytics.hourly_metrics as an incremental model by hour_start and event_type across tenants, resolving schemas via a macro. Include 1-day late data, schema-drift guards, tests (not_null on hour_start, tenant_id; unique on tenant_id/hour_start/event_type), and a global analytics.lineage table; describe per-tenant isolation and dashboard exposure?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Airbnb","Snowflake"]},{"id":"q-3849","question":"In a multi-tenant dbt deployment on Snowflake, implement an anomaly detection layer: analytics.{tenant}.anomalies derived from staging.events_raw (tenant_id, user_id, event_type, occurred_at, value). Build a per-tenant rolling z-score per event_type, daily incremental, with 2-day late data, and add schema-drift guards and tests. Provide a tenant-scoped macro that injects filters into all models and a simple lineage snapshot for dashboards. How would you implement this end-to-end?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Adobe","DoorDash"]},{"id":"q-3939","question":"Design a beginner dbt flow in Snowflake to compute analytics.{tenant}.daily_user_engagement from staging.events (tenant_id, user_id, event_type, occurred_at, country_code), enriched by dimensions.countries and dimensions.products; produce per-tenant daily counts by event_type and country, with 2-day late data handling, per-tenant schema isolation, schema-drift guards, and tests (not_null on tenant_id, user_id, occurred_at; unique on (tenant_id, occurred_at, user_id)); include a snapshot analytics.{tenant}.users to capture cohort changes and a data-contract macro validating staging fields; explain cross-tenant lineage and privacy considerations?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["MongoDB","NVIDIA","Netflix"]},{"id":"q-4033","question":"Design a per-tenant incremental model in Snowflake that computes analytics.{tenant}.retention_daily from staging.events (tenant_id, user_id, event_date, first_seen, country_code). Include a macro-driven approach to switch between per-tenant daily retention and rolling-7-day retention, handle late data up to 3 days, enforce per-tenant schema isolation, and add tests (not_null on tenant_id, user_id, event_date; unique on (tenant_id, user_id, event_date)). Also add a snapshot analytics.{tenant}.users_cohort for first_seen and country changes. Explain how you would implement cross-tenant lineage and per-tenant data quality checks?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["Google","Hashicorp","Robinhood"]},{"id":"q-4103","question":"Design a multi-tenant incremental dbt flow in Snowflake that computes analytics.{tenant}.monthly_cost from staging.sales_events (tenant_id, cost_cents, currency, billed_at, event_hash). Build analytics.{tenant}.monthly_cost by (tenant_id, month_start) with 30-day late_arrival tolerance; implement currency conversion using a per-tenant rates table; handle null currencies. Provide schema-drift guards and tests (not_null on month_start and tenant_id; unique on (tenant_id, month_start)); add a tenant-scoping macro routing models to per-tenant schemas and a global analytics.lineage. Describe testing strategy and cross-tenant isolation?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Airbnb","Amazon","Anthropic"]},{"id":"q-4274","question":"In a multi-tenant Snowflake dbt project, staging.events has tenant_id, user_id, event_type, occurred_at. Create an incremental analytics.{tenant}.daily_events that counts events per tenant/day by event_type from staging.events. Implement 1-day late data, per-tenant schemas via a macro, and a lightweight analytics.lineage table. Tests: not_null on key columns and unique on (tenant_id, day, event_type). How would you ensure isolation and maintainability?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Discord","MongoDB","Netflix"]},{"id":"q-4488","question":"Design a beginner dbt workflow in Snowflake to compute analytics.{tenant}.daily_user_activity from staging.events (tenant_id, user_id, action, occurred_at, country_code). Normalize occurred_at to UTC, bucket by day, and count distinct users per action. Include 1-day late data, per-tenant schema isolation, tests (not_null on keys, unique on (tenant_id, date, action)); add a data-contract macro to validate staging.events schema; snapshot analytics.{tenant}.users for country churn, and outline per-tenant lineage into analytics.lineage?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Cloudflare","Snowflake","Tesla"]},{"id":"q-4686","question":"In a beginner dbt project for multi-tenant analytics on Snowflake, staging.events(tenant_id, user_id, event_type, occurred_at, record_hash) feeds analytics.{tenant}.daily_metrics. Propose an incremental approach that uses a per-tenant schema, handles 2-day late data, and uses a MERGE-based dedup strategy on (tenant_id, day, event_type) while preserving idempotence. Include a macro for tests and a snapshot of analytics.{tenant}.users. Explain cross-tenant lineage and error handling?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Citadel","Snap","Stripe"]},{"id":"q-4747","question":"Design a per-tenant, incremental dbt flow in Snowflake that computes analytics.{tenant}.hourly_engagement from staging.tenant_events (tenant_id, user_id, action, occurred_at). Build via a macro that creates analytics_{{tenant}} schemas and analytics_{{tenant}}.hourly_engagement, plus a global analytics.lineage table to track dependencies. Include 1-day late data, schema-drift guards, and tests (not_null on keys, unique on (tenant_id, hour_start, user_id)). Also snapshot analytics.{tenant}.users for churn. Explain how you would validate cross-tenant isolation in CI and prevent bleed?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["Coinbase","Google","Plaid"]},{"id":"q-848","question":"You're designing a dbt analytics pipeline for an e-commerce platform. The 'staging.events_raw' table ingests page views and purchases and is partitioned by day. Data arrives both on time and as late as 24 hours. Design a practical incremental dbt model 'analytics.daily_sales' that aggregates revenue by day, category, and customer_segment. Explain how you'd implement incremental logic, handle late data, guard against schema drift, and validate with tests and snapshots?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["Amazon","Anthropic","Tesla"]},{"id":"q-972","question":"In a dbt analytics project deployed to Snowflake, three tenants share a common model but data is isolated by schema prefixes (tenant_a_, tenant_b_, tenant_c_). Describe how you would structure tenancy-aware tests and a test runner to prevent cross-tenant data leakage during PR runs. Include how you configure sources, seeds, and per-tenant test filtering, and how you verify isolation in CI without touching production data?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["DoorDash","Lyft","OpenAI"]},{"id":"q-1146","question":"You're building a data ingestion service that validates JSON records before persisting them. Validation steps include NonEmpty fields, Email format, and PasswordStrength. Validators must be pluggable: new validators can be added at runtime by registering them into a Registry without touching the core pipeline. Design a minimal interface and registry, and show how to compose and execute the pipeline with a sample config. Include how to add a new validator and run the pipeline, returning the first failure?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["IBM","Square","Tesla"]},{"id":"q-1201","question":"Design a runtime-pluggable per-message transformation system for a streaming pipeline. Messages include metadata that determines the transformation strategy (e.g., enrich, normalize, validate). Implement with a design pattern that lets you add new strategies without modifying the pipeline core. Provide minimal interfaces, a registry, and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["MongoDB","Plaid","Salesforce"]},{"id":"q-1236","question":"Design a runtime-pluggable, per-route transformer and throttling policy system for a real-time event router. Each route maps an event type to a transformer and a rate-limit policy; new transformers (enrich, redact, normalize) and new policies (token-bucket, fixed-window, leaky-bucket) must be addable at runtime without touching the router core. Which pattern would you adopt and how would you structure the minimal interfaces and registry?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Oracle","Snowflake"]},{"id":"q-1258","question":"Design a runtime-extensible notification dispatch system that handles multiple channels (email, SMS, push) where new transports and their backoff strategies can be added at runtime via a registry without touching the core dispatcher. Specify the minimal interfaces, how you register a new transport and a new backoff policy, and show a usage example including adding a WhatsApp transport and a geometric backoff?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Coinbase","DoorDash","Plaid"]},{"id":"q-1300","question":"Design a feature-flag evaluation engine for a large SaaS product. Flags support boolean, percentage rollout, user-segment, and A/B bucket strategies. Create a pluggable evaluator where new strategies can be added at runtime via a Registry without touching the core evaluator. Provide interfaces, a thread-safe registry, and a usage example with versioned strategy lookup?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Discord","Google","Snowflake"]},{"id":"q-2360","question":"Design a Runtime-Configurable Cache Eviction Engine: build a cache component that supports multiple eviction policies (LRU, LFU, Time-based); policies can be swapped at runtime without restarting the app. Use a pattern that lets you add new eviction strategies via a registry. Provide minimal interfaces, how to register/deregister policies, and an example usage that demonstrates policy switch mid-flight with ongoing hits?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Google","Oracle"]},{"id":"q-2513","question":"Design a runtime-extensible data-format converter: the tool accepts files in JSON, CSV, and XML and should support new formats at runtime without touching the core converter. Which design pattern fits, and how would you structure minimal interfaces and a registry? Provide a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Amazon","Google","MongoDB"]},{"id":"q-2603","question":"Design a runtime-pluggable routing layer for a microservice gateway that allows adding new load-balancing and circuit-breaker strategies without touching the gateway core. Endpoints declare a strategy name and optional config; a central registry wires strategy instances to endpoints. Include minimal interfaces, a registry, and a usage example. How would you structure tests for correctness and fault tolerance?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Apple","Tesla","Zoom"]},{"id":"q-2630","question":"In a mobile analytics SDK, events must be emitted in multiple serialization formats (JSON, MessagePack, Protobuf). Design a runtime-pluggable serializer system so new formats can be added without touching the producer. Which design pattern fits, and how would you structure a minimal Serializer interface, a Registry mapping names to serializers, and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Netflix","Robinhood","Tesla"]},{"id":"q-2798","question":"Design a runtime-pluggable access-control system for a cloud storage service. The core stores resources; you must support multiple policy types (RBAC, ABAC, and custom dynamic policies) added at runtime without recompiling. Explain the minimal interfaces, a Registry, and how policies are evaluated per operation; include an example usage involving uploading a file with user attributes and resource metadata?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Apple","Microsoft"]},{"id":"q-2992","question":"Design a runtime-extensible data-mapping system for a real-time event ingestion pipeline used by Stripe, Two Sigma, and Discord. Incoming events vary by source; implement a pluggable FieldMapper pattern that supports PassThrough, RenameFields, ComputedFields (expressions), and FlattenNested. Mappers must be addable at runtime via a registry without touching core. Describe minimal interfaces, registry structure, and a concrete usage scenario?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Discord","Stripe","Two Sigma"]},{"id":"q-3085","question":"Design a small undoable text editor using the Command pattern. The editor supports insert and delete operations; each action should be a Command with execute and undo, and a History stack should manage undo/redo. How would you structure the interfaces and a minimal example showing inserting 'A' at position 0 and then undoing?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Coinbase","Scale Ai","Stripe"]},{"id":"q-3153","question":"Design a runtime-extensible request-validation engine for an API gateway: validators for JSON schema, business rules, and anti-abuse checks should be addable at runtime without touching the gateway core. Describe the architecture using a design pattern that supports pluggable validators via a registry, minimal interfaces, and a runtime usage example?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Cloudflare","Netflix","Two Sigma"]},{"id":"q-3180","question":"Design a runtime-extensible authorization policy engine that supports multiple policy languages (RBAC, ABAC, ACL) and allows new languages to be added at runtime without touching the core. Describe the minimal interfaces and a registry mechanism; show how evaluation proceeds for a given user, resource, and action. Provide a concrete usage example registering a new language and evaluating a request?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Airbnb","Meta","Twitter"]},{"id":"q-3234","question":"Design a runtime-pluggable authentication mechanism for an API gateway that must support multiple methods: API key, OAuth2 bearer token, and HMAC signatures. The gateway should be able to add new methods at runtime without modifying core logic. Provide a minimal interface, a registry of strategies, and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Hugging Face","LinkedIn","Twitter"]},{"id":"q-3436","question":"Design a runtime-pluggable rate limiter for a multi-tenant API gateway. It should support strategies like Token Bucket, Fixed Window, and Sliding Window, registerable at runtime and swappable per-tenant without recompiling core. Specify minimal interfaces, a registry, and how hot-swap is achieved, with a code-like usage example?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["NVIDIA","Netflix","Robinhood"]},{"id":"q-3523","question":"Design a runtime-extensible API gateway policy engine where routing decisions, auth, and rate-limiting are driven by pluggable policies that can be registered at runtime without redeploying. Each policy implements a minimal interface and is discovered via a registry. Show minimal interfaces, a registry shape, and an example of adding a new 'geo-aware' rate-limiting policy. How would you ensure thread-safety and downtime-free hot reload during policy updates?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Google","MongoDB","Two Sigma"]},{"id":"q-3611","question":"Design a tiny logging framework with pluggable backends (console, file, remote). The core should not depend on concrete backends; instead, a registry maps names to backend factories and allows runtime extension. Provide interfaces, a minimal registry, and a usage example showing adding a 'syslog' backend and selecting backend via config?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Lyft","Slack","Stripe"]},{"id":"q-3736","question":"Design a runtime-extensible log writer that outputs records in different formats. At runtime, the caller specifies the format by name (JSONL, CSV, NDJSON). The core writer must remain unchanged when adding new formats. Describe the architecture using a design pattern, provide minimal interfaces and a registry, and show a usage snippet?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Google","Snap","Snowflake"]},{"id":"q-3826","question":"Design a runtime-extensible feature-flag evaluation system for a multi-tenant platform. The system must support multiple evaluation strategies: boolean flags, context-aware rules (e.g., region and plan), and experiment-based A/B tests. New strategies should be pluggable at runtime via a registry without touching the core evaluator. Provide a minimal interface, a registry, and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Databricks","Instacart","Oracle"]},{"id":"q-3945","question":"Design a system where a base DataSource can be wrapped by any number of processing decorators (e.g., Logging, Masking, Compression). The decorators are registered by name at runtime and can be applied to a data source chain without changing the core. Implement minimal interfaces, a registry, and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["MongoDB","Slack","Stripe"]},{"id":"q-3956","question":"Design a runtime-extensible resource pool that supports multiple eviction strategies (LRU, LFU, TTL). Each pool entry uses a policy selected by name at runtime, and new eviction strategies can be added without touching the pool core. Provide a minimal IPolicy interface, a PolicyRegistry, and a usage example showing how to plug a new policy for a given connection without recompiling the pool?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Citadel","Cloudflare","Plaid"]},{"id":"q-4119","question":"Design a runtime-extensible transport layer for a data ingestion service that can route data to HTTP, Kafka, or gRPC transports. New transports should be pluggable at runtime without changing core; design a minimal interface and a registry to support this. Which design pattern would you choose and how would you implement it? Provide a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Amazon","Stripe","Tesla"]},{"id":"q-4323","question":"Design a pluggable policy-based access control (PBAC) system where authorization decisions can be extended at runtime by adding new policy evaluation strategies (e.g., RBAC, ABAC, context-aware) without recompiling services. Describe minimal interfaces, a registry, and how you would ensure consistency across distributed services. Include a concrete usage example showing how a new policy type is registered and evaluated?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Bloomberg","Stripe","Tesla"]},{"id":"q-4513","question":"Design a pluggable file parser system for a data importer: the core imports files and delegates parsing to a Parser backend chosen by file extension (e.g., .csv, .json, .xml). Propose a registry to map extensions to parsers and show how to add a YAML parser at runtime without touching the importer. Which pattern would you use and why? Provide minimal interfaces and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Apple","Instacart"]},{"id":"q-4529","question":"Design a runtime-extensible command interpreter for a distributed task queue. The system receives JSON commands from producers; new command types should be added as plugins without redeploying the interpreter. Explain how to structure interfaces, a plugin registry, and isolation, and how to handle versioned command schemas. Provide minimal interfaces and usage example?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Google","Hugging Face"]},{"id":"q-4579","question":"Design a pluggable API gateway authentication framework: the gateway handles per-route authentication using multiple strategies (API key, JWT, mTLS). Allow registering new strategies at runtime without touching the gateway core; must support strategy chaining for routes that require multiple checks. Describe interfaces, a registry, and a concrete usage example?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Netflix","Uber","Zoom"]},{"id":"q-4708","question":"Design a runtime-rule engine for a financial transactions platform where new fraud-detection rules can be added without redeploying the engine; each rule computes a risk contribution (e.g., velocity-based spend, IP-geography mismatch, device fingerprint). Implement with a design pattern that lets you register new rules at runtime and apply them in a deterministic order to produce a final risk score and a pass/fail decision. Include minimal interfaces, a registry, and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Meta","Microsoft"]},{"id":"q-4734","question":"Design a runtime-extendable deployment strategy engine for a multi-environment CI/CD pipeline. The engine must support blue/green, canary, rolling, and allow new strategies to be added at runtime via a registry, selected based on environment metadata (region, service tier, risk). Provide minimal interfaces and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Google","Square","Tesla"]},{"id":"q-4741","question":"Design a runtime-extendable input validator system for a data ingestion pipeline. Each JSON record has fields like email, age, and country. Create a Validator interface and a Registry so new validators (e.g., phone, ZIP) can be added at runtime without touching ingestion core. Provide minimal interfaces, a registry, and a brief usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Slack","Snowflake","Twitter"]},{"id":"q-681","question":"You're building a multi-tenant API gateway for a Stripe-like payments service, backed by MongoDB storage and a Twitter-like feed. Each tenant has a per-minute rate limit. Describe a concrete solution using the Decorator pattern to enforce quotas, show how you'd implement atomic Redis updates, discuss burst handling and clock drift, and outline a minimal wrapper skeleton in code?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["MongoDB","Stripe","Twitter"]},{"id":"q-688","question":"You're building a feature flag system where flags can be evaluated as a hard boolean, a percentage rollout, or a targeted user segment. Design the architecture using a design pattern that lets you add new evaluation strategies without changing the caller. Which pattern would you choose and how would you implement it in code? Provide a minimal interface and usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Amazon","Google","Hashicorp"]},{"id":"q-695","question":"You're building a real-time data ingestion pipeline that must apply a sequence of transformations to each record. New transforms (normalization, enrichment, validation, anomaly detection) should be added as plugins without touching producer/consumer code. Design a pluggable Transform pipeline with per-tenant routing and hot-reload of configuration. Provide minimal interface and a usage example, including how to configure a few plugins and compose them for a stream?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Amazon","Google","Twitter"]},{"id":"q-706","question":"You're building an extensible data ingestion framework where new data formats (JSON, Parquet, ORC) must be supported without touching the core ingestion logic. Design the architecture using a pattern that decouples format parsing from the caller and lets you add new format handlers without changing the caller. Which pattern would you choose and how would you implement it in code? Provide a minimal interface and usage example?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Amazon","Google","MongoDB"]},{"id":"q-714","question":"You're building a small HTTP client wrapper that fetches a user profile, but the server occasionally fails. The caller selects a retry policy by name (linear, exponential, jitter) and fetchUser should retry using that policy without changing fetchUser's code. Design the architecture using a design pattern that lets you add new retry strategies without modifying the caller. Provide a minimal interface and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Google","Lyft","Salesforce"]},{"id":"q-717","question":"You're building a real-time event processing pipeline that validates, enriches, and filters events before persisting. New validators, enrichers, and filters must be added at runtime without altering the core processor. Which design pattern enables this extensibility and how would you implement it? Provide minimal interfaces and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Snowflake","Stripe"]},{"id":"q-729","question":"You're building an image-processing pipeline that applies a sequence of filters (blur, brighten, sharpen) to images. The pipeline must run on both CPU and GPU backends, and new backends must be pluggable without touching filter implementations or orchestration code. Design an architecture that decouples filters from backends using a design pattern, enabling adding a backend such as Vulkan without modifying core code. Provide a minimal interface and usage example?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Adobe","NVIDIA"]},{"id":"q-737","question":"In a messaging pipeline used by Zoom and Hugging Face, the system tokenizes, normalizes, and stores messages. You want to support swapping in different normalization strategies (lowercasing, diacritic removal, profanity filtering) without changing the pipeline code. Design the architecture using a design pattern that lets you add new normalization strategies without modifying the pipeline. Provide a minimal interface and a usage example. How would you implement this pattern to make additions painless?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Hugging Face","Zoom"]},{"id":"q-741","question":"In a telemetry alerting system for autonomous fleets, each customer needs a custom alert-threshold strategy for metrics like speed or battery: static value, percentile-based, or rolling window. The aggregator should surface alerts without depending on a concrete strategy. Design the architecture using a design pattern that lets you add new threshold strategies without modifying the aggregator. Provide a minimal interface and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Tesla","Uber"]},{"id":"q-750","question":"You’re building a data export utility that must support multiple formats. New formats can come from external libraries with different APIs. Design an architecture using a pattern that lets you add new formats without changing the core exporter. Which pattern would you use and how would you implement it? Provide a minimal interface and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Anthropic","Cloudflare","Tesla"]},{"id":"q-763","question":"You're designing a data ingestion pipeline where raw inputs pass through optional enhancers (encryption, compression, watermarking) implemented as decorators around a base DataSource. You must add new decorators without touching the core pipeline or existing decorators. Which pattern would you use and how would you implement minimal interfaces to compose them? Provide a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Instacart","Microsoft","Oracle"]},{"id":"q-769","question":"In a log analytics pipeline, you must support multiple formats (JSON, CSV, Protobuf) and multiple sinks (Elasticsearch, BigQuery, S3). Design an architecture using a design pattern that allows adding new formats or sinks without touching the producer. Provide minimal interfaces and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Lyft","MongoDB","Oracle"]},{"id":"q-781","question":"You're building a CLI tool that supports commands and subcommands, forming a tree (e.g., 'git remote add'). Design a Command interface that treats leaves (actual actions) and composites (groups of commands) uniformly. Implement LeafCommand and CommandGroup using the Composite pattern so a single call can execute a command or print help for a whole subtree without changing client code. Provide a minimal interface and usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Meta","Robinhood"]},{"id":"q-785","question":"You're building a pluggable HTTP request/response transformer pipeline inside a reverse proxy. Each Transformer can add, redact, or modify headers/body. The gateway must load new Transformers at runtime by name without redeploying. Design a minimal interface and registry-driven architecture that supports adding new transformer types without touching the gateway core. Provide a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Airbnb","Cloudflare","Meta"]},{"id":"q-793","question":"You're designing a streaming data processing framework where each record passes through a configurable pipeline of transformation steps. New transformations must be added at runtime without touching the orchestrator, and jobs select steps by name. Which design pattern and minimal interfaces would you use to register, compose, and execute transformations, ensuring type-safety and low churn when adding new steps? Provide a concise usage example?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Citadel","LinkedIn","Snowflake"]},{"id":"q-802","question":"Design a unit-test framework runner that supports multiple assertion styles (classic, fluent, should). The goal is to add a new assertion style (e.g., expect) at runtime without modifying the runner core. Which design pattern would be chosen and how would the minimal interfaces and a registry be structured? Provide a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Apple","Salesforce","Scale Ai"]},{"id":"q-809","question":"Design a health-check framework for a fleet-management service. The system aggregates health from multiple subsystems (database, message broker, geolocation API). New checks (checkDiskSpace, checkApiLatency) must be added at runtime without touching the aggregator. Which pattern supports this, and how would you implement minimal interfaces and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Lyft","Salesforce"]},{"id":"q-819","question":"You are designing a log export module for a distributed service. It must support exporting archives to multiple backends (S3, GCS, and an on-prem object store). New backends should be addable at runtime without touching the exporter or consumer code. Design the architecture using a design pattern that lets you plug in new backends via a registry. Provide a minimal interface and usage example?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["DoorDash","Netflix","Uber"]},{"id":"q-824","question":"Design a pluggable text-formatting pipeline for a CLI tool. The pipeline should allow new transforms to be added at runtime via a registry, without touching the core pipeline. Use a suitable pattern to compose these transforms in order; provide a minimal interface and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Bloomberg","Hashicorp","MongoDB"]},{"id":"q-833","question":"You're building a streaming analytics dashboard where widgets render different metrics. New chart renderers can be added at runtime by third-party teams without modifying core widgets. Design the architecture using a design pattern that supports pluggable renderers via a registry. Provide a minimal interface and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["NVIDIA","Netflix","Tesla"]},{"id":"gh-16","question":"What is Infrastructure as Code and why has it become essential for modern DevOps practices?","channel":"devops","subChannel":"automation","difficulty":"beginner","tags":["iac","terraform","ansible"],"companies":["Amazon","Google","Hashicorp","Microsoft","Netflix","Salesforce"]},{"id":"gh-18","question":"What is Ansible and how does it work for infrastructure automation?","channel":"devops","subChannel":"automation","difficulty":"beginner","tags":["iac","terraform","ansible"],"companies":["Amazon Web Services","Google Cloud","Microsoft","Red Hat","Southwest Airlines"]},{"id":"gh-29","question":"What is Configuration Management?","channel":"devops","subChannel":"automation","difficulty":"beginner","tags":["config-mgmt","ansible","chef"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Netflix"]},{"id":"gh-30","question":"What is Puppet and how does it manage infrastructure configuration?","channel":"devops","subChannel":"automation","difficulty":"beginner","tags":["config-mgmt","ansible","chef"],"companies":["Bank Of America","Cisco","Google","Microsoft","Staples"]},{"id":"gh-31","question":"What is Scalability in DevOps?","channel":"devops","subChannel":"automation","difficulty":"advanced","tags":["scale","ha"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-36","question":"How do different backup strategies balance storage efficiency, backup speed, and recovery time?","channel":"devops","subChannel":"automation","difficulty":"intermediate","tags":["backup","dr"],"companies":["Amazon","Google","LinkedIn","Microsoft","Uber"]},{"id":"gh-92","question":"How does a Service Catalog enable self-service infrastructure provisioning in an Internal Developer Platform?","channel":"devops","subChannel":"automation","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Spotify"]},{"id":"q-243","question":"How would you design a zero-downtime deployment strategy using Ansible that includes blue-green infrastructure setup, traffic management, and automated rollback capabilities?","channel":"devops","subChannel":"automation","difficulty":"intermediate","tags":["ansible","puppet","chef"],"companies":["Adobe","Amazon","Google","Microsoft","Netflix","Stripe"]},{"id":"q-269","question":"Compare Ansible, Puppet, and Chef configuration management tools, focusing on their architecture, state management approaches, and ideal use cases for enterprise environments?","channel":"devops","subChannel":"automation","difficulty":"beginner","tags":["ansible","puppet","chef"],"companies":["Adobe","Amazon","Google","IBM","Microsoft","Netflix"]},{"id":"q-304","question":"How would you design a multi-environment configuration management strategy using Ansible that supports development, staging, and production environments with role-based access control?","channel":"devops","subChannel":"automation","difficulty":"advanced","tags":["ansible","puppet","chef"],"companies":["Amazon","Google","Meta"]},{"id":"q-381","question":"You have 10 web servers that all need Nginx installed and configured identically. How would you use Ansible to ensure this configuration is consistent across all servers?","channel":"devops","subChannel":"automation","difficulty":"beginner","tags":["ansible","puppet","chef"],"companies":["Deepmind","Google","MongoDB"]},{"id":"q-421","question":"You're managing infrastructure at scale with Ansible, Puppet, and Chef. How would you design a configuration management strategy that handles secret rotation across 1000+ servers while ensuring zero-downtime deployments?","channel":"devops","subChannel":"automation","difficulty":"intermediate","tags":["ansible","puppet","chef"],"companies":["Discord","Meta","Scale Ai"]},{"id":"q-437","question":"You're migrating from Puppet to Ansible for configuration management. How would you handle idempotency differences and what strategy would you use to ensure zero-downtime during the transition?","channel":"devops","subChannel":"automation","difficulty":"intermediate","tags":["ansible","puppet","chef"],"companies":["Adobe","Amazon","Cloudflare","Hashicorp","IBM","Microsoft","Netflix","Salesforce"]},{"id":"q-459","question":"You're managing infrastructure at scale with Ansible. How would you design a strategy to handle configuration drift across 1000+ servers while ensuring minimal downtime during updates?","channel":"devops","subChannel":"automation","difficulty":"intermediate","tags":["ansible","puppet","chef"],"companies":["Cloudflare","Discord","Tesla"]},{"id":"q-490","question":"You're migrating a 500-server fleet from Puppet to Ansible with zero downtime. How would you design the migration strategy to ensure configuration consistency and rollback capabilities?","channel":"devops","subChannel":"automation","difficulty":"advanced","tags":["ansible","puppet","chef"],"companies":["Adobe","Microsoft","PayPal"]},{"id":"q-573","question":"How would you design a GitOps workflow using Terraform and ArgoCD to manage infrastructure across multiple cloud providers while ensuring zero-downtime deployments?","channel":"devops","subChannel":"automation","difficulty":"intermediate","tags":["ansible","puppet","chef"],"companies":["Cloudflare","OpenAI"]},{"id":"do-2","question":"Compare Blue/Green vs Canary deployment strategies, including traffic routing, monitoring, rollback complexity, and cost implications for a microservices architecture?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["deployment","strategy","cicd","jenkins"],"companies":null},{"id":"gh-1","question":"What are the core principles and practices of DevOps, and how does it bridge the gap between development and operations teams?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["basics"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-10","question":"What is a CI/CD pipeline and how does it automate software delivery?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["cicd","automation"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-102","question":"What is GitHub Actions and how does it work?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Digital Ocean","Goldman Sachs","Google","Microsoft"]},{"id":"gh-104","question":"What is Canary Analysis and how does it work in production deployments?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-11","question":"What is Jenkins and how does it facilitate continuous integration and continuous delivery (CI/CD) in modern software development workflows?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["cicd","automation"],"companies":["Amazon","Deutsche Bank","Goldman Sachs","Microsoft","Netflix"]},{"id":"gh-2","question":"How would you design a DevOps pipeline that reduces deployment time by 60% while improving reliability and security?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["basics"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-3","question":"What is Continuous Integration and how does it improve software development quality?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["basics"],"companies":["Amazon","Google","Microsoft","Netflix","Stripe"]},{"id":"gh-64","question":"What are the four key DORA metrics for measuring DevOps performance and how are they calculated?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["metrics","kpi"],"companies":["Amazon","Google","Hashicorp","Microsoft","Netflix","Salesforce"]},{"id":"gh-67","question":"How does Database DevOps integrate database schema changes into CI/CD pipelines while ensuring data integrity and minimizing downtime?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["db","devops"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Snowflake"]},{"id":"gh-68","question":"How would you implement comprehensive security practices in a DevOps pipeline including SAST/DAST, container security, and secrets management?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["security","network"],"companies":["Amazon","Cloudflare","Google","Microsoft","Netflix","Stripe"]},{"id":"gh-74","question":"How does DevOps culture transform traditional siloed development and operations into collaborative workflows?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["culture","soft-skills"],"companies":["Amazon","Google","LinkedIn","Microsoft","Netflix"]},{"id":"gh-75","question":"What DevOps practices are essential for implementing continuous delivery and fostering team collaboration?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["culture","soft-skills"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-90","question":"What is Blue/Green Deployment?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-177","question":"Explain the key differences between model serving and model deployment in ML systems, including specific technologies, scaling considerations, and real-world implementation patterns?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["mlops","deployment"],"companies":["Amazon","Databricks","Google","Meta","Microsoft","Netflix"]},{"id":"q-194","question":"How would you design a Terragrunt + Atlantis workflow that prevents state lock contention across 50+ microservice environments while maintaining DRY principles?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["dry","terragrunt","atlantis"],"companies":["Airbnb","Coinbase","Databricks","Stripe","Uber"]},{"id":"q-298","question":"Design a large-scale enterprise CI/CD system for an AWS-based application?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["ci-cd","aws","enterprise","containers","automation"],"companies":["Amazon","Google","Microsoft"]},{"id":"q-318","question":"How would you design a GitHub Actions workflow that runs tests in parallel across multiple matrix configurations while ensuring proper artifact management and failure handling?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["github-actions","jenkins","gitlab-ci"],"companies":["DoorDash","LinkedIn","Robinhood"]},{"id":"q-332","question":"You have a GitHub Actions workflow that's failing intermittently due to race conditions when multiple PRs trigger the same deployment pipeline. How would you design a solution to prevent concurrent deployments while maintaining fast feedback for developers?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["github-actions","jenkins","gitlab-ci"],"companies":["Amazon","Hulu","Jane Street"]},{"id":"q-398","question":"You have a GitHub Actions workflow that's failing intermittently due to rate limiting on a third-party API. How would you design a robust retry mechanism with exponential backoff while ensuring the workflow completes within the 6-hour timeout limit?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["github-actions","jenkins","gitlab-ci"],"companies":["Deepmind","Elastic","Webflow"]},{"id":"q-410","question":"You're setting up a CI/CD pipeline for a microservice that needs to run security scans, build a Docker image, and deploy to staging. How would you configure GitHub Actions to fail fast if security vulnerabilities are found, while still allowing the build to proceed for testing?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["github-actions","jenkins","gitlab-ci"],"companies":["Meta","Okta","PayPal"]},{"id":"q-444","question":"You have a GitHub Actions workflow that's failing intermittently due to rate limiting. How would you design a robust CI/CD pipeline that handles API rate limits, implements proper retry logic, and ensures consistent deployments across multiple environments?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["github-actions","jenkins","gitlab-ci"],"companies":["OpenAI","Tesla","Uber"]},{"id":"q-520","question":"You have a GitHub Actions workflow that's failing intermittently due to rate limiting when pulling Docker images. How would you design a robust solution that ensures consistent builds while minimizing costs?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["github-actions","jenkins","gitlab-ci"],"companies":["Apple","Square"]},{"id":"q-1159","question":"Design a CI/CD pipeline for a containerized application that needs to support multiple environment-specific configurations (dev, staging, prod) while maintaining security best practices. How would you structure the pipeline to handle secrets management, image scanning, and environment-specific deployments without duplicating pipeline code?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["ci-cd","container-security","secrets-management","multi-environment","kubernetes"],"companies":[]},{"id":"q-1339","question":"How would you implement a custom Kubernetes scheduler to handle specialized workloads like GPU-intensive ML jobs, and what components would need to be modified compared to the default scheduler?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["kubernetes","scheduler","custom-scheduler","gpu","ml-workloads"],"companies":[]},{"id":"q-1341","question":"How does Kubernetes handle pod preemption and priority classes when scheduling, and what happens when a high-priority pod needs to be placed but all nodes are at capacity?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["kubernetes","pod-scheduling","preemption","priority-classes","resource-management"],"companies":[]},{"id":"q-2616","question":"How does Kubernetes handle pod scheduling during cluster autoscaling events, and what scheduling considerations come into play when new nodes are added or removed from the cluster?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["kubernetes","autoscaling","pod-scheduling","cluster-autoscaler","devops"],"companies":[]},{"id":"q-3110","question":"Design a CI/CD pipeline for a polyglot microservices application where services are written in different languages (Node.js, Python, Java, Go) and each requires different build tools, testing frameworks, and deployment strategies. How would you create a unified pipeline that respects language-specific requirements while maintaining consistency in security scanning, artifact management, and deployment orchestration?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["polyglot","microservices","containerization","security-scanning","pipeline-orchestration"],"companies":[]},{"id":"q-3112","question":"Design a CI/CD pipeline for a hybrid cloud application that runs on both on-premises Kubernetes clusters and cloud-managed services (like AWS EKS). The pipeline must handle compliance requirements, maintain consistent deployments across environments, and support disaster recovery with automated failover. How would you structure the pipeline to ensure regulatory compliance while enabling seamless cross-platform deployments?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["hybrid-cloud","gitops","compliance","disaster-recovery","multi-cluster"],"companies":[]},{"id":"q-3649","question":"How does Kubernetes handle pod scheduling during cluster-wide events like node maintenance windows or cluster upgrades, and what scheduling mechanisms can be configured to minimize service disruption?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["kubernetes","pod-scheduling","maintenance","pod-disruption-budgets","cluster-upgrades"],"companies":[]},{"id":"q-3650","question":"Design a CI/CD pipeline for a multi-tenant SaaS application that needs to support blue-green deployments with database schema migrations, ensuring zero downtime for all tenants during updates. How would you handle tenant isolation, migration coordination, and rollback strategies?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["blue-green","multi-tenant","database-migration","zero-downtime","rollback"],"companies":[]},{"id":"q-4127","question":"How does Kubernetes handle pod scheduling when multiple pods have different priority classes, and what happens to lower-priority pods when cluster resources become scarce?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["kubernetes","pod-scheduling","priority-classes","preemption"],"companies":[]},{"id":"q-4596","question":"How do Kubernetes custom schedulers differ from the default scheduler, and in what scenarios would you implement a custom scheduler for specific workload requirements?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["custom-schedulers","kube-scheduler","workload-optimization","scheduler-plugins"],"companies":[]},{"id":"q-640","question":"Design a multi-region CI/CD pipeline for a global SaaS application that must achieve 99.99% uptime with zero-downtime deployments. The pipeline should handle blue-green deployments across 5 regions, implement circuit breakers for regional failures, and maintain data consistency. How would you architect this pipeline and what specific tools and strategies would you use?","channel":"devops","subChannel":"devops","difficulty":"advanced","tags":["multi-region","blue-green","gitops","circuit-breaker","zero-downtime"],"companies":[]},{"id":"q-641","question":"Design a CI/CD pipeline for a monolithic application that needs to be gradually migrated to microservices. How would you structure the pipeline to support both the monolith and new microservices during the transition period, ensuring minimal downtime and feature parity?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["ci-cd","microservices","migration","pipeline-design","monolith"],"companies":[]},{"id":"q-644","question":"How would you design a CI/CD pipeline that implements feature flagging and progressive delivery to enable zero-downtime deployments for a high-traffic e-commerce platform?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["feature-flags","progressive-delivery","zero-downtime","canary-deployment","monitoring"],"companies":[]},{"id":"q-648","question":"How would you implement a custom Kubernetes scheduler to handle specialized workloads like GPU-intensive ML jobs, and what components would need to be modified compared to the default scheduler?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["kubernetes","scheduler","custom-scheduler","gpu","ml-workloads","devops"],"companies":[]},{"id":"q-652","question":"How would you design a CI/CD pipeline that implements progressive delivery with feature flags, ensuring zero-downtime deployments while maintaining data consistency across multiple database services?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["feature-flags","progressive-delivery","database-migration","zero-downtime","blue-green-deployment"],"companies":[]},{"id":"q-654","question":"How would you implement a custom Kubernetes scheduler to handle specific business requirements like cost optimization or geographic placement, and what are the key components you'd need to modify?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["kubernetes","scheduler","custom-scheduler","cost-optimization","devops"],"companies":[]},{"id":"q-655","question":"How would you design a CI/CD pipeline that implements infrastructure-as-code with blue-green deployments while ensuring zero-downtime database schema migrations?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["infrastructure-as-code","blue-green-deployment","database-migration","zero-downtime"],"companies":[]},{"id":"q-656","question":"How would you implement a custom Kubernetes scheduler to prioritize pods based on business criticality levels, and what components would you need to modify or extend?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["kubernetes","custom-scheduler","pod-priority","scheduling-framework","devops"],"companies":[]},{"id":"q-657","question":"How would you design a CI/CD pipeline that implements infrastructure-as-code with immutable infrastructure patterns, ensuring zero-downtime deployments while maintaining compliance and audit trails for a regulated industry?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["infrastructure-as-code","immutable-infrastructure","compliance","zero-downtime","audit-trails"],"companies":[]},{"id":"gh-37","question":"What is Cloud Native Architecture?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["cloud-native","microservices"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-4","question":"What is Docker and how does containerization differ from traditional virtualization in terms of architecture and resource efficiency?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["docker","containers"],"companies":["Amazon","Google","Microsoft","Netflix","PayPal","Uber"]},{"id":"gh-5","question":"Explain the Docker image and container lifecycle, including image layers, copy-on-write, container states, and resource isolation mechanisms?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["docker","containers"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"gh-6","question":"What is a Dockerfile and how does it enable containerized application deployment?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["docker","containers"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Netflix"]},{"id":"q-171","question":"You have a Docker container that keeps crashing and restarting in production. How would you systematically debug this issue without modifying the container image, and what specific Docker commands and monitoring techniques would you use?","channel":"devops","subChannel":"docker","difficulty":"intermediate","tags":["docker","containers"],"companies":["Amazon","Databricks","Google","Microsoft","Netflix","Snowflake"]},{"id":"q-191","question":"What is the purpose of a multi-stage Docker build and how does it reduce final image size?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["dockerfile","compose","multi-stage"],"companies":["Amazon","Capital One","Google","Microsoft","Uber"]},{"id":"q-289","question":"How do you implement multi-stage builds in Docker to optimize image size and security while maintaining build cache efficiency?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["dockerfile","compose","multi-stage"],"companies":["Amazon","Google","Netflix","Spotify","Uber"]},{"id":"q-344","question":"You're deploying a Node.js microservice to production and notice the Docker image is 850MB. How would you optimize it using multi-stage builds, and what are the key trade-offs between image size and build time?","channel":"devops","subChannel":"docker","difficulty":"intermediate","tags":["dockerfile","compose","multi-stage"],"companies":["Aurora","Shopify","Snowflake"]},{"id":"q-354","question":"You need to deploy a Node.js microservice to SAP's production environment. The current Dockerfile is 1.2GB and includes build tools. How would you optimize it using multi-stage builds to reduce the image size under 200MB?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["dockerfile","compose","multi-stage"],"companies":["Apple","Elastic","Sap"]},{"id":"q-670","question":"Given a monorepo with two services: a Node.js API and a Python worker, design multi-stage Dockerfiles to produce minimal production images, using BuildKit secrets for API keys at build time without bake-in. Write a docker-compose.yml to build and run both services on a shared network, mount a logs volume, and run as a non-root user. How would you implement end-to-end?","channel":"devops","subChannel":"docker","difficulty":"intermediate","tags":["dockerfile","compose","multi-stage"],"companies":["Anthropic","Google","LinkedIn"]},{"id":"q-671","question":"Given a minimal FastAPI app (main.py) with requirements.txt, write a 2-stage Dockerfile to build and run it in a slim final image. Ensure the app runs as a non-root user, and the runtime image only contains Python and the app. Create a docker-compose.yml that starts the web service and a Redis cache, exposes port 8000, and reads config from .env. How would you verify the image size and run locally?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["dockerfile","compose","multi-stage"],"companies":["Apple","Meta","Salesforce"]},{"id":"gh-27","question":"Design a Git-based collaboration system for a 50-person distributed team. How would you implement branching strategies, conflict resolution, and CI/CD integration to ensure 99.9% uptime while handling 1000+ daily commits?","channel":"devops","subChannel":"gitops","difficulty":"advanced","tags":["git","vcs"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"gh-28","question":"What is Git Branching Strategy?","channel":"devops","subChannel":"gitops","difficulty":"beginner","tags":["git","vcs"],"companies":["Amazon","Goldman Sachs","Google","Meta","Microsoft"]},{"id":"gh-53","question":"What is GitOps and how does it work in practice?","channel":"devops","subChannel":"gitops","difficulty":"beginner","tags":["automation","tools"],"companies":["Amazon Web Services","Gitlab","Google","Microsoft","Netflix"]},{"id":"gh-54","question":"What is ArgoCD and how does it implement GitOps for Kubernetes deployments?","channel":"devops","subChannel":"gitops","difficulty":"beginner","tags":["automation","tools"],"companies":["Amazon","Google","Hashicorp","IBM","Microsoft","Netflix"]},{"id":"q-217","question":"How would you design a GitOps multi-cluster deployment strategy using ArgoCD that handles blue-green deployments with zero-downtime rollback across 50+ clusters while maintaining state consistency?","channel":"devops","subChannel":"gitops","difficulty":"advanced","tags":["argocd","flux","declarative"],"companies":["Amazon","Google","Microsoft","Red Hat","Uber"]},{"id":"q-366","question":"How would you design a GitOps workflow using ArgoCD to deploy a microservices application across multiple environments?","channel":"devops","subChannel":"gitops","difficulty":"intermediate","tags":["gitops","argocd","kubernetes","deployment","automation"],"companies":["Amazon","Google","Meta"]},{"id":"q-429","question":"You're setting up GitOps for a microservices deployment. How would you configure ArgoCD to automatically sync changes from your Git repository to Kubernetes, and what's the difference between declarative and imperative approaches in this context?","channel":"devops","subChannel":"gitops","difficulty":"beginner","tags":["argocd","flux","declarative"],"companies":["Amazon","DoorDash","Google","Hashicorp","Lyft","Microsoft","Netflix"]},{"id":"q-547","question":"You're implementing GitOps for a microservices application. How would you configure ArgoCD to automatically sync changes from your Git repository to Kubernetes, and what would you set as the sync policy to ensure safe deployments?","channel":"devops","subChannel":"gitops","difficulty":"beginner","tags":["argocd","flux","declarative"],"companies":["IBM","NVIDIA","Tesla"]},{"id":"q-668","question":"Describe a practical, automated secret rotation flow in a multi-cluster GitOps setup using ArgoCD and Flux. Include how Vault, ExternalSecret/SealedSecret, and per-cluster Secrets interact, what triggers rotation, how drift is prevented, and how rollback/auditing is handled across clusters?","channel":"devops","subChannel":"gitops","difficulty":"advanced","tags":["argocd","flux","declarative"],"companies":["Apple","Plaid","Zoom"]},{"id":"q-669","question":"You manage a single service deployed to Kubernetes with declarative manifests stored in git. Using both Argo CD and Flux in a GitOps pipeline, describe a practical beginner-friendly approach to implement blue-green or canary deployment, including the minimal YAML you'd configure in Argo CD to promote from staging to prod, and how you'd handle secrets?","channel":"devops","subChannel":"gitops","difficulty":"beginner","tags":["argocd","flux","declarative"],"companies":["Amazon","DoorDash","Google"]},{"id":"q-633","question":"Design a CI/CD pipeline for a microservices application with 10 services, where each service has its own repository. The pipeline must support parallel deployments, canary releases, and automatic rollback on failure. How would you structure the pipeline and what tools would you use?","channel":"devops","subChannel":"kubernetes-devops","difficulty":"advanced","tags":["CI/CD","microservices","kubernetes","gitops","canary-deployments"],"companies":["Google","Netflix","Uber","Spotify","Airbnb"]},{"id":"q-600","question":"Design a CI/CD pipeline for a microservices application that includes automated testing, security scanning, and deployment to multiple environments (dev, staging, prod). What are the key components and how would you ensure zero-downtime deployments?","channel":"devops","subChannel":"pipeline-architecture","difficulty":"intermediate","tags":["CI/CD","microservices","docker","kubernetes","security","zero-downtime"],"companies":["Google","Amazon","Microsoft","Netflix","Uber","Spotify"]},{"id":"q-602","question":"Design a CI/CD pipeline for a microservices application with the following requirements: automated testing, containerization, blue-green deployment, and rollback capabilities. What tools and stages would you include?","channel":"devops","subChannel":"pipeline-architecture","difficulty":"intermediate","tags":["CI/CD","microservices","kubernetes","docker","devops"],"companies":["Google","Amazon","Microsoft","Netflix","Uber","Spotify"]},{"id":"q-613","question":"Design a CI/CD pipeline for a microservices application with 10 services. How would you handle deployment strategies, testing, and rollback mechanisms?","channel":"devops","subChannel":"pipeline-architecture","difficulty":"intermediate","tags":["CI/CD","microservices","kubernetes","docker","deployment"],"companies":["Google","Amazon","Microsoft","Netflix","Uber","Spotify"]},{"id":"q-629","question":"Design a CI/CD pipeline for a microservices application that includes automated testing, security scanning, and multi-environment deployments. What components would you include and how would you structure the pipeline stages?","channel":"devops","subChannel":"pipeline-architecture","difficulty":"intermediate","tags":["cicd","microservices","devops","automation","security"],"companies":["Amazon","Google","Microsoft","Netflix","Spotify"]},{"id":"q-594","question":"How does Kubernetes decide which node to schedule a pod on, and what factors can influence this decision?","channel":"devops","subChannel":"pod-scheduling","difficulty":"intermediate","tags":["kubernetes","scheduling","kube-scheduler","node-selection","resource-management"],"companies":["Google","Amazon","Microsoft","Red Hat","VMware","IBM"]},{"id":"q-608","question":"How does Kubernetes handle pod scheduling when a node becomes resource-constrained, and what mechanisms can be used to ensure critical pods remain running?","channel":"devops","subChannel":"pod-scheduling","difficulty":"intermediate","tags":["kubernetes","scheduling","resource-management","priority","preemption"],"companies":["Google","Netflix","Uber","Amazon","Microsoft"]},{"id":"q-638","question":"Explain how Kubernetes scheduler decides which node to place a pod on, and what factors can cause a pod to remain in a Pending state?","channel":"devops","subChannel":"pod-scheduling","difficulty":"intermediate","tags":["kubernetes","scheduling","pod-management","troubleshooting","resource-allocation"],"companies":["Google","Amazon","Microsoft","Red Hat","VMware"]},{"id":"q-635","question":"How does Kubernetes handle pod scheduling when a node becomes unavailable, and what mechanisms ensure high availability?","channel":"devops","subChannel":"pod-scheduling-failover","difficulty":"intermediate","tags":["kubernetes","scheduling","high-availability","pod-management","failover"],"companies":["Google","Amazon","Microsoft","Red Hat","VMware","IBM"]},{"id":"q-620","question":"Explain the difference between node affinity, node selectors, and taints/tolerations in Kubernetes pod scheduling. When would you use each?","channel":"devops","subChannel":"scheduling-mechanisms","difficulty":"intermediate","tags":["kubernetes","pod-scheduling","node-affinity","taints-tolerations","devops"],"companies":["Google","Amazon","Microsoft","Netflix","Uber"]},{"id":"q-1392","question":"In a three-node Docker Swarm spanning two data centers, deploy a stateless API across an overlay network and ensure zero-downtime upgrades via canary, using update_config (start-first, parallelism 1). Outline the exact sequence of commands to init/join the swarm, create the overlay, deploy the service with update settings, and perform a canary upgrade with health checks. Include a minimal docker-compose snippet showing update_config?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Airbnb","Cloudflare","Two Sigma"]},{"id":"q-1412","question":"Scenario: you’re building a beginner-friendly Docker Compose setup for a FastAPI microservice with PostgreSQL on a single host. Create Dockerfiles for the app and an init script, plus a docker-compose.yml with a named volume for Postgres data, healthchecks, and a startup script that waits for PostgreSQL on port 5432 before starting the app. Explain the exact files, commands, and deployment sequence?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Hugging Face","IBM","Stripe"]},{"id":"q-1443","question":"In a 3-node Swarm across DC-A and DC-B, deploy a stateless API via overlay api-net with DC-affinity (2 replicas in DC-A and 1 in DC-B). Outline the exact CLI steps to init/join, create the overlay, and deploy two services with update_config (order: start-first, parallelism: 1). Describe a canary upgrade process that gradually updates replicas across DCs and validates with health checks. Include a minimal docker-compose snippet showing the two services, the overlay network, and update_config?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Amazon","Netflix"]},{"id":"q-1530","question":"In a three-node Docker Swarm spanning two data centers, deploy a stateless API with TLS termination that uses Vault for dynamic TLS certificate rotation. Use Docker secrets to distribute certs, and implement a lightweight sidecar that refreshes certificates without dropping connections. Configure a canary-style rolling upgrade ensuring zero-downtime during cert rotations. Outline the exact steps: swarm init/join, overlay network creation, stack/deploy with secret handling, and the certificate rotation workflow with health checks?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Amazon","Meta"]},{"id":"q-1579","question":"Design a secret-rotation workflow for a Docker Swarm stack that uses Vault to rotate a TLS cert and a database credential with zero downtime. Use Swarm secrets and a rolling update (start-first). Outline exact steps: swarm init/join, overlay network, create secrets, deploy stack, Vault rotate trigger, service update commands. Include a minimal docker-compose snippet showing secret usage and update_config?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Bloomberg","Databricks","Hashicorp"]},{"id":"q-1628","question":"Scenario: In a 3-node Swarm across two DCs, deploy an API service behind Traefik with image signing enforcement (cosign/DOCKER_CONTENT_TRUST) and implement a canary upgrade to v2 with a 10% traffic split via a separate api-v2-canary service. Outline exact commands, Swarm update_config usage, and docker-compose/service definitions to achieve zero-downtime upgrade and safe rollback?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Cloudflare","IBM","MongoDB"]},{"id":"q-1653","question":"Scenario: On a single host, implement a beginner-friendly Docker Compose stack: a Node.js API that talks to Redis and a Fluent Bit logging service that reads logs from the API via a shared volume and forwards them to stdout. Provide a Dockerfile for the API, a docker-compose.yml with api, redis, fluent-bit, a logs volume, and healthchecks; explain the run steps and verification?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Hugging Face","Instacart","Salesforce"]},{"id":"q-1688","question":"Beginner-level: design a local docker-compose stack for a Node.js API that uses Redis as a cache. Provide a Dockerfile for the API, a startup script that waits for Redis to be reachable on 6379 before starting, and a docker-compose.yml with healthchecks for both services on a shared network. Include exact file contents or minimal snippets, the commands to build and run, and how to validate a cache-hit endpoint?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["DoorDash","MongoDB","Scale Ai"]},{"id":"q-1760","question":"In a three-node Docker Swarm spanning two data centers, introduce a new internal auth service that all APIs depend on. Roll it out with zero downtime and a canary, using update_config (start-first, parallelism 1), Docker Secrets, and a routing shim. Provide exact commands to init/join the swarm, create the overlay, deploy the stack, add the secret, and perform the canary upgrade with health checks?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["LinkedIn","Tesla"]},{"id":"q-1786","question":"In a 3-node Docker Swarm spanning two data centers, deploy a GPU-accelerated model-serving API (TorchServe) using the NVIDIA runtime. Expose it behind an internal overlay network and a simple LB. Ensure zero-downtime upgrades with canary traffic shifts and a pre-warm sidecar to warm the new replica without serving traffic. Outline the steps for swarm init/join, overlay creation, service spec with GPU constraints, secret handling, and a separate migration container if needed. Include a minimal docker-compose snippet showing update_config (start-first, parallelism 1)?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Databricks","Google"]},{"id":"q-1832","question":"Design and implement a zero-downtime upgrade for a 4-node Docker Swarm across two data centers hosting a stateful Redis queue and a stateless worker service. The worker consumes messages encoded in a new proto format; the cluster uses TLS mutual authentication with Vault for cert rotation and Docker Secrets for credentials. Outline exact upgrade steps, a 10% canary rollout, health checks, and a rollback plan, and include a minimal docker-compose snippet showing update_config(order: start-first, parallelism: 1)?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Amazon","Two Sigma","Zoom"]},{"id":"q-1967","question":"In a production Swarm across two data centers with four nodes, implement end-to-end image provenance using Cosign. Sign CI artifacts, publish signatures to a registry, and enforce verification before deploys. Outline the exact steps: key management, signing workflow in CI, signature verification at pull-time, updating services with image digests, and a rollback plan if verification fails. Provide concrete Cosign commands and how to incorporate into a CI/CD pipeline?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Amazon","DoorDash","Microsoft"]},{"id":"q-2016","question":"In a three-node Docker Swarm spanning two data centers, deploy a stateless API behind a Traefik ingress with a Redis-backed cache layer and mutual TLS between services. Use Docker secrets/configs for TLS certs and cache creds. Apply zero-downtime upgrades via update_config (start-first, parallelism 1) and implement a canary upgrade path with health checks and automatic rollback. Outline the exact init/join commands, overlay creation, stack file, and upgrade sequence?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Discord","Twitter"]},{"id":"q-2022","question":"Scenario: On a single host, implement a blue-green deployment for a stateless API behind an Nginx reverse proxy using docker-compose. Start with green (v1) receiving all traffic; deploy blue (v2) and switch traffic to blue only after a 30-second health-check window confirms readiness, ensuring zero downtime. Provide: a) docker-compose.yml with two API services (api-green and api-blue) and Nginx, b) nginx.conf with a switchable upstream, c) a Bash script upgrade.sh that promotes blue by reconfiguring upstream and reloading Nginx, d) exact commands to bring up green, deploy blue, run the upgrade, and verify with curl?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Adobe","Hashicorp","Snowflake"]},{"id":"q-2053","question":"In a four-node **Docker Swarm** spanning two data centers, deploy a stateless API with a Redis-backed rate limiter and a shared cache. Implement a canary upgrade of the API using update_config (start-first, parallelism 1), with a front-door that routes the canary subset using label-based routing. Outline exact commands: swarm init/join, overlay creation, stack deploy, and the health-check-driven promotion. Include a minimal docker-compose snippet showing update_config?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Instacart","Meta"]},{"id":"q-2120","question":"In a two-node Swarm, deploy a TLS-secured stateless API behind an Nginx proxy. Use Docker secrets for TLS certs, a Docker config for API settings, and a small sidecar that ships logs without affecting requests. Attach both services to a single overlay network and perform a canary upgrade with a rolling update (start-first, parallelism 1). Provide exact commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Netflix","Salesforce"]},{"id":"q-2140","question":"In a two-datacenter Docker Swarm (2 nodes per DC), deploy a stateless API behind Traefik with TLS termination on an overlay network across DCs. Implement a canary rollout for a feature-flag change using a Swarm Config and route 10% of traffic to canary via Traefik labels. Outline exact swarm init/join commands, overlay creation, stack deploys, config creation, and the canary upgrade with health checks and rollback criteria?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Apple","Meta"]},{"id":"q-2231","question":"In a four-node docker-dca cluster, deploy a TLS-secured stateless API behind an Nginx edge proxy. Use Docker secrets for TLS certs and a Docker config for API endpoints, and add an Envoy sidecar per app to drive traffic-splitting controlled by a central Consul KV flag. Implement a progressive canary: 10% steps every 30s up to 100%, with health checks and automatic rollback on failure. Provide explicit bootstrap commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Discord","Microsoft","Plaid"]},{"id":"q-2254","question":"Create a local Docker Compose setup with two Python services: web (Flask API) and cache (Redis). Duplicate web as web_canary with CANARY=true. Add an Nginx container as a reverse proxy routing /api to web and /canary to web_canary. Use a named volume for Redis data. Add healthchecks for all three containers and provide a docker-compose.yml skeleton plus exact bootstrap commands to seed data and verify endpoints?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Amazon","Robinhood","Snowflake"]},{"id":"q-2295","question":"In a 5-node Docker Swarm spanning two data centers, deploy a TLS-secured stateless API behind an Nginx edge proxy. Use Docker secrets for TLS certs and a Docker config for per-tenant routing templates, consuming routing directives from a central HTTP API. Attach a per-service Envoy sidecar to drive traffic-splitting with a progressive canary (25% steps every 20s). Ship logs via a separate sidecar to a Loki stack without blocking requests. Implement a zero-downtime upgrade using update_config (start-first, parallelism 1). Provide exact commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Slack","Tesla","Uber"]},{"id":"q-2346","question":"In a 3-node docker-dca cluster spanning two data centers, deploy a TLS-secured stateless API behind an Nginx proxy. Use Docker secrets for TLS certs and a Docker config for API endpoints. Add a shadow sidecar that mirrors 5% of production traffic to a canary version. Implement a canary rollout by adjusting weights in 10% increments using update_config, with health checks and automatic rollback on failure. Provide exact commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Anthropic","Google","NVIDIA"]},{"id":"q-2386","question":"In a two-datacenter Docker Swarm with a TLS-enabled REST API behind an Nginx edge proxy, TLS certs are stored as Docker secrets. Design a zero-downtime rotation workflow: publish a new cert secret api_tls_new without restarting Nginx, upgrade the service with the new secret using a canary approach (update_config: order start-first, parallelism 1), and provide the exact shell commands plus a minimal docker-compose.yml skeleton showing secrets, config, and a small log-shipper sidecar that does not delay requests?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["DoorDash","Meta","MongoDB"]},{"id":"q-2432","question":"In a 4-node docker-dca Swarm, deploy a TLS-secured event-processing pipeline behind an Nginx edge proxy. Use Docker secrets for TLS certs, a Docker config for routing rules, and implement a 5% canary upgrade for the Transform service controlled by a Consul KV flag. Provide exact commands and a docker-compose.yml skeleton that demonstrates canary traffic, health checks, and zero-downtime upgrades?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Goldman Sachs","Robinhood","Snowflake"]},{"id":"q-2485","question":"In a three-node Docker Swarm spanning two data centers, deploy a TLS-secured stateless event API behind an Nginx edge proxy. Use Docker secrets for TLS certs and a Docker config for API endpoints. Introduce an SPIRE-based mTLS mesh by wiring an SPIRE agent as a sidecar to each app container to issue short-lived mTLS certs and enforce SPIFFE IDs. Implement automatic certificate rotation every 60 minutes and a progressive canary upgrade (start-first, parallelism 1). Provide exact docker-compose.yml skeleton and bootstrap commands?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Anthropic","Citadel","Google"]},{"id":"q-2558","question":"In a two-node docker-dca Swarm, deploy a TLS-enabled stateless API behind an Nginx proxy. Use Docker secrets for TLS certs, a Docker config for API endpoints, and a lightweight log-shipper sidecar that adds no latency. Attach both services to a single overlay network. Implement a 5% canary rollout with health checks and automatic rollback on failure. Provide exact commands and a docker-compose.yml skeleton, plus how to measure and enforce performance during the rollout?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Apple","DoorDash","Meta"]},{"id":"q-2588","question":"On a two-node Docker Swarm, deploy a TLS-secured stateless API behind an Nginx edge proxy. Use a Docker secret for TLS certs and a Docker config for API endpoints. Ensure the app runs as a non-root user and add a tiny sidecar that tails logs to a shared volume without affecting requests. Attach both services to an overlay network and implement a rolling update with start_first, parallelism 1. Include exact commands to initialize swarm, create secret/config, build/deploy, and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Adobe","Salesforce","Zoom"]},{"id":"q-2829","question":"In a two-node docker-dca Swarm, deploy a TLS-enabled stateless API behind an Nginx edge proxy. Add a shadow version of the API and use traffic mirroring to send 5% of live traffic to the shadow for black-box testing without affecting live latency. Use Docker secrets for TLS certs and a Docker config for API endpoints. Ensure health checks and automatic rollback if the shadow underperforms. Provide exact commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Google","Plaid","Scale Ai"]},{"id":"q-2918","question":"In a two-node docker-dca Swarm, deploy a TLS-enabled stateless API behind an Nginx edge proxy. Use Docker secrets for TLS certs and a Docker config to store per-tenant ACLs. Add an Open Policy Agent (OPA) sidecar to enforce per-request authorization, with policy loaded from a Docker config. Include a separate log-collector sidecar that writes audit logs to a shared volume. Implement a 20% canary rollout every 15 seconds with health checks and automatic rollback on policy violation. Provide exact commands and a minimal docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Apple","Robinhood"]},{"id":"q-2951","question":"On a three-node docker-dca Swarm, deploy a TLS-enabled API behind an Nginx TLS-terminating proxy with mutual TLS to the API, using Vault PKI to issue short-lived certs and a sidecar that auto-refreshes certs into a shared volume; implement a 25% canary rollout every 20s with health checks and auto-rollback. Include exact swarm bootstrap commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Google","Oracle","Slack"]},{"id":"q-3052","question":"In a two-node docker-dca Swarm, deploy a TLS-enabled stateless API behind Nginx. Use Docker secrets for TLS certs and a Docker Config named flag.json mounted at /config/flag.json. Implement a simple feature toggle in the API that reads the flag from /config/flag.json and supports hot reload via SIGHUP without restarting. Add a progressive canary rollout: enable the feature in 0%, 25%, 50%, 75%, 100% in 20-second steps with health checks and automatic rollback on failure. Provide exact docker commands and a docker-compose.yml skeleton that demonstrates the rollout. Also show how to verify latency impact during each step?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Databricks","Meta"]},{"id":"q-3119","question":"In a two-node docker-dca Swarm spanning two data centers, deploy a TLS-enabled stateless API behind an Nginx edge proxy. Use Docker secrets for TLS certs and a Docker config for API endpoints. Attach both services to a single overlay network. Add an Envoy sidecar to the API service to perform region-aware traffic-splitting driven by a global feature flag stored in Consul KV at /features/new-api. Implement a progressive canary: 20% traffic to v2 until the flag is 1, with health checks and automatic rollback on failure. Provide exact commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Microsoft","Oracle","Stripe"]},{"id":"q-3162","question":"In a two-node docker-dca Swarm, deploy a TLS-enabled stateless API behind an Nginx edge proxy. Use Docker secrets for TLS certs, a Docker config for API endpoints, and a lightweight traffic-controller container that adjusts canary weights by updating a shared Nginx config mounted as a Docker config. Route 10% traffic to v2 initially, then scale to 100% per 15-second steps based on p95 latency measured via a small health-check endpoint; auto rollback to previous weight if latency exceeds 250ms for two consecutive checks. Provide exact docker commands and a docker-compose.yml skeleton, plus how to measure latency and trigger rollback?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Bloomberg","Goldman Sachs","MongoDB"]},{"id":"q-3283","question":"In a three-node docker-dca cluster, deploy a TLS-enabled API behind an Nginx gateway using mutual TLS. Secrets: CA and API certs as Docker secrets; a Vault-backed secret-provider sidecar rotates short-lived MTLS creds with zero latency. Add a lightweight tracing sidecar to ship spans non-blockingly. All services on one overlay network. Implement zero-downtime rotation with health checks and automatic rollback on failure. Provide docker-compose skeleton and bootstrap steps?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["LinkedIn","Scale Ai"]},{"id":"q-3297","question":"In a two-node docker-dca Swarm, deploy a TLS-enabled stateless API behind an Nginx proxy. Instrument the API with OpenTelemetry and run an OpenTelemetry Collector as a sidecar in each service, exporting traces to a Jaeger backend running on the swarm. Use Docker secrets for TLS certs and a Docker config for API endpoints. Attach both services to a single overlay network. Set 50% sampling; verify traces in Jaeger; ensure per-request latency overhead stays under 2 ms. Provide exact docker-compose.yml skeleton and minimal OTEL/Jaeger config?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Airbnb","Microsoft"]},{"id":"q-3395","question":"**Advanced Docker DCA Challenge**: In a three-node docker-dca cluster across two data centers, deploy a TLS-enabled, stateful API behind an Nginx edge proxy. Use Docker secrets for TLS, a Docker config for API endpoints, and a sidecar that ships logs to a central ELK stack with zero latency. Implement blue-green deploy with health checks and automatic rollback; include exact commands and a minimal docker-compose.yml, plus how to measure performance during rollout?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Discord","NVIDIA","Scale Ai"]},{"id":"q-3425","question":"In a three-node docker-dca cluster spanning on-prem and cloud, implement a TLS-enabled, multi-tenant API gateway behind an Nginx edge. Use one TLS secret per tenant, per-tenant rate limits via a small config mounted as a Docker config, and a Redis-backed counter. Add a canary rollout for a new per-tenant routing rule driven by a Consul KV flag, with health checks and automatic rollback on failure. Provide exact commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Google","Snowflake","Tesla"]},{"id":"q-3592","question":"In a docker-dca Swarm across two data centers, add end-to-end distributed tracing to a TLS-enabled API behind an Nginx proxy. Deploy an OpenTelemetry Collector as a sidecar on the API service and a central collector, propagate W3C trace context, configure sampling via Docker config, and export via OTLP to Jaeger. Measure and keep overhead under 1 ms per request; provide commands and a docker-compose skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["LinkedIn","Two Sigma","Uber"]},{"id":"q-3762","question":"In a three-node docker-dca Swarm, deploy a TLS-enabled stateless API behind an Nginx proxy. Use Docker secrets for TLS certs, a Docker config for API endpoints, and add a small compliance sidecar that streams headers and audit data to a central collector via UDP without affecting request latency. Attach services to a single overlay network and implement a progressive canary that also validates a policy flag (X-Policy: strict) in 5% increments every 20s with health checks and automatic rollback on failure. Provide exact commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Lyft","Salesforce"]},{"id":"q-3803","question":"In a three-node docker-dca cluster across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy. Use Vault to issue SPIFFE SVIDs, and a sidecar that validates them and enforces an OPA policy loaded via a Docker Config; include a lightweight log-shipper. Attach to a single overlay network. Implement a 20% canary rollout with latency/errors thresholds and automatic rollback. Provide exact commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["MongoDB","Slack"]},{"id":"q-3933","question":"In a two-node docker-dca Swarm, deploy a TLS-enabled REST API behind Traefik as the edge proxy. Configure Traefik's ACME to obtain TLS certificates from Let's Encrypt in staging. Attach all services to a single overlay network. Implement a 15% canary rollout of a new /status/v2 endpoint (v1 remains) using Traefik's weighted routing with health checks, automatic rollback on failures. Add a minimal OpenTelemetry tracing sidecar exporting to stdout; ensure trace headers propagate?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["MongoDB","Scale Ai","Snowflake"]},{"id":"q-3944","question":"In a three-node docker-dca Swarm within a single data center, deploy a TLS-enabled API behind an Nginx edge proxy. Use Docker Secrets for TLS certificates and a Docker Config to supply a canary routing rule. Add a lightweight log-shipper as a sidecar and attach both services to a single overlay network. Implement a 15% canary rollout of a new API image with automatic rollback on failed health checks. Provide exact commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Apple","Meta"]},{"id":"q-3981","question":"In a four-node docker-dca cluster across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy. Use Vault to issue SPIFFE SVIDs; attach an Envoy sidecar per app to validate them and enforce tenant RBAC via OPA policies stored in Consul KV (refreshed via webhook). Attach all to one overlay network. Implement a progressive canary rollout of 15% steps every 30s with latency >200ms or error rate >2% triggering rollback. Provide exact commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Plaid","Square","Uber"]},{"id":"q-4237","question":"Three-node docker-dca Swarm across two data centers; deploy a TLS-enabled API behind an Nginx edge proxy. Enforce a Cosign-based supply-chain policy: only attested images for API and sidecars; add a gate sidecar that verifies attestations at startup with Vault keys. Attach to a single overlay network. Implement a 10% canary rollout with health checks and automatic rollback. Provide exact commands and docker-compose skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Databricks","Netflix","Plaid"]},{"id":"q-4268","question":"In a two-node docker-dca cluster, deploy a TLS-enabled API behind an Nginx edge proxy. Use Redis as a feature flag to roll a 20% canary to a v2 endpoint, controlled by flag feature.canary.v2. Implement latency and error-rate thresholds (latency <300ms, error rate <5%) with automatic rollback. Provide exact commands and a docker-compose.yml skeleton to implement this setup?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Lyft","Snap"]},{"id":"q-4345","question":"In a three-node docker-dca cluster across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy. Enforce image provenance with Docker Content Trust (Notary) by signing base images and validating signatures in a dedicated 'sigver' sidecar before the request reaches the app. Use Vault to issue SPIFFE SVIDs; enforce an OPA policy loaded via a Docker Config to gate access. Implement a 20% canary rollout with latency and error-rate thresholds and automatic rollback. Provide exact commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Airbnb","LinkedIn","Snowflake"]},{"id":"q-4384","question":"In a three-node docker-dca cluster across two data centers, implement a per-tenant dynamic rate limiter as an Envoy-sidecar attached to the API container. The limiter reads quotas from Vault KV, caches in Redis, and uses a token-bucket or sliding-window approach. Edge TLS is terminated by Nginx; tenants are identified by a JWT in Authorization header validated by Vault OIDC. Provide exact commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Adobe","OpenAI","Oracle"]},{"id":"q-4491","question":"In a two-node docker-dca cluster, deploy a TLS-enabled API behind an Nginx edge proxy. Implement 10% canary routing to a v2 image using a Lua-based split controlled by a Docker Config flag. Enforce image provenance with cosign; reject unsigned images at the edge. Provide exact commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Netflix","Scale Ai","Twitter"]},{"id":"q-4507","question":"In a two-node docker-dca Swarm, deploy a TLS-enabled API behind an Nginx edge proxy and enforce mutual TLS (mTLS) with client certificates issued by a private CA. Store server and CA certs as Docker secrets, configure Nginx to require and validate client certs, and ensure the API receives the client identity via a header. Provide exact docker-compose.yml skeleton and the certificate generation steps, plus test commands?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Coinbase","Salesforce"]},{"id":"q-4640","question":"Across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy in a docker-dca cluster. Enforce mTLS between services with SPIFFE SVIDs issued by Vault, and attach a sidecar that validates the SVIDs and enforces an OPA policy loaded via Docker Config for per-tenant feature gates. Implement a canary rollout with traffic shifting and automatic rollback. Include exact bootstrap commands and a minimal docker-compose skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Google","Robinhood"]},{"id":"q-4660","question":"In a two-data-center docker-dca cluster, design a cross-tenant image attestation workflow that runs before deployment: an attest sidecar validates base image digests against an external Notary v2 attestation service, with tenant policies stored in an OPA Docker Config and enforced by the edge proxy. Implement a 10% canary rollout with latency budgets and automatic rollback. Provide exact commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Oracle","Plaid","Slack"]},{"id":"q-856","question":"You're running a Docker Swarm with services frontend, api, and worker. A feature-flag config is provided via Docker Config mounted at /etc/flags.json in all containers. You must rotate this config weekly with zero downtime. Describe the exact sequence of commands to create a new config version, rotate the services to use it, and implement a graceful reload inside apps so the new flags are picked up without losing requests. Include any Swarm update options you would tune?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Hashicorp","Snowflake"]},{"id":"q-862","question":"In a Docker Swarm with a stateful web app that uses Postgres, you must roll out version 3.2 with a DB schema migration and zero downtime. Propose a concrete upgrade plan that uses a start-first, one-task-at-a-time update, a separate migration container, and post-migration validation. Include exact Swarm commands and a minimal docker-compose snippet showing update_config?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["IBM","PayPal"]},{"id":"q-1262","question":"You're given an array A of length n and an integer k. You must select exactly k non-overlapping, contiguous subarrays (non-empty) to maximize the sum of all selected elements. Return the maximum sum and the k subarrays (start and end indices). Propose a dynamic programming formulation with states and recurrences, include reconstruction, and discuss time/space complexity?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Hugging Face","Snap"]},{"id":"q-1872","question":"You're given an n x m grid of integers grid[i][j]. From (0,0) to (n-1,m-1) you may move only right or down. You may turn at most K times (a turn is a change between directions). Return the maximum sum along a valid path and the path coordinates. Propose a DP with states dp[i][j][t][dir], recurrences, base cases, and reconstruction, and discuss time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["LinkedIn","MongoDB","Slack"]},{"id":"q-1941","question":"You're given an array A of length n and an integer k. You must select at most k elements such that no two selected elements are adjacent. Return the maximum sum you can obtain and the list of selected indices. Propose a DP formulation with states dp[i][j], recurrences, base cases, and reconstruction?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Apple","Instacart","Meta"]},{"id":"q-2170","question":"You're given an array values[0..n-1] of positive integers. Two players take turns removing either the leftmost or rightmost value and add it to their score. Assuming both play optimally, return A's maximum guaranteed total score and the move sequence (L/R) for each turn. Provide a DP formulation with dp[i][j] as the maximum score difference for subarray i..j, the base cases, and how to reconstruct the moves?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Apple","Lyft","Tesla"]},{"id":"q-2405","question":"You're given an array A[0..n-1], and integers k, L, R. You must select exactly k non-overlapping contiguous subarrays such that each subarray's length is between L and R (inclusive). The sum of all elements in the selected subarrays is maximized. Return the maximum sum and the k subarrays (start and end indices). Provide a DP formulation with states, recurrences, base cases, and reconstruction, and discuss time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Lyft","MongoDB"]},{"id":"q-2434","question":"You're given an array A of length n and an integer k. Choose exactly k non-empty, non-overlapping subarrays. The score of a subarray [l..r] is sum(A[l..r]) - min(A[l..r]). Return the maximum total score and the list of subarray endpoints (l, r). Propose a DP formulation: dp[i][t] = max_{p in [t-1..i-1]} dp[p][t-1] + sum(A[p+1..i]) - min(A[p+1..i]). Base: dp[i][1] = sum(A[1..i]) - min(A[1..i]). Include reconstruction, base cases, and time/space complexity?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Google","Lyft","MongoDB"]},{"id":"q-2815","question":"You're given a rooted tree with n nodes, each node i has a value val[i]. You must choose a connected subgraph containing exactly k nodes to maximize the sum of values. Return the maximum sum and one such set of nodes. Propose a DP: dp[u][s] = maximum sum of a connected subgraph of size s that lies within the subtree of u and includes u. Explain the recurrence and reconstruction, and discuss time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Amazon","Google","Tesla"]},{"id":"q-2880","question":"You're given an array A of length n and an integer m (1 <= m <= n). You must select exactly m indices i1 < i2 < ... < im to form a subsequence. The cost is the sum over k=2..m of |A[ik] - A[ik-1]|. Return the minimum possible cost and the corresponding indices. Propose a DP formulation with states dp[i][t], recurrences, base cases, and reconstruction, and discuss time/space complexity and potential optimizations?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Citadel","Netflix"]},{"id":"q-2886","question":"You're given an array A of length n and an integer k. Find a non-contiguous subsequence of length exactly k that is non-decreasing (A[i1] <= A[i2] <= ... <= A[ik]), maximizing the sum of its elements. Return the maximum sum and the indices of the chosen elements. Propose a DP: dp[i][t] = max sum of a non-decreasing subsequence of length t ending at i. Recurrence: dp[i][1] = A[i]; dp[i][t] = A[i] + max_{j<i, A[j] <= A[i]} dp[j][t-1]. Include reconstruction, base cases, and time/space complexity?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Meta","Snap","Uber"]},{"id":"q-3070","question":"You're given an array A of length n and an integer d. You may select a subsequence (not necessarily contiguous) such that for every pair of consecutive chosen elements A[i] and A[j] with i<j, |A[i]-A[j]| <= d. Maximize the sum of the chosen elements. Return the maximum sum and one valid subsequence of indices. Provide the DP formulation, reconstruction approach, and discuss time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Apple","Coinbase","Google"]},{"id":"q-3196","question":"You're given an array A of length n and an integer k. Partition A into exactly k non-empty contiguous subarrays. The score of a subarray [l..r] is the sum of |A[i+1]-A[i]| for i in [l..r-1]. Return the maximum total score and the partition endpoints (end indices). Propose a DP: dp[i][t] = max_{p in [t-1..i-1]} dp[p][t-1] + sum_{q=p+1}^{i-1} |A[q+1]-A[q]|. Base: dp[i][1] = sum_{q=1}^{i-1} |A[q+1]-A[q]|. Explain reconstruction, base cases, and time/space complexity?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Airbnb","NVIDIA","Oracle"]},{"id":"q-3322","question":"You're given an array A of length n and an integer k. Partition A into exactly k non-empty contiguous subarrays. Each subarray's score is the number of distinct elements within that subarray. Return the maximum total score across all partitions and the k subarrays' boundaries (start and end indices). Propose a DP formulation with states dp[i][t], recurrences, base cases, reconstruction, and discuss time/space complexity?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Adobe","Anthropic","Apple"]},{"id":"q-3392","question":"You are given an array A of length n and an integer m. You must select exactly m indices i1 < i2 < ... < im. The total cost is sum_{t=2..m} |A[i_t] - A[i_{t-1}]|. Return the minimum total cost and the indices. Propose a DP formulation with states dp[i][t] representing the minimum cost ending at i as the t-th picked element, base cases, recurrences, how to reconstruct the path, and discuss time/space complexity and possible optimizations?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Coinbase","Meta","Robinhood"]},{"id":"q-3492","question":"You're given an array A of length n and an integer k. Partition A into exactly k non-empty contiguous subarrays. Each subarray's cost is the maximum element within that subarray. Return the minimum total cost and the partition endpoints. Propose a DP formulation with states dp[i][t] representing the minimum cost to partition the first i elements into t subarrays, with recurrence dp[i][t] = min_{p in [t-1..i-1]} dp[p][t-1] + max(A[p+1..i]). Base: dp[i][1] = max(A[1..i]). Describe reconstruction, base cases, and time/space complexity?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Apple","Lyft"]},{"id":"q-3812","question":"You are given a string s of length n over lowercase letters. You may delete characters to obtain a string that strictly alternates between two distinct characters, e.g., abab... or xyxy.... Choose any two distinct characters and delete all other characters. Return the maximum possible length and the pair (a,b) of characters used. Propose a DP to compute, for each ordered pair (a,b), the longest alternating subsequence using only a and b; include base cases, transitions, and how to reconstruct the solution?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Citadel","Google"]},{"id":"q-3829","question":"You're given a DAG with n nodes and m directed edges. Each node i has value val[i]. For a fixed k, find a path with exactly k nodes maximizing the sum of values along the path. Return the max sum and the path as node indices. Provide a DP formulation with states, recurrences, base cases, reconstruction, and complexity notes?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Databricks","Uber"]},{"id":"q-3884","question":"You're given a string s of length n and an integer k (1 <= k <= n). Partition s into exactly k non-empty contiguous substrings. For each substring, define cost as the minimum number of character changes required to make that substring a palindrome (i.e., the number of mismatched symmetric pairs). Return the minimum total cost and the partition boundaries that achieve it. Propose a DP formulation with precomputed costs cost[l][r], state dp[i][t] = min_{p in [t-1..i-1]} dp[p][t-1] + cost[p+1][i], base dp[i][1] = cost[1][i]. Describe reconstruction, base cases, and time/space complexity?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Google","NVIDIA"]},{"id":"q-3940","question":"You're given a string s of length n and an integer k. You may delete at most k characters from s. What is the maximum possible length of a palindromic subsequence in the remaining string? Provide a DP formulation using dp[i][j][d] as the max palindrome length in s[i..j] after exactly d deletions within that subrange. Include recurrences, base cases, reconstruction hints, and time/space complexities. Example: s = \"abca\", k = 1?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["DoorDash","Google","MongoDB"]},{"id":"q-4055","question":"You're given a numeric string s of length n and an integer k. Split s into exactly k non-empty contiguous substrings; interpret each substring as a decimal number with no leading zeros unless the substring is '0'. Minimize the sum of these k numbers. Return the minimum sum and the cut positions (indices after which a cut occurs). Propose a DP formulation with dp[i][t], recurrence, base cases, and reconstruction, plus time/space notes?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["PayPal","Tesla"]},{"id":"q-4189","question":"Grid path with constrained turns. You have an R x C grid with non-negative weights. Start at (0,0) and end at (R-1,C-1). You can move only right or down. You must make exactly k turns (a turn is when you switch from moving right to down or from down to right). Return the minimum path sum, or -1 if impossible. Propose a DP formulation using states dp[i][j][dir][t], with dir in {0,1} representing last move direction, t turns used. Include recurrences, base cases, reconstruction, and time/space analysis?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["IBM","Netflix"]},{"id":"q-4401","question":"You're given a binary string s of length n and an integer k. Partition s into exactly k non-empty contiguous substrings. Each substring's cost is the number of transitions inside it (positions i where s[i] != s[i-1] within the substring). Return the minimum total cost and the k partition end indices. Provide a DP formulation with dp[i][t] as the min cost to split the first i chars into t parts, recurrence dp[i][t] = min_{p in [t-1..i-1]} dp[p][t-1] + cost(p+1,i); base dp[i][1] = cost(1,i). Include reconstruction plan and complexity notes?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Adobe","Instacart","Scale Ai"]},{"id":"q-4506","question":"You're given an array A of length n representing per-minute engagement. You may select exactly k non-overlapping, contiguous time blocks in order (you may skip minutes between blocks). Each block [l..r] yields value min(A[l..r]) * (r-l+1). Return the maximum total value and the endpoints of the k blocks?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Apple","Instacart","Tesla"]},{"id":"q-4604","question":"You're given a grid cost of size n x m. You start at (0,0) and must reach (n-1,m-1). You may move right, down, or diagonally down-right. Each diagonal move consumes one token from a budget of K; you may perform at most K diagonals in the path. What is the minimum total cost to reach the bottom-right, and what is the path (sequence of coordinates) taken? Propose a DP formulation with dp[i][j][t] as the minimum cost to reach (i,j) using exactly t diagonals, including recurrences, base cases, reconstruction, and a discussion of time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Instacart","Snap"]},{"id":"q-4757","question":"You're given a string s of length n and an integer k. You may delete at most k characters to transform s into a palindrome. Return whether this is possible and, if so, the resulting palindrome. Propose a DP with dp[i][j] for minimum deletions on s[i..j], recurrences: if s[i]==s[j] then dp[i][j]=dp[i+1][j-1]; else dp[i][j]=min(dp[i+1][j], dp[i][j-1])+1; base: dp[i][i]=0; dp[i][i-1]=0. Explain reconstruction, time/space, and how to recover the deletions?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["MongoDB","Netflix"]},{"id":"q-679","question":"In a warehouse grid of size n x m, each cell has a traversal cost. You can move only right or down from (0,0) to (n-1,m-1). You have a one-time token to halve the cost of exactly one visited cell. Design an O(nm) DP to compute the minimum path cost after optimally using the discount, and describe the recurrences and a space-optimized implementation?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Anthropic","Snap","Twitter"]},{"id":"q-691","question":"You're planning a delivery route along a straight street of n blocks. At block i you can advance up to jumps[i] blocks (at least 1). How many distinct routes reach block n-1 from block 0? If unreachable, return 0. Propose a dynamic-programming approach with dp[i] as ways to reach i and outline its time/space complexity, edge cases, and a brief correctness justification. How would you implement it?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["DoorDash","Meta","Square"]},{"id":"q-693","question":"You're building a daily workout planner. Over n days, you can pick Light (L) or Heavy (H) workouts, with a cooldown: after a Heavy, you must skip the next two days (no workouts). Given integers n and r, how many length-n sequences contain exactly r Heavy workouts and satisfy the cooldown rule? Provide a DP formulation with state dp[i][c][t] (days processed, cooldown days left, heavies used) and outline time/space complexity, edge cases, and a brief correctness justification?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Citadel","Google","Microsoft"]},{"id":"q-703","question":"You're navigating a warehouse grid of size n x m. Each cell (i, j) has a risk value r[i][j] ≥ 0, where higher means less safe. You may move only right or down from (0,0) to (n-1,m-1). Devise a dynamic-programming solution to minimize the maximum risk encountered on the path (i.e., minimize max(r[i][j]) along the path). Define a suitable dp[i][j], give the recurrence and base cases, describe reconstruction of the path, and analyze time/space complexity. How would you handle blocked cells by setting r[i][j] = INF?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["NVIDIA","Scale Ai","Stripe"]},{"id":"q-716","question":"You're building a text formatter. Given a list of word lengths L = [l1, l2, ..., ln] and a maximum line width W, wrap the words into lines so that all lines except the last incur a penalty of (W - usedWidth)^2, where usedWidth = sum of word lengths on the line plus spaces between words. The last line has 0 penalty. Propose a dynamic programming solution with dp[i] representing the minimum penalty for words i..n-1, specify the recurrence, reconstruction method, and time/space complexity. How would you implement it?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Anthropic","DoorDash","Square"]},{"id":"q-718","question":"You're given an array A[1..n] and an integer k. Partition into exactly k contiguous segments. The cost of a segment [t+1..i] is (sum(A[t+1..i]))^2. Return the minimum total cost and the partition indices. Propose DP: P as prefix sums, dp[i][j] = min_{t in [j-1..i-1]} dp[t][j-1] + (P[i]-P[t])^2, with base dp[0][0]=0. Explain reconstruction and discuss naive vs. optimized time, space, and edge cases?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Bloomberg","Netflix"]},{"id":"q-731","question":"You're given an array A of length n with non-negative integers representing daily story points. You must partition the days into consecutive weeks, each containing 2–7 days. The cost of a week is (sum of that week's points − W)^2 where W is a fixed target. Return the minimum total cost to cover all days or INF if impossible. Propose a DP with dp[i] as min cost for first i days; dp[i] = min_{k=2..7, i-k>=0} dp[i-k] + (sum(i-k+1..i) − W)^2, using prefix sums for O(1) range sums. Reconstruct weeks and analyze time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Databricks","Salesforce","Two Sigma"]},{"id":"q-736","question":"You're given an n x m grid. Each cell (i, j) has a color c[i][j] in [0, C-1] and a non-negative cost w[i][j]. Start at (0,0) and move to (n-1,m-1) with only right or down moves. You must visit at least one cell of every color that appears in the grid along your path. Return the minimum total cost to do so, or -1 if impossible. Describe a DP using dp[i][j][mask] where mask tracks visited colors; explain base cases, transitions from top/left, how to reconstruct the path, and complexity?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Microsoft","Netflix","Plaid"]},{"id":"q-743","question":"You're given a list of n task durations L. Partition the tasks into consecutive days (each day at least one task); the cost of a day is the maximum duration on that day. Devise a DP to minimize the total cost across all days. Define dp[i] as the minimum cost for the first i tasks, provide the recurrence, reconstruction method, and complexity. How would you implement it?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Slack","Stripe","Twitter"]},{"id":"q-754","question":"Given a circular array v[1..n] of non-negative values and an integer d≥1, pick a subset of indices such that the cyclic distance between any two chosen indices is at least d. Maximize the sum of selected values; return both the maximum sum and the number of distinct optimal subsets. Constraints: n ≤ 2e5, v[i] ≤ 1e9. Describe an O(n) DP solution with two cases for circularity and how you'd reconstruct counts?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Apple","Netflix","Uber"]},{"id":"q-768","question":"You're given an array A of length n and an integer k. Partition A into exactly k non-empty contiguous subarrays. The cost of a subarray [t+1..i] is (max(A[t+1..i])) * (i-t). Return the minimum total cost and the partition indices. Propose DP: dp[i][j] = min_{t in [j-1..i-1]} dp[t][j-1] + max(A[t+1..i]) * (i-t). Explain reconstruction, base cases, and time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Adobe","Robinhood","Salesforce"]},{"id":"q-777","question":"You're given an array A of non-negative integers representing task durations in order. You must schedule all n tasks into exactly m days. Each day can host a consecutive block with total duration <= D. The cost of a day is (sum of that day's durations)^2. Return the minimum total cost, or -1 if impossible? Propose a DP using dp[i][d] as min cost for first i tasks in d days, with transition dp[i][d] = min_{j<i, prefix[i]-prefix[j] <= D} dp[j][d-1] + (prefix[i]-prefix[j])^2. Include base cases, reconstruction, and time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Amazon","Apple","Robinhood"]},{"id":"q-786","question":"You have an array A of length n. Partition into exactly k non-empty contiguous blocks. The cost of a block [t+1..i] is (max(A[t+1..i])) * (sum(A[t+1..i])). Return the minimum total cost and the partition indices. Propose a DP formulation, reconstruction strategy, and complexity analysis. Assume 1-based indexing?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Coinbase","Scale Ai","Snap"]},{"id":"q-792","question":"You're given an array prices[0..n-1]. You may complete at most k buy-sell transactions (one share at a time, can't hold more than one). Return the maximum profit and the days of each trade. Propose a DP formulation with states cash[i][t] and hold[i][t], include recurrences, base cases, and reconstruction, and discuss time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Goldman Sachs","IBM","MongoDB"]},{"id":"q-797","question":"You have an array of task difficulties A of length n. Partition it into exactly m non-empty contiguous chapters. Each chapter cost equals the maximum difficulty within that chapter. Return the minimum total cost and a valid partition (chapter end indices). Propose a DP: dp[i][j] = min_{t in [j-1..i-1]} dp[t][j-1] + max(A[t..i-1]), with base dp[i][1] = max(A[0..i-1]). Explain reconstruction, base cases, and time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Adobe","LinkedIn","OpenAI"]},{"id":"q-806","question":"You're given an n x m grid of digits grid[i][j] in [0..9]. You may move only right or down from (0,0) to (n-1,m-1). Define a path score as the number of times the next cell's digit is strictly larger than the previous cell's digit along the path. Return the maximum score and a valid path (as coordinates or directions). Propose a dynamic programming formulation with recurrences, base cases, and reconstruction, and discuss time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Google","Hugging Face","Tesla"]},{"id":"q-813","question":"You're given a tree with N nodes (N up to 2e5). Each node i has a value val[i]. Find a connected subtree of exactly K nodes that maximizes the sum of values. Return the maximum sum and the node set. Propose a DP formulation with dp[u][s] = max sum of a connected subtree of size s that contains u and lies entirely within u's subtree when the tree is rooted at 1; include reconstruction, base cases, and discuss time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Google","Microsoft"]},{"id":"q-821","question":"You're given a string s of length n consisting of lowercase letters. Partition s into at most k non-empty contiguous substrings. The cost of a substring is the number of distinct characters in that substring. Return the minimum total cost and one valid partition (end indices). Propose a DP formulation with recurrence dp[i][t] = min_{p in [t-1..i-1]} dp[p][t-1] + cost(p, i-1) where cost(p, q) is the number of distinct letters in s[p..q]. Explain reconstruction, base cases, and time/space complexity?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Citadel","LinkedIn","Snap"]},{"id":"q-829","question":"You're given an array A of length n and an integer k. Partition A into exactly k non-empty contiguous subarrays. Each subarray's cost is max(A[l..r]) - min(A[l..r]). Return the minimum total cost and one valid partition (end indices). Propose a DP: dp[i][t] = min_{p in [t-1..i-1]} dp[p][t-1] + (max(A[p..i-1]) - min(A[p..i-1])); base: dp[i][1] = max(A[0..i-1]) - min(A[0..i-1]). Explain reconstruction, base cases, and time/space complexity?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["DoorDash","Hugging Face","Netflix"]},{"id":"q-235","question":"How do you organize Cypress fixtures for component testing, and what are the key patterns for managing test data dependencies across multiple test suites?","channel":"e2e-testing","subChannel":"cypress","difficulty":"beginner","tags":["cypress","component-testing","fixtures"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"q-1173","question":"Design an end-to-end test for GDPR data deletion in an e-grocery platform: a synthetic user requests account deletion that must purge PII from cart, catalog, checkout, loyalty services, and analytics pipelines across three regions. Describe data setup, purge verification across stores, caches, and search indexes, audit logs, and idempotency after replay; outline concrete tooling (Playwright + REST mocks + Kafka) and how you handle eventual consistency and test isolation?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Apple","DoorDash"]},{"id":"q-1335","question":"Design an end-to-end test strategy for a multi-tenant SaaS app with per-tenant data isolation, dynamic feature flags, and role-based access controls. Outline how you seed tenants, run parallel tests across tenants and roles, validate feature gating, and verify audit logs across services. Include tooling choices, data isolation strategy, and cleanup?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Adobe","Tesla"]},{"id":"q-1420","question":"Design a beginner-friendly E2E test for a CMS bulk-upload feature: a CSV with 100 rows is uploaded; validations run in the background; after completion, 100 pages are created with titles from the CSV; outline specific UI steps, how you verify progress, how you confirm data creation via API, and how you isolate the test to avoid polluting production data?","channel":"e2e-testing","subChannel":"general","difficulty":"beginner","tags":["e2e-testing"],"companies":["Adobe","Twitter"]},{"id":"q-1603","question":"Design an E2E test for a real-time collaborative post editor behind a region-specific feature flag (US vs EU) in a Twitter/Airbnb-like product. The test should cover flag gating, optimistic UI updates, WebSocket propagation across regions, eventual consistency after reconnections, and a rollback path when the flag is disabled. Which tooling and concrete steps would you use?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Airbnb","Twitter"]},{"id":"q-1713","question":"Design an end-to-end test for a Plaid-like bank-link flow embedded in a fintech app, covering OAuth-like redirects, token exchange, and account data pulls. Include how you isolate test data, simulate bank outages, verify idempotent ledger events, and ensure eventual consistency across retries?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Microsoft","Plaid"]},{"id":"q-1746","question":"Design an E2E test for an embeddable form builder widget used by partners (Salesforce, Cloudflare, Coinbase) that runs inside an iframe. It uses postMessage for cross-origin events, drafts in localStorage, and submits to a CORS API. Create an end-to-end test that validates tenant isolation, iframe messaging, draft autosave after brief network blips, and successful submission across two partner domains?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Cloudflare","Coinbase","Salesforce"]},{"id":"q-1893","question":"Design an end-to-end test for a multi-tenant data-export feature: an admin triggers an export via the UI; the export runs in the background, reads tenant-scoped data from multiple services, writes to blob storage, and emails a download link. How would you verify tenant isolation, idempotent retries, and resilience to blob outages without cross-tenant leakage? Include concrete tooling choices and test data strategy?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Airbnb","Microsoft"]},{"id":"q-2213","question":"Design a real-world E2E test for onboarding a new tenant in a multi-region SaaS platform. The flow creates a tenant, provisions resources in 3 regions, configures IAM roles, and activates a 30-day billing trial. Include test data isolation, region failover, eventual consistency checks, idempotent retries, and rollback guarantees. Specify tooling and steps?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Cloudflare","Discord","Goldman Sachs"]},{"id":"q-2302","question":"Design an end-to-end test for a real-time collaboration feature in a chat app with offline support where 3 clients edit the same message concurrently. Include how you simulate network partitions and reconciliations, verify offline queues, message ordering, conflict resolution, and eventual consistency across all clients?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Discord","Google"]},{"id":"q-2367","question":"Design an end-to-end test plan for a multi-tenant, localization-aware onboarding flow in a SaaS app. The flow includes marketing landing in the user's locale, signup that provisions a new workspace, email verification, in-app onboarding tour, and role-based access setup across three regions. Include how you isolate test tenants, seed data, simulate slow networks, verify asynchronous provisioning, and ensure no cross-tenant data leakage. Specify tooling (Playwright or Cypress), data seeding strategies, and validation hooks?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["IBM","LinkedIn"]},{"id":"q-2388","question":"Describe an E2E test for a multi-tenant onboarding flow where a new tenant is provisioned via API, then UI steps (profile, plan, webhooks) must complete across auth, billing, and notification services. Include per-tenant isolation (unique tenantId, separate DB/schema), data lifecycle, cross-service verification via logs/events, and cleanup. Use Playwright, REST, and DB probes?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Adobe","Google","Plaid"]},{"id":"q-2444","question":"Design an end-to-end test for a real-time collaborative editor built on Automerge CRDTs, with three clients editing the same document concurrently, including offline edits, reconnection, and latency across two regions. Describe how you simulate network partitions, verify eventual consistency, ensure idempotent operations, and test presence/undo/redo semantics under reconnect?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["MongoDB","Salesforce","Twitter"]},{"id":"q-2526","question":"Design an advanced E2E test for a real-time order-status pipeline built on a pub/sub backbone (Kafka-like) with a downstream fan-out service. Validate exactly-once delivery, out-of-order processing, regional outages, and per-tenant data isolation; include data generation, replay safety, and latency/throughput metrics under load?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Cloudflare","DoorDash","IBM"]},{"id":"q-2579","question":"Design a beginner-friendly E2E test for a MongoDB-backed admin UI CSV import feature that upserts users by email. The test should cover test data isolation, handling of invalid rows, idempotent re-import, and verification of updated fields in both UI and DB. Include tooling suggestions, concrete steps, and a cleanup strategy?","channel":"e2e-testing","subChannel":"general","difficulty":"beginner","tags":["e2e-testing"],"companies":["Bloomberg","MongoDB"]},{"id":"q-2823","question":"Design a REAL end-to-end test for a Stripe-like checkout in a multi-region, multi-tenant system. The test should validate a user places an order, an authorization is sent to a payment provider, an invoice is generated, and settlement completes across two regions. Include how you isolate test data, simulate regional outages, verify eventual consistency and idempotency, and ensure replay-safety of event logs after retries. Specify tooling and concrete steps?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["MongoDB","Stripe"]},{"id":"q-3025","question":"Design an end-to-end test for a web-based API playground where a user enters a prompt, selects a model and temperature, clicks Run, and receives streaming text chunks that render in real-time in the output pane. Include how you simulate streaming responses, verify partial chunk rendering, ensure final output correctness, test cancel/retry behavior, and data isolation across test runs using a sandboxed backend?","channel":"e2e-testing","subChannel":"general","difficulty":"beginner","tags":["e2e-testing"],"companies":["Airbnb","OpenAI"]},{"id":"q-3041","question":"Design an E2E test for a multi-region Airbnb-like booking flow: listing/inventory, calendar, booking, and payment services. Include per-region data isolation for parallel tests, injected regional outages, eventual consistency checks across calendar, reservations, and invoicing ledgers, and idempotent retry handling with webhooks?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Airbnb","Coinbase"]},{"id":"q-3104","question":"Design a beginner-friendly E2E test for a passwordless login using a magic link in a fintech app. Include: simulating email delivery and token extraction, token validation and expiry, rate-limiting on requests, test data isolation across parallel runs, and ensuring idempotent redirects/navigation after login?","channel":"e2e-testing","subChannel":"general","difficulty":"beginner","tags":["e2e-testing"],"companies":["LinkedIn","Oracle","Plaid"]},{"id":"q-3167","question":"Design a beginner-friendly E2E test for a feature-flag controlled onboarding tour in a web app. The tour should render only when a per-user flag is enabled; otherwise it should be skipped. Outline how you would isolate test data, mock the flag service, verify the UI steps, and validate that telemetry events fire on tour start and completion. Include how you’d prevent cross-test data leakage in a shared staging environment and which tools (Playwright or Cypress) you would use?","channel":"e2e-testing","subChannel":"general","difficulty":"beginner","tags":["e2e-testing"],"companies":["IBM","Netflix","Twitter"]},{"id":"q-3193","question":"Design an end-to-end test for a real-time collaborative document editor built on WebSockets with CRDT-based synchronization. Outline how you simulate 3 users editing different regions concurrently, verify convergence and deterministic conflict resolution, test latency budgets across regions, and ensure offline edits reconcile on reconnect. Include tooling, test data isolation, and failure scenarios?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Bloomberg","PayPal","Twitter"]},{"id":"q-3221","question":"Design an E2E test for a Stripe-like multi-tenant billing flow: seed isolated tenants with synthetic customers and invoices, toggle regional tax settings, and perform a checkout via a real UI (Playwright) to generate invoices. Verify ledger reconciliation across billing, tax, and journal services, rely on event replay (Kafka) to confirm eventual consistency, and include idempotent retry handling under a regional outage?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["NVIDIA","Stripe"]},{"id":"q-3260","question":"Design a cross-device end-to-end test for a streaming platform where a user starts playback on a Smart TV app, then switches to a mobile app, ensuring seamless session restoration, cross-device playback state syncing, and offline caching behavior. Include per-device data isolation, network handoff simulation, and verification that playback resumes from the same timestamp within 2 seconds, with correct UI indicators?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Instacart","Netflix"]},{"id":"q-3308","question":"Design a beginner-level end-to-end test for a Lyft-like ride-booking web flow (pickup → drop-off → ride type → confirm) using Playwright or Cypress. Include map auto-complete for addresses, ETA display, and handling the browser's GPS permission prompt; simulate a slow network and a short GPS outage. Describe test data isolation per run, how to seed a clean user, and ensure no session leakage between tests?","channel":"e2e-testing","subChannel":"general","difficulty":"beginner","tags":["e2e-testing"],"companies":["Google","Lyft"]},{"id":"q-3396","question":"Design an end-to-end test for a Zoom-like video conferencing flow that includes: meeting creation, host and participants joining via WebRTC, screen sharing, live captions, and chat. The scenario should simulate intermittent network conditions (latency, jitter, packet loss), user churn, and screen-sharing start mid-call. Outline data isolation, test data seeding, and actionable steps with concrete tooling: Playwright for UI, a mock signaling server, and synthetic WebRTC peers in headless browsers. Validate metrics: join time, rejoin latency, packet loss observed, caption integrity, and screen-share reliability?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Robinhood","Zoom"]},{"id":"q-3555","question":"Design an end-to-end test for a chat support widget embedded in a SaaS dashboard. The widget uses WebSocket for real-time messages, stores chat history in localStorage, and supports offline mode with automatic retry when the connection returns. Outline test data, steps, and how you verify: message ordering, offline behavior, and reconciliation on reconnect?","channel":"e2e-testing","subChannel":"general","difficulty":"beginner","tags":["e2e-testing"],"companies":["Anthropic","Lyft","Plaid"]},{"id":"q-3839","question":"Design an end-to-end test for a real-time collaborative document platform (web + mobile) that uses CRDT-based conflict resolution. Validate simultaneous edits by multiple users, offline edits with reconnection, and eventual consistency across clients. Describe tooling, network fault injection, data seeds, and how you verify audit trails and per-user diffs?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Adobe","Apple","Lyft"]},{"id":"q-3873","question":"Design an E2E test for a Salesforce-like storefront built with micro-frontends (Catalog, Cart, Checkout) controlled by feature flags. Run parallel across 3 regions with per-region data isolation. Validate flag rollout/rollback, end-to-end flow cart -> checkout -> payment, and cross-service eventual consistency among order, inventory, and payment services. Specify tooling, data isolation, idempotent retries, and flaky-test mitigation?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Instacart","Salesforce"]},{"id":"q-4138","question":"Design an end-to-end test for a real-time collaborative document editor (like Google Docs) integrated into a messaging app. The editor uses CRDTs over WebSockets to sync edits among N clients. Outline how you would implement an E2E test that isolates documents, simulates latency and network partitions, handles offline edits, and validates CRDT convergence after reconnect. Include tooling choices and steps?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Discord","NVIDIA"]},{"id":"q-4346","question":"Design a pragmatic end-to-end test plan for a Salesforce-to-Snowflake integration that, upon lead conversion, publishes an event to a webhook consumer and loads incremental data into Snowflake. Include test data isolation for multi-tenant orgs, synthetic data generation, retry/idempotency of webhook processing, CDC window validation, and performance under burst traffic. What steps and tooling would you use?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Salesforce","Snowflake"]},{"id":"q-4357","question":"Design an advanced E2E test for a real-time analytics dashboard that consumes events from multiple producers via Kafka, handles schema evolution, late-arriving data, and backfills. Include end-to-end data path (producer -> Kafka -> stream processor -> sink -> UI), idempotent replay safety, regional multi-cluster considerations, fault injection (latency, downstream outage), and concrete verification criteria?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Apple","Meta","PayPal"]},{"id":"q-4404","question":"Design an end-to-end test for a real-time collaborative document editor that uses CRDTs. Simulate three clients editing the same document concurrently, including offline edits, network latency, and reconnects. Validate conflict resolution, presence indicators, offline persistence, and eventual consistency across clients. Include test data isolation, partition scenarios, and data integrity checks?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Hugging Face","Oracle"]},{"id":"q-449","question":"How would you design an E2E testing strategy for a distributed edge computing platform that needs to validate functionality across 100+ global data centers with varying network conditions?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Cloudflare","Tesla"]},{"id":"q-4497","question":"Design a GDPR 'right to be forgotten' E2E test for a ride-booking platform: deleting a user should redact personal data across auth, profiles, payments, messages, and audit logs, emit webhook deletion events, and purge data in backups within 24 hours. Explain test data isolation, region-aware backfills, and idempotent retries under simulated outages?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Apple","DoorDash","Snap"]},{"id":"q-4573","question":"Design an E2E test for a mobile-first e-commerce catalog page (viewport 375x812) that searches for 'lamp', applies a price filter (20-50), selects the first result, adds to cart, and initiates checkout. Include data seeding for a per-run test user, ensure test isolation across runs, and verify skeleton loaders appear while data loads?","channel":"e2e-testing","subChannel":"general","difficulty":"beginner","tags":["e2e-testing"],"companies":["Apple","Google"]},{"id":"q-460","question":"You're testing a login form with Playwright. The form has email and password fields, and a submit button. How would you write a basic E2E test to verify successful login and redirect to dashboard?","channel":"e2e-testing","subChannel":"general","difficulty":"beginner","tags":["e2e-testing"],"companies":["Lyft","MongoDB","NVIDIA"]},{"id":"q-4677","question":"Design an end-to-end test for a cross-platform collaborative document app (web and mobile) that supports offline edits and real-time sync. Include how to validate offline-online merge conflicts, eventual consistency, presence, and audit/version history. Outline steps to simulate network partitions and latency, data seeding, and verification criteria for both clients and server?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Goldman Sachs","NVIDIA","Two Sigma"]},{"id":"q-4704","question":"Design an E2E test for a real-time, multi-user collaborative whiteboard app (desktop+mobile) that uses CRDT syncing over WebSocket. Open the same board in 3 contexts, perform conflicting strokes, simulate network partitions and reconnection, and verify that all clients converge to a consistent final drawing within 5 seconds. Include tooling choices, data seeds, and verification methods (stroke- or pixel-level)?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Snap","Tesla","Uber"]},{"id":"q-4839","question":"Design an end-to-end test plan for a real-time collaborative document editor deployed across multiple regions. The test should verify CRDT-based convergence under concurrent edits, simulate inter-region latency and network partitions, validate offline edits sync on reconnect, confirm role-based access control, and ensure audit logs capture user IDs, regions, actions, and timestamps?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Goldman Sachs","Google","IBM"]},{"id":"q-491","question":"How would you set up a basic E2E test for a login form using Playwright?","channel":"e2e-testing","subChannel":"general","difficulty":"beginner","tags":["e2e-testing"],"companies":["IBM","Lyft","Snowflake"]},{"id":"q-521","question":"You're testing a React app with Playwright. Some tests fail intermittently due to API delays. How would you make your e2e tests more reliable without removing the API dependency?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Google","Hugging Face"]},{"id":"q-548","question":"How would you design a scalable E2E testing strategy for a microservices architecture with 50+ services, ensuring test isolation and parallel execution while maintaining realistic user journeys?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["LinkedIn","Scale Ai"]},{"id":"q-574","question":"How would you handle flaky E2E tests in a CI/CD pipeline? What strategies would you implement to ensure reliable test execution?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["LinkedIn","Meta","Oracle"]},{"id":"q-850","question":"In a Stripe-like billing system built on an event-driven microservice architecture, design an E2E test that validates the end-to-end flow from a user initiating a purchase to invoice settlement across regionally distributed services. Include how you verify eventual consistency, idempotency, and state replay safety after a simulated regional outage, with concrete tooling choices and steps?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Google","Stripe"]},{"id":"q-901","question":"Design a beginner-friendly E2E test for a React checkout flow: user visits a product page, adds to cart, proceeds to checkout, fills shipping details, and completes a payment via a sandbox API. Explain how you would ensure test isolation and determinism (seed/reset test data, mock payment endpoint), and show a minimal Playwright script snippet that asserts successful order confirmation and a backend order record?","channel":"e2e-testing","subChannel":"general","difficulty":"beginner","tags":["e2e-testing"],"companies":["MongoDB","Tesla","Twitter"]},{"id":"q-949","question":"Design an end-to-end test plan for a Netflix/Meta-like streaming service that delivers adaptive bitrate video across 4 regions. Include how you model test content, simulate varying network conditions, verify manifest/chunk fetch, DRM/licensing checks, on-device caching, and end-to-end telemetry; specify tooling, data isolation, and how you scale across regions while minimizing flakiness?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Meta","Netflix"]},{"id":"q-979","question":"Design an end-to-end test for a multi-tenant content platform that serves regionally personalized content with feature flags and A/B tests. The platform must ensure per-tenant data isolation, correct content personalization, flag-driven UI, and during regional outages. Outline the test scope, data management, tooling, and steps to verify end-to-end delivery across tenants and regions without flakiness?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Adobe","Meta","Salesforce"]},{"id":"q-279","question":"What are the key differences between getByRole() and getByText() selectors in Playwright, and when would you choose one over the other for reliable E2E testing?","channel":"e2e-testing","subChannel":"playwright","difficulty":"beginner","tags":["playwright","browser-automation","selectors"],"companies":["Adobe","Amazon","Microsoft","Netflix","Salesforce"]},{"id":"q-208","question":"What is the difference between Selenium WebDriver and Selenium Grid, and when would you use each in your testing strategy?","channel":"e2e-testing","subChannel":"selenium","difficulty":"beginner","tags":["selenium","webdriver","grid"],"companies":["Amazon","Google","Meta"]},{"id":"q-1062","question":"You're stewarding reliability for a 24/7 payments platform with two active regions and strict latency requirements. A recent outage exposed brittle failover and slow triage. Outline a practical plan: governance model (platform vs product teams), incident playbooks, auto-remediation, SRE metrics, release controls, and how you measure ROI while preserving feature velocity?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Citadel","Microsoft","Square"]},{"id":"q-1088","question":"Two squads share a single API surface: Platform Reliability (2 senior + 1 mid) and Growth Feature (4 engineers). Platform incidents increased MTTR by 40% last quarter; Growth feature is 70% complete but depends on unstable APIs and tight external deadlines. How would you structure quarterly planning to protect reliability, reallocate capacity, set SLOs and error budgets, and implement gating (flags, canaries, contracts) to finish the Growth feature without amplifying risk?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Google","Robinhood","Uber"]},{"id":"q-1223","question":"You're steward of a shared data platform used by 8 squads across web, mobile, and ML workloads. A new event schema from one squad breaks downstream contracts and delays analytics dashboards. Propose a governance model: data contracts, versioned schemas, deprecation policy, and a cross-squad escalation process. Include concrete ownership, metrics, and a migration plan that minimizes customer impact while meeting quarterly release goals?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Anthropic","Instacart","Twitter"]},{"id":"q-1309","question":"You're leading engineering for a company with 5 teams (Auth, Billing, Search, Recommendations, Web). A directive requires end-to-end delivery quality: cut post-release hotfix rate by 40% while preserving delivery velocity. Propose a concrete plan to implement dual-track planning (feature vs reliability), allocate capacity (e.g., 60/25/15 split), designate cross-cutting owners (telemetry, incident mgmt, release engineering), and the metrics, rituals, and a 12-week rollout. Include milestones and go/no-go criteria?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Airbnb","DoorDash","Meta"]},{"id":"q-1376","question":"How would you design and implement a platform governance model for 6 product squads relying on a shared platform (auth, billing, search) with a cap of 2 engineers for platform work per sprint, ensuring API stability, a deprecation policy, telemetry, and incident response, plus a 12-week rollout and concrete success metrics? Provide the first 90-day plan and the top trade-offs?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Hugging Face","IBM","Instacart"]},{"id":"q-1614","question":"As owner of a real-time analytics platform used by three customer apps, a silent ETL failure intermittently leaves dashboards stale for 20% of users, skewing revenue KPIs. Provide a concrete remediation plan: 1) 24h incident triage, 2) redesigned monitoring with SLIs/SLOs and automated remediation, 3) cross-team ownership and gating, 4) validation and rollback, 5) a 6-week rollout with milestones and go/no-go criteria. Include concrete metrics and an initial implementation plan?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Lyft","PayPal","Robinhood"]},{"id":"q-1687","question":"You're overseeing five platform teams (Networking, Compute, Auth, Billing, Observability). EU data localization is mandated with a 9-month deadline; design a concrete program to migrate data stores and services with minimal downtime, ensure compliance and data sovereignty, and minimize feature disruption. Include ownership matrix, migration waves, rollback plan, telemetry & auditability, go/no-go criteria, and success metrics?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Cloudflare","Google","LinkedIn"]},{"id":"q-1827","question":"You're the head of platform engineering at a fintech with four squads—Payments, Identity, Analytics, and Notifications. A critical legacy event bus must be replaced with Apache Pulsar. Current availability 99.99% and latency <100 ms, ~2M messages/day, strict regulatory retention for audits. You have 6 weeks, no downtime. Outline a phased migration plan: dependency map, cutover, rollback, telemetry, governance, and success metrics. What plan would you implement to accomplish this within the constraints?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Goldman Sachs","Lyft","Stripe"]},{"id":"q-1856","question":"Oversee a data-platform initiative to consolidate 6 product domains into a centralized data catalog with standardized data contracts. Ambiguous expectations cause data quality issues and flaky dashboards. Design a governance model and a 90-day rollout: schema/versioning, tests, SLIs/SLOs for data, data-contract rituals, and cross-stakeholder engagement. How would you measure ROI and prevent contract creep?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Databricks","Meta","Robinhood"]},{"id":"q-2028","question":"How would you implement a **dual-track leadership ladder** (Technical Leader vs People Manager) in a 4-squad, multi-region organization to deepen technical depth without sacrificing delivery velocity? Propose a 90‑day pilot design, governance, mentoring cadence, and shared OKRs; specify success metrics (**velocity**, turnover, time-to-promotion) and a rollout plan?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Google","Netflix"]},{"id":"q-2047","question":"In a fast-scaling organization aiming for 10x traffic, you manage five squads: Platform, Auth, Payments, Data, and Web, plus centralized SRE. A quarterly objective mandates MTTR down 40%, lead time down 25%, and on-call toil down 20%, without sacrificing velocity. Design an org and governance model, specify roles and rituals, metrics to track, and a 12-week rollout with milestones. Include risk mitigations and rollout gates?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Amazon","Anthropic"]},{"id":"q-2310","question":"You're leading a 6-month program to migrate three critical services to an event-driven architecture using Kafka. Teams span Node.js, Python, and Java, with tight uptime requirements and uneven test coverage. Propose a concrete plan detailing governance, risk-based rollout, cutover strategy, rollback procedures, and the metrics/rituals you would use to stay within SLOs while preserving velocity. Include milestones and go/no-go criteria?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Oracle","Slack","Snap"]},{"id":"q-2428","question":"You're leading an ML Platform group supporting 6 product squads with data privacy and cost constraints. How would you design a chargeback-enabled operating model: cost allocation, guardrails (data access, drift monitoring), and reusable components, plus metrics and a 12-week rollout with milestones and go/no-go criteria?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Anthropic","Cloudflare","Meta"]},{"id":"q-2525","question":"How would you implement a cross‑team reliability program that reduces post‑release incidents while preserving feature velocity across three product lines (Auth, Billing, Search) within a 12‑week rollout? Include governance, dual‑track planning, ownership, metrics, rituals, and concrete milestones?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Adobe","DoorDash","OpenAI"]},{"id":"q-2660","question":"You're overseeing four squads across three regions. A unified Authentication Portability Initiative must replace in-house OAuth with a standard provider, risking existing third party integrations. Design a concrete 12 week rollout plan with sequencing, dependency contracts, testing gates, rollback, telemetry, and success criteria. Include cross team governance?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Apple","Bloomberg","Databricks"]},{"id":"q-2749","question":"You're an engineering manager onboarding a distributed 6-person team with patchy docs and inconsistent handoffs. Propose a concrete 4-week onboarding plan that (a) uncovers critical architectural gaps, (b) standardizes release and incident handling, and (c) builds a living knowledge base. Include success metrics and a plan to balance PM priorities with engineering capacity?","channel":"engineering-management","subChannel":"general","difficulty":"beginner","tags":["engineering-management"],"companies":["Airbnb","Amazon","Lyft"]},{"id":"q-2893","question":"You're leading engineering for a latency-sensitive data platform used by 100s of services at MongoDB. A critical release introduces a new storage-encoding feature and coordinated schema migrations across 6 services. One team estimates a 3-week migration; others want incremental rollout. Design an end-to-end plan: ownership, gating, canary strategy, data backfill, rollback, metrics, and go/no-go criteria. Include milestones and cross-team rituals?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Meta","MongoDB"]},{"id":"q-3002","question":"You're an engineering manager with a customer-facing web app and an analytics data pipeline. Release cadences are misaligned (web monthly, data quarterly). Propose a concrete 12-week plan to synchronize, using a shared 2-week integration sprint every 4 weeks. Include ownership, milestones, risk controls, and success metrics; explain how to prevent one track from blocking the other?","channel":"engineering-management","subChannel":"general","difficulty":"beginner","tags":["engineering-management"],"companies":["Airbnb","Google","LinkedIn"]},{"id":"q-3042","question":"You're managing reliability across four product squads (Auth, Payments, Search, Messaging). A live incident reveals inconsistent telemetry and unclear ownership for telemetry, incident response, and release engineering. Propose a concrete 6-week plan: (1) ownership matrix, (2) telemetry standardization and dashboards, (3) incident runbooks and kill switches, (4) rituals and SLIs/SLOs, with go/no-go criteria?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Amazon","Hugging Face","Robinhood"]},{"id":"q-3163","question":"In a matrix org with four feature squads (Payments, Messaging, Discovery, Admin) that share a central Data Platform, a decision to switch the event bus from Kafka to a managed service must be executed without slowing feature delivery. Propose an 8‑week plan that includes ownership, contract testing, observability SLIs/SLOs, a staged cutover with feature flags, and a deprecation timeline, plus 3 milestones and clear go/no-go criteria?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Google","Microsoft"]},{"id":"q-3292","question":"In a scaling org with six squads—Auth, Payments, Catalog, User, Search, Infra—new regulatory requirements demand end-to-end data lineage, strict access controls, and immutable deployment logs within 12 weeks. How would you delineate product vs platform ownership, set governance, and implement a concrete 12-week plan with rituals and milestones to balance compliance and delivery velocity?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Adobe","Coinbase","Goldman Sachs"]},{"id":"q-3380","question":"Your organization runs three teams (Payments, Logistics, Catalog) sharing a shared event bus used for order processing. A critical, high-velocity feature requires emitting two new event types; the bus must support exactly-once delivery and robust retry but current replay bugs threaten idempotency. You have 9 weeks to ship the feature, with a 2-week safety buffer. Propose a concrete cross-team plan: ownership for telemetry, schema evolution, retries and idempotency; capacity split (60/25/15); milestones; go/no-go criteria; and the rituals you would adjust to ensure on-time delivery and reliability?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Amazon","DoorDash","Twitter"]},{"id":"q-3409","question":"You're the head of engineering at a distributed platform with 5 squads across three regions. A regulatory change requires auditable data access logs and privacy-by-default while continuing to ship features. Propose a concrete 12-week plan to implement a Data Observability & Governance program: define MVP audits, telemetry/logging standards, ownership, release constraints, and go/no-go criteria. Include milestones and success metrics (time-to-audit, per-request logging coverage, incident burn rate)?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["IBM","Instacart","Tesla"]},{"id":"q-3596","question":"You're managing four squads (Platform, Data, Frontend, Mobile) delivering a new AI-powered search feature. Privacy, latency p99 < 120ms, and data governance are non-negotiables with limited capacity. Propose a concrete plan to define SLOs/SLIs, implement dual-track planning (feature vs reliability/privacy), allocate capacity, assign ownership, and outline a 12-week rollout with milestones and go/no-go criteria, plus risk mitigation?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["IBM","Meta","NVIDIA"]},{"id":"q-3626","question":"Scenario: a platform with six product squads relies on a centralized Platform team (auth, observability, payments). Quarterly review shows product squads consistently miss platform SLOs and overwhelm capacity. Propose a concrete plan to redefine platform SLOs and reliability budgets, adopt a Platform-as-a-Product model with onboarding, roadmaps, and SLAs, implement a Platform Customer Review and a lightweight API contract process, and measure impact on velocity over 12 weeks?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Instacart","Microsoft","Snowflake"]},{"id":"q-3706","question":"In a multinational product with 6 teams across time zones, enforce a policy of asynchronous deployment windows and telemetry-driven rollbacks within 24 hours of release. Propose a concrete plan to implement this, including governance, capacity allocation, cross-geo on-call, release workflow, metrics, and a 8-week rollout with milestones and go/no-go criteria. Include sample SLOs/SLIs?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Databricks","Hugging Face","Salesforce"]},{"id":"q-3740","question":"You're onboarding a junior engineer on a 5-person data-pipelines team building streaming ETL jobs with production responsibilities. Outline a concrete 6-week ramp plan that pairs them with a senior mentor, assigns a minimal production feature, enforces a code-review SLA, adds production safeguards (feature flags/canaries), and a feedback cadence; include two success metrics and one risk-mitigation tactic?","channel":"engineering-management","subChannel":"general","difficulty":"beginner","tags":["engineering-management"],"companies":["Databricks","Google","Slack"]},{"id":"q-4042","question":"You're a first-time engineering manager overseeing four squads (Backend, Frontend, Data, QA). Flow of requests is ad-hoc via Slack; no formal backlog, no standard sprint planning. Propose a 4-week plan to launch a lightweight intake, a prioritization rubric, and a recurring planning ritual, plus a simple health dashboard. Include artifacts, cadence, ownership, and success metrics?","channel":"engineering-management","subChannel":"general","difficulty":"beginner","tags":["engineering-management"],"companies":["Citadel","Hugging Face","Tesla"]},{"id":"q-4075","question":"You're responsible for sunsetting a legacy service that powers critical user workflows. You have 3 squads: Auth, Payments, and Core API. You must migrate users to a new platform within 12 weeks with zero downtime. Design a concrete migration plan with a 3-phase timeline, data migration strategy, rollback criteria, staffing plan, and success metrics. Include cross-team governance, data integrity checks, and SLAs?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Lyft","Netflix","Snap"]},{"id":"q-4178","question":"As a senior engineering manager of a multi-tenant platform with 6 squads (Cart, Payments, Inventory, Recommendations, Delivery, Auth), a directive requires 40% fewer post-release outages in 12 weeks while preserving velocity. Outline a concrete 'contract-based reliability' plan: per-squad SLOs/SLIs, capacity allocation, cross-cutting owners (telemetry, incident mgmt, release engineering), dashboards and error budgets, rituals, and go/no-go criteria. Include artifacts, milestones, and governance?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Instacart","NVIDIA","Stripe"]},{"id":"q-4250","question":"You're leading a small product squad that just onboarded 4 engineers mid-quarter. You have 2 weeks to onboard them and maintain sprint velocity. Describe a concrete two-week onboarding plan that gets them contributing to critical tasks by sprint end, including buddy system, knowledge-sharing rituals, systems access, and ramp metrics?","channel":"engineering-management","subChannel":"general","difficulty":"beginner","tags":["engineering-management"],"companies":["DoorDash","Salesforce"]},{"id":"q-4351","question":"You're leading four squads (Mobile, Web, API, Data) at a privacy-first company. A regulatory change mandates end-to-end encryption at rest for all customer data within 12 weeks, plus a mandatory lockdown period during rollout. Propose a concrete plan: ownership of migrations, dual-track planning (feature vs security), milestones and go/no-go criteria, rollback and risk mitigations, and how you'd handle external dependencies and leadership updates?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Adobe","Meta"]},{"id":"q-4420","question":"You oversee a multi-tenant feature-flag platform used by 6 product squads; a regulatory mandate requires verifiable audit trails for every feature flag toggle and experiment impact across all tenants. Design a scalable governance plan: ownership, change process, telemetry and data lineage, access control, rollback, testing, and an 8-week rollout with milestones, go/no-go criteria, and success metrics. Provide concrete examples?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Cloudflare","Hugging Face","Twitter"]},{"id":"q-4457","question":"You're the head of a 4-team platform in a fintech company during a peak payout window. A payout orchestration microservice starts failing due to flaky external payment provider API, delaying payouts for a subset of users. Outline a concrete, actionable plan to (1) contain risk and restore service, (2) implement safe fallbacks and circuit breakers, (3) realign cross-team ownership to fix root cause while preserving velocity, and (4) define success metrics, SLAs/OLA targets, and a post-incident learning process?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["DoorDash","IBM","PayPal"]},{"id":"q-4574","question":"In a matrix organization with six squads: API, Web, Mobile, Data, Platform, and SRE, across two regions, a regulatory change requires data locality and strict privacy controls for a new analytics feature. The project must go live in 12 weeks with minimal latency impact and improved MTTR. Propose a concrete plan: define SLOs/SLIs for privacy, performance, and reliability; implement dual-track planning (feature vs compliance); allocate capacity (e.g., 50/25/25 split); assign cross-cutting owners; outline a 12-week rollout with milestones and go/no-go criteria; include telemetry changes, incident response, and risk mitigation?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["IBM","Oracle"]},{"id":"q-461","question":"How would you handle a situation where your top engineer wants to work on a different project, but you need them to complete a critical deadline?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Airbnb","Google"]},{"id":"q-4651","question":"You're scaling fintech org to four squads across two time zones; onboarding new managers is slowing delivery. Design a practical plan to cut manager ramp time by 50% within 90 days without sacrificing velocity. Include role definitions, rituals, metrics, and a 12-week rollout plan?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Plaid","Scale Ai"]},{"id":"q-4728","question":"How would you design a 4-week onboarding ramp for a new engineer joining a fintech team with 4 microservices and a two-week sprint cadence to reach productive velocity while minimizing production risk? Describe concrete milestones, artifacts (onboarding playbook, sandbox tasks, sample PRs), success criteria, and how you would measure progress over the first 8 weeks?","channel":"engineering-management","subChannel":"general","difficulty":"beginner","tags":["engineering-management"],"companies":["Bloomberg","Robinhood"]},{"id":"q-4826","question":"You're overseeing reliability for a data platform used by DataIngest, AnalyticsAPI, Payments, and Infra. A directive requires zero-downtime deployments and formal API versioning with rolling migrations across three regions. One service frequently introduces breaking changes without deprecation paths. How would you design governance (ownership, release gates, deprecation windows), the migration plan (versions, backwards compatibility, feature flags, blue/green), and a coordinated rollout with concrete milestones and metrics?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Coinbase","Databricks","Microsoft"]},{"id":"q-492","question":"How would you handle a situation where your top engineer wants to work on a personal project during work hours, claiming it will benefit the company long-term?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Microsoft","Snap"]},{"id":"q-522","question":"You're leading a team of 5 engineers. Two team members disagree on the technical approach for a critical feature. How do you handle this situation while maintaining team morale and meeting the deadline?","channel":"engineering-management","subChannel":"general","difficulty":"beginner","tags":["engineering-management"],"companies":["Google","Instacart"]},{"id":"q-549","question":"How would you handle a situation where your top engineer wants to work on a different project than what the team needs?","channel":"engineering-management","subChannel":"general","difficulty":"beginner","tags":["engineering-management"],"companies":["Apple","Hashicorp","Scale Ai"]},{"id":"q-575","question":"How do you balance technical debt with feature delivery when managing engineering teams?","channel":"engineering-management","subChannel":"general","difficulty":"beginner","tags":["engineering-management"],"companies":["Bloomberg","PayPal"]},{"id":"q-860","question":"When onboarding new engineers to a project with a legacy codebase and a new component library, operating on a 3-week sprint with shared CI, what concrete onboarding plan and gates would you implement in the first 4 weeks to accelerate learning while preserving code quality and preventing regressions?","channel":"engineering-management","subChannel":"general","difficulty":"beginner","tags":["engineering-management"],"companies":["Meta","Stripe"]},{"id":"q-184","question":"You're managing a critical microservices migration from monolith to Kubernetes with 3 teams. Team A (backend services) is 2 weeks behind due to database connection pooling issues, Team B (frontend) is on track but blocked by API contracts, and Team C (DevOps) needs production-ready Helm charts by EOW. How do you resolve the technical dependencies and get the migration back on schedule while maintaining service availability?","channel":"engineering-management","subChannel":"project-management","difficulty":"advanced","tags":["project","planning"],"companies":["Adobe","Amazon","Google","Microsoft","Netflix","Salesforce"]},{"id":"q-211","question":"How would you implement a technical debt repayment framework using the 20% time allocation model while balancing feature delivery deadlines?","channel":"engineering-management","subChannel":"project-management","difficulty":"intermediate","tags":["delegation","mentoring","growth"],"companies":["Google","LinkedIn","Microsoft","Robinhood","Stripe"]},{"id":"q-261","question":"Design a task delegation matrix system for a 15-person engineering team that balances skill development with project delivery SLAs. Include RACI implementation, automated task assignment algorithms, and success metrics. How would you handle edge cases like skill gaps and conflicting priorities?","channel":"engineering-management","subChannel":"team-leadership","difficulty":"beginner","tags":["delegation","mentoring","growth"],"companies":["Amazon","Google","Meta","Microsoft","Salesforce","Stripe"]},{"id":"q-281","question":"How do you influence technical decisions when you're not the technical lead, and what specific strategies do you use to build technical credibility across different stakeholder groups?","channel":"engineering-management","subChannel":"team-leadership","difficulty":"intermediate","tags":["communication","collaboration","influence"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-168","question":"Explain the CSS box model and how box-sizing affects layout calculations. What's the difference between border-box and content-box?","channel":"frontend","subChannel":"css","difficulty":"beginner","tags":["css","styling"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-363","question":"You have a navigation bar with 3 items that should be evenly spaced. The middle item needs to be centered while the first and last items stick to the edges. How would you implement this using CSS Flexbox?","channel":"frontend","subChannel":"css","difficulty":"beginner","tags":["css","flexbox","grid","animations"],"companies":["Amazon","Google","Hrt","Meta","Microsoft","Netflix"]},{"id":"q-408","question":"You're building a responsive dashboard with a complex grid layout that must support dynamic widget resizing, reordering via drag-and-drop, and maintain performance with 100+ widgets. How would you architect the CSS grid system to handle these requirements while ensuring smooth animations and preventing layout thrashing?","channel":"frontend","subChannel":"css","difficulty":"advanced","tags":["css","flexbox","grid","animations"],"companies":["Affirm","Okta","Shopify"]},{"id":"q-4303","question":"Context: You're prototyping a tiny product grid for a storefront. Build a responsive 1-3 column card grid that shows an image, title, and price per item. Use CSS Grid with minmax for columns, add a hover lift and a slide-in price badge. Ensure keyboard focus (focus-visible) and respect prefers-reduced-motion. Provide a minimal HTML/CSS snippet and explain your choices?","channel":"frontend","subChannel":"css","difficulty":"beginner","tags":["css","flexbox","grid","animations"],"companies":["Google","Microsoft"]},{"id":"q-4494","question":"You're building a high-density analytics dashboard for a retail platform. Implement a responsive widget grid using CSS Grid that supports 40+ widgets with variable heights; provide layout presets (compact, balanced, spacious) switchable via a settings panel. Ensure smooth reflow animations on preset switch, respect reduced-motion, and accessible keyboard navigation for focusing presets. How would you architect the CSS to achieve this?","channel":"frontend","subChannel":"css","difficulty":"advanced","tags":["css","flexbox","grid","animations"],"companies":["Instacart","Tesla"]},{"id":"q-661","question":"Design a responsive image gallery using CSS Grid that shows 4 columns on wide screens, 2 on tablets, and 1 on mobile. Each tile contains an image, title, and caption. Add a hover/focus animation that lifts the tile and deepens the shadow, and ensure keyboard accessibility and respect for reduced-motion preferences?","channel":"frontend","subChannel":"css","difficulty":"beginner","tags":["css","flexbox","grid","animations"],"companies":["Amazon","Snap","Square"]},{"id":"q-664","question":"Question: Create a responsive 3-column feature grid using CSS Grid that collapses to a single column on narrow viewports, with a hover animation that gently scales each card and reveals a caption with a slide-in effect, while keeping focus-visible styles and respecting prefers-reduced-motion?","channel":"frontend","subChannel":"css","difficulty":"beginner","tags":["css","flexbox","grid","animations"],"companies":["Apple","Meta"]},{"id":"q-1342","question":"You're building a React application where certain components need to maintain state that persists across page refreshes but doesn't need to be shared globally. How would you implement a state management solution that combines local component state with browser storage, and what are the key considerations for handling synchronization, performance, and potential race conditions?","channel":"frontend","subChannel":"frontend","difficulty":"intermediate","tags":["react-hooks","local-storage","state-persistence","custom-hooks","performance"],"companies":[]},{"id":"q-2125","question":"You're building a React form with multiple controlled inputs that need to maintain their state during component unmounting/remounting (e.g., when switching between tabs in a single-page application). How would you implement a state management solution that preserves form state across component lifecycle changes while avoiding the performance pitfalls of lifting all state to a global store, and what specific React patterns would you use to handle this efficiently?","channel":"frontend","subChannel":"frontend","difficulty":"intermediate","tags":["react-hooks","form-state","persistence","performance","component-lifecycle"],"companies":[]},{"id":"q-2619","question":"You're building a React application with complex state that needs to be shared across multiple components, but you want to avoid the performance overhead of Context API re-renders. How would you implement a custom hook-based state management solution that uses React's useReducer combined with memoization techniques to create a lightweight, performant state management system, and what specific patterns would you use to prevent unnecessary re-renders?","channel":"frontend","subChannel":"frontend","difficulty":"intermediate","tags":["react","state-management","performance","custom-hooks","useReducer"],"companies":[]},{"id":"q-4593","question":"You're working on a React application where multiple components need to access and modify the same piece of state, but you want to avoid prop drilling while maintaining better performance than Context API. How would you implement a state management solution using a custom hook with React's useRef and useCallback to create a subscription-based pattern that allows components to selectively subscribe to state changes?","channel":"frontend","subChannel":"frontend","difficulty":"intermediate","tags":["react","state-management","custom-hooks","performance","subscriptions"],"companies":[]},{"id":"q-642","question":"You're building a React form with multiple controlled inputs that need to share validation state. How would you implement a custom hook to manage form state and validation logic efficiently, and what are the key considerations for preventing unnecessary re-renders?","channel":"frontend","subChannel":"frontend","difficulty":"intermediate","tags":["react-hooks","form-validation","performance-optimization","custom-hooks","state-management"],"companies":[]},{"id":"q-762","question":"You're building a React dashboard with multiple components that need to share and update real-time data (like stock prices or live metrics). How would you implement a state management solution that minimizes re-renders while ensuring all components receive the latest data, and what specific React patterns would you use to prevent performance bottlenecks?","channel":"frontend","subChannel":"frontend","difficulty":"intermediate","tags":["react-state-management","performance-optimization","context-api","real-time-data","memoization"],"companies":[]},{"id":"fe-2","question":"Explain the JavaScript Event Loop architecture. How do microtasks and macrotasks differ in execution order, and what are the practical implications for async/await code?","channel":"frontend","subChannel":"javascript","difficulty":"beginner","tags":["js","async","core"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"fe-3","question":"Explain JavaScript closures with a practical use case and how they're used in real applications?","channel":"frontend","subChannel":"javascript","difficulty":"intermediate","tags":["js","scope","patterns"],"companies":["Amazon","Google","Meta"]},{"id":"fr-157","question":"What is the difference between `let`, `const`, and `var` in JavaScript, and how do their scoping rules and temporal dead zone affect real-world code?","channel":"frontend","subChannel":"javascript","difficulty":"beginner","tags":["js","core"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"fr-162","question":"Explain how JavaScript's event loop handles microtasks vs macrotasks. What happens when a Promise resolves inside a setTimeout callback?","channel":"frontend","subChannel":"javascript","difficulty":"advanced","tags":["js","core"],"companies":["Airbnb","Google","Meta","Netflix","Stripe"]},{"id":"fr-173","question":"What is the output of this code and explain the event loop behavior: console.log('A'); setTimeout(() => console.log('B'), 0); Promise.resolve().then(() => console.log('C')); Promise.resolve().then(() => console.log('D')); console.log('E'); How do microtask and macrotask queues interact in JavaScript's event loop?","channel":"frontend","subChannel":"javascript","difficulty":"advanced","tags":["js","core"],"companies":["Amazon","Google","Meta","Netflix","Stripe"]},{"id":"q-240","question":"What is a closure in JavaScript and how does it enable data encapsulation?","channel":"frontend","subChannel":"javascript","difficulty":"beginner","tags":["js","es6","closures","promises"],"companies":["Airbnb","Amazon","Apple","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-351","question":"You're building a file upload component that processes multiple files in parallel. How would you implement a concurrent upload queue with a maximum of 3 simultaneous uploads using Promise.allSettled and closures?","channel":"frontend","subChannel":"javascript","difficulty":"intermediate","tags":["js","es6","closures","promises"],"companies":["Amazon","Anthropic","Microsoft"]},{"id":"q-4447","question":"Implement a fetchWithCache(url, opts) utility that caches JSON results in memory with per-key TTL, deduplicates concurrent in-flight requests for the same key using closures, and never caches failed responses. Provide a compact usage example (TTL 5000) and explain how closures keep caches isolated per instance?","channel":"frontend","subChannel":"javascript","difficulty":"advanced","tags":["js","es6","closures","promises"],"companies":["Amazon","Google","Hashicorp"]},{"id":"q-462","question":"Implement a rate-limited API wrapper that queues requests when the limit is reached, using closures to maintain state and promises to handle request ordering?","channel":"frontend","subChannel":"javascript","difficulty":"advanced","tags":["js","es6","closures","promises"],"companies":["Google","Lyft","Scale Ai"]},{"id":"q-550","question":"Explain how closures work in JavaScript and provide a practical example of when you'd use one in a React component?","channel":"frontend","subChannel":"javascript","difficulty":"beginner","tags":["js","es6","closures","promises"],"companies":["Google","Oracle","Tesla"]},{"id":"fr-154","question":"What are the performance implications and layout shift consequences of loading large images without explicit dimensions, and how do modern CSS properties and loading strategies mitigate these issues?","channel":"frontend","subChannel":"performance","difficulty":"beginner","tags":["perf","optimization"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"fr-172","question":"How would you optimize rendering performance for a React component displaying a large list (10,000+ items) with frequent real-time updates?","channel":"frontend","subChannel":"performance","difficulty":"intermediate","tags":["perf","optimization"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-188","question":"How would you implement a performance budget system that automatically detects bundle regressions and enforces lazy-loading boundaries in a large-scale React application?","channel":"frontend","subChannel":"performance","difficulty":"advanced","tags":["lighthouse","bundle","lazy-loading"],"companies":["Airbnb","Atlassian","LinkedIn","Netflix","Uber"]},{"id":"q-301","question":"How would you optimize a React app's bundle size to achieve Lighthouse scores above 90, and what specific tools and metrics would you use to measure success?","channel":"frontend","subChannel":"performance","difficulty":"beginner","tags":["lighthouse","bundle","lazy-loading"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-329","question":"You're tasked with improving a React app's Lighthouse performance score from 65 to 90+. The bundle size is 2.1MB and Time to Interactive is 4.2s. What specific steps would you take to optimize the bundle and implement lazy loading?","channel":"frontend","subChannel":"performance","difficulty":"intermediate","tags":["lighthouse","bundle","lazy-loading"],"companies":["Amazon","Google","Mckinsey","Meta","Microsoft","Netflix","Scale Ai","Stripe"]},{"id":"q-378","question":"You're building a real-time trading dashboard at DE Shaw that needs to display 1000+ rapidly updating price cards. How would you optimize CSS layout and animations to maintain 60fps while cards are being added/removed/updated every 100ms?","channel":"frontend","subChannel":"performance","difficulty":"advanced","tags":["css","flexbox","grid","animations"],"companies":["DE Shaw","Discord","Instacart"]},{"id":"q-390","question":"You're working on a React app that loads slowly. Your Lighthouse performance score is 45. What specific steps would you take to improve it, and how would you implement lazy loading for a heavy component?","channel":"frontend","subChannel":"performance","difficulty":"beginner","tags":["lighthouse","bundle","lazy-loading"],"companies":["Crowdstrike","Deepmind","NVIDIA"]},{"id":"q-395","question":"You're building a complex dashboard with a responsive grid layout that must support dynamic column insertion, reordering, and animated transitions. How would you implement this using CSS Grid and JavaScript while maintaining 60fps performance during large dataset updates (10,000+ items)?","channel":"frontend","subChannel":"performance","difficulty":"advanced","tags":["css","flexbox","grid","animations"],"companies":["Google","Meta","Microsoft","Netflix","Salesforce","Stripe"]},{"id":"q-523","question":"Your React app has a 85 Lighthouse performance score. The bundle analyzer shows a 2.8MB main chunk with heavy libraries like moment.js and lodash. How would you optimize this to reach 95+?","channel":"frontend","subChannel":"performance","difficulty":"intermediate","tags":["lighthouse","bundle","lazy-loading"],"companies":["Cloudflare","Hugging Face","Stripe"]},{"id":"q-576","question":"How would you optimize a React app's Lighthouse score from 65 to 90+ using bundle analysis and lazy loading?","channel":"frontend","subChannel":"performance","difficulty":"beginner","tags":["lighthouse","bundle","lazy-loading"],"companies":["Coinbase","DoorDash"]},{"id":"fe-1","question":"How does React's Virtual DOM diffing algorithm work during reconciliation, and what role do keys play in optimizing list updates?","channel":"frontend","subChannel":"react","difficulty":"intermediate","tags":["react","perf","internals"],"companies":["Airbnb","Google","Meta","Microsoft","Netflix"]},{"id":"fr-161","question":"How would you implement a React hook that tracks component render count and warns when it exceeds a threshold, while avoiding infinite render loops?","channel":"frontend","subChannel":"react","difficulty":"advanced","tags":["react","perf"],"companies":["Airbnb","Microsoft","Netflix","Stripe","Uber"]},{"id":"q-215","question":"How would you implement a custom useDebounce hook that works with React's concurrent features and prevents stale closures?","channel":"frontend","subChannel":"react","difficulty":"intermediate","tags":["react","hooks","context","redux"],"companies":["Amazon","Google","Meta","Microsoft","Uber"]},{"id":"q-239","question":"How would you implement a React useMemo hook to optimize a recursive Fibonacci function with memoization, and what are the key trade-offs between top-down memoization vs bottom-up tabulation in this context?","channel":"frontend","subChannel":"react","difficulty":"intermediate","tags":["dp","memoization","tabulation"],"companies":["Airbnb","Amazon","Apple","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-315","question":"How would you optimize a React app's bundle size and loading performance using lazy loading, code splitting, and webpack optimization strategies?","channel":"frontend","subChannel":"react","difficulty":"intermediate","tags":["lighthouse","bundle","lazy-loading"],"companies":null},{"id":"q-4240","question":"In a React app using a ThemeContext and a Redux store that streams 50k events/sec via WebSocket, design a MetricsPanel that renders a 1-second aggregated line chart with minimal re-renders. Describe architecture, selectors, virtualization, and rendering strategies; include code sketches for a memoized selector that buckets by 1s, a React.memo chart component, and integration with useTransition for burst updates?","channel":"frontend","subChannel":"react","difficulty":"advanced","tags":["react","hooks","context","redux"],"companies":["Google","IBM"]},{"id":"q-4327","question":"In a React dashboard app, a Redux-backed layout schema drives a grid of 200 widgets. Implement a LayoutRenderer that virtualizes rendering, updates via drag-and-drop, and preserves widget instances. Describe per-widget memoized selectors, useTransition for drag updates, and provide a concise code sketch for a memoized Widget and a virtualization wrapper?","channel":"frontend","subChannel":"react","difficulty":"intermediate","tags":["react","hooks","context","redux"],"companies":["IBM","Lyft","NVIDIA"]},{"id":"q-434","question":"You're building a React app with multiple components needing access to user authentication state. When would you choose Context API over Redux, and what are the specific performance implications of each approach?","channel":"frontend","subChannel":"react","difficulty":"intermediate","tags":["react","hooks","context","redux"],"companies":["Anthropic","Microsoft","NVIDIA"]},{"id":"q-595","question":"Explain the difference between useState and useReducer hooks in React and when you would choose one over the other.","channel":"frontend","subChannel":"react-hooks","difficulty":"intermediate","tags":["react","hooks","state-management","useState","useReducer"],"companies":["Meta","Netflix","Airbnb","Uber","Spotify"]},{"id":"q-628","question":"Explain the differences between useState, useReducer, and Context API for state management in React. When would you choose each approach?","channel":"frontend","subChannel":"react-hooks","difficulty":"intermediate","tags":["react","state-management","hooks","context-api"],"companies":["Meta","Netflix","Airbnb","Uber","Spotify"]},{"id":"q-287","question":"How does a Service Worker intercept network requests and implement offline caching strategies?","channel":"frontend","subChannel":"web-apis","difficulty":"intermediate","tags":["dom","fetch","websocket","service-worker"],"companies":["Amazon","Google","Meta"]},{"id":"q-341","question":"You're building a real-time collaborative document editor. How would you implement a service worker to handle offline synchronization, WebSocket reconnection logic, and conflict resolution when multiple users edit simultaneously?","channel":"frontend","subChannel":"web-apis","difficulty":"advanced","tags":["dom","fetch","websocket","service-worker"],"companies":["Affirm","Broadcom","Roblox"]},{"id":"q-419","question":"You're building a real-time food delivery tracking app. How would you implement a WebSocket connection that handles network interruptions and maintains order status updates when the app goes offline?","channel":"frontend","subChannel":"web-apis","difficulty":"intermediate","tags":["dom","fetch","websocket","service-worker"],"companies":["Apple","DoorDash","MongoDB"]},{"id":"q-4211","question":"Design a beginner-friendly real-time activity feed: fetch initial items via fetch('/api/feed'), open a WebSocket for live updates, and register a Service Worker to cache assets and manage offline UI. Explain data flow, error handling, and provide minimal code snippets for fetch, ws, and service worker registration?","channel":"frontend","subChannel":"web-apis","difficulty":"beginner","tags":["dom","fetch","websocket","service-worker"],"companies":["Coinbase","NVIDIA","Snap"]},{"id":"q-426","question":"You're building a real-time chat application that needs to work offline. How would you implement a service worker to cache messages and sync them when the user comes back online?","channel":"frontend","subChannel":"web-apis","difficulty":"beginner","tags":["dom","fetch","websocket","service-worker"],"companies":["Anthropic","Hugging Face","Snap"]},{"id":"q-4560","question":"You're building an offline-first, collaborative whiteboard app that uses a WebSocket for real-time drawing events, fetch for metadata, and a service worker for offline caching. How would you ensure exactly-once delivery, offline event replay, and DOM update performance when reconnection happens, while preserving cursor and stroke positions across edits?","channel":"frontend","subChannel":"web-apis","difficulty":"advanced","tags":["dom","fetch","websocket","service-worker"],"companies":["Apple","IBM","Zoom"]},{"id":"q-493","question":"You're building a real-time grocery delivery tracking app for Instacart. How would you implement a service worker strategy to handle intermittent connectivity, ensure order updates are delivered via WebSockets, and maintain a consistent UI state across network drops?","channel":"frontend","subChannel":"web-apis","difficulty":"advanced","tags":["dom","fetch","websocket","service-worker"],"companies":["Instacart","MongoDB"]},{"id":"q-663","question":"You’re building a chat UI that loads initial messages with fetch('/api/messages'), caches assets via a service worker, and receives live updates over a WebSocket at '/ws'. Explain and implement: (1) pre-cache strategy and cache invalidation in the SW, (2) a fetch wrapper with timeout and offline fallback to cache, (3) DOM updates for incoming WebSocket messages, (4) reconciliation when offline vs online and message ordering?","channel":"frontend","subChannel":"web-apis","difficulty":"intermediate","tags":["dom","fetch","websocket","service-worker"],"companies":["Coinbase","Google","LinkedIn"]},{"id":"q-665","question":"Design and implement a minimal real-time dashboard that loads initial data via fetch('/api/data?limit=200'), then opens a WebSocket to wss://host/ws for live updates, and uses a Service Worker to cache the app shell and latest API responses for offline use. Describe how you batch DOM updates to minimize reflows, ensure reconnection logic, and implement a cache-first strategy with network fallback. Provide core code snippets and trade-offs?","channel":"frontend","subChannel":"web-apis","difficulty":"advanced","tags":["dom","fetch","websocket","service-worker"],"companies":["Apple","PayPal"]},{"id":"q-1003","question":"Design a multi-region DR plan for a Cloud Run API that reads from Cloud SQL and writes results to Cloud Storage and BigQuery. Define RPO/RTO targets, cross-region replication strategy for Cloud SQL, data residency constraints, traffic failover through a global load balancer, and automated DR tests. Include monitoring, IAM least privilege, and post-failover reconciliation steps?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Netflix","Tesla","Two Sigma"]},{"id":"q-1017","question":"Design a beginner-friendly ingestion workflow on GCP for daily CSV exports delivered via partner-signed URLs into a Cloud Storage bucket. Implement a Cloud Function (Python) triggered on finalization to validate, parse, and load aggregates into BigQuery, with structured Cloud Logging including a correlation_id. Outline per-project isolation (dev/stage/prod), idempotent replay, and a simple test plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Amazon","Snap","Snowflake"]},{"id":"q-1092","question":"Design a cross-tenant, multi-region data ingestion pipeline on Google Cloud to handle telemetry from partner apps. Data arrives as daily compressed NDJSON in per-tenant Cloud Storage buckets. Build end-to-end using Cloud Storage triggers or Pub/Sub, Dataflow (Beam) for parsing/transformations, and BigQuery with per-tenant datasets. Enforce least-privilege IAM, strict per-project isolation, Private Service Connect, data residency, auditable Cloud Logging, and idempotent replay with watermarking. Include architecture, data mapping, and a practical test plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Microsoft","Snowflake"]},{"id":"q-1195","question":"Design a beginner data-retention automation on GCP for a daily CSV export that lands in a shared Cloud Storage bucket and is ingested into a partitioned BigQuery table. Implement per-environment isolation (dev/stage/prod) by separate buckets and datasets. Create a Cloud Scheduler job that triggers a Cloud Function (Python) to apply 30-day retention on storage objects, prune BigQuery partitions, and log using Cloud Logging with a correlation_id. Outline validation tests and rollback plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Coinbase","NVIDIA"]},{"id":"q-1233","question":"Design an advanced, per-tenant data lake on Google Cloud for a SaaS platform serving 100 enterprise customers. Ingest on‑prem JSON logs via Pub/Sub to regional Cloud Storage, and use Dataflow to write per‑tenant BigQuery datasets with CMEK; ensure data locality, least‑privilege IAM, and exfiltration controls via VPC Service Controls and Private Service Connect. Outline observability, auditability, and a test plan for IAM changes and retention?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Anthropic","Lyft","Robinhood"]},{"id":"q-1308","question":"Design a beginner-friendly, cost-aware data ingestion pipeline on GCP for a fleet of devices sending JSON events to Pub/Sub. Create per-environment isolation with separate projects, topics, and BigQuery datasets (dev/stage/prod). Use Dataflow for streaming processing into BigQuery, optimize costs with regional resources, and implement Cloud Billing budgets with alerts plus a simple rollback and test plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Lyft","Netflix"]},{"id":"q-1430","question":"Design a multi-tenant ingestion pipeline on GCP where raw data lands in per-tenant GCS buckets across projects and streams into a single partitioned BigQuery dataset. Implement per-tenant isolation, CMEK, least-privilege IAM, cross-project sharing via authorized views, and end-to-end auditing. Include validation, schema evolution, and cost controls?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Citadel","Discord","MongoDB"]},{"id":"q-1483","question":"Design an advanced, multi-tenant streaming pipeline on GCP for a SaaS analytics product. Tenants across 20 projects publish events to Pub/Sub; a Dataflow streaming job validates per-tenant schemas and writes to per-tenant BigQuery datasets with daily partitions. Include least-privilege IAM, per-tenant service accounts, CMEK for BigQuery, cross-project Private Service Connect, idempotent writes, dead-letter handling, retention, and a complete test plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Airbnb","Cloudflare","Tesla"]},{"id":"q-1519","question":"Design a cross-region data ingestion and analytics pipeline on GCP for a global app. Ingest user events (JSON) from Pub/Sub in two regions, store raw data in regional Cloud Storage buckets with CMEK, process with region-specific Dataflow templates to write per-event aggregates to a partitioned BigQuery dataset, and emit redacted summaries to a separate table. Enforce per-environment isolation, VPC Service Controls, and IAM least privilege. Include end-to-end tests and a rollback plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Citadel","Instacart","OpenAI"]},{"id":"q-1636","question":"Design a region-aware streaming fraud pipeline in Google Cloud for a global payments platform. Ingest via Pub/Sub per region, deduplicate and enrich with Dataflow, score in a Cloud Run service, store raw events in Cloud Storage (CMEK) and scores in BigQuery (partitioned by region/tenant); enforce per-tenant isolation via separate projects and IAM; use VPC Service Controls; implement regional DR and monitoring/alerts. Describe architecture, IAM roles, and testing plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Slack","Square","Tesla"]},{"id":"q-1667","question":"Design a beginner-friendly GCP data pipeline: daily partner CSVs arrive in Cloud Storage via signed URLs; build a Dataflow (Python) batch pipeline to validate, deduplicate by id, and upsert into a date-partitioned BigQuery table. Implement per-env isolation with separate buckets/datasets, a dead-letter path for bad rows, and Cloud Logging correlation_id. Include a simple test plan and rollback steps?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Microsoft","Netflix"]},{"id":"q-1802","question":"Design a beginner-friendly, multi-environment GCP data-ingestion pipeline: partner daily CSV exports arrive via signed URLs into per-environment GCS buckets; build a Node.js Cloud Function that validates final URLs and triggers an Apache Beam Dataflow job to scrub PII and load into a partitioned BigQuery table. Outline IAM least-privilege, testing, and observability?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Cloudflare","Discord"]},{"id":"q-1825","question":"Design a cross-region streaming analytics pipeline on GCP where raw events from EU users must remain data-resident, while derived aggregates are queried globally. Describe your architecture using Pub/Sub, Dataflow (Streaming), BigQuery, and Cloud Storage; enforce least-privilege IAM and per-environment isolation; implement data residency checks and automated rollbacks. How would you validate and test this end-to-end?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Coinbase","Netflix","Tesla"]},{"id":"q-1937","question":"Design an end-to-end, multi-tenant event analytics pipeline on Google Cloud: ingest per-tenant events via Pub/Sub push subscriptions, deduplicate and normalize using Dataflow, store data in dedicated BigQuery datasets per tenant, implement strict per-tenant IAM and VPC boundaries, and ensure full auditability with Cloud Audit Logs/Cloud Logging. Include schema evolution handling with a registry and an accompanying test strategy?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Discord","Snowflake","Two Sigma"]},{"id":"q-2050","question":"Design a cost-conscious, multi-env event-driven pipeline for IoT telemetry on GCP: ingest from Pub/Sub, process with Dataflow (Beam) or Cloud Run, and store in partitioned BigQuery. Enforce per-env isolation with distinct topics/subscriptions and datasets; implement idempotent processing via insertId; provide end-to-end replay tests and a rollback plan. How would you implement monitoring and rollback?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Meta","PayPal","Scale Ai"]},{"id":"q-2077","question":"Advanced multi-tenant data lake on GCP: each tenant has isolated Cloud Storage prefixes and BigQuery datasets. Propose architecture enforcing per-tenant IAM conditions, Private Service Connect and VPC Service Controls for restricted egress, and central metadata with Data Catalog/Dataplex. Include ingestion, isolation validation, auditing, and rollback?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Coinbase","Snowflake"]},{"id":"q-2130","question":"Design a beginner-friendly, cost-conscious GCP ingestion and aggregation workflow for daily partner logs uploaded as signed URLs to Cloud Storage. Provide per-environment isolation (dev/stage/prod) via separate prefixes and BigQuery datasets, a Python Cloud Function triggered on finalization to validate, parse, and load aggregates into a partitioned BigQuery table, and simple monitoring/budget alerts. Address idempotence, a light test plan, and a lightweight reconciliation step?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Slack","Snap"]},{"id":"q-2160","question":"Design a secure, multi-tenant data-sharing and analytics pipeline on Google Cloud for a platform hosting ML models (similar to Hugging Face) where customers upload datasets via signed URLs to per-tenant Cloud Storage buckets, data is processed by Dataflow into per-tenant BigQuery datasets, and model training is kicked off in Vertex AI. Include per-tenant IAM least privilege, VPC Service Controls, data residency constraints, audit logging, and automated cost-guardrails?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Cloudflare","Google","Hugging Face"]},{"id":"q-2206","question":"Design a multi-tenant data ingestion and analytics pipeline on Google Cloud for a SaaS platform. Each customer must have isolated BigQuery datasets; data arrives via Pub/Sub and Cloud Storage, processed by Dataflow, and dashboards read from BigQuery. Explain isolation, schema evolution, cost attribution, auditing, and rollback tests?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Adobe","Discord","Meta"]},{"id":"q-2301","question":"Design a cross-project data ingestion and analytics pipeline on Google Cloud that ingests real-time transaction events from multiple partner systems into a per-partner, per-environment BigQuery dataset with strict data residency, isolation, and audit requirements. Use Pub/Sub or Dataflow for streaming, apply envelope encryption with Cloud KMS, ensure exactly-once processing, enable per-project IAM least privilege, implement automated data retention and DR, and provide a test plan and rollback?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Citadel","Oracle","Scale Ai"]},{"id":"q-2423","question":"Design an advanced, compliant data pipeline on GCP for ingesting patient telemetry from clinics in two regions, ensuring data residency, no public endpoints, and per-project isolation. Build an event-driven flow using Pub/Sub, Private Service Connect, Cloud Run/Functions, Dataflow, and BigQuery. Include IAM least privileges, VPC Service Controls boundaries, audit logging, and a rollback plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Adobe","Google"]},{"id":"q-2451","question":"Design a cost-conscious, multi-tenant data pipeline on GCP for a SaaS product. Each tenant's data must live in isolated Cloud Storage buckets and BigQuery datasets with strict IAM boundaries. Implement a daily event ingestion from Pub/Sub to Dataflow, partitioned tables, CMEK encryption, and per-tenant retention. How would you validate tenant isolation, auditability, and rollback?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Apple","MongoDB","Twitter"]},{"id":"q-2580","question":"Design an intermediate-level streaming data pipeline on GCP that ingests user activity from Pub/Sub into a partitioned BigQuery table, applying real-time PII redaction via Data Loss Prevention with per-tenant masking policies stored in a central config. Include CMEK encryption for sensitive fields, tenant-scoped IAM, Cloud Logging audits with correlation_id, and a rollback/replay plan using raw data retained in Cloud Storage. What components, data model, and trade-offs would you choose?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Bloomberg","Cloudflare","Twitter"]},{"id":"q-2627","question":"Design an end-to-end, compliance-focused model-prediction pipeline on GCP for sensitive user data. Data arrives as CSV in per-env Cloud Storage, then is de-identified by Data Loss Prevention and fed to Vertex AI hosting behind Private Service Connect. Enforce per-project isolation, least-privilege IAM, no public endpoints, and robust data lineage. Outline architecture, IAM, perimeters, and a testing plan including replay-safe ingestion and rollback?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Hugging Face","OpenAI"]},{"id":"q-2675","question":"Design a beginner-friendly, per-tenant data processing pipeline on GCP. Incoming JSON messages arrive on Pub/Sub with a tenant_id. Create an event-driven solution using a Cloud Function (Python) to route records to per-tenant BigQuery datasets, with per-environment isolation (dev/stage/prod) and least-privilege IAM. Include a simple rollback using backups in Cloud Storage?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Adobe","NVIDIA","Snap"]},{"id":"q-2777","question":"Design a multi-tenant, real-time analytics pipeline on Google Cloud that serves several customers while guaranteeing strict data isolation, auditable access, and per-tenant cost attribution. Ingest via Pub/Sub, process with Dataflow, store in per-tenant BigQuery datasets and CMEK-protected buckets behind Private Service Connect and VPC Service Controls. Include data catalog lineage, logging/auditing, and replay tests?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Coinbase","Databricks","Tesla"]},{"id":"q-2820","question":"Design a beginner-friendly ingestion workflow on GCP for daily CSV exports delivered via partner-signed URLs into a shared Cloud Storage bucket. The file set includes a tenant_id field. Build a Dataflow batch pipeline (Python Beam) that triggers on new files, validates the schema, normalizes data, and writes to per-tenant BigQuery tables, with environment isolation (dev/stage/prod). Include idempotent processing via per-file checksum and a load manifest, plus a simple rollback approach and a test plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Adobe","Hugging Face","Twitter"]},{"id":"q-2861","question":"Design a cross-cloud analytics ingestion and sharing pipeline where daily partner exports arrive as CSV/Parquet in a GCS bucket (per-env, per-tenant), are validated and normalized by Dataflow, stored in per-tenant BigQuery datasets, and automatically mirrored to Snowflake in a per-tenant schema for downstream BI. Include least-privilege IAM, per-tenant isolation, end-to-end audit logging with correlation IDs, and a robust, testable rollback. Outline deployment orchestration (Cloud Build + Cloud Composer), data lineage, cross-region DR, and cost controls across GCP, Snowflake, and AWS?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Amazon","Snowflake"]},{"id":"q-2871","question":"Scenario: A partner provides daily JSONL exports via signed URLs. Build a cost-conscious, beginner-friendly GCP ingestion: fetch to Cloud Storage, validate schema, load to a partitioned BigQuery table, with per-environment isolation (dev/stage/prod), idempotent replays via per-file checksum, and a lightweight anomaly detector that raises Cloud Monitoring alerts for 3x historical variance in a field. Outline tests and rollback steps?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Hashicorp","Scale Ai","Twitter"]},{"id":"q-2960","question":"Design an intermediate-scale, multi-tenant real-time ingestion on GCP: streaming events from Pub/Sub into BigQuery with per-tenant isolation, dynamic data masking for PII at ingestion, and cross-region failover; specify IAM, network boundaries (VPC-SC, Private Service Connect), processing path (Dataflow vs Cloud Run), and a practical validation and rollback plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Anthropic","Snap","Stripe"]},{"id":"q-3071","question":"Design an advanced, scalable, tenant-aware analytics pipeline on Google Cloud for a SaaS product that streams events from many tenants. Ingest via Pub/Sub, process with Dataflow (Python), and write per-tenant results to a shared BigQuery dataset. Enforce access with BigQuery Row-Level Security and IAM Conditions, and export per-tenant aggregates to dedicated Cloud Storage buckets via signed URLs. Include per-tenant isolation, least-privilege IAM, auditability, rollback, and a testing strategy. How would you approach end-to-end?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Databricks","Salesforce","Tesla"]},{"id":"q-3129","question":"Design a scalable, per-tenant data-sharing gateway on GCP that serves partner apps via Private Service Connect. Implement strict per-tenant isolation (projects, datasets, buckets), streaming updates from Pub/Sub and Dataflow to BigQuery, and an auditable access trail in Cloud Logging. Include cost budgets/alerts and a rollback plan for schema or IAM changes. What components and trade-offs would you choose?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Amazon","Oracle","Square"]},{"id":"q-3211","question":"Scenario: Build a multi-tenant analytics ingest on GCP where partner web logs are written to per-tenant Cloud Storage buckets and loaded into per-tenant BigQuery datasets. Design a scalable, region-aware pipeline (Pub/Sub → Dataflow → BigQuery) with strict tenant isolation, residency rules, and cost controls. Include IAM, monitoring, and a rollback strategy if data integrity is compromised?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Cloudflare","Coinbase","Twitter"]},{"id":"q-3402","question":"Design a secure, multi-tenant data ingestion pipeline on Google Cloud for daily partner feeds delivered as ZIP files containing JSON events. Ingest into Cloud Storage, unzip and validate with Dataflow, apply per-tenant PII masking via Cloud Data Loss Prevention, and load into per-tenant BigQuery datasets across dev/stage/prod. Enforce least-privilege IAM, VPC Service Controls, idempotent replay, and an auditable rollback plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Discord","Scale Ai","Snap"]},{"id":"q-3503","question":"Design a multi-tenant analytics ingestion pipeline on Google Cloud: daily JSON logs arrive via signed URLs into a shared Cloud Storage bucket. Propose a cost-efficient, secure path using Dataflow (Python) that deduplicates by per-file checksum, redacts PII, and writes partitioned tables by tenant to BigQuery. Enforce per-environment isolation (dev/stage/prod) with separate buckets and datasets. Include idempotency, correlation_id logging, validation, tests, and a rollback plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["DoorDash","Meta"]},{"id":"q-3537","question":"Design a beginner-friendly GCP pipeline that ingests daily JSON logs from partner-signed URLs into a regional GCS bucket, then uses a Dataflow (Python Beam) batch job to redact PII fields (email, phone) and load sanitized records into per-environment BigQuery tables. Include per-environment isolation (dev/stage/prod), a simple idempotent per-file checksum strategy, and a minimal test plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Apple","Lyft","Tesla"]},{"id":"q-3556","question":"In a multi-tenant analytics platform on GCP, partners upload daily CSVs into regional Cloud Storage buckets. Build an event-driven pipeline using Pub/Sub and Dataflow that ingests these files, validates per-tenant schemas, writes to per-tenant partitions in regional BigQuery datasets, and enforces per-environment isolation with separate projects, service accounts, and KMS keys. Include a rollback plan, tests, and data-locality considerations if a tenant migrates regions. How would you implement this?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Google","LinkedIn","Meta"]},{"id":"q-3757","question":"You operate a SaaS data platform on GCP serving multiple tenants. Ingest streaming events from Pub/Sub and write per-tenant aggregates into separate BigQuery datasets with strict isolation. Design a cross-region disaster recovery plan that meets RPO 15 minutes and RTO 1 hour, covering data replication, automatic failover, failover testing, and rollback. Include IAM, network controls, and cost safeguards?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Apple","Hashicorp","Salesforce"]},{"id":"q-3775","question":"Design a real-time analytics DR pipeline in GCP that tolerates regional failure. Data sources publish to Pub/Sub in us-central1. Build a Dataflow streaming job that reads from Pub/Sub and writes to BigQuery in us-central1, with a hot standby in us-east1 that can take over with minimal changes. Explain failover triggers, latency goals, data-loss bounds, idempotent writes, monitoring, and testing plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Meta","NVIDIA","OpenAI"]},{"id":"q-3834","question":"Scenario: A multi-tenant analytics platform ingests partner data into a shared Cloud Storage bucket. Design an end-to-end GCP solution that automatically redacts PII using Cloud Data Loss Prevention, then routes to per-tenant BigQuery partitions. Ensure per-environment isolation (dev/stage/prod) via separate projects, buckets, and datasets; implement a Python Dataflow pipeline, handle schema evolution, maintain audit logs, and provide a privacy-centric test and rollback plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Apple","Databricks","Slack"]},{"id":"q-3862","question":"Design an end-to-end, PCI-DSS aware data lake on Google Cloud for payment analytics. Raw events arrive regionally via Pub/Sub, land in regional Cloud Storage, and are processed by Dataflow (Python) that masks PII using the Cloud Data Loss Prevention API, then writes to a partitioned BigQuery warehouse with per-environment datasets (dev/stage/prod) and cross-region DR. Enforce access with IAM and Authorized Views, track lineage with Data Catalog, and implement audits, rollback, and testing plans?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["PayPal","Tesla"]},{"id":"q-3925","question":"Design a real-time, multi-tenant data ingestion and analytics pipeline on GCP. Ingest events per tenant via Pub/Sub to a central Cloud Run service; normalize and write to per-tenant BigQuery datasets in a single region, using CMEK. Enforce least-privilege IAM Conditions, Data Catalog metadata, and strict cross-tenant access controls; ensure data residency and complete auditability. Outline schema, data flow, security, tests, and rollback?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Coinbase","MongoDB","Stripe"]},{"id":"q-4045","question":"Design an end-to-end GCP data ingestion and analytics pipeline for partner JSON events delivered via Pub/Sub. Ensure per-tenant isolation with a separate BigQuery dataset per tenant and CMEK; implement a streaming Dataflow (Python) that validates schema, deduplicates by (tenant_id, event_id), and upserts into per-tenant tables; include idempotency, a load-manifest rollback, IAM/VPC Service Controls isolation, and a regional failover plan with tests?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Airbnb","NVIDIA","Slack"]},{"id":"q-4094","question":"In a multi-tenant fintech on GCP, ingest real-time transaction events via Pub/Sub. Design a streaming pipeline where Dataflow writes per-tenant records to dedicated BigQuery datasets, while calling Vertex AI for anomaly scores and storing results alongside the rows. Ensure least-privilege IAM, per-tenant isolation, and VPC Service Controls. Include idempotency, rollback, and monitoring plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Bloomberg","Microsoft","Square"]},{"id":"q-4180","question":"Design a per-tenant, event-driven pipeline on GCP for daily partner CSVs: daily files land in per-tenant Cloud Storage buckets; publish a Pub/Sub message with tenant_id and file_md5; a Dataflow batch job validates schema, writes to per-tenant BigQuery datasets (partitioned by date). Include per-tenant KMS keys, least-privilege IAM, idempotent load via manifest, and a rollback plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Amazon","Google","Hugging Face"]},{"id":"q-4324","question":"Design a beginner-friendly GCP data ingestion flow: daily CSV exports arrive via partner-signed URLs into a Cloud Storage bucket; build a Python Cloud Function triggered on finalization to validate schema, run a Cloud Data Loss Prevention (DLP) scan to redact or mask PII, and load aggregates into a partitioned BigQuery table; implement per-environment isolation (dev/stage/prod) and a simple test plan with correlation_id logging?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Apple","Goldman Sachs","Snowflake"]},{"id":"q-4492","question":"In a multi-tenant SaaS on GCP with 12+ tenants, design a streaming data ingestion and analytics pipeline that preserves strict per-tenant isolation, uses per-tenant encryption keys, and scales to peak load. Data arrives via Pub/Sub; process with a Dataflow streaming template; land in per-tenant BigQuery datasets. Describe security (IAM, encryption, VPC), idempotency (dedupe, MERGE), error handling, and observability?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Google","Netflix","Twitter"]},{"id":"q-4510","question":"Design a beginner-friendly GCP ingestion workflow using a metadata-first approach with Cloud Data Catalog. Daily partner CSV exports land in a shared Cloud Storage bucket; a Cloud Function (Python) finalizes each file, validates a manifest, assigns a Data Catalog tag with tenant_id, source, and version, and routes data to per-tenant BigQuery datasets via Pub/Sub. Include environment isolation (dev/stage/prod), idempotent processing (per-file checksum + load manifest), and a simple test plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Airbnb","Lyft","Microsoft"]},{"id":"q-4586","question":"Design a beginner-friendly data privacy workflow on GCP: daily partner CSV exports arrive in Cloud Storage and contain PII. Build a pipeline that uses Cloud Functions and the Cloud DLP API to classify and redact PII before loading into a BigQuery dataset. Include per-environment isolation, idempotent processing (checksum + manifest), and structured logging with correlation_id. What would you implement and why?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["NVIDIA","Netflix","Stripe"]},{"id":"q-4676","question":"Scenario: A SaaS product emits JSON events to a Google Pub/Sub topic per customer. Design a beginner-friendly, tenant-isolated, streaming ingestion on GCP: a Dataflow (Python) job reads events, validates schema, aggregates metrics, and writes per-tenant totals to BigQuery. How would you ensure idempotence, per-environment isolation (dev/stage/prod), observability, and a basic test plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Tesla","Two Sigma"]},{"id":"q-4729","question":"Design a beginner-friendly, event-driven ingestion on GCP: partner events arrive as JSON via Pub/Sub to a single topic in project A. Implement a Python Cloud Function triggered by Pub/Sub that validates payload fields (event_id, tenant_id, timestamp, metric), writes per-tenant aggregates into BigQuery with environment isolation (dev/stage/prod via separate datasets and topics), and uses event_id for idempotent processing. Include a simple test plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Cloudflare","NVIDIA","Snap"]},{"id":"q-4841","question":"Design an advanced cross-region data lake on GCP for a multi-tenant SaaS. Ingest per-tenant JSON events from Pub/Sub, deduplicate with event_id, materialize Parquet in Cloud Storage via a streaming Dataflow job, and load into per-tenant partitioned BigQuery tables. Enforce least-privilege IAM, per-project isolation, and VPC Service Controls; include a cross-region DR plan with active-passive failover, RPO/RTO targets, and a test/rollback strategy?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["MongoDB","Robinhood","Tesla"]},{"id":"q-878","question":"How would you implement a beginner-friendly, auditable deployment pipeline in Google Cloud for a Cloud Run app that reads from Cloud SQL and writes logs to Cloud Logging, ensuring least-privilege IAM, per-project isolation, and no public endpoints?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Cloudflare","Goldman Sachs","IBM"]},{"id":"q-907","question":"Design a private, regional data pipeline for a global fintech platform: events land in regional Pub/Sub topics, Dataflow performs streaming ETL, results stored in per-region BigQuery, and audit logs go to Cloud Logging. Enforce per-region IAM, least privilege, CMEK, Private Service Connect, and no public egress. Describe data flow, security controls, disaster recovery, and cost implications. How would you implement this pipeline?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Goldman Sachs","Hugging Face","Instacart"]},{"id":"q-977","question":"In a beginner setup, you deploy a Cloud Run API behind Private Service Connect, with logs going to Cloud Logging and traces to Cloud Trace. Outline a practical observability plan: which metrics, logs, and traces to collect; how to build a useful dashboard; how to configure a low-noise alert for 5xx latency; and a simple test to validate instrumentation and alerting?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Instacart","PayPal"]},{"id":"q-1016","question":"Design a regional streaming pipeline for a fintech app: on‑prem and GKE emit events to region Pub/Sub topics; a Dataflow streaming job enforces exactly-once, writes partitioned regional BigQuery tables, and triggers Vertex AI scoring in near real-time. How would you achieve low latency, data residency, schema evolution, and reliable failure recovery?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Adobe","PayPal","Stripe"]},{"id":"q-1055","question":"Design an audit-logging pipeline for a payments platform on GCP with sub-100ms end-to-end write latency at multi-region scale (millions of events/sec). Ingest via Pub/Sub, process with Dataflow (Beam), and sink to BigQuery. Explain how you ensure idempotent writes (insertId), handle schema evolution, and provide auditor access across projects without exposing sensitive data. Also outline retries, backoffs, and monitoring?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Amazon","Stripe"]},{"id":"q-1105","question":"You're building a beginner-friendly ingestion pipeline: external partners upload daily CSVs to a Cloud Storage bucket; design a minimal flow using a Cloud Function triggered on object finalize to parse the CSV and load a daily summary as a single row into BigQuery. Include IAM permissions, how to trigger on new files, and how to ensure idempotent writes?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Discord","Microsoft","Snap"]},{"id":"q-1269","question":"Design a real-time data pipeline on GCP for a multi-tenant SaaS where each tenant's data must reside in a specified region, is encrypted with CMEK, and access is strictly controlled per-tenant using IAM Conditions and VPC Service Controls; use Pub/Sub, Dataflow, and BigQuery, ensure idempotent writes and exactly-once semantics, and include monitoring and incident response steps?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Google","PayPal","Salesforce"]},{"id":"q-1331","question":"Design a beginner-friendly nightly backup workflow on GCP: a Cloud SQL MySQL export writes a dump to a Cloud Storage bucket each night; a Cloud Function triggers on the new object to gzip, append a date stamp, and move it to backups/archived. Include IAM bindings, trigger method on new files, and how to ensure exactly-one backup per day (idempotency)?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Instacart","NVIDIA","Tesla"]},{"id":"q-1370","question":"Design a private, streaming ingestion from on-prem to GCP for sensitive financial logs using Pub/Sub, Dataflow, and BigQuery. Include CMEK, IAM, VPC Service Controls, idempotent writes, exactly-once semantics, failure recovery, and cost considerations. Explain choices and trade-offs?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Databricks","IBM","MongoDB"]},{"id":"q-1413","question":"Design a real-time image ingestion pipeline on GCP for a multi-tenant SaaS app. Customers upload images to per-tenant Cloud Storage buckets with CMEK; a Pub/Sub topic notifies on new uploads; a Dataflow streaming job processes images to extract features and writes results to BigQuery, while archived copies are moved to per-tenant archive buckets. Outline architecture, IAM bindings, service account isolation, exactly-once guarantees (e.g., row_id dedup), and cost/latency trade-offs. Include monitoring and failure-response plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Amazon","Apple","IBM"]},{"id":"q-1479","question":"Design a beginner-friendly pipeline on GCP where a daily vendor JSON health report is uploaded to Cloud Storage; create a Cloud Function triggered by finalize to validate JSON against a schema, write valid rows to BigQuery, and move invalid files to a quarantine bucket with a Pub/Sub notice to the on-call channel. Include IAM roles and idempotent processing?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Google","Hashicorp","Hugging Face"]},{"id":"q-1661","question":"You're architecting a multi-tenant data lake on GCP. Ingest partner feeds via Pub/Sub, land raw into Cloud Storage, process with Dataflow streaming, and emit per-tenant results to dedicated BigQuery datasets with IAM controls. How would you enforce isolation, meet <60s latency, support schema evolution, and implement per-tenant cost governance and quotas? Include IAM bindings, Dataflow templates, and error handling?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Two Sigma","Uber"]},{"id":"q-1677","question":"In a high-value fintech setting, design a globally distributed, PCI-compliant fraud-detection pipeline on GCP that ingests events from on-prem via Private Service Connect, publishes to Pub/Sub, processes with Dataflow, stores raw and processed data in Cloud Storage and BigQuery, and uses Vertex AI for real-time scoring; describe IAM, CMEK, VPC, and failure handling, with idempotency?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Citadel","Robinhood"]},{"id":"q-1708","question":"Daily vendor CSVs arrive in Cloud Storage; design a beginner-friendly ingestion pipeline that first scans each file with Cloud DLP to detect PII, and if PII is found moves the file to a quarantine bucket and publishes an alert; if not, parse the CSV and load daily rows into a partitioned BigQuery table. Include IAM bindings, object-finalize trigger, and idempotent processing?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Databricks","Discord","Uber"]},{"id":"q-1729","question":"Design a beginner-friendly thumbnail pipeline for user-uploaded images in GCP. Images uploaded to Cloud Storage bucket incoming-images should trigger a Cloud Run container to generate two thumbnails (200px and 400px wide) and store them in image-thumbs. Use a Cloud Function to trigger on finalization and invoke the Cloud Run HTTP endpoint. Output names: <orig>_200_<hash>.jpg and <orig>_400_<hash>.jpg to ensure idempotency. IAM: restrict access so only the Cloud Run service account can write to image-thumbs. Include failure handling and a basic test plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Discord","NVIDIA","Snowflake"]},{"id":"q-1864","question":"Given a fintech app ingesting market ticks from multiple regions via Pub/Sub, design an intermediate streaming pipeline to load data into BigQuery with deduplication, schema evolution, and per-region partitioning. Use Dataflow for ETL, ensure exactly-once processing, handle late data up to 2 minutes, implement TTL retention on raw data, and outline monitoring and testing strategy?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Bloomberg","Google","Salesforce"]},{"id":"q-1912","question":"Design a beginner-friendly end-to-end GCP pipeline for user avatars stored in Cloud Storage. On object finalize, a Cloud Function should invoke a Cloud Run service to produce two thumbnails (100x100 and 256x256), store them in avatars-resized with deterministic naming, and publish a log entry. If processing fails, publish a Pub/Sub alert and quarantine the original file. Keep IAM minimal and describe a basic test plan and idempotency strategy?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["MongoDB","Twitter","Zoom"]},{"id":"q-2061","question":"Design an end-to-end cross-region streaming data pipeline on GCP to ingest high-volume telemetry from Pub/Sub into BigQuery with real-time dashboards. Use Dataflow (Beam) for streaming, enable exactly-once processing, implement automatic primary/DR region failover, use multi-region Pub/Sub topics and cross-region BigQuery datasets, handle schema evolution, and provide monitoring and rollback strategies?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Bloomberg","IBM"]},{"id":"q-2100","question":"Design a beginner-friendly, event-driven data validation pipeline on GCP for daily event JSONs uploaded to Cloud Storage: on file finalize, a Cloud Function validates each record against a simple JSON Schema, quarantines invalid files, and writes valid records to a BigQuery table with partitioning by date and a deterministic write-ID to ensure idempotency; explain IAM bindings and a basic test plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Airbnb","Databricks","Stripe"]},{"id":"q-2214","question":"You run a SaaS platform on GCP and collect telemetry per tenant via Pub/Sub. Design an end-to-end, multi-region ingestion and analytics pipeline that ensures tenant isolation, exactly-once processing, and scalable cost control. Requirements: (a) topic/subscription layout with a central ingress project and per-tenant downstream sinks; (b) Dataflow streaming job that deduplicates using event_id, enriches with metadata from a centralized store, and writes to daily-partitioned BigQuery tables per tenant; (c) reliable dead-letter handling; (d) security (IAM, CMEK, and VPC Service Controls); (e) cross-region DR and testing plan. Provide a diagram?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Bloomberg","Discord","IBM"]},{"id":"q-2245","question":"Design a real-time, multi-tenant clickstream pipeline on Google Cloud Platform. Ingest events per-tenant from Pub/Sub, enrich with product catalog data stored in BigQuery, compute 5-minute per-tenant engagement metrics, and persist both raw enriched events and per-tenant aggregates to BigQuery. Address exactly-once semantics, late data, per-tenant IAM, data cataloging, and failure handling with a dead-letter queue?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Plaid","Uber"]},{"id":"q-2365","question":"Design a multi-tenant data ingestion and analytics pipeline on GCP for three customers (Snap, Nvidia, Zoom). Each tenant streams events to Pub/Sub, which Dataflow consumes and writes to a central BigQuery data lake with per-tenant isolation using CMEK-encrypted Cloud Storage staging and BigQuery Authorized Views/Row-Level Security. Include idempotent processing, audit logging, cost controls, and a test plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["NVIDIA","Snap","Zoom"]},{"id":"q-2438","question":"Design a cross-region, multi-tenant analytics pipeline on GCP that ingests streaming user events from Cloud Pub/Sub, processes with Dataflow, and writes per-tenant BigQuery datasets in the region of arrival. Include strict data isolation via IAM and VPC Service Controls, encryption (CMEK), backup, failover, and a canary rollout plan for tenants?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Databricks","DoorDash","Oracle"]},{"id":"q-2549","question":"You receive telemetry events from a mobile app into Pub/Sub. Design a beginner-friendly end-to-end ingestion on GCP: a Pub/Sub topic telemetry, a Cloud Run HTTP push endpoint processes messages, validates JSON schema, and writes to BigQuery telemetry.events using streaming inserts with insertId = event_id to guarantee idempotency. Include IAM bindings, retry strategy, and a basic test plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Airbnb","Amazon","Databricks"]},{"id":"q-2643","question":"Design a beginner-friendly ingestion pipeline on GCP for mobile app analytics. When a JSONL file lands in Cloud Storage (analytics-logs), a Cloud Function validates each event, de-duplicates by event_id, and appends to a date-partitioned BigQuery table analytics.events. Use insertId for idempotency. Minimal IAM, exponential backoff retries, and a basic test plan with sample files and duplicates?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Bloomberg","DoorDash"]},{"id":"q-2667","question":"Design a real-time telemetry pipeline on Google Cloud Platform for a multi-region ride-hailing fleet that ingests vehicle events from Pub/Sub, deduplicates by event_id, performs streaming enrichment and 1-minute window aggregations in Dataflow, and writes to a region-partitioned BigQuery table with CMEK. Include raw archival in Cloud Storage, cross-region failover, IAM, and a test plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Databricks","Two Sigma","Uber"]},{"id":"q-2728","question":"Design a real-time feature store pipeline on GCP for a fraud-detection model. Ingest 120k events/sec from multiple services into Pub/Sub, stream to Vertex AI Feature Store online and BigQuery for offline features, with cross-region replication, CMEK, and IAM least privilege. Include an idempotent write strategy, testing plan, and a drift-monitoring approach with alerting within 5 minutes?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["IBM","Oracle","Square"]},{"id":"q-2807","question":"Describe a beginner-friendly GCP ingestion workflow: CSV logs arrive in Cloud Storage at logs-origin; a Cloud Function validates the header schema, parses rows, and deduplicates by a per-row key; inserts into a date-partitioned BigQuery table logs.events using insertId for idempotency. Include minimal IAM bindings, exponential backoff retries, and a test plan with sample files and duplicates?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Adobe","Apple","Instacart"]},{"id":"q-2826","question":"Design an intermediate-level, end-to-end GCP streaming ingestion for telemetry data from 10k devices. Devices push JSON messages to a Cloud Pub/Sub topic. Build an idempotent pipeline using Dataflow (Beam) that deduplicates by event_id, aggregates per minute per device_type, and writes results to a partitioned BigQuery table telemetry.events (date partition). Include exactly-once considerations, dead-letter handling, and IAM least privilege. Provide testing plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Cloudflare","Tesla"]},{"id":"q-3014","question":"Design a beginner-friendly privacy-conscious ingestion workflow on GCP for CSV analytics logs. When a CSV file lands in gs://analytics-logs/csv/, a Cloud Function validates the header and redacts PII fields (emails and phone numbers) in every row, writing a sanitized file to gs://redacted-logs/csv/ with the same name. A second Cloud Function loads the sanitized file into a date-partitioned BigQuery table analytics.logs, using a deterministic loadId for idempotency. Outline IAM least privilege, a basic test plan with sample input including PII, and how you would verify redaction and idempotent loads?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Goldman Sachs","NVIDIA","Netflix"]},{"id":"q-3058","question":"Design a beginner-friendly nightly cleanup pipeline on GCP. In bucket gs://user-uploads/temp, files older than 7 days should be moved to gs://archive/YYYY/MM/DD/ preserving original name, and the original should be deleted. Triggered by Cloud Scheduler; Cloud Function performs listing, copy, delete, and logs a row in BigQuery with insertId = filePath+timestamp. Include minimal IAM, retries, and a test plan with sample files and duplicates?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Amazon","Apple","Stripe"]},{"id":"q-3090","question":"Design a real-time, cross-region streaming pipeline on GCP to replicate high-volume financial transactions from a primary OLTP DB to an analytics warehouse. Explain ingestion (CDC from Cloud SQL using Debezium on GKE), Pub/Sub topics, Dataflow streaming with deduplication and exactly-once semantics, enrichment, and error handling; store into regional BigQuery datasets, with cross-region DR and automatic failover; outline IAM, DLQ, cost controls, and a practical test plan including outage simulations?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Databricks","Snap","Snowflake"]},{"id":"q-3143","question":"Advanced: design a production-ready IoT telemetry ingestion pipeline on GCP with strict data isolation and exactly-once guarantees. Devices upload JSONL to Cloud Storage at gs://telemetry-origin; a Cloud Function validates schema and forwards records to Pub/Sub; a Dataflow streaming job consumes Pub/Sub, deduplicates by event_id across tenants, and writes to a per-tenant partitioned BigQuery table telemetry.events using insertId for idempotency. Include IAM bindings, retries,-VPC Service Controls considerations, and a test plan with representative data including duplicates?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Apple","NVIDIA","Oracle"]},{"id":"q-3307","question":"Design a beginner-friendly ingestion pipeline on GCP for server logs. When a file lands in gs://srv-logs/raw/server-logs-*.jsonl, a Cloud Function validates each JSON line (host, ts, level, msg); invalid lines go to gs://srv-logs/dlq/, valid lines are written to BigQuery via streaming insert into analytics.logs with insertId = host|ts|hash(msg). Include IAM, exponential backoff retries, and a basic test plan with duplicates?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Amazon","Meta","Snap"]},{"id":"q-3344","question":"Design a real-time ingestion pipeline to process 1M user activity events per second from mobile clients. Events arrive as JSON in Pub/Sub and must be enriched (e.g., profile lookup), de-duplicated by event_id, and written to a date-partitioned BigQuery table analytics.events with insertId semantics. Propose architecture (Pub/Sub topics, Dataflow/Beam, BigQuery), late data handling, schema evolution strategy, error handling, and a minimal IAM model. Include a test plan with synthetic events and duplicates; justify trade-offs?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Cloudflare","Meta","Slack"]},{"id":"q-3466","question":"In GCP, you operate a real-time clickstream pipeline: Pub/Sub -> Dataflow streaming job computes session-level metrics (visit duration, pages per session) and writes to BigQuery. Data can arrive late and out-of-order. Design an end-to-end strategy to ensure timely yet accurate metrics: include watermarking, windowing, late data handling, dedup, idempotent writes to BigQuery, and testing with synthetic data. Provide concrete DoFn ideas and config choices?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Robinhood","Stripe","Uber"]},{"id":"q-3539","question":"Design a beginner-friendly GCP ingestion flow for daily activity dumps stored in gs://raw-activity/YYYY/MM/DD/logs.jsonl. Implement a Cloud Function (Python) triggered on finalization to validate lines, deduplicate by (user_id, event_id), and write to a date-partitioned BigQuery table activity.events using insertId. Add a Pub/Sub replay path and DLQ for failures; keep IAM minimal and provide a basic test plan with sample data?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Apple","Snap","Uber"]},{"id":"q-3609","question":"Design a fault-tolerant, multi-region ingestion pipeline for real-time clickstream analytics on GCP: events arrive as JSON in regional Pub/Sub topics; a Dataflow streaming job deduplicates by event_id and writes to per-region BigQuery datasets with insertId, while a centralized BigQuery dataset aggregates near-real-time metrics. Explain exactly-once semantics, cross-region replication, IAM boundaries, and a test plan including regional failover scenarios?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["LinkedIn","Netflix"]},{"id":"q-3628","question":"Design a real-time content moderation pipeline on GCP for a globally distributed social app. Messages arrive via regional Pub/Sub topics; implement a Dataflow streaming job that calls a Vertex AI deployed moderation endpoint to score each post. If score >= threshold, publish to a human-review Pub/Sub and write a row to BigQuery with insertId=event_id. Enforce regional processing for data residency; discuss latency, scaling, idempotency, and failure handling?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Meta","Uber"]},{"id":"q-3658","question":"Design a real-world GCP pipeline that automatically redacts PII from new CSV uploads in gs://incoming-csvs using the DLP API, writes redacted copies to gs://redacted-csvs with a _redacted suffix, triggers on Finalize, processes large files in parallel with Dataflow, logs to BigQuery with insertId=filePath+timestamp, ensures idempotency, uses minimal IAM, and provides a robust test plan including duplicates and failure alerts. How would you implement and validate this?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Cloudflare","Databricks","Discord"]},{"id":"q-3783","question":"Design a cross-project, multi-tenant event bus on GCP: producers in project A publish JSON events to a shared Pub/Sub topic; a sink in project B routes events to per-tenant BigQuery datasets and to per-tenant GCS archives, enforcing per-tenant IAM conditions and VPC Service Controls. Explain isolation, exactly-once processing, and failure handling; include a minimal test plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Bloomberg","Google"]},{"id":"q-3840","question":"Design a beginner-friendly CSV ingestion pipeline on GCP: when a CSV file lands in gs://data-exports/csv/YYYY/MM/DD/, a Cloud Function triggered on finalize validates headers, checks numeric columns, and emits a message to Pub/Sub. A second Cloud Function (or Dataflow) consumes the Pub/Sub message and appends to a date-partitioned BigQuery table analytics.sales_YYYYMMDD, using insertId=fileName_rowNumber to ensure idempotency. Include minimal IAM bindings and a simple test plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Bloomberg","Oracle","Robinhood"]},{"id":"q-3894","question":"Design a cross-region telemetry ingest pipeline on GCP for Nvidia edge devices. Ingest JSON events via Pub/Sub (per region), normalize with Dataflow Streaming, deduplicate using device_id+sequence, and write to a partitioned BigQuery table telemetry.events with insertId. Include idempotency guarantees, disaster recovery to a secondary region, testing plan, and cost/monitoring strategies?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Amazon","NVIDIA"]},{"id":"q-4006","question":"Design a beginner-friendly, privacy-aware data routing pipeline on GCP: new user event files land as JSON in gs://company-logs/tenant-*/incoming/. Build a Cloud Function (triggered on object finalize) that validates schema, redacts PII according to a policy file in gs://company-logs/policies.json, and writes sanitized rows to a date- and tenant-partitioned BigQuery table analytics.events. Use insertId for idempotency, minimal IAM, and a basic test plan with sample tenants and edge cases?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Citadel","Goldman Sachs","Tesla"]},{"id":"q-4090","question":"Design an end-to-end real-time telemetry pipeline on GCP for a vehicle fleet. Vehicles publish JSON lines to Pub/Sub (fleet.telemetry). Implement a Dataflow streaming job to parse, deduplicate by message_id, anomaly-detect (speed > 120, abnormal geo drift), and write to BigQuery analytics.fleet_events with daily partitions. Publish alerts to Cloud Tasks for remediation. Include minimal IAM, dead-letter Pub/Sub, test plan, and monitoring strategy. How would you implement?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Anthropic","Tesla","Uber"]},{"id":"q-4111","question":"Design a beginner-friendly data redaction pipeline on GCP: when JSONL files land in gs://customer-logs, a Cloud Function triggers on finalize, reads each line, redacts PII fields (email, phone, SSN) per a schema, writes redacted lines to gs://redacted-logs/YYYY/MM/DD/<orig>.jsonl, and inserts a daily summary into BigQuery (date, file, redacted counts). Use minimal IAM, exponential backoff retries, and a test plan with sample lines including missing fields and nulls?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Coinbase","Databricks","Twitter"]},{"id":"q-4146","question":"Design a beginner-friendly file-audit pipeline on GCP: when a CSV lands in gs://payments/ingest/, a Cloud Function validates headers (sid, amount, currency, ts), normalizes currency to USD, and writes cleaned rows to gs://payments/clean/YYYY/MM/DD/filename.cleaned.csv. It should also append an audit row to BigQuery (insertId = filePath+ts). If any row is invalid, publish a message to Pub/Sub and quarantine the file. How would you implement this end-to-end, including idempotency and test plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Goldman Sachs","Lyft","Plaid"]},{"id":"q-4157","question":"Design a beginner-friendly GCP pipeline for per-customer data isolation when new data arrives in Cloud Storage: each customer has a bucket gs://customer-data/<customerId> that receives JSON lines files. Create a Cloud Function triggered on object finalize that copies the file to an archive bucket gs://archive-<customerId>/YYYY/MM/DD/<orig>, updates the IAM so only the customer’s service account can read the archived data, and appends a manifest row to BigQuery with fields customerId, filePath, timestamp. Include idempotency, minimal IAM bindings, and a simple test plan with sample files?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Goldman Sachs","Square"]},{"id":"q-4202","question":"Design a real-time, multi-tenant data ingestion and analytics pipeline on GCP for a SaaS app. Ingest per-tenant events into Pub/Sub, process with Dataflow to apply per-tenant isolation, write to a partitioned BigQuery dataset, and enforce per-tenant retention and audit trails. Include VPC Service Controls/Private Service Connect, Cloud DLP masking, IAM bindings, cross-region replication, and a practical test plan for burst traffic?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Salesforce","Zoom"]},{"id":"q-4267","question":"Design a secure, cost-aware telemetry ingestion pipeline for multi-tenant data on GCP. Inbound JSON logs arrive in gs://telemetry-raw and include tenant_id and event_id. Build an end-to-end flow with Cloud Functions (finalize), Pub/Sub, Dataflow, and BigQuery that isolates tenants, ensures idempotent processing (insertId = tenant_id|event_id|ts), supports schema evolution, and quarantines invalid events. Include a minimal IAM policy and a test plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Meta","MongoDB","Netflix"]},{"id":"q-4385","question":"Design a real-time anomaly detection pipeline on GCP for authentication events. Source: Pub/Sub topic projects/ORG/auth.events; stream to Dataflow (Python) with dedup by event_id, 30s per-user windows, and per-user fail_rate = failures/attempts. Trigger alert if fail_rate > 0.3; store enriched events in BigQuery analytics.auth_events partitioned by date. Ensure exactly-once processing, idempotent BigQuery writes, and cost controls. Include a test plan and trade-offs?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Discord","Goldman Sachs","Two Sigma"]},{"id":"q-4504","question":"Design a beginner-friendly end-to-end GCP pipeline to process user-uploaded videos. When a video is uploaded to gs://user-videos/raw/, a Cloud Function should enqueue a Cloud Task that calls a Cloud Run service to transcode to 720p and 360p MP4 outputs stored in gs://user-videos/processed/720p/ and gs://user-videos/processed/360p/ with names <orig>_<hash>.mp4. Ensure idempotency via the hash, handle Cloud Tasks retries, and keep IAM minimal. Provide a basic test plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Citadel","Google","Scale Ai"]},{"id":"q-4541","question":"Design a real-time data masking and routing pipeline on GCP for Pub/Sub telemetry-raw. Build a Dataflow streaming pipeline that reads from Pub/Sub, sources masking rules from Firestore (collection policy.telemetry) and applies dynamic masking to PII fields (e.g., email, phone_number, device_id). Support rule updates without redeploy, write masked events to BigQuery (telemetry.region tables partitioned by event_date) and archive a copy to Cloud Storage. Late data handling, dead-letter Pub/Sub, idempotent writes, and minimal IAM must be considered. Include a concise test plan and policy-update scenario?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Microsoft","Square"]},{"id":"q-4553","question":"Design a real-time anomaly detection pipeline for user login events across multiple regions on GCP. Ingest via Pub/Sub, run a streaming Dataflow (Beam) pipeline computing a rolling per-user risk score using features like IP, device, region, and time of day. Persist features and scores to BigQuery, publish anomalies to a separate Pub/Sub topic, and trigger a Cloud Run remediation service when a threshold is crossed. Consider idempotency, privacy, tracing, and a practical test plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["LinkedIn","Square"]},{"id":"q-4789","question":"Design a real-time analytics pipeline on Google Cloud for IoT telemetry: devices publish JSON to Pub/Sub; a Dataflow streaming job parses and window-aggregates per device to compute anomaly scores using a lightweight model, writes anomalies to BigQuery, and triggers alerts via a Cloud Function to Slack; also emit enrichment artifacts to Cloud Storage. How would you ensure exactly-once semantics, late data handling, and minimal IAM, with a testing plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Amazon","Google"]},{"id":"q-904","question":"How would you configure a Cloud Run (fully managed) service to securely connect to a Cloud SQL PostgreSQL instance using a private connection, including IAM bindings and deployment steps to ensure the app talks via the Cloud SQL socket and never uses the instance's public IP?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["OpenAI","Slack","Snap"]},{"id":"q-1222","question":"Design a real-time ad-click analytics pipeline in GCP that ingests Pub/Sub events via Dataflow into BigQuery, while implementing 90-day TTL on PII data, exporting audits to Cloud Storage, and handling schema evolution, late data, dedup, and rollback. Provide concrete architecture, data models, and operational steps?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Cloudflare","NVIDIA","Snap"]},{"id":"q-1310","question":"You operate a streaming ingestion pipeline: Pub/Sub -> Dataflow -> BigQuery. Daily 50k events with fields user_id, event_type, amount (which can be string). Propose a simple ingestion-time data quality plan that validates JSON schema, coerces types, and routes invalid records to a dead-letter Pub/Sub topic without stopping the pipeline. Include testing with a small sample and how you monitor invalid counts?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Lyft","PayPal","Snap"]},{"id":"q-1319","question":"Design a compliant streaming data platform in GCP for a fintech app that ingests transaction events from Pub/Sub through Dataflow into BigQuery. New fields may appear over time; PII must be detected and masked before analytics, while raw data is retained with restricted access. Outline the end-to-end architecture, data models, schema evolution strategy, PII handling, lineage, retention, and operational monitoring with concrete steps and trade-offs?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Bloomberg","Plaid","Robinhood"]},{"id":"q-1389","question":"You receive streaming JSON events from Pub/Sub. Design a beginner-friendly GCP pipeline using Dataflow (Beam) to parse/flatten events, write to a partitioned BigQuery table (partition by event_date), and deduplicate by event_id. Late data allowed up to 6 hours. Outline architecture, data model, schema-change handling, and monitoring?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["MongoDB","Snap","Square"]},{"id":"q-1415","question":"You are building a multi-source data platform where raw data arrives as Parquet in Google Cloud Storage, Avro via Pub/Sub, and JSON through a partner API; design a two-stage pipeline that normalizes all formats into a canonical Parquet dataset, ingested into a partitioned BigQuery table, with schema drift handling, data validation, and end-to-end lineage. Include how you would implement idempotent ingests, rollback, and cross-team access controls?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Google","Snowflake","Uber"]},{"id":"q-1449","question":"Design a streaming enrichment pipeline: Pub/Sub → Dataflow → BigQuery, with an inline governance layer enforcing per-record rules (required fields, value ranges, cross-field consistency) at ingestion. Records failing rules go to a quarantine Cloud Storage bucket; good records load to a partitioned BigQuery table. Describe data model, rule engine approach, idempotence, backpressure, and testing/monitoring plan?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Apple","Instacart","Uber"]},{"id":"q-1473","question":"In a beginner friendly GCP data pipeline Pub/Sub -> Dataflow -> BigQuery design robust observability and recovery. Describe how to instrument Dataflow with metrics backlog throughput processed failed, implement a dead letter policy that dumps failed JSON messages to Cloud Storage, set retry backoff, and outline a replay path to re ing est from Cloud Storage. Include a concrete alert rule backlog throughput greater than 0.2 for 5 minutes and a brief example snippet?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Goldman Sachs","IBM"]},{"id":"q-1546","question":"In GCP, ingest daily Pub/Sub JSON events through Dataflow into BigQuery. Implement basic observability and reliability: 1) emit metrics for ingested, processed, and failed events; 2) export metrics to Cloud Monitoring and alert on failures for three consecutive checks; 3) deduplicate by event_id; 4) log audits to Cloud Storage. Outline architecture, data model, and concrete steps?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Coinbase","Microsoft","Salesforce"]},{"id":"q-1581","question":"In Pub/Sub, daily JSON events contain PHI fields. Design a beginner-friendly GCP pipeline using Dataflow (Beam) to redact PHI with Cloud DLP before loading into BigQuery, and emit a separate audit log to Cloud Storage. Outline architecture, data model, and validation steps, including how you’d monitor masking failures?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Amazon","Google","Uber"]},{"id":"q-1596","question":"Design a streaming pipeline: Pub/Sub → Dataflow (Beam) → BigQuery. Core: a dynamic data quality gate using a Firestore-stored policy engine that validates fields/types at ingest; invalid records serialized to Cloud Storage as JSONL with metadata; valid records load to BigQuery; publish quality metrics to Cloud Monitoring; handle drift/backpressure without downtime. Explain architecture, data model, and operational steps?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Cloudflare","Hashicorp","Scale Ai"]},{"id":"q-1722","question":"Design a cross-region, real-time analytics pipeline for a fintech on GCP. Ingest 100 GB/day of JSON events from Pub/Sub into BigQuery while ensuring per-tenant data isolation, a 90-day TTL on PII, and a rollback path. Show the architecture, data models, and operational steps for schema evolution, late data, dedup, and cost control?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Apple","Oracle","Tesla"]},{"id":"q-1754","question":"Design a beginner-friendly GCP pipeline to load daily 2-5 GB of newline-delimited JSON logs from Cloud Storage into BigQuery. Keep only the latest 45 days in a partitioned table (partition by load_date, clustered by record_id). Ensure dedup by record_id, handle optional new fields with a permissive schema, allow late data up to 24 hours, and provide basic monitoring. Choose between Dataflow (Beam) or Cloud Functions, justify, and outline data model, transformations, error handling, and testing strategies?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Hugging Face","Lyft","Oracle"]},{"id":"q-1787","question":"Design a data quality framework for a streaming pipeline ingesting 500 GB/day of JSON events from Pub/Sub into BigQuery. Specify in-flight schema validation, per-field checks (types, nullability, ranges), quarantining bad records, and automatic schema evolution with Data Catalog, plus end-to-end monitoring, alerting, and rollback procedures?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Apple","Goldman Sachs","PayPal"]},{"id":"q-1834","question":"Design a cross-tenant data governance pipeline on GCP for 1 TB/day of JSON user activity ingested from Pub/Sub into BigQuery in two regions. Enforce per-tenant privacy with field-level masking, auto-generate end-to-end data lineage with Data Catalog, support backward-compatible schema evolution, and export immutable audit logs to Cloud Storage. Include architecture, data models, rollback plan per-tenant (within 24h), and testing strategy?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Apple","Tesla"]},{"id":"q-1869","question":"Design a GCP streaming pipeline for 2 TB/day of multi-tenant JSON events from Pub/Sub into BigQuery. Create a schema registry in Cloud Storage, validate with a Beam DoFn, quarantine nonconforming records to an invalid dataset, and apply per-tenant DLP masking. Enforce row-level access with BigQuery policies, capture lineage in Data Catalog, and provide rollback and testing plan?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Goldman Sachs","Snap","Uber"]},{"id":"q-1906","question":"Ingest a daily batch of JSON records stored in Cloud Storage into BigQuery via a Dataflow (Beam) job. Build a beginner-friendly pipeline that validates a minimal schema (tenant_id, event_id, event_ts), filters out records missing required fields, and deduplicates by (tenant_id, event_id). Load valid data into a partitioned BigQuery table; write invalid records to a separate GCS errors bucket. Include basic health counters and a simple template?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Amazon","Twitter"]},{"id":"q-1953","question":"Design a GCP data pipeline for a multinational SaaS product that ingests 200 GB/day of JSON telemetry from Pub/Sub into BigQuery across two regions. Requirements: multi-tenant isolation, automatic schema evolution, per-tenant TTL, data quality checks (schema conformance, nulls, duplicates), and an automated rollback path for schema changes. Include data model sketches, partitioning strategy, testing, and rollback steps?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Apple","Hashicorp","NVIDIA"]},{"id":"q-1997","question":"Ingest daily 1 TB JSON exports dumped into Cloud Storage by partner apps. Design a beginner-friendly GCP batch pipeline to load into BigQuery with per-tenant isolation (datasets), ingestion-date partitioning, and a simple schema-evolution strategy (new fields added as nullable columns). Include data-quality checks (nulls, types) and a rollback path to revert a day’s ingest within 24h. Outline architecture and testing?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Databricks","Meta","Uber"]},{"id":"q-2073","question":"On GCP, design an observability-driven pipeline that detects schema drift and data-quality anomalies in near real-time for 1 TB/day of JSON events ingested from Pub/Sub into BigQuery across two regions. Include per-tenant lineage, automatic schema evolution, drift-triggered alerts, a self-healing rollback path, and export of audit trails to Cloud Storage. Provide architecture, data models, tests, and operational playbooks?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Anthropic","Databricks","Google"]},{"id":"q-2153","question":"Design a cost-aware multi-tenant GCP data pipeline for streaming telemetry that ingests 150 GB/day of JSON from Pub/Sub into BigQuery across two regions. Enforce per-tenant access with BigQuery row level security and per-tenant dataset versioning; support backward compatible schema evolution; and export versioned snapshots to Cloud Storage. Include data models, partitioning, testing, rollback, and synthetic-tenant generation?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["NVIDIA","Oracle","Snap"]},{"id":"q-2227","question":"Design a real-time data ingestion pipeline on GCP for a game analytics platform: 60 GB/day of JSON events from Pub/Sub into BigQuery across two regions. Enforce per-tenant isolation with field-level masking, automatic schema evolution, and a per-tenant canary rollout with a controlled rollback path. Include data models, testing strategy, lineage via Data Catalog, and cost controls. Provide a concrete implementation plan?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Discord","Meta","OpenAI"]},{"id":"q-2364","question":"Design a cross-tenant data exchange on GCP: ingest 1 TB/day of JSON telemetry from Pub/Sub into two BigQuery regions. Implement per-tenant isolation via dataset-level access controls and masked views, and use Data Catalog policy tags to enforce visibility. Model a pragmatic two-layer architecture: raw + curated; ensure end-to-end lineage; support controlled data sharing via explicit contracts. Provide a 24h per-tenant rollback plan and testing strategy?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Google","NVIDIA","Zoom"]},{"id":"q-2410","question":"Design an end-to-end GCP data ingestion pipeline for 1-2 TB/day of JSON telemetry streamed from Pub/Sub into BigQuery with per-tenant isolation and strict data residency policies. The solution must mask PII per tenant at ingestion (field-level), use CMEK, and employ DLP where needed. Provide two-region architecture, automatic schema evolution, late data handling, and a rollback/backfill plan, plus end-to-end data lineage in Data Catalog and audit exports. Include architecture, data model, testing, and runbook details?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Bloomberg","Hugging Face","Tesla"]},{"id":"q-2523","question":"On GCP, design a data mesh for 3 domains (Sales, Product, Ops) where each domain owns its BigQuery data products, publishes a standard schema, and uses Data Catalog and IAM for cross-domain governance. Include an automated lineage from source events to data products, a policy-driven access layer with per-domain permissions, and a testing/rollback plan for schema drift. Provide concrete data models and workflow?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Netflix","Snowflake","Two Sigma"]},{"id":"q-2602","question":"Design a multi-tenant, streaming data platform on GCP for a financial app that ingests 1 TB/day of JSON events via Pub/Sub, processes them with Apache Beam on Dataflow, and writes per-tenant data products to BigQuery across three regions. Explain per-tenant isolation, automatic schema evolution, late data handling, data quality checks, rollback paths, cross-region replication, Data Catalog lineage, and testing strategy?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Microsoft","Robinhood","Stripe"]},{"id":"q-2744","question":"Design a multi-tenant data pipeline on GCP that ingests 50–100 GB/day of JSON events from Pub/Sub into BigQuery across two regions. Implement per-tenant isolation with dedicated datasets and topics, a robust data quality observability layer that validates schema conformance, nulls, duplicates, and timeliness, and real-time alerts. Include an automated remediation path (schema bumps and selective re-ingestion) and a clear rollback strategy using time travel. Provide architecture, data models, partitioning, testing, and rollback steps?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Anthropic","Google","Square"]},{"id":"q-2755","question":"Design a two-region, cross-tenant data lakehouse on GCP for 2 TB/day of JSON events ingested via Pub/Sub, storing raw Parquet in regional Cloud Storage and tenant-scoped tables in BigQuery; include per-tenant isolation, automatic schema evolution, late-arrival handling, TTL, and end-to-end lineage with Data Catalog. Provide data model, workflows, and rollback strategy?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Amazon","Google"]},{"id":"q-2855","question":"Design a real-time cross-tenant data pipeline on GCP ingesting 1-2 TB/day of JSON events from Pub/Sub into two regions. Implement per-tenant privacy via field-level masking and tokenization using DLP and Cloud KMS, enforce data residency rules, provide end-to-end lineage with Data Catalog, support forward/backward schema evolution, and build a testing strategy with synthetic tenants. Include a rollback plan for privacy policy changes within 24 hours?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Airbnb","NVIDIA","Twitter"]},{"id":"q-2903","question":"Design a real-time cross-tenant telemetry pipeline on GCP for 5 TB/day of JSON events ingested via Pub/Sub and stored into per-tenant BigQuery tables across two regions. Requirements: strict tenant isolation, field-level privacy masking at ingestion (PII redaction with DLP), auto schema evolution with backward compatibility, late-arrival handling, per-tenant TTL retention, and immutable audit logs exported to Cloud Storage. Include end-to-end lineage via Data Catalog, rollback strategy, testing plan, and concrete data model sketches, Dataflow templates, and failure modes?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Netflix","Tesla"]},{"id":"q-2958","question":"Design a real-time, cross-tenant analytics pipeline on GCP that ingests 4 TB/day of JSON clickstream from Pub/Sub across three regions, with per-tenant isolation: raw data stored regionally as Parquet, aggregated BI-ready tables in BigQuery; enforce field-level masking with DLP; automatic schema drift handling using Data Catalog and Dataplex; maintain end-to-end lineage; and implement a rollback plan for tenant-specific schema changes within 24 hours, including testing and failover playbooks?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["PayPal","Tesla","Twitter"]},{"id":"q-2982","question":"Design a per-tenant data-sharing microservice on GCP that ingests 100 GB/day of JSON activity from Pub/Sub into BigQuery, then serves tenant-scoped datasets to external partners via a Cloud Run API. Include per-tenant isolation with partitioned BigQuery tables, authorized views for external access, IAM/Endpoints-based OAuth, audit trails to Cloud Logging, and automatic schema drift handling with backward-compatible updates and rollback?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Google","Instacart","PayPal"]},{"id":"q-2999","question":"On GCP, design a multi-tenant streaming validation and quality framework for 10-20B events/day from Pub/Sub into BigQuery across regions. Each tenant has its own schema and masking rules. Describe the end-to-end data model, per-tenant schema registry, per-tenant masking/quality rules, lineage via Data Catalog, and a canary rollback plan to the last-good schema within 6 hours?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Anthropic","Snap"]},{"id":"q-3093","question":"Design a batch Dataflow pipeline that reads per-tenant JSON logs from Cloud Storage, validates against a schema.json in GCS, flattens nested fields, and writes to a BigQuery table partitioned by ingest_date with event_id dedup. Allow late-arriving files up to 24 hours via re-ingest, and emit an audit log to Cloud Storage plus a quick observability summary to Pub/Sub?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["MongoDB","Slack","Snap"]},{"id":"q-3251","question":"You operate a multi-tenant analytics service on GCP with 50 tenants. 5 GB/day of JSON events arrive via Pub/Sub. Design a beginner-friendly pipeline that ingests events with Dataflow, stores them in a single BigQuery table partitioned by ingestion_date, and includes tenant_id. Enforce per-tenant access using BigQuery authorized views (or per-tenant filters), support additive schema evolution, deduplicate by event_id, handle late data up to 1 hour, and capture end-to-end lineage in Data Catalog. Provide architecture sketch and a minimal test plan?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Twitter","Two Sigma","Zoom"]},{"id":"q-3270","question":"Design a multi-tenant feature store on GCP for real-time ML inference using Vertex AI Feature Store with online/offline stores and BigQuery as the offline layer. Ingest features via Pub/Sub; enforce per-tenant isolation and versioned schemas; support canary rollouts and rollback, with testing and Data Catalog lineage. Outline data model, ingestion, validation, and monitoring?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Adobe","Microsoft","Zoom"]},{"id":"q-3415","question":"New streaming data: 150 MB/day of JSON click events arrive via Pub/Sub and must be ingested into BigQuery in a single region. Design a beginner-friendly GCP pipeline using Dataflow (Beam) to parse, validate required fields, and enrich with a userId lookup; store results in a daily-partitioned BigQuery table; implement idempotent writes using insertId or MERGE; route malformed messages to a dead-letter Pub/Sub topic and archive originals to Cloud Storage; propose a 24-hour rollback plan and a basic data quality/monitoring approach?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["NVIDIA","Salesforce"]},{"id":"q-3512","question":"On GCP, design a two-tenant data platform for an advertising analytics service ingesting 800 GB/day of JSON events via Pub/Sub. Each tenant has isolated dashboards and ML models. Build versioned data contracts in Data Catalog, enforce them via a registry, route data to per-tenant BigQuery tables and to Vertex AI Feature Store for ML, apply per-tenant PII masking, and maintain end-to-end lineage. Include testing, a canary rollout, and a per-tenant rollback window (6 hours)?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Bloomberg","Databricks","Salesforce"]},{"id":"q-3553","question":"On GCP, build a streaming ML feature pipeline that ingests 1 TB/day of JSON user events via Pub/Sub, computes features in Dataflow, stores in BigQuery and Vertex AI Feature Store, and traces end-to-end lineage for model training. Describe data models, feature store integration, drift detection, rollback strategy, and how you'd reproduce training with exact feature versions?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Instacart","Tesla","Two Sigma"]},{"id":"q-3716","question":"Design a GCP-based multi-tenant analytics platform for 300 tenants ingesting 500 GB/day of JSON events via Pub/Sub. Build per-tenant data products in BigQuery with strict isolation (Authorized Views), enforce data contracts via Data Catalog, mask PII at ingest with DLP, and store immutable audit logs in Cloud Storage. Include automatic schema evolution, event-time processing with late arrivals (Dataflow), cross-region replication, and a 24h tenant rollback?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Google","Meta","Oracle"]},{"id":"q-3753","question":"Design a cross-tenant analytics platform on GCP that ingests 2 TB/day of JSON events per tenant via Pub/Sub, stores raw Parquet in regional Cloud Storage, and serves per-tenant aggregated views in BigQuery with strict isolation. Implement per-tenant differential privacy budgets at query time using DP libraries and UDFs, track budgets in Data Catalog, and enable a 24h rollback of privacy budgets. Include data models, lineage, testing strategy, and rollback plan?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Amazon","Apple","Instacart"]},{"id":"q-3795","question":"Design a GCP streaming ML feature pipeline for five tenants: ingest 250 GB/day of JSON events from Pub/Sub into Vertex AI Feature Store (online for real-time scoring and offline for batch) with per-tenant isolation, data contracts in Data Catalog, and PII masking at ingest. Include schema evolution, drift monitoring, cross-region replication, and a 24h rollback plan?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Bloomberg","Salesforce","Slack"]},{"id":"q-3879","question":"Design a cost-aware, two-region multi-tenant analytics pipeline on GCP that ingests 1 TB/day of JSON events via Pub/Sub into BigQuery. Add tenant-scoped data quality scoring at ingest, per-tenant row-level isolation, and automated schema drift rollback with canary deployments. Include data models, partitioning, testing plan, and rollback strategy per tenant?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Instacart","Plaid","Salesforce"]},{"id":"q-3926","question":"Design a cost-aware, multi-tenant streaming pipeline: Pub/Sub feeds 1 TB/day of JSON events; Dataflow writes per-tenant, day-partitioned BigQuery tables (tenant_id, user_id). Run a shared Vertex AI anomaly model with per-tenant thresholds via side inputs; store scores in BigQuery and publish alerts to Pub/Sub. Archive older data to Cloud Storage Coldline; Data Catalog provides lineage. Rollback uses canary deployment and per-tenant guardrails; test with synthetic data and drift checks?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Adobe","Snap","Stripe"]},{"id":"q-4022","question":"Design a two-region streaming pipeline on GCP for multi-tenant analytics (Pub/Sub → Dataflow → BigQuery). Enforce per-tenant isolation (Authorized Views), two-region active-active replication, exactly-once processing, late data handling, and per-tenant TTL (90 days). Support automatic schema evolution and provide a rollback plan using versioned tables and canary validation?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Hugging Face","Plaid"]},{"id":"q-4317","question":"Design a beginner-friendly GCP pipeline that ingests 300 MB/day of JSON streaming events from Pub/Sub into BigQuery in a single region. Requirements: 1) parse and validate fields: eventId, deviceId, ts, metric, value; 2) normalize ts to UTC; 3) deduplicate by eventId; 4) allow up to 2 minutes lateness for a 1-minute sliding window; 5) produce a daily-partitioned BigQuery summary with average value per device/metric; 6) write raw JSON to Cloud Storage and publish malformed messages to a DLQ Pub/Sub; 7) provide a 24-hour rollback by deleting last day’s summary partition and reprocessing; 8) basic monitoring?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Goldman Sachs","Google","Microsoft"]},{"id":"q-4364","question":"Design a real-time, multi-tenant fraud-detection pipeline on GCP for 100 tenants, each with ~200k events/day via Pub/Sub; implement per-tenant Vertex AI Feature Store, streaming scoring with Dataflow, model versioning, explainability logs, and drift monitoring; enforce isolation with IAM and Authorized Views, ensure end-to-end lineage via Data Catalog, and provide a rollback to a previous model version within 2 hours plus a testing strategy?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Cloudflare","Tesla","Uber"]},{"id":"q-4495","question":"Batch ingestion: 500k JSON events/day stored in Cloud Storage (gs://data/events/YYYY/MM/DD/*.json). Build a beginner-friendly Dataflow (Python) pipeline that parses JSON, validates required fields (user_id, event_type, ts), enriches with a user tier lookup from BigQuery analytics.user_traits, and writes to analytics.events$YYYYMMDD (daily partition) using insertId for idempotence. Route malformed records to gs://data/events/errors/YYYY-MM-DD/ and outline a 24h rollback approach plus basic data quality monitoring?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Apple","Google","Lyft"]},{"id":"q-4569","question":"Design a GCP streaming fraud-detection pipeline for ride-hailing that ingests 1–2 TB/day of JSON events via Pub/Sub into two regions. Enforce per-tenant contracts with schema validation, isolate tenants with dedicated datasets, compute real-time features and anomaly scores in Dataflow, and expose a low-latency BigQuery view. Include end-to-end lineage via Data Catalog, automatic drift rollback, and a concrete testing plan?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Instacart","Lyft","Uber"]},{"id":"q-4631","question":"Design a GCP streaming pipeline for 1 TB/day of multi-tenant clickstream events ingested via Pub/Sub into per-tenant BigQuery datasets. Include isolated tenants with separate schemas, backward-compatible schema evolution, per-tenant data masking at ingest (DLP), end-to-end lineage via Data Catalog, and a rolling 24h rollback that replays data for affected tenants without touching others; also specify per-tenant cost controls with slot reservations. Provide architecture, data models, and testing strategy?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Google","Tesla","Uber"]},{"id":"q-4731","question":"Design a real-time, cross-tenant analytics pipeline on GCP ingesting 2 TB/day of JSON events via Pub/Sub into BigQuery across two regions. Enforce per-tenant isolation with field-level masking, support automatic schema evolution, and implement a tenant-scoped data replay (24h window) without impacting others. Add end-to-end lineage in Data Catalog and a Vertex AI-based data observability layer. Describe architecture, data models, rollback, and tests?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Citadel","Hugging Face","Snap"]},{"id":"q-4827","question":"Design a real-time, cross-region, multi-tenant analytics pipeline on GCP that ingests 3 TB/day of JSON events from Pub/Sub into BigQuery in two regions. Each tenant must be isolated (per-tenant datasets or authorized views), with field-level privacy masking at ingest, automatic schema evolution, and end-to-end lineage captured in Data Catalog. Include data model, partitioning / clustering, rollback plan per tenant within 24h, testing strategy, and how you'd monitor latency?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Anthropic","Google","Meta"]},{"id":"q-931","question":"A GCP pipeline ingests 1 TB of JSON user activity daily from Pub/Sub into BigQuery via Dataflow. New fields appear over time; you must evolve the schema without downtime. What approach would you use for schema drift and nested fields, and outline concrete steps to implement it?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Citadel","Discord","Uber"]},{"id":"q-962","question":"In a GCP streaming pipeline, Pub/Sub feeds millions of events into BigQuery via Dataflow. Out-of-order arrivals and duplicates occur. Design an idempotent sink using a staging table and a final partitioned table, leveraging insertId for dedup and a MERGE strategy to upsert into the final table. Outline concrete steps and trade-offs to implement?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Databricks","Google","Instacart"]},{"id":"q-1013","question":"You're managing a multi-tenant SaaS on GCP across five projects connected via Shared VPC. You must enforce per-tenant network isolation, IAM conditions, and budget governance while keeping CI/CD simple. Propose an end-to-end setup using Shared VPC, IAM Conditions, VPC Service Controls, Billing Budgets, and Cloud Asset Inventory, and outline testing and rollback steps?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Discord","DoorDash","Snap"]},{"id":"q-1041","question":"Design a scalable, compliant log routing pipeline on GCP that collects logs from Kubernetes clusters, Cloud Run, and Cloud Functions, redacts PII, and stores in BigQuery with environment separation. Include data flow sinks, IAM and CMEK governance, failure modes and retries, and an end-to-end testing plan?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Databricks","Square"]},{"id":"q-1155","question":"You're deploying a globally distributed service on GKE across three regions, with Cloud Run and Cloud Functions used for specific workloads. Design a deployment pipeline that enforces policy-as-code, encryption at rest via CMEK, drift detection, automated rollback, and cross-region failover. Describe tooling, data planes, tests, and how you handle outages and compliance?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["Adobe","NVIDIA","Zoom"]},{"id":"q-1245","question":"You operate a global chat app with components on **GKE**, **Cloud Run**, and **Cloud Functions**. Latency spikes in one region go unnoticed in aggregated metrics. Design an end-to-end observability approach: (1) how to unify traces across runtimes, (2) how to instrument with **OpenTelemetry** and **OTLP** to a central collector, (3) how to build region-scoped dashboards and **SLO-based alerts**, and (4) how to validate during release with **canary** and **chaos testing**. Include tool choices, sample metrics, and a minimal config sketch?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Discord","Meta","Microsoft"]},{"id":"q-1281","question":"You're maintaining a Cloud Run Python service that reads an API key from Secret Manager. Implement a 30-day secret rotation using Cloud Scheduler to publish a rotation event to Pub/Sub, and enable the Cloud Run instance to fetch updated secret without a restart. Detail the IAM permissions, wiring, and a test plan to verify end-to-end rotation?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Hashicorp","Plaid","Robinhood"]},{"id":"q-1514","question":"Design a cost-aware DR plan for a multi-region GKE + Cloud Run service handling 2M events/min. implement active-active with regional load balancers, Istio-based traffic shifting, CMEK, and a VPC Service Controls perimeter. automate failover tests via Cloud Build + Chaos Mesh; define SLOs and error budgets, observability, and automatic rollback triggers on latency/error breaches?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["Airbnb","Hugging Face","Snap"]},{"id":"q-1664","question":"You’re tasked with enabling per-branch ephemeral environments in Google Cloud Platform for a microservice. Every feature branch should provision an isolated Cloud Run service, a dedicated Cloud SQL instance, and Secrets Manager entries, with least-privilege IAM, a per-branch namespace, and automatic teardown after 48 hours. Outline the exact steps using only GCP-native tools (no external CI), including budgets alerts, health checks, and teardown workflow?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Discord","MongoDB"]},{"id":"q-1685","question":"You’re tasked with cost-aware deployment for a Cloud Run service used by a mobile frontend. Create a policy that (1) halts new deployments when the monthly spend exceeds a threshold, (2) scales traffic to 20% during overages, and (3) posts a Slack alert via a Cloud Function if the bill crosses the threshold. Describe exact steps using only GCP-native tools (Billing Budgets, Cloud Monitoring, Cloud Functions) and outline a minimal end-to-end workflow?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["LinkedIn","Lyft"]},{"id":"q-1763","question":"You operate a large-scale IoT ingestion pipeline on GCP: millions of device events per minute flow from Pub/Sub into Dataflow (Apache Beam) and write to BigQuery. Design an architecture that guarantees near real-time processing with exactly-once semantics, handles late data, and supports schema evolution; include windowing, idempotent writes, backpressure, and a rollback/testing plan for Dataflow job updates. What would you implement and why?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Amazon","Google"]},{"id":"q-1819","question":"You manage a Cloud SQL PostgreSQL instance in GCP and need a reliable disaster-recovery workflow with automated daily backups to Cloud Storage. Explain how to implement this using Cloud Scheduler and a Cloud Function that calls the Cloud SQL Admin API to export to gs://my-backups/sql-dump-YYYYMMDD.sql. Include required IAM roles, bucket lifecycle policies, and how you would validate a restore?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Instacart","Snap","Snowflake"]},{"id":"q-1890","question":"In a multi-tenant SaaS on GCP requiring isolated per-tenant networks with a shared services hub, propose a scalable hub-and-spoke topology using Shared VPC and Private Service Connect. Include least-privilege IAM, per-tenant firewall rules, and Private Google Access. How would you implement policy-as-code, drift detection, automated rollback, and observability across tenants?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Discord","Microsoft"]},{"id":"q-1917","question":"Scenario: A Cloud Run service connects to a Cloud SQL instance. Secrets are stored in Cloud Secret Manager and injected into the container at runtime. Propose a beginner-friendly, low-risk, weekly password rotation workflow that updates the secret, triggers a rolling update with zero downtime, and provides an immutable audit trail. Include the exact GCP services you would use and a minimal 3-step sequence to implement?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Amazon","Databricks"]},{"id":"q-2006","question":"You're delivering a multi-tenant SaaS on GKE and Cloud Run with strict data residency and cost controls. Design an end-to-end pattern using only GCP-native tools to achieve per-tenant isolation, regional data stores, Canary/blue-green deployment, CMEK, VPC Service Controls, and automated rollback on degraded telemetry. Outline architecture, steps, and essential config snippets?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["Apple","PayPal","Tesla"]},{"id":"q-2054","question":"Design a beginner-friendly, GCP-native CI/CD for a Cloud Run service with a private artifact repository. Describe how you would: (a) configure Cloud Build to pull a private repo and push a container image to Artifact Registry, (b) inject a database password from Secret Manager into the Cloud Run container at runtime, and (c) rotate that secret monthly using Cloud Scheduler and a Cloud Function that updates Secret Manager. Include the specific services and minimal steps?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Amazon","Google"]},{"id":"q-2234","question":"Scenario: deploy a Cloud Run service private to a specific VPC using Private Service Connect. Detail the Google-native steps: create a Serverless VPC Access connector, set up a PSC endpoint, configure private DNS, restrict ingress to internal, grant a dedicated service account run.invoker, and validate access from a VM inside the VPC while public access is blocked. What is your minimal rollout plan and validation approach?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Apple","Citadel","Instacart"]},{"id":"q-2252","question":"How would you implement a secure, GCP-native deployment pipeline for a Cloud Run service that uses vulnerability scanning and Binary Authorization to ensure only signed, non-vulnerable images are deployed, including how you would integrate Cloud Build attestation and a guard policy?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Discord","Scale Ai"]},{"id":"q-2332","question":"Scenario: you must implement a real-time pricing and fraud-detection service on GCP that ingests from Pub/Sub, processes with Dataflow, and stores results in BigQuery across multiple regions. Design a production-ready pipeline with canary releases, strict IAM, CMEK, and automated recovery. Include deployment strategy, observability, and teardown plan?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["Airbnb","Plaid"]},{"id":"q-2347","question":"You're running a stateful service on GKE with a Cloud SQL primary in US-CENTRAL1 and a read replica in EUROPE-WEST1. Design an end-to-end deployment and DR strategy that (1) supports blue/green or canary rollouts with automatic rollback based on latency and error-rate gates, (2) enables cross-region failover for compute and DB, (3) enforces CMEK for storage and DB, (4) implements policy-as-code and drift detection, (5) includes observability, outage handling, and compliance. Specify tooling, data planes, tests, and failure handling?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Goldman Sachs","Google","Instacart"]},{"id":"q-2383","question":"You're deploying a new Cloud Run service named text-classifier with a v2 revision. You want a 15% canary for 30 minutes and automatic rollback if latency exceeds 1.5s or error rate exceeds 0.5%. Using only GCP-native tools, specify exact steps to build/push v2 to Artifact Registry, split traffic, configure monitoring alerts, and trigger rollback via traffic changes, including minimal IAM requirements?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Discord","Hugging Face","Two Sigma"]},{"id":"q-2398","question":"You're deploying a beginner-friendly Cloud Run service named text-processor. Using only GCP-native tools, design a minimal CI/CD: build and push the container to Artifact Registry, deploy v1 to Cloud Run, implement a simple traffic split (10% dev, 90% prod) for canary testing, require a manual gate before production via an additional Cloud Build trigger, and outline a rollback plan and minimal IAM?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Apple","Meta","NVIDIA"]},{"id":"q-2480","question":"You're running a high-traffic payment API **payline** on Cloud Run (v2) with multi-region active-active behind a Global HTTP(S) Load Balancer. Design a production deployment plan to roll out v3 with automatic rollback based on latency and error-rate metrics. Use only Google Cloud native tools. Specify: canary strategy (**5% for 15 minutes**), traffic-split commands, monitoring alerts, CMEK/Secrets Manager handling, IAM least privilege, and a rollback workflow across regions?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["MongoDB","Stripe"]},{"id":"q-2642","question":"You're designing a streaming ingestion pipeline on GCP: Pub/Sub topic events.raw, Dataflow job events-ingest-v2 (streaming) writing to BigQuery.events_all. Using only Google-native tools, specify a concrete plan to meet: (1) exactly-once processing semantics; (2) 25% traffic canary for v2 with safe rollback; (3) malformed events go to a Dead-Letter topic; (4) automatic rollback to v1 if latency > 1.8s or error rate > 0.8% persists for 10 minutes; include monitoring, alerting, and a traffic-switching workflow that minimizes data loss?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["OpenAI","Snowflake"]},{"id":"q-2677","question":"You're maintaining a small Cloud Run (fully managed) service named image-processor with a Git-driven CI; you want to enable per-branch deployments to isolated Cloud Run services (e.g., image-processor-dev, image-processor-staging, image-processor-prod) using only GCP-native tools. Outline concrete steps to build and push the container to Artifact Registry, deploy each environment with dedicated service accounts and IAM bindings, configure traffic routing so dev/staging canaries don't affect prod, implement a gating flow to promote from dev to staging to prod via Cloud Build triggers, and specify a rollback workflow if latency > 2s or error rate > 1% persists for 10 minutes. Include minimal Secrets Manager usage and monitoring setup?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Google","Snap","Snowflake"]},{"id":"q-2761","question":"You're releasing a new version of a real-time image-processing service on GKE. The service is deployed in us-central1 with a hot standby in us-west1. Design a cross-region canary rollout (v2) using only Google-native tools: 20% traffic for 30 minutes, automatic rollback if end-to-end latency exceeds 0.9s or error rate exceeds 0.3% for 12 minutes. Provide exact steps to build/push to Artifact Registry, deploy with traffic-splitting, configure Cloud Monitoring alerts, and rollback workflow across regions. Include CMEK/Secrets Manager usage and IAM least privilege?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Apple","Databricks","Tesla"]},{"id":"q-2786","question":"You manage a Cloud Run API named directory-service (v1) used across internal apps. To enforce a cost-control policy with only Google-native tools, design a workflow: (a) set a Billing Budget of $100/month with 50% and 90% alerts; (b) mirror these thresholds in a Cloud Monitoring alert policy with email notifications; (c) implement a simple remediation to scale min instances to 0 or pause non-prod revisions when alerted; (d) outline verification steps for alerts and remediation?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Databricks","Meta","Robinhood"]},{"id":"q-2849","question":"You're deploying a beginner-friendly Cloud Run service named image-processor with a v2 revision. Using only Google-native tools, design a minimal canary: 10% traffic to v2 for 15 minutes, automatic rollback to v1 if latency > 1.5s or error rate > 0.5%. Include exact gcloud steps (traffic split, rollback), monitoring alerts, and minimal IAM roles?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Apple","Google","Square"]},{"id":"q-2899","question":"You're deploying a critical microservice on GKE. Use Google-native tools to rollout v2 with a 20% canary for 20 minutes to production, with automatic rollback if latency or error rate cross thresholds. Provide exact steps: build/push v2 to Artifact Registry, update manifests, configure Cloud Deploy canary, set monitoring alerts, and define rollback triggers. Include minimal IAM roles?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Airbnb","IBM","Oracle"]},{"id":"q-3020","question":"You're deploying a Cloud Run (fully managed) service 'parser-service' that ingests Pub/Sub messages and writes to BigQuery. Implement a cross-project blue/green rollout with a DR project and Private Service Connect. Using only Google-native tools, specify exact steps to build/push v4 to Artifact Registry, deploy prod-v4 and dr-v4, configure traffic splits (start: prod 15%, DR 5% for 20 minutes; then 80/20), set monitoring alerts for Pub/Sub backlog and BigQuery latency, assign minimal IAM roles, and implement automatic rollback to v3 if error rate or backlog exceed thresholds across both regions?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["Bloomberg","Citadel","Twitter"]},{"id":"q-3048","question":"You’re deploying a new v3 of a GKE-based order-processor using Cloud Deploy with a canary rollout: 25% of traffic to v3 for 40 minutes; auto rollback if P95 latency > 1.2s or error rate > 0.4% for 5 minutes. Using only Google-native tools, outline exact steps to build/push v3 to Artifact Registry, create a Cloud Deploy delivery config with the canary strategy, wire traffic split, set Cloud Monitoring SLOs/alerts, and implement automatic rollback to v2. Include minimal IAM roles and prerequisites like service accounts and required permissions?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Lyft","PayPal","Robinhood"]},{"id":"q-3126","question":"Deploy a Cloud Run service audit-ingest (v1) to two regions (us-central1 and us-east1) behind a single global HTTP(S) Load Balancer. Default 100% traffic to central; auto-failover to east within 60s if central health-check shows 3 consecutive failures. Use only Google-native tools; provide exact gcloud steps for building/pushing to Artifact Registry, per-region deployments, LB setup, health checks, traffic policy, and minimal IAM?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Cloudflare","Coinbase","Uber"]},{"id":"q-3209","question":"You're releasing a new Cloud Run service revision (v2) that processes real-time events. You want a 20% canary for 40 minutes with automatic rollback if latency p95 > 0.75s or error rate > 0.4% persists for 6 minutes. Using only GCP-native tools, specify exact steps to build/push v2 to Artifact Registry, deploy with traffic-split, set Cloud Monitoring alerts for the two metrics, and rollback by restoring 100% to v1. Include minimal IAM roles and Secrets Manager/CMEK usage?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Meta","OpenAI","Uber"]},{"id":"q-3231","question":"Design a multi-region blue/green rollout for a Cloud Run service (image-processor) in us-central1 and europe-west1. Release v2 with 25% canary traffic per region for 30 minutes; auto-rollback if p95 latency > 0.6s or error rate > 0.3% for 5 minutes. Use only Google-native tools. Provide exact steps to build/push v2 to Artifact Registry, deploy in both regions, configure traffic and monitoring alerts, and rollback to v1. Include minimal IAM and Secrets Manager usage?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Apple","Meta","Microsoft"]},{"id":"q-3362","question":"You're deploying a new v3 of a high-volume API on two GKE clusters in US-EAST1 and US-CENTRAL1 behind a global HTTP(S) Load Balancer. Using Google-native tools only, design a blue/green progressive rollout with region-specific canary: 5% traffic to v3 in US-EAST1 for 15 minutes, alert-triggered rollback if p95 latency > 400ms or error rate > 0.5% for 5 minutes, then promote to full traffic if both regions pass. Include exact steps for building/pushing the image to Artifact Registry, Kubernetes manifests, Cloud Deploy config, monitoring, and IAM?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["LinkedIn","NVIDIA"]},{"id":"q-3502","question":"You're running a multi-region Cloud Run service named video-processor behind a Global HTTP(S) Load Balancer. A new requirement mandates exactly-once processing for streaming events from Pub/Sub across regions, with progressive rollout to v4 and automated rollback on latency or dedupe failures. Using only Google-native tools, design the rollout plan including traffic-shift, Pub/Sub EOD, idempotent processing, dedupe store, alerts, and a rollback workflow with minimal IAM?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["Netflix","Oracle","Stripe"]},{"id":"q-3678","question":"You're deploying a beginner-friendly Cloud Run service named weather-api (v1) that must connect to a private Cloud SQL PostgreSQL instance via a Serverless VPC Access connector. Using only Google-native tools, specify exact steps to: 1) create a private Cloud SQL instance, 2) set up a Serverless VPC Access connector, 3) store a DB password in Secret Manager, 4) grant a service account minimal IAM roles (roles/cloudsql.client, roles/secretmanager.secretAccessor), 5) deploy the Cloud Run image from Artifact Registry with the VPC connector and the secret exposed as an environment variable, and 6) test connectivity from Cloud Run to Cloud SQL with a simple curl. Include concrete gcloud commands?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["OpenAI","Oracle","Plaid"]},{"id":"q-3707","question":"You're releasing v2 of a high-throughput order-processor on Cloud Run. Implement a 25% canary for 60 minutes with automatic rollback if p95 latency >0.8s or error rate >0.4% persists for 5 minutes using only Google-native tools. Include exact steps to push v2 to Artifact Registry, deploy with traffic-split, set Cloud Monitoring alerts, and rollback by routing 100% to v1. Include minimal IAM roles and Secrets Manager usage?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Google","Two Sigma","Uber"]},{"id":"q-3745","question":"Deploy a real-time fraud API on Cloud Run in two regions (us-central1 and europe-west1) behind a Global HTTP(S) Load Balancer. Release v3 with a 5% multi-region canary for 15 minutes; rollback to v2 automatically if per-region p95 latency exceeds 0.8s or error rate exceeds 0.6% for 6 minutes. Using only Google-native tools, outline exact steps to build/push v3 to Artifact Registry, run a Cloud Deploy rollout across regions, configure per-region monitoring alerts, and implement a minimal IAM/CMEK/Secrets Manager usage?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["Apple","Coinbase","Netflix"]},{"id":"q-3927","question":"You're deploying a beginner-friendly Cloud Run service named image-resizer. Using only Google-native tools, design a minimal CI/CD pipeline: build/push to Artifact Registry, deploy v1, implement a manual gate before production for v2, and configure Cloud Monitoring alerts for latency and error rate that automatically rollback by re-pointing all traffic to v1. Include exact IAM considerations and a rollback workflow?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Oracle","PayPal","Twitter"]},{"id":"q-4009","question":"Roll out a gated v4 revision of a real-time alert service on Cloud Run (fully managed) behind a Global HTTP(S) Load Balancer and Pub/Sub. Using Google-native tools, provide an exact deployment plan: traffic-split (10% canary), automatic rollback if p95 latency >0.9s or error rate >0.5% for 12m, feature flag in Secret Manager, multi-region DR, and minimal IAM bindings?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["Citadel","Uber"]},{"id":"q-4081","question":"You're orchestrating a progressive rollout for two Cloud Run services, A and B, that are part of a shared feature flag. Use Cloud Deploy to run a canary of v2 across both services with a 25% traffic allocation for 30 minutes, then 80% for 15 minutes, and full rollout if latency p95 stays under 0.8s and error rate under 0.4%. If either metric breaches for two consecutive 5-minute windows, automatically rollback to v1 by routing 100% of traffic back to v1 for both services. Outline exact steps: 1) build/push images to Artifact Registry, 2) Cloud Deploy manifest for multi-service release, 3) traffic-split and rollout timings, 4) Cloud Monitoring alerting thresholds and auto-rollback gate, 5) region handling/us-central1, 6) minimal IAM roles and Secrets/CMEK usage?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Anthropic","Cloudflare"]},{"id":"q-4121","question":"You're releasing v3 of a multi-region Cloud Run API behind a Global HTTP(S) Load Balancer. Implement a canary with stages: 2% for 15m, then 10% for 30m, then 40% for 60m, across all regions. Automatic rollback if p95 latency > 0.75s or error rate > 0.5% for 5 minutes. Use only Google-native tools. Provide exact steps: build/push to Artifact Registry, deploy with traffic-split (per-region), set Cloud Monitoring alert policies, and rollback by shifting all traffic back to v2. Include minimal IAM roles and Secrets/CMEK usage?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["Microsoft","Oracle","Twitter"]},{"id":"q-4160","question":"You're operating a Cloud Run API that consumes Pub/Sub messages and writes to BigQuery. A v2 producer schema adds an optional field and changes timestamp semantics. Using only Google-native tools, design a safe rollout that decouples producers from consumers: create a v2 Pub/Sub topic with a schema, publish 20% of messages to v2 for 30 minutes, route both subscriptions to the same Cloud Run service via a compatibility layer, monitor backlog and error rate with Cloud Monitoring, and rollback by deactivating v2 publishing and migrating all traffic back to v1. Also define minimal IAM roles?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Lyft","Tesla"]},{"id":"q-4209","question":"You're exposing a Cloud Run API behind an HTTP(S) Load Balancer. Implement a minimal Google-native Cloud Armor defense: block a known bad IP range while allowing production traffic, apply it to the backend service, and verify blocking via test requests. Include minimal IAM roles and attach steps, plus how to validate blocking?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Meta","Microsoft","OpenAI"]},{"id":"q-4335","question":"You're releasing v4 of a Cloud Run API behind a Global HTTP(S) Load Balancer with regional backends in us-east1, us-central1, and us-west1. Using only Google-native tools, design a progressive, region-aware rollout with automatic rollback. Provide exact steps to build/push v4 to Artifact Registry, configure a Cloud Deploy pipeline with regional traffic controls, set Cloud Monitoring alerts for latency and error rate, and rollback by shifting traffic back to v3?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Adobe","Microsoft","Uber"]},{"id":"q-4374","question":"You're deploying a Cloud Run service named inventory-sync with bursty traffic and cost controls. Using only Google-native tools, specify exact steps to configure autoscaling (min/max instances, concurrency), deploy a v2 revision, and set a Cloud Monitoring alert plan with a minimal dashboard for p95 latency and estimated spend. End with ?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Citadel","Hashicorp","Zoom"]},{"id":"q-4508","question":"You're releasing v6 of a latency-sensitive microservice on GKE behind a Global HTTP(S) Load Balancer with regional backends in us-central1, us-east1, and us-west1. Using only Google-native tools, design a canary rollout with regional traffic shifts: 2% in the first region for 15m, then 10% across all regions for 30m, then 25% across all regions for 60m, with automatic rollback if p95 latency > 0.8s or error rate > 0.6% for 5m. Include build/push to Artifact Registry, Cloud Deploy pipeline config, per-region traffic controls, monitoring alerts, and rollback method to prior version. Also specify minimal IAM roles and Secrets usage?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Lyft","MongoDB"]},{"id":"q-4600","question":"You're deploying a Cloud Run service named weather-api. Develop a minimal, secure deployment workflow: (1) create a dedicated service account with the least privileges needed for deployment and secret access; (2) store a config value in Secret Manager and inject it into the container at runtime; (3) configure Cloud Run to run with that service account and the secret; (4) describe a secret-rotation process that requires zero downtime, with exact gcloud commands?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Amazon","Apple","Hugging Face"]},{"id":"q-4641","question":"You're deploying a Cloud Run service named text-processor. To enforce secure deployments using Google-native tools only, design an automated vulnerability gate: push images to Artifact Registry only after Container Analysis scans; block deploys with high-severity CVEs; add a post-deploy alert if CVEs are detected later. Include exact commands and minimal IAM roles?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Adobe","Amazon","IBM"]},{"id":"q-4705","question":"You're operating a multi-region Cloud Run API backend that uses a database password stored in Secret Manager. Implement an automated, zero-downtime secret rotation every 24 hours that updates Cloud Run service env vars without downtime, revokes old secrets, and keeps in-flight connections secure. Outline exact steps to create a rotating secret, configure a Cloud Scheduler job that triggers a Cloud Function to perform patch updates, roll back if rotation fails, and verify audit logs and IAM permissions. Use only Google-native tools?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Amazon","Instacart","Salesforce"]},{"id":"q-4834","question":"Design a multi-region blue-green deployment for a Cloud Run API behind a Global HTTP(S) Load Balancer. Use Cloud Deploy to promote green (v2) from blue (v1) with region-level gated rollouts and automatic rollback if per-region health checks fail. Include exact build/push to Artifact Registry, deployment manifests per region, health-check thresholds, monitoring alerts, and rollback steps?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["Google","IBM"]},{"id":"q-877","question":"Design a cross-region disaster recovery plan for a streaming data pipeline on GCP (Pub/Sub, Dataflow, BigQuery) that must survive a regional outage with RTO < 15 minutes and RPO < 5 minutes. The primary region is us-central1; second region is us-east1. Include data paths, failover triggers, data integrity guarantees, and operational testing steps?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["Amazon","Apple","Scale Ai"]},{"id":"q-921","question":"In a multi-tenant GKE deployment across two GCP projects, you must enforce strict per-tenant network isolation and controlled egress to external services. Design a scalable architecture using Shared VPC, Private Service Connect, and per-tenant firewall policies to ensure tenants only reach whitelisted external endpoints, while preventing cross-tenant access. Include identity management, auditing, drift control, and operational notes for adding new tenants?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Amazon","Tesla"]},{"id":"gcp-ml-engineer-data-prep-1768249406549-2","question":"When tracking experiments and model lineage across teams, which combination provides end-to-end provenance and reproducibility?","channel":"gcp-ml-engineer","subChannel":"data-prep","difficulty":"intermediate","tags":["Vertex AI","Metadata","Data Catalog","Experiment Tracking","GKE","Terraform","certification-mcq","domain-weight-16"],"companies":null},{"id":"q-1008","question":"You're building a real-time customer-review sentiment classifier on GCP. Design a beginner-friendly end-to-end pipeline using Vertex AI for training and hosting, Vertex AI Feature Store for online features, Dataflow for ETL, and Pub/Sub for ingestion. Describe data flow, feature materialization cadence, a canary rollout strategy, and basic drift monitoring with rollback triggers. Include cost considerations?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Oracle","Snowflake","Two Sigma"]},{"id":"q-1199","question":"Design a multi-tenant, privacy-preserving online inference and feature materialization pipeline on GCP for a cross-region ride-hailing platform. Each tenant has its own feature schema and data residency needs. Outline how you would manage per-tenant Feature Store namespaces, Canary deployments across tenants, live vs. batch feature materialization, drift/bias monitoring, provenance, and automated rollback with Vertex AI Endpoints, Dataflow, and Pub/Sub. Include concrete rollback criteria and cost considerations?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Amazon","Lyft"]},{"id":"q-1225","question":"Design a beginner-friendly end-to-end GCP pipeline for a price-optimization model. Use Vertex AI for training and hosting, Vertex AI Feature Store for online/offline features, Dataflow for ETL into BigQuery, and Pub/Sub for ingestion. Describe data flow, feature derivation cadence, training trigger cadence, online/offline feature consistency, and a simple rollback strategy if offline metrics degrade. Include a basic cost plan?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Cloudflare","PayPal"]},{"id":"q-1438","question":"Design a production pipeline for a multi-tenant, real-time pricing model on GCP that isolates tenant data, supports per-tenant feature store versions, and enables tenant-scoped A/B testing. Use **Vertex AI**, **Feature Store**, **Pub/Sub**, and **Dataflow** to ingest events, materialize features, serve online predictions, and drive canary rollouts. Include tenancy isolation strategies, encryption at rest and in transit, drift monitoring, and cost-visibility dashboards across tenants?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Citadel","PayPal","Robinhood"]},{"id":"q-1462","question":"You're building a multi-tenant ML platform on GCP where each business unit requires isolated feature stores, per-tenant data locality, and separate budgets. Describe how you'd implement tenant isolation in Vertex AI Feature Store, manage per-tenant data lineage, and enable per-tenant canary model rollouts with drift checks and automated rollback. Include a concrete data path and cost controls?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Amazon","Databricks","Plaid"]},{"id":"q-1557","question":"Design a beginner-friendly end-to-end GCP pipeline for a real-time product-recommendation score using Vertex AI, Dataflow, Pub/Sub, and Vertex AI Feature Store. Include: 1) data validation and schema drift checks at ingestion, 2) per-customer feature isolation via IAM/VPC, 3) online feature materialization cadence and low-latency serving, 4) canary rollout strategy and rollback triggers, 5) practical cost-management tips?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Amazon","NVIDIA","Uber"]},{"id":"q-1694","question":"Design a beginner-friendly GCP ML pipeline for daily demand forecasting: Pub/Sub ingest, Dataflow ETL into BigQuery, Vertex AI training, and a Vertex AI online endpoint. Focus on observability: specify minimal metrics, dashboards, alerts for data drift and latency, and a safe rollback workflow that reverts to a previous model version when drift is detected. Include rough cost notes?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Apple","Square"]},{"id":"q-1799","question":"You run a real-time product risk scoring service on GCP with 50k QPS and 20 ms P95 latency, deployed in NA and EU. Design an end-to-end pipeline using Pub/Sub, Dataflow, Vertex AI, and BigQuery that enforces regional data residency, materializes features per region, serves online predictions with per-request explainability, and supports drift-driven rollback and cost controls. Outline architecture, data flow, and escalation criteria?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Adobe","DoorDash"]},{"id":"q-1837","question":"Design a beginner-friendly GCP ML pipeline to classify customer tickets with privacy in mind: Pub/Sub streams tickets, Dataflow applies DLP redaction and writes to BigQuery, Vertex AI trains a text classifier weekly, deploys a canary Vertex AI online endpoint, and uses drift metrics with alerts. If drift is detected, route traffic to the previous model version; include rough cost notes?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Bloomberg","Cloudflare"]},{"id":"q-1871","question":"Design an end-to-end, privacy-preserving multi-tenant ML pipeline on GCP that isolates customer data, uses Vertex AI for training and hosting, Dataflow for ETL, Pub/Sub for ingestion, and Data Catalog for lineage. Include differential privacy options, KMS-based key management, access controls, audit logging, and a rollback strategy for drift or privacy policy violations. Be concrete about components and data paths?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Anthropic","IBM","Stripe"]},{"id":"q-1940","question":"In a geo-distributed personalization pipeline on GCP, design a geo-canary rollout for a real-time ranking model across regions. Outline end-to-end usage of Vertex AI, Feature Store, Pub/Sub, and Dataflow with online/offline feature separation, drift monitoring, canary criteria, automatic rollback, and per-region cost controls?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Google","LinkedIn","Microsoft"]},{"id":"q-1966","question":"Design a region-aware, real-time update workflow for a multilingual product-support bot on GCP. Ingest user feedback via Pub/Sub; route to per-region Feature Store with Dataflow; train a multilingual NLU model in Vertex AI; deploy per-region canaries with automatic rollback; and implement drift alerts plus strict data residency controls and region-based cost caps?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Apple","Discord"]},{"id":"q-1973","question":"Design a beginner-friendly GCP ML pipeline to moderate user-uploaded product images in a marketplace. Ingest image events via Pub/Sub, Dataflow resizes and extracts safe metadata, stores references in BigQuery; Vertex AI trains a basic image classifier weekly using stored images, deploys an online endpoint with a canary rollout, and monitors drift per-tenant isolation. Include privacy safeguards and rough cost range?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Coinbase","Instacart","Twitter"]},{"id":"q-2020","question":"Design a multi-tenant, region-isolated content ranking system on GCP where each tenant enforces data residency in their region and supports per-tenant feature flags. Build with Vertex AI for model hosting, Vertex Feature Store for per-tenant features, Pub/Sub and Dataflow for streaming feature updates, and BigQuery for offline features. Describe tenant isolation, canary rollouts by tenant, drift detection thresholds, and rollback criteria with minimal impact?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Anthropic","LinkedIn","Uber"]},{"id":"q-2210","question":"Design a geo/tenant-isolated inference pipeline on GCP for a multi-tenant ranking model: each tenant has isolated Feature Store namespaces and a model registry; explain how you would structure Pub/Sub, Dataflow, Feature Store, and Vertex AI to support online/offline features, per-tenant drift monitoring, automatic rollback, and per-tenant cost controls across regions?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Bloomberg","Microsoft","Salesforce"]},{"id":"q-2226","question":"You’re designing a beginner-friendly GCP ML pipeline for a churn classifier with an emphasis on reproducibility and simple drift control, avoiding canaries. Outline data ingestion, dataset versioning, training, evaluation, and a rollback plan using Vertex AI, BigQuery, and Cloud Storage. Include a concrete example of a versioning strategy and a drift-threshold trigger?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Citadel","Netflix","Tesla"]},{"id":"q-2349","question":"You’re building a real-time recommendation model on Google Cloud for a multi-tenant SaaS product where tenants span regulated industries with data residency constraints. Design an end-to-end pipeline using Vertex AI for training and serving, BigQuery for per-tenant datasets, and Feature Store with per-tenant namespaces. Include tenancy-aware drift detection, per-tenant rollback strategy, auditing via Cloud Audit Logs, and cost controls. Describe data flow, governance, and failure modes?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["NVIDIA","Salesforce","Two Sigma"]},{"id":"q-2488","question":"You're building a privacy-preserving, multi-tenant credit-scoring service on GCP. Design a production pipeline using Vertex AI, Feature Store, Dataflow, and BigQuery that enforces per-tenant data isolation, versioned online/offline features, real-time drift detection with tenant-level rollbacks, and data residency constraints while meeting sub-200 ms latency for online predictions?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Databricks","Microsoft"]},{"id":"q-2566","question":"Design a beginner-friendly GCP ML pipeline for a ride-hailing ETA predictor with streaming data. Ingest event data via Pub/Sub, validate and enrich in Dataflow (invalid records go to a dead-letter Pub/Sub), write clean data to BigQuery, and retrain a Vertex AI tabular model daily on the latest validated batch. Include a data-quality gate for schema evolution (new column) and a simple rollback to the previous model version if validation or drift metrics fail. Be concrete about components and thresholds?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Tesla","Two Sigma","Uber"]},{"id":"q-2608","question":"You operate an IoT anomaly-detection system across multiple factories in GCP. Ingest telemetry via Pub/Sub, ETL in Dataflow to BigQuery, train a Vertex AI custom anomaly model with per-plant features, and serve via per-plant endpoints with traffic-splitting. Design versioning for data and models, drift thresholds, a per-plant canary rollout, automated rollback, and cost controls?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Cloudflare","MongoDB","Tesla"]},{"id":"q-2713","question":"You run a geo-distributed video recommendation system on GCP with strict privacy requirements. Design an end-to-end pipeline using Vertex AI, Feature Store, Dataflow, and Pub/Sub to train and serve a real-time ranking model while enforcing per-region data residency, differential privacy for user features, and secure feature materialization. Include drift detection, automatic rollback, and cost controls?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Apple","Microsoft","Netflix"]},{"id":"q-2722","question":"Design a data-governed, per-tenant ML pipeline for a real-time ad-scoring model on GCP. Requirements: enforce data residency and policy controls via Data Catalog and IAM; isolate per-tenant Feature Store namespaces and per-tenant BigQuery datasets; train with Vertex AI Pipelines; route features with Dataflow; provide drift checks, automated rollback per tenant, and audit trails in Cloud Logging. Outline end-to-end, include a sample per-tenant versioning strategy and a rollback trigger?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["LinkedIn","Meta","Robinhood"]},{"id":"q-2813","question":"You're running a privacy-conscious cross-tenant recommendation service on GCP for a SaaS product with three regional tenants. Design an end-to-end pipeline using Vertex AI, Feature Store, Pub/Sub, Dataflow, and BigQuery that enforces per-tenant data residency, separates online/offline features, and supports per-tenant canary rollouts. Include differential privacy for sensitive features, drift monitoring, automated rollback, audit logging, and data lineage. Also outline SLAs and cost controls?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Adobe","MongoDB","Plaid"]},{"id":"q-2885","question":"Design a site-aware GCP ML pipeline for a multi-site industrial anomaly detector. Ingest telemetry via Pub/Sub, process with Dataflow, store features in Vertex AI Feature Store and BigQuery for offline analysis, and host per-site regional Vertex AI online endpoints with region-aware routing. Describe drift monitoring, rollback to previous model versions, data residency, and cost controls?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["NVIDIA","Tesla"]},{"id":"q-3023","question":"Design a multi-tenant, region-aware demand-forecasting pipeline on GCP that serves dozens of retailers. Using Vertex AI for training, Vertex AI Predictions, Feature Store, Pub/Sub, and Dataflow, outline how you enforce tenant isolation (data/models), per-tenant data residency across regions, online/offline feature separation, drift monitoring, canary rollouts per tenant, and per-tenant cost accounting with budgets. Include security controls (IAM, VPC Service Controls) and a rollback plan?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Databricks","DoorDash","Robinhood"]},{"id":"q-3050","question":"In a regulated analytics platform on GCP for multi-tenant trading signals, design a production pipeline with 10 regions ensuring data residency, tenant isolation in Feature Store, and online/offline feature separation. Outline end-to-end architecture using Vertex AI, Feature Store, Pub/Sub, Dataflow, and BigQuery, including per-tenant RBAC, audit logging, automated canary rollouts with rollback, and regulatory/explainability testing?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Bloomberg","Google","Snowflake"]},{"id":"q-3083","question":"Scenario: You run a multi-tenant recommendation service on GCP with strict data residency for enterprise tenants. Each tenant has a separate offline feature store but shares a common feature namespace, and you must train per-tenant ranking models in Vertex AI. Design an end-to-end pipeline that ingests events via Pub/Sub, materializes features in Dataflow, trains and deploys models per-tenant endpoints, and serves real-time predictions. Include tenant isolation, data residency controls in Dataflow, drift detection, canary rollouts with automatic rollback, and per-tenant cost controls. Explain how you would test rollback, monitor drift, and handle schema evolution across tenants?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Hashicorp","MongoDB","Slack"]},{"id":"q-3142","question":"Design a geo-resident, encryption-first ML pipeline for a real-time content ranking model used by Instacart, Robinhood, and Discord. Enforce data residency: regional storage, Cloud KMS CMKs, region-bound training, and private endpoints. Outline online/offline feature separation (Feature Store), streaming ingestion (Pub/Sub/Dataflow), per-region endpoints, drift monitoring, auto-rollback, and regional cost controls?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Discord","Instacart","Robinhood"]},{"id":"q-3240","question":"In a beginner-friendly GCP ML pipeline to classify customer tickets, design a data-quality-gated workflow: ingest with Pub/Sub, ETL with Dataflow enforcing a Parquet/BigQuery schema, train with Vertex AI, and deploy a 1% canary; outline the gate checks, canary criteria, and rollback strategy if the gate fails or drift is detected?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Lyft","NVIDIA","Zoom"]},{"id":"q-3336","question":"Design a geo-distributed, privacy-conscious recommendation system on GCP for a large consumer platform. Outline an end-to-end pipeline using Vertex AI, Feature Store, Pub/Sub, and Dataflow that enforces regional data residency, supports online/offline features, implements per-region canary rollouts with automatic rollback, and includes drift/fairness monitoring and per-region cost controls. Provide concrete choices for data schemas, routing, and monitoring thresholds?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Adobe","Airbnb","Twitter"]},{"id":"q-3375","question":"Design a beginner-friendly GCP ML pipeline focused on reproducibility and lineage for product reviews: ingest streaming reviews via Pub/Sub; Dataflow sanitizes and tags with a dataset version in Data Catalog; Vertex AI runs a weekly training with Experiments logging hyperparameters and metrics; deploy an online endpoint with a simple drift/version gate that retrains when drift or a new dataset version is detected. Include a concrete versioning scheme?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Cloudflare","Instacart"]},{"id":"q-3421","question":"Design a beginner-friendly GCP ML pipeline for a sentiment classifier on two brands' reviews in a shared Vertex AI workspace. Ingest via Pub/Sub, clean with Dataflow, store raw in per-brand BigQuery datasets, train with Vertex AI per brand, deploy per-brand canary online endpoints, and implement simple isolation (per-brand datasets and service accounts) with a drift-trigger rollback?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Databricks","Netflix"]},{"id":"q-3612","question":"You're building a privacy-preserving, multi-tenant ML pipeline on GCP for healthcare analytics. Data resides in patient records across regions with HIPAA constraints. Design a production flow using Vertex AI, BigQuery, Cloud Storage, and Dataflow. Include: per-tenant Feature Store isolation; differential privacy in training DP-SGD or PATE; data lineage audits; online/offline features; drift and fairness monitoring; automated rollback and cost controls; and a compliant rollout plan?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Google","Oracle","Twitter"]},{"id":"q-3634","question":"Design an end-to-end GCP ML pipeline to orchestrate thousands of edge ML models across a geo-distributed device fleet using Vertex AI Edge Manager. Ingest device telemetry via Pub/Sub, preprocess with Dataflow, store offline features in BigQuery, train centralized models in Vertex AI, and deploy edge-revision canaries with region-specific rollouts. Include automatic rollback on drift/latency violations, per-tenant isolation, security with KMS, and cost controls?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Anthropic","OpenAI","Snowflake"]},{"id":"q-3664","question":"Design a multi-tenant GCP ML platform where each customer's data sits in its own BigQuery dataset and per-tenant service accounts, while training a shared Vertex AI model with per-tenant canary rollouts and drift monitoring. Describe data routing, Feature Store isolation, cost accounting, and compliance controls with clear rollback criteria?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Airbnb","Discord","Hugging Face"]},{"id":"q-3694","question":"You need a beginner-friendly GCP ML workflow for a product sentiment classifier, but with a focus on data governance and reproducibility. Describe how you would implement dataset versioning, metadata tagging in Data Catalog, Vertex AI Experiments for tracking hyperparameters and metrics, and a reproducible training-and-deployment run with a simple audit log. Include concrete steps for ingest, storage, and querying lineage?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Amazon","Netflix"]},{"id":"q-3784","question":"You're deploying a multilingual speech-to-text model used by Netflix, Zoom, and Hugging Face on GCP with strict per-tenant data isolation and residency. Design a production pipeline that ingests audio, builds per-tenant and shared features, trains via Vertex AI, stores datasets in per-tenant BigQuery datasets, and serves online/offline predictions with per-tenant canary rollouts. Include encryption, audit logs, drift/fairness monitoring, and rollback criteria?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Hugging Face","Netflix","Zoom"]},{"id":"q-3817","question":"Design a multi-tenant, GDPR/GLBA-compliant loan-scoring platform on GCP for a global bank network. Each tenant must have strict data isolation (per-tenant BigQuery datasets, Cloud Storage, and IAM), regional data residency, and per-tenant governance. Propose end-to-end ingestion (Pub/Sub), preprocessing (Dataflow), model training with Vertex AI Experiments, and canary deployments to per-tenant Endpoints. Include a centralized policy engine for data minimization, explainability, drift/fairness thresholds, audit logs via Data Catalog, and cost controls?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Airbnb","Bloomberg","Square"]},{"id":"q-3997","question":"Design a beginner friendly GCP ML pipeline to train a sentiment classifier on customer support chats with privacy and explainability in mind. Ingest chat streams via Pub/Sub, Dataflow cleans and redacts PII, writes raw data to Cloud Storage and de-identified aggregates to BigQuery. Train weekly in Vertex AI, deploy an online endpoint with Explainable AI enabled, log per-prediction attributions to BigQuery, and implement a simple drift check on attribution distributions with rollback to the previous model if drift is detected. Include rough cost notes?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Google","NVIDIA","Twitter"]},{"id":"q-4096","question":"You're designing a time-aware pricing ML model deployed across regions with varying data freshness. To prevent leakage, design a pipeline using Vertex AI, Feature Store, Dataflow, and BigQuery that enforces per-window training, TTL for features, and dataset versioning via Data Catalog. Explain how you validate training data windows, isolate regional data, and implement rollback with canary testing?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Hashicorp","Plaid","Salesforce"]},{"id":"q-4289","question":"Design a privacy-aware global content-moderation ML pipeline on GCP that enforces per-region data residency and policy constraints. Outline data ingestion from Pub/Sub to BigQuery, de-identification in Dataflow, feature storage in Vertex AI Feature Store, region-scoped model training in Vertex AI, online predictions with guardrails, drift and policy monitoring, and automatic rollback criteria; include concrete versioning and audit strategies?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["OpenAI","Scale Ai","Stripe"]},{"id":"q-4376","question":"Design an advanced, multi-tenant Vertex AI ML platform on GCP for a global SaaS. Describe end-to-end architecture that enforces strict per-tenant isolation for data, features, and models, with per-tenant quotas and budget controls, private endpoints, and VPC Service Controls. Include online/offline feature flows, drift monitoring with automatic per-tenant rollback, and data residency/audit requirements?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Anthropic","LinkedIn","Meta"]},{"id":"q-4411","question":"Design a privacy-preserving edge-to-cloud ML platform on GCP for real-time dynamic pricing across 40 regional tenants. The system must run on-device inferences via Vertex AI Edge with sub-5 ms latency, stream telemetry to the cloud for retraining via Vertex AI, Dataflow, and Feature Store. Enforce per-tenant isolation, data residency, quotas, and automated drift-triggered per-tenant rollbacks with canary deployments. Specify data schemas, feature routing, governance, and failure modes?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Citadel","MongoDB","NVIDIA"]},{"id":"q-4470","question":"Design an end-to-end Retrieval-Augmented Generation (RAG) platform on GCP for a multi-tenant SaaS that delivers enterprise-grade chat assistants. Tenants require strict data residency, per-tenant budgets, private endpoints, and model isolation. Outline architecture using Vertex AI for models and a per-tenant vector store, with Dataflow/Pub/Sub pipelines, and per-tenant data lakes. Explain isolation, prompt governance, drift monitoring with automatic per-tenant rollback, and cost controls. Include concrete data schemas and security controls?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["MongoDB","OpenAI"]},{"id":"q-4534","question":"Design a region-aware pricing ML platform on GCP Vertex AI for a global ecommerce site. Requirements: per-region data residency, strict per-tenant isolation, real-time feature serving via Feature Store, online/offline paths, drift and fairness monitoring, per-tenant budgets with alerts, and canary rollouts with automatic rollback. Describe data schemas, routing, and governance?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Databricks","LinkedIn"]},{"id":"q-4603","question":"Design a privacy-conscious cross-border fraud-detection pipeline on GCP for multiple fintech tenants. Data residency requires EU data stay in EU; others can use synthetic data. Propose architecture with Vertex AI, Dataflow, BigQuery, and Data Catalog, featuring per-tenant governance, isolated feature stores, synthetic-data generation for non-EU tenants, and automated drift-triggered canary rollbacks. Include concrete data schemas, lineage, and deployment triggers?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Goldman Sachs","LinkedIn","Meta"]},{"id":"q-4646","question":"Design a beginner-friendly GCP ML pipeline to train an image classifier on user-uploaded photos while preserving privacy: ingest via Pub/Sub, Dataflow blur faces and store masked copies in Cloud Storage, write image metadata to BigQuery, train weekly in Vertex AI on the masked data, deploy a canary endpoint, and log latency, accuracy, and drift metrics to BigQuery with alerting. Include a rough cost note?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Cloudflare","MongoDB"]},{"id":"q-4709","question":"Design a beginner-friendly GCP ML pipeline that ensures reproducible experiments for a text classifier on customer support tickets. Ingest via Pub/Sub, preprocess with Dataflow, store raw and preprocessed artifacts in Cloud Storage with per-run folders, and train weekly in Vertex AI under a new Experiment using ML Metadata. Log dataset hash, hyperparameters, and feature steps to BigQuery; trigger drift alerts and rollback to last-good model if threshold exceeded. Include rough cost notes?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["NVIDIA","Netflix"]},{"id":"q-4761","question":"Design a beginner-friendly GCP ML pipeline for a churn predictor in a multi-tenant banking app that enforces per-tenant data isolation and a single Vertex AI online endpoint. Ingest events via Pub/Sub, redact PII in Dataflow, store raw in Cloud Storage and de-identified summaries in BigQuery, train weekly in Vertex AI, serve predictions via one endpoint, and implement a drift alert with a safe rollback to the previous model version. Include concrete data lineage and cost notes?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Goldman Sachs","Robinhood"]},{"id":"q-882","question":"You run a real-time fraud-detection model on GCP at ~25k QPS with sub-20 ms P95 latency. Design a production pipeline using Vertex AI, Feature Store, Pub/Sub, and Dataflow that ingests events, materializes features, serves online predictions, and supports canary rollouts. Include data/model drift monitoring, automated rollback, and cost considerations?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Goldman Sachs","Snap"]},{"id":"q-905","question":"You operate a multi-tenant content recommender service on GCP used by advertisers. Each tenant has its own feature schema and data retention. Design a production ML pipeline (training and online serving) using Vertex AI, Feature Store, Pub/Sub, and Dataflow that supports **per-tenant namespaces**, **policy-based access**, **drift monitoring**, **automated rollback**, and **cost isolation**. Include data leakage prevention, schema evolution, and canary rollouts across regions?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Scale Ai","Tesla","Twitter"]},{"id":"q-922","question":"Design a real-time content moderation inference path on GCP that adds a Redis-based in‑memory cache in front of a Vertex AI online endpoint to reduce latency and cost. Outline what you cache (embeddings vs predictions), key schema (e.g., user_id, model_version, language), TTL and eviction policy, and how you invalidate cache on model deploy or feature updates. Include data privacy considerations and a plan for monitoring latency, cache hit rate, and drift. Provide a small Python sketch of the cache lookup?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Airbnb","Discord","Snap"]},{"id":"q-955","question":"Design a multi-tenant ML service on GCP that serves diverse customers with strict data isolation and retention policies. Propose a deployment and feature governance pattern using Vertex AI, Feature Store, Private Service Connect, Data Catalog, and Pub/Sub to isolate customer data, manage per-tenant feature lifecycles, perform drift monitoring, and enable tenant-specific canary rollouts with automated rollback and cost controls. Include concrete components, data flow, and rollback criteria?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Amazon","Meta"]},{"id":"q-1176","question":"In a multi-tenant GCP security environment, design an ephemeral admin-session workflow for Kubernetes and Cloud Run resources using IAP, Workload Identity Federation, and Binary Authorization. Include policy design, auditability, rollback, and how you'd verify there are no lingering grants after revocation?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Discord","LinkedIn","Microsoft"]},{"id":"q-1194","question":"In a GCP multi-tenant fintech data lake, design end-to-end safeguards to prevent tenant data leakage when multiple tenants share datasets in BigQuery and Cloud Storage. Propose a guardrail that blocks cross-tenant access at the data-product level using IAM Conditions, per-data-product VPC Service Controls, and Private Service Connect to a shared analytics endpoint. Include testing with synthetic misconfigurations and a rollback/audit plan?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Bloomberg","Plaid","Tesla"]},{"id":"q-1325","question":"In a multi-tenant GCP data platform used by Netflix‑like partners, outline a per-session data access model for a partner analytics job using IAM Conditions, Workload Identity Federation, and Access Context Manager to grant time-bounded, least-privilege access; include revocation, auditing, and validation with synthetic data?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Adobe","Microsoft","Netflix"]},{"id":"q-1400","question":"Design a multi-tenant GCP analytics stack (**Shared VPC**, **GKE**, Dataflow, BigQuery) with strict tenant isolation. Propose concrete network/IAM boundaries (per-tenant Namespaces, **NetworkPolicy**, **Private Service Connect**, **IAM Conditions**), a policy-as-code guard (**OPA**) in CI/CD to block cross-tenant paths, and a synthetic verification + rollback plan?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Airbnb","Apple","Slack"]},{"id":"q-1478","question":"In a GCP data-logging pipeline (Pub/Sub -> Dataflow -> BigQuery) for a fintech starter, outline a concrete beginner-friendly plan to ensure data never leaks: dedicated service accounts with least privilege, CMEK on the raw-logs bucket, IAM Conditions restricting access by time and principal, PII masking in outputs, and a CI test that injects synthetic logs to verify redaction and auditability. How would you validate in production?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Goldman Sachs","NVIDIA"]},{"id":"q-1505","question":"Design a secure, auditable cross‑organization data sharing pipeline in GCP: a data lake (Cloud Storage + BigQuery) holds PII-derived features; an external analytics partner connects via Private Service Connect to a synthetic dataset exposed for ad-hoc analysis while no raw PII leaves; specify ACM IAM Conditions per-tenant, PSC endpoints in a Shared VPC, CMEK for both stores, DLP masking prior to export, and automated revocation tests with synthetic data and rollback steps?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["NVIDIA","Scale Ai","Snowflake"]},{"id":"q-1587","question":"In a live GCP analytics stack (Pub/Sub → Dataflow → BigQuery) shared with external partners, you detect a suspected compromise of a Dataflow worker service account. Describe a concrete incident response playbook: containment (disable keys, rotate CMEKs), evidence preservation (export Cloud Audit Logs), access revocation with IAM Conditions, network containment (VPC Service Controls), and a post‑mortem with hardened controls. Include production validation steps?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Databricks","Tesla"]},{"id":"q-1719","question":"In a GCP multi-tenant data lake (BigQuery, Cloud Storage, Dataflow) used by three partners, design a crypto-agile CMEK rotation plan with zero downtime. Detail per-tenant key rings, IAM Conditions, and automatic key versioning; ensure data plane can switch keys without reprocessing. Include Access Context Manager, VPC Service Controls (PSC), DLP masking, and an automated attestations/rollback workflow with synthetic data?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Amazon","Google"]},{"id":"q-1736","question":"A GCP project hosts a Cloud Run API and a BigQuery warehouse. A contractor needs 2 weeks of read-only access to a small BI dataset, without touching production data. Draft a beginner-friendly plan: dedicated read-only service account, a dataset view for masking, IAM Bindings with an IAM Condition for a 14-day window, no public endpoints, and a test plan using a synthetic dataset and audit logs to validate revocation. How would you implement this?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Microsoft","PayPal","Robinhood"]},{"id":"q-1813","question":"In a GCP data science workspace using Vertex AI Workbench and BigQuery, draft a beginner-friendly plan to guarantee per-tenant data isolation across multiple tenants. Include: 1) per-tenant IAM roles/service accounts, 2) dataset-level access controls, 3) network borders via VPC Service Controls or Private Service Connect, 4) a CI test that injects synthetic data and validates isolation, and a safe rollback if misconfig is detected?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Hugging Face","Salesforce","Slack"]},{"id":"q-1830","question":"In a GCP-based security control plane for a fintech platform used by Robinhood and Zoom, external CI/CD pipelines must deploy and validate security baselines without long‑lived credentials. Design a Workload Identity Federation solution that maps external OIDC identities to short‑lived GCP service accounts, enforcing least privilege with IAM Conditions. Include per‑tenant isolation, auditability, token lifetimes, revocation, and automated drift/rollback tests in the CI pipeline?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Robinhood","Zoom"]},{"id":"q-1854","question":"In a GCP analytics platform with BigQuery, Dataflow, and Data Catalog used by multiple tenants, design a per-tenant row-level security model using BigQuery Row Access Policies tied to TenantID, and IAM Conditions for dataset access, complemented by Data Catalog tags and per-tenant CMEK. Include ingestion, query-time enforcement, testing with synthetic tenants, rollback, and auditability?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Google","Meta","Uber"]},{"id":"q-1986","question":"In a GCP-based data science platform shared by three tenants, design a practical beginner-friendly security baseline to prevent cross-tenant access during notebook runs, data prep, and model training. Include: 1) per-tenant Secrets Manager keys with rotation; 2) dataset/bucket access controls and per-tenant IAM/service accounts; 3) a VPC Service Controls perimeter around processing endpoints; 4) a CI check that injects synthetic data and validates isolation; 5) a rollback plan if misconfig detected?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Anthropic","Databricks","Two Sigma"]},{"id":"q-2002","question":"In a real-time GCP data lake (Pub/Sub -> Dataflow -> BigQuery -> GCS) shared across three brands, design an automated incident containment scenario: when a service account shows anomalous access patterns, how would you instantly restrict access, rotate keys, and quarantine the project while preserving pipelines? Include exact IAM bindings, PSC/VPC controls, CMEK strategies, and rollback tests?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Instacart","Tesla","Twitter"]},{"id":"q-2026","question":"In a shared GCP data lake (Pub/Sub → Dataflow → BigQuery) used by Snowflake, Netflix, and Apple, design a per-brand data isolation and crypto agility strategy. Specify per-brand CMEK key rings, IAM Conditions to enforce least privilege, per-brand VPC Service Controls perimeters, and an automated key rotation and rollback plan with end-to-end tests for authorization, auditing, and data access paths?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Apple","Netflix","Snowflake"]},{"id":"q-2055","question":"In a GCP multi-tenant streaming lake (Pub/Sub -> Dataflow -> BigQuery) shared by three brands, a service account is suspected of exfiltrating data. Design an automated, tenant-aware containment plan that isolates the compromised tenant without interrupting others: 1) tenant-scoped IAM bindings with conditional denies, 2) per-tenant VPC Service Controls and Private Service Connect paths, 3) CMEK-driven key rotation with automatic rotation triggers, 4) pipeline cutover and rollback procedures, 5) synthetic-data canaries and automated validation before full failover?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Apple","Instacart","Zoom"]},{"id":"q-2104","question":"Design a cryptographically verifiable data provenance system for a real-time GCP data lake (Pub/Sub -> Dataflow -> BigQuery -> GCS) shared by three brands. How would you generate per-record provenance tokens, sign them with Cloud KMS, attach them to the data, rotate keys, and verify integrity at read time while preserving pipeline throughput? Include how you'd store proofs, enforce tenant isolation, and rollback tests?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Anthropic","MongoDB","Plaid"]},{"id":"q-2182","question":"In a global GCP security setup hosting a real-time data lake (Pub/Sub -> Dataflow -> BigQuery -> GCS) shared by three brands, a service account in one project shows anomalous access to Secrets Manager and Cloud SQL. Design an automated containment workflow that immediately revokes keys, rotates CMEK-protected secrets, applies IAM Conditions or Deny policies to block access, tightens PSC/VPC Service Controls, and quarantines the affected project while preserving pipelines. Include exact bindings, key rotation steps, and rollback tests?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Apple","Meta"]},{"id":"q-2285","question":"In onboarding a new tenant to a shared GCP data lake (Pub/Sub → Dataflow → BigQuery/Cloud Storage) implement a Just-In-Time (JIT) access flow that temporarily elevates a service account for a 2-hour data load, then revokes it automatically. Include per-tenant IAM bindings with conditions, ACM access policies, PSC endpoints, CMEK management, and an auditable rollback plan?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Lyft","Netflix","Two Sigma"]},{"id":"q-2467","question":"In a GCP-native real-time feature store shared by three brands, design a per-brand access model for streaming features from Pub/Sub through Dataflow to BigQuery. Define exact IAM bindings and conditions, per-brand VPC Service Controls boundaries, Private Service Connect endpoints for feature API calls, and CMEK strategies; include rotation cadence and a rollback test plan that preserves in-flight data?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Hugging Face","Snowflake","Two Sigma"]},{"id":"q-2628","question":"In a multi-tenant data exchange on GCP, a third-party analytics partner exports data from a shared BigQuery dataset to their project via Data Transfer Service. The dataset contains PII. Design a practical, real-time containment workflow to detect anomalous exports and automatically enforce containment: temporary IAM Conditions on export, rotate partner service account keys, reconfigure PSC/Private Service Connect to the partner, enable VPC Service Controls perimeters, apply CMEK or DLP masking, and establish a rollback/audit procedure with synthetic data checks?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Instacart","Netflix","Zoom"]},{"id":"q-2681","question":"Design a secure onboarding workflow for a shared Vertex AI inference platform used by three financial tenants; how would you implement per-tenant isolation and least privilege, federate client identities via Workload Identity Federation, enforce network egress controls with Private Service Connect and VPC Service Controls, apply CMEK at storage and model artifacts, and verify revocation with automated rollback tests?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Goldman Sachs","Google"]},{"id":"q-2760","question":"In a new GCP project hosting a multi-tenant web app, draft a concrete, beginner-friendly security baseline for secrets and data access: 1) define per-tenant IAM bindings and a central app service account for Secret Manager access, 2) implement per-tenant Cloud Storage prefixes with uniform bucket-level access and least privilege, 3) enable Cloud Audit Logs and alerting for IAM changes, 4) create a CI test that attempts access with an invalid credential and fails, 5) outline rollback steps if a misconfiguration is detected?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Bloomberg","Discord","Plaid"]},{"id":"q-2774","question":"In a real-time GCP data pipeline (Pub/Sub → Dataflow → BigQuery) that uses third‑party container images from Artifact Registry for Dataflow workers, design a secure supply‑chain strategy to prevent tampering while preserving uptime. Include Binary Authorization gates, image signing and provenance constraints, per-project IAM bindings, digest pinning, monitoring, rollback/test plans, and synthetic tamper tests?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Microsoft","Robinhood","Salesforce"]},{"id":"q-2842","question":"In a GCP project shared by three brands, a Cloud Run service 'auth-service' accesses a Cloud SQL Postgres instance and writes audit logs to Cloud Storage. Outline a beginner-friendly plan to enforce isolation: per-brand service accounts with least privilege, Private IP access to Cloud SQL, restricted outbound traffic, and a CI gate that tests cross-brand access and fails on leakage; include rollback steps?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Meta","OpenAI","Stripe"]},{"id":"q-2986","question":"In a Vertex AI deployment with multiple environments (dev/staging/prod) and a shared data platform, ensure artifact provenance and integrity from training to production using Binary Authorization, CMEK, Artifact Registry, and CI/CD. Outline a concrete workflow: signing artifacts with CMEK keys, requiring attestations for deployment, gating deployments with policy checks, and testing rollback by revoking attestations and re-deploying the last-good artifact. How would you implement auditing and rollback tests?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Anthropic","Cloudflare","Plaid"]},{"id":"q-3015","question":"In a GCP multi-tenant data lake (Pub/Sub → Dataflow → BigQuery), design a beginner-friendly isolation strategy using per-tenant datasets and authorized views. Include naming conventions and IAM bindings, how to implement authorized views or row-level access, a CI pipeline test that provisions a synthetic tenant, ingests data, and validates only that tenant can read their data, and a rollback plan for misconfig?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Citadel","Cloudflare","Coinbase"]},{"id":"q-3039","question":"For a Cloud Run API consumed by three partner firms, design a beginner-friendly plan to securely expose it while preventing data leakage: use least-privilege IAM for the service account, enable IAP for user auth, enforce per-partner isolation with separate service accounts or a simple gateway policy, rotate credentials, and add a synthetic test that verifies blocked access when a misconfiguration occurs?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Citadel","Cloudflare","Uber"]},{"id":"q-3128","question":"In a GCP-based ML inference service using Vertex AI Endpoints backed by a shared Cloud Run API, two tenants (TenantA and TenantB) access different model variants from the same endpoint. Describe a beginner-friendly plan to ensure strict per-tenant isolation and prevent data leakage, covering: 1) tenant-scoped IAM bindings and per-tenant service accounts with IAM Conditions, 2) model/version tagging and resource labeling plus dataset isolation strategies, 3) network controls using Private Service Connect or VPC Service Controls to restrict access to internal artifacts, 4) a lightweight CI test that injects synthetic tenant data and verifies isolation, and 5) rollback/detection steps if misconfig occurs?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["IBM","Meta"]},{"id":"q-3178","question":"In a multi-tenant SaaS API hosted on Cloud Run behind API Gateway, using a single Cloud SQL instance for data, draft a beginner-friendly security plan that prevents cross-tenant access. Include: (1) per-tenant IAM boundaries or service accounts, (2) a data isolation scheme (tenant_id enforcement or separate schemas), (3) transport and storage encryption (TLS, CMEK/Secret Manager), (4) basic monitoring/alerts for anomalous access, and (5) a safe rollback/validation plan using synthetic tenants?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Citadel","Plaid","Stripe"]},{"id":"q-3207","question":"How would you implement per-tenant VPC Service Controls perimeters around a shared data lake and sinks, plus a deny rule for export to non-perimeter endpoints, IAM Conditions on export actions, DLP labeling, and automated rollback with auditable logs to preserve ongoing pipelines?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["PayPal","Tesla","Twitter"]},{"id":"q-3220","question":"In a GCP multi-tenant deployment (Instacart, Airbnb, LinkedIn) where Terraform baselines are deployed via CI/CD, design a Binary Authorization-driven attestation plan to gate production deployments. Include attestor setup, signing workflow, per-environment policies, key rotation, rollback testing, and drift audits?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Airbnb","Instacart","LinkedIn"]},{"id":"q-3348","question":"Trigger on anomalous egress from a service account. Apply a project-level IAM Deny policy to block outbound traffic except to approved sinks, rotate keys and rebind to a fresh ephemeral SA via federation, and enable per-brand VPC Service Controls isolation. Rotate CMEK on affected buckets and quarantine the project from external access while keeping pipelines. Provide a rollback path to revert Deny and key changes?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["DoorDash","LinkedIn"]},{"id":"q-3429","question":"In a GCP setup with a single API backend on Cloud Run, behind Cloud Endpoints API Gateway and IAP, multiple tenants share the same backend and database. Draft a beginner-friendly plan to enforce per-tenant data isolation at runtime using JWT tenant_id claims and backend middleware, ensure minimal token lifetimes, use IAM to constrain service accounts, simulate tenanted traffic in CI, and roll back safely if misconfig is detected?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Cloudflare","Instacart"]},{"id":"q-3471","question":"In a GCP security scenario, a multi-tenant data lake (BigQuery, Cloud Storage, Dataflow) shared by three brands experiences a compromised service account used by the ingest pipeline. Design an automated containment plan that preserves pipelines while isolating exposure. Include exact IAM bindings and conditions, PSC/VPC controls, CMEK rotation strategy, Access Context Manager usage, and a rollback/validation workflow with synthetic data?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Adobe","Databricks"]},{"id":"q-3717","question":"Scenario: A multi-tenant web app runs on Cloud Run in a single GCP project for three brands. Docker images are supplied by external vendors. How would you enforce that only signed images can be deployed to Cloud Run using Binary Authorization, and ensure per-brand isolation so a signer can only deploy images for their tenant? Include policy details, image provenance, per-tenant constraints, testing with synthetic data, and rollback steps if a compromise is detected?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Cloudflare","Discord","Tesla"]},{"id":"q-3734","question":"How would you implement a beginner-friendly plan to scrub PII from logs ingested across three brands before long-term storage in GCP? Describe a pipeline: source Pub/Sub per-brand, Dataflow with DLP-based redaction, per-brand Cloud Storage (CMEK), and IAM Conditions restricting access; include a VPC Service Controls boundary and a CI test that injects synthetic PII and validates redaction; outline rollback steps if redaction fails?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Google","Meta","Robinhood"]},{"id":"q-3811","question":"In a GCP multi-tenant analytics stack (BigQuery, Cloud Storage, Dataflow) sharing a centralized data lake, a misclassified PII dataset risks cross-tenant exposure. Design an automated containment flow that detects drift via DLP/IAM signals, isolates the dataset to the tenant project with per-tenant IAM Conditions and Access Context Manager, enforces CMEK rotation, tightens VPC Service Controls, and uses a rollback/validation with synthetic data to resume pipelines safely. Include concrete IAM bindings and rollback steps?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Airbnb","Anthropic","Instacart"]},{"id":"q-3886","question":"Scenario: A single GCP project hosts a multi-tenant SaaS app where tenant activity generates Cloud Logging entries. Draft a beginner-friendly plan to guarantee per-tenant log isolation and prevent cross-tenant access. Include: 1) tenant-scoped IAM Conditions for Logs, 2) per-tenant log sinks to dedicated Cloud Storage buckets, 3) labeling/filters to prevent leakage, 4) a CI check that attempts cross-tenant access and a rollback path if misconfig found?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Google","Meta","OpenAI"]},{"id":"q-3986","question":"In a GCP security scenario, a Vertex AI model served from a shared project is used by three brands. The model artifacts live in Cloud Storage and telemetry data in BigQuery. Design a tenant-isolated containment plan that prevents data leakage if the model endpoint is compromised, while preserving service availability. Specify exact IAM bindings and Conditions, ACM perimeters, VPC-SC usage, CMEK rotation strategy, per-tenant data tagging, automated rollback/validation with synthetic data, and a rollback workflow?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Hugging Face","Netflix","Snowflake"]},{"id":"q-4093","question":"In a Vertex AI Model Registry shared by three brands, a deployment pipeline account is compromised and attempts to promote a poisoned model to production. Design an automated containment and recovery workflow that preserves legitimate pipelines while quarantining the attacker, using IAM Conditions on the model registry, VPC Service Controls boundaries, CMEK rotation for model artifacts, Access Context Manager constraints, and a synthetic-data rollback plan?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Databricks","Google","Twitter"]},{"id":"q-4137","question":"Context: A GCP data lake shared by Apple, Amazon, and Databricks faces a contractor ingestion job that inadvertently created cross-tenant access due to a misconfigured dataset ACL. Design an automated containment plan that quarantines the compromised tenant's data while keeping streaming intact. Specify per-tenant IAM Conditions, Access Context Manager rules, VPC Service Controls boundaries, CMEK rotation plan for affected data, and a rollback/validation workflow with synthetic data?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Amazon","Apple","Databricks"]},{"id":"q-4166","question":"Scenario: A GCP-hosted SaaS API gateway serves tenants across regions via a single Cloud Run backend. Draft a beginner-friendly plan to enforce tenant isolation at the API layer: 1) per-tenant API keys stored in Secret Manager, 2) request-scoped IAM service accounts with restricted roles, 3) gateway quotas and per-tenant rate limits, 4) tenant-aware logs and tracing, 5) automated key rotation and revocation, and 6) a CI test that simulates cross-tenant access and a rollback path?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Apple","Microsoft"]},{"id":"q-4244","question":"In a GCP-based data lake shared by Coinbase, Apple, and Uber, a vendor's external identity is found accessing multiple datasets via Pub/Sub and Dataflow. Design an automated containment plan that preserves pipelines while isolating exposure. Include exact IAM Conditions and bindings, per-tenant Access Context Manager levels, VPC Service Controls adjustments, CMEK rotation, Workload Identity Federation mappings, and a rollback/validation workflow with synthetic data?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Apple","Coinbase","Uber"]},{"id":"q-4277","question":"In a multi-tenant Vertex AI deployment for a beginner-friendly analytics workspace, design a practical plan to isolate tenants at the inference layer. Include: 1) tenant-scoped IAM conditions for endpoint access, 2) Private Service Connect exposure, 3) per-tenant model/artifact isolation, 4) a CI test validating isolation and a rollback workflow?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Bloomberg","Meta","PayPal"]},{"id":"q-4407","question":"In a GCP telemetry platform spanning two brands in a shared VPC, a Cloud Run ingestion service is suspected of leaking credentials to an external endpoint. Design an automated containment plan that preserves ingest pipelines while isolating exposure. Include exact IAM bindings and conditions, per-service-account scoping, VPC Service Controls, Private Service Connect usage, CMEK rotation strategy, Access Context Manager policy, and a rollback/validation workflow with synthetic data?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Citadel","Robinhood"]},{"id":"q-4544","question":"In a GCP multi-tenant analytics stack (Pub/Sub -> Dataflow -> BigQuery -> Cloud Storage) shared by brands A, B, and C, a misconfigured IAM binding allows a tenant's service account to access another tenant's PII via an external BigQuery connection. Design a fail-fast containment plan that preserves streaming and batch jobs while isolating the affected tenant and preventing further access. Include exact IAM bindings and conditions, ACM/VPC controls, PSC endpoints, CMEK rotation strategy, DLP masking, and a rollback/validation workflow with synthetic data?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Discord","Twitter"]},{"id":"q-4691","question":"In a GCP ML data lake used by Hugging Face, Tesla, and MongoDB, design a tenant-aware security model for Vertex AI Feature Store and linked GCS/BigQuery assets that allows offline training with shared features but blocks cross-tenant data exfil. detail IAM Conditions, per-tenant Access Context Manager perimeters, VPC Service Controls, CMEK strategy, and an automated synthetic-data validation with rollback?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Hugging Face","MongoDB","Tesla"]},{"id":"q-4719","question":"In a GCP security scenario, a three-brand data science platform (Vertex AI Workbench, Pipelines, BigQuery, GCS) has a compromised notebook service account exfiltrating data to a shared bucket. Design an automated containment plan that preserves workloads while isolating exposure. Include exact IAM Deny policies with conditions, Access Context Manager, VPC Service Controls, CMEK rotation, and rollback/validation workflow with synthetic data?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Cloudflare","Google","Uber"]},{"id":"q-4782","question":"Scenario: A GCP multi-tenant data science workspace serving Nvidia and Databricks uses Vertex AI Workbench, shared BigQuery datasets, and Cloud Storage. A tenant accidentally binds a shared service account across tenants, risking data exfiltration. Draft a beginner plan to prevent cross-tenant access. Include: 1) tenant-scoped IAM bindings and service accounts with least privilege, 2) dataset/bucket ACLs with per-tenant isolation, 3) per-tenant PSC/Boundary enforcement, 4) a CI test simulating cross-tenant access and a rollback strategy?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Databricks","NVIDIA"]},{"id":"q-863","question":"In a GCP data pipeline that streams PII from Pub/Sub through Dataflow to BigQuery and Cloud Storage, outline a concrete hardening plan: exact IAM bindings with least privilege, VPC Service Controls, private access to API endpoints, encryption key management, access reviews, and monitoring/alerting. Include how you’d validate controls in production?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Adobe","Amazon","Meta"]},{"id":"q-899","question":"In a GCP data pipeline streaming PII from Pub/Sub through Dataflow to BigQuery and Cloud Storage, enable secure cross‑org sharing with an external analytics partner via Private Service Connect. Draft a concrete architecture: least‑privilege IAM bindings per data product, cross‑project scopes, PSC endpoints in a shared VPC, CMEK, DLP masking, and automated validation with synthetic data and revocation tests. End with auditable controls and rollback plan?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Slack","Zoom"]},{"id":"q-911","question":"In a GCP data pipeline that streams PII from Pub/Sub through Dataflow to BigQuery and Cloud Storage across two orgs, enable time-bounded access for an external analytics partner without static credentials. Draft a concrete design using Workload Identity Federation, IAM conditions, ephemeral credentials, Private Service Connect, and VPC Service Controls. Include trust, token lifetimes, auditing, automated revocation tests, and rollback?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Cloudflare","LinkedIn","PayPal"]},{"id":"q-945","question":"In a **Terraform-driven GCP** multi-tenant environment used by **Salesforce** and **Discord**, implement automated guardrails that block any public bucket or dataset and enforce least-privilege IAM at module boundaries. Describe how you’d implement **OPA constraints**, integrate with **CI/CD**, test with synthetic misconfigs, and provide rollback/auditability?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Discord","Salesforce"]},{"id":"q-1970","question":"Design a LangChain-based agent that uses Autogen to plan and execute a multi-step inquiry: check stock via /inventory API, fetch ETA via /shipping API, and present a precise delivery window with caveats; implement robust error handling with retries, fallbacks, and timeouts, and show how the agent would adjust its plan if the stock check returns uncertain results?","channel":"generative-ai","subChannel":"agents","difficulty":"intermediate","tags":["langchain","autogen","tool-use","planning"],"companies":["Instacart","Square"]},{"id":"q-2596","question":"Design a beginner-friendly LangChain task: given 'Summarize the top 3 Nvidia and Apple AI news from the last 24h', build a small Autogen-driven plan that uses tools (news API, summarizer, fact-checker). Show the planning steps, how tools are chained, and how you handle tool errors and duplicates. Include a minimal code sketch to register tools and run the plan?","channel":"generative-ai","subChannel":"agents","difficulty":"beginner","tags":["langchain","autogen","tool-use","planning"],"companies":["Apple","NVIDIA","Twitter"]},{"id":"q-3286","question":"In a LangChain-driven agent that uses Autogen for planning, you need to implement an end-to-end ETL from a flaky external API to a Snowflake warehouse. Outline how you would select tools, orchestrate plan→execute, implement retry/backoff, ensure idempotency, and persist planning state across restarts. Include how you'd simulate API failures and verify end-to-end correctness?","channel":"generative-ai","subChannel":"agents","difficulty":"advanced","tags":["langchain","autogen","tool-use","planning"],"companies":["Meta","Snowflake"]},{"id":"q-3381","question":"How would you design a minimal LangChain planning workflow for the user request: 'Show me the latest mortgage rate changes and a one-page summary' that splits into steps: (1) fetch latest rates via a MortgageRates API, (2) compare with prior day, (3) format a formatted summary, and (4) assemble a report? Include tool wiring, error handling, and basic tests?","channel":"generative-ai","subChannel":"agents","difficulty":"beginner","tags":["langchain","autogen","tool-use","planning"],"companies":["LinkedIn","Plaid"]},{"id":"q-3814","question":"Design and deploy a LangChain-based autonomous agent that consumes live market data, uses Autogen to generate stepwise plans, and orchestrates multiple tools (data fetch, risk model, analytics, alerting); describe the planning loop, state management, tool orchestration, and failure handling, including data freshness, latency, and rollback strategies?","channel":"generative-ai","subChannel":"agents","difficulty":"advanced","tags":["langchain","autogen","tool-use","planning"],"companies":["Goldman Sachs","IBM"]},{"id":"q-4004","question":"In a production incident response system, design an autonomous agent using LangChain to orchestrate tools (metrics API, runbooks DB, ticketing API, chat). Use Autogen-like planning to produce a runbook sequence and adapt on failure. Explain how you structure prompts, plan decomposition, tool selection, and idempotence checks. Include a minimal pseudocode snippet for the planner loop and a concrete example with at least 3 steps?","channel":"generative-ai","subChannel":"agents","difficulty":"intermediate","tags":["langchain","autogen","tool-use","planning"],"companies":["Cloudflare","IBM","Zoom"]},{"id":"q-430","question":"How would you implement a basic AI agent using LangChain that can use tools to answer user questions about weather data?","channel":"generative-ai","subChannel":"agents","difficulty":"beginner","tags":["langchain","autogen","tool-use","planning"],"companies":["Amazon","Anthropic","Apple","Google","Microsoft","OpenAI","Snowflake"]},{"id":"q-4425","question":"Design a LangChain agent using Autogen planning to answer a user query by querying a flaky pricing API and a cache. Explain when to plan vs execute, how to pick tools (API, cache, fallback), how to implement retries and circuit breaking, and how to structure prompts and contracts. Provide a concrete step-by-step scenario with exact calls and data flow?","channel":"generative-ai","subChannel":"agents","difficulty":"intermediate","tags":["langchain","autogen","tool-use","planning"],"companies":["Hugging Face","Instacart","Snowflake"]},{"id":"q-445","question":"You're building a multi-agent system using LangChain and AutoGen for autonomous code generation. How would you design a robust tool-use framework that prevents malicious code execution while maintaining agent autonomy?","channel":"generative-ai","subChannel":"agents","difficulty":"advanced","tags":["langchain","autogen","tool-use","planning"],"companies":["Databricks","Google","Tesla"]},{"id":"q-4654","question":"Design an autonomous trading assistant using LangChain and AutoGen that ingests live price data via a REST tool, computes a 50/200 SMA, and places orders through a trading API. Explain how you structure the planning graph, tool usage, error handling, rate limits, and risk checks, and discuss eager vs lazy planning trade-offs?","channel":"generative-ai","subChannel":"agents","difficulty":"advanced","tags":["langchain","autogen","tool-use","planning"],"companies":["Google","Robinhood"]},{"id":"q-322","question":"How would you measure and reduce hallucination in a large language model deployed for customer service?","channel":"generative-ai","subChannel":"evaluation","difficulty":"beginner","tags":["hallucination","faithfulness","relevance"],"companies":["Amazon","Google","IBM","Mckinsey","Meta","Microsoft","Salesforce"]},{"id":"q-3557","question":"You're deploying a multilingual support chatbot across brands like Uber, Snap, and Adobe. The model sometimes hallucinates policy details or cites obsolete rules. Design a practical evaluation approach that (1) detects hallucinations across languages, (2) verifies faithfulness to the latest live policy docs, and (3) tracks policy drift with versioned citations. Include datasets, metrics, tooling, and a sample evaluation script?","channel":"generative-ai","subChannel":"evaluation","difficulty":"advanced","tags":["hallucination","faithfulness","relevance"],"companies":["Adobe","Snap","Uber"]},{"id":"q-385","question":"You're building a hallucination detection system for a production LLM service. Design a multi-layered evaluation pipeline that balances false positives/negatives while maintaining sub-100ms latency. How would you implement confidence scoring and fallback mechanisms?","channel":"generative-ai","subChannel":"evaluation","difficulty":"advanced","tags":["hallucination","faithfulness","relevance"],"companies":["Anthropic","Google","Stripe"]},{"id":"q-402","question":"How would you design a hallucination detection system for a medical AI assistant that evaluates faithfulness against verified drug databases while maintaining 99.9% accuracy?","channel":"generative-ai","subChannel":"evaluation","difficulty":"advanced","tags":["hallucination","faithfulness","relevance"],"companies":["Microsoft","Netflix","Veeva"]},{"id":"q-463","question":"How would you evaluate if an LLM's response is faithful to the provided source documents?","channel":"generative-ai","subChannel":"evaluation","difficulty":"beginner","tags":["hallucination","faithfulness","relevance"],"companies":["Cloudflare","Microsoft","PayPal"]},{"id":"q-494","question":"How would you design a comprehensive evaluation framework to detect hallucinations in a large language model deployed for customer support, considering both factual accuracy and faithfulness to provided context?","channel":"generative-ai","subChannel":"evaluation","difficulty":"advanced","tags":["hallucination","faithfulness","relevance"],"companies":["Discord","Meta","Plaid"]},{"id":"q-524","question":"How would you evaluate a generative AI model's tendency to hallucinate when answering factual questions about company policies?","channel":"generative-ai","subChannel":"evaluation","difficulty":"beginner","tags":["hallucination","faithfulness","relevance"],"companies":["Microsoft","Uber"]},{"id":"q-1673","question":"Given a 7B-class LLM, design a practical PEFT plan to add domain-specific knowledge for a banking app using LoRA/QLoRA/adapter techniques. Specify target modules, r, alpha, dropout, quantization approach (e.g., 4-bit), dataset size, batch size, learning rate, and epochs. Explain how you'd validate improvements without harming generalization and how to deploy adapters for on-device inference?","channel":"generative-ai","subChannel":"fine-tuning","difficulty":"intermediate","tags":["lora","qlora","peft","adapter"],"companies":["Apple","Plaid"]},{"id":"q-1780","question":"How would you fine-tune a 7B base model for a live chat assistant using a LoRA adapter (via QLoRA/PEFT) on a 2k-example dataset? Include modules to target, rank, and precision, data prep, training setup, and evaluation strategy. Provide a minimal code snippet to attach a LoRA adapter to a transformer layer?","channel":"generative-ai","subChannel":"fine-tuning","difficulty":"beginner","tags":["lora","qlora","peft","adapter"],"companies":["Airbnb","MongoDB","Zoom"]},{"id":"q-2224","question":"You're fine-tuning a 7B parameter LLM on 1k examples. Configure a LoRA adapter with PEFT/QLora: r=8, lora_alpha=32, lora_dropout=0.1, target_modules=['q_proj','k_proj','v_proj']; enable 4-bit quantization with bitsandbytes. Specify how you would integrate and verify that only adapter weights are trained?","channel":"generative-ai","subChannel":"fine-tuning","difficulty":"beginner","tags":["lora","qlora","peft","adapter"],"companies":["Apple","Two Sigma","Uber"]},{"id":"q-225","question":"When implementing LoRA fine-tuning for a 7B parameter LLM, how do you determine the optimal rank (r) and alpha values to balance performance and memory efficiency while maintaining model quality?","channel":"generative-ai","subChannel":"fine-tuning","difficulty":"intermediate","tags":["lora","qlora","peft","adapter"],"companies":["Amazon","Databricks","Google","Meta","Microsoft","NVIDIA"]},{"id":"q-250","question":"What is LoRA and how does it reduce parameters when fine-tuning large language models?","channel":"generative-ai","subChannel":"fine-tuning","difficulty":"beginner","tags":["lora","qlora","peft","adapter"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"q-3238","question":"You're fine-tuning a 7B LLM for a customer-support chatbot using PEFT adapters. With a single 16GB GPU and ~100k tokens of data, compare LoRA vs QLoRA: which approach would you start with and why? Outline a concrete plan including: adapter rank, alpha, dropout, data split, and a minimal Python snippet showing how to apply an adapter with peft. Include expected validation checks?","channel":"generative-ai","subChannel":"fine-tuning","difficulty":"beginner","tags":["lora","qlora","peft","adapter"],"companies":["Amazon","Google","Slack"]},{"id":"q-3682","question":"Given a 1.2B parameter transformer and 5M domain tokens to fine-tune on a 16GB GPU, design a concrete plan using **LoRA**, **QLoRA**, or adapters: choose method, set rank, alpha, bottleneck, and merging strategy; address quantization, training steps, memory layout, and evaluation metrics; include potential failure modes and debugging steps?","channel":"generative-ai","subChannel":"fine-tuning","difficulty":"advanced","tags":["lora","qlora","peft","adapter"],"companies":["Goldman Sachs","Google","OpenAI"]},{"id":"q-197","question":"How would you implement efficient KV caching in a transformer decoder to reduce redundant computation during autoregressive generation?","channel":"generative-ai","subChannel":"llm-fundamentals","difficulty":"intermediate","tags":["transformer","attention","tokenization"],"companies":["Amazon","Google","Meta","Microsoft","OpenAI"]},{"id":"q-2791","question":"Design a concrete plan to run transformer attention on 1024 tokens under memory and latency constraints in a Hugging Face-style inference service. Choose a practical strategy (local/block-sparse attention, sliding windows, or recomputation) and justify your choice. Specify how you would reshape Q/K/V, attention masks, and batching to maintain accuracy, latency, and memory. Provide a minimal patch sketch in PyTorch/JS-like pseudocode showing blockwise attention, and outline validation steps?","channel":"generative-ai","subChannel":"llm-fundamentals","difficulty":"advanced","tags":["transformer","attention","tokenization"],"companies":["Hugging Face","MongoDB","Salesforce"]},{"id":"q-2934","question":"Explain with a concrete, implementable scenario: for a 4-token sequence with padding, describe the exact tensor shapes and steps to compute one attention head's output in a transformer, including masking for padding and causality, and provide a minimal description of the PyTorch sequence to perform Q,K,V -> scores -> weights -> context?","channel":"generative-ai","subChannel":"llm-fundamentals","difficulty":"beginner","tags":["transformer","attention","tokenization"],"companies":["Cloudflare","Netflix","OpenAI"]},{"id":"q-308","question":"How does the self-attention mechanism in transformers compute token relationships?","channel":"generative-ai","subChannel":"llm-fundamentals","difficulty":"intermediate","tags":["transformer","attention","tokenization"],"companies":["Amazon","Google","Meta"]},{"id":"q-371","question":"You're designing a custom tokenizer for a multilingual LLM that needs to handle code-switching between English and Chinese. How would you optimize the vocabulary to minimize token count while preserving semantic meaning, and what attention mechanism modifications would you consider?","channel":"generative-ai","subChannel":"llm-fundamentals","difficulty":"advanced","tags":["transformer","attention","tokenization"],"companies":["Meta","Microsoft","NVIDIA"]},{"id":"q-3868","question":"You're building a Transformer-based search autocomplete for a Robinhood-like trading app, needing top-5 completions within 20ms latency at peak 1M queries per day. Describe a concrete tokenization and attention plan: (a) tokenization for numbers and symbols, (b) attention pattern (local/sparse + caching), (c) how to handle long prefixes with relative positions or memory, (d) deployment tactics to reach latency and scale?","channel":"generative-ai","subChannel":"llm-fundamentals","difficulty":"advanced","tags":["transformer","attention","tokenization"],"companies":["MongoDB","Robinhood"]},{"id":"q-3960","question":"In a transformer-based real-time translation system deployed at scale, explain how you would implement and compare full softmax attention to block-sparse attention for long sequences, using a streaming tokenizer and relative position encodings (rotary or Alibi), including stability and latency trade-offs?","channel":"generative-ai","subChannel":"llm-fundamentals","difficulty":"advanced","tags":["transformer","attention","tokenization"],"companies":["Amazon","Tesla"]},{"id":"q-414","question":"Explain how the self-attention mechanism in a transformer works and why it's more effective than RNNs for processing long sequences?","channel":"generative-ai","subChannel":"llm-fundamentals","difficulty":"beginner","tags":["transformer","attention","tokenization"],"companies":["Amazon","Anduril","Google","Meta","Microsoft","NVIDIA","OpenAI","Tesla"]},{"id":"q-4811","question":"You're building a HashiCorp doc QA assistant that must answer from a 1M+ token corpus. The model can attend at 4k tokens per block; propose a concrete hybrid attention design (local sliding window plus global tokens) and a tokenization choice (SentencePiece vs BPE) that preserves cross-block context. How would you implement causal masking and relative position embeddings to keep generation correct?","channel":"generative-ai","subChannel":"llm-fundamentals","difficulty":"intermediate","tags":["transformer","attention","tokenization"],"companies":["Anthropic","Hashicorp"]},{"id":"q-577","question":"How would you debug a transformer model where attention weights are becoming uniform across all tokens, leading to poor performance?","channel":"generative-ai","subChannel":"llm-fundamentals","difficulty":"intermediate","tags":["transformer","attention","tokenization"],"companies":["IBM","PayPal"]},{"id":"q-2246","question":"Design a retrieval-augmented QA service for internal engineering docs spanning product manuals, API specs, and code samples. Outline the end-to-end pipeline: chunking strategy, embedding models, vector DB indexing, cross-collection retrieval priors, re-ranking, versioning, provenance, latency targets, and how you'd test and monitor to minimize hallucinations?","channel":"generative-ai","subChannel":"rag","difficulty":"intermediate","tags":["retrieval","embeddings","vector-db","chunking"],"companies":["Meta","Tesla"]},{"id":"q-293","question":"How do you optimize chunking strategies for different document types in RAG systems?","channel":"generative-ai","subChannel":"rag","difficulty":"advanced","tags":["retrieval","embeddings","vector-db","chunking"],"companies":["Amazon","Google","Meta"]},{"id":"q-2963","question":"Design a cross-lingual retrieval-augmented generation workflow for a multi-tenant docs store containing English and Spanish API specs, incident reports, and partner notes. Explain chunking per document type, multilingual embeddings, vector DB indexing, versioning, and time-aware retrieval policies. Include a minimal code sketch showing bilingual embedding initialization and a cross-language retrieve call?","channel":"generative-ai","subChannel":"rag","difficulty":"advanced","tags":["retrieval","embeddings","vector-db","chunking"],"companies":["Salesforce","Stripe","Twitter"]},{"id":"q-3265","question":"Design an end-to-end retrieval-augmented generation pipeline for an internal engineering knowledge assistant used by engineers at a large tech company. The system ingests docs, code snippets, and issue tickets; uses three vector stores (docs, code, tickets). Explain chunking per source, embedding models, indexing/refresh cadence, cross-store retrieval with a learned re-ranker, de-duplication and provenance, latency targets (e.g., 300ms retrieval, 2s answer), and outage fallbacks?","channel":"generative-ai","subChannel":"rag","difficulty":"advanced","tags":["retrieval","embeddings","vector-db","chunking"],"companies":["Google","Twitter"]},{"id":"q-335","question":"You're building a RAG system for SAP's customer support. How would you chunk a 10-page technical manual to ensure relevant sections are retrieved?","channel":"generative-ai","subChannel":"rag","difficulty":"beginner","tags":["retrieval","embeddings","vector-db","chunking"],"companies":["Amazon","Google","IBM","Microsoft","MongoDB","Planetscale","Sap"]},{"id":"q-4348","question":"Design a retrieval-augmented pipeline for real-time stock analytics. Given a user query about sector performance, implement (a) a chunking strategy for earnings reports and filings, (b) an embedding + vector store setup, (c) a two-stage retriever with optional re-ranking, and (d) data freshness checks against live feeds. Explain latency budgets, drift handling, and failure modes; include a minimal code sketch for indexing and querying?","channel":"generative-ai","subChannel":"rag","difficulty":"advanced","tags":["retrieval","embeddings","vector-db","chunking"],"companies":["Apple","Robinhood"]},{"id":"q-438","question":"You're building a RAG system for DoorDash's restaurant search. How would you design a hybrid retrieval strategy combining semantic and keyword search to handle queries like 'cheap Italian delivery near me' while maintaining sub-100ms latency?","channel":"generative-ai","subChannel":"rag","difficulty":"advanced","tags":["retrieval","embeddings","vector-db","chunking"],"companies":["Amazon","Apple","DoorDash","Google","Lyft","Meta","Microsoft","Netflix"]},{"id":"q-4775","question":"Design a retrieval-augmented QA system for a multilingual corporate knowledge base that answers policy questions. Use chunking for long PDFs, embeddings with a multilingual model, and a vector DB with per-document provenance. Describe indexing, chunk size and overlap, re-ranking, freshness with delta updates, latency targets, privacy controls, and end-to-end evaluation?","channel":"generative-ai","subChannel":"rag","difficulty":"advanced","tags":["retrieval","embeddings","vector-db","chunking"],"companies":["Google","Stripe"]},{"id":"q-551","question":"You're building a RAG system for Discord's message search. Messages have varying lengths, code blocks, and threaded conversations. How would you design your chunking strategy and what embedding model would you choose?","channel":"generative-ai","subChannel":"rag","difficulty":"advanced","tags":["retrieval","embeddings","vector-db","chunking"],"companies":["Discord","Snowflake"]},{"id":"q-1145","question":"Design an online nonlinear ICA pipeline for a 12-mic, 6-camera live broadcast where mixing is nonlinear and time-varying due to the environment. Propose an invertible neural network demixing model, online training with forgetting, a temporal prior to capture dynamics, and strategies for permutation/scale alignment. Include evaluation plan and DSP constraints?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Adobe","Citadel","Coinbase"]},{"id":"q-1208","question":"Design a privacy-preserving, edge-based ICA pipeline to separate overlapping speech captured by a distributed 32‑mic array where raw audio never leaves devices and only anonymized components are aggregated. Describe per‑device whitening with partial channels, online demixing updates, secure aggregation methods, permutation/scale alignment across devices, latency targets, drift handling, and an evaluation plan with synthetic ground truth and transcripts?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Discord","Goldman Sachs","Instacart"]},{"id":"q-1231","question":"In a smart conference room, 6 microphones capture audio while 2 cameras provide synchronized lip-movement visuals. Propose an ICA-based pipeline to jointly separate independent audio sources and align them to speaker identities using visual cues as auxiliary information. Include (i) whitening and joint diagonalization strategy, (ii) how to integrate visual cues into the contrast to improve permutation recovery, (iii) online adaptation for moving speakers, and (iv) a concrete evaluation plan with ground-truth sources and lip-sync metrics?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["LinkedIn","Meta"]},{"id":"q-1316","question":"3-mic wearable office demo: design a beginner ICA pipeline to separate two voices captured by near-field mics in a noisy room. Outline: (i) whiten the 3 channels, (ii) apply a simple real-valued FastICA on short FFT frames to recover two sources, (iii) align permutation across frames via spatial-map correlation, (iv) include a lightweight online update to the demixing matrix for motion, (v) evaluate with synthetic ground truth (SDR/SIR) and a listening check; target <100 ms latency on a low-power CPU?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["NVIDIA","Salesforce"]},{"id":"q-1458","question":"In a 4-channel EEG headset recording a brief resting-state session, design a beginner ICA pipeline to separate neural components from ocular and EMG artifacts. Outline: whitening, choice between real-valued FastICA or Infomax, frame-wise vs block-wise ICA with permutation alignment across windows, online adaptation for impedance drift, and a practical evaluation plan using simulated ground truth sources and known event-related potentials?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Google","Salesforce"]},{"id":"q-1500","question":"In a city-scale IoT deployment for smart buildings, 128 sensors stream heterogeneous, non-stationary signals (temperature, occupancy, vibration) that mix linearly in the cloud. Design a distributed online ICA to recover independent sources in real time, handling missing channels (packet loss), nonstationary mixing, and limited inter-node communication. Include whitening, update rules, drift handling, and an evaluation plan with ground truth?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Airbnb","Databricks","Uber"]},{"id":"q-1601","question":"Intermediate: In a 6-mic conference room capturing two moving speakers with strong reverberation and nonlinear mic distortions (AGC/compression), design a real-time post-nonlinear ICA (PNL-ICA) pipeline to separate sources. Include: a y = g(Ax) model, whitening strategy, online demixing updates, choice between per-bin ICA vs joint diagonalization, handling time-varying mixing, and an evaluation plan using SDR/SIR and transcript-aligned ground truth?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["Adobe","MongoDB","Zoom"]},{"id":"q-1646","question":"Beginner ICA task: A 4-mic array in a small classroom records two overlapping speakers with room reverberation. Design and implement a simple offline ICA pipeline (FastICA) to recover the two voices. Explain preprocessing (centering, whitening), nonlinearity, choice of whitening (PCA vs ZCA), how to align permutations across time windows, and provide a basic SDR/SIR evaluation using ground-truth signals?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Goldman Sachs","Google","OpenAI"]},{"id":"q-1742","question":"With an 8-mic array where each microphone samples at a slightly different rate causing synchronization drift, design a real-time ICA pipeline to separate sources under asynchronous observations. Specify (i) pre-alignment/handling of irregular samples, (ii) whitening strategy, (iii) online demixing updates tolerant to drift, (iv) latency targets and evaluation plan (SDR/SIR) with ground-truth alignment?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Databricks","LinkedIn","Twitter"]},{"id":"q-1768","question":"Beginner ICA task: In a live street interview, a 2-mic smartphone captures two overlapping speakers with wind noise. Design a lightweight online ICA pipeline that runs under 50 ms/frame on a mobile CPU. Include: (i) 20 ms frames with 50% overlap, (ii) whitening, (iii) a simple online ICA (2×2 demixing) with a tanh nonlinearity, (iv) frame-to-frame permutation alignment via cross-frame non-Gaussianity smoothing, (v) evaluation plan with synthetic ground-truth SDR/SIR and listening checks?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Adobe","Amazon","Coinbase"]},{"id":"q-1918","question":"In a 12-mic automotive cabin with three moving talkers and non-stationary noise, design an online convolutive ICA pipeline to separate sources in real time. Specify: whitening and natural-gradient demixing, per-bin vs joint diagonalization choice, online permutation/scale alignment with temporal continuity, drift handling, and an evaluation plan with SDR/SIR and ground-truth transcripts?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Amazon","Coinbase","Lyft"]},{"id":"q-1936","question":"Real-time 4‑mic ICA in a browser: use a 32 ms sliding window at 48 kHz with 50% overlap. Whitening via incremental PCA on 4 channels to produce uncorrelated components. Learn a 4×2 demixing matrix W with online gradient on y = W x using a tanh nonlinearity; adapt the learning rate to keep total latency under 60 ms. Resolve permutation by tracking envelope correlations frame-to-frame and reordering by energy; test with synthetic overlapping voices plus noise and report SDR/SIR?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Cloudflare","Databricks","Netflix"]},{"id":"q-1996","question":"You have 8 EEG channels capturing a simple task. Design an offline ICA pipeline (e.g., FastICA) to separate neural sources from eye-blink and muscle artifacts. Specify preprocessing (bandpass 1–40 Hz, centering), whitening, nonlinearity choice (tanh), number of components, criteria to identify artifact components (correlation with EOG/EMG, topographies), and a basic validation plan using simulated ground-truth sources and reference channels. End with a question mark?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Goldman Sachs","Stripe"]},{"id":"q-2117","question":"In a factory setting, an 8‑mic array is mounted on a mobile robot navigating among moving machinery noise. Design a real-time online convolutive ICA pipeline that (a) uses STFT framing, (b) models a time‑varying mixing matrix M(t), and (c) decides between per‑bin ICA vs joint diagonalization. Propose how to utilize IMU/pose priors to stabilize permutation and scale across time, handle drift, and meet real-time constraints. Include an evaluation plan with SDR/SIR and task‑specific metrics?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Citadel","Hashicorp","Square"]},{"id":"q-2129","question":"Design a real-time online ICA pipeline for a 12‑mic array on a factory floor capturing three moving sound sources (robot arm motors, alarms, and human chatter). The system must tolerate intermittent mic dropout, impulsive noise bursts, and a constrained CPU. Describe frame structure, whitening, demixing updates, per-bin vs joint diagonalization, dropout handling, latency targets, and evaluation plan (ground-truth SDR/SIR)?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["Plaid","Tesla","Twitter"]},{"id":"q-2200","question":"Beginner ICA with reference signal: A 3-mic array records two overlapping speakers in a small classroom with moderate reverberation. Design a lightweight semi-supervised ICA pipeline that uses a short reference sample from the target speaker to bias the demixing toward extracting that voice. Include whitening, online 2×2 demixing, reference-guided permutation, robustness to time-varying noise, latency < 100 ms, and an evaluation plan with ground-truth SDR/SIR and listening checks?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Google","IBM","Stripe"]},{"id":"q-2288","question":"Advanced: A 12-mic drone-mounted array records a busy construction site with Doppler shifts and wind noise. Design a real-time online convolutive ICA pipeline that (a) uses a neural prior to guide non-Gaussianity, (b) models a time-varying mixing matrix with a Kalman filter, (c) handles Doppler and wind, (d) meets latency <80 ms, and (e) provides a robust evaluation plan (SDR/SIR and speaker IDs)?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Cloudflare","Google","Slack"]},{"id":"q-2416","question":"Design an online audiovisual ICA pipeline for a 6-mic conference room where two speakers move and reverberation is strong. Use lip-tracking from video to provide soft priors that guide permutation and demixing across STFT bins. Specify frame size, whitening, online demixing updates, cross-modal fusion weights, handling time-varying mixing, latency targets, and an evaluation plan (SDR/SIR + listening checks)?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["Adobe","Snap"]},{"id":"q-2443","question":"In a moving car interior, an 8-mic array captures two near-field speakers plus ambient noise. Design a real-time ICA pipeline that adapts to changing geometry, handles intermittent mic dropouts, and preserves source permutations across time. Specify: (i) whitening and online demixing strategy, (ii) how to handle missing channels without reinitialization, (iii) latency target, (iv) evaluation plan with ground-truth SDR and listening checks?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["DoorDash","OpenAI","Zoom"]},{"id":"q-2514","question":"Video-guided ICA with lip cues: 4-mic array records two speakers; synchronized video provides lip-region activity. Propose a beginner-friendly real-time pipeline that uses lip cues as a soft reference to bias 2×2 demixing. Include whitening, online ICA, lip-based bias, frame permutation alignment, latency target (<100 ms), and evaluation plan (SDR/SIR + listening checks)?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Netflix","Scale Ai","Snap"]},{"id":"q-2536","question":"You're building a remote collaboration tool used by Discord/ Coinbase where 4 handheld mics on different users capture speech, but network jitter causes asynchronous sampling and occasional packet loss. Design a practical blind source separation pipeline to recover two active speakers in near real-time. Include: (i) synchronization strategy for misaligned frames, (ii) choice between spatially varying whitening and joint diagonalization, (iii) robustness to packet loss and time-varying mixing, (iv) latency targets, and (v) an evaluation plan with SDR/SIR and listener checks?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["Coinbase","Discord"]},{"id":"q-2545","question":"Beginner ICA in a noisy desk environment: a 4-mic array on a conference desk records two coworkers speaking while a desk fan and HVAC introduce non-stationary noise. Design a real-time ICA pipeline that runs on CPU. Specify: whitening, 2×2 demixing, choice between time-domain vs frequency-domain ICA, online update rule with learning rate, frame-to-frame permutation alignment, strategies for non-stationary noise, latency target, and an evaluation plan (synthetic ground-truth SDR/SIR plus listening checks)?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Apple","Google","MongoDB"]},{"id":"q-2691","question":"In a 9-mic wearable array mounted on a moving vehicle, three voices drift in and out amid wind noise. Design an online convolutive ICA pipeline to separate sources. Include: (i) STFT framing and whitening, (ii) online demixing for a 9×9 matrix, (iii) nonstationary mixing handling with a forgetting factor, (iv) frame-wise permutation alignment, and (v) a concrete SDR/SIR evaluation plan with ground truth?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["Apple","Coinbase","Databricks"]},{"id":"q-2695","question":"In a web app, you capture stereo audio from a 2-mic headset during a cafe meetup where two people speak and wind noise is occasional. Design a beginner ICA pipeline that runs entirely in the browser (JavaScript/WebAudio) using 50% overlap frames of 1024 samples (assuming 44.1kHz), with (i) whitening, (ii) a simple online 2×2 ICA using tanh nonlinearity, (iii) frame-to-frame permutation alignment based on non-Gaussianity, and (iv) a lightweight adaptation to nonstationary noise. Include latency targets (<80 ms) and a plan to evaluate with ground-truth SDR/SIR and listening checks?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["NVIDIA","OpenAI","Robinhood"]},{"id":"q-2737","question":"Design a distributed online ICA pipeline for a conference captured by three devices (laptop, smartphone, smart speaker) scattered across a room. Two overlapping talkers, reverberation, and packet loss. Outline synchronization, per-device whitening, an adaptive 2×2 demixer, gossip-style updates, and cross-device permutation alignment. Include latency, bandwidth, and evaluation plan?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Anthropic","Microsoft","Snap"]},{"id":"q-2763","question":"Scenario: A 4‑microphone desk array in a classroom records two voices with HVAC noise. Design a beginner, fixed-point ICA pipeline that runs on a microcontroller (16‑bit samples, limited RAM). Specify: (1) per-frame whitening, (2) 2×2 real-valued demixing, (3) cross-frame permutation/sign alignment, (4) a tiny online update to DM for motion, (5) evaluation plan with ground-truth SDR/SIR and latency targets?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Databricks","Snowflake"]},{"id":"q-2816","question":"Scenario: In a smart factory, 10 microphones capture a noisy, moving-robot environment with intermittent channel dropouts. Design a real-time streaming ICA pipeline that remains stable when channels fail. Include: (i) convolutional mixing model with frame-based processing, (ii) whitening strategy, (iii) online demixing with a 10×10 matrix, (iv) choice between per-bin ICA vs joint diagonalization, (v) handling of missing channels, and (vi) evaluation plan with ground-truth sources and latency targets?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["DoorDash","Lyft","Robinhood"]},{"id":"q-2908","question":"In a 16-mic car-cabin setting with moving speaker, reverberation and non-stationary noise, design a real-time convolutional ICA pipeline. Use frame-based processing (128-sample frames), whiten, and online 16×16 demixing updates. Compare per-bin ICA vs joint diagonalization, implement cross-bin permutation alignment, and handle mic dropouts gracefully. Provide an evaluation plan with ground-truth sources, SDR/SIR, and latency targets?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Discord","OpenAI"]},{"id":"q-2973","question":"Scenario: In an 8‑mic car cabin with moving talkers, wind noise, and nonlinear mic distortion, design a real-time convolutive ICA pipeline for an embedded GPU. Include: (i) 20 ms frames with whitening and online 8×8 demixing, (ii) a decision between per-bin ICA vs joint diagonalization, (iii) cross-bin permutation alignment, (iv) dropout robustness, and (v) an evaluation plan with ground-truth SDR/SIR and a latency target. How would you implement and validate it?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["Tesla","Twitter"]},{"id":"q-3069","question":"Scenario: A stadium-wide 20-mic array captures a live concert with dynamic crowd noise and intermittent mic dropouts. Design an online convolutional ICA pipeline robust to channel loss and nonstationary noise. Include: (i) frame-based processing with 64-sample frames, (ii) whitening, (iii) online 20×20 demixing with a choice between per-bin ICA and joint diagonalization, (iv) cross-bin permutation alignment and dropout handling, (v) latency under 120 ms, (vi) evaluation with ground-truth sources and a field demo. How would you approach this?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Meta","Netflix","Salesforce"]},{"id":"q-3140","question":"In a street interview scenario, a 3-mic array embedded in a wearable necklace records moving speakers and changing reverberation. Design a beginner online ICA pipeline with a fixed whitening stage and a 3×3 demixing matrix updated every 40 ms. Include: (i) a simple online ICA rule (e.g., natural gradient), (ii) frame-to-frame permutation alignment, (iii) dropout-resilient strategy for intermittent mic failures, and (iv) an evaluation plan using synthetic ground-truth SDR/SIR and quick listening checks. Explain trade-offs?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Goldman Sachs","Netflix"]},{"id":"q-3213","question":"Scenario: A conference room uses 8 fixed mics and 2 wearable mics on participants. Clock drift and intermittent wireless dropouts cause nonuniform timing. Design a real time online convolutive ICA pipeline to separate speakers without centralized synchronization. Include: (i) frame based STFT with online whitening, (ii) an 8 by 10 demixing matrix updated via online joint diagonalization or gradient ICA, (iii) cross device permutation alignment and dropout handling, (iv) clock drift compensation, (v) latency target, and (vi) evaluation plan using ground truth sources and SDR?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["Meta","Robinhood","Slack"]},{"id":"q-3268","question":"You have a 3-mic wearable array attached near the driver's shoulder in a car cabin. The target speaker is moving and the mixing path drifts slowly with road noise and HVAC. Design a beginner-friendly online ICA pipeline to separate two sources under slowly varying mixing. Include whitening, a 3×3 online demixing, a forgetting-factor update, simple frame-to-frame permutation alignment, and a pragmatic evaluation plan with ground-truth SDR/SIR and listening checks?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Google","Oracle"]},{"id":"q-3427","question":"In a live esports arena with 12 microphones (wearables on players and fixed stage mics) where participants move, sources emerge and disappear and reverberation is high, design a real-time nonlinear ICA-inspired BSS pipeline. Include: (i) a geometry-conditioned neural demixer with frame-based processing, (ii) online whitening and stable updates, (iii) dynamic demixing matrices conditioned on real-time geometry, (iv) handling of source emergence/dropout and variable channel quality, (v) latency target and an evaluation plan using ground-truth or high-fidelity synthetic data?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Google","NVIDIA","Uber"]},{"id":"q-3509","question":"In a 12-mic drone swarm where sensors move relative to sources, design an online blind source separation pipeline that handles a time-varying mixing matrix A(t) and up to 6 sources. Use a tensor-ICA or streaming joint diagonalization approach, include online whitening, fast updates, permutation alignment across time frames, dropout handling for occluded channels, and a practical evaluation plan with SDR/SIR and latency targets?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Coinbase","Instacart","Microsoft"]},{"id":"q-3534","question":"A 4-microphone conference-room recording experiences slow gain drift and moving sources; design a beginner ICA pipeline that (i) uses short-time Fourier domain whitening with per-frame gain normalization to counter drift, (ii) performs online 4×4 demixing for two sources, (iii) chooses between per-bin ICA and joint diagonalization, (iv) tracks nonstationary mixing across frames with a lightweight estimator, and (v) specifies a latency target plus an evaluation plan with ground-truth SDR/SIR — all in a practical, implementable way?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Amazon","Plaid","Zoom"]},{"id":"q-3747","question":"Propose a real-time, multi-modal ICA design for a 6-mic array in a conference room with two moving speakers and strong reverberation. Your answer should specify how video lip-sync cues guide permutation alignment across frequency bins, and detail the online convolutive ICA pipeline: STFT settings, whitening, 6×6 demixing updates, per-bin ICA vs joint diagonalization, cross-frame permutation tracking, and latency plus evaluation plan?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["Meta","Robinhood","Scale Ai"]},{"id":"q-3759","question":"In a 64-mic array deployed in a noisy, reverberant environment with intermittent channel dropouts, design an online nonlinear ICA pipeline that handles non-Gaussian sources. Specify nonlinear priors, online whitening and demixing, adaptive nonstationarity handling, permutation alignment across bands, and a concrete latency target (~100 ms). Include validation plan?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Databricks","Robinhood","Tesla"]},{"id":"q-3792","question":"In a 12-mic drone swarm, design a real-time online convolutional ICA pipeline under tight CPU constraints with moving sources and nonstationary crowd noise. Detail: 64-sample frames, online whitening with recursive PCA (forget factor ~0.95), a 12x12 demixing matrix updated via online joint diagonalization, cross-bin permutation tracking, and dropout-resilient updates. Compare per-bin ICA vs joint diagonalization and target latency under 80 ms. How would you evaluate?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Citadel","Scale Ai","Stripe"]},{"id":"q-3863","question":"Scenario: A 24-mic conference array in a dynamic room experiences moving talkers and highly time-varying reverberation. Propose a real-time online convolutional ICA pipeline that uses a lightweight neural whitening stage to adapt to nonstationarity, and compare per-bin ICA vs joint diagonalization, including cross-bin permutation tracking, dropout resilience, and a robust evaluation plan with ground-truth sources?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Adobe","Airbnb"]},{"id":"q-3998","question":"Beginner ICA for outdoors: a 3-mic array on a delivery drone records two distant speakers with wind noise. Design a real-time ICA pipeline using fixed whitening and a 2×2 demixer updated every 120 ms. Compare per-bin ICA vs joint diagonalization; add a cross-bin permutation strategy using energy and phase cues; propose an evaluation plan with ground-truth sources?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Instacart","Snowflake","Two Sigma"]},{"id":"q-4162","question":"Beginner ICA in a moving car: A 5-mic array embedded in a car cabin records a driver's speech with road hum and HVAC. Design a real-time convolutional ICA pipeline that accounts for time-varying geometry by fusing IMU data to adapt whitening and demixing. Compare frame-wise online ICA vs joint diagonalization with a Kalman-style tracker for the demixing matrix; outline a practical evaluation with ground-truth sources?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["DoorDash","Google","Hugging Face"]},{"id":"q-4185","question":"Design a fixed-point online convolutive ICA pipeline for a 12-mic live meeting room, constrained to embedded DSP. Use STFT-based whitening with fixed precision, a 12x12 demixer updated every 8 ms, and evaluate per-bin ICA vs joint diagonalization under quantization. Include drift tracking for nonstationary mixing and occasional mic dropouts, with a 200 ms end-to-end latency target and SDR/SIR ground-truth evaluation plan?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Oracle","Stripe","Zoom"]},{"id":"q-4247","question":"Scenario: A 12-antenna RF receiver in a mobile urban environment captures overlapping transmissions with time-varying channels and Doppler. Design a real-time convolutional ICA pipeline to separate sources. Include: (i) block-based processing, (ii) whitening, (iii) online demixing with a 12×12 matrix, (iv) choice between per-bin ICA vs joint diagonalization, (v) cross-frame permutation alignment, (vi) Doppler/adaptation handling, (vii) latency target, (viii) evaluation plan with ground-truth waveforms or synthetic signals and SDR/SIR. Provide key trade-offs and early implementation steps?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["Anthropic","MongoDB","Snap"]},{"id":"q-4308","question":"Scenario: A 3-mic array on a smartphone records two nearby voices in a cafe with ambient noise. Propose a beginner online ICA pipeline that runs on-device with a lightweight whitening stage over a 32-frame window and a 3x3 demixer updated every 50 ms. Decide between time-domain ICA vs STFT-domain ICA, and outline a simple cross-frame permutation method using envelope correlation. Include an evaluation plan with ground-truth sources and listening checks?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Cloudflare","Salesforce","Snowflake"]},{"id":"q-4377","question":"Scenario: In a car cabin, a 14-mic array records a live meeting with engine and road noise. Design a real-time online convolutive ICA pipeline to separate up to 6 sources with moving talkers. Include: STFT-based mixing, whitening, online demixing with a 14×14 matrix, choice between per-bin ICA vs joint diagonalization, cross-bin permutation tracking, nonstationarity handling, latency target, and an evaluation plan with ground-truth sources and SDR/SIR?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Google","IBM","OpenAI"]},{"id":"q-4471","question":"In-car cabin scenario: a 4-mic array records the driver and passenger while engine and road noise dominate. As a beginner, design a real-time ICA pipeline for on-device use with a fixed whitening stage and a 2×2 demixer updated every 100 ms. Decide between time-domain ICA and STFT-domain ICA, and outline a simple cross-frame permutation strategy using energy envelopes. Include an evaluation plan with ground-truth sources?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Airbnb","Instacart","Snowflake"]},{"id":"q-4549","question":"Scenario: a 60-mic array in a busy conference hall records a live talk with moving speakers and crowd noise. Propose a scalable, real-time ICA pipeline that handles asynchronous sampling and occasional mic dropout, using streaming STFT and an online demixer. Cover (i) channel imputation for dropped mics, (ii) adaptive whitening with partial covariance, (iii) cross-band permutation tracking, (iv) online joint diagonalization vs per-bin ICA trade-offs, (v) latency goals, and (vi) validation with ground-truth sources and SDR/SIR?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Meta","Twitter","Zoom"]},{"id":"q-4623","question":"Scenario: In-car cabin with a 4-mic array, engine and road noise, two speakers. Design a beginner online ICA pipeline that runs on-device with fixed whitening and a 2×2 demixer updated every 60 ms to separate the voices. Decide between STFT-domain ICA vs time-domain ICA, and describe a simple cross-frame permutation-tracking method using inter-channel phase differences and energy cues. Include an evaluation plan with ground-truth sources and listening checks?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Lyft","Meta","MongoDB"]},{"id":"q-4774","question":"Scenario: A 3-mic array on a moving delivery robot in a bustling office records two speakers with time-varying room acoustics and intermittent HVAC noise. Propose a beginner ICA-based blind source separation pipeline running offline on a laptop. Include: representation choice (time vs STFT), whitening, a 2×2 demixer updated every 60 ms, a simple cross-frame permutation using energy/phase cues, and an evaluation plan with ground-truth sources?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Airbnb","Discord","Google"]},{"id":"q-4840","question":"Scenario: A 3-mic desk array in a busy cafe records two talking people and ambient chatter. Design a lightweight online convolutional ICA pipeline that runs on CPU with time-domain whitening and a 2x2 demixer updated every 100 ms. Include a fault-tolerance path: if one microphone drops, detect it quickly and reinitialize whitening and the demixer with minimal latency. Outline permutation-tracking, latency implications, and a practical eval plan using ground-truth sources and SDR/SIR?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Plaid","Salesforce","Twitter"]},{"id":"q-839","question":"**Advanced ICA Challenge**: Given a 3×N mixed signal matrix from sensors, outline a concrete plan to recover independent sources with FastICA. Include whitening steps, nonlinearity choice (e.g., g(u)=tanh(u)), convergence criteria, how you resolve sign/perm ambiguity, and how you compare to PCA on the same data. Include practical inputs and diagnostics?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["IBM","Two Sigma","Uber"]},{"id":"q-875","question":"Given a 6-channel wearable time-series with non-stationary motion artifacts and slowly varying latent sources, design an online ICA workflow to separate the sources in real time. Specify streaming whitening, adaptive contrast functions, choice of nonlinearity, handling sign and permutation drift, and a robust validation plan against synthetic ground truth and a PCA baseline?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["Citadel","Salesforce"]},{"id":"q-910","question":"An 8-microphone array records a live conference with moving speakers and reverberation. Design a practical convolutive ICA pipeline to separate sources. Include: STFT-based mixing, choice between per-bin ICA vs joint diagonalization, permutation alignment across frequency bins, tracking non-stationary mixing, real-time feasibility, and evaluation plan (SDR/SIR, ground truth)?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["LinkedIn","OpenAI","Robinhood"]},{"id":"q-953","question":"Design a real-time, online complex-valued ICA pipeline to separate RF sources from an 8-antenna receiver in a dynamic multipath environment with drifting mixing. Describe (a) complex whitening + fixed-point ICA update, (b) the complex contrast and convergence rule, (c) tracking sign and permutation drift over time, and (d) a practical evaluation plan with synthetic ground truth and over-the-air tests under DSP constraints?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["IBM","Microsoft","NVIDIA"]},{"id":"q-1064","question":"Design and implement a streaming app's episode grid using a UICollectionView with a compositional layout that supports dynamic item sizes, infinite scrolling, and efficient image caching; use a modern iOS approach (Diffable Data Source, NSCache, prefetching), ensure accessibility and testability?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["Apple","Netflix","Tesla"]},{"id":"q-1164","question":"You're building a lightweight notes app for iOS. Implement a NotesEditor view (SwiftUI) that autosaves to disk as the user types, using a 500ms debounce. Persist to a local JSON file in the app's documents directory. Ensure rapid typing doesn't cause multiple disk writes, and provide a minimal unit test that verifies the file content is updated after a debounce period?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Discord","Google","Tesla"]},{"id":"q-1185","question":"In an iOS app, you are displaying a feed of user avatars in a UICollectionView with infinite scrolling. Explain how you would implement incremental image loading that cancels obsolete requests, caches images both in memory and on disk, uses Swift concurrency for loading, and ensures smooth scrolling under memory pressure, including prefetching and memory-pressure handling strategies?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["Airbnb","Bloomberg","Goldman Sachs"]},{"id":"q-1439","question":"In a recipe-list iOS app, implement a multi-select ingredient filter as tappable chips using UIKit. Given a static dataset of recipes with ingredients, create a chips bar that allows selecting multiple ingredients, a Clear button, and a table view that shows only recipes containing all selected ingredients. Ensure accessibility labels and VoiceOver order, and provide a simple unit test validating the filter logic?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Apple","Databricks","DoorDash"]},{"id":"q-1573","question":"Design a beginner iOS feature: a SwiftUI-based **Favorites** screen for a recipe app. It should load a predefined array of Recipe objects from a local JSON file, display in a single-column list, allow tapping to mark/unmark as favorite, persist favorites in **UserDefaults**, and provide a search bar to filter by name **case-insensitively**. Explain how you'd structure the model, storage, and UI, and how you'd test search and persistence?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Airbnb","Google","Netflix"]},{"id":"q-1611","question":"Design and implement an offline-first notes feature for iOS: store locally in SQLite, use a simple CRDT-like merge or last-writer-wins with version vectors for conflict resolution, and sync changes via a WebSocket protocol, all in Swift using async/await. Include data model, conflict handling, and testing strategy?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["OpenAI","Oracle"]},{"id":"q-1681","question":"In an iOS SwiftUI app, build a minimal Notes editor screen: a multiline TextEditor bound to a string. Implement an autosave that triggers 1 second after the user stops typing, saving the draft to UserDefaults under the key 'noteDraft'. Load the draft on view appear. Include a tiny unit test that simulates typing and asserts the draft is saved after the debounce delay?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Airbnb","Google"]},{"id":"q-1718","question":"In a high-traffic iOS app for autonomous telemetry, describe a robust data pipeline that streams sensor data into local batches and uploads to cloud with offline queueing and crash recovery. Include how to handle backpressure, idempotency, and testing. Provide concrete Swift components and a minimal prototype?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["IBM","NVIDIA","Tesla"]},{"id":"q-1951","question":"You're building an **offline-first** image gallery in an iOS app. Describe in detail how you would implement a robust caching and download strategy that supports offline browsing, seamless updates when connectivity returns, and **conflict resolution**. Include data models, caching policy, background downloads, and testing approaches?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["Scale Ai","Tesla","Zoom"]},{"id":"q-2089","question":"Design an offline-first album sync for iOS that uses a CRDT-based OR-Set to resolve conflicts in PhotoMetadata across devices. Include data model, merge rules with per-album ACLs, and a test plan for intermittent connectivity?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["Discord","IBM","Scale Ai"]},{"id":"q-2184","question":"Begin with a concrete scenario: You have an iOS app that shows a list of product names. Build a minimal UIKit view controller that renders the list in a `UITableView` with a search bar. Implement a 300ms **debounce** using Swift's async/await to filter results on a background queue, then present them on the main thread. Filter should be case-insensitive and preserve order. Provide the core view controller code and explain cancellation behavior?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Coinbase","Google","Microsoft"]},{"id":"q-2341","question":"Scenario: Build a real-time presence client for an iOS app used by distributed teams (Zoom/Square/PayPal). Use a WebSocket to publish/receive presence events, implement exponential backoff reconnect, background processing, and idempotent event application; provide a minimal Swift prototype and a test plan for intermittent networks. How would you implement this?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["PayPal","Square","Zoom"]},{"id":"q-2355","question":"Design a live-map ETA/route update system for a rideshare app with intermittent connectivity. Describe an architecture using MapKit, CoreData offline persistence, and CloudKit sync with Swift concurrency. Include conflict resolution strategy, data integrity guarantees, and test plan?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["Amazon","Lyft"]},{"id":"q-2391","question":"Design a high-performance, offline-first photo feed in an iOS app. Render a UICollectionView that shows square image tiles with captions, support infinite scrolling, and prioritize visible cells for image downloads. Implement disk caching (LRU), memory pressure eviction, and offline fallback when network is unavailable. Ensure VoiceOver and Dynamic Type. Provide data model, caching strategy, testing plan, and a concise code sketch for an ImageLoader using async/await?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["Amazon","Discord","PayPal"]},{"id":"q-2570","question":"**iOS Beginner Challenge:** Build a minimal journaling screen where a user types notes, autosaves to a local file atomically, and displays a list of notes. Ensure data is preserved across app restarts, support background/foreground transitions, and debounce saves to avoid thrashing. Use Swift with SwiftUI, Codable Note, and FileManager in the Documents directory. Include a simple test plan?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Hashicorp","Snap"]},{"id":"q-2776","question":"Build a SwiftUI screen that loads a local JSON array of flashcards (id, question, answer) from the app bundle, displays one card at a time with a tap to flip, and includes a Next button. Persist the current index and per-card view counts to a local file atomically so the session resumes where left off on restart. Ensure Dynamic Type and Dark Mode compatibility; keep it beginner-friendly and testable?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Apple","OpenAI","Oracle"]},{"id":"q-2805","question":"Design an offline-first, conflict-aware sync for an iOS chat feature using Core Data for local storage and a REST API. Describe the Change Journal you'd implement, incremental pull using a since timestamp, an upsert push queue for new/edited messages, and a deterministic conflict resolution policy (e.g., last-updated-wins with timestamps). Include data structures, synchronization flow, backoff strategy, and a minimal test plan?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["LinkedIn","Twitter","Two Sigma"]},{"id":"q-2859","question":"Design and implement a high-throughput annotation queue in an iOS app used for labeling image datasets. Build a per-image task manager using Swift Concurrency (actors, AsyncSequence) to assign image slices to workers, enforce in-order UI updates, backpressure when the back-end lags, and automatic retries with exponential backoff. Provide API surface, a minimal SwiftUI view demo, and explain trade-offs?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["Google","NVIDIA","Scale Ai"]},{"id":"q-2976","question":"Implement a local-first photo uploader in an iOS app: when users select multiple photos for upload, queue them in a disk-backed manifest, process with a bounded concurrency worker, upload via URLSession background tasks, retry with exponential backoff on failures, and expose progress to the UI. How would you implement and test this to ensure persistence across relaunch and background/foreground transitions?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["Meta","Netflix","Scale Ai"]},{"id":"q-3149","question":"Build a tiny iOS 'Task Timer' app: create tasks each with a duration, run per-task timers that pause on background, resume on foreground, and persist elapsed time to disk atomically via a Codable Task model stored in the Documents directory. Use SwiftUI and Combine; debounce saves to avoid thrashing; provide per-task accessibility labels and a filter for Active/Completed. How would you implement this end-to-end?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Cloudflare","Salesforce","Tesla"]},{"id":"q-3326","question":"Design a real-time collaborative note editor for iOS that works offline and syncs via CloudKit. Describe architecture for a sequence CRDT text model with per-note version vectors, deterministic merges, and offline persistence in CoreData. Explain testing with simulated disconnects, concurrent edits, and data integrity checks?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["Meta","MongoDB"]},{"id":"q-3357","question":"Design a secure offline-first vault feature for iOS that stores sensitive notes and attachments encrypted at rest with per-note keys, deriving keys from a user passphrase using a KDF, storing metadata in CoreData, and syncing encrypted blobs via CloudKit. Explain key management, rotation, and a testing plan including device loss, passphrase changes, and offline use?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["OpenAI","Tesla"]},{"id":"q-3696","question":"As a module for a rideshare delivery dashboard, implement a map-based order tracker that consumes live updates over a WebSocket. The app must survive offline: persist last-known Orders in Core Data, deduplicate updates by eventId, and replay queued events on reconnect with exponential backoff. Also handle out-of-order messages and debounce UI updates. Use Swift, MapKit, Core Data; prefer URLSessionWebSocketTask?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["DoorDash","Tesla"]},{"id":"q-3796","question":"Design a beginner-friendly iOS SwiftUI screen: a note editor with a live word count. Debounce user input with Combine (300ms) before updating the count, and persist the last note to disk across restarts. Include handling of background/foreground transitions and a basic test plan?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Scale Ai","Stripe","Twitter"]},{"id":"q-3921","question":"Design an iOS data pipeline for a live analytics feed. The app connects to a WebSocket to receive events (JSON per event), decodes them into a Codable Event, and renders a live, diffable list. Requirements: (1) use URLSessionWebSocketTask for the socket, (2) an in-memory buffer capped at 1,000 events with oldest drops, (3) on-disk queue preserving order with atomic writes, (4) a deterministic replay API for offline mode, (5) UI stays responsive; discuss concurrency, error handling, backpressure, and testing?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["Citadel","Google","Slack"]},{"id":"q-3953","question":"Design and implement an offline-first, multi-device to-do collaboration feature for an iOS app. It should store locally in Core Data, sync with a REST API, and reconcile conflicts using a versioned record with last-writer-wins strategy. Use SwiftUI, async/await, and an Actor-based SyncManager to serialize cross-device updates. Provide an outline of data models, a minimal SwiftUI view, and a test plan?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["Adobe","Oracle"]},{"id":"q-3975","question":"Design an offline-first PDF annotation workflow for iOS: use PDFKit to render and annotate PDFs, persist annotation state in Core Data with per-document versioning, and implement a sync to a remote REST API that merges edits using last-modified timestamps. Include optimistic UI updates, conflict handling when the same annotation is edited offline across devices, and a test strategy for offline edits, conflict cases, and relaunch integrity?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["Apple","Google","Hashicorp"]},{"id":"q-4019","question":"Design an on-device analytics module for an iOS app that preserves user privacy. The module should: record lightweight events locally; batch and aggregate them into daily buckets; upload via a background URLSession with exponential backoff and jitter; honor a user opt-in/opt-out toggle and per-event consent; persist across restarts. Explain data model (Core Data or SQLite), storage lifecycle, encryption, and a test plan that simulates offline, partial writes, and data corruption?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["Discord","LinkedIn","Robinhood"]},{"id":"q-4135","question":"Implement a small flashcard quiz in iOS using SwiftUI: load questions from a bundled JSON (title, prompt, answer, category). Show a card with the prompt; tap to flip and reveal the answer; add Next/Prev to navigate; persist last index across restarts via UserDefaults; debounce taps; ensure accessibility by labeling the card and using VoiceOver hints. Include a basic unit test for JSON decoding and scoring?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["DoorDash","Square"]},{"id":"q-4204","question":"Build a lightweight audio memo feature: in a SwiftUI app, allow recording a 15–60 second memo using AVAudioRecorder, save to the app's Documents directory with an atomic write, and display a list of memos with duration and date. Enable playback via AVAudioPlayer, persist memos across launches, and handle background/foreground transitions. Provide a minimal test plan focusing on file I/O integrity and playback reliability. Use Swift/SwiftUI?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Citadel","Coinbase","Robinhood"]},{"id":"q-4298","question":"Build a beginner iOS SwiftUI FAQ viewer: load a bundled JSON of FAQs (id, question, answer, category), display a searchable list with a debounced search (Combine) that filters on question or answer, a tap expands to show the answer, and persist the last search query to UserDefaults. Include a basic unit test for JSON decoding and search filtering?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Bloomberg","Meta","Netflix"]},{"id":"q-4369","question":"Design offline-first iOS chat feature: drafts are queued locally and synced to the server when online. Describe the CoreData data model (fields: id, convoId, localSeq, serverId, status, timestamp), the background sync pipeline using URLSession, and the conflict resolution strategy (per-convo ordering, idempotent upserts). Include a test plan for disconnects, message reordering, and partial failures?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["Cloudflare","Two Sigma","Uber"]},{"id":"q-4442","question":"Build a beginner SwiftUI iOS Gallery screen that loads image URLs from a bundled JSON, displays a grid, downloads images via URLSession, and caches each image on disk atomically in the Documents directory so it persists across restarts. Include a manual Refresh to re-fetch metadata and a basic unit test verifying disk write/read of a cached image?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Instacart","LinkedIn","Uber"]},{"id":"q-4535","question":"Design offline-first iOS feature: a 'Comments Console' for a document collaboration app. Users draft comments offline; changes persist in Core Data and are synced as delta patches when the network becomes available. Describe the data model, patch format, conflict resolution, and a test plan for offline edits, concurrent edits, and reconnect scenarios?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["Discord","Hugging Face"]},{"id":"q-464","question":"How would you implement a custom UICollectionViewFlowLayout that supports dynamic cell heights and sticky headers while maintaining smooth scrolling performance?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["Amazon","Lyft","Meta"]},{"id":"q-4648","question":"In an iOS streaming app with offline transcripts per user, implement a secure offline cache that is per-user encrypted at rest, uses keys stored in the Keychain, and automatically re-encrypts on logout; design the data model, pick storage (Core Data vs SQLite), implement background downloads with URLSessionBackgroundTask, and a test plan to verify data isolation, migration, and crash recovery?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["Meta","Netflix"]},{"id":"q-4710","question":"**Offline-First Ledger:** Design an offline-first ledger viewer for a fintech app where data is encrypted end-to-end with keys in Secure Enclave, persisted atomically in a local database, and synced to a central server when online. Include per-user permissions, deterministic conflict resolution, and crash-safe writes. Use Swift, SwiftUI, and async/await; outline architecture, data model, encryption workflow, and a practical test plan; provide a concise code skeleton.**?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["Coinbase","Meta","Netflix"]},{"id":"q-4751","question":"Implement a minimal To-Read list in SwiftUI: load articles from a bundled JSON (title, author, tag, publishedDate). Display as a list, allow tapping a row to toggle 'read' status, and debounce taps. Persist the array to UserDefaults using JSONEncoder/Decoder so state survives app restarts. Add a search bar that filters by title or tag with a 300ms debounce. Ensure accessibility for VoiceOver and Dynamic Type. What is your test plan?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Amazon","Google"]},{"id":"q-4792","question":"Build a minimal note-taking screen in iOS (SwiftUI) that lets users create notes with a title and body, assign 0-3 tags, and persist all notes to a single local JSON file in the Documents directory. Add a tag-based filter and a live search bar that debounces input to keep UI snappy. Ensure data persists across restarts, handle app background/foreground transitions, and provide basic accessibility hints?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Cloudflare","Microsoft","Uber"]},{"id":"q-495","question":"How would you implement a simple UITableView with custom cells in iOS using Swift?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Oracle","Snowflake","Uber"]},{"id":"q-525","question":"You're building a food delivery app like DoorDash. How would you implement background location updates to track delivery drivers while balancing battery life and accuracy?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["DoorDash","Microsoft","Zoom"]},{"id":"q-578","question":"What's the difference between weak and unowned references in Swift, and when would you use each?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Meta","NVIDIA"]},{"id":"q-181","question":"Explain the difference between weak and unowned references in Swift and provide practical use cases for each?","channel":"ios","subChannel":"swift","difficulty":"intermediate","tags":["swift","language"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"q-257","question":"What is optional chaining in Swift and how does it compare to force unwrapping, optional binding, and optional chaining with method calls when accessing nested optional properties?","channel":"ios","subChannel":"swift","difficulty":"beginner","tags":["optionals","protocols","generics"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Stripe"]},{"id":"q-204","question":"How would you optimize a UITableView with 10,000+ complex cells using Auto Layout while maintaining 60fps scrolling and memory efficiency?","channel":"ios","subChannel":"uikit","difficulty":"advanced","tags":["autolayout","tableview","collectionview"],"companies":["Airbnb","Apple","Capital One","Lyft","Uber"]},{"id":"q-232","question":"How does Auto Layout constraint resolution work when creating a UITableView with dynamic cell heights?","channel":"ios","subChannel":"uikit","difficulty":"beginner","tags":["autolayout","tableview","collectionview"],"companies":["Airbnb","Apple","Google","Meta","Uber"]},{"id":"q-1075","question":"You're building a beginner-friendly KCA flow for a data science platform where notebooks run in isolated containers across tenants. The platform uses a cloud CA to issue short-lived client certificates (2 hours) via token-based enrollment (JWT) rather than CSR, with a gateway that validates certs and maps tenants. Describe the enrollment flow, renewal, and how the gateway handles clock skew, expired certs, and failed enrollments. Include a minimal test plan and example API calls?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Databricks","Hugging Face"]},{"id":"q-1082","question":"Design a beginner-friendly KCA workflow focusing on policy-driven certificate issuance for a multi-tenant SaaS gateway: tenants publish per-tenant certificate policies (allowed CNs, key type, validity). A central CA issues short-lived client certs, and edge gateways refresh policy every 60 minutes while rotating certs to avoid handshake failures. Explain enrollment, policy propagation, revocation, and provide a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Adobe","Cloudflare","DoorDash"]},{"id":"q-1120","question":"Explain how you would implement a beginner-friendly KCA workflow for a multi-region SaaS where regional CAs issue per-service client certs with a 2-hour TTL. Include enrollment (CSR-based vs token-based), cross-region policy propagation, revocation, and how to validate certificates at the edge during region failover. Provide a minimal test plan and rollback strategy?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Apple","Databricks","Instacart"]},{"id":"q-1141","question":"For an industrial IoT fleet of 50k field devices with intermittent connectivity, design a KCA workflow where each device boots with a hardware-backed key and obtains a short-lived client certificate from a central CA via an attestation-enabled bootstrap. Include enrollment, attestation checks, certificate lifetimes (6 hours), automatic renewal after reconnection, revocation for decommissioned devices via OCSP/CRLs, offline root fallback, auditing, and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Meta","NVIDIA"]},{"id":"q-1189","question":"Design a beginner-friendly KCA flow for a multi-tenant SaaS gateway that issues per-tenant, short-lived client certificates via token-based enrollment, but enforces per-tenant issuance quotas (e.g., 100 certs/hour with burst to 10x). Explain enrollment, quota enforcement, renewal, revocation, and audit logging, and provide a minimal test plan. Include how you handle quota exhaustion during spikes?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Meta","Slack","Uber"]},{"id":"q-1353","question":"Design a beginner-friendly KCA workflow for a multi-region SaaS API gateway where a primary CA replicates to a secondary region for DR. Explain policy propagation, certificate rollover during failover, cache freshness, cross-region revocation handling, and how edge gateways and clients stay in sync with minimal downtime. Include a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Bloomberg","Databricks","Uber"]},{"id":"q-1527","question":"In a multi-region SaaS gateway architecture, design a KCA-based onboarding flow for both tenants and devices using mTLS. Devices boot with hardware attestation and CSR enrollment to a regional intermediate CA, which signs a 15-minute device cert bound to device identity and tenant policy. Describe enrollment, policy propagation, cross-region trust, cert rotation, revocation (OCSP/CRL), and audit logging. Include failure modes and a concrete 1-page test plan with synthetic onboarding scenarios?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Amazon","Microsoft"]},{"id":"q-1568","question":"Design a global **KCA onboarding** flow for an edge gateway fleet serving 100k tenants. Each gateway boots with a hardware-backed secret and attestation-enabled CSR enrollment, then obtains a short-lived client certificate whose policy extension encodes per-tenant access rules and rate limits. Explain policy refresh, renewal, revocation, and audit trails. Include a concrete example policy and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Adobe","Salesforce"]},{"id":"q-1645","question":"Design a KCA onboarding workflow for 20k edge gateways deployed across regions where each device boots with hardware attestation and a measured firmware hash. The regional CA issues a 12-hour TLS client certificate bound to the firmware hash and attestation identity. Describe enrollment, firmware-hash policy binding, renewal on upgrade, rollback revocation via OCSP/CRLs, offline root fallback, and comprehensive auditing. Include a practical test plan with upgrade/rollback scenarios?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["MongoDB","Scale Ai","Tesla"]},{"id":"q-1651","question":"Design a KCA onboarding and certificate lifecycle flow for 200 edge API gateways deployed across three data centers, serving both internal microservices and external partners. Each gateway boots with a hardware-backed root and attestation-enabled CSR enrollment, then obtains a 10-minute mTLS client certificate from a regional CA. Include enrollment, region-aware policy propagation via a streaming config service, cross-region trust, certificate renewal, revocation via OCSP/CRLs, offline root fallback, auditing, and a concrete 1-page test plan?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["IBM","Meta","Two Sigma"]},{"id":"q-1748","question":"Design a beginner-friendly KCA workflow for a data ingestion platform used by Snowflake, Airbnb, and OpenAI that authenticates per-tenant producers and external partners via mTLS? Implement per-session certificates (30 min) with hardware-backed attestation and policy refresh via streaming config every 15 min. Cover enrollment, renewal, revocation, audit logs, and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Airbnb","OpenAI","Snowflake"]},{"id":"q-1878","question":"In a high-throughput data desk environment, design a KCA-based onboarding for 1,000 streaming data connectors across two data centers. Each connector boots with hardware attestation and obtains a 60-second TLS client certificate from a regional CA, rotated automatically on reconnection. Include enrollment, attestation checks, policy binding to data-stream permissions, cross-region trust, revocation (OCSP/CRLs), offline root fallback, auditing, and a test plan with simulated reconnections and data bursts?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Bloomberg","Two Sigma"]},{"id":"q-1961","question":"Design a KCA onboarding for a global fleet of 5,000 IoT/edge devices migrating from RSA TLS to post-quantum TLS during active operation. Devices boot with hardware-backed keys and attestation-based CSR; regional CAs issue both RSA and PQC certificates with 6-hour lifetimes, automatic renewal on reconnect, and cross-region trust. Include dual-PKI policy binding, revocation (OCSP/CRLs), offline root fallback, auditing, and a 1-page test plan with PQC handshake fallbacks and rollback?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Citadel","LinkedIn","Uber"]},{"id":"q-1978","question":"Design a beginner-friendly KCA workflow for a serverless edge gateway fleet serving 50k tenants. Issue per-tenant client certs with 15-minute lifetimes and a 'scope' extension describing allowed APIs and rate limits. Enrollment via CSR or token; propagate policy to edge nodes via streaming config with staggered renewals. Support per-tenant revocation via OCSP stapling or small CRLs; include audit logs and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Amazon","Google","Lyft"]},{"id":"q-2004","question":"Design a beginner-friendly KCA workflow issuing per-store client certificates with 1h TTL for a distributed edge gateway fleet in 200 retail locations. Enrollment via CSR templates or token-based enrollment; embed per-store API access and rate limits in a certificate policy extension. Include policy propagation via streaming config with staggered renewals, offline revocation via local CRLs, and audit logs. Provide a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Instacart","Snowflake","Two Sigma"]},{"id":"q-2069","question":"Design a KCA onboarding and certificate lifecycle for an autonomous drone fleet deployed across three countries, operating with intermittent connectivity and strict flight-permission policies. Drones boot with hardware-backed keys and attestation; obtain a short-lived TLS client certificate from a regional CA, with policy-bound flight rights, cross-region trust, and automatic certificate rotation on rejoin. Include revocation via OCSP/CRLs, offline root fallback, auditing, and a 1-page test plan with realistic sorties?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Cloudflare","NVIDIA","Oracle"]},{"id":"q-2166","question":"Design a KCA flow for a global microservice mesh where workloads across 4 regions obtain ephemeral TLS client certificates for mTLS. Each workload presents hardware-backed attestation, is issued a short-lived certificate with a SPIFFE ID, and must maintain cross-region trust, rapid revocation, and auditable logs. Include enrollment, attestation, rotation cadence, and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Apple","Lyft","NVIDIA"]},{"id":"q-2181","question":"Design a KCA architecture to issue post-quantum‑ready client certificates for 200 edge gateways across three regions. Gateways boot with hardware-backed roots and attestation, enroll via CSR or token, and receive short-lived certificates with a region policy extension. Include PQC agility, cross-region trust, renewal, revocation (OCSP/CRLs), auditing, and a 1-page test plan?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Amazon","Apple","Google"]},{"id":"q-2217","question":"In a multi-tenant edge compute fabric deployed across 3 regions with 1,000 nodes per region, design a KCA flow where each node boots with a hardware-backed root and attestation, enrolls via a token, and obtains a 120-second TLS client certificate binding to its tenant. Include enrollment, attestation checks, per-tenant policy binding to the data plane, cross-region trust, automatic renewal on reconnect, revocation via OCSP/CRLs, offline root fallback, auditing, and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Google","Tesla"]},{"id":"q-2250","question":"Design a KCA strategy where 5000 edge gateways boot with hardware-backed roots and perform attestation-enabled CSR enrollment to obtain short-lived mTLS certs, but without a centralized CA: use threshold signatures across regional HSM clusters to issue certs, ensuring cross-region trust, policy scoping per gateway, region-aware policy streaming, and robust revocation (gossip-CRLs/OCSP). Include enrollment, renewal, audit, offline scenarios, and a concrete 1-page test plan?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Hashicorp","Robinhood","Tesla"]},{"id":"q-2487","question":"Design a KCA workflow for 100k industrial IoT devices deployed across 5 regions, each with a hardware-backed root and device attestation. Devices enroll via a one-time token and CSR, obtain a 1-year client certificate with per-device scoping, and refresh every 6 hours. Include policy propagation through a streaming config service, cross-region trust, revocation via OCSP/CRLs, offline root fallback, auditing, and a concrete 1-page test plan. Ensure resilience against device compromise and root loss?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Discord","MongoDB","Uber"]},{"id":"q-2500","question":"Design a KCA workflow for 250 edge gateways across four regions that serve multiple tenants with strict data isolation. Gateways boot from hardware-backed roots and attestation-enabled CSR enrollment, then obtain short-lived client certificates whose policy extension enforces per-tenant audience restrictions for a service mesh and cross-tenant data boundaries. Include policy propagation, renewal, revocation (OCSP/CRLs), offline root fallback, auditing, and a 1-page test plan?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Airbnb","Anthropic","Netflix"]},{"id":"q-2640","question":"Design a scalable KCA workflow to issue and rotate mTLS client certificates for 2,500 microservices deployed across six regions, each host with hardware-backed roots and attestation (TPM/TEE). Include CSR enrollment with hardware attestation, region-aware policy propagation via a streaming config service, cross-region trust anchors, short-lived leaf certs (5 minutes), automated renewal, revocation via OCSP/CRLs, offline root fallback, auditing, and a concrete 1-page test plan. Also provide a policy example and a migration path from a legacy PKI?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Airbnb","Coinbase","Lyft"]},{"id":"q-2697","question":"Design a KCA onboarding flow for 50 ephemeral edge workers across a multi-cloud environment where each worker boots from a hardware-backed root of trust (TPM/HSM) and obtains a short-lived TLS client certificate (15 minutes) from a regional CA via attestation-based bootstrap. Include enrollment, attestation checks, automatic renewal on rehydration, revocation, auditing, and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Amazon","Robinhood"]},{"id":"q-2739","question":"Design a KCA workflow to issue and rotate 2,000 leaf client certificates for 400 microservices across 6 regions in a multi-cloud data platform. Each service uses hardware-backed roots and attestation for CSR enrollment. Encode per-service policies (datasets, quotas) as certificate extensions and propagate region-specific policies via a streaming config service with cross-region trust. Lifetimes: 2 minutes; include renewal, revocation (OCSP/CRLs), offline root fallback, auditing, and a concrete 1-page test plan?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Hashicorp","Snowflake","Twitter"]},{"id":"q-2860","question":"Design a KCA workflow for ephemeral CI runners in a multi-cloud setup. Runners launch on demand, boot with a hardware-backed root, enroll via attestation-enabled CSR, and request a 3-minute mTLS client certificate scoped to the build job. Include policy propagation via a streaming config service, per-job revocation, rapid rotation, auditing, and a concrete 1-page test plan?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["DoorDash","Hashicorp","Snowflake"]},{"id":"q-2909","question":"Design a KCA onboarding flow for 50k ephemeral GPU compute pods in a multi-cloud data fabric where each pod boots with a TPM-backed attestation, requests a 20-minute TLS client certificate from a regional CA, and must support rapid revocation, cross-cloud trust, PQC-capable algorithms, and audit trails; include enrollment, attestation checks, certificate rotation, offline root fallback, and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Coinbase","NVIDIA","OpenAI"]},{"id":"q-2947","question":"Design a KCA onboarding flow for a real-time financial analytics platform spanning two sovereign clouds with strict data residency. 2,000 risk-scoring microservices boot with TPM-backed attestation and request a short-lived TLS client certificate from region-specific CAs. Certificates carry a data-residency tag and instrument scope, expire in 6 minutes, and auto-renew on reconnect. Include enrollment, attestation checks, cross-region trust, revocation (OCSP/CRLs), offline root fallback, auditing, and a practical test plan?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Google","Scale Ai"]},{"id":"q-2994","question":"Design a beginner-friendly KCA workflow for a global CDN edge network with 300 locations that must rotate client certs with zero downtime. Propose a lightweight two-tier CA hierarchy (regional CA on edge devices and central Root CA in HSMs), 15-minute edge cert lifetimes, enrollment via token, and per-edge revocation handling during outages. Include a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Google","Hugging Face","Meta"]},{"id":"q-3232","question":"In a multi-tenant DBaaS running on a shared Kubernetes cluster, design a KCA onboarding flow where each node boots with a TPM-backed attestation, enrolls with its customer’s regional CA via a bootstrap token, and is issued a short-lived TLS client certificate scoped to the tenant SPIFFE ID and database namespace. Include per-tenant policy binding, cross-tenant revocation, audit logs, certificate rotation on reattachment, and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["DoorDash","Hashicorp","MongoDB"]},{"id":"q-3254","question":"Design a KCA workflow for a 1000-node serverless edge platform serving 200 tenants across 6 regions. Each edge node boots with TPM-attestation, enrolls to a regional CA, and obtains a short-lived TLS client certificate bound to tenant and function namespace. Include tenant-aware policy binding (refreshed via a central policy store), certificate rotation on tenant changes, cross-region trust, rapid revocation with OCSP/CRLs, offline root fallback, and auditability. Also provide a minimal test plan simulating tenant churn and regional failover?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Cloudflare","MongoDB","Uber"]},{"id":"q-3371","question":"Design a KCA workflow to provision ephemeral TLS client certificates for serverless functions on a public-cloud edge platform, scaling to millions of invocations daily. Each function boots with hardware-backed attestation, derives a per-runtime policy, and requests a short-lived client cert. Include policy derivation, attestation checks, TTL, renewal, revocation, auditing, and a 1-page test plan?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["MongoDB","Square","Twitter"]},{"id":"q-3426","question":"Design a beginner-friendly KCA workflow for a multi-region SaaS where tenants' apps run in separate Kubernetes namespaces and authenticate to regional edge gateways using short-lived client certificates. Include enrollment (token-based or CSR), per-tenant policy (allowed APIs, rate limits), policy propagation via streaming config with staggered renewals, and offline revocation via local CRLs when WAN is down. Provide a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Google","Lyft","Microsoft"]},{"id":"q-3461","question":"Design a KCA onboarding flow for a multi-tenant SaaS platform with a 4-region microservice mesh. Each customer runs isolated workloads that boot with TPM-attested identities and obtain short-lived TLS client certificates from regional CAs. Include per-tenant policy binding, cross-tenant revocation, auditing, certificate rotation on regional failover, and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Instacart","MongoDB","Square"]},{"id":"q-3550","question":"Design a beginner-friendly KCA workflow for internal service-to-service mTLS in a single-region microservices app. Use per-service certificates with a 10-minute TTL, enrollment via token-based CSR, and a policy extension encoding allowed endpoints and quotas. Propagate policy via a lightweight streaming channel from a central CA to edge/service proxies, with simple revocation. Include a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Airbnb","PayPal"]},{"id":"q-3640","question":"Design a KCA workflow to issue and rotate ephemeral mTLS client certificates for services inside a service mesh (Istio/Linkerd) across three cloud regions, with hardware-backed roots and per-service SPIFFE IDs. Include how to derive per-service policy from SPIFFE trust domains, policy propagation via a streaming config service, cross-mesh trust anchors, TTLs of 2 minutes, automated renewal, revocation via OCSP/CRLs, auditing, and offline root fallback. Provide a concrete 1-page test plan?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Meta","MongoDB"]},{"id":"q-3721","question":"Design a beginner-friendly KCA workflow for a ride-hailing platform where drivers' mobile apps obtain short-lived client certificates via token-based enrollment and present them to regional edge gateways for ride requests. Include per-driver policy (API scope, rate limits), region-aware cert rotation on GPS geofence changes, and revocation via OCSP stapling or local CRLs, plus an on-device cert store and audit logs. Provide a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Bloomberg","Coinbase","Lyft"]},{"id":"q-4038","question":"Design a KCA workflow for a multi-tenant edge video analytics platform across 6 urban centers where each edge node boots with a hardware-backed root and attestation-enabled bootstrap, then obtains a short-lived TLS client certificate (TTL 12h) from a central CA via KCA. Include enrollment, per-tenant policy binding via SPIFFE IDs, cross-center trust, automatic renewal on reconnection, revocation via OCSP/CRLs, offline root fallback, auditing, and a minimal test plan. Provide concrete components and data flows?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["DoorDash","Microsoft"]},{"id":"q-4076","question":"Design a beginner-friendly KCA workflow for an on-premises customer agent fleet that boots from a pre-provisioned bundle, obtains a short-lived client certificate from central KCA, binds it to a tenant via SPIFFE ID, and uses streaming policy updates. Include enrollment, renewal on reconnect, per-tenant policy binding, offline/online revocation, auditing, and a minimal test plan. Provide concrete components and data flows?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Adobe","Microsoft","Netflix"]},{"id":"q-4088","question":"Design a KCA workflow to issue and rotate ephemeral TLS client certificates for a fleet of hospital IoT medical devices that operate in patient rooms with intermittent connectivity. Include hardware-backed attestation (TPM/TEE), CSR enrollment, per-hospital policy binding via SPIFFE IDs, adaptive TTL (60s–5m) based on risk signals, offline root fallback, renewal on reconnect, revocation via OCSP/CRLs, auditing, and a concrete 1-page test plan?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Meta","MongoDB"]},{"id":"q-4128","question":"Design a KCA workflow to migrate a multi-tenant, multi-cloud TLS client certificate system to post-quantum cryptography without disrupting existing clients. Each tenant has a policy for PQC vs classical algorithms; implement dual-stack certificates during migration, algorithm negotiation in TLS handshakes, SPIFFE-based policy binding, cross-cloud trust anchors, renewal, revocation (OCSP/CRLs), auditing, and rollback mechanisms. Include concrete data flows and a 1-page test plan?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Goldman Sachs","Hugging Face","Meta"]},{"id":"q-4181","question":"Design a KCA workflow for a multi-tenant streaming analytics platform where tenant workloads in a shared data mesh obtain ephemeral TLS client certificates via hardware-attested bootstrap. Include TTL 1800s, automatic renewal on reconnect, per-tenant SPIFFE IDs, revocation via OCSP/CRLs, offline root fallback, auditing, and a minimal test plan. Provide concrete components and data flows?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Databricks","Tesla"]},{"id":"q-4287","question":"Design a KCA workflow for an edge AI inference fleet of Nvidia edge devices (ARM+TPM2.0) that boot with a hardware-backed key and obtain a short-lived client certificate from a central CA after device attestation. Include per-device policy enforcement for GPU memory isolation, health attestations to a device health service, TTL 900s, auto-renew on reconnect, revocation via OCSP/CRLs, offline root fallback, auditing, and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Adobe","NVIDIA"]},{"id":"q-4408","question":"Design a KCA workflow to issue ephemeral code-signing certificates for CI/CD runners across multiple cloud accounts. Each runner boots with a hardware-backed root and attestation, enrolls a CSR, and obtains a short-lived signing cert from a central CA. Bind signing policies to per-repo SPIFFE IDs, enable cross-account trust, renew on runner lifecycle events, revoke via OCSP/CRLs, support offline root fallback, provide auditing, and outline a concrete 1-page test plan?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Google","Plaid"]},{"id":"q-4479","question":"Design a KCA-based bootstrap and certificate lifecycle for ephemeral CI/CD runners provisioned on-demand across AWS, Azure, and GCP. Runners authenticate to a central CA using hardware-backed attestation to receive short-lived TLS client certificates (TTL 300s) for mTLS inside the CI/CD mesh. Include enrollment, attestation checks, renewal on re-provision, tenant isolation, revocation via OCSP/CRLs, offline root fallback, auditing, and a minimal test plan. Provide concrete components and data flows?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Coinbase","Hashicorp","Stripe"]},{"id":"q-4516","question":"Design a KCA workflow for a serverless data-pipeline where each invocation spins a fresh container that must authenticate to internal services via mTLS. The system uses regional KCAs to issue 60s client certificates per invocation; include attestation checks at startup, per-invocation scope via SPIFFE IDs, rapid revocation, offline root fallback, auditing, and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Bloomberg","Databricks","Uber"]},{"id":"q-4563","question":"Design a beginner-friendly KCA workflow for a multi-tenant mobile app backend where tenants may bring their own root CAs. Each device authenticates by obtaining a short-lived client certificate from a central KCA, bound to a per-tenant SPIFFE-like ID, with attestation (TPM/TEE). Describe enrollment, policy binding, certificate rotation on reconnect, revocation strategy (OCSP/CRLs), auditing, and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["LinkedIn","Meta","Tesla"]},{"id":"q-4765","question":"Design a KCA bootstrap for a city-wide fleet of autonomous delivery vehicles. Each vehicle boots with a hardware-backed root, attests to a regional bootstrap service, and enrolls a CSR to a central KCA that issues a short-lived mTLS client certificate (TTL 900s). Include per-vehicle SPIFFE IDs, cross-region trust anchors, auto-renewal on re-provision, revocation via OCSP/CRLs, offline root fallback, auditing, and a concise test plan with data flows?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Plaid","Scale Ai","Uber"]},{"id":"q-844","question":"You have a CSV log with columns: user_id, event_timestamp (ISO 8601), and event_type. Write a Python function using pandas to compute the number of unique active users per day for a given timezone, returning a dict mapping 'YYYY-MM-DD' to count. Explain how you handle timezone normalization and missing data. Provide sample usage?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Coinbase","Databricks"]},{"id":"q-884","question":"You're operating a Kafka-to-Spark streaming job in production and observe sporadic latency spikes; detail a concrete diagnostic plan to identify bottlenecks and a remediation strategy, including metrics, tooling (OpenTelemetry, Prometheus, Grafana), and validation steps?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Anthropic","DoorDash","Google"]},{"id":"q-889","question":"**How would you design a scalable KCA integration** for a multi-tenant SaaS using short-lived client certificates? Central CA in an HSM issues per-tenant CSRs, rotates certificates every 24h, and supports revocation via OCSP stapling and short CRLs with edge caching. Include audit trails, MFA for CA access, and clear renewal, compromise, and revocation workflows?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Cloudflare","Microsoft","Plaid"]},{"id":"q-952","question":"How would you design an on-demand KCA for service-to-service mTLS in a multi-tenant data platform (Stripe/Databricks) where a Kubernetes-hosted CA issues per-service CSRs, each cert valid for 5–15 minutes, supports cross-region trust, anomaly-driven revocation via telemetry, and full audit trails; include MFA protection and an offline root fallback?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Databricks","Stripe"]},{"id":"q-997","question":"Design a beginner-friendly KCA flow for a SaaS API gateway where a cloud CA issues per-tenant, short-lived client certificates (4 hours) for app authentication. Outline provisioning (CSR-based enrollment or token-based enrollment), automatic renewal, revocation strategy, and how the gateway validates certs and records audit logs. Include failure modes and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Bloomberg","Netflix","OpenAI"]},{"id":"q-1138","question":"You are building a real-time KCNA feed service used by Snap, Meta, and Discord-scale clients to publish and deliver announcements across regions with sub-100ms tail latency. Describe the end-to-end architecture, data model for Announcement, ingestion and delivery pipeline, guarantees (at-least-once vs exactly-once), ordering, deduplication, failover, tests, and observability. How would you scale to 10k updates/sec with 99.999% uptime?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Discord","Meta","Snap"]},{"id":"q-1295","question":"**KCNA Consumer Backpressure & Gap Handling**: In a beginner-friendly KCNA consumer, design a pull-based ingestion path that preserves per-topic offsets, guarantees at-least-once processing, and recovers from transient network slowdowns without duplicating messages. Describe the API shape, offset persistence, retry/backoff strategy, and a minimal test plan including a canary scenario?","channel":"kcna","subChannel":"general","difficulty":"beginner","tags":["kcna"],"companies":["Discord","Plaid","Two Sigma"]},{"id":"q-1431","question":"KCNA cross-region, multi-tenant QoS: Propose an architecture and API for declaring per-tenant Topic SLAs (retention, max throughput) and a cross-region offset store with region-local commit logs. How would you implement per-tenant backpressure, quota enforcement, and exactly-once vs at-least-once semantics under regional outages? Include a concrete canary and testing plan?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Oracle","Snowflake"]},{"id":"q-1471","question":"KCNA multi-tenant schema-evolution: design a zero-downtime migration for a KCNA-based event bus used by many tenants where the Event schema evolves from v1 to v2 (add region field, deprecate payload wrapper). How do you enforce backward/forward compatibility, isolate tenants during migration, and validate with canaries? Include tooling, rollback plans, and observability?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["DoorDash","Stripe","Twitter"]},{"id":"q-1503","question":"KCNA cross-region tenancy isolation: design a replication topology where tenants' streams stay regional unless opted into global analytics; implement per-tenant topic partitioning, region-aware routing, and idempotent retries with de-dup. How do you enforce locality, protect privacy in cross-region analytics, and handle failover/lag? Include concrete configs, testing strategy, and rollback plan?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Amazon","NVIDIA"]},{"id":"q-1520","question":"KCNA TTL Retention: design a per-topic TTL policy for KCNA events. How would you store TTL metadata, drop expired events without breaking consumer offsets, handle late-arriving events post-expiry, and observe/verify with canary tests? Provide a minimal API shape for setting TTL per topic, a compact storage layout, and a lightweight cleanup workflow?","channel":"kcna","subChannel":"general","difficulty":"beginner","tags":["kcna"],"companies":["Anthropic","Citadel","Tesla"]},{"id":"q-1602","question":"KCNA key management & tenant isolation: In a **KCNA**-based multi-tenant event bus, each tenant uses a per-tenant envelope encryption key managed by a centralized **KMS**. Design a **zero-downtime** key rotation workflow that rotates tenant keys without breaking consumption, re-encrypts in-flight payloads, and prevents cross-tenant leakage. Include data model (**tenant_id**, key_version, wrapped_key), rollout strategy, rollback, and observability?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Microsoft","Tesla"]},{"id":"q-1698","question":"Design a KCNA privacy-first feed where tenants specify a region (US/EU/APAC) and all data remains local. Use per-tenant envelope encryption with KMS, region-scoped brokers, field-level redaction before delivery, and tenant-aware access controls. Add audit trails and canary tests to prove zero cross-tenant leakage, correct redaction, and retention under burst load?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Anthropic","Tesla","Zoom"]},{"id":"q-1723","question":"KCNA dynamic tenancy fairness under bursty workloads: design a per-tenant ingestion path with token-bucket quotas and fair queuing; detail API surface, quota persistence, backpressure signaling, and dynamic rebalancing from telemetry. How do you validate isolation under burst traffic, and what would your canary rollout look like?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Databricks","NVIDIA"]},{"id":"q-1806","question":"KCNA privacy-by-design: design a tenant-isolated KCNA ingestion and delivery path that enforces per-tenant encryption keys for in-flight and at-rest data, supports on-the-fly key rotation with zero-downtime, and provides auditable access controls for analytic consumers. Describe the KMS integration, key-wrapping strategy, performance impact, and a minimal canary test for rotation?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Adobe","Robinhood","Slack"]},{"id":"q-1901","question":"KCNA Time-Travel Replay for Tenants: Suppose a tenant needs to audit a precise 2-hour window of events without halting live ingestion. Design a tenant-scoped time-travel replay feature in KCNA that allows replaying events from a given timestamp while live streams continue, guarantees exactly-once delivery to downstream analytics, and preserves per-tenant offsets. Describe API shapes, storage layout, consistency guarantees, security controls, and a minimal test plan (canaries)?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Anthropic","Meta","Snowflake"]},{"id":"q-1926","question":"Design a beginner-friendly KCNA feature: tenant-scoped data TTL. Each tenant can configure a TTL (e.g., 24h) for their streams. Describe the API (setTTL on a topic with ttlMs), how per-message metadata is stored, how a background purge runs safely without breaking consumers, and a minimal test plan with canaries?","channel":"kcna","subChannel":"general","difficulty":"beginner","tags":["kcna"],"companies":["LinkedIn","NVIDIA"]},{"id":"q-2025","question":"KCNA ingest fairness at scale: design a per-tenant fair-queuing strategy for bursty producers in a multi-tenant KCNA channel. Implement a two-layer approach with a per-tenant token-bucket at the gateway and a global weighted-round-robin scheduler across tenants to prevent starvation. Define quotas, bounded bursts, and backpressure signaling; outline API contracts, config knobs, and a test plan with synthetic tenants and canaries?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Databricks","Hugging Face","IBM"]},{"id":"q-2101","question":"KCNA policy-driven tenant isolation: design a per-tenant access-control mechanism at the gateway and stream processors that enforces tenant-scoped authorization for ingest and delivery, supports dynamic policy updates with zero-downtime rollout, and provides an auditable per-event trail. Outline the policy model (tenants, roles, actions), integration with a policy engine (e.g., OPA/ABAC), JWT-based identity, and testing strategy including canaries and rollback?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Instacart","NVIDIA","Stripe"]},{"id":"q-2135","question":"KCNA Dead-Letter & Poison Message Handling: For a multi-tenant KCNA deployment, design a per-tenant dead-letter queue strategy that routes malformed events out of the normal path, enforces per-tenant retry budgets, and preserves idempotent replays. Describe DLQ schema, routing rules, retention, operator visibility, and a safe rollout with canaries?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Citadel","Instacart","Meta"]},{"id":"q-2165","question":"KCNA observability & tenant-scoped tracing: design end-to-end observability for a multi-tenant KCNA event bus under burst traffic, preserving tenant data isolation while enabling operations debugging. Specify: how to propagate tenant_id and correlation_id across producer, gateway, and consumer; per-tenant metrics and alerting; privacy-preserving trace UI that exposes only metadata; retention, RBAC, and failure-mode testing with canaries?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Bloomberg","Goldman Sachs","Netflix"]},{"id":"q-2265","question":"Design a KCNA-based privacy-preserving cross-tenant analytics pipeline where tenants publish raw events but analysts receive aggregated metrics only. Tenants can opt into regional sharing with differential privacy, per-tenant encryption keys, and auditable data lineage. Describe topology, data model, access control, latency targets, and a canary rollout plan?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Meta","Netflix","Snap"]},{"id":"q-2311","question":"KCNA Dead-Letter Queue (DLQ): For a beginner-friendly KCNA, design a per-tenant DLQ strategy for messages that fail processing after N retries. Describe API shape to move from main topic to DLQ, retention, how to reprocess, and a simple test canary that demonstrates DLQ routing, backoff between retries, and alerting?","channel":"kcna","subChannel":"general","difficulty":"beginner","tags":["kcna"],"companies":["Meta","Snap"]},{"id":"q-2425","question":"KCNA per-tenant feature flags: design a control plane to selectively enable a new routing path and compression for KCNA streams, with zero-downtime rollout, tenant-scoped rollback, and audit logs; how would you model toggles, propagate config, implement canaries, and validate impact before full enablement?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Lyft","Uber"]},{"id":"q-2464","question":"KCNA retention governance: design a per-tenant data lifecycle in KCNA that enforces tenant-specific retention windows, supports legal holds, and runs background tombstone-based deletions with GC across partitions. Explain metadata storage, purge triggers without breaking at-least-once semantics, observability, and a rollback/hold-release workflow?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Google","Microsoft"]},{"id":"q-2471","question":"KCNA security and governance: design a per-tenant envelope encryption scheme for KCNA payloads using a central KMS. Each tenant has a dedicated DEK wrapped by a tenant-specific KEK. Encrypt payloads at produce time and decrypt only at authorized consumers with per-tenant IAM. Support per-tenant key rotation with zero-downtime re-encryption, and maintain per-tenant audit trails and deletion rules that respect retention. How would you implement lifecycle, performance trade-offs, and backward-compatibility?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Amazon","Goldman Sachs","LinkedIn"]},{"id":"q-2510","question":"KCNA tamper-evident audit trails: design per-tenant verifiable logs for event streams using append-only shards and Merkle proofs, with per-batch signing and external audit proofs. Outline data structures, key rotation, canary rollout, rollback, and a test plan that proves tamper-resistance without leaking tenant data?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Anthropic","Cloudflare","Snap"]},{"id":"q-2565","question":"KCNA per-tenant data isolation with confidential payloads: in a single KCNA cluster serving multiple tenants, design a mechanism to store tenant-scoped payloads with per-tenant encryption keys, support cross-tenant analytics only if opted-in, ensure query isolation, key rotation, and audit trails. Describe API contracts, key management, and performance implications?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Coinbase","Microsoft","Plaid"]},{"id":"q-2582","question":"In KCNA, design a per-tenant event-time processing layer that tolerates late events within a tenant-defined latency budget, computes per-tenant 5-minute tumbling window aggregations, and guarantees at-least-once delivery. Describe per-tenant watermarks, state partitioning, late-data handling, fault tolerance, and a test plan to validate SLA compliance?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["DoorDash","Goldman Sachs","Meta"]},{"id":"q-2621","question":"In KCNA, design a per-tenant field-level access control and masking layer that operates in real time on streaming payloads for analytics, ensuring authorized tenants can decrypt unmasked fields while unauthorized tenants only see masked data, without breaking at-least-once semantics or replay safety. Explain data structures, policy evaluation, KMS key management, and testing with canaries?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Amazon","Apple"]},{"id":"q-2710","question":"KCNA per-tenant envelope encryption: design a CMK-backed scheme where each tenant's messages are encrypted at ingest with a tenant KEK wrapped by a KMS CMK, keys rotate monthly, and revocation triggers re-encryption with minimal downtime while preserving canary readers. Describe data model, API surface, rotation and revocation flows, and test strategy?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Adobe","Google"]},{"id":"q-2771","question":"KCNA privacy-preserving analytics: design a per-tenant analytics pipeline where tenants opt into server-side aggregation over their event streams without exposing raw data. Include architecture for data isolation, privacy budgets via differential privacy, a streaming topology (topics, shards, processors), and how you validate tamper-resistance while preserving privacy. Provide testing, rollback, and performance targets?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Discord","PayPal","Zoom"]},{"id":"q-2989","question":"KCNA cost-aware metering at scale: design a tenant-scoped metering system that bills on ingress, egress, and partition-hours, with per-tenant soft quotas, burst credits, and an offline audit log. Describe data models, sampling windows, aggregation, reconciliation, and how to enforce limits without dropping messages. Include API contracts, observability, and rollback/dispute handling; provide a high-level migration plan?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Apple","Google","Lyft"]},{"id":"q-3047","question":"KCNA Audit Trail: Build a per-tenant, append-only audit log for KCNA actions (topic creation, partition changes, offset commits) that is tamper-evident and queryable without impacting throughput. Describe storage format, API surface, retention, and a minimal canary test that demonstrates append, read, and integrity checks?","channel":"kcna","subChannel":"general","difficulty":"beginner","tags":["kcna"],"companies":["Citadel","Cloudflare","Databricks"]},{"id":"q-3131","question":"KCNA Security: Per-tenant encryption at rest using envelope encryption in KCNA. Each tenant has a unique data encryption key (DEK) wrapped by a centralized master key; rotate DEKs monthly and re-encrypt in-flight messages during rotation? Describe key storage, access controls, rotation workflow, and a small canary test to validate encryption at rest and post-rotation decryption?","channel":"kcna","subChannel":"general","difficulty":"beginner","tags":["kcna"],"companies":["Apple","Oracle"]},{"id":"q-3252","question":"KCNA per-tenant real-time anomaly detection: design a streaming pipeline that flags tenants with anomalous event-rate patterns at ingest time, ensuring isolation, data locality, and minimal false positives. Outline architecture, per-tenant state, windowing strategy, privacy considerations, rollout plan, and testing?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Meta","Microsoft"]},{"id":"q-3300","question":"KCNA tenancy revocation and data deletion: design a scalable policy to revoke a tenant's access within 2 minutes, gracefully quiesce in-flight events, switch downstream analytics to de-identified data, and preserve historical auditability. Outline data flow, API contracts, and test strategy, including canaries?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Google","Hashicorp","MongoDB"]},{"id":"q-3323","question":"Design a per-tenant deduplication system for KCNA that guarantees exactly-once-like semantics for idempotent event processing across tenants, without cross-tenant data leakage. Propose event_id schema, fast per-tenant dedupe checks, eviction policy, tombstones, and failure handling; discuss producer/consumer changes and a practical test plan that demonstrates correctness and scalability?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Meta","PayPal","Twitter"]},{"id":"q-3350","question":"KCNA privacy-preserving analytics: design a server-side cross-tenant aggregation layer that lets dashboards derive insights from multi-tenant event streams without exposing raw data. Specify a differential privacy or secure-aggregation protocol, per-tenant access controls, data formats, auditable results, and a minimal canary test plan to verify privacy and accuracy under load?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Cloudflare","Hashicorp","Hugging Face"]},{"id":"q-3419","question":"KCNA per-tenant data purge: design a per-tenant purge operation that deletes messages older than a given timestamp while preserving per-tenant offset semantics and avoiding consumer surprises. Describe API shape, tombstone strategy, offset handling, GC durability, and a minimal canary test that demonstrates purge, offset stability, and alerting?","channel":"kcna","subChannel":"general","difficulty":"beginner","tags":["kcna"],"companies":["DoorDash","MongoDB"]},{"id":"q-3511","question":"In KCNA, tenants share a cluster. Propose a per-tenant envelope encryption scheme for streams where each tenant's messages are encrypted at rest with a tenant-specific key managed by a central KMS. Describe key rotation, revocation, legacy data handling, latency impact, and a concrete test plan. How would you implement and verify it?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Apple","MongoDB"]},{"id":"q-3540","question":"KCNA cross-region replication: design a protocol that preserves per-tenant data sovereignty across two regions, ensuring at-least-once delivery and per-tenant ordering during failover. Describe region-local shards, per-tenant keys for encryption, key rotation, and tombstone GC, plus a DR test plan with progressive canaries and rollback. How would you ensure correctness and observability?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Bloomberg","Cloudflare","Microsoft"]},{"id":"q-3621","question":"KCNA Cross-Region Replication: In a globally deployed KCNA, design cross-region replication that guarantees per-tenant event order and cross-region consistency, while producing end-to-end verifiable batch proofs across regions. Include shard ownership, canaries, key rotation, rollback, and a DR test plan?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Apple","Snap","Square"]},{"id":"q-3670","question":"KCNA: Design a tenant-scoped encryption model for KCNA events where each tenant's data is encrypted at rest with a per-tenant KEK sourced from a KMS, and payloads are envelope-encrypted with per-tenant DEKs that are rotated without downtime. Explain provisioning, rotation, revocation, audit proofs, and isolation guarantees under load. Include performance trade-offs and a concrete failure scenario?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Anthropic","Netflix","Tesla"]},{"id":"q-3691","question":"KCNA Security: Design a tenant-scoped envelope-encryption framework for KCNA where each event payload is encrypted with a per-tenant data key (DEK) wrapped by a centralized KMS/HSM, enabling hot key rotation, forward secrecy, and revocation. Outline the key hierarchy, data structures, per-tenant access checks, audit proofs, and a rollout and rollback strategy with performance considerations?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Citadel","Microsoft","MongoDB"]},{"id":"q-3758","question":"KCNA per-tenant encryption & key management: design a CMEK-backed model for KCNA where each tenant's event payload is encrypted at ingestion with per-tenant keys managed by an external KMS. How would you implement key rotation without downtime, revocation, and auditable proofs (Merkle-based) without exposing payloads? Include data structures mapping tenants to keys, API surfaces, and a practical test plan?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Meta","Snowflake"]},{"id":"q-3790","question":"KCNA exactly-once semantics at scale: design a per-tenant streaming pipeline in KCNA that guarantees exactly-once delivery from producer to consumer despite retries and partition rebalances. Propose a per-tenant commit-log with transactional writes, a monotonic sequence number, and a bounded dedup cache. How do you handle failure modes, rebalances, and tombstone processing? Include API contracts and a testing plan with canaries?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Cloudflare","Hashicorp"]},{"id":"q-3861","question":"KCNA policy-driven field-level de-identification: design an ingestion-time pipeline that applies per-tenant de-identification rules (tokenize or redact PII fields) while preserving deterministic tokens for analytics, and supports on-demand re-identification with strict authorization. Explain policy model, evaluation order, key management, performance bounds, and a test plan for leak-avoidance and auditability?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Amazon","Citadel","Tesla"]},{"id":"q-3965","question":"You're building a beginner KCNA-based event bus for a multi-tenant chat/telemetry platform used by Slack-like teams. Each tenant has isolated topics. Design a per-tenant, rate-limited KCNA consumer that preserves per-tenant offsets, guarantees at-least-once delivery, and does not block healthy tenants when a downstream service lags. Describe API surface, offset persistence, fault handling, and a minimal test harness to simulate a slow downstream while other tenants continue?","channel":"kcna","subChannel":"general","difficulty":"beginner","tags":["kcna"],"companies":["Hashicorp","Slack"]},{"id":"q-4080","question":"In KCNA, design a privacy-preserving analytics layer that lets tenants run cross-tenant aggregates (e.g., events per hour) without enabling reconstruction of any single tenant's data, using differential privacy and optional zero-knowledge proofs; specify the data-plane, privacy budgets, DP noise scaling, auditability, and how you'd validate resilience against collusion and timing attacks?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Google","Scale Ai","Square"]},{"id":"q-4122","question":"KCNA: Verifiable cross-tenant analytics API: design a per-tenant aggregation endpoint (e.g., total events, average payload size) that returns results with a cryptographic proof showing only data from the requesting tenant was included and no double-counting occurred. Outline data structures, proof generation/verification, key rotation, and a regression test plan with security properties?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["MongoDB","Robinhood","Snowflake"]},{"id":"q-4339","question":"KCNA Data Locality & Regional Pinning: In a multi-tenant KCNA deployment, design a policy to pin each tenant's event stream to a specific region to minimize latency, while allowing automatic re-pinning during regional outages. Detail data structures, decision criteria, failover workflow, and testing strategy for correctness under burst load?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Apple","Discord","Stripe"]},{"id":"q-4371","question":"KCNA Burst Guard: For a beginner KCNA producer scenario, design a per-topic token-bucket rate limiter that caps publish to 100 messages/sec with a burst capacity of 200, preserving per-topic order, and deferring excess messages. Describe the API surface, token accounting, backpressure strategy, and a minimal test plan simulating a 2x burst?","channel":"kcna","subChannel":"general","difficulty":"beginner","tags":["kcna"],"companies":["IBM","LinkedIn","Snowflake"]},{"id":"q-4422","question":"KCNA policy-driven access: design a per-tenant data-access policy system that enforces data locality, minimizes cross-tenant leakage during ingestion, storage, and query. Provide a concrete design for label-based access (RBAC/ABAC), policy evaluation points (ingest, index, query planner), and auditability with tamper-evident logs. Include API surfaces, data structures, and a testing plan with rollback and canary deployment?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Discord","DoorDash","Snowflake"]},{"id":"q-4550","question":"KCNA security and access control: design a per-tenant ACL model for KCNA where tenants define topic-level read/write permissions, with short-lived credentials issued by an authorization service. Explain how to enforce at gateway and broker, handle key rotation, token revocation, and audit logging, plus a test plan simulating misissued credentials and revocation delays?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Instacart","Netflix"]},{"id":"q-4610","question":"KCNA: Design a per-tenant real-time aggregation layer that joins events across multiple KCNA namespaces into a tenant-scoped view, delivering low-latency dashboards while guaranteeing isolation and at-least-once delivery. Describe dataflow, partitioning, fault tolerance, security (ACLs, encryption), and a test plan with canaries?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Google","Meta","Netflix"]},{"id":"q-4717","question":"For a multi-tenant KCNA-based event bus used by teams at Citadel and Lyft, design a tenant-scoped observability layer that collects metrics, traces, and logs per tenant while preventing data leakage; include per-tenant sampling, redaction rules, and privacy controls. Explain data models, storage, dashboards, and canary-validation plan?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Citadel","Lyft"]},{"id":"q-4744","question":"KCNA access control and data isolation: design a per-tenant access policy for a shared KCNA cluster. Propose a token-based scheme with short-lived tokens containing tenant_id and per-topic permissions; enforce via an authorization service that validates signatures against a rotating KMS keyset; implement per-tenant revocation and audit logs; describe how you test isolation under bursty traffic and simulated token compromise. Include API signatures, config knobs, and a concrete test plan?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Instacart","Robinhood","Snowflake"]},{"id":"q-4814","question":"KCNA cross-tenant exactly-once delivery: design an end-to-end exactly-once mode per tenant across producers, topics, and rebalance events in a multi-tenant KCNA cluster. Propose a dedup scheme (tenant_id + message_id), transactional offsets to downstream stores, and per-tenant transaction boundaries. How would you test robustness under producer failures, rebalance, and tenant migrations?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Plaid","Robinhood","Snowflake"]},{"id":"q-941","question":"Scenario: A global chat platform with 2B MAUs must detect policy-violating content (spam, hate speech) in near real-time while preserving user privacy and multilingual support. Propose an end-to-end pipeline: ingestion, moderation models (rules + ML), latency SLOs (1-2s), privacy safeguards, backpressure handling, retries, and dead-letter queues. Compare on-device vs cloud inference and monitoring?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Slack","Twitter"]},{"id":"q-1021","question":"Design a globally distributed feature-flag engine for per-user rollout. Provide data model, consistency guarantees, and deployment strategy for scale across multiple regions. Include choice of storage (DynamoDB vs Redis), how regionOverrides and segments are merged, and hot-flip safety. Provide evaluation protocol and a concise pseudocode snippet for evaluateFeature(user, flag, context)?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Coinbase","Meta","Tesla"]},{"id":"q-1042","question":"Design a high-throughput, fault-tolerant event ingestion pipeline for a fraud-detection service. It must guarantee exactly-once processing, deduplicate events across shards, support backpressure, and provide end-to-end observability. Explain the data model for dedup IDs, queue choice (Kafka vs Kinesis), idempotent sinks, and how you'd test failure scenarios?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Meta","Plaid","Twitter"]},{"id":"q-1067","question":"In a Zoom-like multi-tenant Kubernetes cluster, you discover a pod running as root with a hostPath mounted. What concrete remediation plan would you implement using OPA Gatekeeper, Pod Security Standards, RBAC, NetworkPolicies, and image scanning? Include how you would verify tenant isolation with a minimal test that should fail if misconfigured?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Scale Ai","Zoom"]},{"id":"q-1096","question":"In a simple analytics microservice, implement a Python function that reads a CSV file with headers user_id, action, timestamp (timestamp optional) and returns a dict mapping each user_id to the list of unique actions performed in chronological order; if timestamp is missing, preserve input order. Explain your approach and trade-offs?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["OpenAI","Oracle","Twitter"]},{"id":"q-1129","question":"Given a Kubernetes-deployed event-processing service that suddenly drops throughput from 10k to 6k after a feature release, outline a concrete, testable plan to diagnose and restore throughput. Include: **metrics** to monitor, **bottlenecks** to consider, and concrete, incremental changes like **rate limiting**, **backpressure**, and **idempotency**. Provide a safe rollback strategy and canary plan?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Google","Scale Ai","Twitter"]},{"id":"q-1184","question":"You maintain a public API for ride estimates and expect bursts. Implement a per-API-key rate limiter that allows 60 requests per minute using an in-memory structure. Provide the core logic in JavaScript (Node.js) and briefly justify your data structure, test approach, and how you’d handle restart scenarios?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Discord","Lyft","Slack"]},{"id":"q-1227","question":"In a tiny model-usage service, write a Python function that validates a JSON payload with user_id, model_id, action, and timestamp, then inserts a document into MongoDB with a created_at field. Ensure an index exists on (model_id, timestamp) and handle duplicate submissions gracefully (idempotent using a unique composite index on (user_id, model_id, timestamp)). Include basic error handling and a minimal test snippet?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Apple","Hugging Face","MongoDB"]},{"id":"q-1246","question":"Design a policy for a shared Kubernetes cluster serving multiple teams, ensuring namespace isolation, least privilege RBAC, image provenance, Pod Security Standards, and auditable policies. Propose concrete constraints using RBAC, PSP/PSA, NetworkPolicy, and OPA Gatekeeper; include a sample ConstraintTemplate and a test that rejects nonconforming pods. Explain trade-offs and monitoring strategy?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Cloudflare","Google"]},{"id":"q-1298","question":"Design a globally-distributed rate limiter for a high-traffic service with per-tenant limits and cross-region fairness. Compare token-bucket and sliding-window approaches, specify data structures and caching, describe hot-path optimization, failure modes, and testing strategy. Assume latency budgets and real-time enforcement?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Amazon","LinkedIn","Meta"]},{"id":"q-1351","question":"In a Node.js Express API backed by PostgreSQL, POST /search accepts { term } and currently builds a SQL with string concatenation. Describe a secure rewrite using parameterized queries and input validation, including error handling and ensuring the DB user has minimal privileges. How would you structure this to prevent SQL injection?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Adobe","Salesforce","Tesla"]},{"id":"q-1680","question":"In a Kubernetes-deployed microservice that ingests up to 200 events/sec from a queue, each event has event_id and payload. The handler must be idempotent so a duplicate delivery does not write to Postgres. Propose a concrete Redis-based dedup strategy (SETNX with EXPIRE TTL) and outline how you'd implement the dedup path in code, including how you'd handle retries, restarts, and cleanup?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Instacart","NVIDIA","Netflix"]},{"id":"q-1788","question":"You're designing a payment authorization path for a fintech platform (think Plaid) where a mobile tap triggers multiple services: Auth, Fraud/Risk, Ledger, and Notification. How would you ensure idempotent processing, at-least-once retries, and eventual consistency across services? Describe data models, the flow (outbox or saga), and fault tolerance (backoffs, DLQ)?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Lyft","Plaid"]},{"id":"q-1828","question":"You operate a polyglot data stack with MongoDB and Oracle. A global e-commerce app experiences 300–500ms latency on order placement during peak hours, despite modest CPU usage. Propose an end-to-end plan to diagnose and fix, covering data model, indexing, shard/cluster topology, connection pooling, caching, and cross-database consistency. Include concrete knobs you would adjust and how you'd validate impact?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["MongoDB","Oracle"]},{"id":"q-1877","question":"In a multi-tenant data pipeline on Kubernetes serving a PayPal-like payments gateway, events flow from a single Kafka topic to a Spark streaming job that writes to a data lake. How would you enforce per-tenant data isolation, encryption, auditing, and masking while maintaining a 95th percentile latency under peak load?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Databricks","Google","PayPal"]},{"id":"q-1950","question":"You’re asked to implement a simple in-memory rate limiter for a Node.js/Express API: cap each API key at 100 requests per 10 minutes. Provide a minimal in-process solution using a per-key timestamp array, how you prune old entries, and how you handle bursts. Explain trade-offs and a plan to scale to multiple processes?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["DoorDash","Instacart","Meta"]},{"id":"q-2008","question":"In a cloud-native data platform delivering streaming telemetry to a data lake, how would you implement end-to-end data quality and schema drift control for Kafka → Flink → Parquet, when tenants frequently emit extra fields? Describe schema versioning, compatibility, drift detection, and automated remediation to downstream dashboards, with concrete knobs and testing steps?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Databricks","Google"]},{"id":"q-2085","question":"How would you implement compliant per-tenant data erasure in a streaming analytics pipeline that ingests tenant events from Kafka, writes to a data lake as Parquet, and serves BI queries, ensuring immutable data, auditability, and zero-downtime erasure while preserving peak-load latency? Include data-modeling, catalog updates, and operational steps?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Anthropic","Robinhood","Square"]},{"id":"q-2119","question":"In a frontend data pipeline, you receive two sorted arrays of strings representing tags. Write a function mergeUnique(a,b) that returns a new array with all unique elements in sorted order, without mutating inputs. Assume inputs are sorted. Provide a minimal, robust JS implementation and explain its time/space complexity?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Discord","Instacart"]},{"id":"q-2193","question":"In a real-time analytics pipeline processing user activity events from a mobile app, you must support dynamic event schemas, provide backward compatibility, and enforce per-user data retention, while keeping end-to-end latency under 300 ms at 99th percentile. Describe the architecture, data model, schema evolution strategy, validation in-flight, and how you would monitor and test latency?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Hugging Face","MongoDB","Uber"]},{"id":"q-2251","question":"In a Kubernetes-based microservices stack processing per-tenant user events, design a per-tenant dynamic tracing sampling policy to keep OTLP ingestion overhead under 5% while preserving 95th percentile latency visibility for the most latency-sensitive tenants. Discuss config storage, crash-safe fallback, and validation under peak load?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Hugging Face","Plaid","Zoom"]},{"id":"q-2368","question":"You’re building a real-time fraud-detection scoring service for a payments platform that must process 100k transactions per second with sub-20ms latency and strict privacy constraints. Describe an architecture to ingest events, compute per-transaction risk scores, handle backpressure, ensure idempotent processing, and observe the system. Include data model, latency budget, and fault-tolerance trade-offs?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Instacart","OpenAI","PayPal"]},{"id":"q-2408","question":"In a 3-namespace Kubernetes cluster (dev, stage, prod), design a least-privilege model: per-namespace Role/RoleBinding (no ClusterRole bindings), enforce Pod Security Standards:restricted via OPA Gatekeeper, and apply default-deny NetworkPolicies with explicit allows. Explain testing with kubectl can-i and provide minimal YAML samples for Role, RoleBinding, NetworkPolicy, and a Gatekeeper constraint template?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["IBM","LinkedIn"]},{"id":"q-2462","question":"Scenario: You’re maintaining a Kubernetes cluster used by Plaid and Oracle developers. Provide a minimal RBAC setup to grant a ServiceAccount named 'dev-read' in namespace 'dev' read-only access to pods in that namespace. Include the ServiceAccount, a Role with get, list, watch on pods, and a RoleBinding. Explain how you would test it and why this is least privilege?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Oracle","Plaid"]},{"id":"q-2518","question":"Design a zero-trust, multi-region deployment for a real-time chat service on Kubernetes using Istio. The system must support mTLS, service-to-service authentication, PII audit logging, and RBAC for on-call engineers. Describe namespace layout, policy enforcement, DR strategy, and how you’d validate SLOs during a regional outage?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Square","Zoom"]},{"id":"q-2612","question":"In a Kubernetes cluster, a microservice named 'orders' communicates with a MongoDB instance. Describe a beginner-friendly RBAC and Secret strategy to grant only this pod access to MongoDB credentials stored in a Kubernetes Secret. Include minimal YAML references (namespace, ServiceAccount, Role/RoleBinding, Secret mount), explain rotation, and how you would verify a pod can connect without leaking credentials?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Discord","MongoDB","NVIDIA"]},{"id":"q-2674","question":"In a **KCSA** beginner scenario, a namespace hosts a small web app on **Kubernetes**. Under load, pods restart and 5xx errors appear. Describe a concrete, step‑by‑step debugging plan using real commands (kubectl, logs, metrics) and show how you would fix resource limits, readiness/liveness probes, and a minimal **NetworkPolicy** to improve security?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Cloudflare","LinkedIn","NVIDIA"]},{"id":"q-2766","question":"In a kcsa-beginner namespace, a small web app reads its Postgres credentials from a Kubernetes Secret mounted as environment variables. After a secret rotation, pods crash with authentication errors. Describe a concrete, step-by-step plan to diagnose and fix, including kubectl commands to inspect secrets, decode values, confirm they are mounted, and how to roll the deployment without downtime; also show minimal secret rotation best practices?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Apple","Uber"]},{"id":"q-2897","question":"In a kcsa-prod namespace hosting a microservices app with Istio mTLS, a CA rotation disrupted service-to-service trust, causing cascading 5xx errors. Provide a concrete, step-by-step diagnostic plan with exact kubectl/istioctl commands to verify cert validity, trust anchors, and sidecar config; outline a zero-downtime rollout with canaries and a rollback path; include future rotation guidance?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["DoorDash","Microsoft","Snap"]},{"id":"q-3074","question":"In a kcsa-prod namespace hosting a GPU-accelerated ML inference service on Kubernetes with Nvidia device plugins and Istio mTLS, a recent config change triggers sporadic 5xx errors during model warmup under peak load. Provide a concrete, step-by-step diagnostic plan with exact kubectl/istioctl commands to verify GPU provisioning (nvidia-smi on nodes), device-plugin DaemonSet health, CSI devices, and Istio sidecar behavior; outline a zero-downtime canary rollout and rollback path; include validation that P95 latency stays under 60ms during rollout?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Instacart","NVIDIA"]},{"id":"q-3089","question":"In kcsa-prod namespace, a payments API behind an Istio service mesh with mTLS shows 401s for a subset of tenants after a policy update that changes JWT claims. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify the JWT filter config, inspect Envoy sidecars, check JWKS rotation, and per-tenant routing; propose a zero-downtime canary rollout path and rollback, and how to validate P95 latency stays under 120 ms during rollout?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Adobe","Airbnb","Plaid"]},{"id":"q-3161","question":"In kcsa-prod, a multi-tenant cluster uses OPA Gatekeeper to enforce per-tenant API access. A policy update yields intermittent 403s when tenants read ConfigMaps in their namespaces. Outline a concrete diagnostic plan with exact kubectl/opa commands to validate ConstraintTemplates, inspect violations, and verify Gatekeeper behavior; propose a zero-downtime rollback path and tests to confirm access for all tenants remains intact during rollback?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Airbnb","Google","OpenAI"]},{"id":"q-3187","question":"In kcsa-prod namespace, a multi-tenant streaming API behind Istio mTLS intermittently returns 5xx during a Chaos Mesh CPU-throttle experiment targeting the sink service. Provide a concrete diagnostic plan with exact kubectl/istioctl/chaosctl commands to verify the experiment, inspect per-tenant routing, check Envoy stats, CPU quotas, and rollback safely while ensuring a P95 latency under 120 ms during the rollout?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["DoorDash","Microsoft","OpenAI"]},{"id":"q-3327","question":"In kcsa-general, a multi-tenant web app runs in per-tenant namespaces. A recent RBAC policy update restricted read access to a shared ConfigMap containing per-tenant feature flags. Some namespaces report UI features not toggled (403 errors when reading the ConfigMap). Provide a concrete diagnostic plan with exact kubectl commands to verify RBAC across affected namespaces, inspect RoleBindings/ClusterRoleBindings, test access with kubectl auth can-i, and outline a safe rollback strategy that restores per-tenant config access with minimal downtime?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Meta","PayPal","Stripe"]},{"id":"q-3567","question":"In a kcsa namespace, a small web service behind an API Gateway intermittently returns 503s after a ConfigMap-driven feature flag update. Provide a concrete diagnostic plan with exact kubectl/describe/logs commands to verify the flag is loaded by pods, inspect in-pod config reload behavior, confirm rollout status, check readiness probes, and outline a safe, zero-downtime rollback path with latency validation that P95 stays under 200 ms?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Goldman Sachs","Lyft","Twitter"]},{"id":"q-3655","question":"In kcsa-prod, a multi-tenant REST API fronted by Istio mTLS experiences sporadic 403s and delayed responses for a subset of tenants after a policy change enabling tenant-scoped routing to a Redis-backed cache layer. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify per-tenant header propagation, VirtualService/DestinationRule routing, and Redis ACLs; propose a safe 5% canary rollout and rollback path; how to confirm P95 latency stays under 120 ms during rollout?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Cloudflare","Zoom"]},{"id":"q-3900","question":"In kcsa-prod, a multi-tenant real-time analytics API using gRPC over Istio mTLS experiences sporadic 502s on streaming calls after a per-tenant header normalization EnvoyFilter was introduced. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to inspect EnvoyFilter rules, per-tenant route/listener config, and stream state; propose a zero-downtime disable/rollback path and describe how to validate P95 latency stays below 150 ms during rollout?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["LinkedIn","Netflix"]},{"id":"q-3919","question":"In kcsa-prod, a multi-tenant data-ingestion API uses a Redis-backed rate limiter behind Istio mTLS and a per-tenant quota policy. After a policy update, a subset of tenants observe 429s during bursts. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify quota rules, inspect Envoy stats, check rate-limit service health, and per-tenant counters; propose a zero-downtime canary rollout path and rollback, and how to validate P95 latency stays under 200 ms during rollout?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Netflix","Tesla"]},{"id":"q-4016","question":"In kcsa-prod, a multi-tenant API gateway behind Istio mTLS shows intermittent TLS handshake failures for a subset of tenants after a policy update that changed gateway TLS origination mode from SIMPLE to MUTUAL. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify Gateway, VirtualService, and DestinationRule TLS configs, inspect per-tenant SNI routing and Envoy listeners, check certificate secrets rotation, and propose a zero-downtime canary rollout path and rollback; show how to validate P95 latency stays under 180 ms during rollout?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Apple","IBM"]},{"id":"q-4056","question":"In kcsa-prod, a multi-tenant telemetry backend using an OpenTelemetry Collector and Istio mTLS shows tail latency spikes and intermittent 5xx in peak ingest as the per-tenant sampling policy changes. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to inspect the otel-collector config (per-tenant sampling, exporters), pod resource usage, and per-tenant traces; propose a zero-downtime canary rollout to adjust sampling and verify P95 latency stays under 150 ms during rollout?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Instacart","Netflix","Twitter"]},{"id":"q-4107","question":"kcsa-prod in a multi-region Istio mesh hosts a payments API behind an Istio mTLS ingress. A new WASM-based field-redaction filter is applied per-tenant, but intermittent leakage occurs under cache warmup. Provide a concrete, practical diagnostic plan with exact kubectl/istioctl commands to verify the WASM module version loaded by sidecars, inspect per-tenant redaction rules in ConfigMaps, check Envoy config_dump and wasm stats, and design a zero-downtime rollback path with validation that P95 latency stays under 180 ms?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Google","PayPal"]},{"id":"q-4150","question":"In kcsa-prod, a multi-tenant data ingestion API behind Istio mTLS shows intermittent 5xx under peak load after introducing a per-tenant rate-limiting EnvoyFilter backed by Redis. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify the EnvoyFilter config, inspect per-tenant quotas, check the ratelimit service and Redis metrics, and validate per-tenant routing; propose a zero-downtime canary rollout path and rollback, and how to ensure P95 latency stays under 150 ms during rollout?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Databricks","Goldman Sachs","Meta"]},{"id":"q-4191","question":"In kcsa-prod, a small web API deployed as a Kubernetes Deployment experiences intermittent 5xx during a recent image update. Provide a concrete diagnostic plan with exact kubectl commands to inspect the Deployment, ReplicaSets, Pods, readiness probes, and the image rollout; outline a zero-downtime canary rollout (including patch/scale steps) and rollback path, plus how to validate that P95 latency stays under 200 ms during rollout?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["PayPal","Scale Ai","Snowflake"]},{"id":"q-4207","question":"KCsa-prod: A multi-tenant payments API behind Istio mTLS uses a per-tenant Envoy WASM rate-limiter injected via an EnvoyFilter. After releasing a new WASM module, certain tenants see 429s during peak payment bursts. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify WASM module loading and version, inspect per-tenant rate-limit config in the module, check Envoy stats, inspect rate-limit service health, and implement a zero-downtime canary rollout with per-tenant gating and rollback; ensure P95 latency stays under 120 ms?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Apple","Lyft","NVIDIA"]},{"id":"q-4284","question":"In kcsa-prod, a multi-tenant event-ingestion API behind Istio mTLS uses a custom Envoy Lua script to enforce per-tenant rate limiting. After a change to the script, some tenants hit 429s during bursts while others are fine. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to inspect the Lua filter, verify per-tenant keys in Redis, confirm rate-limit service health, and examine Envoy stats; propose a zero-downtime canary rollout path and rollback, and how to validate P95 latency stays under 180 ms during rollout?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["OpenAI","Robinhood","Salesforce"]},{"id":"q-4380","question":"In kcsa-prod, a Deployment payments-collector behind a Service intermittently serves 503s as readiness probes fail during a config change. Provide a concrete diagnostic plan with exact kubectl commands to inspect pod readiness, probe definitions, events, and health endpoints; propose a zero-downtime rollback and a safe rollout path, and explain how you would verify latency stays under 200 ms during rollout?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Discord","Lyft","Meta"]},{"id":"q-4417","question":"In kcsa-prod namespace, a multi-tenant event ingestion API behind Istio mTLS experiences 403s for a subset of tenants after an update to an External Authorization (OPA) policy that scopes access by a tenant header. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify the ExternalAuthorization filter config, OPA reachability, per-tenant policy evaluation, and per-tenant routing; propose a zero-downtime canary rollout and rollback; and explain how to validate P95 latency stays under 180 ms during rollout?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Citadel","Meta","Microsoft"]},{"id":"q-4561","question":"In kcsa-prod, a multi-tenant data ingestion API behind Istio mTLS intermittently returns 429s for a subset of tenants after enabling a Redis-backed per-tenant quota service. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify quota service reachability, inspect per-tenant quota keys in Redis, validate the Envoy rate-limit filter config and per-tenant routing, and design a zero-downtime canary rollout with rollback; include how to validate P95 latency stays under 150 ms during rollout?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Databricks","NVIDIA"]},{"id":"q-4606","question":"In kcsa-prod, a multi-tenant API gateway behind Istio mTLS experiences intermittent TLS handshake failures for tenants after automated workload certificate rotation. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify certificate issuance, Envoy TLS context, per-tenant SPIFFE IDs, and next-hop TLS settings; propose a zero-downtime rotation path and rollback, and how to validate P95 latency stays under 120 ms during rollout?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Google","Slack","Stripe"]},{"id":"q-4688","question":"In kcsa-prod, a multi-tenant data-ingest API behind Istio mTLS intermittently returns 503s when a new per-tenant quota sidecar was deployed. Quotas are stored in Redis and enforced via a custom EnvoyFilter. Provide a concrete diagnostic plan with exact kubectl/istioctl commands to verify the EnvoyFilter, per-tenant route config, Redis keys/TTL, and quota filter stats; propose a zero-downtime canary rollout to adjust TTL or move to in-memory caching, and outline how to validate P95 latency stays under 150 ms during rollout?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["NVIDIA","Snap","Tesla"]},{"id":"q-4776","question":"In kcsa-prod, a multi-tenant data-processing service on Kubernetes uses Gatekeeper for security policies. A policy update enforces non-root execution and removal of hostPath across all namespaces; a subset of tenants experiences deployment delays and intermittent 4xx during upgrades. Provide a concrete diagnostic plan with exact kubectl and gatekeeper commands to identify violating policies, inspect ConstraintTemplates and Constraints, review namespace exemptions, and trace the admission pipeline; propose a zero-downtime rollout with per-tenant allowlists and rollback, and explain how you would validate latency and throughput during rollout?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Bloomberg","DoorDash","Lyft"]},{"id":"q-836","question":"You are analyzing a web app's login events, captured as an array of objects: { userId: string, ts: string (ISO 8601) }. Write a JavaScript function that returns the top 3 users by login count in the past 24 hours. Tie-break with alphabetical userId. Provide a concise implementation and explain its time complexity?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Meta","NVIDIA"]},{"id":"q-869","question":"In a high-traffic search suggestions feature, implement a Node.js module that debounces input by 300ms and caches per-query results in memory. Show cache invalidation and handle concurrent requests for the same key without duplicating work. Provide a compact, testable example with usage?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Apple","Lyft","Zoom"]},{"id":"q-912","question":"**KCSA Beginner Question: Inventory Pagination** Scenario: In a warehouse app, design a minimal endpoint GET /items?limit=&offset= that returns items ordered by id, supports pagination, and scales with 10k+ rows. Explain data model, indexing, and error handling. How would you implement a function to validate and apply LIMIT/OFFSET in code, ensuring correctness?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Amazon","Apple"]},{"id":"q-947","question":"You're building a fintech API used by wallets and partner services to transfer funds; describe a secure, scalable auth and data-protection design for REST endpoints, including per-partner API keys, short-lived access tokens with rotating signing keys, mTLS between services, replay protection, and observability to detect abuse, plus failure modes and trade-offs?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Coinbase","PayPal","Plaid"]},{"id":"gh-56","question":"What are the key deployment strategies in Kubernetes, and how do you configure them considering resource limits, health checks, and rollback scenarios?","channel":"kubernetes","subChannel":"deployments","difficulty":"intermediate","tags":["automation","tools"],"companies":["Amazon","Google","Hashicorp","Microsoft","Netflix","Salesforce"]},{"id":"gh-7","question":"What is Kubernetes and how does it orchestrate containerized applications at scale?","channel":"kubernetes","subChannel":"deployments","difficulty":"beginner","tags":["k8s","orchestration"],"companies":["Airbnb","Amazon","Google","Microsoft","Uber"]},{"id":"gh-8","question":"Design a highly available Kubernetes cluster architecture. What are the main components, their interactions, and how do you ensure 99.95% uptime across multiple availability zones?","channel":"kubernetes","subChannel":"deployments","difficulty":"intermediate","tags":["k8s","orchestration"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-306","question":"How would you implement a canary deployment strategy in Kubernetes to minimize risk during application updates?","channel":"kubernetes","subChannel":"deployments","difficulty":"advanced","tags":["rolling-update","canary","blue-green"],"companies":["Amazon","Google","Meta"]},{"id":"q-334","question":"You're deploying a new version of a microservice in Kubernetes. Describe how you would perform a rolling update and what would you do if the new version starts failing health checks?","channel":"kubernetes","subChannel":"deployments","difficulty":"beginner","tags":["rolling-update","canary","blue-green"],"companies":["Amazon","Cisco","Google","Microsoft","Netflix","New Relic","Stripe"]},{"id":"q-369","question":"You're deploying a critical video processing service at Zoom. During a rolling update, 30% of users experience degraded performance while the new version is being deployed. How would you diagnose and resolve this issue, and what deployment strategy would you recommend instead?","channel":"kubernetes","subChannel":"deployments","difficulty":"intermediate","tags":["rolling-update","canary","blue-green"],"companies":["MongoDB","New Relic","Zoom"]},{"id":"q-3125","question":"Scenario: You have a Deployment with 3 replicas of a widget app listening on port 8080. With an Ingress controller installed, create a Service and an Ingress to expose /v1 to the app. Explain the resources, DNS, and traffic flow, and how you would validate end-to-end?","channel":"kubernetes","subChannel":"general","difficulty":"beginner","tags":["kubernetes"],"companies":["Adobe","Instacart","Snowflake"]},{"id":"q-3204","question":"Scenario: You run a 3-replica MongoDB replica set as a StatefulSet on Kubernetes. You must upgrade MongoDB from 4.4 to 5.0 with zero downtime. How would you orchestrate the upgrade using StatefulSet RollingUpdate (with partition) to upgrade secondaries first, ensure data durability, and then upgrade the primary? Include validation steps and how to keep traffic available during the upgrade?","channel":"kubernetes","subChannel":"general","difficulty":"intermediate","tags":["kubernetes"],"companies":["Google","MongoDB"]},{"id":"q-3394","question":"You're running a multi-tenant SaaS on a single Kubernetes cluster. Each tenant has a dedicated namespace with quotas, network isolation, and restricted API access. Outline a concrete plan using: (i) OPA Gatekeeper constraint templates for image provenance and privileged flags, (ii) default-deny RBAC with per-tenant Roles, (iii) per-namespace NetworkPolicy enforcing egress to approved services, and (iv) a canary rollout plus automated rollback (GitOps) strategy. Include testing, monitoring, and breach-response steps?","channel":"kubernetes","subChannel":"general","difficulty":"advanced","tags":["kubernetes"],"companies":["Cloudflare","Meta"]},{"id":"q-3448","question":"Given a 300-node Kubernetes cluster hosting latency-sensitive frontend services and batch analytics, how would you implement zero-downtime deployments while migrating traffic from a canary release to production, handling long-lived TCP connections, and ensuring quick rollback using only native Kubernetes primitives (Deployments, Services, Readiness/Liveness, and NetworkPolicies)?","channel":"kubernetes","subChannel":"general","difficulty":"advanced","tags":["kubernetes"],"companies":["Discord","Twitter","Uber"]},{"id":"q-3491","question":"Design a per-environment namespace strategy in Kubernetes to limit cluster resource usage while enabling automated CI to deploy preview environments. Which primitives would you use, and how would you configure ResourceQuota, LimitRange, Namespaces, and NetworkPolicy to meet these goals?","channel":"kubernetes","subChannel":"general","difficulty":"beginner","tags":["kubernetes"],"companies":["Microsoft","NVIDIA","Twitter"]},{"id":"q-3595","question":"Design a Kubernetes-native BatchJob CRD and operator to orchestrate a DAG of data-processing tasks across namespaces. Each BatchJob specifies tasks with dependencies; the operator translates ready tasks into Kubernetes Jobs, handles per-task retries with exponential backoff, and uses finalizers for cleanup. Outline status modeling and observability?","channel":"kubernetes","subChannel":"general","difficulty":"intermediate","tags":["kubernetes"],"companies":["Apple","Robinhood","Twitter"]},{"id":"q-3719","question":"In a Kubernetes deployment for a stateless frontend, you must fetch a TLS certificate from a remote Vault before the main container starts. Describe a native-primitives solution using an InitContainer that retrieves the cert and writes it to an emptyDir shared with the app, and configures readiness so the Pod isn't Ready until /certs/cert.pem exists. Include how to implement retries and exponential backoff?","channel":"kubernetes","subChannel":"general","difficulty":"beginner","tags":["kubernetes"],"companies":["Adobe","Discord","Netflix"]},{"id":"q-3877","question":"Scenario: In a shared Kubernetes cluster, a stateless API front-end (50 pods across 4 namespaces) shows occasional 100–300ms tail latency during namespace bursts, while per-pod CPU remains within quotas. Design a concrete, Kubernetes-native debugging and mitigation plan: identify noisy neighbor, throttling, or network saturation. Include what you would measure, how you would reproduce, and a rollback path. Use native primitives (HPA, QoS, ResourceQuotas, NetworkPolicy, PodDisruptionBudget) and observability tools?","channel":"kubernetes","subChannel":"general","difficulty":"intermediate","tags":["kubernetes"],"companies":["Amazon","Apple"]},{"id":"q-4199","question":"In a multi-tenant Kubernetes cluster, each team gets a namespace and a shared PVC. Describe a beginner-friendly, Kubernetes-native setup (no CRDs or external tooling) to enforce per-namespace quotas with default limits, deterministic pod scheduling, and a safe upgrade path with quick rollback for a frontend service. Include concrete steps and commands?","channel":"kubernetes","subChannel":"general","difficulty":"beginner","tags":["kubernetes"],"companies":["Databricks","Robinhood"]},{"id":"q-4208","question":"Design a Kubernetes operator that enforces per-namespace batch scheduling using a global BatchQueue CRD. Each BatchQueue entry represents a task with namespace, image, command, and resources; operator schedules eligible tasks as Jobs, enforces per-namespace ResourceQuota, uses exponential backoff on retries, and cleans up completed Jobs. Expose metrics: latency, success, failures; status: queued, running, done, failed?","channel":"kubernetes","subChannel":"general","difficulty":"advanced","tags":["kubernetes"],"companies":["Airbnb","Google"]},{"id":"q-4315","question":"Scenario: A 5-replica Deployment serving a latency-sensitive web app in prod. CPU usage spikes during peak hours. How would you configure a CPU-based HorizontalPodAutoscaler (min 2, max 6), set resource requests/limits, ensure zero-downtime rolling updates, and validate behavior in staging before prod? Provide concrete YAML and testing steps?","channel":"kubernetes","subChannel":"general","difficulty":"beginner","tags":["kubernetes"],"companies":["IBM","Twitter"]},{"id":"q-4432","question":"In a multi-tenant Kubernetes cluster, implement policy enforcement to ensure stage namespaces only run containers from an approved registry and disallow privileged containers. Use Kubernetes-native tooling with OPA Gatekeeper (ConstraintTemplate and Constraint). Provide a minimal manifest snippet and testing steps, and explain trade-offs?","channel":"kubernetes","subChannel":"general","difficulty":"intermediate","tags":["kubernetes"],"companies":["Airbnb","MongoDB"]},{"id":"q-4454","question":"Scenario: You run a shared Kubernetes cluster with latency-sensitive services and batch tasks across multiple namespaces. Propose a selective OpenTelemetry sidecar injection strategy using a MutatingAdmissionWebhook. How would you gate by namespace annotations, ensure idempotence, handle upgrades, and provide rollback and observability hooks?","channel":"kubernetes","subChannel":"general","difficulty":"intermediate","tags":["kubernetes"],"companies":["Anthropic","Google","OpenAI"]},{"id":"q-4557","question":"Design a Kubernetes operator that enforces per-namespace egress bandwidth quotas in a multi-tenant cluster. Create a CRD NamespaceEgressQuota with fields: namespace, maxBytesPerSecond, burstBytes, and breachAction. The operator should translate quotas into Calico/Cilium egress policies, use a per-namespace token-bucket for bursts, and update policies on quota changes or scale events. How would you model reconciliation, handle conflicts, and expose SLA metrics?","channel":"kubernetes","subChannel":"general","difficulty":"advanced","tags":["kubernetes"],"companies":["Adobe","Snowflake","Two Sigma"]},{"id":"q-4581","question":"Design a Kubernetes operator for a CustomResourceDefinition named DataMigrationPlan that coordinates cross-namespace data migrations. The plan encodes an explicit DAG of migration tasks; each task specifies a source namespace, a destination namespace, a data selector, and per-namespace quotas. The operator must translate ready tasks into Kubernetes Jobs, enforce idempotency, support per-task retries with exponential backoff and jitter, and use finalizers to clean up temporary artifacts (PVCs, ConfigMaps). Describe your approach to scheduling, error handling, RBAC, observability, and testing at scale?","channel":"kubernetes","subChannel":"general","difficulty":"advanced","tags":["kubernetes"],"companies":["MongoDB","Salesforce","Tesla"]},{"id":"q-4616","question":"On a shared Kubernetes cluster hosting high-traffic services (Slack-like chat, ride-hailing, and edge proxy workloads), design a per-namespace, dynamic quota controller using a CRD that adjusts ResourceQuota and LimitRange every 5 minutes based on observed usage, ensuring fairness, burst allowances, and safe preemption. What would you implement and why?","channel":"kubernetes","subChannel":"general","difficulty":"advanced","tags":["kubernetes"],"companies":["Cloudflare","DoorDash","Slack"]},{"id":"q-465","question":"You're running a production Kubernetes cluster and notice pods are frequently getting OOMKilled despite having sufficient memory limits. How would you diagnose and resolve this issue?","channel":"kubernetes","subChannel":"general","difficulty":"intermediate","tags":["kubernetes"],"companies":["Adobe","Meta","Square"]},{"id":"q-4730","question":"Scenario: In a cluster hosting finance apps, implement a per-namespace egress policy that allows pods in the 'finance' namespace to reach only https://api.fin.example and https://gateway.fin.example, while denying all other outbound traffic. Outline the approach using Kubernetes NetworkPolicy (default-deny, namespace-scoped), any caveats with DNS-based allowlists, and how to test/observe. Include a minimal manifest snippet for the policy?","channel":"kubernetes","subChannel":"general","difficulty":"intermediate","tags":["kubernetes"],"companies":["Apple","Bloomberg"]},{"id":"q-4783","question":"Beginner Kubernetes diagnostic: You deploy a simple HTTP server in a Deployment with a readinessProbe httpGet on /health. After a code push, pods remain NotReady and a RollingUpdate stalls. Describe exact steps to diagnose and fix using only native primitives (Deployments, Pods, Services, Probes): what to inspect, how to test from inside a pod, and how to adjust probes to resume traffic with no downtime?","channel":"kubernetes","subChannel":"general","difficulty":"beginner","tags":["kubernetes"],"companies":["Google","Lyft","NVIDIA"]},{"id":"q-4842","question":"In a Kubernetes cluster hosting multi-tenant data workloads, design a Kubernetes-native data tiering operator. Define a CRD TieredDataset with spec.hotPVC, spec.coldObjectStore, spec.migrationPolicy (immediate, time-based), and status fields (phase, conditions, lastMigration). Explain how the controller shards data, uses CSI snapshots or data movers to move blocks between hot PVCs and a cold store, ensures idempotence, handles retries, and exposes metrics/events for observability?","channel":"kubernetes","subChannel":"general","difficulty":"advanced","tags":["kubernetes"],"companies":["Databricks","Discord","Microsoft"]},{"id":"q-526","question":"You're running a production Kubernetes cluster with 1000+ pods. Your monitoring shows that certain nodes are experiencing high memory pressure, causing pod evictions. How would you diagnose and resolve this issue systematically?","channel":"kubernetes","subChannel":"general","difficulty":"advanced","tags":["kubernetes"],"companies":["Bloomberg","Snap","Two Sigma"]},{"id":"q-552","question":"You're running a production Kubernetes cluster at scale and notice that some pods are experiencing intermittent network timeouts. How would you diagnose and resolve this issue, considering both application-level and cluster-level networking components?","channel":"kubernetes","subChannel":"general","difficulty":"advanced","tags":["kubernetes"],"companies":["Citadel","NVIDIA","Tesla"]},{"id":"q-579","question":"How would you debug a pod that's stuck in CrashLoopBackOff state in a production Kubernetes cluster?","channel":"kubernetes","subChannel":"general","difficulty":"intermediate","tags":["kubernetes"],"companies":["Cloudflare","Tesla"]},{"id":"de-135","question":"You have a Helm chart that needs to deploy different configurations for staging and production environments. The staging environment should use 2 replicas with 512Mi memory limit, while production should use 5 replicas with 2Gi memory limit. How would you structure your values files and templates to handle this requirement?","channel":"kubernetes","subChannel":"helm","difficulty":"intermediate","tags":["helm","k8s"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-49","question":"How does Helm manage Kubernetes application lifecycle through charts, releases, and templates?","channel":"kubernetes","subChannel":"helm","difficulty":"advanced","tags":["k8s","advanced"],"companies":["Amazon","Apple","Google","Hashicorp","Microsoft","Netflix"]},{"id":"q-400","question":"You're deploying a microservice using Helm and notice that the pod keeps crashing with 'ImagePullBackOff' error. The values.yaml specifies 'image.repository: my-service' and 'image.tag: latest'. How would you debug this issue and what's the proper way to configure image pull policies in production?","channel":"kubernetes","subChannel":"helm","difficulty":"intermediate","tags":["charts","values","templating"],"companies":["Adobe","Amazon","Google","Hashicorp","Jane Street","Microsoft","Netflix","Snowflake"]},{"id":"gh-55","question":"How does Tekton provide a cloud-native framework for building CI/CD pipelines on Kubernetes?","channel":"kubernetes","subChannel":"operators","difficulty":"beginner","tags":["automation","tools"],"companies":["Digitalocean","Google","IBM","Microsoft","Red Hat"]},{"id":"q-193","question":"What is the role of a Custom Resource Definition (CRD) in a Kubernetes Operator and how does it enable custom functionality?","channel":"kubernetes","subChannel":"operators","difficulty":"beginner","tags":["crds","controllers","reconciliation"],"companies":["Amazon","Datadog","Google","Microsoft","Prove"]},{"id":"q-291","question":"What is the role of a reconciliation loop in a Kubernetes operator controller?","channel":"kubernetes","subChannel":"operators","difficulty":"beginner","tags":["crds","controllers","reconciliation"],"companies":["Amazon","Google","Meta"]},{"id":"q-346","question":"You're building a Kubernetes operator for a custom resource that manages a fleet of microservices. Your controller is experiencing high memory usage and slow reconciliation loops. How would you design a solution to handle 10,000+ custom resources efficiently while ensuring proper event handling and preventing resource leaks?","channel":"kubernetes","subChannel":"operators","difficulty":"advanced","tags":["crds","controllers","reconciliation"],"companies":["Amazon","Cloudflare","Gitlab","Google","Hashicorp","Microsoft","MongoDB","Workday"]},{"id":"q-383","question":"You're building a Kubernetes operator for a custom database CRD. During reconciliation, you notice the controller is constantly updating the status even when no actual changes occur. How would you implement proper change detection and prevent unnecessary updates?","channel":"kubernetes","subChannel":"operators","difficulty":"intermediate","tags":["crds","controllers","reconciliation"],"companies":["AMD","Google","OpenAI"]},{"id":"gh-100","question":"What is a Sidecar Pattern in Kubernetes?","channel":"kubernetes","subChannel":"pods","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-48","question":"Explain how DaemonSets ensure pod distribution across Kubernetes nodes and describe the controller reconciliation loop that maintains this guarantee?","channel":"kubernetes","subChannel":"pods","difficulty":"advanced","tags":["k8s","advanced"],"companies":["Amazon","Google","Hashicorp","Microsoft","Netflix","Snowflake"]},{"id":"gh-51","question":"What is Container Runtime Interface (CRI) and why is it important in Kubernetes?","channel":"kubernetes","subChannel":"pods","difficulty":"advanced","tags":["k8s","advanced"],"companies":["Amazon","Google","Microsoft"]},{"id":"gh-9","question":"What is a Pod in Kubernetes and why is it considered the smallest deployable unit?","channel":"kubernetes","subChannel":"pods","difficulty":"beginner","tags":["k8s","orchestration"],"companies":["Amazon","Google","LinkedIn","Microsoft","Uber"]},{"id":"q-173","question":"What is a Kubernetes Pod and what is its primary purpose?","channel":"kubernetes","subChannel":"pods","difficulty":"beginner","tags":["pods","containers"],"companies":null},{"id":"q-245","question":"How do init containers differ from sidecar containers in Kubernetes pod lifecycle and resource sharing patterns?","channel":"kubernetes","subChannel":"pods","difficulty":"beginner","tags":["containers","init-containers","sidecars"],"companies":["Amazon","Databricks","Google","Microsoft","Netflix","Stripe"]},{"id":"q-271","question":"Design a zero-downtime database migration system using Kubernetes multi-container pods with init containers, sidecars, and shared volumes. How would you handle schema validation, migration execution, rollback, and coordination while maintaining service availability?","channel":"kubernetes","subChannel":"pods","difficulty":"intermediate","tags":["containers","init-containers","sidecars"],"companies":["Amazon","Databricks","Google","Microsoft","Netflix","Stripe"]},{"id":"q-356","question":"You're deploying a security scanning sidecar with a main application pod. The sidecar must complete its vulnerability scan before the main container starts, then continue monitoring runtime threats. Design this pod configuration with shared volumes, health checks, and graceful shutdown. What key components ensure the security scanning completes before application startup?","channel":"kubernetes","subChannel":"pods","difficulty":"intermediate","tags":["containers","init-containers","sidecars"],"companies":null},{"id":"q-412","question":"You're deploying a web application that needs to run database migrations before the main container starts. How would you configure a Pod with an init container to handle this, and what happens if the init container fails?","channel":"kubernetes","subChannel":"pods","difficulty":"beginner","tags":["containers","init-containers","sidecars"],"companies":["Figma","NVIDIA","Okta"]},{"id":"q-511","question":"How does Kubernetes handle pod scheduling and what factors influence scheduling decisions?","channel":"kubernetes","subChannel":"pods","difficulty":"intermediate","tags":["kubernetes","scheduling","pods","resource-management","container-orchestration"],"companies":["Amazon","Google","Microsoft","Red Hat"]},{"id":"q-636","question":"What are init containers in Kubernetes and how do they differ from regular containers?","channel":"kubernetes","subChannel":"pods","difficulty":"intermediate","tags":["kubernetes","containers","pod-lifecycle","initialization"],"companies":["Amazon","Google","Microsoft"]},{"id":"q-637","question":"What are the key differences between init containers, sidecar containers, and static pods in Kubernetes?","channel":"kubernetes","subChannel":"pods","difficulty":"intermediate","tags":["kubernetes","containers","pod-lifecycle","deployment-patterns"],"companies":["Amazon","Google","Microsoft","Red Hat"]},{"id":"gh-101","question":"What is a Service Mesh Control Plane and how does it manage microservices communication?","channel":"kubernetes","subChannel":"services","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Salesforce","Uber"]},{"id":"gh-50","question":"How does Istio implement service mesh architecture using sidecar proxies, and what are the key components for traffic management, security, and observability?","channel":"kubernetes","subChannel":"services","difficulty":"advanced","tags":["k8s","advanced"],"companies":null},{"id":"q-219","question":"How would you design a zero-downtime service migration strategy using Kubernetes Service selectors and Endpoints controller to avoid connection drops during rolling updates?","channel":"kubernetes","subChannel":"services","difficulty":"advanced","tags":["clusterip","nodeport","loadbalancer","ingress"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-320","question":"You have a microservice deployed in Kubernetes that needs to be accessible both internally within the cluster and externally via a custom domain. How would you configure the service and ingress to achieve this, and what are the trade-offs between using ClusterIP, NodePort, and LoadBalancer service types?","channel":"kubernetes","subChannel":"services","difficulty":"advanced","tags":["clusterip","nodeport","loadbalancer","ingress"],"companies":["Elastic","Snowflake","Zoom"]},{"id":"gh-26","question":"What are the essential Linux commands every DevOps engineer should master for system administration, troubleshooting, and automation?","channel":"linux","subChannel":"commands","difficulty":"beginner","tags":["linux","shell"],"companies":["Amazon","Cloudflare","Google","Hashicorp","Microsoft","Netflix"]},{"id":"q-1000","question":"A Linux host runs several Docker containers; during peak load, API responses slow and some requests time out. Describe a beginner-friendly, concrete diagnostic workflow to (1) confirm whether host saturation is CPU, memory, or I/O, (2) identify the container most responsible, and (3) apply a safe mitigation (e.g., graceful restart or CPU/memory throttling) while monitoring impact. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Instacart","LinkedIn"]},{"id":"q-1142","question":"Scenario: A Linux server hosting a small web service used by customers suddenly shows disk usage climbing on the root filesystem. Provide a beginner-friendly, concrete diagnostic workflow to (1) locate the directories/files consuming the most space, (2) identify top offenders, and (3) apply a safe mitigation (e.g., rotate/compress/archive logs or purge old data) while keeping the service up. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["DoorDash","Google","Salesforce"]},{"id":"q-1205","question":"You're operating a fleet of Linux hosts running a low-latency web API. Intermittent 100–200 ms latency spikes appear under load. Design a practical, end-to-end diagnostic plan using eBPF/BPFtrace or perf, iostat/vmstat, and container metrics to collect data, identify root cause (CPU scheduling, IO wait, or network), and propose minimal-disruption mitigations?","channel":"linux","subChannel":"general","difficulty":"advanced","tags":["linux"],"companies":["Salesforce","Snap"]},{"id":"q-1244","question":"Scenario: A Linux host runs a MongoDB primary in a high-traffic cluster. During peak hours, latency spikes and tail latency increases while CPU and memory appear stable. Using only default tooling, no downtime, describe a concrete, actionable diagnostic workflow to (1) determine if I/O wait, CPU, or memory is the bottleneck, (2) pinpoint the offending disk/device or process, and (3) apply a safe mitigation (e.g., adjust I/O scheduler, tune dirty writeback parameters, or throttle MongoDB) while monitoring impact. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Adobe","Databricks","MongoDB"]},{"id":"q-1266","question":"Context: Linux node in a Kubernetes cluster hosting a high-throughput data ingestion service. Intermittent tail latency spikes (>1s) appear during peak traffic, affecting processing. Without downtime, design a concrete troubleshooting workflow to (1) confirm whether CPU, I/O, or network is the bottleneck, (2) identify the exact subsystem or process responsible, and (3) implement a safe mitigation with minimal impact while maintaining observability. Include exact commands and realistic outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Airbnb","Anthropic","Snowflake"]},{"id":"q-1364","question":"A Linux host running multiple microservices behind NGINX exhibits intermittent latency spikes; new connections occasionally fail and FD usage appears high. Using only default tools, describe a beginner-friendly workflow to (1) confirm FD exhaustion is the bottleneck, (2) identify the offending process by per‑process FD usage, and (3) apply a safe mitigation (increase NOFILE limits, adjust per-service limits, or set a systemd limit) while monitoring impact. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Hugging Face","PayPal","Tesla"]},{"id":"q-1410","question":"On a Linux host, a data-processing job occasionally stalls during bursts. Using only default tooling and no downtime, outline a concrete, beginner-friendly diagnostic flow to (1) confirm CPU, memory, or I/O bound, (2) identify the offending process and its operation, and (3) apply a safe mitigation (e.g., adjust priority, pause/resume, or throttle) with monitoring. Which commands would you run and what outputs would you expect?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Instacart","OpenAI","Square"]},{"id":"q-1424","question":"Scenario: a high-traffic Linux service behind a reverse proxy experiences sporadic tail latency spikes during peak hours. CPU and IO look normal in aggregate. Design a practical end-to-end diagnostic using eBPF to identify root causes quickly: (1) instrument per-request latency across kernel and user-space, (2) attribute latency to network, disk, or app code, (3) propose safe mitigations and validate impact. Include specific probes, sample commands, and expected outputs?","channel":"linux","subChannel":"general","difficulty":"advanced","tags":["linux"],"companies":["Citadel","Cloudflare","Twitter"]},{"id":"q-1445","question":"A Linux host running GPU-accelerated video processing containers shows intermittent frame latency spikes during peak load, with no obvious CPU/memory/I/O bottlenecks. Without downtime, describe a concrete, real-world diagnostic workflow using only default tools to (1) verify if latency is caused by page cache pressure or Transparent Huge Pages, (2) identify the subsystem or container involved, and (3) apply a safe mitigation (e.g., disable THP on affected NUMA nodes, tune swappiness, or pin tasks) with minimal impact and observable results. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Amazon","DoorDash","NVIDIA"]},{"id":"q-1570","question":"A Linux CI node running multiple git builds experiences occasional stalls during parallel jobs. Using only default tooling and no downtime, describe a concrete diagnostic workflow to (1) determine if CPU, memory, or I/O is the bottleneck, (2) identify the specific component (e.g., git, filesystem, network) causing the stall, and (3) apply a safe mitigation (e.g., throttle parallel jobs, adjust I/O scheduler, or raise file descriptor limits) while monitoring impact. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["IBM","Tesla","Uber"]},{"id":"q-1705","question":"You have a Linux host running a Rust-based data-processing daemon that stalls for 30–60 seconds under peak load. Using only default tooling, design a concrete diagnostic workflow to (1) determine if the stall is CPU-, I/O-, or memory-bound, (2) identify the exact subsystem or process causing the stall, and (3) apply a safe mitigation (e.g., cgroup throttling or IO-scheduler tweaks) while preserving observability. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Google","Hugging Face","Lyft"]},{"id":"q-1873","question":"Context: A Linux host runs a Kubernetes-deployed real-time inference service behind Nginx; latency tail spikes occur during bursts. Without downtime, describe a concrete, repeatable workflow to determine whether latency is caused by CPU throttling, memory pressure, or network queueing, identify the offending container/pod, and apply a safe mitigation (e.g., adjust cgroup limits, scale the deployment, or tune kernel parameters) while preserving observability. Include exact commands, expected outputs, and a simple rollback plan?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Airbnb","Hugging Face","Snap"]},{"id":"q-2038","question":"Scenario: A Linux host running a batch queue occasionally fails to start new jobs with 'Too many open files' under moderate load. Without downtime, describe a concrete, beginner-friendly diagnostic workflow using default tools to (1) confirm FD limits are the bottleneck, (2) identify the process or user hitting the limit, and (3) apply a safe mitigation (e.g., raise per-user limits, adjust LimitNOFILE for the service) while keeping service available. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Amazon","Goldman Sachs","Salesforce"]},{"id":"q-2273","question":"Context: A Linux server hosts a real-time ingestion service and a Redis cache on the same box. During peak ingestion, tail latency spikes. Using only default tools, outline a concrete, reproducible workflow to (1) confirm whether CPU, IO, memory, or network is the bottleneck, (2) pinpoint the offender process or disk, and (3) apply a safe mitigation with minimal disruption. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["MongoDB","Slack","Snap"]},{"id":"q-2318","question":"Scenario: A Linux host in a high-throughput UDP ingestion pipeline experiences random latency spikes under peak traffic. Using only default tooling, design a concrete diagnostic workflow to (1) determine whether NIC, IRQs, or queue saturation is the bottleneck, (2) identify the offending interface/driver and IRQ affinity, and (3) apply a safe mitigation (e.g., adjust IRQ affinity, enable RSS, increase Rx queue depth) while preserving observability. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"advanced","tags":["linux"],"companies":["Databricks","Goldman Sachs","MongoDB"]},{"id":"q-2353","question":"A Linux host runs three LXC containers. During peak hours, one container's API latency spikes while others stay responsive. Using only default tooling, describe a concrete, beginner-friendly diagnostic workflow to (1) determine if bottleneck is CPU, memory, or I/O at host or container level, (2) identify the container and the exact process responsible for the spike, and (3) propose a safe mitigation (e.g., adjust container limits, throttle I/O, or relocate service) with observability. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Hashicorp","OpenAI","Snowflake"]},{"id":"q-2426","question":"Context: A Linux host runs multiple containers on a single NIC behind a load balancer. During a traffic spike, a critical API shows increased latency and occasional 5xx errors. Using only default Linux tools, outline a concrete, practical diagnostic workflow to (1) determine whether latency comes from CPU, memory pressure, disk I/O, or network contention, (2) identify the exact container or process responsible, and (3) apply a safe mitigation (e.g., throttle or move to a different cgroup, adjust IO scheduler) with minimal disruption and full observability. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Google","Netflix","Uber"]},{"id":"q-2598","question":"On a Linux host running a high-throughput data-ingestion service, intermittent 5–10s stalls appear under sustained load. Using only default tooling, design a concrete diagnostic workflow to determine whether stalls are caused by Transparent Huge Pages, memory fragmentation, or IO pressure; identify the exact subsystem responsible; and apply a safe mitigation (e.g., disable THP, adjust NUMA binding) while preserving observability. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"advanced","tags":["linux"],"companies":["Coinbase","Databricks","IBM"]},{"id":"q-2740","question":"Scenario: A Linux server's boot time has increased after a kernel update. Using only default tooling, describe a concrete diagnostic workflow to (1) determine which boot phase takes the longest, (2) pinpoint the slowest unit or driver, and (3) apply a safe mitigation (e.g., mask a service, adjust a TimeoutStartSec, or blacklist a module) while preserving observability. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Adobe","Citadel"]},{"id":"q-2819","question":"A Linux server runs a Python data-collection daemon under systemd; during bursts the daemon slows dramatically while overall CPU and memory appear normal. Using only default tools, outline a beginner-friendly diagnostic workflow to (1) determine if CPU throttling via cgroups or the scheduler is the bottleneck, (2) identify the throttled process, and (3) apply a safe mitigation (e.g., adjust cpu.max or Nice value) while preserving observability. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Bloomberg","LinkedIn","Netflix"]},{"id":"q-2900","question":"Context: A Linux host running dozens of containers under cgroups v2 experiences intermittent tail-latency spikes on one service during peak load. Without downtime, design a precise diagnostic workflow to (1) verify if cpu.max throttling is the culprit, (2) locate the offending container/process, and (3) apply a safe mitigation (adjust quotas, pin CPUs, or throttle traffic) while preserving QoS. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["IBM","Instacart"]},{"id":"q-2917","question":"You're running a Linux host in a high-load microservice environment. Under peak traffic, 95th percentile latency on a critical path spikes 2–5x while CPU, I/O, and memory appear nominal. Using only default tools, design a concrete diagnostic workflow to (1) determine if bottleneck is CPU, memory, disk I/O, or network, (2) identify the exact offender (process, device, or subsystem), and (3) apply a safe, observable mitigation (e.g., cgroup throttling, scheduler tuning, or buffering changes) with minimal downtime. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"advanced","tags":["linux"],"companies":["Discord","Lyft"]},{"id":"q-3310","question":"Linux server runs a Redis cluster and a busy web app; tail latency spikes under load while CPU and memory look healthy. Without downtime, design a concrete, repeatable diagnostic workflow to (1) pinpoint if network I/O, disk I/O, or interrupts are the bottleneck, (2) identify the offender, and (3) apply a safe mitigation with minimal disruption. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Cloudflare","Salesforce","Snap"]},{"id":"q-3489","question":"You have a Linux server hosting a real-time data ingestion service and a multi-stage database on a 2-socket NUMA machine. Under peak load, tail latency spikes. Using only default tooling, design a concrete diagnostic workflow to (1) determine if stalls are CPU, memory, or I/O bound, (2) identify the NUMA locality or device causing pressure and the offending process, and (3) apply a safe mitigation (e.g., CPU pinning, NUMA binding, or IO scheduling tweaks) while preserving observability. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"advanced","tags":["linux"],"companies":["Databricks","MongoDB","Tesla"]},{"id":"q-3535","question":"Scenario: A Linux host running several VMs experiences intermittent memory pressure during business hours, causing guest pauses. Using only default tools, outline a beginner-friendly diagnostic workflow to (1) determine if host memory pressure or VM ballooning is the bottleneck, (2) identify which VM(s) are affected and estimate their memory footprint, and (3) apply a safe mitigation (e.g., cap VM memory via libvirt, adjust host swappiness) while preserving observability. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Adobe","NVIDIA","Robinhood"]},{"id":"q-3610","question":"You run a Linux host with a small API service behind Nginx. During bursts, tail latency spikes even though CPU, memory, and network look normal. Using only default tooling, design a beginner-friendly diagnostic workflow to (1) determine if CPU, memory, or I/O wait is the bottleneck, (2) identify the exact process or disk causing the bottleneck, and (3) apply a safe mitigation (e.g., adjust I/O scheduler, tune dirty writeback, or scale workers) while preserving observability. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Airbnb","IBM","NVIDIA"]},{"id":"q-4263","question":"On a single Linux host running multiple Python services in separate cgroups, tail latency spikes under peak load even though aggregate CPU usage stays under 70%. Using only default tools, design a concrete diagnostic workflow to (1) determine if tail latency is due to CPU throttling by cgroups, (2) identify which cgroup and process is hitting quotas, and (3) propose a safe mitigation (e.g., adjust quotas, shares, or migrate a heavy service to its own host) while preserving observability. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Airbnb","Google","LinkedIn"]},{"id":"q-4294","question":"On a Linux host serving low-latency ML inference REST endpoints, tail latency spikes under peak load while average latency stays acceptable. Using only default tools, design a concrete diagnostic workflow to (1) determine if tail latency is due to CPU, memory, IO, or interrupts, (2) identify the bottlenecked core/NUMA domain and the offending process, and (3) propose a safe mitigation (e.g., CPU pinning, shielding, IO scheduler tweaks) while preserving observability. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Anthropic","MongoDB","Two Sigma"]},{"id":"q-4319","question":"**Scenario:** A Linux host runs a multi-tenant service and experiences intermittent network latency spikes during peak load. Using only default tooling, design a concrete diagnostic workflow to (1) determine if latency is kernel-network-stack, NIC, or user-space, (2) identify the exact TX/RX queue or IRQ causing it, and (3) apply a safe mitigation (e.g., adjust IRQ affinity, interrupt coalescing, or RPS) while preserving availability. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"advanced","tags":["linux"],"companies":["Airbnb","Databricks","Oracle"]},{"id":"q-4465","question":"On a dual-socket Linux server running a CPU-bound analytics workload, latency spikes occur under peak load even when CPU usage seems normal. Using only default tooling, design a concrete diagnostic workflow to (1) determine if the spike is NUMA memory locality, CPU throttling, or kernel I/O, (2) pinpoint the offending subsystem or process, and (3) apply a safe mitigation (e.g., bind memory/threads with numactl, isolate CPUs, or adjust memory policies) while preserving observability. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"advanced","tags":["linux"],"companies":["Citadel","Zoom"]},{"id":"q-466","question":"You're debugging a production Linux server where processes are randomly dying with 'Out of memory' errors, but `free -m` shows 8GB available RAM. How would you diagnose and fix this issue?","channel":"linux","subChannel":"general","difficulty":"advanced","tags":["linux"],"companies":["Amazon","Google","Tesla"]},{"id":"q-4703","question":"Scenario: A Linux host running a containerized payment service experiences intermittent latency spikes under peak load. Using only default tooling, design a concrete diagnostic workflow to (1) determine if latency is CPU-, IO-, or network-bound, (2) identify the exact process, namespace, or block device involved, and (3) apply a safe mitigation (e.g., CPU shielding/cgroup throttling, IO scheduler tweak, or tc-based traffic shaping) while preserving observability. Include exact commands and example outputs?","channel":"linux","subChannel":"general","difficulty":"advanced","tags":["linux"],"companies":["Microsoft","MongoDB","Robinhood"]},{"id":"q-4748","question":"Scenario: A Linux host runs multiple containers with a unified cgroup v2 hierarchy. A payment service experiences intermittent latency under sustained load, with memory pressure but no OOM. Using only default tools, design a concrete diagnostic workflow to (1) confirm memory pressure origin (anonymous vs file cache), (2) identify the specific container/cgroup causing pressure, and (3) apply a safe mitigation (e.g., adjust memory.max for that cgroup or tune swap accounting) while preserving observability. Include exact commands and example outputs?","channel":"linux","subChannel":"general","difficulty":"advanced","tags":["linux"],"companies":["Airbnb","DoorDash"]},{"id":"q-4769","question":"On a single Linux host with NUMA, tail latency spikes under sustained load from a real-time event processor while CPU usage remains moderate. Using only default Linux tools, design a concrete diagnostic workflow to (1) confirm whether stalls are caused by Transparent Huge Pages or page cache thrashing, (2) identify the offending memory consumer and its allocations, and (3) apply a safe mitigation (e.g., disable THP, adjust swappiness, pin the hot thread) while preserving observability. Include exact commands and example outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Oracle","Plaid","Snowflake"]},{"id":"q-4818","question":"On a Linux server delivering real-time analytics, tail latency spikes to ~200 ms under sustained load. The setup uses a NVMe drive and a multi-queue NIC. Using only default Linux tools, design a concrete diagnostic workflow to (1) confirm whether latency ties to IO scheduling or NIC interrupts, (2) identify the exact IRQ or kernel thread involved, and (3) propose safe mitigations (e.g., pinning IRQs, tuning the I/O scheduler, adjusting CPU affinity) while preserving observability. Include exact commands and example outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Airbnb","MongoDB","Oracle"]},{"id":"q-4844","question":"On a Linux server hosting a Redis cluster, tail latency spikes under sustained write load while CPU and memory look normal. Using only default tools, design a concrete diagnostic workflow to (1) confirm if tail latency is due to I/O wait, NIC queuing, or page cache, (2) pinpoint the exact subsystem causing it, and (3) propose safe mitigations (IO scheduler, CPU pinning, or cache tuning) with observability. Include exact commands and sample outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Amazon","Citadel"]},{"id":"q-496","question":"How would you find all processes running on port 8080 and terminate them safely?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Apple","Google"]},{"id":"q-527","question":"How would you find and kill a process that's using port 8080 on a Linux system?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Amazon","Oracle","Two Sigma"]},{"id":"q-553","question":"You're troubleshooting a production server where a critical process keeps getting killed. How would you diagnose if it's an OOM kill versus other issues, and what specific commands would you use to investigate?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Databricks","Scale Ai","Snowflake"]},{"id":"q-580","question":"How would you find all processes using a specific port and terminate one safely?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Hashicorp","MongoDB","Twitter"]},{"id":"q-917","question":"Scenario: a Linux server hosting a web app experiences sporadic high response times during peak hours. Using only default tools and no downtime, describe a concrete, beginner-friendly diagnostic workflow to (1) determine whether CPU, memory, or I/O is the bottleneck, (2) identify the offending process, and (3) apply a safe mitigation (e.g., graceful restart) while monitoring impact. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Microsoft","PayPal","Tesla"]},{"id":"q-1044","question":"On a Linux host running multiple tenant services, peak I/O from a data ingestion daemon saturates the disk, causing latency spikes for others. Propose a production plan to isolate and throttle disk I/O across tenants using systemd slices and cgroup v2 io.max. Include concrete unit and slice snippets, per-device limits for a SATA HDD and NVMe, and a test plan using fio and iostat to verify tail latency improvements under concurrent workloads?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Databricks","Microsoft","Twitter"]},{"id":"q-1056","question":"On a Linux host, a TLS proxy reads its certificate from /etc/ssl/certs/app.crt and currently reloads by restarting, causing brief downtime during renewal. Propose a zero-downtime rotation using a systemd.path trigger that fires on a new cert symlink, with a separate service unit for ExecReload and an atomic update scheme (swap in a new cert, then switch a symlink). Include concrete unit snippets and a test plan?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Apple","Two Sigma"]},{"id":"q-1122","question":"On a Linux host running multiple ML model servers in separate systemd slices using cgroup v2, a spike in one tenant consumes memory and triggers host memory pressure despite per-tenant limits. Propose a production plan to enforce strict isolation and backpressure using memory.max and memory.high, enable group OOM behavior, and implement eviction/playback strategies. Include concrete unit and cgroup settings and a test plan with a reproducible spike and validation steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Hugging Face","MongoDB","Tesla"]},{"id":"q-1230","question":"On a Linux host with a multi-tenant workload sharing a single 10GbE NIC configured with multiple receive queues, one tenant bursts UDP traffic and starves others, causing increased latency and packet loss. Propose a production plan to enforce tenant isolation and fairness using NIC multi-queue, RSS/XPS mapping, IRQ affinity, and per-tenant cgroups (v2) with io.max and tc shaping. Include concrete steps and a test plan with realistic traffic?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["DoorDash","MongoDB","Twitter"]},{"id":"q-1327","question":"On a Linux host running multiple tenants via containers, a CPU-intensive workload from one tenant starves others. Propose a production plan to enforce CPU isolation using cgroup v2 slices, systemd integration, and per-tenant quotas; include concrete settings (cpu.max, cpu.weight), scheduling options (cpuset and isolcpus), and a test plan with realistic workloads and verification steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Bloomberg","Google","IBM"]},{"id":"q-1346","question":"On a production Linux host serving multiple ephemeral monitoring daemons, a heartbeat worker must emit a 5-second ping to a central collector. During peak load, the heartbeat misses several intervals, delaying alerts. Propose a concrete production plan to guarantee cadence and resilience using: (a) per-service CPU isolation and pinning, (b) high-priority timer/RT scheduling or systemd timers with precise tuning, (c) a fallback mechanism for missed heartbeats, and (d) a validation strategy with a controlled load. Include concrete config snippets for systemd, cpuset, and timer settings?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Citadel","LinkedIn"]},{"id":"q-1385","question":"On a Linux host running ephemeral build jobs managed by systemd, a rogue job floods /tmp with tiny files, exhausting inodes and delaying builds. Propose a production plan to isolate and bound /tmp usage and auto-clean stale files. Include: (a) per-service PrivateTmp and tmpfs /tmp with a size cap; (b) systemd-tmpfiles cleanup rules; (c) inode usage monitoring; (d) a reproducible test that proves isolation and cleanup. How would you implement this?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Anthropic","Coinbase","MongoDB"]},{"id":"q-1490","question":"On a Linux server, a small web app runs as a systemd service and reads its port and log level from /etc/myapp/config.yaml. Changes to this file must apply without dropping connections. Design a production-safe hot-reload mechanism using systemd ExecReload, a SIGHUP-based reload, and a small validation script. Include unit file snippets and a test plan with a reproducible config change and verification steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["DoorDash","Google"]},{"id":"q-1497","question":"On a Linux host running multiple tenants in systemd services sharing a single NVMe SSD and a 25Gbps NIC, a new requirement enforces strict I/O isolation to prevent a noisy tenant from stalling others. Propose a production plan to enforce per-tenant I/O isolation and backpressure using: (a) cgroup v2 io.max per service; (b) per-tenant filesystem isolation via PrivateMounts and PrivateDevices with a dedicated data path; (c) I/O scheduler tuning (none/deadline) and per-tenant block IO queuing discipline with tc; (d) a deterministic test plan with a reproducible burst scenario and rollback steps. Include concrete config snippets for systemd units and cgroups?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Discord","MongoDB"]},{"id":"q-1588","question":"On a dual-socket Linux host running PostgreSQL and a data-processor ETL job, both services contend for memory and cache across NUMA nodes; design an advanced plan to enforce NUMA-aware isolation with cpuset/memcg, disable NUMA balancing, set per-node IRQ affinity, and validate via targeted benchmarks. Include concrete commands and a test plan?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Coinbase","Twitter"]},{"id":"q-1665","question":"On a Linux server hosting a latency-sensitive database and a heavy batch analytics job that share a single 2-socket NUMA node and HDD I/O, propose a production plan to guarantee tail latency (P95/P99) for the database while allowing analytics to finish within a bounded window. Include NUMA memory binding, CPU pinning via cpuset/systemd slices, I/O throttling via cgroup v2 io.max, IRQ affinity, and a validation test plan with realistic workloads?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Apple","Snap"]},{"id":"q-1700","question":"On a Linux server running several io_uring-based data ingestors, one worker's backlog bursts, triggering tail latency spikes for others. Propose a production plan to guarantee per-worker latency and fairness using: (a) per-worker io_uring ring sizing and SQ depth; (b) per-worker cgroups v2 with io.max and task limits; (c) backpressure and pacing (budget tokens, per-worker deadlines); (d) monitoring and a reproducible test plan to validate latency under burst?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Apple","Snowflake"]},{"id":"q-1774","question":"On a Linux host, a file-watcher daemon uses inotify to monitor a large directory tree. As load grows, events start being missed and the daemon lags. Propose a production plan to keep event delivery reliable: tune inotify limits, raise file descriptor limits for the service, add a polling fallback or batching, and validate with a reproducible test that simulates churn?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Instacart","Oracle","Snowflake"]},{"id":"q-1807","question":"On a Linux host running a hot-reload deployment workflow for a critical service, mid-deploy tampering could swap the binary between copy and start, breaking integrity. Design a production-grade binary integrity strategy using Linux IMA with runtime attestation. Include how to sign binaries in CI, a signed binary repository, systemd integration (ExecStartPre), policy placement, rollback tests, and how you'd validate against tampering during deployment?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Coinbase","MongoDB"]},{"id":"q-1939","question":"On a Linux host running a web stack (Nginx proxy to a Node/Go app), the app writes to /var/log/webapp/app.log and /var/log/webapp/access.log. Bursts fill the disk and logrotation sometimes fails because the app keeps logs open. Propose a production-grade plan to ensure reliable log rotation with no data loss and no disk-full events. Include concrete logrotate config (copytruncate vs postrotate), systemd unit tweaks (Restart, ExecReload), filesystem layout advice, and a test plan to reproduce a burst and verify rotation completes without downtime?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Cloudflare","Stripe","Uber"]},{"id":"q-1954","question":"On a Linux host running three CI runners as systemd services sharing a single NVMe and a 10 Gb NIC, a long-running build on one runner starves CPU and I/O, delaying others. Propose a production plan to guarantee fair CPU and I/O while preserving peak throughput: (a) per-service CPU limits via cgroup v2 and CPU affinity; (b) per-service I/O throttling with io.max and a suitable I/O scheduler (BFQ/mq-deadline); (c) CPU pinning and IRQ isolation; (d) validation with bursts and latency metrics?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Anthropic","Apple","Databricks"]},{"id":"q-1987","question":"On a Linux host running CI jobs in rootless containers (e.g., Podman) shared across teams, a misconfigured container could attempt host escape. Propose a production hardening plan using user namespaces, rootless mode, Seccomp and AppArmor, and a tight per-job capset + cgroupv2 quotas. Include concrete commands and a rollback plan?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["IBM","Square","Twitter"]},{"id":"q-2014","question":"On a Linux host running multiple GPU-accelerated services in separate systemd slices under containerized workloads, occasional CPU contention causes tail latency spikes and timeouts for one service. Design a production plan to bound latency and preserve throughput without service interruption. Include per-slice isolation (isolcpus, cpuset), systemd slice configuration, cpu.max, and a kernel I/O scheduler tuning (mq-deadline), plus a reproducible test plan with mixed workloads and latency verification steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Adobe","Instacart","OpenAI"]},{"id":"q-2167","question":"On a KVM host running dozens of VMs with memory ballooning enabled, a burst in guest memory growth triggers host memory pressure and occasional OOM events. Propose a production-safe plan to cap balloon aggressiveness, enforce host backpressure, and prevent thrash, using libvirt XML (memory.size/currentMemory, memoryBacking, balloon device) and cgroup v2 memory.max/memory.high, plus a test plan with a reproducible spike and verification steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Google","Instacart","Snap"]},{"id":"q-2223","question":"On a Linux host running multiple CI jobs in containers, CPU contention causes builds to time out. Propose a production-ready plan to bound CPU for CI workloads using cgroup v2 and systemd slices, including concrete unit settings and a test plan with a CPU-bound load?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Citadel","Snowflake","Square"]},{"id":"q-2294","question":"On a Linux host running multi-tenant, rootless containers for isolation, a rogue tenant's DNS resolver leaks queries and inflates latency for others. Propose a production plan to enforce strict per-tenant DNS isolation without compromising performance. Include concrete steps for: (a) per-tenant network namespaces and DNS forwarders, (b) nftables rules to confine DNS to tenant resolvers, (c) container runtime config to prevent cross-tenant DNS leakage, and (d) a reproducible test plan to validate isolation under bursty DNS load?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Adobe","Citadel","Cloudflare"]},{"id":"q-2515","question":"On a Linux host running a multi-tenant container workload sharing a single 25 GbE NIC, one tenant suddenly bursts writes to a shared storage and causes CPU and FD contention, threatening SLA for others. Propose a production plan to enforce strict per-tenant isolation using cgroup v2 (cpu, memory, pids, io.max) and systemd.slice, plus network QoS with tc.clsact and per-tenant TX queues. Include concrete unit snippets and a test plan with reproducible surge and SLA verification?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Databricks","Discord","Instacart"]},{"id":"q-2538","question":"On a Linux host running a multi-tenant data ingestion pipeline and a separate real-time alerting service, ingestion bursts cause alert latency. Propose a production plan to guarantee low-latency alerts while preserving ingestion throughput using: (a) per-service CPU isolation with systemd slices and cpuset, (b) RT scheduling for the alerting process, (c) CPU affinity and NUMA bindings, (d) a validation plan with burst traffic and SLA checks?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Goldman Sachs","PayPal"]},{"id":"q-2573","question":"On a Linux host running a real-time audio processing pipeline that must meet deterministic latency, a single 8-core CPU shows occasional dropouts during peak input. Design a production plan to guarantee latency isolation for the critical path: specify hardware topology, kernel boot params (isolcpus, nohz_full), IRQ affinity, CPU sets, cgroups v2 with a dedicated rt.slice using SCHED_FIFO priorities, memory locking, and a test plan with cyclictest and a synthetic workload simulating 16-channel 192 kHz audio. What exact steps would you take and how would you verify?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Citadel","Square","Two Sigma"]},{"id":"q-2632","question":"On a Linux host running a UDP telemetry ingest pipeline at high pps, tail latency spikes under peak load threaten alerts. Design a production plan to measure end-to-end latency with eBPF (tracepoints, kprobes, uprobes) and BPF maps, identify bottlenecks (kernel vs user-space), implement mitigations (backpressure, NIC tuning, IRQ affinity, NUMA pinning), and validate with a reproducible load test and SLA targets?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Google","OpenAI","Two Sigma"]},{"id":"q-2708","question":"A Linux server hosting a small web app sees frequent 'No space left on device' errors despite disk space being free; df -h shows space but df -i shows inode exhaustion. Outline a beginner-friendly plan to diagnose and fix inode exhaustion, including exact commands to identify hotspots, recommended cleanup or restructuring, and a simple test plan to reproduce and verify the fix?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Anthropic","Instacart","Salesforce"]},{"id":"q-2787","question":"On a Linux host running several KVM guests, one guest issues heavy bursts to a shared virtio-block device, causing host scheduler starvation and jitter in the management plane. Propose a production plan to guarantee host responsiveness and deterministic per-guest I/O latency under mixed workloads. Include host CPU pinning, NUMA affinity, VFIO isolation, IRQ steering, per-guest cgroups (cpu, io.max), and storage QoS with an appropriate I/O scheduler (bfq or mq-deadline), plus a test plan using fio and cyclictest?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Apple","Hashicorp"]},{"id":"q-2931","question":"On a Linux server hosting a web app, the /var partition has space but inode usage on /var/lib/app/cache is near exhaustion due to many tiny files created daily. Propose a production plan to prevent inode exhaustion without downtime: redesign cache storage (hashed dirs), implement cleanup/rotation, enable tmpfiles purge, and a lightweight watchdog. Include concrete commands and config snippets, plus a reproducible test plan?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Bloomberg","Snowflake","Uber"]},{"id":"q-2937","question":"On a fleet of 12 Linux hosts serving a high-traffic API behind a load balancer, implement zero-downtime kernel updates using live patching (kpatch/kgraft or distro Livepatch). Design a production plan covering patch discovery, staging, canary validation, rollout strategy, rollback criteria, and instrumentation to ensure SLA adherence during updates?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Cloudflare","DoorDash","Scale Ai"]},{"id":"q-3013","question":"On a Linux host running a multi-tenant container stack over a single 25 GbE NIC, one tenant deploys a high-volume eBPF-based load balancer that exhausts kernel memory via per-connection maps, risking SLA for others. Design a production plan to cap per-tenant BPF map quotas, enforce cgroup v2 memory.max, io.max, isolate CPUs, and validate with reproducible traffic and memory graphs. Include concrete steps and a test plan?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Airbnb","Apple","Bloomberg"]},{"id":"q-3168","question":"On a Linux host running multiple long‑lived services, a single process intermittently hits the file descriptor limit during peak traffic, breaking log shipping. Design a production plan to prevent FD exhaustion: enforce per‑process limits (LimitNOFILE), raise fs.file-max, enable systemd TasksMax, isolate services with dedicated cgroups, add a watchdog that restarts on quota breach, and implement a burst FD test to verify. What concrete steps would you take?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Google","Robinhood","Salesforce"]},{"id":"q-3309","question":"On a Linux host running multiple tenants' containers sharing a single 40Gbps NIC, design a production plan for deterministic per-tenant network performance: telemetry and enforcement without sacrificing throughput. Propose a plan using XDP/eBPF to (a) tag/redirect packets to per-tenant maps, (b) enforce per-tenant egress rate via tc or XDP, (c) minimize overhead with high flow counts, (d) test with synthetic traffic. Include kernel config, BPF maps, and verification steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Goldman Sachs","Square"]},{"id":"q-3393","question":"On a Linux host acting as a 40 Gbps edge router, implement per-tenant ingress rate limiting and DDoS protection using an XDP-based fast path. Design a BPF program with per-tenant maps (tenant_id -> token bucket), dynamic quota reloads, and safe memory management. Explain topology, attach mode, safety against OOM, and a test plan with multi-tenant traffic to validate fairness and latency?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Airbnb","Amazon","LinkedIn"]},{"id":"q-3522","question":"On a Linux host running a high-volume NAT gateway with multiple containers behind a single 100Gb NIC, a surge in new connections triggers nf_conntrack contention and latency spikes. Design a production plan to guarantee sub-2ms connection-setup latency at the 99th percentile: tune nf_conntrack (max, hashsize), enable per-namespace conntrack tables, adopt an eBPF/XDP path for early filtering, isolate tenants with separate netns and tc policing, and instrument with bpftrace. Include exact steps and a test plan with 10k concurrent connections?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Citadel","DoorDash","Google"]},{"id":"q-3570","question":"On a Linux host running multiple high-sensitivity services in a NUMA system, memory pressure from one service causes paging and latency spikes for others. Propose a production plan to strictly bound per-service memory usage and preserve throughput. Include concrete steps: enabling cgroup v2 quotas (memory.max, memory.high, memory.swap.max), per-service NUMA binding, kernel tunables (vm.min_free_kbytes, vm.swappiness), and a test plan with 10k allocations and 60-minute stability checks?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Adobe","Databricks","Oracle"]},{"id":"q-3662","question":"On a Linux server running a high-volume data ingestion service that ships logs to a cloud sink, sporadic tail latency spikes occur under burst I/O. Design a production plan to guarantee deterministic latency for the ingestion path: specify hardware topology (NUMA affinities), kernel boot params (isolcpus, nohz_full), IRQ affinity, CPU sets, and a containers/systemd v2 cgroup layout with a dedicated rt.slice or critical.slice using io.max and a CPU quota, memory locking, and a concrete test plan with fio and cyclictest simulating burst I/O. What exact steps would you take and how would you verify?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Stripe","Twitter"]},{"id":"q-3726","question":"On a Linux host running an in-memory database that uses hugepages, intermittent THP defragmentation spikes cause tail latency regressions. Design a production plan to eliminate THP pauses while preserving memory efficiency. Include exact steps to disable THP, reserve hugepages for the DB, pin memory (memlock), NUMA binding and per-service cgroups, and a test plan proving p95 latency stays under target at 50k qps; add rollback?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Cloudflare","Lyft","NVIDIA"]},{"id":"q-3866","question":"On a Linux host running multiple KVM guests with shared storage, a noisy neighbor VM spikes I/O and delays CI workloads on a critical VM. Propose a practical plan to guarantee sub-5ms tail I/O latency for that VM, including concrete steps to (a) switch to BFQ I/O scheduler, (b) enforce per-VM I/O limits with cgroup v2 io.weight/io.max, (c) isolate disks or queues for the critical VM, and (d) a test plan with reproducible load and latency verification?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Discord","LinkedIn","Zoom"]},{"id":"q-3875","question":"On a Linux host handling high-velocity production workloads, implement a low-overhead tracing plan to debug intermittent latency spikes without impacting throughput? Design an approach using eBPF/BPFtrace to capture key latency paths, isolate tenants, and trigger alerts for SLA breaches. Include what to instrument, data structures, sampling strategy, and a test plan?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Instacart","Plaid","Two Sigma"]},{"id":"q-3908","question":"On a Linux host running a high-throughput log ingestion stack (rsyslog -> local NVME store -> analytic pipeline) experiencing intermittent 100–300 ms I/O latency under peak writes. Design a production plan to diagnose and eliminate storage I/O bottlenecks while preserving log reliability. Include: I/O schedulers, per-disk vs per-tenant I/O shaping (cgroups v2 io.max), test plan with fio/workloads, and validation steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Google","Netflix"]},{"id":"q-3958","question":"On a Linux host running both a real-time data-feed and a batch analytics job sharing one NVMe pool, bursts trigger 2–6 ms stalls. Design a plan to guarantee deterministic tail latency for the data-feed: per-tenant IO isolation (io.max), CPU pinning, NUMA-aware memory, appropriate I/O scheduler, and a test harness with mixed READ/WRITE fio workloads to verify p99 latency and failure modes?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Plaid","Two Sigma"]},{"id":"q-4020","question":"On a Linux host running a multi-tenant data-collection service, tail latency spikes appear under 99th percentile during bursts. Outline a production plan to diagnose and mitigate root causes using eBPF tracing (bpftrace), per-tenant metrics, and strict QoS isolation. Include steps to install tracing, collect sched/net/block latency per tenant, enforce per-tenant cgroups v2 with cpu.max and io.max, and validate with synthetic bursts. What exact steps would you take and how would you revert if latency worsens?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["LinkedIn","Snowflake","Stripe"]},{"id":"q-4151","question":"On a Linux host running a high-fidelity trading UI service that must meet a 5 ms end-to-end latency SLA under mixed workloads, design a production plan to guarantee deterministic latency for the critical path. Specify hardware topology (NUMA, CPU pinning), kernel params (isolcpus, nohz_full, rcu_nocb), per-group scheduling using SCHED_DEADLINE with deadlines, IRQ affinity, cgroups v2 (rt.slice) and memory bindings, plus a test plan using cyclictest and a synthetic tick workload. What exact steps would you take and how would you verify?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Salesforce","Snap","Snowflake"]},{"id":"q-4264","question":"On a fleet of Linux CI workers, bursts from a single job spike CPU and memory, starving other jobs. Propose a beginner-friendly plan to isolate each job using a dedicated systemd slice and cgroup v2, enforce per-job CPU quotas, limit PIDs, and ensure quick reversion if a burst is detected. Include concrete unit settings, a test plan with a 5-minute burst, and commands to verify outcomes?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Databricks","Discord"]},{"id":"q-4307","question":"On a Linux host, a long-running batch job scheduled via a systemd timer occasionally drifts under load, causing data window misses. Propose a production plan to ensure timely execution and reliability. Include concrete timer settings (OnCalendar, Persistent, AccuracySec, RandomizedDelaySec), service isolation (LimitNOFILE, CPUQuota, PrivateTmp, ProtectSystem), a lightweight monitoring/alerting approach, and a rollback path if drift worsens. Provide exact commands and config snippets?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Amazon","Meta"]},{"id":"q-4400","question":"On a Linux host, a periodic cleanup script used by a systemd timer occasionally fails because it relies on a writable /tmp that can be cleaned out during bursts. Propose a production-safe migration to a systemd-timer+service with isolation and idempotence. Include concrete unit settings (RuntimeDirectory, PrivateTmp, User/Group, WorkingDirectory, Restart), a minimal script pattern, and a test plan to reproduce and validate reliability?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Google","Instacart","NVIDIA"]},{"id":"q-4478","question":"On a Linux server hosting a lightweight web app, a bursty logging workflow writes many small files under /var/log/app and, during peak bursts, inode exhaustion occurs even though disk space remains. Propose a practical plan to prevent inode pressure and keep logs reliable. Include concrete steps for quick triage, log rotation strategies, filesystem choices or partitioning, and a test plan to reproduce bursts and verify stability?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Microsoft","NVIDIA","Snap"]},{"id":"q-4679","question":"On a Linux-based distributed analytics cluster spanning data-centers, time synchronization is critical for event ordering. A misalignment >200 µs breaks correlation across logs and triggers alert drift. Design a production-ready time-sync plan using IEEE 1588v2 (PTP) with hardware timestamping. Include transport choice (dedicated network vs grandmaster), NIC/PHC setup, daemon config (ptp4l/chronyd), fault tolerance, monitoring, and a test plan with cyclictest and log correlation?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Hashicorp","Hugging Face","Oracle"]},{"id":"q-4752","question":"On a Linux host serving a GPU-accelerated real-time inference pipeline, multiple workloads share a single 2-socket system. Occasional stalls increase tail latency of the critical path under peak load. Design a production plan to guarantee deterministic latency for the critical path. Include hardware topology (NUMA binding), kernel/driver knobs (nohz_full, isolcpus, IRQ affinity), per-workload scheduling (SCHED_DEADLINE with deadlines), cgroups v2 (io.max, memory.min), and a validation plan with synthetic bursts. What exact steps would you take and how would you verify?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["DoorDash","NVIDIA","Tesla"]},{"id":"q-881","question":"On a Linux host, a long-running daemon writes to `/var/log/myapp.log` and is managed by systemd, but log rotation occasionally causes logging to stop after rotation. Propose a practical fix to ensure logging continues after rotation without restarting the daemon. Include the exact approach and a sample logrotate config snippet and testing steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Apple","Meta","Plaid"]},{"id":"q-888","question":"A production Linux host runs a data-backup agent that uses /var/lib/backup/backup.lock to enforce a single instance. Sometimes a stale lock remains after a crash, blocking new runs; the agent also leaves non-terminating children on stop, risking partial backups. Propose a systemd‑based lifecycle fix: ensure one instance, auto-clean stale lock, and graceful stop with timeout and fallback to kill. Include concrete unit snippets and verification steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Goldman Sachs","Snowflake"]},{"id":"q-961","question":"On a Linux host, a memory-hungry log-harvester daemon managed by systemd sporadically triggers the kernel OOM killer during peak load, crashing the service and delaying alerts. Propose a production-safe plan to prevent OOM termination while preserving throughput. Include concrete systemd settings (MemoryLimit, MemorySwapMax, OOMScoreAdjust, Restart), kernel tuning (swap, swappiness), and a test plan with a reproducible high-memory scenario and verification steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Databricks","Two Sigma"]},{"id":"linux-foundation-sysadmin-networking-1768260262823-4","question":"You suspect ARP storms on a host with interface eth1. To quickly verify ARP traffic and identify abnormal ARP activity on that interface, which command should you run?","channel":"linux-foundation-sysadmin","subChannel":"networking","difficulty":"intermediate","tags":["Networking","ARP","Linux","Kubernetes","certification-mcq","domain-weight-12"],"companies":null},{"id":"q-199","question":"When deploying LLM inference with vLLM and Triton Inference Server, how do you handle request batching across multiple GPU nodes while maintaining sub-100ms latency for individual requests?","channel":"llm-ops","subChannel":"deployment","difficulty":"advanced","tags":["vllm","tgi","triton","onnx"],"companies":["Amazon","Google","Meta","Microsoft","NVIDIA"]},{"id":"q-1045","question":"You're running a beginner LLM inference API used by multiple clients. Design a low-cost, safe plan to implement per-tenant model versioning and zero-downtime hot-swapping. Include routing to the correct version, how you deploy new versions, rollback strategy, and observability to verify no regressions before full rollout?","channel":"llm-ops","subChannel":"general","difficulty":"beginner","tags":["llm-ops"],"companies":["Discord","NVIDIA"]},{"id":"q-1060","question":"You're delivering a privacy-preserving, regionally scoped, multi-tenant LLM assistant used by teams across geographies. Describe an end-to-end design that ensures tenant isolation, data residency, and per-tenant policy enforcement while meeting sub-200ms latency for common prompts. Include routing, key management, redaction, auditability, and deletion workflows, plus how you'd validate compliance across regions?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["LinkedIn","NVIDIA","Zoom"]},{"id":"q-1147","question":"You're operating a multi-tenant, multi-region LLM inference service for regulated financial firms. Design an architecture that enforces per-tenant data residency and per-tenant model pools, plus dynamic model-version rollout with feature flags and safe rollback. Describe policy-driven routing, on-the-fly prompt sanitization, observability, and incident response. Include concrete data-plane components and a rollout sequence for a new model version?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Apple","Goldman Sachs","Plaid"]},{"id":"q-1263","question":"You operate a global LLM inference service across three regions and must upgrade models with zero downtime. Describe a concrete plan for rolling upgrades with canaries, traffic-splitting, warm-up, health checks, and fast rollback, ensuring SLA adherence and minimal cold-start impact during the switch?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["Databricks","Slack","Snap"]},{"id":"q-1640","question":"You're launching a beginner-friendly LLM chat in a mobile-first app. To balance latency and privacy, design an edge-first routing: small prompts stay on-device; larger or sensitive prompts go to the cloud. Describe concrete components, a simple policy rule, data flow, and how you'd test it for privacy and latency before release?","channel":"llm-ops","subChannel":"general","difficulty":"beginner","tags":["llm-ops"],"companies":["Amazon","Apple","Snap"]},{"id":"q-1695","question":"Design a compliant, auditable LLM inference pipeline for a regulated financial platform that must deliver per-transaction data lineage, prompt provenance, and immutable end-to-end logging with 24-month retention while keeping latency under 150 ms end-to-end. Describe data flows, encryption, storage, and how you test for data leakage and drift?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Airbnb","Oracle","PayPal"]},{"id":"q-1858","question":"Design a beginner-friendly chat moderation assistant for a live-stream app with edge-first routing: short prompts stay on-device; longer or risk-prone prompts go to cloud. Specify the minimal architecture, a concrete routing rule (token threshold and risk score), data flow with redaction, and a practical test plan to validate latency (<200ms on-device, <600ms cloud) and privacy?","channel":"llm-ops","subChannel":"general","difficulty":"beginner","tags":["llm-ops"],"companies":["Cloudflare","Databricks"]},{"id":"q-2003","question":"Design a tenancy-aware LLM inference layer where 60 regional tenants share a single GPU pool but must enforce per-tenant data isolation, prompt sanitization, and per-tenant model versioning. When burst traffic occurs, guarantee 95th percentile latency under 300 ms while preventing cross-tenant data leakage. Describe data flows, policy evaluation, routing, and rollback?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["Amazon","Google","IBM"]},{"id":"q-2146","question":"Design a hybrid edge-cloud LLM inference system where tenants move between corporate networks and remote locations. How would you enforce per-tenant data locality, latency budgets, model versioning, and graceful failover while keeping observability clear and simple?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["Discord","Google","Snap"]},{"id":"q-2190","question":"Design a lightweight PII redaction gate that runs before prompts reach the model in a multi-tenant LLM-ops pipeline. Describe concrete redaction rules, per-tenant policy config, data locality considerations, and how you'd validate latency and redaction accuracy with a minimal test suite?","channel":"llm-ops","subChannel":"general","difficulty":"beginner","tags":["llm-ops"],"companies":["Databricks","Goldman Sachs","Tesla"]},{"id":"q-2201","question":"Design a per-tenant, order-preserving prompt queue for a multi-tenant LLM chat backend. Ensure prompts from different tenants can be processed in parallel, but each tenant's prompts are served strictly in arrival order. Describe data models, queueing strategy, and a minimal Python asyncio prototype showing enqueue, dispatch, and a mock model call with per-tenant isolation and timeouts?","channel":"llm-ops","subChannel":"general","difficulty":"beginner","tags":["llm-ops"],"companies":["Robinhood","Salesforce","Square"]},{"id":"q-2309","question":"In a multi-tenant LLM-ops gateway used by Oracle and Apple, design a real-time policy-driven guardrail that prevents tenant data leakage via prompt injection, enforces per-tenant data locality and content policies, and streams a tamper-evident audit log. Describe data models, policy evaluation flow, latency targets (<50 ms per prompt), and a minimal Rust/protobuf prototype snippet for policy evaluation?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Apple","Oracle"]},{"id":"q-2339","question":"In a multi-tenant LLM-ops platform that supports dynamic model tiering (tiny/fast vs large/accurate) with per-tenant QoS budgets, how would you design the model selection, routing, and accounting to meet SLAs while minimizing cross-tenant warmups? Include the data model, API surface, and a minimal config snippet showing how tenants express budgets and tier preferences?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Airbnb","MongoDB","Square"]},{"id":"q-2407","question":"Design a per-brand, hot-swappable steering layer between gateway and models. JSON-based policies enforce safety and tone per brand; compute a per-prompt risk score in under 20 ms; policy versions are immutable with a blue/green rollout for rollback; describe data models, policy evaluation flow, rollback strategy, and a minimal Rust/WASM prototype for policy evaluation?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Adobe","Instacart","Lyft"]},{"id":"q-2746","question":"Design a beginner-friendly, multi-tenant LLM gateway workflow focused on per-tenant prompt provenance and data minimization. Describe a minimal data model for tenant policy and provenance, the runtime flow (pre-processing, policy eval, redaction, logging, routing), and a short practical example of how you'd redact PII before logging. Include latency target under 100 ms per prompt?","channel":"llm-ops","subChannel":"general","difficulty":"beginner","tags":["llm-ops"],"companies":["Hashicorp","Meta","Microsoft"]},{"id":"q-2873","question":"Design a production LLM-ops gateway for a multi-tenant environment (Instacart, Oracle, Apple) that routes prompts to three model variants (base, tuned, safety-guarded) with per-tenant policies and strict data locality. Include real-time A/B canary rollouts, drift/safety monitoring, latency targets (<200 ms), and a minimal test suite to validate both performance and policy conformance?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Apple","Instacart","Oracle"]},{"id":"q-2983","question":"Design a per-tenant dynamic policy firewall for an enterprise LLM-ops pipeline that guarantees post-processing independence and data locality while supporting auditability and drift detection. Include a policy-as-code schema, per-tenant retention constraints, end-to-end provenance, and a testing plan with concrete metrics?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Cloudflare","Instacart","Microsoft"]},{"id":"q-3003","question":"You're running a multi-tenant LLM service for Snap and Zoom. Design a real-time policy drift containment system that monitors tenants with different safety policies, automatically detects drift in safety thresholds, and degrades to a safer variant or blocks prompts if drift exceeds a per-tenant SLA. Include telemetry, rollback plan, and a minimal test harness?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Snap","Zoom"]},{"id":"q-3030","question":"Design a per-tenant model versioning and policy-drift system for a globally distributed LLM inference service used by regulated tenants (e.g., fintechs, exchanges). Requirements: pin tenants to specific model versions with staged canary rollouts; detect policy drift in near real-time by comparing outputs to policy gold references; auto-rollback to a previous version on drift/quality triggers; enforce strict data locality and tamper-evident logging. Outline architecture, data flows, and observability, and provide a minimal Python prototype for drift checking?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["Slack","Square","Two Sigma"]},{"id":"q-3059","question":"You’re deploying a beginner-friendly, multi-tenant LLM gateway for Instacart, Two Sigma, and Cloudflare. Design a minimal per-tenant prompt provenance system that cryptographically attests both the prompt and the model output, stores proofs in an append-only log, and allows auditors to verify integrity under latency target (<150 ms). Outline data model, flow, and a small prototype approach?","channel":"llm-ops","subChannel":"general","difficulty":"beginner","tags":["llm-ops"],"companies":["Cloudflare","Instacart","Two Sigma"]},{"id":"q-3261","question":"Design a data-provenance and policy-driven routing layer for a multi-tenant LLM service that handles PII and non-PII prompts. Requirements: per-tenant data locality, immutable audit trail with tamper-evident hashes, pre-flight privacy policy checks, route to one of three model variants (base, tuned, safety-guarded), real-time drift monitoring, and end-to-end latency under 300 ms. How would you implement this?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["Anthropic","OpenAI","Robinhood"]},{"id":"q-3317","question":"In a multi-tenant LLM gateway serving tenants like Cloudflare and Airbnb, design a per-tenant prompt provenance and model-output provenance system with tamper-evident, append-only logs. Describe the data model, log routing, cryptographic hash chaining, and how you'd verify integrity at query time while keeping latency under 200 ms. Include tenant data isolation and retention considerations?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Airbnb","Cloudflare"]},{"id":"q-3341","question":"In a multi-tenant LLM gateway serving tenants such as Microsoft and Plaid, design a policy-driven prompt provenance and data minimization pipeline. Explain (1) per-tenant policy schema (retention, redaction, provenance scope, data locality), (2) real-time policy evaluation and routing with canary rollouts, (3) tamper-evident logging with prompt hashes, and (4) a minimal test plan to detect drift, latency impact, and policy violations under sub-200ms latency?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Microsoft","Plaid"]},{"id":"q-3456","question":"Design a per-tenant prompt sanitization gate that runs before prompts reach the LLM in a multi-tenant gateway. Each tenant publishes a tiny policy DSL (redact emails, strip SSNs, mask tokens). Describe data flow, a minimal DSL evaluator, and a test harness to verify correctness and latency under 60 ms?","channel":"llm-ops","subChannel":"general","difficulty":"beginner","tags":["llm-ops"],"companies":["Google","Microsoft","Zoom"]},{"id":"q-3463","question":"Design a pragmatic, production-ready framework to automatically validate model outputs against per-tenant safety and privacy policies in a multi-cloud, multi-region LLM-ops pipeline. Include: per-tenant policy schemas, drift-detection triggers for policy compliance, rollback/roll-forward strategies, observability KPIs, and a concrete testing plan with a minimal but representative data set?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Cloudflare","Meta","Square"]},{"id":"q-3737","question":"Design a low-latency, per-tenant prompt provenance and data-minimization gate for a multi-tenant LLM-ops pipeline serving Amazon and Lyft. The gate must enforce per-tenant policies (PII masking, data retention, and prompt lineage), operate on streaming ingestion with <150 ms tail latency, support real-time drift detection, a minimal test suite, and observability hooks. How would you implement it, and what metrics would you track?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Amazon","Lyft"]},{"id":"q-3782","question":"For a beginner-friendly, multi-tenant LLM gateway powering apps like rideshare and chat, design a lightweight per-tenant prompt templating system that enforces tenantId and intent in preprocessing. Define the data model, how templates are applied, and a minimal test plan to verify policy enforcement and latency impact. Include a concrete example?","channel":"llm-ops","subChannel":"general","difficulty":"beginner","tags":["llm-ops"],"companies":["Cloudflare","Discord","Tesla"]},{"id":"q-3824","question":"In a beginner-friendly, multi-tenant LLM gateway, design a per-tenant prompt mutation gate that shortens prompts to a configurable maxTokens by applying deterministic paraphrase and selective truncation while enforcing policy tokens and preserving intent. Specify data models, mutation order, concrete examples, and a minimal test plan with synthetic prompts to validate token caps and semantic preservation?","channel":"llm-ops","subChannel":"general","difficulty":"beginner","tags":["llm-ops"],"companies":["Coinbase","MongoDB","Tesla"]},{"id":"q-3930","question":"Design a cost-aware, multi-tenant LLM routing system that assigns prompts to model variants (base, tuned, safety-guarded) based on per-tenant budgets and risk profiles. Explain the decision engine, data-minimization, and per-tenant policy enforcement. Ensure ≤200ms latency, deterministic fail-safe routing for violations, and describe a minimal synthetic tenant dataset for validation?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Amazon","Bloomberg","Google"]},{"id":"q-4037","question":"Design a real-time prompt de-identification and redaction pipeline for a multi-tenant LLM gateway that processes user queries containing PII. Requirements: per-tenant privacy policies, field-level redaction templates, immutable audit trails, latency under 250 ms, and a test suite proving redaction correctness and no data leakage across model variants. Include synthetic data examples and a plan for region-localized data handling?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["NVIDIA","Salesforce","Tesla"]},{"id":"q-4092","question":"Design a beginner-friendly LLM-ops gateway feature for per-tenant prompt provenance and auditability. Implement a lightweight provenance store (append-only) recording tenantId, promptHash, modelVariant, timestamp, and responseId; expose an audit API with tenant-scoped access and an integrity check (hash chain). Include a minimal data model, a sample record, and a basic test plan to verify tamper-evidence and privacy constraints?","channel":"llm-ops","subChannel":"general","difficulty":"beginner","tags":["llm-ops"],"companies":["Apple","Cloudflare"]},{"id":"q-4114","question":"Design a cost-aware, multi-tenant inference scheduler for an LLM-ops gateway that guarantees per-tenant SLA (p95 latency under 180ms) while optimizing GPU utilization across regions and clouds. Include: per-tenant quotas and budgets, priority classes with preemption, burst handling, cross-region routing, and a minimal test plan validating SLA, cost, and fairness?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Meta","MongoDB","Twitter"]},{"id":"q-4145","question":"Design a per-tenant, cost-aware routing layer for an LLM gateway serving tenants with distinct budgets and SLAs (e.g., MongoDB, Robinhood-like brokerage, OpenAI-like services). Route prompts to three variants (base, tuned, safety-guarded) to respect monthly budgets while meeting latency targets (<200 ms p95) and data locality. Describe data model, burst handling, scaling, and a test plan with realistic traffic patterns and budget scenarios?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["MongoDB","OpenAI","Robinhood"]},{"id":"q-4248","question":"Design a real-time, per-tenant streaming LLM-ops gateway that ingests prompts, enforces per-tenant policies (data locality, PII redaction, budget enforcement, latency targets), routes to one of three model variants (base, tuned, safety-guarded), and guarantees exactly-once, idempotent delivery with backpressure and circuit breakers. Include chaos-testing plans for outages and partial failures?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["MongoDB","Netflix","Twitter"]},{"id":"q-4309","question":"Design an autonomous per-tenant prompt batching and routing layer for a high-throughput LLM gateway servicing tenants with distinct budgets and SLAs. The system must batch prompts by tenantId and urgency within a small global window, route batches to base, tuned, or safety-guarded variants, enforce per-tenant budgets with backpressure, and guarantee data isolation across tenants. Provide data model, batching windowing strategy, routing rules, drift monitoring, and a realistic test plan with traffic patterns and latency targets?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["LinkedIn","Twitter"]},{"id":"q-4437","question":"Design a cryptographically verifiable prompt provenance ledger for a multi-tenant LLM-ops gateway serving MongoDB-like analytics, Google-scale workloads, and Robinhood-like brokerage. Record tenant_id, prompt_hash, model_variant, latency, and outputs_hash in an append-only log with per-tenant redaction. Describe data model, signing scheme, shard strategy, and a test plan under 10k RPS with p95 latency <200 ms?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Google","MongoDB","Robinhood"]},{"id":"q-4463","question":"Design a per-tenant risk-scoring proxy for an LLM gateway that evaluates prompts against tenant-specific policies in real time, then routes to base, tuned, or safety-guarded variants. Include data model, policy versioning, drift detection, rollback plan, latency targets (<200 ms p95), and a test plan with synthetic prompts and live traffic scenarios?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["Cloudflare","Databricks","Two Sigma"]},{"id":"q-4476","question":"Design an end-to-end tracing and data-lineage approach for a multi-tenant LLM-ops gateway. How would you propagate tenantId across services, ensure PII redaction in logs, and validate policy-compliant, privacy-preserving data flow? Include a minimal OpenTelemetry config, a concrete message flow (tenant A -> gateway -> model -> response), and a sample trace/log snippet?","channel":"llm-ops","subChannel":"general","difficulty":"beginner","tags":["llm-ops"],"companies":["IBM","Slack","Snap"]},{"id":"q-4568","question":"Design a multi-tenant audit-and-provenance layer for a production LLM-ops gateway servicing MongoDB, Robinhood-like finance, and Apple-scale workloads. Describe how to capture end-to-end data lineage for every prompt and model output, enforce per-tenant retention and data locality, provide tamper-evident logs, and validate integrity under high throughput (e.g., 10k RPS)?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Apple","MongoDB","Robinhood"]},{"id":"q-4652","question":"Design a cryptographic provenance and data locality framework for a multi-tenant LLM-ops gateway that ensures per-tenant prompt provenance, attestations, and tamper-evident auditing while meeting region-local data handling and latency targets. Outline the crypto stack, data-partitioning, tenant onboarding, and a minimal test plan?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["DoorDash","Hashicorp"]},{"id":"q-4668","question":"Design a deterministic, time-windowed caching layer for an LLM gateway that serves multiple tenants with distinct SLAs and budgets. The cache stores top-N prompts and outputs, with per-tenant policy versions and data locality constraints. Describe cache key schema, eviction policy combining TTL and per-tenant budget burn-rate, cache coherence during policy changes, and a test plan with realistic workloads to validate latency and cost impact?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["Cloudflare","Instacart","Salesforce"]},{"id":"q-467","question":"You're deploying a LLM inference service that must handle 10,000 RPS with <100ms latency. How would you design the architecture to balance cost, performance, and reliability?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["Discord","Microsoft","Netflix"]},{"id":"q-497","question":"How would you design a distributed inference serving system for LLMs that handles 100K RPS with sub-100ms latency while managing GPU memory fragmentation and ensuring high availability?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Hugging Face","Tesla"]},{"id":"q-528","question":"You're deploying a LLM inference service that must handle 1000 concurrent requests with <500ms latency. Your current setup uses a single GPU with vLLM. How would you architect the system to meet these requirements?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["Airbnb","Hugging Face","Robinhood"]},{"id":"q-554","question":"You're deploying a multi-tenant LLM inference service that must handle 10,000 concurrent requests with sub-100ms latency. How would you design the request routing, model loading strategy, and autoscaling to meet these SLAs while optimizing GPU utilization?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["DoorDash","IBM"]},{"id":"q-581","question":"How would you design a production LLM inference pipeline that handles 10K RPS with sub-200ms latency while managing GPU memory fragmentation and cold start issues?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Airbnb","Google"]},{"id":"q-849","question":"You operate a dual-tenant LLM inference service for sensitive internal docs and external users. Design a policy-driven routing and isolation architecture that guarantees tenant data separation, per-tenant model pools, on-the-fly prompt sanitization, and strict latency budgets under burst traffic. Include observability, data handling, and fail-open/closed strategies?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Cloudflare","Google","Instacart"]},{"id":"q-252","question":"What are the key techniques and trade-offs for optimizing large language models in production, including quantization strategies and their impact on performance?","channel":"llm-ops","subChannel":"optimization","difficulty":"beginner","tags":["quantization","pruning","distillation"],"companies":["Amazon","Apple","Google","Meta","Microsoft","NVIDIA"]},{"id":"q-422","question":"You're building a production LLM service handling 10K requests/second with transformer models experiencing memory spikes during multi-head attention. How would you optimize memory usage while maintaining throughput and latency requirements?","channel":"llm-ops","subChannel":"optimization","difficulty":"advanced","tags":["transformer","attention","tokenization"],"companies":null},{"id":"q-1177","question":"Scenario: a high-throughput edge service must enforce precise timeouts for thousands of connections. Design a lock-free, per-core timer wheel that manages up to 1,000,000 timers with microsecond granularity on a 4-socket server. API: add_timer(id, due_us, cb, ctx), cancel_timer(id), tick(). Requirements: no global locks, handle cancellation safely, cache-friendly layout, and crash-safe recovery. Include pseudo-code for add_timer, cancel_timer, and tick, plus a microbenchmark plan?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["DoorDash","Robinhood","Tesla"]},{"id":"q-1261","question":"Implement a cache-friendly transpose using tile-based approach for an N×N matrix with N a power of two. Provide a function to transpose A into B using 32×32 blocks, and a minimal test harness validating correctness. Explain how tiling reduces cache misses and how to pick block size relative to L1/L2 caches. Include a microbenchmark plan comparing with a naive transpose?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Microsoft","Plaid","Uber"]},{"id":"q-2105","question":"Design and implement a NUMA-aware per-thread arena allocator in C for a 2-socket server. Features: per-core cache-line-aligned pools, 128-byte padding to prevent false sharing, fast paths for 8/16/32-byte blocks, per-size freelists, and a cross-thread deallocation quarantine. Provide API (init, alloc, free, destroy) plus a minimal test harness and a microbenchmark plan comparing intra- vs inter-NUMA performance against malloc?","channel":"low-level","subChannel":"general","difficulty":"intermediate","tags":["low-level"],"companies":["Databricks","Two Sigma","Uber"]},{"id":"q-2340","question":"Design and implement a tiny fixed-block allocator in C for a 64KB arena to serve 32-byte objects. Provide API: init_allocator(uint8_t* arena, size_t size), void* alloc32(), void free32(void*). Use an in-block free list (8-byte header) and 24-byte payload per block, with the arena head at offset 0. Explain alignment, fragmentation, and how it compares to malloc. Include a minimal test plan?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Goldman Sachs","Meta","Tesla"]},{"id":"q-2361","question":"Design a NUMA-aware fixed-size allocator in C++ for objects of 64 bytes, with per-NUMA-node free lists and a lightweight cross-node migration policy; describe structure, allocation/free, and how to avoid inter-node contention and false sharing; provide a small code sketch and a validation plan?","channel":"low-level","subChannel":"general","difficulty":"intermediate","tags":["low-level"],"companies":["Robinhood","Two Sigma"]},{"id":"q-2448","question":"Design a NUMA-aware, lock-free work-stealing deque with per-NUMA-node local queues and cross-node stealing to sustain high throughput on a 2-socket server. Provide per-slot layout, push/pop/steal pseudo-code with sequence counters, CAS, and memory fences; handle wrap-around; describe hazard pointers/epochs for lifetime management; discuss cache-line padding and validation under cross-NUMA contention?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Microsoft","Plaid"]},{"id":"q-2517","question":"Design a beginner-friendly micro-benchmark in C to quantify false sharing. Create two layouts of per-thread counters: one unpadded (counters reside on the same cache line) and one padded to a full 64-byte cache line. Two threads update their own counter 10,000,000 times each. Measure throughput with clock_gettime(CLOCK_MONOTONIC) and compare results, explaining the impact of padding on cache coherence in multi-core systems?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Netflix","Oracle","Uber"]},{"id":"q-2688","question":"Design and implement a per-thread arena allocator for 16-byte blocks from a fixed 256KB pool. Provide API: init_arena(void* arena, size_t size), void* alloc16(), void free16(void*). Use a global bitmap to track 16,384 blocks and a thread-local cache of 8 blocks to reduce contention. Explain alignment, fragmentation, and how this approach compares to malloc under multi-core contention. Include a minimal test harness?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Coinbase","Google","OpenAI"]},{"id":"q-2699","question":"Design a bounded, lock-free, multi-producer/multi-consumer queue where each slot holds a variable-length payload with a 4-byte length prefix. Use a power-of-two ring, per-slot sequence numbers, and epoch-based reclamation. Provide enqueue/dequeue pseudo-code, describe memory layout, backpressure handling, and a microbenchmark plan under high contention?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Amazon","Netflix","Plaid"]},{"id":"q-2750","question":"On a dual-socket NUMA server with persistent memory (PMEM), design a crash-consistent, lock-free ring buffer that supports multiple producers and a single consumer. Data is written to PMEM and a compact in-memory log captures operations for recovery after power loss. Include slot layout, enqueue/dequeue steps, memory ordering, and a recovery protocol, plus a microbenchmark plan to verify throughput and durability under power interruption?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Anthropic","NVIDIA","Stripe"]},{"id":"q-2916","question":"Design and implement a 256-entry software TLB for 32-bit virtual addresses mapping to 4KB pages. Provide API: tlb_init(), tlb_lookup(uint32_t va, uint32_t* pa), tlb_insert(uint32_t va, uint32_t pa). Use linear probing with an LRU replacement policy. Explain interaction with a two-level page table and a microbenchmark plan to measure hit rate under changing working-set sizes?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Google","NVIDIA","Zoom"]},{"id":"q-3079","question":"Design a cross-process, lock-free bounded queue in shared memory for two processes on a NUMA system, with 1<<18 slots of 128 bytes. Use per-slot 64-bit sequence numbers and head/tail indices. Producers spin until slot.seq==tail, write payload, and publish slot.seq=tail+1 with a release; consumers read with acquire, validate, and advance head. Reclaim via epoch-based scheme across both processes. Microbenchmark: vary producers/consumers, report p95/p99 latency and sustained throughput; check data integrity under contention?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Hugging Face","Lyft","Microsoft"]},{"id":"q-3280","question":"Design and implement a lock-free concurrent map using a skip-list with probabilistic levels, supporting O(log n) inserts, finds, and deletes under high contention. Use hazard pointers for memory reclamation and marked pointers for logical deletion. Provide node layout with per-level forward pointers, random level generator, and pseudocode for insert, search, and delete that avoids ABA. Include a minimal microbenchmark plan to validate throughput and correctness under heavy parallelism?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Airbnb","Slack","Twitter"]},{"id":"q-3455","question":"Implement a simple spinlock in C using C11 atomics. Provide lock and unlock functions for a shared 32-bit counter. Use an exponential backoff (pause) when contention is detected. Then write a small test with four threads each incrementing the counter one million times. Explain how backoff avoids livelock and how you would measure contention?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["DoorDash","Google"]},{"id":"q-3478","question":"Design a small, beginner-friendly spinlock in C to protect a shared 64-bit counter on a multi-core system. Implement init(), lock(), unlock() using C11 atomics (memory_order_acquire for lock, memory_order_release for unlock). Provide a minimal test plan with 8 threads incrementing the counter and a short fairness note. How would you verify correctness under contention?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Apple","Stripe"]},{"id":"q-3485","question":"Design a crash‑consistent, lock‑free, bounded ring buffer in shared memory that acts as a persistent queue across restarts. It must be backed by a memory‑mapped file, use per‑slot CRCs, and a two‑phase commit so writers can resume after power loss. Describe enqueue/dequeue algorithms, restoration after crash, and how you handle memory ordering, reclamation of retired descriptors, and backpressure. Include a microbenchmark plan?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Square","Stripe"]},{"id":"q-3813","question":"Design a crash‑consistent, lock‑free allocator over persistent memory (PMEM). Provide API: init_pmem_pool(void* base, size_t size), void* pmem_alloc(size_t n), void pmem_free(void* p). Use a log-structured approach with per‑block headers (offset, version, CRC) and a bitmap to track free blocks. Explain recovery after power loss, how to prevent double frees, and minimize writes. Include pseudo-code for alloc and recovery?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Amazon","Databricks","Google"]},{"id":"q-3917","question":"Implement a portable spinlock in C using stdatomic.h. Provide acquire() and release() with an exponential backoff strategy for contention. Explain memory ordering choices (memory_order_acquire on acquire and memory_order_release on release), and how you would test correctness with two threads incrementing a shared counter to 1e7. Include a minimal C snippet and a test harness plan?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Citadel","Google","Tesla"]},{"id":"q-3978","question":"Write a cooperative user-space scheduler that preempts green threads using a timer signal. In a single process, implement N coroutines with contexts (ucontext or setjmp/longjmp), maintain a ready queue, and use a per-thread timer (ITIMER_REAL) to trigger SIGALRM, performing yield by swapping to the next context. Include stack guard pages, an async-signal-safe enqueue/dequeue, and discuss race conditions, reentrancy, and performance implications?","channel":"low-level","subChannel":"general","difficulty":"intermediate","tags":["low-level"],"companies":["Google","Microsoft","Square"]},{"id":"q-4008","question":"Design a per-core, bounded publish-subscribe queue for a NUMA multi-socket server. Producers write to local rings and publish to a shared log with per-slot sequence numbers. Provide data structures, enqueue/dequeue pseudo-code, handle wrap-around, memory ordering on x86-64, and validation plan for throughput and message ordering under cross-core contention?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Databricks","Instacart","Lyft"]},{"id":"q-4021","question":"Implement a NUMA-aware fixed-size block allocator for 64-byte blocks in a multi-threaded process. Each thread uses a per-NUMA-node cache and refills from a central pool on miss; design lock-free freelists with atomic operations, ensure alignment and quick-path fast-path, and discuss fragmentation and cross-socket traffic. Include a microbenchmark plan comparing against a naive malloc?","channel":"low-level","subChannel":"general","difficulty":"intermediate","tags":["low-level"],"companies":["Anthropic","Meta","Netflix"]},{"id":"q-4110","question":"Design a NUMA-aware, lock-free free-list based allocator in C for a 2-socket system. Each thread should allocate from its local allocator first; if local memory runs out, steal from the other node with minimal synchronization. Provide data structures, allocate/free pseudo-code using atomic ops, and discuss fragmentation, cross-node traffic, and microbenchmark plan?","channel":"low-level","subChannel":"general","difficulty":"intermediate","tags":["low-level"],"companies":["Databricks","Microsoft","NVIDIA"]},{"id":"q-4241","question":"Design a fixed-size-object allocator for a 4-core NUMA machine that minimizes inter-core traffic. Implement per-core caches, a small global cache, non-blocking refill of core caches, and safe bulk release during teardown. Explain cache-line alignment and false sharing avoidance, how you reclaim freed objects, and how you'd benchmark latency under bursty allocations?","channel":"low-level","subChannel":"general","difficulty":"intermediate","tags":["low-level"],"companies":["Adobe","Cloudflare","Google"]},{"id":"q-4632","question":"Design a bounded MPMC DMA ring for a 2-socket NUMA server. Use N=1<<16 128-byte slots (16B header, 112B payload) aligned to 128B. Per-slot fields: seq and len. Producers publish payload with store-release, consumers read with acquire and recycle; wrap around via seq xor 1. Use epoch-based reclamation for descriptors. Propose a 100G microbenchmark and how you verify stall, latency, and backpressure?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Databricks","Scale Ai","Snowflake"]},{"id":"q-684","question":"Design a fixed-size ring buffer in C that stores bytes. Capacity N is a power of two (e.g., 1024). Show how to compute the next index using a mask (idx & (N-1)), and explain full vs empty detection using only head and tail counters. Provide enqueue and dequeue logic for a single-producer/single-consumer scenario?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Discord","Hugging Face","Instacart"]},{"id":"q-692","question":"Design a lock-free ring buffer that supports multiple producers and multiple consumers with bounded capacity. Provide enqueue/dequeue pseudo-code, explain how you avoid ABA, how memory reclamation is handled (hazard pointers or epochs), and why it scales under high contention. Include caveats on cache lines and false sharing. How would you validate under stress?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Anthropic","Databricks","Stripe"]},{"id":"q-694","question":"Design a cache-friendly, per-thread deque work-stealer for a multi-core task executor. Each worker maintains a fixed-size ring buffer for bottom push/pop; thieves steal from the top of other workers via CAS on a top index with a version counter. Explain ABA avoidance, memory ordering, and padding to avoid false sharing. Provide precise pseudo-code and a realistic burst workload scenario?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Coinbase","NVIDIA"]},{"id":"q-707","question":"Design a crash‑consistent, multi‑producer/multi‑consumer ring buffer backed by persistent memory PMEM. How would you ensure last enqueued item durability across power loss, implement recovery, and validate correctness? Provide concise enqueue/dequeue pseudocode with proper flush/fence ordering and discuss failure scenarios?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Airbnb","Coinbase","Two Sigma"]},{"id":"q-712","question":"Design a NUMA-aware in-memory index with per-node shards and a lock-free cross-node coordinator. Provide insertion and lookup with minimal locking, specify data layout (shards, padding, key/value encodings), and memory-order guarantees (fences, atomic ops). Describe a deadlock-free shard rebalancing protocol under high contention and outline tests with realistic Snowflake/Twitter-scale workloads?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Snowflake","Twitter"]},{"id":"q-723","question":"Design a software-based **TLB** for a 4-core 64-bit system with 4KiB pages. Each core has a private 128-entry **TLB** and a global page-table invalidation path. Provide lookup/refill pseudo-code, discuss eviction strategy (LRU vs. random), synchronization via **memory fences**, and cross-core shootdowns. Include a test under memory pressure and explain validation?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Instacart","Oracle","Tesla"]},{"id":"q-725","question":"Design a crash-consistent, in-memory index for a 4-byte key, 8-byte value store on an 8-core Linux machine. Use per-core log-structured segments and a Bloom filter; describe durable append ordering, startup recovery by replaying per-core logs, and validation under power-loss scenarios?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Goldman Sachs","LinkedIn","Plaid"]},{"id":"q-734","question":"Design an epoch-based reclamation scheme for a lock-free stack in a 64-bit, multi-threaded user-space library. Each thread publishes its local epoch; a global clock advances periodically. On pop, move the node to a retire-list with its epoch and reclaim only after all threads have observed an epoch older than that node. Include a stress test with high contention?","channel":"low-level","subChannel":"general","difficulty":"intermediate","tags":["low-level"],"companies":["Google","Lyft","Slack"]},{"id":"q-744","question":"In a 2-socket x86-64 server with MESI coherence, design a lock-free, multi-producer single-consumer ring buffer in shared memory for a 10 Gbps network path. Use per-slot sequence numbers to avoid ABA, with a fixed size N=1<<16 and 64-byte payload slots. Provide slot layout, push/pop pseudo-code with memory fences, discuss wrap-around and backpressure, and outline a minimal microbenchmark to validate throughput and data integrity under contention?","channel":"low-level","subChannel":"general","difficulty":"intermediate","tags":["low-level"],"companies":["Instacart","Tesla","Two Sigma"]},{"id":"q-752","question":"Design a crash-safe, persistent ring buffer in NVRAM for a 2-socket NUMA system with a PCIe NIC. Capacity N=1<<18, 128-byte payloads, per-slot 64-bit sequence to prevent ABA. Slot: [payload|seq|meta]. Enqueue: publish payload, fence, then update seq. Dequeue: verify seq before consume. Durability via per-slot commit log: flush payload and seq, then epoch commit. On crash, recover by replaying committed epochs and validating seq monotonicity. Provide a minimal microbenchmark to validate throughput and correctness under power loss?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Adobe","IBM","Tesla"]},{"id":"q-760","question":"On a dual-socket NUMA server, design a cross-NUMA, zero-copy ring buffer for a 40 Gbps path between two processes where producers live on socket A and a consumer on socket B. Use per-slot 64-bit sequence numbers to prevent ABA, 128-byte payload slots, and capacity 1<<18 with 64-byte alignment. Provide slot layout, enqueue/dequeue pseudo-code with memory fences and cache-line padding, wrap-around and backpressure handling, and outline a minimal microbenchmark to validate throughput and data integrity under cross-socket contention?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Amazon","Google"]},{"id":"q-772","question":"Design a cache-friendly fixed-size object pool for 128-byte blocks used by a multi-threaded producer-consumer path. Implement init, alloc, and free in C for a pool of 1<<20 blocks, each 128 bytes and 64-byte aligned. Use per-thread caches to reduce contention and a global lock-free free-list for overflow. Explain how you avoid false sharing, show the slot layout with a freelist pointer, and outline a microbenchmark to measure throughput and tail latency under contention?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Adobe","Netflix"]},{"id":"q-778","question":"Write a C function sum_prefetch that sums N 64-bit integers from an aligned array of length n using software prefetching to hide memory latency. Use 4-way unrolling and __builtin_prefetch to bring data 256 elements ahead. Ensure correctness for any length. Propose a microbenchmark plan to compare with a naive loop and discuss cache-line utilization and false sharing concerns?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Google","Tesla","Twitter"]},{"id":"q-784","question":"Design a deterministic, time-sliced barrier for a three-stage streaming pipeline on a 2-socket x86-64 system. Each stage runs on a fixed subset of cores; implement a barrier that advances phases only after every core finishes its assigned slice within a bounded time. Explain how to ensure bounded latency under cache-line contention, preserve MESI coherence, and prevent starvation. Provide pseudo-code for enter_barrier for a core and discuss validation?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Bloomberg","OpenAI"]},{"id":"q-794","question":"Design and implement a cache-friendly tiled matrix multiply for two large double-precision matrices stored in row-major order on a two-socket NUMA system. Propose a tile size of 32x32, provide C code for the tiled kernel with boundary handling, explain how to maximize L1/L2 reuse, NUMA locality (first-touch), avoid false sharing, and an inline 4-wide inner-loop unrolling. Include a microbenchmark plan comparing to a naive triple-nested loop and how you would measure throughput and cache behavior?","channel":"low-level","subChannel":"general","difficulty":"intermediate","tags":["low-level"],"companies":["Google","LinkedIn"]},{"id":"q-800","question":"Design a per-core, lock-free memory allocator for a shared-memory, NUMA-aware object store. Each core maintains a 2 MB local heap; allocations first attempt local allocation, with a fast cross-core path using atomic hand-offs; reclaimed memory is managed via hazard pointers and epoch-based reclamation. Provide allocate/free APIs, a sketch of the free-list structure, and a microbenchmark plan that shows fragmentation under steady-state load?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Discord","MongoDB","NVIDIA"]},{"id":"q-810","question":"Design a crash-consistent, bounded, multi-producer/multi-consumer queue backed by non-volatile memory. It must survive power loss, use per-slot sequence numbers to avoid ABA, provide push/pop pseudo-code with proper memory fences, and support epoch-based reclamation to avoid hazard pointers. Outline recovery on boot and a microbenchmark plan?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Airbnb","Plaid","Uber"]},{"id":"q-816","question":"Design a crash-consistent, persistent ring buffer for a multi-producer/multi-consumer event stream backed by non-volatile memory, with 1<<18 slots of 128-byte payloads; explain ABA avoidance via per-slot sequence numbers, two-phase publish with durable commit, and required flush/barrier order; describe recovery and a microbenchmark plan under simulated power loss?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Robinhood","Tesla"]},{"id":"q-826","question":"Design and implement a tiny spinlock in C11 for a shared memory region. Provide lock() and unlock() using stdatomic.h primitives, ensuring memory_order_acquire on successful lock and memory_order_release on unlock. Include a minimal two-thread test contending for the lock and explain your backoff/yield strategy and fairness limitations?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Goldman Sachs","Google","Snap"]},{"id":"q-834","question":"Design and implement a fixed-size object pool: a pre-allocated buffer partitioned into 1024 blocks of 64 bytes. Provide thread-safe allocate() and free() using a free-list stored in the blocks and a lightweight spinlock guarding the list. Include initialization and a small test snippet with 2 threads. Discuss fragmentation, cache locality, and how you'd validate concurrency?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Meta","Netflix","Uber"]},{"id":"q-358","question":"You're building a customer churn prediction model. Given a dataset with customer features (age, usage frequency, subscription type), how would you determine whether to use linear regression or logistic regression?","channel":"machine-learning","subChannel":"algorithms","difficulty":"beginner","tags":["regression","classification","clustering"],"companies":["Amazon","Datadog","Robinhood"]},{"id":"q-386","question":"You're building a fraud detection system for a large e-commerce platform. Your initial model using logistic regression has 85% accuracy but high false positives. How would you improve this system using ensemble methods, and what specific trade-offs would you consider between precision and recall?","channel":"machine-learning","subChannel":"algorithms","difficulty":"intermediate","tags":["regression","classification","clustering"],"companies":["Anthropic","Google","OpenAI"]},{"id":"q-201","question":"How does an LSTM cell's forget gate regulate information flow compared to a simple RNN?","channel":"machine-learning","subChannel":"deep-learning","difficulty":"beginner","tags":["lstm","gru","seq2seq"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"q-254","question":"When implementing a bidirectional GRU vs LSTM for sequence labeling, how do gradient clipping thresholds and batch size affect convergence and what are the memory trade-offs?","channel":"machine-learning","subChannel":"deep-learning","difficulty":"intermediate","tags":["lstm","gru","seq2seq"],"companies":["Airbnb","Amazon","Apple","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-415","question":"You're training a CNN for Tesla's Autopilot lane detection. The model works well on clear roads but fails in rainy conditions. How would you modify the architecture and training process to handle weather variations while maintaining real-time performance?","channel":"machine-learning","subChannel":"deep-learning","difficulty":"intermediate","tags":["cnn","rnn","transformer","attention"],"companies":["Coinbase","Epic Games","Tesla"]},{"id":"q-195","question":"How would you implement a canary deployment strategy for a TensorFlow model using MLflow and Kubernetes, ensuring zero downtime and automatic rollback on performance degradation?","channel":"machine-learning","subChannel":"deployment","difficulty":"intermediate","tags":["mlflow","kubeflow","sagemaker"],"companies":["Amazon","Databricks","Google","Microsoft","Uber"]},{"id":"q-227","question":"How would you implement dynamic quantization-aware training with mixed-precision to optimize inference latency while maintaining model accuracy across varying hardware constraints?","channel":"machine-learning","subChannel":"deployment","difficulty":"advanced","tags":["quantization","pruning","distillation"],"companies":["Google","Meta","Microsoft","NVIDIA","Tesla"]},{"id":"q-309","question":"Design a complete MLOps pipeline using MLflow and Kubeflow for a production ML system handling 1M daily predictions with 99.9% availability. What components would you include and how would you ensure model governance and automated retraining?","channel":"machine-learning","subChannel":"deployment","difficulty":"advanced","tags":["mlflow","kubeflow","sagemaker"],"companies":null},{"id":"q-323","question":"How would you design an ML pipeline using Kubeflow that handles model versioning, A/B testing, and automated rollback for a fraud detection system at Coinbase?","channel":"machine-learning","subChannel":"deployment","difficulty":"advanced","tags":["mlflow","kubeflow","sagemaker"],"companies":["Coinbase","Cruise","Tesla"]},{"id":"q-347","question":"You're deploying a machine learning model using MLflow. How would you track experiments and ensure reproducibility when moving from development to production?","channel":"machine-learning","subChannel":"deployment","difficulty":"beginner","tags":["mlflow","kubeflow","sagemaker"],"companies":["MongoDB","Okta","Warner Bros"]},{"id":"q-223","question":"How would you design a production-scale evaluation pipeline that dynamically adjusts metrics based on class imbalance and business priorities while maintaining sub-second latency?","channel":"machine-learning","subChannel":"evaluation","difficulty":"advanced","tags":["precision","recall","auc-roc","f1"],"companies":["Amazon","Google","Meta","Netflix","Stripe"]},{"id":"q-336","question":"You're evaluating a movie recommendation system at Warner Bros. The system has 95% accuracy but poor precision for popular movies. How would you diagnose and fix this issue using precision-recall analysis?","channel":"machine-learning","subChannel":"evaluation","difficulty":"intermediate","tags":["precision","recall","auc-roc","f1"],"companies":["Expedia","Microsoft","Warner Bros"]},{"id":"q-1126","question":"You're deploying a real-time anomaly detector for edge CDN traffic at a cloud provider. Spikes during events cause distribution drift. Propose an online learning approach that adapts without catastrophic forgetting, maintains latency under 30 ms, and keeps calibration. Include data retention policy, drift detection, update rules, and monitoring dashboards?","channel":"machine-learning","subChannel":"general","difficulty":"intermediate","tags":["machine-learning"],"companies":["Cloudflare","Google"]},{"id":"q-1170","question":"You're training a binary classifier on a dataset with 1% positives. After a baseline model, overall accuracy is high but positive precision is very low. Describe a practical plan to diagnose whether the issue is threshold choice or true data imbalance, and implement a minimal pipeline with stratified splits, class weights or resampling, and threshold tuning; outline metrics and validation?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Adobe","Tesla","Two Sigma"]},{"id":"q-1220","question":"You're deploying a single on-device model for real-time video analytics on an edge device with 12 ms per frame latency and 200 MB RAM. The model must perform both object detection and semantic segmentation. Describe a concrete plan to meet latency while preserving accuracy: architecture choices (shared backbone, task heads, feature pyramids), training strategies (loss weighting, distillation, data augmentation), deployment optimizations (quantization, operator fusion, memory layout, early exits), and validation strategy (latency budgets, mAP, mIoU, robustness across weather)?","channel":"machine-learning","subChannel":"general","difficulty":"advanced","tags":["machine-learning"],"companies":["NVIDIA","Tesla","Twitter"]},{"id":"q-1290","question":"You’re training a binary classifier for signup conversion on a dataset with numeric features (age, session_time) and categorical features (device, country). A logistic regression baseline yields high AUROC but poor calibration on holdout. Outline a practical plan to diagnose and fix calibration, comparing Platt scaling and isotonic regression, data preprocessing tweaks, and how you’d validate the fix with a minimal code sketch?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Snowflake","Uber"]},{"id":"q-2291","question":"You're deploying a real-time anomaly detection system for financial transactions using a hybrid model: a fast statistical detector plus a deeper autoencoder. After deployment, you notice an uptick in false positives during major holidays due to seasonal patterns. Design a practical plan to improve robustness: data collection, re-calibration, gating strategy to route events to the appropriate detector, and evaluation metrics including online A/B testing. What changes would you implement and why?","channel":"machine-learning","subChannel":"general","difficulty":"intermediate","tags":["machine-learning"],"companies":["Google","Netflix"]},{"id":"q-2389","question":"You're deploying real-time anomaly detection on telemetry from 50k edge cameras across campuses. Labels are scarce, data drifts with time, and bandwidth to central is limited. Propose a practical architecture and training plan that balances latency, privacy, and accuracy: choose models, training regime (self-supervised + federated updates), deployment (edge vs cloud), monitoring, and evaluation metrics. Be concrete about components and data flow?","channel":"machine-learning","subChannel":"general","difficulty":"advanced","tags":["machine-learning"],"companies":["Adobe","Amazon"]},{"id":"q-2530","question":"Design a production-ready calibration-and-drift pipeline for a binary classifier deployed in a high-stakes domain (e.g., loan approvals). The system must calibrate probabilities in real time as data drifts occur, detect distribution shift, trigger targeted retraining with limited labels, and provide explainability and auditing. Describe architecture, data flow, concrete metrics, and trade-offs?","channel":"machine-learning","subChannel":"general","difficulty":"advanced","tags":["machine-learning"],"companies":["Google","IBM"]},{"id":"q-2682","question":"Describe a minimal, end-to-end churn-prediction pipeline for 25k records with numeric features (tenure, spend), categorical features (region, device_type, plan) and a high-cardinality feature like 'customer_segment_id'. Data drift occurs monthly; outline preprocessing (encode high-cardinality features, handle missing), model choice (logistic vs tree-based) and rationale, evaluation (AUC, calibration), and drift monitoring/retraining cadence with concrete steps?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Goldman Sachs","Tesla","Twitter"]},{"id":"q-2709","question":"In a production ride-recommendation system with a two-tower model for candidate generation at scale, design a concrete plan to handle data skew, long-tail items, and concept drift while maintaining sub-50ms latency per request. Include data/versioning, evaluation, deployment strategies, and monitoring specifics?","channel":"machine-learning","subChannel":"general","difficulty":"advanced","tags":["machine-learning"],"companies":["Meta","Salesforce","Uber"]},{"id":"q-2783","question":"You're designing a real-time video ranking model for a global streaming platform. Data is highly skewed, frequent updates, and strict latency (≤20 ms) on edge devices; privacy constraints prevent raw data leaving devices. How would you architect a solution that personalizes rankings, preserves privacy, and remains production-safe? Be concrete about models, training, and evaluation?","channel":"machine-learning","subChannel":"general","difficulty":"advanced","tags":["machine-learning"],"companies":["Meta","Netflix"]},{"id":"q-2799","question":"You're deploying an on-device FL system for image-editing feature personalization on a mobile app (non-IID clients, bursty connectivity, limited compute). Outline a practical end-to-end FL pipeline: client sampling, DP clipping, FedAvg-like aggregation, convergence guarantees, communication budgeting, and evaluation strategy (offline and online)?","channel":"machine-learning","subChannel":"general","difficulty":"intermediate","tags":["machine-learning"],"companies":["Adobe","Google","Snap"]},{"id":"q-2864","question":"You're maintaining a real-time recommendation model serving 100k requests/sec. Recent data drift causes performance drop on new item categories, while offline metrics look fine. Outline an end-to-end plan to diagnose drift, validate fixes with A/B/shadow testing, implement feature store versioning, and a safe retraining/rollback workflow with guardrails (canaries, exposure controls)?","channel":"machine-learning","subChannel":"general","difficulty":"intermediate","tags":["machine-learning"],"companies":["Meta","Oracle"]},{"id":"q-2954","question":"In a beginner-friendly setting, you have a small tabular dataset with features a, b, c and target y. Propose a concrete approach to prevent data leakage during evaluation, choose a baseline model, and explain how you would validate performance using a robust cross-validation strategy, including data preprocessing and evaluation metrics?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Hashicorp","Meta","Oracle"]},{"id":"q-3065","question":"You're building a real-time fraud-detection system for a streaming e-commerce platform with multimodal data: tabular signals, text reviews, and time-series events. Data is high-velocity, highly imbalanced, and privacy-constrained. Propose an end-to-end architecture that processes streams with concept-drift handling, delivers low latency (<100 ms per event), and preserves privacy via on-device or DP/FL. What models, data pipelines, and evaluation approach would you choose, and why?","channel":"machine-learning","subChannel":"general","difficulty":"advanced","tags":["machine-learning"],"companies":["LinkedIn","Robinhood","Slack"]},{"id":"q-3258","question":"You're building a binary classifier to flag fraudulent transactions in a streaming feed and start with a simple logistic regression on features like amount, timestamp, and categorical encoding of merchant. Describe a beginner-friendly, end-to-end workflow to: (a) choose metrics for imbalance (e.g., precision/recall, F1, AUROC), (b) handle imbalance (thresholding, class weighting, or simple resampling), (c) select a decision threshold to hit a target F1, and (d) set up lightweight monitoring for drift after deployment. Provide concrete steps and commands you would use in a typical ML stack?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Citadel","Salesforce","Snowflake"]},{"id":"q-3269","question":"You're given a tiny tabular dataset for predicting loan default with features: income, age, debt_to_income, and target default. Design a reproducible train/validation pipeline that prevents leakage, compare a logistic regression and a small MLP, choose a suitable cross‑validation strategy, and outline how you would evaluate both discrimination (AUC) and calibration (calibration curve/Brier score)?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Bloomberg","NVIDIA","Robinhood"]},{"id":"q-3305","question":"You're building a real-time credit-risk scoring system for a fintech platform. Data arrives as multimodal streams: numeric transaction features, user chat transcripts, and mobile device telemetry. Labels (approved/declined) are sparse and delayed. Propose an end-to-end streaming model and data pipeline that (a) achieves sub-150 ms latency per event, (b) maintains privacy via on-device or DP/FL, (c) handles concept drift and rare events, and (d) includes calibration and fairness checks. Detail architecture, training, evaluation, and deployment considerations?","channel":"machine-learning","subChannel":"general","difficulty":"advanced","tags":["machine-learning"],"companies":["Coinbase","Salesforce","Two Sigma"]},{"id":"q-3484","question":"You're given a 3-class text classification dataset (positive, neutral, negative) with a small sample size. Design a practical baseline: use TF‑IDF + logistic regression, handle imbalance with class_weight='balanced', evaluate with macro-F1 and confusion matrix, and describe threshold tuning per class to maximize macro-F1. How would you implement this?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Discord","Hugging Face","Tesla"]},{"id":"q-3760","question":"Design a production real-time fraud-detection ML system for a global exchange handling streaming events at high velocity. The model must adapt to concept drift with low latency and privacy constraints, spanning multiple regions. Describe the end-to-end stack: feature store, online/offline models, drift detection, auto-rollback via canary rollout, and monitoring/alerts?","channel":"machine-learning","subChannel":"general","difficulty":"advanced","tags":["machine-learning"],"companies":["Coinbase","Zoom"]},{"id":"q-3802","question":"You're given a small, imbalanced dataset from a trading app with 1,500 sessions. Features: session_length, trades_last_7d, country, app_version, and target churn (0/1). Propose a concrete baseline ML pipeline to predict churn, covering: data preprocessing and encoding, handling class imbalance, a validation strategy that avoids leakage (time-based or user-grouped), and a comparison between logistic regression and a tree-based model with interpretability considerations and evaluation metrics (AUROC and PR-AUC)?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["DoorDash","Robinhood"]},{"id":"q-3871","question":"You're deploying a real-time fraud scoring model on a streaming platform using an offline XGBoost model. During peak hours, performance degrades though offline validation is fine. Outline a concrete strategy to detect and handle concept drift and data skew in production, detailing data windowing, drift metrics, retraining triggers, feature refreshes, calibration, and a minimal Python sketch to trigger retraining?","channel":"machine-learning","subChannel":"general","difficulty":"intermediate","tags":["machine-learning"],"companies":["Discord","MongoDB","Salesforce"]},{"id":"q-3872","question":"You're tasked with building a real-time anomaly detection model for a high-volume payment processor. The system must run under 5 ms latency per event on CPU, process tabular data with high-cardinality categorical features, handle concept drift, and provide feature-level explanations for regulatory audits. Propose an end-to-end architecture, model choices, deployment plan (including feature store, model registry, and on-device vs server), drift detection strategy, and evaluation plan (online/offline, backtesting)?","channel":"machine-learning","subChannel":"general","difficulty":"advanced","tags":["machine-learning"],"companies":["Bloomberg","PayPal","Scale Ai"]},{"id":"q-4231","question":"You're deploying a real-time fraud detection model for a payments platform that experiences distributional shifts as user behavior changes (e.g., new merchants, seasonality). Describe an end-to-end monitoring and retraining strategy that detects data drift, concept drift, protects privacy, and minimizes downtime. Specify metrics, thresholds, data pipelines, and rollback canary deployment?","channel":"machine-learning","subChannel":"general","difficulty":"advanced","tags":["machine-learning"],"companies":["Coinbase","Square","Two Sigma"]},{"id":"q-4415","question":"You're building a lightweight on-device binary classifier for a food-delivery app to predict whether a user will tap a recommended restaurant in the current session. Features: time_of_day, user_lat, user_long, distance_to_restaurant, and recommended_category. Propose a minimal end-to-end pipeline: 1) feature encoding and scaling strategy suitable for on-device constraints, 2) model choice with justification for a sub-100 KB footprint, 3) privacy-aware validation strategy (per-user or session-level), 4) evaluation metrics and thresholding plan, and 5) a simple on-device inference path and a lightweight update mechanism. Also discuss monitoring and update cadence?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["DoorDash","LinkedIn"]},{"id":"q-4472","question":"You're building a cross-platform recommendation engine with user data from web and mobile apps. Design an end-to-end ML feature store and deployment pipeline that respects privacy (DP/FL), handles concept drift, and supports rapid A/B testing with rollback. Describe data/versioning, offline/online sync, drift detection, evaluation metrics, and trade-offs within latency constraints?","channel":"machine-learning","subChannel":"general","difficulty":"advanced","tags":["machine-learning"],"companies":["DoorDash","Google","Robinhood"]},{"id":"q-4644","question":"You're building a realtime churn-prediction platform for a global SaaS product. Data streams include usage logs, tickets, and billing events across regions with different privacy rules. The model must be multimodal, handle concept drift, and respond in under 200 ms. Propose end-to-end architecture, multimodal fusion strategy, privacy controls (DP/FL), and an evaluation plan?","channel":"machine-learning","subChannel":"general","difficulty":"advanced","tags":["machine-learning"],"companies":["Microsoft","Salesforce"]},{"id":"q-468","question":"You're training a neural network and notice the validation loss starts increasing while training loss continues decreasing. What's happening and how would you diagnose and fix it?","channel":"machine-learning","subChannel":"general","difficulty":"intermediate","tags":["machine-learning"],"companies":["Amazon","Google","OpenAI"]},{"id":"q-4712","question":"You're deploying a real-time fraud risk scorer for a global payments app. The model outputs a probability and must meet a 50 ms latency per request, with region-specific calibration drift and privacy constraints. Describe a practical end-to-end plan to detect drift, recalibrate regionally, and validate impact before rollout, including data pipelines, metrics, and rollback strategy?","channel":"machine-learning","subChannel":"general","difficulty":"advanced","tags":["machine-learning"],"companies":["Coinbase","OpenAI","Square"]},{"id":"q-4767","question":"You’re building a lightweight classifier to predict whether a user taps a new AR lens in their first session after install. Dataset: per-session features include time_of_day (0-23), device_type (categorical), app_version (string), lens_popularity (float), user_id (high-cardinality), and target_tapped (0/1). Propose a concrete baseline ML pipeline that handles high-cardinality IDs without leakage, chooses an encoding, uses a simple model, and evaluates with AUROC and PR-AUC. Include a minimal implementation plan and a short code snippet?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Cloudflare","IBM","Snap"]},{"id":"q-498","question":"You're building a simple spam classifier for emails. What's the difference between precision and recall, and which metric would you prioritize if false positives are more costly than false negatives?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Airbnb","IBM","Tesla"]},{"id":"q-529","question":"You're building a customer churn prediction model for a SaaS platform. What are the key steps you'd take from data preprocessing to model evaluation, and which metrics would you prioritize?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Databricks","Salesforce"]},{"id":"q-555","question":"Explain how gradient descent works and why it's fundamental to training neural networks?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Citadel","Microsoft","Tesla"]},{"id":"q-582","question":"Explain the difference between classification and regression in machine learning, and provide a simple example of when to use each?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Meta","Oracle"]},{"id":"q-273","question":"What's the difference between hyperparameters and parameters in machine learning, and why is cross-validation important for selecting optimal hyperparameters?","channel":"machine-learning","subChannel":"model-training","difficulty":"beginner","tags":["hyperparameter","cross-validation","regularization"],"companies":["Amazon","Google","Meta","Microsoft"]},{"id":"q-372","question":"You're training a CNN for Snapchat lens effects and notice your validation loss increases after epoch 3 while training loss decreases. What's happening and how would you implement a comprehensive solution including data augmentation, learning rate scheduling, and monitoring strategies?","channel":"machine-learning","subChannel":"model-training","difficulty":"beginner","tags":["hyperparameter","cross-validation","regularization"],"companies":null},{"id":"q-403","question":"You're training a large language model for Notion's AI features. Your model is overfitting on the training data but underperforming on validation. Design a comprehensive regularization strategy that addresses both L1/L2 regularization and more advanced techniques like dropout and early stopping. How would you implement cross-validation to ensure your hyperparameters generalize across different user data distributions?","channel":"machine-learning","subChannel":"model-training","difficulty":"advanced","tags":["hyperparameter","cross-validation","regularization"],"companies":["Chime","Google","Notion"]},{"id":"q-1133","question":"Scenario: a constraint-satisfaction problem with four tasks A,B,C,D each can be in domains {Pending, Running, Done}. Constraints: (A = Done) → (B = Running); (B = Running) → (C = Done); (C = Pending) → (D = Running); and (A = Pending) ∨ (D = Done). Is there a feasible assignment with at least one Running? Explain reasoning and outline a backtracking algorithm with forward-checking and 3-valued propagation to decide arbitrary such constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Bloomberg","Lyft"]},{"id":"q-1253","question":"Scenario: A deployment feature-flag policy defines three booleans F1, F2, F3 with these constraints: F1 -> F2, F2 -> F3, and exactly one of F1 and F3 is true, plus at least one flag is true. Is there a satisfying assignment? If yes, give one; if not, explain why. Then outline a tiny solver that encodes such constraints into a 2-SAT instance using an implication graph and SCC, with pseudocode?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Amazon","Meta","PayPal"]},{"id":"q-1915","question":"You design a tiny policy engine for network ports where each port's state is Allow, Deny, or Unknown (A, D, U). Ports: 80, 443, 22, 23, 8080. Constraints: (80 → 443), (22 ∨ 23), (80 ∨ 22), (443 → ¬23). Is there an assignment with U that does not force any constraint to be definitively false? Explain how you would implement a 3-valued backtracking propagation to verify arbitrary such rule sets?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Cloudflare","LinkedIn","Scale Ai"]},{"id":"q-1930","question":"In a policy graph with rules of the form (P ∧ Q) → R, (P ∧ R) → S, and (Q ∧ S) → T, plus a set of base facts that you may set to true as needed, is there an assignment making T true without violating any rule? Provide an algorithm to compute a minimal witness (smallest base set) and apply it to this instance to show T is derivable with base facts {P, Q}?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Apple","Databricks","Twitter"]},{"id":"q-2088","question":"You manage access flags as a directed graph with A ≤ B meaning 'A true implies B true'. Given a required set T of flags that must be true, decide if a consistent assignment exists and produce the least-true model containing T. Describe a linear-time forward-closure algorithm that propagates truth along edges until a fixpoint, and how it handles cycles. Include a concrete 6-flag example with 7 relations?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Bloomberg","Goldman Sachs","Google"]},{"id":"q-2194","question":"Given a finite Kripke model M = (W, {R_a, R_b}, V) with W = {w1,w2,w3}, agents a,b, and a proposition p with V(p) = {w1,w2}. After a public announcement of p, compute the updated model M|p and determine which worlds satisfy K_a p and K_b p. Then describe a linear-time algorithm using product update and forward closure to handle arbitrary finite models and sequences of announcements, including a small 3-world example to illustrate the steps?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Adobe","Instacart","Salesforce"]},{"id":"q-2286","question":"Design a solver for booleans with two constraint types: XOR constraints (subset S XOR = b) and at-least-k constraints on subsets. How would you decide satisfiability efficiently, encode the mix for a practical solver, and handle contradictions and scalability?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["DoorDash","Instacart","Tesla"]},{"id":"q-2441","question":"In a 3-valued logic with T, F, U, build a tiny circuit: inputs A, B, C; gates: G1 = A AND B; G2 = B OR C; G3 = NOT G1; G4 = G2 AND G3. Is there an assignment to (A, B, C) using U that yields G4 = U after standard Kleene-style tables? Explain how you would implement a small propagator to verify arbitrary such circuits?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Microsoft","NVIDIA","Square"]},{"id":"q-2496","question":"Scenario: four attributes X,Y,Z,W take values 0,1, or U. Constraints: X+Y ≥ 1, Y ≥ Z, Z+W ≥ 1, W ≤ X. Is there an assignment that uses U for at least one variable while satisfying all constraints? Outline a straightforward 3-valued propagation plus backtracking to verify arbitrary such constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Airbnb","Meta","Tesla"]},{"id":"q-2696","question":"Design a tiny policy-engine puzzle: There are flags F1..F5 taking values Enable (E), Disable (D), or Unknown (U). Constraints:\n- If F1 is E then F2 is E.\n- If F2 is D then F3 is D.\n- If F4 is E then F5 is E.\n- At least one of F1 or F3 must be E unless F5 is D.\n- F3 cannot be E if F4 is D.\nIs there an assignment that uses U for at least one flag and satisfies all constraints? Propose a 3-valued propagation with backtracking approach and apply it to the concrete instantiation: F1=U, F2=U, F3=U, F4=D, F5=U; determine feasibility?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Apple","Slack","Tesla"]},{"id":"q-3078","question":"You model a SQL-like WHERE clause with 4 predicates A,B,C,D each taking values TRUE, FALSE, or UNKNOWN. The formula is F = (A OR NOT B) AND (C XOR D). Is there an assignment that uses at least one UNKNOWN and makes F evaluate to TRUE under Kleene three-valued logic? Outline a concrete 3-valued propagation algorithm and a backtracking strategy to verify arbitrary such constraints?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["DoorDash","Goldman Sachs","Google"]},{"id":"q-3166","question":"You maintain a tiny 3-valued data-validation rule engine. Variables X, Y, Z, W take values True, False, Unknown. Constraints: (X OR Y) AND (X AND Z) -> W AND (W OR Z) is True. Is there an assignment with at least one Unknown that makes the whole formula True? Provide a concrete assignment and concise justification?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["MongoDB","Tesla"]},{"id":"q-3247","question":"Given a directed graph where each node i has a threshold t_i (0 ≤ t_i ≤ indeg(i)) and a node is true iff at least t_i of its in-neighbors are true, does there exist a fixed point S containing a specified T? If yes, compute the least such fixed point. Outline a linear-time propagation algorithm that handles cycles and illustrate with a concrete 6-node, 7-edge example?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Discord","NVIDIA"]},{"id":"q-3329","question":"Given N boolean flags X1..XN and constraints Xi XOR Xj = c (c in {0,1}) and optional fixed Xi = di, determine if the system is consistent and output a model with the minimum number of true flags. Describe a linear-time algorithm to check consistency, derive per-component solutions, and pick root values to minimize total true bits. For a concrete 5-flag example with 6 constraints, what is the minimum-weight solution?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Adobe","Discord","LinkedIn"]},{"id":"q-3355","question":"In a tiny 3-valued dataflow analysis with values {T, F, U}, given SSA vars B, C, E (inputs) and derived A = B AND C; D = NOT E; F = D OR A. Is there a choice of B, C, E from {T, F, U} such that after Kleene-style propagation to a fixpoint, F evaluates to U? Explain the propagation steps and a succinct algorithm to verify arbitrary such sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Meta","Scale Ai","Square"]},{"id":"q-3607","question":"Design a tiny policy engine for a microservice access rule set in 3-valued logic (T, F, U). Variables A,B,C,D ∈ {T,F,U}. Constraints: (A ∧ B) → D; (¬A ∨ C) → D; (D ∧ ¬C) → A. Is there an assignment to (A,B,C,D) using U that does not force any constraint to be definitively false? Explain propagation steps and outline an algorithm to verify arbitrary such rule sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Lyft","Snowflake"]},{"id":"q-3685","question":"Let V be a set of boolean variables. We have Horn clauses of the form (A1 ∧ ... ∧ Ap) -> B. Each variable v has a nonnegative cost c(v) for setting it true. Given a required set T ⊆ V that must be true and a budget K, decide if there exists a model with T true and total cost ≤ K, and produce the minimum-cost model if possible. Explain a forward-chaining approach to minimize cost, handle cycles, and illustrate with a small 5-variable example showing two feasible models with different costs?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Netflix","Snap"]},{"id":"q-3801","question":"Scenario: Build a tiny constraint solver for four gates Ship1..Ship4 in 3-valued logic (T, F, U). Constraints: (Ship1 ∧ Ship2) → Ship3; Ship2 ∨ Ship4; Ship1 ∨ ¬Ship3; Ship4 → ¬Ship1. Is there an assignment using U that does not force any constraint to be false? Propose a 3-valued backtracking propagation approach to verify arbitrary such rule sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Adobe","DoorDash","Salesforce"]},{"id":"q-3850","question":"Scenario: A tiny policy engine uses four predicates P,Q,R,S ∈ {T, F, U} with Kleene three-valued logic. Constraints: P ∨ Q = T; R ∧ S = F; P → R = T; Q → S = F. Is there an assignment that includes at least one U and that makes all constraints evaluate as required? Explain a simple 3-valued propagation approach to verify arbitrary such rule sets?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Anthropic","Discord","Scale Ai"]},{"id":"q-4095","question":"Scenario: A tiny feature-flag policy engine with four features F1..F4, states in {T, F, U}. Constraints: (F1 → F2), (F2 → F3), (F3 → ¬F4), (F4 → F1). Is there an assignment that uses U for at least one flag and preserves all constraints as not definitively false? Outline a simple 3-valued propagation/backtracking approach to verify arbitrary such rule sets?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Snap","Two Sigma","Uber"]},{"id":"q-4139","question":"In a microservice health policy, four services A, B, C, D can be Up, Down, or Unknown. Over two time steps t=0 and t=1, constraints: 1) If A is Up at t=0, then B is Up at t=1; 2) If D is Unknown at t=0, then B is Up at t=1 or C is Down at t=1; 3) If B is Down at t=0, then A is Down at t=1; 4) At each time t, at least one service is Unknown. Is there an assignment for (A, B, C, D) at t=0 and t=1 that satisfies all constraints with at least one Unknown at each time? Propose a 3-valued backtracking propagation approach to verify arbitrary such policy sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Netflix","Scale Ai"]},{"id":"q-4171","question":"Scenario: A tiny propositional graph uses three signals A, B, C drawn from {T, F, U}. Constraints: (A → (B ∨ C)), ((B ∧ ¬C) → A), and ((A ∨ B) → ¬C). Is there an assignment that uses at least one U and keeps every constraint not definitively F? Outline a simple 3-valued propagation approach to verify arbitrary such rule sets?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Instacart","Meta"]},{"id":"q-4234","question":"You are given a graph with boolean variables p_v for each node, and edges (u,v,w) enforcing p_u XOR p_v = w ∈ {0,1}. A subset S of nodes is forced to true. Determine if a consistent assignment exists and produce one that minimizes the total number of true variables. Describe a linear-time solver that propagates parity per connected component, detects contradictions, and decides root flips; include a 5-node example with 6 edges to illustrate solvable and unsolvable cases?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Google","Meta","PayPal"]},{"id":"q-4261","question":"Given a directed graph of boolean flags with two constraint types: (i) A -> B meaning A true forces B true, and (ii) XOR clauses among disjoint pairs (e.g., exactly one of X and Y is true), plus a required set T that must be true. Decide if there exists an assignment satisfying all constraints and produce the smallest model (fewest true flags) containing T. Provide a small 6-flag example using both constraint types and describe a practical solver that combines forward-propagation with a parity solver (2-SAT) for the XORs?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Meta","Slack"]},{"id":"q-4430","question":"Scenario: You maintain a 3-valued logic validator for three signals S1, S2, S3 in {T, F, U}. Constraints: (S1 -> S2), (S2 ∨ S3), (S1 ∧ ¬S3) -> T. Is there an assignment using at least one U that preserves all constraints as not definitively false? Outline a concrete 3-valued propagation strategy and a concrete backtracking sketch?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Tesla","Two Sigma"]},{"id":"q-4739","question":"Scenario: A tiny policy-flag set F1..F4 uses three-valued logic {T, F, U}. Constraints: (F1 → F2), (F1 ⊕ F3) [exactly-one true], (F2 ∨ F4), (F3 → ¬F4). Is there an assignment with at least one U that keeps every constraint not definitively false? Outline a 3-valued propagation/backtracking approach to verify such constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Bloomberg","Microsoft","Snowflake"]},{"id":"q-683","question":"You're managing a streaming DAG with tasks A,B,C,D; edges enforce: A before B; B before C; at most one of C or D can occur in a window of size H. Given a log of events with timestamps per task, implement an O(n log n) verifier to determine if the log is valid under these constraints and describe how you'd extend to multiple windows in a distributed system?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Databricks","Google"]},{"id":"q-685","question":"Context: In a data-cleaning pipeline, records have boolean attributes a, b, c, d. Rules are Horn clauses of the form X ∧ Y -> Z. Given a partial assignment, decide if a full assignment exists that satisfies all clauses. Design a linear-time forward-chaining solver, justify its correctness, and discuss incremental updates and cycles with a concrete four-variable example (two rules)?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Adobe","Databricks","Scale Ai"]},{"id":"q-699","question":"You have a Horn-clauses policy language for access control. Given the following rules and facts, determine if allow(alice,read,records) is entailed using forward-chaining to a least fixpoint. Rules: 1) grant(U,act,res) :- haveRole(U,R), privilege(R,act,res). 2) allow(U,act,res) :- grant(U,act,res). Facts: haveRole(alice,dataEngineer). privilege(dataEngineer,read,records). Show your derivation steps?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Adobe","Databricks","MongoDB"]},{"id":"q-702","question":"In a home security system, three sensors—door, window, and motion—report activity as booleans a, b, c for the last minute. The alert should fire only when exactly one sensor is active. How would you implement a function that takes a, b, c and returns true iff exactly one is true, and what are its time and space complexities?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Amazon","Google"]},{"id":"q-715","question":"In a distributed data-processing pipeline across three services Ingest (I), Compute (C), Persist (P), each event carries a 3-element vector clock. Given E1 at I with [2,0,1] and E2 at C with [1,3,0], decide whether E1 happened-before E2, E2 happened-before E1, or they are concurrent. Explain the rule and show the comparison. How would you scale this to N services and detect concurrency in large logs?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Amazon","Bloomberg","Slack"]},{"id":"q-719","question":"Design a tiny solver for three boolean inputs A, B, C given constraints: (A -> B) and (B -> C) and (A or C). Is there an assignment that satisfies all three? Explain your reasoning and outline a simple backtracking approach you would implement to check arbitrary small sets of such implications?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Adobe","Google","Scale Ai"]},{"id":"q-728","question":"Scenario: You’re building a tiny policy engine for feature flags with three booleans A, B, C. Constraints: A implies B, B implies C, and at least two of the three must be true. Is there an assignment that satisfies all constraints? If so, give one example and briefly justify. Then outline a straightforward backtracking approach to enumerate all satisfying assignments for any n flags and any such constraints?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Amazon","Instacart","Plaid"]},{"id":"q-733","question":"You’re auditing a rule-set for access policy. There are four booleans **X1**, **X2**, **X3**, **X4** with constraints: (X1 -> X2), (X1 ∨ X3), (X2 -> X4), and (X3 -> ¬X4). Is there a satisfying assignment? If yes, provide one; if not, explain why. Then outline a minimal backtracking strategy with unit propagation to verify arbitrary similar constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Microsoft","Netflix","Salesforce"]},{"id":"q-745","question":"You are building a tiny rule engine for feature toggles with three booleans A, B, C. Constraints: A → B, B → ¬C, and at least one of A, B, C must be true. Is there a satisfying assignment? Explain how you would verify it via brute-force backtracking across the eight possibilities and return one concrete example if it exists?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["NVIDIA","Twitter"]},{"id":"q-753","question":"In a feature-flag synthesis task, you have booleans A1..A8. Constraints include: (Ai -> Aj) implications, (Ai XOR Aj) mutual exclusions, and (Ai OR Aj OR Ak) group obligations. The implication graph is acyclic and each node has at most two outgoing edges. Design a backtracking solver that exploits the DAG to decide satisfiability for up to 12 vars. Provide a concrete 8-variable instance and show the solution or UNSAT?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Apple","LinkedIn","Netflix"]},{"id":"q-758","question":"You manage feature flags for a distributed service. Let F1..F5 be booleans with constraints: (F1 -> F2), (F1 ∨ F3), (F2 ⊕ F4), (F3 -> F5), and (F4 -> ¬F5). Is there a satisfying assignment? If yes, provide one; if not, explain. Then outline a minimal backtracking strategy with forward checking to verify arbitrary similar constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Cloudflare","Discord","Snowflake"]},{"id":"q-771","question":"You are validating a tiny access-control policy with two booleans: A = 'account is active', B = 'email verified'. The policy must satisfy: (¬A -> B), (B -> A), and (A ∨ B). Is there a satisfying assignment for A and B? Explain your reasoning and outline a simple backtracking check to verify arbitrary small sets of such clauses?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["NVIDIA","PayPal","Snowflake"]},{"id":"q-775","question":"You're building a constraint-solver for a feature-flag system across a microservices deployment. Given four flags S1..S4 with constraints: (S1 -> S2), (S2 -> S3), (S4 -> ¬S1), and (S1 ∨ S4) and (S2 ∨ S4). Is there a satisfying assignment? Provide one concrete assignment and outline a backtracking algorithm with unit propagation to verify arbitrary similar constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Slack","Two Sigma"]},{"id":"q-789","question":"Is there a truth assignment with exactly three flags true that satisfies all constraints A -> B; B -> C; D -> E; F -> G; not C -> H; A -> D? If yes, provide one and justify why it satisfies every implication?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Apple","Square","Twitter"]},{"id":"q-799","question":"Scenario: four booleans P, Q, R, S where P = user has role X, Q = grants read, R = has role Y, S = grants write. Constraints: (P -> Q), (R -> S), (P ∨ R), and at least two of {Q,S} must be true. Is there a satisfying assignment? If yes, give one; if not, explain why. Then outline a minimal backtracking strategy with unit propagation for arbitrary similar constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Cloudflare","DoorDash","Google"]},{"id":"q-812","question":"Instance-based SAT with mixed Horn and parity constraints: Variables A,B,C,D,E,F. Constraints: (A ∧ B) → C; (C ∧ D) → E; A ⊕ D ⊕ F = 1; (B ∧ E) → F. Is there an assignment to A..F that satisfies all constraints? If yes, provide one; then describe how you would build a solver that combines forward-chaining on Horn clauses with Gaussian elimination over GF(2) for parity constraints, including data structures and complexity considerations?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Citadel","Google","Lyft"]},{"id":"q-822","question":"Scenario: a tiny feature-toggle system for a data-annotation pipeline uses five booleans A–E: A = 'data augmentation enabled', B = 'sampling enabled', C = 'privacy mode on', D = 'live monitoring on', E = 'throttle rate limited'. Constraints: (A → B), (B → D), (C → ¬D), (A ∨ C), (D → E), (E → ¬A). Is there a satisfying assignment? If yes, give one; if not, explain why. Then outline a minimal backtracking strategy with unit propagation to verify arbitrary similar constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Databricks","Hugging Face","Square"]},{"id":"q-828","question":"Scenario: a feature-flag set uses four booleans A,B,C,D with rollout statuses True, False, Unknown (U). Constraints: (A → B), (C ∨ D), (A ∨ C), (B → ¬D). Is there a satisfying assignment allowing Unknowns? Explain reasoning and outline a backtracking approach using 3-valued logic to propagate U and verify arbitrary such constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Hashicorp","MongoDB","Robinhood"]},{"id":"q-180","question":"What is the primary purpose of DNS in computer networking and how does it enable internet communication?","channel":"networking","subChannel":"dns","difficulty":"beginner","tags":["dns","resolution"],"companies":["Amazon","Cisco","Google","Meta","Microsoft"]},{"id":"q-1031","question":"In a beginner-friendly scenario, a REST API behind a global CDN shows sporadic 1–2s latency for some users while synthetic tests pass. Outline a practical, end-to-end diagnostic plan to isolate DNS, TLS, caching, and client-network factors, including concrete commands and data you would collect and an initial fix you would try?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Discord","DoorDash","Snowflake"]},{"id":"q-1078","question":"In a small office network, a workstation intermittently cannot reach a public API behind Cloudflare during peak hours while other destinations are responsive; outline a practical, hands-on plan to diagnose using DNS (A/AAAA, TTLs), path tracing, TLS handshakes (ALPN/SNI), and edge routing behavior, with concrete mitigations to test?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Cloudflare","Tesla"]},{"id":"q-1089","question":"In a multi-region Kubernetes deployment using VXLAN overlays for pod networking, you notice intermittent packet loss and high tail latency when pods in region A talk to pods in region B. The overlay adds headers that push MTU beyond 1500 on inter-region links, causing fragmentation in some paths. You can adjust MTU, MSS clamping, and overlay parameters but cannot modify application code. How would you diagnose end-to-end and implement a robust fix that preserves ECMP load balancing and minimizes fragmentation? Provide concrete steps?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["DoorDash","Google"]},{"id":"q-1163","question":"In a two-region service, IPv6 clients report higher latency and intermittent timeouts to a TLS-enabled API when accessed from IPv6 only networks. You cannot modify app code. Outline a practical diagnostic plan focusing on IPv6 path MTU discovery, ICMPv6/firewall behavior, and how to verify with traceroute6, tcpdump, and TLS handshake timings. Propose concrete mitigations like adjusting VPN MTU and enabling IPv4 fallback?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Discord","Robinhood","Snowflake"]},{"id":"q-1232","question":"In a two-region deployment (us-east-1, eu-west-1) with a TLS-enabled API behind a global load balancer, peak hours yield p95 latency spikes to 350 ms while basic tests pass. No app changes allowed. Provide a concrete diagnostic plan to distinguish TLS handshake delays, ALPN/SNI issues, path MTU fragmentation, and load balancer behavior, with exact commands and data you’d collect?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Hashicorp","MongoDB"]},{"id":"q-1461","question":"In a globally distributed API behind a CDN and global load balancer, Asia users report login latency significantly higher than North America, while overall API latency remains acceptable. The client app is mobile and uses TLS with SNI; no code changes are allowed. Outline a concrete diagnostic plan to determine whether ECS (EDNS Client Subnet), DNS TTL, CDN edge variance, or inter-region routing is responsible, including exact commands, data to collect, and initial mitigations to test?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Google","MongoDB","Tesla"]},{"id":"q-1507","question":"In a globally distributed TLS-enabled API for real-time inference, traffic flows from Asia, EU, US behind a global load balancer and regional edge caches. During peak, Asia users see p95 latency spikes while NA remains stable. No code changes allowed. Provide a concrete diagnostic plan to distinguish between (1) TLS handshake/ALPN at edge vs origin, (2) inter-region routing and MTU fragmentation, (3) load balancer ECMP behavior, and (4) CDN edge miss penalties. Include exact commands, data to collect, and initial mitigations to test?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["NVIDIA","OpenAI","PayPal"]},{"id":"q-1671","question":"In a multi-cloud microservice mesh spanning AWS and Azure, inter-region service-to-service calls intermittently fail under load with rising p95 latencies. Tracing shows ECMP paths changing mid-request and MTU-related fragmentation on some hops. Without modifying applications, outline a concrete diagnostic plan and a stabilization strategy addressing PMTUD behavior, IPv4/IPv6 MTU alignment, ICMP blocking, and MTU discovery pitfalls across clouds, while preserving ECMP load balancing?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["Microsoft","Netflix","Oracle"]},{"id":"q-1766","question":"In a global data service using DNS-based global load balancing across us-east-1 and eu-west-1, cross-region reads spike latency during peak hours while intra-region calls remain fast. No app changes allowed. Outline a concrete diagnostic plan to distinguish stale DNS routing, BGP route flaps, and edge TLS termination delays, with exact commands and data you’d collect, interpretation rules, and recommended mitigations?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["Instacart","NVIDIA","Snowflake"]},{"id":"q-2322","question":"In a global deployment using anycast DNS fronting a TLS-terminated API gateway, intermittent TLS handshake stalls and p95 latency spikes appear during peak hours. No app changes allowed. The client path crosses regions via regional load balancers and edge caches. Outline a concrete diagnostic plan to distinguish issues caused by anycast routing, TLS handshakes (ALPN, SNI, 0-RTT), and cross-region MTU/PMTUD behavior, plus a practical mitigation path?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["Airbnb","DoorDash","Snowflake"]},{"id":"q-2382","question":"In a regional microservice mesh with DNS-based routing and TLS termination at edge gateways, intermittent p95 latency spikes occur during peak hours despite healthy synthetic tests. No app changes allowed. Outline a concrete diagnostic plan to distinguish DNS routing churn, edge TLS handshake variability, and backbone path changes, with exact commands, data to collect, and a practical mitigation path?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["Adobe","NVIDIA","Twitter"]},{"id":"q-2394","question":"In a large-scale service mesh across three data centers using VXLAN overlays for pod networking, you observe intermittent packet loss and tail latency spikes when east-west traffic redirection occurs during rebalancing. No app changes allowed. Design a concrete, end-to-end diagnostic plan to isolate whether overlay encapsulation, MTU/PMTUD, or inter-domain routing is the root cause, and outline a robust mitigation path that preserves ECMP and minimizes tunnel churn?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["Airbnb","Snap","Tesla"]},{"id":"q-2764","question":"In a globally distributed API deployed across two regions, inter-region latency spikes occur for a subset of traffic while intra-region latency remains low. The underlay uses an EVPN/VXLAN fabric with multiple interconnects and ECMP across them. You suspect per-flow ECMP hash collisions cause path oscillation and cache misses. Without changing app code, outline a concrete diagnostic plan to confirm the cause and propose mitigations that preserve ECMP while stabilizing inter-region paths?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["LinkedIn","Snap","Stripe"]},{"id":"q-2831","question":"In a globally distributed service using HTTP/3 over QUIC and TLS at edge, mobile clients roaming between networks experience intermittent QUIC connection resets and p95 latency spikes during peak hours. No app changes. Outline a concrete diagnostic plan to distinguish issues caused by QUIC connection migration, edge-cache revalidation, and DNS path changes, and propose practical mitigations that preserve ECMP and TLS posture?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Google","Hugging Face","MongoDB"]},{"id":"q-2844","question":"In a three-region WAN using VXLAN overlays with ECMP-enabled interconnects, EU to APAC traffic occasionally follows a suboptimal bounce path via NA, increasing tail latency of UDP telemetry without app changes. Provide a concrete diagnostic plan to identify whether inter-region underlay ECMP, tunnel encapsulation, or NAT translation is responsible, including exact commands and data to collect, and propose a robust mitigation that preserves ECMP while stabilizing end-to-end latency?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["Amazon","Apple","Lyft"]},{"id":"q-2891","question":"Global QUIC-based telemetry across US-East, EU-Central, and APAC suffers intermittent tail latency during bursts; no code changes allowed. Design a concrete diagnostic plan to separate handshake latency, path MTU/PMTUD, NAT/firewall drops of 0-RTT, and ECMP path flips, with exact commands and data to collect, and propose mitigations that preserve QUIC performance?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["Lyft","Meta","Twitter"]},{"id":"q-3012","question":"In a two-region deployment behind regional CDNs with TLS termination at the edge, EU users report intermittent HTTPS connection setup delays during peaks while data transfer after connect is fine. Outline a concrete diagnostic plan to distinguish whether DNS TTL/path issues, TLS handshake stalls, CDN edge cache misses, or regional routing is to blame, including exact commands and data to collect, and propose a mitigation that preserves existing load balancing without app changes?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Discord","Lyft","NVIDIA"]},{"id":"q-3088","question":"In a global three-region deployment (NA, EU, APAC) using a UDP telemetry overlay and a TLS-terminated API gateway behind a CDN, rare mid-session UDP telemetry stalls occur during regional failovers. No app changes. Provide a concrete diagnostic plan to distinguish whether WAN path selection/ECMP, CDN edge TLS handshakes/ALPN, or UDP NAT traversal is the culprit, including exact commands and data to collect, and propose a robust mitigation that preserves ECMP while stabilizing latency?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["Goldman Sachs","IBM","Robinhood"]},{"id":"q-3382","question":"Context: Two-region deployment with DNS-fronted TLS API gateway. During peak hours UDP DNS queries show tail latency; TCP works. No app changes. Provide a concrete diagnostic plan to determine if EDNS0 payload fragmentation, DNSSEC validation, or resolver-specific UDP rate-limiting is the root cause. Include exact Linux commands and data to collect (pcap, dig traces, EDNS0 size), and propose a robust mitigation preserving UDP DNS performance?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Cloudflare","Coinbase","Google"]},{"id":"q-3405","question":"In a global service using MPTCP over an overlay VPN across three regions, intermittent p95 latency spikes appear on UDP telemetry despite healthy synthetic tests. No app changes allowed. Provide a concrete diagnostic plan to determine if MPTCP subflow selection, per-path queueing, or VPN encapsulation is causing mid-flow path drift, including exact commands and data to collect and a robust mitigation that preserves MPTCP while stabilizing latency?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["Two Sigma","Uber","Zoom"]},{"id":"q-3417","question":"In a two-region deployment where a cloud firewall performs TLS inspection in front of a regional API gateway, login requests intermittently spike in tail latency during peak hours while static content remains fast. Without changing app code, outline a concrete diagnostic plan to distinguish whether TLS-inspection queueing, firewall rate-limiting, or edge-cache invalidations drive the latency, including exact data to collect and realistic mitigations that preserve security?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Airbnb","Twitter"]},{"id":"q-3472","question":"In a three-region deployment (NA, EU, APAC) of a real-time service using a WireGuard-over-UDP overlay, UDP telemetry exhibits intermittent p95 latency spikes under load with no app changes. Outline a concrete diagnostic plan to determine whether overlay MTU fragmentation, WireGuard per-peer RTT, or NAT traversal is the culprit, including exact commands to run and data to collect, and propose a robust mitigation that preserves overlay security and ECMP stability?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["Discord","NVIDIA"]},{"id":"q-3677","question":"In a globally distributed VPN with IPv6-only edge where telemetry endpoints are IPv4, NAT64/DNS64 is used. Intermittent UDP telemetry tail latency spikes occur when clients cross-region. Without app changes, outline a concrete diagnostic plan to identify whether DNS64 resolution, NAT64 translation state, or UDP fragmentation is responsible, including exact commands and data to collect, and propose mitigations that preserve connectivity?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Amazon","Google","Instacart"]},{"id":"q-3748","question":"In a globally distributed network using Segment Routing with IPv6 (SRv6) across three regions, inter-region traffic experiences sporadic long-tail latency due to per-flow path selection not aligning with ECMP groupings. Without changing applications, propose a concrete diagnostic plan to determine whether per-flow SID steering, TE-rule misconfig, or ingress replication is causing instability, including exact commands and data to collect, and propose a robust mitigation that preserves ECMP while stabilizing latency?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["Amazon","Oracle","Twitter"]},{"id":"q-3756","question":"In a global telemetry pipeline over UDP QUIC, edge collectors in three regions send to a central sink through cloud regions. During peak hours, tail latency spikes appear with no app changes. The path includes NIC offloads (GSO/TSO), IRQ coalescing, and hypervisor scheduling. Provide a concrete diagnostic plan to distinguish NIC offloads, interrupt coalescing, and scheduler jitter, with exact commands and data to collect, and a robust mitigation that preserves throughput and latency, without changing app code?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["Meta","Stripe","Tesla"]},{"id":"q-3870","question":"Three-region deployment NA, EU, APAC uses edge TLS termination and a UDP telemetry overlay. Tail latency spikes appear EU→APAC during peak; suspect TLS session resumption misses on regional proxies, clock skew, or ticket rotation across regions. Without app changes, design a concrete diagnostic plan with commands and data to collect, and propose a mitigation that preserves ECMP while stabilizing latency?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["Google","Robinhood"]},{"id":"q-4198","question":"Two WAN uplinks (ISP-A, ISP-B) on a single edge router use ECMP for outbound traffic to a global telemetry collector. UDP telemetry to the collector occasionally drops when the path leaves via ISP-B; ISP-A path remains healthy. Without app changes, provide a concrete diagnostic plan to confirm if reverse-path filtering or asymmetric routing is to blame, detailing exact commands and data to collect, and propose a mitigation that preserves ECMP while stabilizing end-to-end delivery?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["DoorDash","NVIDIA"]},{"id":"q-4215","question":"In a two-region EVPN/VXLAN fabric with INT telemetry, inter-region traffic experiences sudden tail-latency spikes tied to backbone rebalancing events. No app changes. Outline a concrete diagnostic plan to determine whether INT metadata misalignment, ingress replication, or per-flow hashing changes drive the spikes, including exact commands and data to collect, and propose a robust mitigation that preserves ECMP while stabilizing latency?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["Citadel","Meta","Twitter"]},{"id":"q-4255","question":"In a globally distributed anycast frontend with ECMP across three regional edges, EU clients intermittently exit via NA during peak, causing tail latency spikes in UDP telemetry. Without app changes, design a concrete diagnostic plan to determine whether BGP path selection, inter-provider ECMP, or edge-cache routing is at fault, including exact commands and data to collect. Propose mitigations that preserve ECMP while stabilizing latency?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["Google","Hashicorp","IBM"]},{"id":"q-4585","question":"IPv6-only clients intermittently fail TLS handshakes in a regional service behind NAT64/DNS64 and a dual-stack load balancer. Outline a concrete diagnostic plan to confirm whether DNS64 synthesis, NAT64 translation table exhaustion, or TLS 0-RTT/ALPN behavior is the culprit. Include exact commands and data to collect, likely log locations, and a practical mitigation that preserves IPv6 reachability without app changes?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Amazon","Discord","Goldman Sachs"]},{"id":"q-4614","question":"In a three-region real-time messaging service using QUIC with TLS, users report sporadic tail latency spikes on new connections during peak hours. No app changes allowed. Provide a concrete diagnostic plan to distinguish (a) QUIC handshake stalls and 0-RTT acceptance, (b) TLS handshake negotiation on edge proxies, and (c) per-edge queueing or NAT translation, with exact commands and data to collect, and propose a robust mitigation that preserves ECMP?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["Airbnb","Discord","Hugging Face"]},{"id":"q-4628","question":"In a dual-region deployment (NA and EU) using IPsec-over-VXLAN inter-region tunnels within an EVPN fabric, UDP telemetry experiences sporadic stalls during bursts and failover. No app changes. Design a concrete diagnostic plan to confirm whether IPsec SA rekey, ESP overhead, or IGP/ECMP reshaping is the culprit, including exact commands and data to collect, and propose a robust mitigation that preserves ECMP while stabilizing latency?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["Apple","Instacart"]},{"id":"q-4670","question":"In a three-region deployment of a WebRTC-based collaboration feature with TURN relays and enterprise firewalls, users report intermittent 2–5s stalls during peak hours. No app changes allowed. Provide a concrete diagnostic plan to distinguish (a) ICE connectivity checks stuck behind NAT/firewall, (b) TURN relay queueing/overload, (c) UDP multiplexing/packet pacing affecting media, with exact commands and data to collect, and propose a mitigation preserving end-to-end encryption?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["Databricks","Discord","Salesforce"]},{"id":"q-469","question":"Explain what happens when you type google.com into your browser and press Enter, focusing on the networking layers involved?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Discord","Goldman Sachs","IBM"]},{"id":"q-4713","question":"Three-region deployment with a TLS-terminated edge gateway and a service mesh. New HTTPS connections spike tail latency under load; existing sessions unaffected. No app changes. Provide a concrete diagnostic plan to distinguish edge TLS stalls (certificate chain or OCSP), region NAT/firewall rate-limits, mesh CA rotation delays, and DNS endpoint churn. Include exact commands/data to collect and a mitigation that preserves ECMP?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["Airbnb","Hashicorp","Square"]},{"id":"q-499","question":"How would you design a TCP load balancer that handles 1M concurrent connections with consistent hashing while preventing connection thrashing during backend failures?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["Snowflake","Twitter"]},{"id":"q-583","question":"How would you design a load balancer to handle 1M concurrent connections with sub-10ms latency, considering TCP connection pooling, health checks, and graceful degradation?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["Adobe","Goldman Sachs"]},{"id":"q-926","question":"In a multi-region deployment (US-East, EU-West) for a high-throughput service, end-to-end latency spikes to 150–300 ms during peak hours while synthetic tests pass. You observe edge drops and retransmissions. Provide a practical plan to diagnose and mitigate network-related factors, including path MTU discovery, ECN, TCP congestion control, TLS handshakes, SNI/ALPN behavior, and load-balancing strategy across regions, with data you’d collect and initial fixes?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["NVIDIA","Snowflake","Uber"]},{"id":"q-946","question":"In a globally distributed service behind a multi-region, ECMP-enabled load balancer, you observe sporadic high-tail latency while averages look fine. Explain the network mechanisms that could cause tail latency in this setup (per-path RTT variance, path MTU, reordering, retransmissions). Propose a concrete diagnostic workflow using production telemetry (eBPF per-flow histograms, NetFlow/SFlow, MTU checks) and practical remediation steps (MTU tuning, pacing, queue management)?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["Coinbase","Salesforce","Two Sigma"]},{"id":"gh-72","question":"How would you design and implement network segmentation for a microservices architecture, including Zero Trust principles, east-west traffic monitoring, and compliance requirements?","channel":"networking","subChannel":"load-balancing","difficulty":"advanced","tags":["security","network"],"companies":["Amazon","Cloudflare","Google","Hashicorp","Microsoft","Stripe"]},{"id":"q-186","question":"How would you implement session affinity (sticky sessions) in HAProxy while maintaining high availability, and what are the trade-offs compared to stateless load balancing?","channel":"networking","subChannel":"load-balancing","difficulty":"intermediate","tags":["lb","traffic","nginx","haproxy"],"companies":["Amazon","Bloomberg","Google","Microsoft","Netflix"]},{"id":"sd-1","question":"Explain load balancing strategies and when to use Layer 4 vs Layer 7. How do round-robin, least connections, and IP hash algorithms compare?","channel":"networking","subChannel":"load-balancing","difficulty":"advanced","tags":["infra","scale","networking"],"companies":["Amazon","Cloudflare","Google","Meta","Microsoft","Netflix"]},{"id":"q-203","question":"How does TCP's congestion control algorithm interact with HTTP/2's multiplexing when multiple streams compete for bandwidth?","channel":"networking","subChannel":"tcp-ip","difficulty":"intermediate","tags":["tcp","udp","http2","quic"],"companies":["Amazon Aws","Cloudflare","Google","Microsoft","Netflix"]},{"id":"q-256","question":"How does QUIC solve TCP's head-of-line blocking problem in HTTP/2 multiplexing, and what are the implementation trade-offs?","channel":"networking","subChannel":"tcp-ip","difficulty":"intermediate","tags":["tcp","udp","http2","quic"],"companies":["Amazon","Cloudflare","Google","Meta","Microsoft","Netflix"]},{"id":"q-275","question":"How does QUIC solve HTTP/2's head-of-line blocking issue over TCP, and what are the implementation trade-offs?","channel":"networking","subChannel":"tcp-ip","difficulty":"intermediate","tags":["tcp","udp","http2","quic"],"companies":["Amazon","Google","Meta","Microsoft"]},{"id":"q-1035","question":"In an advanced NLP interview, design an end-to-end multilingual QA system over English, Spanish, and Mandarin medical documents. The user asks in English. Outline architecture, data flow, privacy controls, latency targets, domain adaptation, and an evaluation plan. Include concrete components, trade-offs, and a short example of validation for a high-risk medical claim?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Adobe","IBM","MongoDB"]},{"id":"q-1101","question":"You're building a real-time brand-monitoring NLP service that ingests up to 100k tweets per minute in multiple languages. Design a scalable pipeline to classify sentiment and issue categories (e.g., billing, outages) with <300 ms latency per tweet, handle code-switching and slang, detect and adapt to drift, and provide a rollout plan including testing, monitoring, and rollback?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Stripe","Twitter"]},{"id":"q-1215","question":"Design a beginner-friendly pipeline for a Slack-based support bot that lives in a single workspace. It should: (1) classify Slack messages into intents: 'password_reset', 'access_request', 'billing_issue', 'incident'. (2) retrieve and present the most relevant FAQ article from a 100-article KB in English or Spanish. (3) operate with minimal latency on a shared CPU, and include a simple drift-detection plan and a rollout strategy with a safe fallback. Provide concrete components, data flow, and a short code snippet showing the classifier and retriever?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Slack","Snap"]},{"id":"q-1332","question":"Design an offline-first, on-device NLP pipeline for field technicians in remote areas. The device must classify support requests into hardware, network, or software issues and extract actionable items from multilingual speech transcripts (English, Spanish, Portuguese). Constraints: 256MB RAM, ≤200ms latency per utterance, no network access except periodic OTA updates, privacy-preserving embeddings, and robust drift detection with authenticated weight patches. Provide architecture, models, data handling, evaluation, and rollout plan?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Databricks","Hashicorp","Snap"]},{"id":"q-1394","question":"You are given a multilingual customer support dataset containing code-switching between English and Spanish and occasional emojis. Design an end-to-end NLP solution for intent classification and slot filling, with limited labeled data in the target language. Describe data collection, preprocessing, model choice, and evaluation strategy, including how you'd handle code-switching and emoji semantics?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Google","IBM","Two Sigma"]},{"id":"q-1456","question":"You're deploying a multilingual on-device sentiment and intent classifier for a mobile app, handling English/Spanish with code-switching and emojis. Latency budget: <200 ms on a 2-core device, offline-first. Design an end-to-end pipeline: data flow, model architecture (tiny quantized model plus emoji/slang rules), feature extraction, on-device drift detection, and a practical evaluation plan using ~50 labeled examples for quick adaptation?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Salesforce","Scale Ai","Twitter"]},{"id":"q-1657","question":"Design a beginner-friendly NLP pipeline to extract Date, Money, and Person entities from bilingual English/Spanish Slack-like messages with slang and emojis, using minimal labeled data. Outline preprocessing, tool choices (regex, spaCy), and a concrete evaluation plan with a simple baseline?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Slack","Snap","Two Sigma"]},{"id":"q-1702","question":"Design an end-to-end NLP system to detect safety-critical incidents from real-time chat and voice transcripts in a multi-tenant ride-hailing platform. Include data ingestion, ASR/translation, latency targets, privacy controls, and model versioning/deployment. Compare rule-based vs learned approaches and detail production evaluation (offline metrics plus live A/B and rollback plans)?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Hashicorp","Lyft","Robinhood"]},{"id":"q-2021","question":"Design a multilingual, low-latency NLP pipeline to summarize and extract action items from enterprise meeting transcripts in English, Spanish, and Mandarin. Include language detection, on-device vs cloud trade-offs, privacy controls, and a lifecycle for models and data. How would you evaluate accuracy and latency, and handle update rollouts with minimal downtime?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Google","LinkedIn","Snap"]},{"id":"q-2171","question":"Design a beginner-friendly on-device NLP classifier for a multilingual chat dataset in English and Spanish labeled as spam in a Discord-like app. Outline data prep, feature choices such as character n-grams, a lightweight model (logistic regression), multilingual handling, and a minimal evaluation plan with a held-out test and drift checks. Include a tiny Python snippet to train on a toy dataset?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Amazon","Discord"]},{"id":"q-2249","question":"You're given a dataset of 3,000 English customer-chat messages labeled with 5 intents (greeting, ask_status, report_issue, request_refund, other). Design a minimal on-device NLP classifier that runs in under 40 ms per request on a mid-range smartphone. Specify preprocessing, feature extraction (e.g., bag-of-words vs. embeddings), model choice, and a simple evaluation plan with train/val/test splits and drift checks. Include how you would measure privacy and latency in practice?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["DoorDash","Google","LinkedIn"]},{"id":"q-2314","question":"Design a scalable NLP pipeline that flags hazardous product reviews in a live e-commerce feed, combining real-time abuse detection, policy violation checks, and multilingual support, with privacy constraints and <100 ms latency, plus explainability. Compare embedding-based detectors vs rule-based detectors and outline production evaluation (offline metrics, live A/B, rollback plan)?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Google","Instacart","MongoDB"]},{"id":"q-2370","question":"Design a beginner-friendly NLP workflow to detect copyright-infringing paraphrase in multilingual video captions for a streaming service. Include data collection from captions, a lightweight detector (rule-based plus a small ML model), latency targets, privacy constraints, and how you’d validate with offline metrics and staged rollouts?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Bloomberg","Cloudflare","Netflix"]},{"id":"q-2406","question":"Design a scalable real-time multilingual intent recognition and routing system for chat and voice channels in a global support platform, supporting English, Spanish, and Japanese, with privacy constraints, a retrieval-augmented generation path, and a policy-driven fallback. What architecture, latency targets, privacy controls, and evaluation plan would you propose?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Amazon","Google","Snap"]},{"id":"q-2466","question":"Design a scalable, real-time multilingual NER system for customer support chat that can adapt to domain-specific entities (brands, products) with drift monitoring and minimal latency. Include data pipeline, model choice, schema for entities, evaluation strategy, and privacy/bias considerations?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Oracle","Snap"]},{"id":"q-2574","question":"Design a beginner NLP pipeline for real-time intent classification of chat messages in an e-commerce support setting. Dataset: ~10k English chats labeled with intents: 'billing', 'technical', 'account', 'other'. Build an end-to-end pipeline using a non-neural baseline (TF-IDF + logistic regression). Constraints: inference latency < 50 ms per message, optional stopword handling, easy retraining, and explainability via top contributing tokens per class. Include preprocessing, feature extraction, model choice, evaluation (cross-validation, macro-F1), and a simple staged rollout with monitoring and rollback?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Google","Microsoft","Slack"]},{"id":"q-2661","question":"Design an on-device, privacy-preserving NLP pipeline for a fintech mobile app that handles sensitive user chats. The system should classify intents (**security**, **funding**, **trading**, **account help**), detect high-risk content, support multilingual input (including low-resource languages), deliver latency under 50 ms per message on-device, and support offline model updates. Include data lifecycle, privacy guarantees, model versioning, and a live A/B/rollback plan; compare on-device vs cloud/offload trade-offs?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Hugging Face","Robinhood"]},{"id":"q-2700","question":"Design an end-to-end, real-time NLP system to detect misinformation in multilingual social posts that frequently code-switch between English and another language (e.g., Hindi or Spanish). Include data ingestion, code-switch aware embeddings, latency targets, privacy constraints, cross-language evaluation (macro-F1 and per-language fairness), debiasing, explainability (token attributions), and a staged rollout with monitoring and rollback?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Google","Meta","Salesforce"]},{"id":"q-2720","question":"Design a beginner NLP classifier for multilingual customer support chats that labels intents: 'billing', 'technical', 'account'. Data may include English-Spanish code-switching and emojis. Use a lightweight pipeline: baseline TF-IDF + logistic regression, augmented with character n-grams and emoji tokens. Add a small rule-based post-filter for profanity. Define data split, macro-F1, per-language eval, and a three-stage rollout with monitoring and rollback?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Meta","MongoDB","Twitter"]},{"id":"q-2793","question":"Design an NLP-powered policy-editor inside a Cloudflare-like edge platform that translates natural-language security intents into concrete firewall/CDN rules, with multilingual input, strict privacy constraints, and explainability. Include a MongoDB-backed audit/versioning system, latency target <150 ms, drift detection, and a rollout plan comparing retrieval-augmented vs template-based translation?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Cloudflare","MongoDB"]},{"id":"q-2851","question":"Design an end-to-end real-time NLP pipeline to detect and neutralize adversarial text perturbations (typos, leetspeak, Unicode homoglyphs) in a multilingual social feed spanning 50+ languages, achieving sub-150 ms latency per event, with on-device preprocessing and privacy-preserving cloud inference. Include a hybrid detector (character- and word-level), defense against obfuscation, and a drift-aware evaluation plan?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Meta","Twitter"]},{"id":"q-2948","question":"Design a beginner-friendly NLP task: build an emoji-aware sentiment classifier for bilingual English/Spanish customer support chats in a fintech product context. Requirements: baseline bag-of-words + logistic regression; optionally a small fastText-like model for code-switching; handle code-switching; target latency < 50 ms on CPU; memory under 100 MB; no external APIs; provide explainability via keyword-level attribution; evaluate offline with macro-F1 and accuracy; outline a staged rollout with data drift checks?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Cloudflare","Plaid","Snowflake"]},{"id":"q-2997","question":"Design a beginner-friendly NLP pipeline to classify customer intents in a PayPal-style chat widget (e.g., refunds, transfers, verification), using a lightweight hybrid of rule-based cues and a small ML model. Include on-device inference for privacy, a cloud fallback, data-labeling plan, latency targets, and a simple evaluation strategy?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["PayPal","Tesla"]},{"id":"q-3239","question":"Design a live multilingual support assistant that uses retrieval-augmented generation to propose policy-compliant responses in 50 languages. The system must preserve user privacy with on-device encoding and secure cloud inference, target sub-180 ms latency per message, use vetted templates plus safety filters to prevent hallucinations, and include human-in-the-loop fallback, drift monitoring, and per-language rollback?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Anthropic","Instacart","Twitter"]},{"id":"q-3301","question":"Design a beginner-friendly NLP pipeline to classify customer-support intent for a fintech app in multilingual chats (e.g., account issues, payments, product info). Include on-device preprocessing, privacy-preserving cloud inference, and a hybrid baseline (rule-based + small ML). Specify data collection from chat logs, latency target (<200 ms per message), and how you'd validate with offline metrics and staged rollouts, including handling code-switching and labeling?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["IBM","PayPal"]},{"id":"q-3330","question":"You have a multilingual customer-review stream for a travel app in English and Spanish. Design a beginner NLP pipeline to classify reviews as positive or negative using a two-layer approach: a rule-based lexicon for negations and intensifiers plus a lightweight ML model (logistic regression on TF-IDF). Include preprocessing, code-switching handling, latency target, and how you’d evaluate offline metrics and rollout?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Airbnb","Hashicorp","Two Sigma"]},{"id":"q-3447","question":"Design a beginner NLP workflow for a Discord-like app that turns user feedback posts into structured support tickets. Classify intent as Bug, Feature, or Question, and extract fields like component and priority. Describe on-device preprocessing for privacy, a tiny ML baseline plus a rule-based post-processor, data labeling plan, and a staged rollout with latency targets?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Amazon","Discord","Hashicorp"]},{"id":"q-3590","question":"Design a streaming NLP detector that flags disallowed political content in live chat for a multinational streaming platform, delivering sub-100 ms per-message latency, multilingual coverage with zero-shot language detection, on-device preprocessing for privacy, and a drift-aware server-side updater; outline architecture, feature choices, evaluation, and escalation policy?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Cloudflare","IBM","Netflix"]},{"id":"q-3739","question":"Design a real-time, privacy-preserving NLP pipeline to monitor driver and rider feedback in a multilingual ride-hailing platform for safety and compliance signals. Include on-device preprocessing, federated updates via secure aggregation, <200 ms per event, drift detection, and a rollout plan with monitoring, rollback, and privacy guarantees?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Databricks","Lyft","Tesla"]},{"id":"q-3774","question":"Design a privacy-preserving real-time NLP pipeline for live customer support chats in a multilingual food-delivery platform to classify intent, sentiment, and risk (PII, hate). Use on-device preprocessing or DP-compliant cloud inference, target <200 ms latency, and strict data minimization. Include drift detection, rollout governance, rollback, and an A/B plan?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["DoorDash","Oracle"]},{"id":"q-3891","question":"Design a deployed multilingual NLP service for customer-support tickets that simultaneously classifies intent and redacts PII. Include data flow, privacy controls (data minimization, tenant isolation), scalable infra, per-language fairness metrics (macro-F1), PII recall, drift detection, and a rollback plan with canary deployments?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Adobe","Bloomberg","IBM"]},{"id":"q-3979","question":"Design a beginner-friendly NLP workflow to detect sarcasm in multilingual social comments with code-switching. Build a hybrid detector (rule-based cues + a small ML model), robust to misspellings, and outline data labeling, language coverage, latency targets, privacy, and how you'd evaluate offline and via staged rollouts?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Coinbase","LinkedIn","Netflix"]},{"id":"q-4005","question":"Design an end-to-end NLP pipeline that processes live corporate communications across 30 languages to extract and classify legally sensitive clauses (liability caps, indemnities, data processing terms), redact PII, and surface explainable risk scores in a centralized dashboard with tenant isolation and privacy guarantees. Include model versioning, drift detection, rollback, and a comparison of rule-based vs learned approaches for clause detection, with latency target under 250 ms per message?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Robinhood","Tesla","Twitter"]},{"id":"q-4030","question":"Design a real-time multilingual fraud-detection pipeline for a fintech customer-support chat that flags fraud intents, risky claims, and policy violations across 60 languages, with edge processing for privacy, latency under 100 ms per message, and drift-aware evaluation. Include data schemas, per-language adapters, and a governance plan with canary rollouts?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Coinbase","DoorDash","Salesforce"]},{"id":"q-4052","question":"Design a real-time on-device multilingual intent classifier for a consumer chat app that must learn from federated user data while preserving privacy. Target English and Spanish with code-switching; aim latency <40 ms per message; support remote addition of new intents with minimal labeled data. Outline model choice (quantized encoder vs TF-IDF + logistic), on-device training, privacy controls (DP-SGD, secure aggregation), deployment, and evaluation plan (per-language macro-F1, latency, privacy risk)?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Airbnb","Google"]},{"id":"q-4155","question":"Design a real-time NLP pipeline that ingests financial news feeds and social chatter in 12 languages to output market-signal events (e.g., sentiment shifts, entity-level risk signals for tickers, commodities) with sub-200 ms latency. Outline architecture, multilingual models, tokenization strategy, drift detection, privacy controls, and a live evaluation plan including backtesting with simulated trades and a rollback canary?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Bloomberg","Databricks"]},{"id":"q-4300","question":"Design a streaming NLP pipeline to detect three actionable intents (escalation, fraud flag, negative sentiment) in multilingual customer support chats for a multi-tenant SaaS platform. Constraints: mixed English and product terms, end-to-end latency < 20 ms per message on CPU, interpretable per-tenant explanations, and safe canary rollouts with per-tenant macro-F1 monitoring and rollback?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["MongoDB","Snap","Snowflake"]},{"id":"q-4304","question":"Design an end-to-end real-time NLP system to detect and extract operational incidents from live developer chat in a large ML platform. Identify mentions of outages or degradations, classify severity (S0-S3), and extract structured fields (service, region, timestamp, user impact). Route tickets to a Jira-like system, enforce multi-tenant privacy, and keep latency under 120 ms per message. Include ingestion, architecture, evaluation, drift monitoring, and rollout plan?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Google","NVIDIA"]},{"id":"q-4390","question":"Design a beginner NLP task to detect authenticity signals (genuine vs synthetic reviews) in a multilingual e-commerce feed. Build a lightweight hybrid detector that combines a text classifier with non-text signals (review age, user history, rating patterns) and run it with latency targets under 120 ms per review, prioritizing privacy (on-device)?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Amazon","Apple","Snap"]},{"id":"q-4398","question":"Design a beginner-friendly NLP pipeline to detect outage-related chat messages in a SaaS operations workspace (e.g., Slack/Teams) across English and Spanish, using a hybrid detector (rule-based keywords + a small ML classifier). Target latency <200 ms per message on commodity CPU, protect privacy, and provide explainability. Include data labeling, multilingual handling, deployment plan, and offline + staged rollout evaluation?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["DoorDash","Hashicorp"]},{"id":"q-4489","question":"Design a real-time, code-switched abuse detector for English–Spanish–Portuguese chats on a multinational platform. Include data strategy (synthetic + privacy-preserving collection), model (XLM-R with language adapters), latency target (<200 ms), and explainability (token attribution). Outline evaluation by language macro-F1, drift monitoring, and rollback plan?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Amazon","Hugging Face","Salesforce"]},{"id":"q-4530","question":"Design a multilingual, on-device plus cloud NLP system for automatic reply moderation in a global enterprise chat platform. Include a policy-as-code safety layer, counterfactual explanations for moderation decisions, and a privacy-preserving update mechanism that avoids redeploys. Specify latency targets, architecture, drift handling, and auditing?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Hugging Face","LinkedIn","Tesla"]},{"id":"q-470","question":"How would you implement a sentiment analysis pipeline for customer reviews that handles negation and domain-specific slang? What preprocessing steps would you prioritize?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["DoorDash","IBM","Square"]},{"id":"q-4795","question":"Design a real-time, multilingual NLP pipeline that ingests live chats and calls, performs ASR, translates on the fly, and outputs intent, sentiment, and risk indicators (fraud/abuse). Target sub-200 ms per utterance with on-device first-pass and privacy-preserving cloud inference, plus drift detection, explainability, model versioning, and a compliant audit log. How would you implement end-to-end?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Amazon","Google"]},{"id":"q-500","question":"How would you implement basic text preprocessing for sentiment analysis, including tokenization, stop word removal, and stemming?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Airbnb","Apple","Google"]},{"id":"q-584","question":"How would you implement a transformer-based model for real-time text generation with attention mechanisms that handle variable-length sequences efficiently?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Databricks","Lyft","Tesla"]},{"id":"q-229","question":"What is the difference between tokenization and stemming in NLP text preprocessing, and when would you choose lemmatization over stemming?","channel":"nlp","subChannel":"text-processing","difficulty":"beginner","tags":["tokenization","stemming","ner"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-294","question":"How does the attention mechanism in transformers allow the model to handle variable-length sequences without recurrent connections?","channel":"nlp","subChannel":"transformers","difficulty":"intermediate","tags":["cnn","rnn","transformer","attention"],"companies":["Amazon","Google","Meta"]},{"id":"q-1124","question":"On a NUMA‑aware Linux host with a fixed‑size worker pool handling high‑frequency RPCs, cross‑socket memory traffic is a bottleneck. Propose a concrete plan to minimize inter‑socket traffic: pin threads to sockets, allocate per‑socket data, and choose memory policies (numa_bind/numa_alloc_onnode). Include measurement steps and success criteria?","channel":"operating-systems","subChannel":"general","difficulty":"advanced","tags":["operating-systems"],"companies":["Discord","Netflix","Salesforce"]},{"id":"q-1299","question":"Scenario: two threads share a global 32-bit counter. Thread A increments it in a tight loop; Thread B logs the value once per second. Without synchronization, describe a concrete interleaving that yields a stale read or lost update, and explain the cache/coherence mechanics behind it. Then outline the minimal fix and how it ensures atomicity and visibility (e.g., atomic fetch_add or a mutex)?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Instacart","Square"]},{"id":"q-1354","question":"You implement a lock-free ring buffer with two atomics (head, tail) and a data array for inter-thread communication. Describe a concrete interleaving on a weak memory model (e.g., ARM64) where the consumer observes a stale value or an invalid read due to missing ordering. Propose a minimal fix using memory_order_release on the data write/tail update and memory_order_acquire on the consumer read, and sketch a safe patch?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Apple","Hugging Face","Meta"]},{"id":"q-1578","question":"On a Linux server, four worker processes share a 100GB read-only memory-mapped dataset loaded from disk on demand via mmap. Describe the sequence of page fault handling, TLB behavior, and how the kernel page cache and optional swap interact with this pattern. Propose two concrete knobs to maximize throughput without starving others (e.g., MADV_WILLNEED with madvise, NUMA binding with mbind) and how you would measure success?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Coinbase","DoorDash","Two Sigma"]},{"id":"q-1733","question":"Scenario: After a fork, a child writes to a Copy-On-Write (COW) page. Describe the end-to-end kernel steps from the write fault to the point where the parent and child have separate views, including page table updates, TLB changes, and how the private copy is created and isolated from the parent's mapping?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Databricks","Discord","Salesforce"]},{"id":"q-1759","question":"In a multithreaded server, each worker maintains a per-thread stats counter in an array of N 64-byte structs (one per thread). A separate thread periodically sums these counters every second. Without padding, explain a concrete interleaving that leads to cache line false sharing and degraded throughput, and propose a fix (padding, alignas cache-line, or per-thread local counters plus a reduction) that preserves correctness and improves performance?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["DoorDash","Netflix"]},{"id":"q-2084","question":"In a kernel memory allocator with per-core freelists and a global free pool protected by a spinlock, describe a concrete interleaving that yields a use-after-free for a block still in use by a reader, and explain how either hazard pointers or epoch-based reclamation prevents it, including the required memory-order guarantees on x86-64 and how grace periods are enforced?","channel":"operating-systems","subChannel":"general","difficulty":"advanced","tags":["operating-systems"],"companies":["Citadel","Netflix","Oracle"]},{"id":"q-2220","question":"Scenario: A process has two threads: T1 holds a mutex to update a shared log buffer; T2 is blocked trying to acquire the same mutex. A SIGINT is delivered to the process while T1 is in the middle of updating. Explain the sequence of events from signal delivery to termination, including the mutex state, potential race conditions, and safe patterns for signal handling in multi-threaded programs (like async-signal-safe handlers and deferred cleanup via a dedicated signal-handling thread or self-pipe)?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Coinbase","Google","NVIDIA"]},{"id":"q-2280","question":"In a simplified OS with a file-backed page cache, a process repeatedly calls read() to fetch 4 KiB blocks from a file. If a 4 KiB block is not in the file system page cache, describe step-by-step what the kernel does from read() entry to user-space return, including page-cache lookup, block-device reads, potential read-ahead, and how the data ends up in the user buffer. Compare a cache miss vs a cache hit path and timing?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Apple","MongoDB"]},{"id":"q-2404","question":"A multi-threaded app writes to a single file via buffered I/O on a journaling filesystem. Thread A appends 64KiB blocks; Thread B occasionally calls fsync(). Describe the path from write through the page cache to disk, including journal commits and barriers, and give a concrete interleaving where a crash can lose data or leave metadata only. Propose a minimal patch to guarantee durability (e.g., fdatasync after writes)?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Citadel","Databricks","NVIDIA"]},{"id":"q-2477","question":"Explain how Linux handles a write to a 2MB Transparent Huge Page (THP) that only partially overlaps with a 4KB region. Describe the fault path, the split_huge_page path (and khugepaged if applicable), PTE updates, TLB shootdowns, and the performance implications vs pre-splitting THPs?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Meta","Twitter"]},{"id":"q-2544","question":"Scenario: A process memory-maps a 4 KiB device block via mmap with MAP_SHARED and then writes through a user pointer that maps to that page. Explain end-to-end how the kernel handles the write, including the MMU page fault, page cache interaction, dirty bit propagation, writeback, and how the data reaches the physical device, highlighting synchronization with other mappings and potential coherence issues?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Coinbase","OpenAI","PayPal"]},{"id":"q-2714","question":"In a high-throughput key-value service on Linux/x86_64, two threads share a 128-byte struct: a 64-bit value and a 64-bit sequence counter used for a sequence lock (seqlock). Describe how to implement a lock-free reader with a writer that updates the value safely, specifying the exact operation order, required memory barriers, and how you verify correctness under contention. Compare against a mutex approach and practical tradeoffs?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Apple","Instacart","Meta"]},{"id":"q-2734","question":"In a Linux‑like system, a user‑space producer and a kernel driver share a lock‑free 256‑entry ring buffer using per‑entry sequence numbers. Describe a safe publish/claim/consume protocol that guarantees no lost entries and a consistent view under concurrent updates, specifying the exact write order and memory barriers on x86‑64. Include a concrete test harness to validate under contention?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Meta","Twitter"]},{"id":"q-2943","question":"Describe a robust strategy for a parent process to collect exit statuses of multiple forked children in Linux. Include waitpid usage in a loop, handling of SIGCHLD, and zombie prevention. Also cover edge cases like stopped/continued children and race-free accounting?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Google","IBM"]},{"id":"q-2966","question":"Scenario: A latency‑sensitive service pins 32 worker threads to NUMA node 0 on a dual‑node server. A background writer allocates a large shared ring buffer used by all workers, but allocator fragmentation causes many pages to reside on Node 1. Explain how page allocation, NUMA policies, and TLB coherence interact to produce remote misses and increased cross‑node traffic. Propose a minimal policy (e.g., mbind/sysfs knobs or numactl) to keep hot data local and describe validation via microbenchmarks?","channel":"operating-systems","subChannel":"general","difficulty":"advanced","tags":["operating-systems"],"companies":["Apple","NVIDIA"]},{"id":"q-3028","question":"In Linux, two processes map the same 4 KiB file region with MAP_SHARED. One writer updates eight bytes; a reader loops to read the same region concurrently. Describe end-to-end what happens from the write through the page cache to the reader, including dirty/writeback, cache coherence, and TLB; then propose a minimal synchronization mechanism (e.g., a 4-byte lock or seqlock in the shared mapping) to guarantee a predictable visibility order and discuss tradeoffs?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Apple","Bloomberg","Netflix"]},{"id":"q-3184","question":"Scenario: A two-socket NUMA Linux server uses a shared 8 GiB mmap region for request data. Threads on both sockets touch pages frequently; explain how NUMA balancing, page migration, TLB shootdowns, and cache coherence interact to affect latency. Propose a concrete microbenchmark to measure cross-node memory access and a pragmatic fix (pin memory to local nodes or adjust policies) with expected trade-offs?","channel":"operating-systems","subChannel":"general","difficulty":"advanced","tags":["operating-systems"],"companies":["Google","LinkedIn","Meta"]},{"id":"q-3243","question":"Explain a concrete race in a Linux-like kernel when a memory-mapped page is faulted in while a concurrent writeback evicts the same page. How does page cache coherence, dirty flags, and TLB invalidation interact to yield a stale read or lost write? Propose a minimal fix to guarantee visibility without sacrificing performance?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Meta","Microsoft","Uber"]},{"id":"q-3490","question":"Explain the exact behavior and safety implications when one process mmap()s a file region with MAP_SHARED, and another process truncates the same file while the mapping exists; detail what happens on access at an offset beyond the new end, which kernel data structures are involved, and minimal steps to guarantee correctness in a multi-process program?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Google","Tesla","Zoom"]},{"id":"q-3525","question":"Design a lock-free bounded ring buffer for 1 producer and 1 consumer sharing memory. Each slot stores a 64-byte payload and a 64-bit sequence. The producer writes payload, then updates the sequence to indicate new data; the consumer reads only when the sequence indicates fresh data, then advances. Describe exact write/read order, x86-64 memory barriers, wraparound handling, and empty/full detection without locks?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Microsoft","Netflix"]},{"id":"q-3554","question":"In a Linux-like OS using io_uring for async I/O, an 8-core NUMA system with a shared submission/completion ring handles bursts of 4KiB reads to NVMe. A per-I/O sequence is proposed; describe a concrete interleaving that could cause a completion to be lost or delivered out of order, and specify the memory ordering needed to guarantee in-order, at-least-once completions. Propose a minimal fix (e.g., per-I/O sequence numbers) and discuss performance trade-offs?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Discord","IBM","Snowflake"]},{"id":"q-3614","question":"On a Linux-like kernel using hazard-pointer-based reclamation for a shared in-memory index, writers replace entries by atomic pointer swaps and reclaim old nodes after a grace period. Describe a concrete interleaving on a 2-socket NUMA system that could cause a reader to observe a newly published but uninitialized node, and explain the required memory ordering to prevent it (publish with release, read with acquire, or use a ready flag). Propose a minimal fix and discuss trade-offs?","channel":"operating-systems","subChannel":"general","difficulty":"advanced","tags":["operating-systems"],"companies":["Adobe","Meta","Plaid"]},{"id":"q-3625","question":"In a Linux kernel driver for a PCIe DMA device, the host uses a shared descriptor ring with the device. A producer thread writes descriptors and updates a head pointer; the device consumes descriptors and raises an interrupt on completion. Describe a concrete interleaving that could yield a lost or out-of-order completion if there is no proper synchronization between producer writes and device processing. Propose a minimal fix using per-descriptor sequence numbers and memory barriers (e.g., smp_wmb, dma_wmb) and explain how this guarantees in-order, at-least-once completion, including cache coherence and DMA semantics?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Citadel","Google","MongoDB"]},{"id":"q-3764","question":"Scenario: A 10 Gbps NIC with multiple receive queues experiences a burst that floods the RX path. Explain the end-to-end path from RX interrupt to user-space delivery, including hardirq, NAPI polling, and the spillover to ksoftirqd, and how CPU affinity and irqbalance affect latency. Provide concrete measurement steps and mitigations (NAPI budget, backlog tuning, per-queue IRQ affinity, CPU isolation)?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Microsoft","Snap","Tesla"]},{"id":"q-3892","question":"Two processes write concurrently to the same file opened with O_APPEND. Describe end-to-end how the kernel ensures atomic appends, what can cause atomicity to fail (buffered I/O, network FS), and how you would guarantee visibility and ordering in practice?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Hugging Face","Instacart","NVIDIA"]},{"id":"q-4229","question":"On a Linux host running multiple containers, memory pressure spikes and kswapd starts reclaiming pages while some containers still serve high request rates. Describe the full sequence from a user-space page fault to eviction, including page cache lookup, anonym vs file-backed handling, reclaim, writeback, and LRU state movement. Explain how THP interacts with this path and the trade-offs of swapping vs not swapping under pressure?","channel":"operating-systems","subChannel":"general","difficulty":"advanced","tags":["operating-systems"],"companies":["Airbnb","Citadel","IBM"]},{"id":"q-4258","question":"Explain, at a practical level, the sequence of events when a process touches a stack address just beyond the current stack limit and triggers a stack-growth fault in a simple Linux-like VM. Include how the kernel decides to grow the stack, allocates and zeros the new page, updates the user page table, adjusts the stack pointer, and invalidates the TLB. Mention edge cases such as concurrent growth and guard-page handling?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Adobe","Plaid","Twitter"]},{"id":"q-4313","question":"Scenario: A database process uses a 1 GiB region mapped with MAP_ANONYMOUS and userfaultfd to handle on-demand paging. Multiple workers fault pages concurrently to populate the region with hot data. Describe the race conditions when two workers fault the same page, how to coordinate with per-page state, and how to ensure visibility and ordering across faults. Propose a concrete synchronization strategy (e.g., per-page lock with double-checked loading) and discuss performance trade-offs?","channel":"operating-systems","subChannel":"general","difficulty":"advanced","tags":["operating-systems"],"companies":["Apple","Google","MongoDB"]},{"id":"q-4356","question":"In a MAP_SHARED region, two threads access a 4-byte integer at offset 0 without synchronization. Provide a concrete interleaving that can yield a torn/partial read and explain why. Then specify the fix: use atomic stores or memory barriers, or a proper synchronization primitive; describe which memory_order to use and why, and how to ensure visibility across cores?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Databricks","Netflix"]},{"id":"q-4373","question":"In a Linux server with Transparent Huge Pages (THP) enabled and memory overcommit disabled, a spike in allocations causes THP allocations to fail while small pages are still free. Describe the exact THP allocation path, why fragmentation triggers this failure, and provide a concrete mitigation plan with tuning steps and measurable signals?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Amazon","LinkedIn","Square"]},{"id":"q-4410","question":"Scenario: a Linux server uses an anonymous mmap buffer as a circular IPC queue shared by 8 workers. Each slot contains an 8-byte header (seq, done) and a 256-byte payload. Writers update header before/after writing payload; readers rely on header to detect complete entries. Propose a precise, lock-free protocol that guarantees readers never observe partially updated entries, including the exact write/read order, required memory barriers, and a concrete contention test on real hardware?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Netflix","Stripe"]},{"id":"q-448","question":"Explain how process scheduling works in an operating system. Which scheduling algorithm would you choose for a real-time system and why?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["DoorDash","OpenAI","Snap"]},{"id":"q-4484","question":"In the Linux kernel, design a lock-free single-producer, multiple-consumer ring buffer for a device interrupt that enqueues event pointers. Describe the exact sequence and memory ordering you would enforce (which atomic ops, release/acquire semantics, and ABA avoidance), how you prevent data races across slots, and why a spinlock would be slower. Illustrate with a concrete interleaving involving two consumers and one producer?","channel":"operating-systems","subChannel":"general","difficulty":"advanced","tags":["operating-systems"],"companies":["LinkedIn","Microsoft","Oracle"]},{"id":"q-4502","question":"On a single-core Linux-like system, two threads share a CPU: a CPU-bound worker and an I/O-bound worker that often sleeps on I/O. Explain how the Completely Fair Scheduler (CFS) uses vruntime and per-thread weights to decide which thread runs next, and why the I/O-bound thread tends to break up CPU time. Propose a minimal, practical adjustment (e.g., using cgroups cpu.shares or adjusting nice values) to improve fairness without sacrificing throughput?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Databricks","Instacart"]},{"id":"q-4538","question":"On a NUMA Linux host running latency-sensitive containers, 2–5 ms spikes appear under heavy CPU load. Describe an end-to-end investigation and fix focusing on IRQ affinity, softirq, and scheduler interplay. Include how to identify root cause with ftrace or perf, isolate a CPU, pin the latency task, redirect device interrupts away, and validate that throughput remains stable?","channel":"operating-systems","subChannel":"general","difficulty":"advanced","tags":["operating-systems"],"companies":["Adobe","Zoom"]},{"id":"q-4554","question":"Explain, at the kernel level, how a blocking read() on a pipe is woken up when data arrives. Describe the sequence from data entering the pipe buffer to read() returning in user space, including the pipe's wait queue, the wakeup path, context switch, and the copy_to_user step?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Airbnb","Robinhood","Twitter"]},{"id":"q-4643","question":"In Linux, two processes share a 256-byte ring buffer mapped from persistent memory. The producer writes items with a 64-bit sequence number and a 64-bit payload. Propose a lock-free protocol guaranteeing no torn reads, in-order delivery, and crash-consistent recovery. Include exact operation order, memory barriers, and how to validate correctness on both x86-64 and ARM64?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Google","Tesla"]},{"id":"q-4692","question":"Design a robust lock-free multi-producer, single-consumer ring buffer used for inter-thread messaging with two producers and one consumer. Each slot holds a 64-bit key, a 64-bit value, and a 64-bit sequence. Describe the protocol (indices, per-slot sequence), the exact memory barriers required on x86_64 and ARM64, and a minimal code sketch for enqueue and dequeue. Include a concrete test plan that stresses visibility, wrap-around, and ABA issues?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Anthropic","Salesforce","Tesla"]},{"id":"q-471","question":"You're designing a GPU memory manager for CUDA applications. How would you implement a memory allocator that handles both unified memory and explicit device memory, considering fragmentation, coalescing, and the 48-bit address space limitations?","channel":"operating-systems","subChannel":"general","difficulty":"advanced","tags":["operating-systems"],"companies":["Amazon","NVIDIA"]},{"id":"q-4714","question":"In Linux, two processes share a 64-bit counter via a memory-mapped shared region (MAP_SHARED). Process A writes the counter as a single 64-bit write; Process B reads the counter once per second. Describe a concrete interleaving that yields a torn read on 32-bit architectures and explain why atomicity of 64-bit writes is not guaranteed there. Propose a minimal safe protocol to ensure atomicity and visibility, including exact operation order (atomic 64-bit ops vs version-tag approach)?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Google","Netflix"]},{"id":"q-4786","question":"Explain end-to-end how the Linux-like kernel handles a page fault that triggers stack growth for a thread due to deep recursion. Describe the steps from faulting instruction to resumed execution: fault diagnosis, stack VMA checks, guard pages, allocating a zero-filled page, updating the stack PTE, and issuing a TLB shootdown; address synchronization for multi-threaded growth and potential performance costs?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Adobe","Bloomberg","Two Sigma"]},{"id":"q-4820","question":"Design an in-memory, shared-hash-map for a high-concurrency database worker pool. Implement an RCU-based read path so reads never block even during hash-table resizing, and writers perform safe updates by replacing a bucket structure and deferring reclamation. Explain grace periods, memory barriers on x86_64, and how to reclaim old nodes without use-after-free. Include a minimal code sketch and a microbenchmark outline?","channel":"operating-systems","subChannel":"general","difficulty":"advanced","tags":["operating-systems"],"companies":["Citadel","MongoDB","Scale Ai"]},{"id":"q-530","question":"A process is stuck in 'D' state (uninterruptible sleep) during I/O operations. How would you debug this, what causes it, and how does it differ from 'Z' zombie state?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Adobe","LinkedIn","Netflix"]},{"id":"q-556","question":"How would you debug a process that's consuming 100% CPU but not responding to signals? What tools and steps would you use?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Snowflake","Two Sigma"]},{"id":"q-585","question":"How would you implement a lock-free concurrent queue using atomic operations and memory barriers? What are the trade-offs between ABA problem solutions?","channel":"operating-systems","subChannel":"general","difficulty":"advanced","tags":["operating-systems"],"companies":["Citadel","OpenAI","Square"]},{"id":"q-927","question":"In a system with a fixed-size circular buffer of size N shared by a producer and a consumer thread, implement a thread-safe producer-consumer solution using semaphores and a mutex in C. Include initialization, edge cases (buffer full/empty), and show how you would test for deadlocks and correctness under concurrent producers/consumers?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Adobe","OpenAI","Scale Ai"]},{"id":"q-995","question":"In a system using paging with a TLB, describe the end-to-end sequence when a 4 KB page accessed by a process is not mapped in RAM, from fault to resume, including the fault handler, page-table walk, TLB update, disk I/O to swap, and how the eviction policy decides which page to replace?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Lyft","NVIDIA","Stripe"]},{"id":"q-263","question":"How does demand paging optimize memory utilization in virtual memory systems, what triggers page faults, and which algorithms handle page replacement when physical memory is full?","channel":"operating-systems","subChannel":"memory","difficulty":"intermediate","tags":["virtual-memory","paging","segmentation","cache"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-1070","question":"You are building a latency-sensitive telemetry ingestion pipeline for autonomous fleets. Design an end-to-end data path from edge sensors to a real-time analytics sink. Include data schemas, serialization (protobuf vs Avro), transport (Kafka vs Kinesis), fault tolerance (idempotency, replay), schema evolution, and testing strategy. Explain trade-offs for throughput, latency, and reliability?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Lyft","NVIDIA","Tesla"]},{"id":"q-1090","question":"Design a real-time notification system for 1M+ concurrent users with end-to-end latency under 200ms. Describe architecture, data model, delivery guarantees, back-pressure, disaster recovery, and how you measure and enforce SLOs?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Apple","Cloudflare","Discord"]},{"id":"q-1115","question":"In a global OTCA stack for a large platform, design a fault-tolerant telemetry pipeline that ingests 10k events/sec, preserves dashboards with sub-30s freshness, and scales regionally. Describe data model, streaming, storage, and tracing choices (e.g., Kafka, OpenTelemetry, ClickHouse/BigQuery), backpressure handling, and testing strategy?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["DoorDash","Meta","Twitter"]},{"id":"q-1171","question":"Design a multi-tenant OTCA telemetry pipeline with per-tenant quotas and privacy masking. Ingest 20k events/sec regionally via Kafka, route by tenant_id, and compute deduplicated aggregates in Flink. Hot metrics in Redis, long-term data in ClickHouse/BigQuery, traces via OpenTelemetry. Include idempotent writes, backpressure handling, and robust testing?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Adobe","Coinbase","Google"]},{"id":"q-1219","question":"In a multi-tenant OTCA pipeline for a global fintech platform, how would you implement tenant-scoped telemetry collection that isolates data, preserves sub-second dashboards, and enforces per-tenant quotas? Describe data model, per-tenant exporters, sampling, storage with row-level security, and testing strategy?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Goldman Sachs","PayPal"]},{"id":"q-1237","question":"In a multi-tenant OTCA telemetry stack for a global SaaS, migrate from a fixed event schema to an evolving, backward/forward-compatible schema while preserving per-tenant data residency. The system must sustain up to 60k events/sec with regional bursts. Describe data model, schema versioning, streaming backbone, storage strategy (raw + materialized), tracing, backpressure handling, tenant-level sampling, and testing plan?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Databricks","MongoDB","Zoom"]},{"id":"q-1286","question":"In a global OTCA stack, design a fault-tolerant telemetry pipeline that ingests 20k events/sec per region from web/mobile clients, enforces per-tenant data residency, and uses adaptive sampling for long-tail tenants. Specify data model, streaming, storage, tracing, backpressure, and a test plan that validates burst behavior, schema evolution, and failover?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Meta","Snowflake"]},{"id":"q-1307","question":"In a global OTCA telemetry stack with three regions, enforce tenant residency by region while enabling real-time global dashboards with sub-500ms latency. Provide the end-to-end ingestion, storage, and aggregation plan, including data model, streaming/backplane choices, per-tenant partitioning, cross-region replication policy, and a validation strategy for residency, schema evolution, and burst traffic?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Cloudflare","Oracle","Twitter"]},{"id":"q-1495","question":"In a two-region OTCA telemetry pipeline for a microservices platform, design a privacy-preserving, adaptive sampling plan for distributed traces that enforces per-tenant data residency, minimizes data egress, and sustains sub-400ms end-to-end latency for dashboards. Detail the trace data model, propagation scheme, sampling algorithm, backpressure handling, and a validation plan?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["LinkedIn","NVIDIA","Snap"]},{"id":"q-1598","question":"Design a lightweight client-side OTCA telemetry exporter for a mobile app used across regions. The exporter must batch events (5 seconds or 1000 events), attach fields: tenant_id, device_id, app_version, event_type, and timestamp; implement offline queuing with local storage, retry with exponential backoff and jitter, and ensure eventual delivery when connectivity returns. Describe data model, batching, retry, and testing plan, plus how you measure correctness and dashboards?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["Anthropic","Tesla","Zoom"]},{"id":"q-1731","question":"In a global OTCA telemetry stack spanning five regions, tenants' raw events must remain within their origin region; only anonymized aggregates cross regions for global dashboards with sub-200ms latency. Design the end-to-end ingestion, streaming, storage, and access controls. Specify data models, de-identification/privacy controls, cross-region aggregation, backpressure, and a testing plan to validate residency, privacy, and latency under burst traffic?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Bloomberg","Microsoft","Oracle"]},{"id":"q-1847","question":"For a global OTCA telemetry stack supporting a multi-tenant ML inference platform, design a region-aware telemetry pipeline that records: tenant_id, model_id, input_hash, latency_ms, outcome, and drift_score. Propose a streaming backbone, OLAP store, and a per-tenant residency policy with SLA-based QoS, plus backpressure, schema evolution, and testing plan?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Hugging Face","Robinhood","Two Sigma"]},{"id":"q-1900","question":"You're building a beginner OTCA telemetry pipeline for a mobile app used in two regions. Design a minimal streaming path that logs only essential fields (tenant_id, event_type, timestamp, latency_ms) and enforces per-tenant data access and privacy (redaction/anonymization). Describe the data model, streaming backbone, storage, and a practical test plan to validate privacy, QoS, and dashboard freshness (<=60s)?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["Apple","Microsoft"]},{"id":"q-1924","question":"Design a beginner OTCA telemetry path for a mobile app where events include tenant_id, event_type, timestamp, latency_ms. Implement a simple on-device dedup using event_id, normalize event_type into a canonical event family, and publish to a single Kafka topic with per-tenant routing to regional storage (Parquet on S3) to meet residency. Describe data model, streaming path, storage layout, and a minimal test plan validating dedup, schema evolution, and cross-region consistency?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["Amazon","Plaid"]},{"id":"q-1999","question":"Design a global OTCA telemetry pipeline for real-time feature experimentation across MongoDB, Netflix, and Nvidia workloads. Each event includes device_id, experiment_id, feature_id, timestamp, latency_ms, and consent_flag. Requirements: per-tenant QoS with adaptive sampling, privacy masking for device_id, versioned schema with backward compatibility, cross-region attribution, and sub-200ms dashboard freshness. Outline data model, streaming backbone, storage layout, backpressure handling, and testing strategy?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["MongoDB","NVIDIA","Netflix"]},{"id":"q-2044","question":"Design a beginner OTCA telemetry path for a mobile app that must enforce per-tenant residency, basic privacy redaction, and monthly cost quotas while handling up to 5k events/sec. Provide a concrete data model (tenant_id, event_type, timestamp, latency_ms, event_id), a streaming topology (on-device dedup, region-specific Kafka, region-local Parquet storage with cross-region replication), and a test plan to verify residency, privacy, dedup, quota enforcement, and dashboard freshness?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["NVIDIA","Oracle","Scale Ai"]},{"id":"q-2157","question":"Design a federated OTCA telemetry pipeline for a multi-tenant platform deployed on AWS, GCP, and Azure, where each event includes tenant_id, service_id, event_type, timestamp, latency_ms, and model_version. Propose a central schema registry with per-tenant Protobuf contracts and strict forward/backward compatibility, a streaming backbone with tenant-scoped topics (e.g., Pulsar), cross-cloud replication, and a compliant data deletion/retention policy. Outline data contracts, backfill strategy, testing, and observability?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Anthropic","Plaid","Salesforce"]},{"id":"q-2198","question":"Design an edge-to-cloud OTCA telemetry pipeline for a latency-sensitive mobile app with intermittent connectivity. Edge devices buffer per-tenant events locally and flush when online; ensure per-tenant isolation, at-least-once delivery with de-dup, and sub-5s dashboard freshness after reconnection. Describe data model, edge processing, local storage, backpressure, retry, and testing plan with failure scenarios?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Hashicorp","Plaid","Two Sigma"]},{"id":"q-2358","question":"You're deploying an edge-to-cloud OTCA telemetry path for a fleet of Nvidia Drive-like autonomous robots across two continents. Each robot emits ~10k events/sec; dashboards must refresh in under 15 seconds with per-tenant residency. Design the data model, streaming backbone, storage format, backpressure, privacy, and testing strategy, including failure scenarios and cross-region replication?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Lyft","NVIDIA","Tesla"]},{"id":"q-2452","question":"In a global OTCA telemetry pipeline for a multi-tenant SaaS platform, design a policy-driven per-tenant data lifecycle (e.g., 7, 30, 90, 365 days) that governs hot storage retention, archiving, and purging. Describe data model changes, a central policy store, per-tenant routing to regional storage, an immutable journaling layer, cross-region archiving, and testing plan for lifecycle transitions?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Citadel","Oracle","Tesla"]},{"id":"q-2473","question":"Design an OTCA path for a real-time trading platform to capture per-venue latency, routing decisions, and queue depth. Stream events to venue-partitioned sinks (per-venue Kafka topics with region routing) and store immutable, versioned logs in regional object stores with a write-once policy. Enforce residency by region, apply in-flight caps and lag throttling, test with synthetic bursts and replay checks?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Citadel","Robinhood"]},{"id":"q-2505","question":"In an OTCA telemetry pipeline spanning multiple regions with edge gateways that reconnect intermittently, design a region-aware, edge-first streaming path that ingests up to 20k events/sec. Include local bounded buffering, per-tenant privacy masking, dedup logic, eventual consistency on reconnect, Avro schema evolution, and cross-region aggregation. Provide data model, streaming backbone, storage plan, backpressure strategy, and a practical test plan?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Cloudflare","Meta","Oracle"]},{"id":"q-2600","question":"Scenario: A fintech-grade, multi-tenant OTCA telemetry pipeline must enforce per-tenant data residency, privacy, and SLA-bounded costs across mobile and embedded devices. Design: on-device aggregation, per-tenant sampling, and cross-region routing. Specify data model, streaming backbone, schema evolution, privacy controls, testing plan, and failure handling with backpressure?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Apple","PayPal","Stripe"]},{"id":"q-2634","question":"You operate a global OTCA telemetry stack for a multi-tenant analytics platform with 1,000 tenants across 5 regions. Design a tamper-evident audit trail path that supports per-tenant data retention and forensic replay while guaranteeing data integrity and compliance. Describe the data model, streaming backbone, immutability strategy, cross-region replication, and testing plan for tamper detection and replay accuracy?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Adobe","Microsoft","Scale Ai"]},{"id":"q-2742","question":"Design a global OTCA telemetry pipeline for an AI inference platform deployed at regional gateways. Each event includes tenant_id, model_id, input_type, inference_latency_ms, accuracy_pct, privacy_mask_level, timestamp, version. Propose an edge-to-core streaming path with per-tenant residency, canary rollouts for model upgrades, online drift detectors for latency and accuracy, and rollback triggers. Describe data model, streaming backbone, storage, governance, and testing plan?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Meta","NVIDIA"]},{"id":"q-2758","question":"Design an OTCA pipeline for multi-tenant ML feature provenance across regions. Build a provenance-first path that traces raw events to generated feature vectors, with per-tenant isolation, immutable region logs, and versioned writes. Specify data contracts, storage layout, cross-region replication, and a testing plan for reproducibility, drift, and auditability?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Amazon","Databricks","Google"]},{"id":"q-2929","question":"Design a privacy-preserving OTCA data path for a global analytics platform that ingests per-tenant user events containing PII. Implement per-tenant masking and strict residency: store raw events only in regional stores for 30 days with write-once policy, while computed aggregates are exposed globally. Specify data contracts, masking rules, lineage, and testing strategy?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Apple","Netflix","Oracle"]},{"id":"q-2967","question":"Design a beginner OTCA telemetry path for a mobile app that collects tenant_id, event_type, timestamp, latency_ms, event_id. Add per-tenant at-rest encryption with envelope keys and a basic auditable access log; describe data model, streaming topology (on-device -> Kafka -> region storage), storage layout, key rotation policy, and a minimal test plan to verify encryption, access logs, retention, and dashboard freshness (<=60s)?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["Citadel","Cloudflare","NVIDIA"]},{"id":"q-2996","question":"Design a beginner OTCA telemetry path for a mobile app feature-flag rollout across two regions. Events include tenant_id, event_type, timestamp, latency_ms. Requirements: deterministic on-device sampling per tenant; per-tenant data residency with privacy redaction unless consent; streaming path to regional storage with a minimal schema (tenant_id, event_type, timestamp, latency_ms, event_id). Describe data model, streaming topology, storage layout, and a practical test plan for privacy, QoS, and dashboard freshness (<=60s)?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["DoorDash","Two Sigma"]},{"id":"q-3010","question":"Design a beginner OTCA telemetry path for a mobile app that logs tenant_id, event_type, timestamp, latency_ms, and event_id. Add data lineage metadata at the edge (source_app, device_model, os_version) and propagate to regional storage with a small versioned schema. Enforce per-tenant privacy by redacting optional fields unless consent, and implement a lightweight routing policy to regional stores. Describe data model, streaming topology, storage layout, and a test plan for lineage accuracy, privacy, and dashboard freshness (<=60s)?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["Adobe","Citadel","Databricks"]},{"id":"q-3190","question":"In a global OTCA telemetry fabric across three clouds, design a data-quality focused OTCA pipeline that ingests 20k events/sec from heterogeneous producers, enforces a central schema registry with compatibility checks, and automatically remediates drift (e.g., missing fields, type changes) while preserving tenant isolation. Describe data model, streaming backbone, storage, lineage, per-tenant governance, canary rollout, and test strategy including synthetic drift, rollback triggers, and end-to-end QoS guarantees?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Amazon","Google","Netflix"]},{"id":"q-3226","question":"Design a global OTCA telemetry and config path for a CDN's edge POPs. Edge events ~20k/s per region; enforce per-tenant residency. Use region-local streams (Kafka/Pulsar) and immutable regional logs plus a global index. Canary-region feature flags, backpressure control, and testing: replay, chaos, drift checks, and privacy redaction?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Citadel","Snap"]},{"id":"q-3434","question":"Design a beginner OTCA telemetry path for a cloud API used globally. Events include tenant_id, api_endpoint, http_method, status_code, latency_ms, timestamp. Constraints: per-tenant dynamic sampling to cap data volume, privacy (IP redaction), regionally scoped storage, and 2 regional stores. Describe data model, streaming topology (SDK -> Pub/Sub -> regional storage), storage layout (Parquet/BigQuery), and a practical test plan for volume control, privacy, and dashboard freshness?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["Cloudflare","Google","Meta"]},{"id":"q-3458","question":"Design a beginner OTCA telemetry path for an offline-first mobile app that must sync when online. Data model includes tenant_id, event_type, timestamp, latency_ms, event_id, device_session. Requirements: local per-tenant isolation, on-device dedup, deterministic sync order, conflict resolution strategy (CRDT or last-writer-wins). Streaming: device queue → edge gateway → regional Kafka → Parquet on S3 with tenant/day partitioning. Tests: offline sync latency, privacy redaction, and cross-region consistency?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["Scale Ai","Tesla"]},{"id":"q-3608","question":"Design a provenance-first OTCA telemetry and config path for a global feature-flag system with per-tenant residency and cross-region policy evaluation. Specify data contracts, schema evolution, and how lineage from event to decision to outcome is stored and queried. Describe region-immutable logs, a global index, backpressure strategy, and a testing plan including drift detection, replay tests, privacy redaction, and canary correctness?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Airbnb","Google"]},{"id":"q-3845","question":"Design a multi-region OTCA path for a rideshare platform that ingests ride events (request, accept, GPS pings, payment) across markets. Route to per-market sinks with region routing, enforce data residency, and write to a write-once, versioned regional store. Include an immutable audit log, cross-region replay checks, and a clear failure-mode strategy for transient network blips?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["DoorDash","Square"]},{"id":"q-3973","question":"In a beginner OTCA telemetry scenario for a web app, introduce a new data-category 'sensitive_financial'. Design a lightweight governance flow: classify incoming events, redact or drop sensitive fields at ingestion, and apply per-tenant policies while preserving lineage. Specify the data model (tenant_id, event_type, timestamp, payload, category), streaming path (Kafka), storage layout (Parquet, tenant/date partitions), and a test plan validating policy evaluation, privacy, and audit logs?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["Netflix","OpenAI","Slack"]},{"id":"q-4029","question":"Design an OTCA path for a multi-region fleet telematics platform where each vehicle streams sensor data (gps, speed, battery) to regional sinks with per-vehicle residency, ensuring dashboards update within 150ms latency globally. Specify streaming backbone, per-vehicle routing, schema evolution, deduplication, backpressure, and how you test burstiness and data loss?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Hugging Face","MongoDB","Tesla"]},{"id":"q-4100","question":"In an OTCA-powered multi-tenant analytics service, design a real-time telemetry pipeline with per-tenant quotas and dynamic throttling. Enforce strict isolation, backpressure handling, and ingestion-time privacy via redaction or summarization. Include data model, streaming topology, storage for dashboards, latency budget, testing plan, and rollout strategy?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Apple","Two Sigma","Uber"]},{"id":"q-4170","question":"You're building a beginner OTCA telemetry pipeline for a multi-tenant mobile app that must support on-the-fly privacy toggles and real-time anomaly detection using a streaming SQL layer (ksqlDB or Materialize). Design the data model, streaming path, and a per-tenant policy mechanism to enable dynamic redaction or dropping of fields, plus a test plan validating policy correctness, latency under load, and anomaly signals during policy changes?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["Anthropic","Coinbase","Salesforce"]},{"id":"q-4177","question":"Design an OTCA path for a cross-region telemetry pipeline powering a Salesforce mobile SDK used by Tesla service centers. The system must support per-tenant opt-in/opt-out, in-flight PII redaction, rights to deletion, region residency, at-least-once delivery, and real-time dashboards plus immutable audit logs. Describe data model, streaming backbone, redaction strategy, and test plan?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Salesforce","Tesla"]},{"id":"q-4256","question":"Design a multi-tenant OTCA telemetry pipeline that enforces per-tenant data residency by routing events to region-local stores while exposing a privacy-preserving global analytics layer. Include data contracts, immutable regional logs, cross-region replication, per-tenant retention/redaction policies, and a test strategy for drift, compliance, and performance under burst traffic. Provide concrete topologies and trade-offs?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Coinbase","DoorDash","Goldman Sachs"]},{"id":"q-4290","question":"Design a cross-region OTCA telemetry pipeline for a multi-tenant analytics platform. Provide per-tenant isolation (namespaced streams and tenant-scoped schema), choose Kafka or Pulsar as the backbone, and store region logs immutably (Parquet/Delta). Add dynamic sampling, backpressure handling, drift detection, privacy redaction, and a replayable test path to validate end-to-end freshness?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Databricks","Oracle"]},{"id":"q-4325","question":"Design a 6-region OTCA-style telemetry pipeline for a security monitoring platform. Ingest ~50k security events/sec per region, enforce per-tenant isolation, redact PII at ingress, and enable deterministic replay for incident response. Specify data contracts, streaming backbone (Kafka with Schema Registry or Pulsar), storage format/location, cross-region replication, backpressure, and a testing plan (chaos, replay accuracy, drift checks)?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Bloomberg","IBM","Snap"]},{"id":"q-4354","question":"Design a fault-tolerant OTCA telemetry and config path for a multi-tenant autonomous vehicle platform deployed across 10 regions and 50k fleets, ingesting 100k events/sec. Ensure per-tenant residency, region-local immutable logs, a global index, data contracts, and schema evolution. Describe data model, streaming with Kafka vs Pulsar, storage choices (Parquet/Delta), cross-region replication, privacy, testing (replay, chaos, drift), and deployment strategy?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Bloomberg","Lyft","Oracle"]},{"id":"q-4382","question":"Design an OTCA telemetry path for a multi-tenant Nvidia GPU-accelerated inference platform spread across regions. Capture tenant_id, model_id, batch_id, latency_ms, throughput, GPU_utilization, and error_rate. Propose region-scoped Kafka topics, immutable regional logs, per-tenant residency with SLA-driven QoS and backpressure. Include data model, streaming, storage, drift detection, schema evolution, and a test plan with bursts and cross-region checks?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["MongoDB","NVIDIA"]},{"id":"q-4405","question":"In a multi-region OTCA telemetry stack for a fintech product, design a pipeline that scrubs PII at the edge, routes by tenant to region-specific sinks, and feeds a centralized ML inference channel with sub-200ms latency. Specify the streaming backbone, per-tenant routing, regional storage (Parquet in object stores), schema evolution strategy, backpressure controls, and a concrete test plan for residency, encryption, and drift detection?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Coinbase","LinkedIn","MongoDB"]},{"id":"q-4514","question":"Design a beginner OTCA telemetry path for a mixed edge-cloud app where tenants configure per-tenant sampling rates and data retention to manage cost. Include a data model (tenant_id, event_type, timestamp, latency_ms, sample_rate), a streaming path (edge filtering -> Kafka -> schema registry), and per-tenant privacy (PII redaction) with regional storage. Provide a test plan and dashboards for cost vs fidelity, with 60s end-to-end freshness?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["Apple","Goldman Sachs","Tesla"]},{"id":"q-4540","question":"Design an OTCA control-plane for cross-region, tenant-scoped quota and burst-management in a multi-tenant SaaS platform. Each tenant has a monthly credit, per-region quotas, and dynamic bursts. Specify data model (tenant_id, region, quota, used, reset), streaming path (region-local Kafka), storage (immutable regional logs + analytics store), and a policy engine with drift detection, privacy redaction, and rollback. Include testing plan and canary rollout?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Amazon","Microsoft","Stripe"]},{"id":"q-4558","question":"Design an OTCA path for a global ride-hailing fleet with intermittent connectivity. Edge devices buffer events, forward to regional Kafka topics with per-tenant routing. Data model: ride_id, driver_id_h, vehicle_id, lat, lon, event_type, ts. Apply region-salted hashing for PII; immutable logs stored in region stores (Parquet). Use streaming (Kafka Streams) for dedup and windowed analytics; enforce residency, backpressure, and 30s dashboard freshness. Outline data flow, storage, privacy controls, and tests?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Apple","Slack","Uber"]},{"id":"q-838","question":"You’re building a minimal product API in Express. Implement routes GET /products and GET /products/:id that returns 404 when not found. Include input validation, safe DB access via parameterized queries, and robust error handling. Explain your approach and provide a concise code sample?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["Airbnb","Discord","Meta"]},{"id":"q-908","question":"You have a global edge API gateway handling peak bursts. How would you implement a distributed token-bucket rate limiter per API key across regions using Redis? Outline the Lua script for atomic refill and consume (capacity 1000, refill 50 tokens/sec), how you expose headers, handle clock skew, and fallback when Redis is unavailable?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Bloomberg","Cloudflare","Square"]},{"id":"q-1012","question":"In a churn prediction problem, you have 20k customers and 500 features (mix of binary indicators and continuous metrics). PCA will be used before a logistic regression model to predict churn. Describe an end-to-end plan to (1) handle missing values and mixed data types, (2) scale features appropriately, (3) choose the number of components with cross-validated downstream performance, (4) interpret the top loadings for business insight, and (5) guard against leakage and overfitting in a production pipeline?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["DoorDash","NVIDIA","Plaid"]},{"id":"q-1068","question":"Design an online robust PCA for a streaming fraud-detection pipeline: 50k events/sec, 1000 features with missing values. Describe incremental component updates, robust outlier handling (robust PCA or GoDec variants), missing-data strategy, drift monitoring (eigenvalue gaps, loadings stability, score distribution), and how you’d validate downstream classifier performance under tight memory/time constraints?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Meta","Oracle","Tesla"]},{"id":"q-1104","question":"In a production streaming recommender system, you maintain an incremental PCA basis on 1M users and 3k features, updated hourly. Design an online robust PCA pipeline that adapts to non-stationary covariances, handles missing data, and detects concept drift. Describe algorithm choices (incremental/robust variants, forgetting factors), when to re-train vs update, how to align with past loadings, validation of downstream models after projection, and monitoring for numerical stability?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Airbnb","Goldman Sachs","Google"]},{"id":"q-1128","question":"You have 50k samples, 1k gene-expression features with many missing values. Design an end-to-end Sparse PCA pipeline to produce 40 interpretable components. How would you handle missing data, choose sparsity vs components, validate downstream models, and assess stability and biological coherence of loadings across folds?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Adobe","Twitter","Two Sigma"]},{"id":"q-1153","question":"Design a PPCA-based dimensionality reduction pipeline for real-time telemetry data: 1B feature vectors daily, 500 real-valued features with missing values and skew, to feed a downstream anomaly detector. Explain fitting PPCA with EM, selecting k via BIC on a rolling window, comparing to standard PCA, handling streaming updates with forgetting factors, and validating in production?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Salesforce","Zoom"]},{"id":"q-1175","question":"In a factory IoT setting, 20 devices stream 40 features each (numeric, with occasional missing values). You want a beginner-friendly PCA-based anomaly detector on the edge. Describe how you would handle missing values, decide the number of components, and translate top loadings into actionable maintenance signals for operators, while keeping the model lightweight on-device?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Apple","Twitter"]},{"id":"q-1218","question":"You have a 40-feature numeric customer-survey dataset with some missing values and skewed distributions. You want a beginner-friendly PCA-based feature set for a churn-classification model. Describe preprocessing steps (imputation, transformations, outlier handling), how to choose the number of components, and how to translate top loadings into concrete business signals for a dashboard while keeping the pipeline lightweight?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Databricks","Google"]},{"id":"q-1249","question":"You're building a real-time risk-scoring system for cross-border payments. Data arrives as numeric features with occasional missing values and a few graph-derived signals, streaming at high velocity. You need an incremental PCA that adapts to non-stationary distributions and yields 40 components. Describe how you would: (a) choose/update the number of components under drift, (b) perform online imputation without data leakage, (c) keep loadings interpretable for dashboards, (d) coordinate PCA updates with downstream models to control drift, and (e) design a robust rollback strategy with governance in production?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Cloudflare","Coinbase","Uber"]},{"id":"q-1288","question":"Design a daily-updated PCA-based representation for streaming telemetry with 300 features per vector, many missing values and sparse signals. Outline preprocessing, choice of incremental PCA approach (IPCA vs randomized SVD), when to refresh the basis, how to align new loadings with the existing basis, and how to validate the downstream anomaly detector after dimensionality reduction. Include concrete knobs (batch size, forgetting factor, drift thresholds)?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Google","Snowflake"]},{"id":"q-1306","question":"Imagine a factory IoT setup where edge devices stream 60 numeric features (with occasional missing values). You need a beginner-friendly, PCA-based anomaly detector that runs on-device. Outline a concrete pipeline: (a) how to handle missing values, (b) how to standardize and fit PCA (including incremental options), (c) how to decide the number of components for robust anomaly signals, and (d) how to translate top loadings into actionable operator cues on a dashboard or device indicator?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["MongoDB","Tesla","Uber"]},{"id":"q-1350","question":"Design a privacy-preserving, incremental PCA system for a fintech platform with 2M daily sessions and 150 numeric features, where missing values occur and regulatory privacy requires differential privacy. You must deliver a 50-component representation, support federated updates, monitor drift, and keep loadings interpretable for dashboards. Describe architecture, privacy budget, and an evaluation plan; include a concrete update protocol?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["PayPal","Salesforce"]},{"id":"q-1387","question":"Design an online, privacy-preserving PCA for streaming multi-tenant transaction data to support real-time fraud detection. Specify how to perform incremental PCA with a sliding window, apply differential privacy to loadings, detect drift and decide when to refresh the basis, handle missing values online, allocate the privacy budget, and validate downstream models under DP constraints. Include concrete metrics and thresholds?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Cloudflare","MongoDB","PayPal"]},{"id":"q-1467","question":"In a real-time analytics pipeline that ingests 50k vectors/sec, each with 128 features (some missing), you previously computed offline **PCA** on historical data. Design a streaming, incremental **PCA** approach to (a) decide when to refresh the basis, (b) handle missing values on arrival, (c) monitor component drift, and (d) validate downstream models after dimensionality reduction?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Google","Stripe","Twitter"]},{"id":"q-1512","question":"Design a real-time PCA-based anomaly detector for a streaming platform like Airbnb where a listing's feature vector mixes 150 numeric sensor values and 30 one-hot categorical indicators; describe an incremental PCA workflow that handles mixed data, missing values, and concept drift while maintaining interpretability and low latency; specify preprocessing, component refresh triggers, evaluation, and how to map loadings back to actionable signals?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Airbnb","Salesforce","Scale Ai"]},{"id":"q-1534","question":"Edge PCA on 30 numeric sensors: design a beginner-friendly pipeline that runs on a Raspberry Pi for real-time data compression and anomaly flagging. Describe normalization and simple imputation, how to pick components (explained variance with knee), how to interpret top loadings for operators, and a lightweight drift check over a day with few dependencies?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["IBM","NVIDIA","Uber"]},{"id":"q-1703","question":"Scenario: A healthcare analytics team has a dataset with 100 predictors, mix of continuous and binary indicators, plus missing values. They want to apply PCA to reduce to 5 components for a dashboard and as features for a simple logistic classifier. Describe the exact preprocessing steps, including handling missing data, dealing with binary features during PCA, scaling, and how you would decide the number of components. How would you translate the top loadings into interpretable dashboard signals for clinicians, and how would you validate this approach on a small holdout set?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Google","IBM","Two Sigma"]},{"id":"q-1863","question":"You're building a real-time fraud-detection pipeline that ingests 200 features per transaction, with many outliers and missing values. You choose RPCA to reduce to 6 components for an online anomaly detector and a logistic classifier. Outline concrete steps: how to impute/mangle missing data for RPCA, how to handle heavy tails, how to select k using cross-validated reconstruction error with a complexity penalty, how RPCA would differ from standard PCA in this scenario, how to implement incremental RPCA updates (forgetting factor, windowed EM) in streaming, and how to map top loadings to business signals in a dashboard and detect component drift across batches?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Coinbase","Discord","Snap"]},{"id":"q-2034","question":"Design an online, memory-bounded PCA system for a fleet of 1,000 warehouse-edge sensors producing 200 features per vector. Data include missing values and heavy-tailed noise. Describe end-to-end: (a) how many components to retain under a fixed RAM, (b) how to incrementally update the PCA basis and trigger refresh on drift, (c) online imputation for missing values, (d) how to translate top loadings into actionable alerts for maintenance or throughput, and (e) how to validate offline and online performance. Include concrete metrics and thresholds?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Anthropic","Databricks","Instacart"]},{"id":"q-2106","question":"You have a production-monitoring dataset with 60 numerical telemetry features and 30 binary indicators across 20 services. For a PCA-based compression to 5 components powering a dashboard health score, outline practical preprocessing (imputation, scaling, handling binary features), how to pick the 5 components (explained variance threshold and cross-validated downstream anomaly detection), and how to translate the top loadings into actionable operator signals, keeping the pipeline lightweight?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Hashicorp","PayPal"]},{"id":"q-2164","question":"In a live ad-placement system, a streaming feature set of 320 numeric features arrives at 5 Hz with intermittent missing values and occasional outliers. Design an online, robust PCA (sliding-window, robust covariance, and outlier-resilient loadings) that maintains a 40-component basis. Explain how you would (1) update the basis with drift detection, (2) handle missing data online without corrupting the basis, (3) translate top loadings into low-latency serving signals, and (4) validate downstream models under non-stationarity?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Discord","DoorDash","Twitter"]},{"id":"q-2259","question":"Given a retail analytics dataset with 50 numeric features, including binary flags and ordinal categories, you plan to run PCA on-device to reduce to 6 components for a lightweight gateway in a store. How would you handle mixed data types, imputation, scaling, component selection, and translating top loadings into concrete gateway actions while keeping compute and memory usage low?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Cloudflare","Square","Tesla"]},{"id":"q-2287","question":"How would you implement an incremental PCA-based real-time feature extractor for a streaming ad-click dataset with 120 numeric features (including many missing values) and 40 categorical features encoded via target encoding? Describe preprocessing (imputation, scaling, sparse data handling), incremental PCA update (k choice, forgetting factors), drift detection for explained variance and loadings, and how to translate top loadings into dashboard signals while avoiding leakage of sensitive target info?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["DoorDash","LinkedIn","Snap"]},{"id":"q-2313","question":"Design a privacy-preserving federated PCA-based anomaly detector across multiple regional data centers. Each site has 60k–120k samples with 150 numeric features, with missing data. No raw data leaves sites; a central server learns a global rank-k PCA basis via secure aggregation and incremental updates. Describe the end-to-end protocol: initialization, local imputation, online PCA updates, drift detection, handling non‑IID data, communication budget, and edge deployment constraints. Include concrete metrics and thresholds?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Citadel","Slack","Snap"]},{"id":"q-2372","question":"You operate a Netflix/Plaid-scale real-time content recommender with a 180-feature numeric telemetry table (e.g., interactions, dwell time, buffer events). Data arrives as a stream with occasional missing values. You decide to use an online incremental PCA to maintain a rank-k basis for a lightweight ranking model. Describe how you would: (1) select k under drift; (2) update the PCA online with a forgetting factor and missing-data imputation; (3) detect concept drift and trigger retraining; (4) translate top loadings into actionable signals for the live dashboard; and (5) validate both offline and in production, including a practical rollback plan?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Netflix","Plaid"]},{"id":"q-2506","question":"Scenario: In a fintech analytics gateway used by Plaid, Zoom, and PayPal, you process hourly matrices of 40 numeric features per session. Data are skewed and occasionally missing. Design a beginner PCA pipeline to feed a lightweight anomaly detector on the gateway; describe skew handling, missing value strategy, component selection, and how to translate top loadings into actionable alerts while keeping latency low?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["PayPal","Plaid","Zoom"]},{"id":"q-2534","question":"Design an online PCA system for streaming telemetry with 200 features arriving in minute batches from thousands of devices in a Databricks/Zoom-scale analytics gateway. Explain incremental updates, adaptive component count, online normalization and missing-value handling, drift detection, and how to translate top loadings into real-time alert rules while maintaining sub-second latency?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Databricks","Zoom"]},{"id":"q-2672","question":"Scenario: DoorDash handles streaming order features with 120 numeric indicators per event. Data drifts over time and some values are missing skewed. Propose an online PCA-based detector to power a lightweight anomaly alert in real-time routing and fraud checks. Describe incremental PCA approach, window size, handling of missing/skewed data, component selection, and how to translate top loadings into actionable alerts under strict latency constraints?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["DoorDash","Oracle"]},{"id":"q-2705","question":"Scenario: A wearable device streams 70 numerical sensor readings every second into a mobile gateway with 128 KB RAM. You need a beginner-friendly PCA-based anomaly detector running on-device. How would you design the pipeline, including (a) data preprocessing, (b) incremental PCA configuration and update policy, (c) component count strategy under memory limits, and (d) translating top loadings into real-time alerts while keeping latency under 25 ms?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Cloudflare","Lyft","Meta"]},{"id":"q-2748","question":"In a cloud-monitoring pipeline, 20 numeric metrics per server are streamed every minute from 1,000 servers. You want a beginner PCA-based anomaly detector that updates incrementally to adapt to concept drift while respecting a tight memory budget. Describe: (a) preprocessing and a sliding window strategy, (b) how to perform Incremental PCA and choose the number of components under a fixed memory footprint, (c) how to translate top loadings into real-time alerts, and (d) how you would validate and monitor for drift before deployment?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["LinkedIn","Netflix","Scale Ai"]},{"id":"q-2926","question":"In a privacy‑centric chat app analytics pipeline, 60 numeric telemetry features per session must be reduced with PCA to 12 components before sharing downstream. As a beginner, describe end‑to‑end steps: preprocessing (scaling, missing values), incremental vs batch PCA for streaming data, selecting the number of components to reach 90% explained variance, interpreting top loadings into actionable product signals, and validating downstream detectors while discussing privacy/latency tradeoffs?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Hugging Face","Lyft","Slack"]},{"id":"q-2959","question":"In a high-velocity fintech streaming gateway with 60 numeric features, design an online PCA pipeline that updates on mini-batches (e.g., 1000 events), handles intermittent missing/corrupted values, and chooses between incremental SVD and Oja's rule. Include online imputation, fading memory, component count, drift detection, and translating top loadings into real-time alerts with latency under 50 ms. What components, data flow, and evaluation would you implement?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Databricks","Robinhood","Two Sigma"]},{"id":"q-3072","question":"Scenario: A global delivery network uses 25 numeric telemetry features per drone; data arrive as a stream with occasional missing values and spikes. Describe a beginner PCA pipeline that runs in a streaming edge context: (a) missing value handling, (b) skew handling and scaling, (c) a rolling/incremental PCA with a fixed window to capture drift, (d) how many components and why, and (e) how to translate top loadings into simple, low-latency maintenance alerts on a dashboard while keeping latency under 50 ms per message?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["IBM","Uber","Zoom"]},{"id":"q-3106","question":"Scenario: A real-time surveillance analytics system streams 300 features per timestamp at up to 1k samples/sec. Use PCA to assist an edge anomaly detector and adaptive classifier. Design a time-aware Incremental PCA pipeline that handles missing data, non-stationarity, and concept drift. Explain how you pick the number of components, refresh cadence, online scaling/imputation, and how to translate top loadings into alerts while keeping latency under 10 ms per sample and low bandwidth?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Airbnb","Hugging Face","MongoDB"]},{"id":"q-3288","question":"In a streaming analytics pipeline for a professional network, events arrive with 100 numeric features, with occasional missing values and nonstationary drift. Design an online PCA using incremental SVD that keeps top-10 components, supports streaming imputation, adapts forgetting factor, and includes a drift detector. Describe reconstruction-error monitoring, update cadence, and how to map top loadings to real-time alerts on a dashboard while keeping per-event latency under 5 ms?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Databricks","LinkedIn","NVIDIA"]},{"id":"q-3365","question":"Scenario: An autonomous EV data pipeline streams 120 numeric features per ride, with occasional missing values and sensor dropouts. Build an online PCA-based anomaly detector that updates in under 20 ms per sample and adapts to concept drift. Describe initialization, online update (IPCA or incremental SVD), streaming imputation, drift detection, and how top loadings translate to edge alerts (e.g., sensor recalibration or adaptive sampling). Include validation plan with drift scenarios?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Meta","Snap","Tesla"]},{"id":"q-3527","question":"A real-time chat moderation pipeline for a popular platform collects 35 numeric features per message (e.g., sentiment, length, punctuation, emoji counts, time since last message). Data are skewed and occasionally missing. Design a beginner-friendly streaming PCA workflow that feeds a lightweight anomaly detector in real time; specify (a) missing-value strategy, (b) normalization, (c) how to select the number of components and adapt over time (e.g., IncrementalPCA), and (d) how to translate top loadings into actionable risk indicators on a dashboard while keeping end-to-end latency under 50 ms?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Adobe","Discord"]},{"id":"q-3552","question":"Scenario: A consumer analytics app collects 35 numeric features per user session, many features highly correlated and some missing. You need a beginner PCA-based feature extractor on-device (mobile) to reduce to 6 components before syncing to the backend. Describe how you would handle missing values, scaling, variance-based feature pruning, selecting the number of components, and how to translate top loadings into concrete in-app signals while keeping the client footprint small?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Coinbase","Netflix","Salesforce"]},{"id":"q-3684","question":"In a mobile analytics gateway for a social app, 32 numeric features stream at ~10 Hz. To protect privacy, implement PCA on-device and only send the reduced 8 components to the backend. Propose a beginner PCA pipeline: (a) on-device centering and scaling, (b) incremental PCA or warm-start SVD to update components with streaming data, (c) a simple differential privacy plan on transformed features (noise scale and privacy budget), (d) how many components to balance 90-95% explained variance with DP constraints, and (e) how to map top loadings to explainable alerts on the backend while preserving privacy?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["MongoDB","Snap","Uber"]},{"id":"q-3750","question":"In a real-time fraud-detection system processing 60 streaming features at a high event rate, design an online PCA to reduce to 10 components and feed a lightweight anomaly detector. Describe streaming imputation, incremental PCA updates, drift handling, component selection, and how to translate top loadings into actionable alerts while meeting latency goals?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Instacart","Meta","Robinhood"]},{"id":"q-3847","question":"In a streaming mobile analytics gateway collecting 120 numeric features per session in real time, data drift occurs. Design a beginner PCA-based feature extractor that adapts over time to compress features for a lightweight anomaly detector; explain missing-value handling, incremental fitting, how to pick k, and how to monitor drift to trigger retraining, all under strict latency constraints?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Goldman Sachs","Netflix","Snap"]},{"id":"q-3858","question":"Scenario: A fleet of autonomous delivery robots streams a 70-feature telemetry vector per second. Data drift occurs as environments change; you must maintain a rank-10 online PCA basis with strict memory and latency budgets. Describe the online update method (e.g., incremental SVD or Oja's algorithm), streaming-imputation for missing features, how you choose the component count under budget, and how you map top loadings to real-time alerts for subsystem health while keeping latency low?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Microsoft","Tesla","Zoom"]},{"id":"q-3895","question":"You're deploying a streaming PCA-based anomaly detector for a fleet of autonomous delivery vehicles. Each vehicle streams 50 numeric sensor features at 10 Hz. Design an online PCA pipeline that updates a rank-k basis incrementally, detects concept drift, and keeps latency under 5 ms per sample. How would you validate on holdout streams, manage missing data, and choose k and window size?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Apple","Tesla","Uber"]},{"id":"q-3928","question":"Design an online PCA solution for streaming telemetry from three edge gateways, each with 64 numeric features. Data arrive with missing values and non-stationary distributions. Propose a memory-bounded rank-k update, missing-value strategy, and a mechanism to translate top loadings into real-time alerts for system health, while keeping latency under 50 ms per batch?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Bloomberg","Netflix","Tesla"]},{"id":"q-4269","question":"You manage a fleet of autonomous delivery robots producing 120 numeric sensor features per second. Sensor quality varies; data is intermittently noisy with missing values. Design an incremental, weighted, robust PCA pipeline that (1) uses per-feature noise weights to handle heteroscedasticity, (2) imputes online, (3) updates a rank-k basis with a sliding window under memory constraints, (4) interprets top loadings into actionable maintenance alerts, and (5) keeps inference latency under 20 ms per robot?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Amazon","Apple","OpenAI"]},{"id":"q-4293","question":"You are designing a real-time anomaly detector for a fleet of industrial sensors generating 10,000 streams at 2 Hz, each with 60 numeric features. You need to run a PCA-based detector on-device with limited memory and intermittent missing data. Propose an incremental PCA approach with drift handling, including imputation, scaling, component selection, and how to translate top loadings into actionable alerts under tight latency (<50 ms per batch)?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Anthropic","Oracle","Tesla"]},{"id":"q-4359","question":"In a real-time streaming analytics system for a video platform, 180 numeric features per frame (comprising dense sensors and sparse one-hot indicators) arrive with occasional missing values and potential concept drift. Design an online PCA solution that (1) handles missing data and sparsity efficiently, (2) adaptively selects the number of components under drift, (3) yields interpretable loadings for explainable alerts, and (4) keeps end-to-end latency under 5-10 ms per frame. Detail algorithms, data structures, and evaluation plan?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Hugging Face","MongoDB","NVIDIA"]},{"id":"q-4396","question":"In a regulated analytics platform used by multiple teams, you must compute PCA without sharing raw data across teams. Design a privacy-preserving PCA pipeline (federated or DP-PCA) that handles missing values, chooses components, and translates top loadings into governance-safe alerts while meeting latency constraints?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Databricks","IBM","Meta"]},{"id":"q-4431","question":"You’re deploying a real-time edge gateway for a healthcare analytics platform that processes streaming data with 64 numeric features per event. The data are noisy, with intermittent missing values. Implement an incremental PCA producing 12 components with sub-5 ms latency per event. Describe missing value handling, streaming standardization, windowing for drift, component selection, and how top loadings translate into actionable alerts, plus a validation plan?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Google","Microsoft"]},{"id":"q-4485","question":"In a real-time analytics gateway used by Snowflake, LinkedIn, and Apple, streaming 25 numeric features arrive every minute with skew and occasional missing values. Design a beginner PCA pipeline to feed a lightweight anomaly detector: specify missing-value handling, scaling, how many components and why, and how to translate top loadings into actionable alerts while keeping latency under 100 ms per batch?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Apple","LinkedIn","Snowflake"]},{"id":"q-4687","question":"In a real-time risk gateway used by a popular trading app, you have 60 numeric features capturing order book stats, price changes, volatility, and volume. Data streams continuously with occasional missing values and outliers. You must deploy an online PCA that updates on a rolling window to feed a lightweight anomaly detector. Describe: online PCA approach, window size, missing data handling, component drift checks, and how to translate top loadings into actionable alerts within low latency?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Discord","Robinhood"]},{"id":"q-4733","question":"In a Netflix-style CDN telemetry feed, you stream 50 numeric features every 30s per edge node (latency, throughput, error rate, queue depth). Missing values appear and distributions drift over time. Design an online PCA pipeline that updates within 1 ms per update and uses ≤100 MB RAM. Explain data imputation, component count, update strategy (incremental PCA), and how top loadings translate to real-time QoS alerts without noisy dashboards?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Bloomberg","Netflix"]},{"id":"q-4745","question":"Design an online incremental PCA pipeline for a global telemetry platform where streams of 64 numeric features arrive per second from millions of devices. Data exhibit concept drift and occasional missing values. Propose a memory-bounded, drift-aware approach to decide the number of components, handle imputation and scaling, and translate top loadings into governance alerts that stay accurate as data evolves within latency constraints?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["IBM","Meta","Twitter"]},{"id":"q-837","question":"In a dataset with 120k samples and 200 features, after standardizing, you fit PCA and observe 95% variance explained by the first 8 components. How would you validate using PCA for downstream linear regression, decide the number of components, and interpret the top loadings? Consider missing values and large-scale data in your answer?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Amazon","Google","Plaid"]},{"id":"q-934","question":"Suppose a streaming analytics pipeline ingests 10k new vectors daily, each with 180 features, and PCA was computed on historical data. Describe an end-to-end approach to decide when to refresh the PCA vs keep the existing basis, how to measure component drift, how to align new loadings with the old basis, how to handle missing values in streaming data, and how to validate downstream models after dimensionality reduction?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Citadel","Discord","Hashicorp"]},{"id":"q-1117","question":"You're adding a real-time AI-powered product search for an Instacart-like app. With 8,000 concurrent users during lunch peak, design a beginner-friendly performance test that isolates the ML inference path from normal search, including cache warming and a clear success criteria?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Instacart","Snap","Zoom"]},{"id":"q-1210","question":"You're introducing a search API for a streaming e-commerce platform during a flash sale; expect 8k concurrent users, target median latency under 120ms and p99 under 250ms. The stack uses Redis caching with a relational DB. Design a beginner-friendly performance test plan to validate this, including tools, test data strategy, ramp pattern, metrics (p50, p90, p95, p99, error rate), and how you identify bottlenecks without affecting production?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["DoorDash","Netflix","Snowflake"]},{"id":"q-1533","question":"You're benchmarking a real-time bidding platform that processes 200k events per second with a 150ms SLA, deployed on Kubernetes with horizontal autoscaling and external services (ad server, fraud check, payment). Design a practical performance testing plan to identify bottlenecks and validate autoscaling, including workload mix, metrics, tools, and failure scenarios?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["IBM","LinkedIn"]},{"id":"q-1697","question":"You're benchmarking a real-time IoT telemetry pipeline: 2M events/sec ingested into Kafka, processed by Spark Structured Streaming, stored to a warehouse, and routed to a ML inference service with a 100ms SLA. Design a performance test plan to validate end-to-end latency, backpressure handling, and autoscaling when downstream latency spikes cause backlog. Include workload mix, metrics, tooling, and failure scenarios?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["Google","MongoDB","PayPal"]},{"id":"q-1712","question":"In a real-time payments microservice stack used by Amazon/PayPal, the /payments/process path calls fraud checks, credit checks, and a settlement queue. Bursts occur daily, with architectures needing to verify latency SLAs, error budgets, and autoscaling. Design a practical performance-test plan: workload model, test harness, metrics, failure scenarios, and how you would handle external-service variability and idempotency?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["Amazon","PayPal"]},{"id":"q-1803","question":"Design a performance-test for a real-time trade-confirmation service handling 150k events/s with an 8 ms SLA (99th percentile) during market open. System: Kubernetes, Kafka, Postgres, Redis; multi-region. Validate autoscaling, backpressure, circuit breakers, and tail latency under bursty traffic and regional failover. Outline workload, metrics, tools, and failure scenarios?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["Amazon","IBM","Robinhood"]},{"id":"q-2107","question":"Design a performance-test plan for a real-time personalized recommender serving 3 regional markets and relying on a centralized feature-flag service to drive A/B experiments. The flag service can throttle or fail under load. Outline traffic modeling, latency/outage injection, metrics (end-to-end P95/P99, throughput, error rate), backpressure handling, circuit-breakers, and graceful degradation criteria to validate SLOs?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Databricks","Tesla","Twitter"]},{"id":"q-2147","question":"You're performance-testing a ride-booking app's driver-matching API used by 3,000 concurrent riders during a rainstorm. The API uses an in-memory queue before dispatching to drivers. Design a beginner-friendly test plan to quantify end-to-end latency, identify bottlenecks in queue processing, and validate the impact of increasing the queue size?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Hugging Face","IBM","Meta"]},{"id":"q-2183","question":"In a ride-hailing app, the trip-matching service accepts ride requests and matches with drivers through an asynchronous pipeline using Kafka, with end-to-end SLA of 500ms for 95th percentile under peak load. Design a performance-testing plan to validate tail latency, backpressure handling, and autoscaling. Include workload mix, metrics, tools, and failure scenarios?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["Meta","Stripe"]},{"id":"q-2247","question":"You're performance testing a serverless data ingestion API on AWS that sees bursts of 1,000 requests per second and where Lambda cold starts spike latency. Design a practical, beginner-friendly test to quantify cold-start impact, isolate bottlenecks in the ingestion path, and propose mitigations (provisioned concurrency, warmups, or VPC endpoints)?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Citadel","Meta"]},{"id":"q-2261","question":"You're evaluating a Kafka-based streaming pipeline (producer -> Kafka -> Flink -> sink) that must sustain 2x peak event rate with 95th percentile end-to-end latency under 300 ms. Design a performance test plan that (a) models bursty arrival with backpressure, (b) validates partition rebalance under scale-out, (c) measures consumer lag and checkpoint impact, and (d) proposes instrumentation and success criteria. How would you execute this end-to-end?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Hashicorp","OpenAI","Scale Ai"]},{"id":"q-2342","question":"You're operating a video-on-demand service with a CDN in front of a regional origin. During a spike to 5x peak traffic, origin latency spikes and startup stalls for many users. Design a beginner-friendly performance test to isolate CDN edge caching vs origin impact, specifying test inputs, metrics, and how you would validate TTL and cache-busting strategies?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Amazon","Google","Uber"]},{"id":"q-2409","question":"Design a performance test for a real-time analytics pipeline: 1–2M events/sec ingested via Kafka, processed by a Spark Structured Streaming job, writes to MongoDB, and dashboards reading from a materialized view. The test must quantify end-to-end tail latency under data skew, backpressure, and autoscaling of Spark workers and MongoDB shards, with realistic burst patterns and data privacy constraints. Outline workload models, metrics, and failure scenarios?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["IBM","MongoDB","Two Sigma"]},{"id":"q-2521","question":"Design a performance test for a Netflix-like live-transcoding pipeline that ingests 50k streams/sec bursts, with a 200ms SLA on first-5-second segment encoding, using Redis for metadata, MongoDB for persistence, and Kubernetes-backed microservices. Outline workload, metrics (p95/p99, tail latency, saturation), bottlenecks, and failure scenarios, including backpressure and circuit-breaking?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["MongoDB","Netflix","Scale Ai"]},{"id":"q-2670","question":"You're performance testing a BI dashboard SaaS that queries a shared data warehouse; during bursts, end-to-end latency increases 2-3x. Design a beginner-friendly test to isolate query latency from rendering; specify workload parameters, metrics, and how you would compare warm vs cold runs and measure tail latency?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Meta","Salesforce","Snowflake"]},{"id":"q-2808","question":"Design a performance test for a Discord-like chat system with 100k+ simultaneous users across 5k channels; each message fans out to 50–100 recipients and flows through a moderation service with ~150 ms latency. Target P99 end-to-end latency, test backpressure, and validate region-aware autoscaling of gateways, queues, and storage; specify workload mix, failure modes, and observability?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["Discord","Twitter"]},{"id":"q-2869","question":"You're validating a real-time event-logging pipeline that streams JSON events at 1M/sec into Snowflake via Kafka Connect to Snowpipe in a multi-region setup (AWS+GCP). During an 8-hour test, traffic spikes to 3x with 10x bursts every 90 minutes. How would you design a performance test to measure ingestion latency, backpressure, autoscaling, data-loss risk, and downstream query SLAs, including tooling and rollback plan?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Google","Scale Ai","Snowflake"]},{"id":"q-2890","question":"You're scaling a real-time payments pipeline (Kafka -> microservices) from 60k TPS to 250k TPS. Design a performance test that validates end-to-end latency, throughput, and fault tolerance, including workload model, test environment, distributed load generation, data strategy, backpressure handling, and autoscaling triggers. What are the acceptance criteria and rollback plan if SLOs are not met?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Goldman Sachs","PayPal","Robinhood"]},{"id":"q-3116","question":"You're performance testing a microservice that streams price updates to 5k clients via WebSocket. Under peak traffic latency tails spike from 60ms to 520ms. Design a beginner-friendly plan to identify whether the bottleneck is the API thread pool, external price sources, or the database. Specify metrics, tooling (Locust or k6), test steps, and how to isolate components with mocks and tracing?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Citadel","NVIDIA"]},{"id":"q-3227","question":"You're running a multi-region Instacart-like platform using MongoDB as the primary catalog data store and Kafka for event streaming. Design a targeted performance test to validate end-to-end latency of the order-placement path under a sudden 20x spike in concurrent carts, including Consul service discovery and Nomad-based deployments. Specify workload mix, SLAs, metrics, tooling, and how you'd simulate real user think-time and partial outages?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["Hashicorp","Instacart","MongoDB"]},{"id":"q-3296","question":"Design a beginner-friendly performance test for an AWS Lambda-backed REST API behind API Gateway serving a mobile game lobby. Traffic ranges from 2k requests per minute to sudden bursts of 25k requests over 2 minutes. Focus on cold-start impact, latency tails, and error rate. What metrics, ramp plan, and simple warming strategy would you implement, using a tool like k6 or Artillery, to distinguish cold-start latency from warmed-function latency?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Amazon","Anthropic","Meta"]},{"id":"q-3403","question":"You are performance testing a social feed API at scale (IBM/LinkedIn). The feed endpoint calls a downstream content service and uses Redis for caching. How would you design a beginner-friendly, repeatable test to quantify how the caching layer affects throughput and latency as concurrent users grow? Include load strategy, metrics, and how you isolate the cache's impact?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["IBM","LinkedIn"]},{"id":"q-3445","question":"You’re performance testing a real-time chat app with 5,000 concurrent users during peak hours. The auth service shows high CPU and the message router experiences increased latency under load. Build a practical, beginner-friendly test plan: what metrics to collect, which tools to use (e.g., k6/Locust), how to isolate bottlenecks, and how you’d validate fixes before re-running tests?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Discord","Oracle","Salesforce"]},{"id":"q-3533","question":"You're performance testing a real-time GPU-accelerated inference service deployed on Nvidia GPUs in Kubernetes. The 8B-parameter model must sustain 99th percentile latency under 30 ms at 1k requests/sec. Design a pragmatic performance testing plan to verify autoscaling, spin-up times, and GPU contention. Include workload mix, metrics, tooling, and failure scenarios?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["Lyft","NVIDIA","Oracle"]},{"id":"q-3599","question":"You're performance testing a real-time collaboration feature in a web app used by 2,000 concurrent editors. The stack includes a Node.js gateway, a WebSocket server, a REST API, Redis, and PostgreSQL. How would you design a beginner-friendly performance test to identify bottlenecks across frontend, gateway, and data stores? Include load profile, metrics, and a concrete test plan with component isolation?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Adobe","Discord","Robinhood"]},{"id":"q-3672","question":"Design a performance-testing plan to simulate bursty regional traffic mixing Lambda cold starts and containerized services, and quantify end-to-end P99 latency for order placement under 2–5x sustained load with 2–3 minute spikes? Include data strategy, cold-start isolation, autoscaling observations, and cost impact across regions?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["DoorDash","Two Sigma","Uber"]},{"id":"q-3681","question":"You're performance testing a real-time chat feature used by a consumer app like Snap, handling 20,000 concurrent long‑lived WebSocket connections behind a Redis Pub/Sub gateway. How would you design a beginner-friendly performance test plan to measure latency, throughput, and resource saturation, and specify how you would simulate clients, collect metrics, and identify bottlenecks across gateway, Redis, and backend services?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["DoorDash","Oracle","Snap"]},{"id":"q-3893","question":"Design a performance test for a real-time fraud-detection service using a GPU-accelerated model on Nvidia GPUs, processing 2M events/sec in Kubernetes. End-to-end latency tail at 99th percentile must stay under 120 ms under a 2x traffic spike. Include workload profiles (feature skew, user distribution), GPU vs CPU paths, memory pressure, autoscaling rules, and failure scenarios (node, GPU throttling, network)?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["MongoDB","NVIDIA","Plaid"]},{"id":"q-3935","question":"You’re validating a new analytics API consumed by 25 microservices in a SaaS platform. It enforces per-tenant quotas and caches results in Redis. Warm responses are typically 40–60ms; cold responses can exceed 200ms. Design a beginner-friendly performance test plan to identify quota bottlenecks and cache-related slowdowns, including load profile, metrics, and a minimal script outline?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Citadel","Databricks","Salesforce"]},{"id":"q-3970","question":"You're performance testing a real-time driver-location streaming system that must ingest 1k updates/sec per city, with bursts to 20k/sec during surge periods. Design a test plan to validate autoscaling, backpressure, and tail latency under bursty traffic. Specify load generation, metrics, tooling, canary strategy, and how you would interpret SLO violations?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Instacart","Lyft","Snap"]},{"id":"q-4051","question":"You’re performance testing a real-time analytics platform that ingests 50k events/sec with 15% external enrichment calls. Design a practical plan to ensure end-to-end latency stays under 200 ms at 2x peak, including topology, load patterns, and metrics. Explain how you would instrument tracing, simulate backpressure, and validate autoscaling, circuit breakers, and SLA adherence?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Google","Salesforce","Two Sigma"]},{"id":"q-4072","question":"You're adding an asynchronous background job system to a high-traffic web app (Django + Celery + Redis). During a marketing burst, enqueue rate jumps 5x and the worker queue grows. How would you design a beginner-friendly performance test to measure end-to-end latency from enqueue to completion, detect backpressure, and validate autoscaling of workers? Include a concrete test plan, metrics, and how you'd isolate enqueue vs processing?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Apple","Bloomberg","Meta"]},{"id":"q-4089","question":"Design a performance test for a multi-region video platform delivering ABR streams (HLS/DASH) to 100k peak concurrent viewers. Specify workload model, metrics, tooling, and how you validate edge autoscaling and CDN-origin failover under simulated packet loss and bursty traffic?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["Netflix","Snap"]},{"id":"q-4168","question":"You're validating a real-time multiplayer game backend that maintains 200k WebSocket connections across us-east-1 and eu-west-1. Clients emit 60 Hz position updates; occasional chat bursts occur. Design a performance test plan that measures end-to-end latency, throughput, and autoscaling behavior. Include workload model, ramp strategy, metrics, backpressure handling, and data-safety considerations?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Google","LinkedIn","NVIDIA"]},{"id":"q-4286","question":"You're testing a serverless telemetry path that ingests 120k events/sec, calls a 3rd-party enrichment API with ~200 ms latency, and writes to Delta Lake. Design a performance test to guarantee end-to-end latency stays under 400 ms at 2x peak, while 30% of traffic hits the external API and a sudden 3x spike occurs within 60 seconds. Include topology, load patterns, instrumentation, autoscaling triggers, backpressure strategies, and SLA validation?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Anthropic","Databricks"]},{"id":"q-4391","question":"You're performance testing a PayPal-like payment gateway that uses a gateway → auth → fraud → settlement chain, with MongoDB as the transaction store (sharded) and Kafka for events. Peak load: 2000 TPS with 4x burst. How would you design a test plan to validate end-to-end latency, idempotency, backpressure, and circuit breaker behavior under partial service degradation (fraud service slow, DB shard hot), including topology, workloads, and success criteria?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["MongoDB","PayPal"]},{"id":"q-4462","question":"You are performance testing a ride-hailing platform's dispatch service. During a city-wide event, requests surge 20x in several zones while drivers cluster unevenly. The system relies on geospatial routing and an external geocoding service with rate limits. How would you design a practical performance test to identify bottlenecks in matchmaking, routing, and external dependencies, ensuring fairness across zones and stable SLA for 95th latency?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Citadel","PayPal","Slack"]},{"id":"q-4626","question":"You're performance testing a search autocomplete API for a social app. The API returns top 5 suggestions from a 1M-term index, uses a Node.js gateway, Redis cache, and PostgreSQL. Under 200 RPS at peak, design a beginner-friendly plan to identify bottlenecks across gateway, cache, and DB. Include load profile, metrics, and a simple, component-isolated test plan?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Instacart","Meta","Slack"]},{"id":"q-472","question":"You're load testing a high-frequency trading platform that processes 100K requests/second. Your load generator becomes the bottleneck. How would you design a distributed load testing architecture to accurately simulate production traffic patterns?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Goldman Sachs","Two Sigma"]},{"id":"q-4735","question":"You're rolling out a new recommendation microservice behind a feature flag in a multi-region e-commerce platform. The service will receive 10k RPS at peak, across two regions, and traffic should ramp from 5% to 50% over 4 hours. Design a beginner-friendly performance test plan to validate latency, error rate, cache-hit ratio, and cross-region consistency while the feature flag shifts traffic. Include load profile, metrics, and a concrete test plan with component isolation?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["IBM","Instacart","Netflix"]},{"id":"q-4806","question":"Design a performance test plan for a global real-time drawing app using WebSocket connections with 50k concurrent users across 3 regions. Backend: Redis Pub/Sub, stateless gRPC services, edge gateway with TLS termination and rate limiting. Your plan should specify workload model, success criteria, metrics (connection lifecycle, event latency, backpressure indicators), tooling, and failure scenarios (regional outage, Redis shard loss, burst traffic)?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["Cloudflare","Microsoft","Salesforce"]},{"id":"q-501","question":"You're testing a grocery delivery app like Instacart that handles 10,000 concurrent users during peak hours. How would you design a performance testing strategy to identify bottlenecks in the order processing pipeline?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Instacart","Tesla"]},{"id":"q-531","question":"You're load testing a food delivery platform's order processing system. How would you design a performance testing strategy to identify bottlenecks during peak lunch hours (12-2 PM) when order volume increases 10x?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Citadel","DoorDash"]},{"id":"q-557","question":"You're load testing a trading platform that processes 10,000 orders/second. Your load generator shows 95th percentile latency at 200ms, but actual users report 2-3 second delays. What's happening and how would you diagnose it?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["NVIDIA","Robinhood"]},{"id":"q-586","question":"How would you measure and optimize the performance of a REST API endpoint that's responding slowly?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Plaid","Uber"]},{"id":"q-996","question":"Design a performance testing plan for a MongoDB-backed ride-hailing backend where a single /alloc-trip endpoint coordinates availability updates and cross-service trip allocations under production-like bursts up to 50k RPS; outline how you would identify bottlenecks across DB, services, and network, and concrete steps to reduce tail latency?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["Google","MongoDB","Uber"]},{"id":"gh-40","question":"What is Performance Testing and how does it differ from Load and Stress Testing?","channel":"performance-testing","subChannel":"load-testing","difficulty":"beginner","tags":["perf","testing"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-41","question":"What are the different types of performance testing and when would you apply each type in a real-world scenario?","channel":"performance-testing","subChannel":"load-testing","difficulty":"intermediate","tags":["perf","testing"],"companies":["Amazon","Google","Meta"]},{"id":"q-237","question":"How would you design a distributed load testing setup using k6 with multiple cloud regions to simulate 100k concurrent users while avoiding rate limiting and ensuring accurate metrics collection?","channel":"performance-testing","subChannel":"load-testing","difficulty":"intermediate","tags":["jmeter","k6","gatling","locust"],"companies":["Amazon","Google","Netflix","Stripe","Uber"]},{"id":"q-210","question":"How would you implement comprehensive CPU profiling with flame graphs using clinic.js and async hooks to identify performance bottlenecks in a Node.js microservice handling concurrent requests, including production considerations and memory leak detection?","channel":"performance-testing","subChannel":"profiling","difficulty":"intermediate","tags":["cpu-profiling","memory-profiling","flame-graphs"],"companies":["Amazon","Cloudflare","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-280","question":"What is the difference between CPU profiling and memory profiling, and when would you use a flame graph?","channel":"performance-testing","subChannel":"profiling","difficulty":"beginner","tags":["cpu-profiling","memory-profiling","flame-graphs"],"companies":["Amazon","Google","Microsoft","Netflix"]},{"id":"q-1024","question":"You're building a prompt-routing system for a consumer-support assistant serving Apple, Airbnb, and Snap customers. It must decide auto-response, request clarification, or escalate to a human agent, based on intent, risk, PII presence, and policy compliance. Describe end-to-end design, including a 3-template prompt bank (concise, friendly, authoritative), a safety/brand-voice rubric, and a minimal Python prototype that routes auto vs escalate under edge cases. Include a testing plan?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Airbnb","Apple","Snap"]},{"id":"q-1037","question":"You're building a dynamic prompt orchestration system for Scale AI and MongoDB enterprise-support chatbots. It must select from a bank of five templates (concise, empathetic, technical, authoritative, business-friendly) based on user intent, data sensitivity, and risk signals, while applying strict safety guardrails to prevent prompt injection and data leakage. Describe the architecture, routing rules, and a minimal Python prototype that demonstrates template selection and a veto guardrail for edge cases?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["MongoDB","Scale Ai"]},{"id":"q-1119","question":"You're building a beginner-friendly prompt routing module for a multilingual customer-support chatbot serving Snowflake and Airbnb users. Design a two-step prompt selection: first classify intent (order_status, account_help, security_alert), then select a single template from (concise, friendly, authoritative) that preserves safety and privacy. Provide the concrete routing rules and a tiny Python prototype that demonstrates intent classification and template selection with sample inputs?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Airbnb","Snowflake"]},{"id":"q-1137","question":"Design a multilingual prompt evaluation pipeline for a Tesla/Square customer-support bot. It must detect language, route prompts to language-specific template banks, apply safety gates for PII and injection, and track drift metrics to trigger template updates. Provide architecture and a minimal Python prototype that returns a selected template and a veto flag?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Square","Tesla"]},{"id":"q-1206","question":"You're building a prompt lifecycle service for a multi-tenant chat assistant used by Tesla support and MongoDB customers. It must manage versioned templates, canary rollouts, per-tenant experiments, and safe rollback if a new version underperforms or violates safety guards. Describe the architecture, data model, and provide a minimal Python prototype that resolves the tenant's latest approved version and supports rollback via a veto gate?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["MongoDB","Tesla"]},{"id":"q-1217","question":"You're building a budgeted prompt engine for a multilingual support bot. With a 60-token cap for prompts in English and Spanish, design a rule-based condenser that preserves intent, routes to one of three templates (concise, empathetic, clarifying), and rejects unsafe prompts. What would the architecture look like, and provide a minimal Python prototype that compresses input to fit the budget and demonstrates template routing?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Google","Lyft","Microsoft"]},{"id":"q-1317","question":"You’re building a privacy-preserving prompt pipeline for a customer-support chatbot that must operate under GDPR. Outline a design to redact PII (emails, phone numbers) from prompts before feeding them to an LLM, while preserving intent to route to three templates (concise, empathetic, escalate). Include a minimal Python prototype that demonstrates redaction and routing, and discuss edge-case testing and auditability?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Anthropic","Google"]},{"id":"q-1455","question":"You're building a beginner-friendly, client-side prompt calibrator for a real-time support chat used by Tesla, Uber, and Scale AI customers. Design a scoring rubric that evaluates prompts on clarity, safety, and bias risk. Implement a tiny TypeScript prototype that: 1) scores prompts with the rubric, 2) routes to one of three templates (concise, empathetic, authoritative), and 3) flags prompts needing human review. Include basic tests and a sample input?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Scale Ai","Tesla","Uber"]},{"id":"q-1842","question":"Scenario: a large enterprise runs a prompt orchestration layer that routes user prompts to tenant-specific policies and models. You must design a dynamic gating layer that preserves privacy across tenants, adheres to latency budgets, and enforces safety guardrails against prompt injection. Describe the architecture, the routing rules, and provide a minimal Python prototype that demonstrates the gating decision (tenant, model, template) and a pluggable sanitizer?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["Citadel","IBM","Meta"]},{"id":"q-1852","question":"You're building a real-time, multi-tenant prompt router for an all-in-one chat assistant used by Instacart, Tesla, and Netflix employees. The system must route each user prompt to one of three personas: support-centric, revenue-aware, and compliance-oriented, based on user role, prior interactions, context window, and explicit data-sensitivity cues. It should apply a safety veto for prompts that could leak policy or PII, and adjust routing to meet SLA targets. Describe the architecture, routing rules, and provide a minimal Python prototype that demonstrates persona selection and a veto gate for edge cases. How would you measure latency, correctness, and safety?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Instacart","Netflix","Tesla"]},{"id":"q-2032","question":"You're building a multilingual prompt-routing system for a live Discord-like chat platform. Design an approach to detect language, assess safety risk, and route prompts to one of four templates (concise, friendly, formal, safety-first). Include data schemas, routing rules, and a minimal Python prototype that demonstrates language detection, risk scoring, and template selection?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Databricks","Discord"]},{"id":"q-2454","question":"You are building a real-time prompt routing layer for an enterprise AI assistant used by DBAs and developers at a large database platform. The router must assign prompts to one of four modules: 1) code-generation with sandboxed execution, 2) schema design reasoning, 3) performance tuning suggestions, 4) compliance/audit notes. Given a prompt: 'Create an index on users(name, email, last_login) for read-heavy workloads; ensure GDPR minimization and explain any trade-offs', outline the routing signals (intent, data sensitivity, latency), define routing rules, and provide a minimal Python prototype that returns the chosen module and a guardrail message. Include edge-case handling (e.g., conflicting signals)?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Google","Meta","MongoDB"]},{"id":"q-2468","question":"You're building a beginner-friendly prompt evaluation harness for a multilingual customer-support bot that must handle English, Spanish, and Mandarin prompts. Design a minimal, rule-based evaluation that checks if three locales produce semantically equivalent intents for a given user query. Provide a tiny Python prototype that feeds a fixed prompt through a mocked LLM API, compares outputs for a set of intents (order_status, refund, and product_info), and flags mismatches?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Cloudflare","Snap","Tesla"]},{"id":"q-2552","question":"You're building a real-time prompt routing system for a multinational streaming platform's content moderation assistant. It must direct prompts to: 1) automatic policy-compliant reply generation, 2) human escalation, 3) safe-deflection. Enforce jurisdiction-specific guardrails (GDPR, COPPA, local content laws) and data-sensitivity tagging. Propose routing signals, rules, and a minimal Python prototype that returns the chosen module and a jurisdiction-appropriate guardrail message. Include edge-case handling (conflicting signals)?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["Adobe","Netflix","Scale Ai"]},{"id":"q-2852","question":"You're building a beginner prompt router for a Netflix-like show assistant. Given a user message like 'Show me funny sci-fi from this decade', design a lightweight prompt that classifies intent into four categories: 1) find_title, 2) genre_recs, 3) availability, 4) escalate to human agent. Provide a minimal Python prototype that outputs the chosen category and a guardrail note. Include edge cases (ambiguous prompts, empty input)?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Netflix","Plaid"]},{"id":"q-2866","question":"In a multitenant customer-support AI (DoorDash/Zoom/Citadel), design a prompt-guarded routing scheme that classifies prompts into three modes: taboo-free, context-constrained, escalate. Specify signals (policy, data-sensitivity, intent), constraints, and provide a minimal Python prototype that returns mode and a safety summary, including edge cases like ambiguous intent or mixed data categories?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["Citadel","DoorDash","Zoom"]},{"id":"q-2953","question":"Design a beginner-friendly prompt decomposer for a multilingual customer-support AI used by Hugging Face, Google, or Nvidia. Given a user query in any language, output a 1) detected language and normalized text, 2) a 2–3 step plan to fulfill the request, and 3) a chosen response template (concise, empathetic, or formal). Include a minimal Python prototype returning these fields and a safety guard to block disallowed topics. What would you implement?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Google","Hugging Face","NVIDIA"]},{"id":"q-2980","question":"Design a policy-aware prompt orchestrator for an enterprise AI assistant used by Bloomberg analysts and Google engineers. For every user query, decide which of four modules to activate: 1) data-fetch, 2) computation, 3) language translation, 4) compliance/audit logger. Outline routing signals, decision rules, and provide a minimal Python prototype that returns the chosen module and a gatekeeper verdict (pass/fail). Include edge cases like conflicting signals or missing signals?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Bloomberg","Google"]},{"id":"q-3017","question":"You're prototyping a prompt orchestration layer that must safely and deterministically decide when to invoke external tools (e.g., SQL, filesystem) based on a user’s natural-language prompt. Design a lightweight, rule-based policy that maps prompts to at most two tools and provide a minimal Python prototype that returns the chosen tools and a guard message if a disallowed combination appears. Include edge-case handling (e.g., conflicting tool requests)?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Amazon","Google"]},{"id":"q-3253","question":"Design a beginner-friendly prompt test bench for an analytics assistant used by Databricks and Adobe. Propose a prompt versioning and drift-detection scheme: given a base prompt and 3 paraphrases, outline how you'd measure output drift with a simple similarity metric and guardrails, and sketch a tiny Python prototype that reports drift flags for each paraphrase?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Adobe","Databricks"]},{"id":"q-3518","question":"Design a beginner-friendly prompt-to-action router for a cloud-data assistant. Given a user prompt like 'count active users by region in last 30 days; redact emails to GDPR standards', build a tiny router that (a) selects one of three modules: SQL-builder, PII-redactor, and Output-formatter, (b) outputs a structured 2-3 step plan, and (c) returns a concrete example payload for the chosen module. Include routing signals (intent, data_sensitivity, latency) and edge cases (conflicting signals, missing date ranges)?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Google","Oracle"]},{"id":"q-3688","question":"Design a real-time prompt routing prompt for an enterprise data assistant. Create a self-healing router that, given user intent and privacy constraints, routes to three modules: DataQuery, PrivacyGuard, ResultFormatter. Output a 2–3 step plan and a concrete example payload for the chosen module. Define routing signals (intent, data_sensitivity, latency) and edge cases (conflicting signals, missing fields)?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Amazon","NVIDIA","Stripe"]},{"id":"q-3723","question":"You're building a beginner-friendly prompt routing layer for a cloud-edge incidents assistant used by on-call engineers at a CDN provider. Given a user request: 'Summarize last week's incidents across three regions, redact customer emails, and extract uptime trends', design a router that (a) routes to one of three modules: Data-Extractor, Redactor, Summary-Builder, (b) outputs a 2-3 step plan, and (c) returns a concrete payload example for the chosen module. Include routing signals (intent, data_sensitivity, latency_budget) and edge cases (conflicting signals, missing region data)?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Cloudflare","NVIDIA","Uber"]},{"id":"q-3897","question":"Design a real-time prompt routing layer for a multinational enterprise AI assistant that must comply with GDPR and HIPAA. Given a prompt: 'Draft an ingestion pipeline from S3 to Snowflake with row-level PII redaction', route to one of three modules: 1) data-ingestion-pipeline generator (SQL/ETL), 2) privacy-enforcement transformer, 3) governance-report builder. Provide a 2–3 step plan and a concrete payload for the chosen module. Include routing signals (intent, data_classification, regulatory_domains, latency) and edge cases (conflicting signals, missing policy)?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["Oracle","Scale Ai","Twitter"]},{"id":"q-3974","question":"You're building an enterprise analytics assistant that accepts natural language prompts but must never alter its system prompt or reveal secrets. Design a lightweight prompt-safety guard before routing: identify injection patterns, enforce a static policy, sanitize inputs, and reject risky prompts. Provide a minimal Python prototype and an outline for testing with sample injections?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Google","LinkedIn","Snowflake"]},{"id":"q-3989","question":"Design a real-time prompt composition layer for a high-sensitivity data-analytics assistant. It must route to three micro-prompts: SQL-builder, Privacy-masker, Result-summarizer. Selection relies on signals: intent (query/analysis), data_sensitivity (public/internal/restricted), latency_budget (ms). Provide routing rules, edge cases (conflicts/missing budget), and a minimal Python prototype that outputs the chosen module and a sample payload?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["Apple","Bloomberg","Meta"]},{"id":"q-4120","question":"You're building a real-time prompt routing layer for a nationwide fraud-detection system used by multiple fintech teams. The router must ensure tenant data isolation and prevent secrets leakage while composing prompts for downstream modules. Design a minimal policy-driven router that 1) classifies prompts by data_sensitivity and tenancy, 2) routes to one of three modules: 1) query-builder, 2) risk-scoring, 3) explainability, and 3) outputs a structured 2-3 step plan and a concrete example payload. Include routing signals, edge cases (conflicting signals, missing context), and provide a tiny Python prototype returning the chosen module and a guardrail message. Include tests and a brief justification?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Citadel","Scale Ai","Uber"]},{"id":"q-4230","question":"Design a beginner-friendly prompt router for a data-analytics assistant used in enterprise dashboards. Given a user request such as 'analyze churn by region for the last 4 quarters', route to one of three modules: SQL-builder, Visualization-generator, or Guardrail-checker. Define routing signals (intent, data_sensitivity, user_role), edge cases (ambiguous intent, missing date range), and provide a minimal Python prototype that returns the chosen module and a concrete example payload for that module?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["LinkedIn","Netflix","Tesla"]},{"id":"q-4429","question":"Design a beginner-friendly prompt router for a multilingual enterprise data assistant. Given a prompt like Generate a sales summary by region for last quarter; ensure emails are redacted, route to one of three modules: Translation-Preprocessor, SQL-Builder, or Summary-Formatter. Define routing signals, edge cases, and provide a minimal Python prototype returning the chosen module and a concrete payload?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["LinkedIn","PayPal","Snap"]},{"id":"q-447","question":"You're building a prompt for a customer service chatbot that needs to extract order details from unstructured user messages. How would you design the prompt to handle variations like 'I need to cancel order #12345' vs 'Can't find my recent purchase 12345' while maintaining high accuracy?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["DoorDash","Google","NVIDIA"]},{"id":"q-450","question":"You're building a prompt optimization system for a large language model API. The system needs to automatically improve prompt performance while maintaining safety constraints. How would you design an architecture that balances prompt effectiveness with content safety, and what metrics would you track?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["Apple","Robinhood","Tesla"]},{"id":"q-4521","question":"You're building a multilingual analytics assistant used by traders and compliance officers. User prompts are unstructured and may request SQL generation, governance checks, or risk summaries, often containing sensitive data. Design a routing policy that (1) maps prompts to one of four modules: SQL-builder, PII-redactor, Compliance-auditor, or Risk-summarizer; (2) adds a self-critiquing step to verify plan correctness and catch adversarial prompts; (3) returns a minimal Python prototype showing route(prompt) -> (module, guardrail). Include edge cases like conflicting signals or data leakage risks?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["Meta","Snowflake","Two Sigma"]},{"id":"q-4546","question":"Design a prompt-routing blueprint for a multi-tenant data assistant used by Lyft and MongoDB that (a) maps each user prompt to a set of candidate tools (SQL-builder, docs-search, schematizer) with a formal output contract (schema, confidence, provenance), (b) enforces per-tenant privacy via versioned prompts and rollback capability, and (c) robustly handles conflicting tool outputs or signals? Provide a minimal Python prototype that returns the chosen module and a guardrail note, plus edge-case handling?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["Lyft","MongoDB"]},{"id":"q-4613","question":"Design a production-ready prompt orchestration layer for a multi-tenant knowledge-base assistant with strict latency and auditability. Outline an architecture that uses retrieval-augmented generation, per-tenant context queues, a context window manager that slices or fetches relevant docs, a modular policy layer for safety/privacy, and an execution-budgeter that caps tokens and enforces latency budgets. Provide data structures, a minimal Python prototype of the orchestrator, and testing strategies for latency, provenance, and drift?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["Apple","DoorDash","Netflix"]},{"id":"q-4690","question":"Design a multilingual enterprise prompt layer that prevents cross-tenant leakage while preserving language style. Four tenants: finance, healthcare, marketing, R&D with per-tenant policies (no secrets, PII redaction, attribution). Build a dynamic policy engine and a prompt-routing schema that handles overrides, conflicts, and latency constraints. Provide a minimal Python prototype showing policy evaluation and routing, plus tests for injections and policy overrides?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Apple","Snowflake"]},{"id":"q-473","question":"You're building a chatbot for Instacart's customer service. How would you design a prompt template that handles both order status inquiries and refund requests while maintaining consistent tone and preventing prompt injection?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Goldman Sachs","Instacart","Netflix"]},{"id":"q-4732","question":"Design a prompt orchestration layer for a real-time analytics assistant used by data scientists at Nvidia, Snowflake, or Hugging Face. It must route prompts to three modules: 1) Retrieval-Augmented Generator (vector-store + LLM), 2) Safety Bias Guard, 3) Verifier against ground-truth metrics. Define routing signals (intent, data sensitivity, vector freshness, latency), edge cases (drift, conflicting signals), and provide a minimal payload example for the chosen module plus a guardrail message?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["Hugging Face","NVIDIA","Snowflake"]},{"id":"q-4784","question":"You're designing a prompt orchestration layer for an enterprise coding assistant used by engineers at Google/Oracle. The system routes user intents to four modules: CodeGen, DocGen, SecurityAudit, and ContextRefiner. Given the prompt: 'Create a Python function using boto3 to list S3 buckets, filter by tag env=prod, and print names', outline guardrails to prevent leakage of system prompts or secrets, enforce least-privilege routing, coordinate module outputs deterministically, and handle edge cases like conflicting signals. Provide a minimal Python prototype that returns the chosen module and a safety summary?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Google","Oracle"]},{"id":"q-4787","question":"You're building a real-time prompt orchestration layer for a multi-tenant AI workspace that must safely chain to 3 modules: DataExtractor, ModelTrainer, and Evaluator. Given a user prompt like 'train a classifier on next-week sales data with privacy constraints', design: (a) a routing signal set (intent, data_sensitivity, latency, reproducibility), (b) routing rules and a deterministic tie-breaker, (c) a minimal Python prototype that returns the target module and a guard message, and (d) edge-case handling (conflicting signals, partial data availability)?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["Cloudflare","Databricks","Hugging Face"]},{"id":"q-502","question":"How would you design a prompt engineering system to handle multi-turn conversations with context windows, ensuring consistent persona adherence while managing token limits and preventing prompt injection attacks?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["IBM","Meta","Zoom"]},{"id":"q-532","question":"You're building a prompt engineering system for a cloud infrastructure tool. How would you design prompts to handle ambiguous user input like 'setup database' while maintaining context and preventing hallucination?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Google","Hashicorp","Stripe"]},{"id":"q-558","question":"You're building a prompt optimization system for a large language model serving 10M+ daily requests. How would you design a system to automatically detect and mitigate prompt injection attacks while maintaining 99.9% uptime?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["Coinbase","NVIDIA","PayPal"]},{"id":"q-587","question":"How would you design a prompt to extract structured data from unstructured text while handling edge cases and ensuring consistent output format?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Meta","Snowflake","Zoom"]},{"id":"q-892","question":"You're building a beginner-friendly prompt evaluation harness for a customer-support chatbot. Given user prompts about orders or refunds, design a lightweight, rule-based template selector that picks among three templates (concise, friendly, authoritative). How would you implement and test this with a tiny Python prototype that scores templates on safety, tone, and length?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Coinbase","Google","Lyft"]},{"id":"q-251","question":"How would you implement a DSPy optimizer to automatically improve few-shot prompts for a classification task using BootstrapFewShot with evaluation metrics?","channel":"prompt-engineering","subChannel":"optimization","difficulty":"intermediate","tags":["prompt-tuning","dspy","automatic-prompting"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"q-198","question":"How would you design a multi-layered guardrail system to prevent prompt injection and jailbreak attacks while maintaining legitimate user functionality, and what are the key trade-offs between security and user experience?","channel":"prompt-engineering","subChannel":"safety","difficulty":"beginner","tags":["jailbreak","guardrails","content-filtering"],"companies":["Amazon","Google","Meta"]},{"id":"q-226","question":"How would you design a prompt-engineering system that dynamically selects between chain-of-thought, few-shot, and zero-shot prompting based on real-time performance metrics and task complexity?","channel":"prompt-engineering","subChannel":"techniques","difficulty":"advanced","tags":["chain-of-thought","few-shot","zero-shot"],"companies":["Amazon","Google","Meta","Microsoft","OpenAI"]},{"id":"q-196","question":"How would you implement a rate-limited async HTTP client using aiohttp and asyncio.Semaphore to handle 1000 requests while respecting API limits?","channel":"python","subChannel":"async","difficulty":"intermediate","tags":["asyncio","aiohttp","concurrency"],"companies":["Amazon","Google","Meta"]},{"id":"q-224","question":"How would you implement a thread-safe singleton in Python with lazy initialization, proper type hints, and discuss the trade-offs between metaclass, decorator, and module-level approaches for a production system?","channel":"python","subChannel":"best-practices","difficulty":"advanced","tags":["pep8","typing","testing"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-178","question":"In Python, when do `is` and `==` return different results, and why does this happen with object identity vs equality?","channel":"python","subChannel":"fundamentals","difficulty":"intermediate","tags":["python","basics"],"companies":["Amazon","Google","Meta","Microsoft","Uber"]},{"id":"q-1085","question":"You’re building a real-time log processor in Python for a messaging app. You receive a stream of JSON lines like {\"user_id\": 123, \"event\": \"message_sent\", \"ts\": 1610000000}. Emit only the first event per (user_id, event) within a 60-second sliding window. Design a memory-bounded, generator-based pipeline that reads from an iterable of strings and yields deduplicated dicts in order. Include edge cases and memory considerations?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Hugging Face","Slack"]},{"id":"q-1149","question":"In a streaming data etl, you're given a JSONL stream where each line is a JSON object with 'host' and 'value'. Implement a memory-efficient Python generator top_n_hosts(n, lines) that yields the top N hosts by total value after consuming the stream, without keeping the entire per-host totals in memory. Use a min-heap to maintain current top-N and skip invalid lines?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Google","Meta"]},{"id":"q-1250","question":"Design an asynchronous NDJSON ingestion pipeline in Python that reads an async iterable of lines, validates each with a Pydantic model, and routes records to per-tenant partitions based on the 'tenant_id'. Implement bounded queues per partition with a total memory cap, guarantee per-tenant in-order processing, and a slower downstream sink that creates backpressure. Provide a test plan with skewed partitions and slow sinks?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Snowflake","Uber"]},{"id":"q-1275","question":"Design an advanced async Python pipeline: NDJSON lines arrive via a TCP socket, each is validated with a Pydantic model, then a deduplication stage uses a sliding-window Bloom filter to emit each event at most once in a 24-hour window. The pipeline must stay memory-bounded, support backpressure to the producer, and allow the downstream sink to pace itself. Provide a concrete test plan with clock skew and bursty traffic?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["DoorDash","Hugging Face","Two Sigma"]},{"id":"q-1285","question":"Given a list of dictionaries representing users, implement normalize_users(users) that returns a list of User dataclass instances with fields id:int, name:str, signup_ts:datetime, is_active:bool. Coerce id from str to int, parse signup_ts ISO8601 strings, interpret is_active from common truthy values, and fill defaults for missing fields. If any record is invalid, raise ValueError with the indices of invalid records?","channel":"python","subChannel":"general","difficulty":"beginner","tags":["python"],"companies":["Apple","MongoDB","Uber"]},{"id":"q-1476","question":"Design an asynchronous Python engine to join two JSON event streams by id within a 5-second window. Streams A and B are async iterables yielding {'id': str, 'ts': int, 'payload': Any}. Emit matched pairs to a sink when both sides have an event with same id within the 5s window. Route late events past the lateness bound to a 'late' sink. Enforce a global memory bound for buffered events and propose a test plan with out-of-order arrivals?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Airbnb","Lyft","Stripe"]},{"id":"q-1639","question":"Write a Python function process_events(file_path) that reads a newline-delimited JSON (JSONL) log file of Stripe-like events. Each line is a JSON object with 'type' (str) and 'data' (dict with 'id' key). The function should return a dict mapping event 'type' to count, skipping lines with missing keys or invalid JSON, and writing errors to a separate errors.log. Make it memory-efficient using a streaming approach?","channel":"python","subChannel":"general","difficulty":"beginner","tags":["python"],"companies":["Hugging Face","Stripe"]},{"id":"q-1675","question":"Design a memory-bounded streaming processor in Python for a JSONL event stream with fields: timestamp, service, event_type, payload. Group by (service, event_type), maintain an in-order per-key queue with bounded capacity, and emit a 60-second rolling histogram of payload sizes per group to a downstream sink. Ensure backpressure via per-key queues and a global memory cap, and provide a test plan with skewed keys and slow sinks?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Airbnb","Amazon","Scale Ai"]},{"id":"q-1783","question":"Implement a memory-efficient Python function top_n_words(filepath, n) that streams a text file line by line to find the n most frequent words. Normalize case, strip punctuation, ignore empty tokens, and return a list of the top n words sorted by frequency. Ensure it never loads the whole file into memory?","channel":"python","subChannel":"general","difficulty":"beginner","tags":["python"],"companies":["Amazon","Google","PayPal"]},{"id":"q-1867","question":"Design a Python async NDJSON ingestion pipeline that reads lines from an async source, validates each line against a versioned Pydantic model, supports hot-reloadable schema versions from a shared config, and guarantees exactly-once delivery with an idempotent sink and per-record deduplication, all while enforcing bounded memory and backpressure. How would you implement it?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Amazon","Netflix"]},{"id":"q-1884","question":"Implement an asynchronous Python data transformer for a JSONL event stream where each line includes a 'version' field. Build transform_stream(input: AsyncIterable[str], schemas: Dict[int, Type[BaseModel]]) that validates each line against its versioned Pydantic model, applies a version-aware field mapping, and outputs transformed JSONL lines to a downstream sink while guaranteeing per-version in-order processing, memory-bounded streaming, and backpressure. Include a small test scaffold showing a v1→v2 migration?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Google","NVIDIA","Tesla"]},{"id":"q-2090","question":"Design a memory-bounded streaming top-K aggregator in Python. Data arrives as an async iterable of numeric events with timestamps. Implement a class that maintains an approximate top-10 using a Count-Min Sketch plus a min-heap, supports a sliding time window, and exposes add(value, ts) and get_top_k() reflecting the current window. Describe API, memory guarantees, and a test plan for bursty traffic?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Anthropic","Databricks","Google"]},{"id":"q-2402","question":"You're building an asyncio Python client for a rate-limited REST API. How would you implement a function fetch_with_backoff(url, method='GET', max_retries=5, session, rate_limiter, metrics) that performs retries with exponential backoff and jitter, enforces a per-endpoint token-bucket rate limit, and records latency and outcomes into metrics? Include a minimal code sketch and a test plan?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Adobe","NVIDIA","Twitter"]},{"id":"q-2704","question":"Design an async Python NDJSON processor that reads lines from an AsyncIterable[str], each line containing a top-level version and payload. It must validate using a dynamic, versioned Pydantic model registry, support on-the-fly hot-swapping of versions without restart, ensure per-version in-order processing, memory-bounded streaming, and backpressure to a downstream sink. Include a test scaffold showing v1→v2 migration and a runtime version swap?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Airbnb","Anthropic","Uber"]},{"id":"q-2835","question":"Design a Python async streaming solution: implement an async function process_metrics(feed: AsyncIterator[bytes]) -> AsyncIterator[Tuple[str, int, float]] that reads a live UDP multicast binary stream of metric events. Each message contains a 4-byte big-endian epoch timestamp (seconds), a 2-byte metric_id, and an 8-byte float value. Validate metric_id against a map, maintain a 60-second rolling window per metric_id, and emit (metric_id, window_start, sum) whenever the rolling sum changes by at least 5% or every 10 seconds, whichever comes first. It must be memory-bounded and backpressure friendly with a bounded internal queue. Include a concise test plan using bursty traffic and clock skew?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Cloudflare","Discord"]},{"id":"q-2930","question":"How would you implement a memory-efficient Python function process_ndjson_by_type(input_path, out_dir) that streams an NDJSON file, parses each line as JSON, and writes every line to a per-type file named '{type}.ndjson' in out_dir? The function should create files on demand, reuse file handles to keep memory constant, track per-type line counts, and skip malformed lines while continuing?","channel":"python","subChannel":"general","difficulty":"beginner","tags":["python"],"companies":["Google","Meta"]},{"id":"q-2955","question":"Design an async NDJSON processor in Python: read lines from an AsyncIterable[str], each line is JSON with a 'service' key. Implement partitioned_aggregate(stream, key_func, sink, max_keys=4096) that maintains per-key in-order processing, bounded buffering, and backpressure to the sink. It must allow hot-swapping key_func at runtime without restart and include a small test scaffold showing two services migrating?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Adobe","Apple","Google"]},{"id":"q-3183","question":"Implement a memory-friendly CSV reader in Python: create a function stream_csv_with_required(file_path: str, required_fields: List[str]) that streams rows using csv.DictReader and yields only rows where all required_fields are present and non-empty. It should count and expose the number of skipped rows, without loading the entire file into memory. Provide a brief usage example?","channel":"python","subChannel":"general","difficulty":"beginner","tags":["python"],"companies":["Databricks","Oracle","Stripe"]},{"id":"q-3267","question":"Design and implement a memory-efficient Python function aggregate_csvs(file_paths, group_by, sum_cols, output_path, max_mem_mb=100). It should streaming-read CSVs, group by the given columns, and sum the specified numeric columns per group. To stay within the memory bound, spill partial aggregates to disk after processing a chunk, then merge spills into the final output. Provide a minimal working skeleton and a test plan?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Goldman Sachs","Google"]},{"id":"q-3339","question":"Design and implement an asynchronous Python function process_jsonl_stream(source: AsyncIterable[str], model: Type[BaseModel], sink: Callable[[dict], Awaitable[None]], max_memory_mb: int) that reads JSONL lines, validates each with the provided Pydantic model, deduplicates by the line's 'id' field using a Bloom filter, and streams only unique events to sink while enforcing a hard memory cap and applying backpressure when the sink laggs?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Apple","Cloudflare","MongoDB"]},{"id":"q-3397","question":"Design a beginner-friendly Python function analyze_log(file_path) that streams a log file line by line and returns a dict counting occurrences of INFO, WARN, and ERROR. Ignore malformed lines; use a generator to read lines and a defaultdict for counts. Ensure it scales to multi-GB files without loading all content?","channel":"python","subChannel":"general","difficulty":"beginner","tags":["python"],"companies":["DoorDash","Snowflake","Stripe"]},{"id":"q-3420","question":"Design a Python async event router that ingests NDJSON lines from an AsyncIterable[str], routes each line to a per-type Pydantic model registry based on a 'type' field, supports hot-swapping of routing rules at runtime without restart, guarantees per-type in-order processing, and uses bounded asyncio.Queues to provide backpressure to downstream sinks. Include a minimal test scaffold showing a type A mapping to V1 and a runtime swap to V2?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Google","IBM","Stripe"]},{"id":"q-3449","question":"You have a log file that grows indefinitely containing one numeric value per line (e.g., latency in ms) along with occasional non-numeric lines. Implement a Python function running_stats(file_path) that streams lines, ignores invalid ones, and after each valid line yields a tuple (count, mean, min, max) computed so far. Keep memory usage O(1)?","channel":"python","subChannel":"general","difficulty":"beginner","tags":["python"],"companies":["Goldman Sachs","Plaid","Two Sigma"]},{"id":"q-3513","question":"In Python, implement a real-time metrics ingester that reads an async NDJSON stream where each line has host, metric, value, and ts. Write windowed_stats(stream, window_secs) to maintain a sliding window per (host, metric) using a deque of (ts, value) plus monotonic min/max queues; emit updated stats only when they move by more than 1%. Handle slight out-of-order arrivals with a small per-key buffer. Explain design choices and edge cases?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["LinkedIn","Microsoft","NVIDIA"]},{"id":"q-3532","question":"Implement a Python function parse_semver_line(line) that parses a line of the form 'name: vMAJOR.MINOR.PATCH-PRERELEASE+BUILD' where v and prerelease/build are optional. Return a dict with keys: name, major, minor, patch, prerelease (or None), build (or None). Ignore extra whitespace. If invalid, raise ValueError. The line may have additional spaces; handle edge cases like 'pkg:1.2.3' or 'pkg: v1.2.3-alpha+001'?","channel":"python","subChannel":"general","difficulty":"beginner","tags":["python"],"companies":["Apple","Databricks"]},{"id":"q-3698","question":"Design a Python async NDJSON ingest pipeline that reads lines from an AsyncIterable[str], validates each line against a per-schema Pydantic model chosen by a schema_id field, and dispatches to per-schema handlers via bounded asyncio.Queues. Implement runtime hot-swapping of schemas (without restart) and guarantee in-order processing per schema even as schemas are swapped. Include a minimal test scaffold showing schema_id=1 mapped to V1 and swapped to V2 at runtime; discuss memory, latency, and safety trade-offs?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Bloomberg","NVIDIA","Plaid"]},{"id":"q-3778","question":"Design a Python async streaming pipeline for a live NDJSON feed of transactions over a socket. Each line is a JSON object with 'user_id', 'amount', and 'ts'. Implement windowed aggregation: per-user total amount in 5-minute tumbling windows, emit results when a window closes, and handle out-of-order events up to 2 seconds. Use asyncio, memory-bounded structures, and a backpressure-friendly sink. Provide a minimal test plan?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Google","OpenAI"]},{"id":"q-4343","question":"You're processing a CSV log of signups with fields: user_id, email, signup_ts. The file is potentially several gigabytes. Implement a function yield_valid_signups(path) that streams lines using the csv module, validates email with a simple regex, and verifies signup_ts is a valid ISO 8601 timestamp. Yield dictionaries for valid lines only; ignore malformed lines and extra columns. Ensure minimal memory usage?","channel":"python","subChannel":"general","difficulty":"beginner","tags":["python"],"companies":["NVIDIA","Slack","Tesla"]},{"id":"q-4466","question":"Build an asyncio-based NDJSON pipeline that ingests lines with a 'module' key, fetches runtime transformation code from a registry, and applies transform(payload) in a sandboxed environment. Requirements: preserve input order, cap per-item CPU/memory, support hot-reload of module code without restart, and apply backpressure to downstream via bounded queues. Provide a test scaffold?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Amazon","Meta","MongoDB"]},{"id":"q-4486","question":"Implement a memory-efficient Python generator function user_action_stream(file_path, batch_size=1000) that streams a nightly user activity log line by line. Each line is 'user_id action' (e.g., 'u123 login'). Ignore malformed lines. Maintain per-user action counts in memory and yield a snapshot dict every batch_size lines with shape {user_id: {action: count}}. Explain your approach and edge cases?","channel":"python","subChannel":"general","difficulty":"beginner","tags":["python"],"companies":["Bloomberg","Cloudflare","Square"]},{"id":"q-4658","question":"Design an asynchronous NDJSON pipeline that consumes lines from an AsyncIterable[str]. Each line contains: partition (str), opcode (int), payload (dict), version (int). Build a dynamic registry mapping (opcode, version) to a handler that transforms payload into an output dict. The registry must support hot-swapping a handler for a given (opcode, version) at runtime without stopping the pipeline, using an atomic switch. Ensure per-partition in-order processing with bounded queues and backpressure to a downstream sink. Include a test scaffold swapping (opcode=5, version=1) to version=2 mid-stream and verify no data loss or reordering?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Adobe","Tesla"]},{"id":"q-474","question":"Write a Python function that takes a list of integers and returns the sum of all even numbers. How would you handle edge cases?","channel":"python","subChannel":"general","difficulty":"beginner","tags":["python"],"companies":["Bloomberg","Microsoft","Robinhood"]},{"id":"q-4808","question":"You're building a real-time log processor in Python. NDJSON lines arrive via a socket. Each line has timestamp (ISO 8601), service, level (DEBUG/INFO/WARN/ERROR), and message. Implement an async function stream_counts(source, window_seconds) that consumes an async iterator of NDJSON lines, validates with a Pydantic model, and keeps a sliding window of the last window_seconds by timestamp. It should emit every second a dict mapping (service, level) to counts in the window; memory bounded by active groups. Drop lines older than the window; include newer lines even if late arrival?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Google","Robinhood","Slack"]},{"id":"q-4845","question":"Design a streaming binary frame decoder in Python that reads from an asyncio stream with length-prefixed frames: each frame starts with a 4-byte big-endian length, then 1-byte type, 2-byte version, then payload; implement a registry of per-type parsers, support runtime hot-swapping of a parser, ensure frames are decoded in-order, and backpressure is applied via a bounded asyncio.Queue; include a minimal example of type 0x01 v1 and a runtime switch to a new v2 parser?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Discord","MongoDB","Netflix"]},{"id":"q-503","question":"How would you implement a distributed rate limiter using Redis with sliding window algorithm to handle 10,000 requests per second across multiple API servers?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Google","LinkedIn","Zoom"]},{"id":"q-559","question":"You're building a data processing pipeline that needs to handle large CSV files efficiently. How would you implement a memory-efficient solution using Python generators to process files that don't fit in RAM?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Apple","Snap","Snowflake"]},{"id":"q-588","question":"How would you implement a rate limiter using Python's asyncio to prevent API abuse while maintaining high throughput?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Apple","DoorDash","MongoDB"]},{"id":"q-852","question":"Design a memory-efficient streaming dedup pipeline in Python that reads a multi-GB log file line-by-line and prints only the first occurrence of each unique line, skipping duplicates, while persisting dedup state across restarts. Use a probabilistic data structure with a configurable false-positive rate and provide a minimal runnable snippet?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["IBM","MongoDB","Robinhood"]},{"id":"q-975","question":"Design an asynchronous NDJSON pipeline in Python that ingests lines of JSON from an async source, validates each line against a Pydantic model, applies a lightweight transform, and yields results to a downstream consumer. Ensure memory stays bounded when the consumer slows by using a bounded asyncio.Queue?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Amazon","Coinbase","Stripe"]},{"id":"q-1048","question":"You're building a React Native field-ops app with intermittent connectivity that must display offline-first tasks with images and support incremental sync across devices. Describe architecture and provide a small implementation sketch using a local DB (WatermelonDB or Realm), a sync service, and conflict resolution. What edge cases and tests would you include?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["Google","Hugging Face","Tesla"]},{"id":"q-1065","question":"You're building a React Native app for live collaboration that streams up to 4 simultaneous camera feeds using WebRTC. Describe end-to-end architecture for capture, encoding, and transport, how you'd implement backpressure and frame pacing to sustain ~30fps per feed, and provide a small implementation sketch (camera hook + simple backpressure queue) in code?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["Discord","Google","NVIDIA"]},{"id":"q-1098","question":"You're building a React Native app for field technicians that must collect GPS and sensor data in the background every 15 minutes, even when the app is suspended. Describe a cross‑platform architecture using Android WorkManager and iOS BGTaskScheduler, a minimal RN bridge, and a small code sketch of a BackgroundTaskManager that schedules tasks, persists deadlines, and handles results. Include edge cases like battery saver, app termination, and user-initiated cancel?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Google","Microsoft"]},{"id":"q-1265","question":"You're building a real-time collaborative whiteboard in React Native that must support up to 1,000 participants with low latency and offline fallback. Describe an end-to-end architecture using WebRTC data channels for deltas, WebSocket signaling, and a CRDT for merging concurrent strokes. Include data model (stroke encoding, timestamps), backpressure handling, and a small code sketch implementing a delta encoder and an in-app delta queue that feeds an RN Canvas/Skia surface, with clear acceptance criteria?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Cloudflare","Two Sigma"]},{"id":"q-1537","question":"You're building a React Native app for field engineers in regulated environments. It collects GPS and accelerometer telemetry, signs each record with a device key, and batches uploads to a backend with offline-first guarantees and tamper-evident auditing. Describe the architecture, data flow, and provide a small code sketch: a LocalAuditLog module backed by SQLite and TweetNaCl for Ed25519 signing, plus a transport adapter with exponential backoff and key-rotation handling. Include edge cases around tenant isolation, data retention, and replay protection?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["Cloudflare","Goldman Sachs","Zoom"]},{"id":"q-1790","question":"You're building a cross-platform React Native app for field technicians that must display a map with thousands of POIs, support offline caching of tiles and markers, and perform incremental sync of POI updates when online. Design a data layer and UI flow to support offline-first maps, marker clustering, and conflict resolution. Provide a small code sketch for a WatermelonDB/Realm model and a MarkerLayer component, plus test ideas?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["DoorDash","Meta","Oracle"]},{"id":"q-2072","question":"You’re building a beginner-friendly React Native gallery that shows a 2x2 grid of images loaded from a remote JSON endpoint. Describe how you would implement offline-first caching with AsyncStorage, automatic data refresh on reconnect, and a small hook sketch to fetch and cache the image list (including error handling and a simple retry strategy)?","channel":"react-native","subChannel":"general","difficulty":"beginner","tags":["react-native"],"companies":["LinkedIn","Oracle"]},{"id":"q-2292","question":"In a React Native app for real-time dashboards, you must render up to 16 live charts and 8 image tiles refreshed via WebSocket without UI jank. How would you implement a cross-platform image tile pipeline using TurboModules (JSI) that decodes, caches, and delivers tiles on a background thread, while using a virtualization list to keep 60fps? Include a concise native interface sketch and a JS wrapper?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Bloomberg","Databricks"]},{"id":"q-2436","question":"In a DoorDash-style React Native app with intermittent connectivity, how would you implement offline-first order placement using an AsyncStorage-backed Outbox queue, a background worker that processes queued actions when online, and idempotent retry semantics? Provide a minimal enqueueOrder and processQueue sketch?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["DoorDash","Microsoft"]},{"id":"q-2459","question":"You're building a cross-platform React Native app that requires a native module 'TileCache' to prefetch and cache map tiles for offline use. Outline the architecture, how the JavaScript bridge would interact with the iOS and Android implementations, memory management and eviction strategy, and a minimal code sketch showing the JS API and a native bridge interface to fetch a tile by z/x/y within a viewport. Include testing considerations?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["Airbnb","IBM","Snap"]},{"id":"q-2501","question":"You're building a beginner-friendly React Native field-data app for solar installations that allows technicians to submit a checklist and attach photos while offline. Describe an offline-first submission queue design: data model, how to store locally, how to detect connectivity, batching strategy, and a small hook sketch that enqueues items with a retry/backoff policy using AsyncStorage. Include a minimal example snippet for the hook?","channel":"react-native","subChannel":"general","difficulty":"beginner","tags":["react-native"],"companies":["IBM","Tesla"]},{"id":"q-2615","question":"You're building a field-service React Native app that must perform a background data refresh every 15 minutes to fetch location-tagged asset metadata, even when the app is terminated on both Android and iOS. Design a cross-platform background task broker using Android WorkManager and iOS BGTaskScheduler. Specify data models, scheduling constraints (network/battery), how to handle token refresh, and provide a minimal React hook sketch (useBackgroundSync) to start/stop the background work and expose status?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Meta","Microsoft","Two Sigma"]},{"id":"q-2631","question":"You're building a React Native field map app that shows a live route panel with up to 500 waypoints delivered over a WebSocket. Design a component architecture that minimizes re-renders, handles backpressure when bursts occur, and keeps the map smooth at 60fps. Provide a concise implementation sketch for a useLiveWaypoints hook (buffering and a render queue) and a minimal native bridge sketch if needed. What tests would you add to verify performance and correctness?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["Google","Instacart","Meta"]},{"id":"q-2841","question":"You're building a React Native feed that displays images from a CDN with three size variants (thumb, medium, large). Implement an adaptive image loader that selects the best variant based current network speed, device pixel density, and battery saver mode, with graceful fallbacks on flaky networks. Describe data models, caching (SQLite/Realm) strategy, and provide a minimal hook useAdaptiveImage that returns {uri, loading, error} and triggers prefetches. Provide a small code sketch?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["MongoDB","NVIDIA"]},{"id":"q-2883","question":"You're building an offline-first React Native field-inspection app where technicians submit checklists with photos while offline. Describe end-to-end sync: data model (Checklists, Items, Attachments, Revisions), local storage choice (Realm vs SQLite), sync engine (queueing, retry/backoff, push vs pull), and conflict resolution strategy (vector clocks vs server revisions or CRDT). Include a small hook sketch that enqueues updates with backoff using a background task?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Meta","MongoDB","Uber"]},{"id":"q-2964","question":"You're building a React Native app for field technicians that must upload short audio notes recorded in-app while the app is in the background on both Android and iOS. The design should ensure battery and network constraints are respected, handle token refresh, and tolerate app termination. Design a cross-platform background upload pipeline: Android (WorkManager/ForegroundService) and iOS (BGProcessingTask), define data models (AudioNote, UploadState), scheduling constraints, retry strategy, and provide a minimal React hook sketch (useAudioUpload) to enqueue notes, start/stop background tasks, and expose progress?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["Citadel","Google","Tesla"]},{"id":"q-3096","question":"You have a React Native dashboard app that renders a FlatList of 1,000+ items with images; data arrives in bursts and causes frame drops on both iOS and Android. Design a performance strategy to keep scroll smooth: virtualization tweaks, memoization, and batched updates. Implement a minimal hook useLiveList that subscribes to a native update channel, batches updates, and flushes periodically. Provide a concise sketch of usage in a component?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["Google","Salesforce","Two Sigma"]},{"id":"q-3113","question":"You're building a React Native product catalog for a large retailer. The catalog has 5,000 SKUs with high-res thumbnails and stock status updates via a streaming WebSocket. Implement a concrete plan: a useLiveCatalog hook to batch updates, and a 3-column FlatList with progressive images to keep scroll smooth. Include a minimal usage sketch and trade-offs?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["Adobe","Instacart","Two Sigma"]},{"id":"q-3520","question":"Design a cross-platform ML inference bridge for React Native that loads models from the bundle or remote storage, runs inference on a dedicated worker thread (Android: TensorFlow Lite, iOS: Core ML), enforces a memory budget, and exposes a hook useMLInference(modelName, inputs) that queues inputs with backpressure and returns predictions asynchronously. Include architecture, data flow, error handling, and a minimal hook + native module outline?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Apple","Hugging Face"]},{"id":"q-3579","question":"You're building a React Native app used offline by technicians that stores sensitive data locally (checklists, photos). Propose a secure offline storage design that preserves data-at-rest encryption with per-user keys, biometric unlock, and key rotation. Include data model considerations, library choices (Realm vs SQLite), a key-wrapping strategy, and a minimal hook sketch for encrypting on save and decrypting on read?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Microsoft","Tesla"]},{"id":"q-3620","question":"You're building a field-work React Native app that must remind technicians of their daily checklist. Design a cross-platform local notification flow: request permission, schedule a daily reminder at a configurable time, handle foreground/background taps, and provide a small hook useDailyReminder(hour, minute, message) to schedule and cancel reminders. Include edge cases like time zone changes and app restarts?","channel":"react-native","subChannel":"general","difficulty":"beginner","tags":["react-native"],"companies":["Databricks","Meta","Tesla"]},{"id":"q-3746","question":"You're shipping a React Native app that streams live location updates for 1,000 devices on a map. How would you design a scalable data pipeline and UI to render pins with 60fps panning, prioritizing tiles in the visible region, using a native spatial index (R-tree) and a JS dataflow with observable streams? Include batching, backpressure, and a hook sketch to enqueue updates?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Lyft","Meta","Snowflake"]},{"id":"q-3820","question":"You're building a React Native shopping app for an enterprise with 1,000+ product images per category and flaky connectivity. Design a cross-platform image prefetching system that preloads images for the currently visible category plus the next two categories, runs on app foreground and background (Android WorkManager + iOS BGTaskScheduler), uses an LRU disk cache, handles cache invalidation when images update, and provides an RN hook usePrefetchImages(categories, maxConcurrent). Provide a minimal usage sketch with FlatList integration?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["Instacart","Tesla","Zoom"]},{"id":"q-3860","question":"Design a real-time, cross-asset heatmap dashboard in React Native that renders 1,000+ cells updating at 60fps. Data arrives via WebSocket; explain data modeling, a high-throughput renderer (Canvas or GL), and how you would offload updates to a native module/worker (JSI/TurboModule) to avoid UI thread jank. Include a hook sketch for backpressure batching?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Goldman Sachs","Google","Snowflake"]},{"id":"q-3957","question":"You're building a React Native feed with FlatList that shows posts with images; on mid-range devices the scrolling stutters and images pop in slowly, what practical, beginner-friendly plan would you follow to diagnose and fix it, including FlatList tuning, memoization, and image loading strategies, and a tiny hook to measure frame times plus a minimal optimized item component?","channel":"react-native","subChannel":"general","difficulty":"beginner","tags":["react-native"],"companies":["LinkedIn","Zoom"]},{"id":"q-3984","question":"Design an advanced real-time data feed in a cross-platform React Native app (Meta/Microsoft level). The UI must render live stock prices for 200 symbols with updates every 50–100 ms. Explain end-to-end: native bridge architecture (TurboModule/Fabric vs legacy bridge), threading model for decoding, memory management, and how you would push updates to JS with minimal GC pressure. Include a concise outline of a hook like useLivePrices(symbols) and a small testing plan?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Meta","Microsoft"]},{"id":"q-4172","question":"You're building a beginner React Native app that lists articles from an API. On device rotation and relaunch, you must preserve scroll position and minimize network usage. Design a minimal offline-first cache (data + timestamp) using AsyncStorage, plus a small hook to read/write and an optimized ListItem. Explain eviction and error handling?","channel":"react-native","subChannel":"general","difficulty":"beginner","tags":["react-native"],"companies":["LinkedIn","NVIDIA","Plaid"]},{"id":"q-4236","question":"You're building a delivery-driver app at Uber-scale or Scale AI where push notifications must reliably navigate to the correct screen even when the app is backgrounded or killed. Describe a notification router architecture that maps payloads to routes, persists last navigated state, prevents duplicate navigations, and handles data-payload vs notification-payload differences. Include a small hook sketch that debounces navigation and a minimal navigation service using react-navigation?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Scale Ai","Uber"]},{"id":"q-4266","question":"You’re building a beginner-friendly React Native app that shows field markers on a map. Implement an offline-capable data layer: fetch markers from a REST endpoint, cache them locally, and render only markers in the current viewport. Include a small hook useViewportMarkers(region) that throttles network calls (1 request/sec) and merges remote updates into the local cache with a minimal UI example?","channel":"react-native","subChannel":"general","difficulty":"beginner","tags":["react-native"],"companies":["Apple","IBM","Oracle"]},{"id":"q-4402","question":"You're building a beginner-friendly React Native screen that lists latest articles from a paginated API. The screen auto-refreshes every 30 seconds and supports pull-to-refresh. Ensure that in-flight fetches are canceled when a new fetch starts or the screen unmounts, using AbortController. Provide a small hook useCancelableFetch(url) and show minimal UI wiring?","channel":"react-native","subChannel":"general","difficulty":"beginner","tags":["react-native"],"companies":["Coinbase","Netflix","Tesla"]},{"id":"q-4503","question":"You’re building a beginner-friendly React Native app that shows city events fetched from an API. Implement a local cache with AsyncStorage and a hook useCacheList(endpoint) that loads cached data first, then fetches fresh data, and merges by id while preserving user order. Also handle cache invalidation and duplicates. Provide a minimal hook sketch and a small component example?","channel":"react-native","subChannel":"general","difficulty":"beginner","tags":["react-native"],"companies":["Bloomberg","Meta","Salesforce"]},{"id":"q-4584","question":"You're building an Incident Feed in React Native (mobile) with paginated posts (images/videos) that must work offline. Design a data layer and a hook useInfiniteViewportFeed(region, opts) that loads the current viewport pages, prefetches ahead, caches results locally (LRU image cache + JSON store), throttles network calls, merges remote updates deterministically, and preserves scroll position when navigating away and back. Include edge cases and a minimal code sketch?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["LinkedIn","Netflix"]},{"id":"q-4630","question":"You're building a React Native field-service app that supports offline work and real-time collaboration on shared checklists. Propose a CRDT-based local-first data layer (data model, conflict resolution, and offline/online sync) implemented via a Rust-native module bridged to React Native for efficient merges. Describe how you'd handle garbage collection, security, and latency, and provide a small hook useCRDTSync() sketch and a minimal UI snippet to illustrate usage; include trade-offs?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Instacart","Microsoft","Tesla"]},{"id":"q-4742","question":"You're building a beginner-friendly React Native app that displays a grid of product cards. Implement a reusable Card component with accessible labels for iOS and Android, support a light/dark theme via the Appearance API, and provide a small hook useTheme() to expose the current theme. Show minimal usage and discuss accessibility considerations and trade-offs?","channel":"react-native","subChannel":"general","difficulty":"beginner","tags":["react-native"],"companies":["Cloudflare","OpenAI","Square"]},{"id":"q-475","question":"You're building a React Native app with complex animations that need to run at 60fps. The app has multiple animated components including a custom carousel, gesture-driven interactions, and background video processing. How would you optimize performance to maintain smooth animations?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Amazon","Google","NVIDIA"]},{"id":"q-4831","question":"You're building a beginner-friendly React Native chat app with offline support. Outline a minimal local-first layer using SQLite to store messages, with a timestamp-based conflict resolution (latest wins) when syncing with the server. Implement a useChatSync(conversationId) hook that queues outgoing messages, retries failed sends with exponential backoff, and batches receipt/status updates. Include a small UI snippet showing sending and optimistic rendering?","channel":"react-native","subChannel":"general","difficulty":"beginner","tags":["react-native"],"companies":["Microsoft","Two Sigma"]},{"id":"q-504","question":"How would you implement a custom button component in React Native that handles both iOS and Android platform-specific styling while maintaining consistent behavior?","channel":"react-native","subChannel":"general","difficulty":"beginner","tags":["react-native"],"companies":["Meta","Robinhood","Stripe"]},{"id":"q-533","question":"How would you optimize a React Native app with 50+ screens that's experiencing slow navigation and memory leaks, particularly on lower-end devices?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Citadel","PayPal"]},{"id":"q-560","question":"You're building a React Native app that needs to display a list of user profiles with images. The list should be performant with 1000+ items and support pull-to-refresh. How would you implement this using FlatList and what optimizations would you apply?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["Databricks","Microsoft"]},{"id":"q-589","question":"How do you handle different screen sizes and orientations in React Native?","channel":"react-native","subChannel":"general","difficulty":"beginner","tags":["react-native"],"companies":["Hashicorp","Instacart","LinkedIn"]},{"id":"q-183","question":"What are Native Modules in React Native, when should you use them, and what are the key performance and threading considerations?","channel":"react-native","subChannel":"native-modules","difficulty":"beginner","tags":["native","bridge"],"companies":null},{"id":"q-206","question":"How would you optimize React Native list performance with Hermes and Reanimated when dealing with 10k+ items containing complex animations?","channel":"react-native","subChannel":"performance","difficulty":"advanced","tags":["hermes","reanimated","profiling"],"companies":["Airbnb","Coinbase","Meta","Microsoft","Uber"]},{"id":"q-233","question":"How does the Hermes engine improve React Native app startup performance compared to JavaScriptCore, and what are the specific trade-offs?","channel":"react-native","subChannel":"performance","difficulty":"beginner","tags":["hermes","reanimated","profiling"],"companies":["Airbnb","Meta","Microsoft","Netflix","Salesforce","Shopify"]},{"id":"q-1023","question":"Design a centralized, tamper-evident logging pipeline for 6 RHEL hosts. Include: enable persistent journald, forward logs over TLS to a central collector, configure rotation/retention, protect in transit with certificate-based auth, and a rollback/validation plan that proves delivery during outages. Outline testing steps and failure scenarios?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Apple","Hashicorp"]},{"id":"q-1118","question":"On a RHEL 9 host, a service named app writes to /srv/app/data. After deployment, SELinux denials prevent writes. Without disabling SELinux, outline exact, implementable steps to restore functionality, including identifying the AVC, creating a targeted policy module with audit2allow, loading it, labeling the data directory, and validating the fix with a controlled write and audit checks?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Apple","Hugging Face","Twitter"]},{"id":"q-1148","question":"On a RHEL 8 server, implement a daily backup of /home/userdata to /backup/userdata-YYYYMMDD.tgz, exclude caches and temp dirs, preserve permissions, and generate a sha256 checksum. Schedule at 02:30 via cron and rotate backups to keep last 7 days. Provide commands and a script outline?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Goldman Sachs","Instacart","Lyft"]},{"id":"q-1169","question":"Design and implement an encrypted root on LVM for a production RH host. Boot partition remains unencrypted; the root filesystem sits on a LUKS2 container inside an LVM PV, with a keyfile for unattended boot. Provide a concrete, command-level plan including crypttab, fstab, initramfs (dracut) generation, and grub configuration to ensure the system boots automatically after rotation?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Bloomberg","Plaid","Snap"]},{"id":"q-1192","question":"On a RHEL8 host, set up a minimal Python HTTP server listening on port 8080, accessible only from 192.168.100.0/24. Use a non-root user, a systemd service, SELinux port labeling, and firewalld rules. Provide exact commands to create the service, configure semanage for port 8080, apply firewall rules, and test from a client. Address potential SELinux and port conflict caveats?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Instacart","LinkedIn"]},{"id":"q-1247","question":"On a RHEL 8 server, a web service listens on 127.0.0.1:9090. Configure firewalld to expose port 9090 only to the 10.1.0.0/24 management network, log drops, and persist across reboots. Provide exact commands, and describe test steps using curl from an allowed host and from a non-allowed host?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Amazon","Snap"]},{"id":"q-1399","question":"You manage a RHEL 8 server running SSH on port 22. To improve security, change SSH to listen on port 2222, disable root SSH login, and require key-based authentication, ensuring no downtime for existing sessions. List the exact commands and steps to implement this, including firewall and SELinux considerations, and how you verify connectivity afterwards?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Hugging Face","LinkedIn","MongoDB"]},{"id":"q-1484","question":"On a Red Hat-based host, you must deploy a statically compiled Go web app that runs on port 8080 behind firewalld with SELinux enforcing. The app should auto-start on boot, restart on failure, and log to rsyslog. Propose the concrete steps, files, and exact commands to configure systemd, firewalld, and SELinux contexts, ensuring minimal downtime during deploy?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["Meta","Salesforce"]},{"id":"q-1517","question":"On a RHEL 9 host, a custom service started via systemd fails to write logs to /var/www/app/logs after a patch, with SELinux enforcing. Describe a precise, minimal-risk remediation plan to identify and fix the root cause without broad permission grants, including AVC collection, targeted policy generation, and validation under load. What exact commands and steps would you perform?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Meta","Snap"]},{"id":"q-1577","question":"You must serve a static site from content stored on an NFS mount at /srv/www with SELinux enforcing on a RHEL-based system. The SELinux policy blocks httpd from reading the files. Describe the exact sequence of commands and configurations to allow Apache to serve the site without disabling SELinux or putting the system in permissive mode, including boolean toggles, labeling, and firewall rules?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["Lyft","Meta","Twitter"]},{"id":"q-1610","question":"Configure log rotation for a custom web app log path /var/log/myapp/*.log on a live Linux host. The log is written by a root-owned process and must rotate weekly, keep 4 copies, compress old logs, and after rotation reload nginx to reopen its handles. Provide a minimal, working logrotate.conf snippet and explain how this prevents log loss?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Coinbase","IBM","Plaid"]},{"id":"q-1649","question":"On a RHEL system with root on an LV named /dev/vg_rhel/root, describe a precise method to test a disruptive system update using an LVM snapshot: create a 20G snapshot, apply the update inside the snapshot, validate service health, and rollback by booting the live system from the original LV if needed. Include exact commands?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["Citadel","Google"]},{"id":"q-1679","question":"On a RHEL/CentOS host with two NICs (enp0s3 and enp0s8) connected to two switches, configure a 802.3ad (LACP) bond0 using NetworkManager. Bond should have a static IP 192.0.2.100/24, and both NICs must be slaves. Provide exact nmcli commands to create bond0, add slaves, bring it up, and verify; note required switch settings (LACP active on both ports)?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["Databricks","Scale Ai","Stripe"]},{"id":"q-1738","question":"On a RHEL-based host, configure firewalld to allow SSH access only from 192.0.2.0/24 and deny all other inbound SSH; ensure the changes persist after reboot and can be tested quickly; outline exact commands and verification steps, including revert plan?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Microsoft","Tesla","Zoom"]},{"id":"q-1756","question":"On a freshly provisioned Linux host (RHEL8), configure a new user 'audit' to log in exclusively via SSH key authentication, with a restricted shell rbash so only basic commands are allowed, and grant passwordless sudo to restart the 'auditd' service. Provide exact commands and edits to users, sshd_config, and sudoers, and how you would verify the setup?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Google","IBM"]},{"id":"q-1771","question":"On a RHEL 8 server, you must host a small static site with Nginx, ensure it starts on boot, expose only port 80, and serve content from /var/www/html with correct SELinux context. Provide exact commands to configure Nginx, firewall, and SELinux so SELinux stays enforcing?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Meta","Snap","Snowflake"]},{"id":"q-1804","question":"On a Red Hat-based host, a Python web app writes to /srv/app/data and /var/log/app. After a system update, SELinux denies these writes. Outline an operational plan to diagnose and restore write access without disabling SELinux, including commands for audit review, context restoration, and persistent policy adjustments?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Amazon","Meta","Square"]},{"id":"q-1982","question":"On a Linux host, create a project share at /srv/project where Alice can read/write and Bob can read only, with all others denied. Use POSIX permissions plus ACLs so new files created by Alice inherit Bob's read access. Provide exact commands to: 1) create the group, 2) add users, 3) set up the directory with setgid, 4) apply ACLs (explicit for Bob and default for new files), 5) verify with tests?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["DoorDash","Oracle","Zoom"]},{"id":"q-2225","question":"On a RHEL 9 server running Apache httpd hosting a custom PHP app, uploads are saved to /srv/app/uploads owned by webuser with SELinux type httpd_sys_content_t. After a denied upload, design and implement a minimal SELinux policy module that lets httpd_t write to that directory and create new files, without broad permissive mode. Include exact commands and testing steps?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Lyft","MongoDB"]},{"id":"q-2308","question":"On a fresh RHEL8 host with an unallocated disk at /dev/sdb, describe and implement the steps to (1) create a PV, (2) VG named vg_data, (3) LV named data of 900G, (4) format with XFS, (5) mount at /data with auto-mount in /etc/fstab, and (6) set ownership to user app and proper SELinux context. Provide exact commands?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Google","Hashicorp","Scale Ai"]},{"id":"q-2357","question":"Scenario: You manage a RHEL host where /var/www and /etc live on an LVM-managed XFS volume. Implement a DR restore pipeline: daily incremental backups via rsync to an offsite server, preserving SELinux contexts and ACLs, with integrity verification. Describe the exact commands, systemd timer/unit setup, and restore steps to recover to the latest backup within 60 minutes?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Hashicorp","Oracle"]},{"id":"q-2415","question":"On a fresh RHEL 9 server, implement a small utility that prints the host name and current date, running it as a non-login user via systemd and logging to a dedicated file with rotation. Provide exact commands for user creation, script, service, and log rotation?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Airbnb","Meta"]},{"id":"q-2457","question":"On a Linux host, a Node.js web app runs in a container and connects to Postgres on localhost:5432. After reboot, the web app sometimes starts before Postgres is ready, causing startup failures. Propose a concrete, production-ready systemd-based solution: (1) implement a db-wait.service that blocks until pg_isready reports healthy, (2) make web-app.service depend on and After=db-wait.service, (3) add a lightweight container readiness check for the app, and (4) describe how you would test this reliably?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Apple","Discord"]},{"id":"q-2479","question":"On a RHEL 9 host, a production service writes logs to /var/app/logs with 0777 perms. You must relocate logs to a dedicated 1 TB LVM-backed XFS volume mounted at /var/app/logs with no downtime, ensure SELinux context, and configure logrotate to compress weekly. Provide exact steps including LV creation, filesystem, fstab, SELinux relabeling, and logrotate config?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Netflix","PayPal","Two Sigma"]},{"id":"q-2557","question":"On a fresh RHEL 9 host, configure firewalld to allow SSH only from 203.0.113.0/24 and drop SSH from all other sources. Provide exact permanent commands, reload, and verification steps?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Databricks","Hashicorp","MongoDB"]},{"id":"q-2637","question":"On a fresh RHEL 9 host, deploy a tiny Python HTTP health server that binds to 127.0.0.1:9090 and returns 200 OK for GET /health when the file /tmp/healthy exists; otherwise 503. Run this as a non-root systemd service with a dedicated unit file, a startup script, and a log file rotated by logrotate. Provide exact commands to create the non-login user, script, systemd unit, logrotate config, and enable the service. Ensure idempotence?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Anthropic","Hashicorp","OpenAI"]},{"id":"q-2647","question":"On a fresh RHEL 9 server, create a non-root user named devops and configure passwordless sudo for only two commands: /bin/systemctl restart httpd and /usr/bin/journalctl -xe. Restrict SSH login for this user to localhost by updating sshd_config. Provide exact commands and file contents to accomplish this?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Microsoft","MongoDB"]},{"id":"q-2756","question":"Design and implement an offline, local repository patching workflow for 20 RHEL 9 servers to achieve zero downtime. Outline mirroring steps with reposync, GPG signing, local repo config, a rolling update procedure (kernel first, then user-space), reboot strategy, rollback via dnf history, and auditing. Provide exact commands and a deployment checklist?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Hugging Face","OpenAI","Oracle"]},{"id":"q-2838","question":"On a RHEL 9 host exposed to the internet, harden SSH with key-based login only, disable root login, enforce 2FA via pam_google_authenticator for SSH, and configure auditd to log authentication attempts with a daily report; provide exact commands and config snippets to implement this with minimal downtime?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Goldman Sachs","LinkedIn","Robinhood"]},{"id":"q-3053","question":"On a fresh RHEL 9 server, implement a minimal nftables-based firewall that permits SSH only from 192.168.1.0/24, allows loopback, and drops all other inbound traffic. Make the rules persistent across reboots and verify. Provide exact commands to install nftables, create /etc/nftables.conf with the rules, enable the nftables service, and test connectivity from an allowed host and a disallowed host?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Coinbase","Microsoft","Snap"]},{"id":"q-3130","question":"You have a running RHEL 9 server hosting an Apache site. The DocumentRoot /var/www/html must move to a dedicated XFS LV mounted at /srv/www, with zero downtime, preserving SELinux contexts and without changing client URLs. Provide exact commands, config changes, and rollback steps to perform the migration safely?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["Microsoft","Robinhood"]},{"id":"q-3216","question":"Devise a three-tenant container deployment on a single RHEL 9 host (advanced RHCSA). Each tenant runs in its own container using non-root user namespaces, isolated network namespace, and defined CPU and memory quotas. Implement per-tenant logging and logrotate, and address SELinux considerations. Provide exact commands and files: UID/GID mappings, subuid/subgid setup, Podman usage, systemd unit files, and a minimal HTTP server container example?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Google","Snap"]},{"id":"q-3303","question":"On a RHEL 9 host behind Nginx serving a critical API, describe a zero-downtime upgrade path for a Python Flask app using blue/green deployment. Provide exact steps and commands to prepare two virtualenvs, two systemd service units, traffic switch with Nginx, health checks, and rollback plan, including SELinux considerations and verifying in-flight requests are not dropped?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Instacart","Robinhood"]},{"id":"q-3433","question":"On a fresh RHEL 9 server, set up a minimal local Yum repository mirror to host a subset of packages. Create a non-privileged user reposync, configure httpd to serve /var/www/html/repos, set up reposync -p /var/www/html/repos -r base -n to populate the mirror, and automate with a systemd service and daily timer. Enable GPG verification and log rotation. Provide exact commands for user creation, systemd unit, timer, httpd, and logrotate?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Cloudflare","NVIDIA","Snowflake"]},{"id":"q-3462","question":"Scenario: A prod RHEL 9 fleet generates app logs at /var/log/myapp; implement a centralized TLS-enabled rsyslog forwarder to log-ops.example.local:6514. Use mutual TLS with a private CA, configure per-host queues to tolerate outages, ensure proper SELinux context, and update firewalld. Provide exact rsyslog.conf fragments, certificate paths, and verification steps?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Instacart","Meta","Netflix"]},{"id":"q-3501","question":"On a fresh RHEL 9 host, configure a minimal static website that serves files from /var/www/html/app using httpd. Ensure httpd starts on boot, firewall allows HTTP, and SELinux context is correct so httpd can read the files. Provide exact commands and files touched?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Salesforce","Zoom"]},{"id":"q-3586","question":"On a fresh RHEL 9 server, configure a non-login systemd timer that runs every 15 minutes a small script which prints the host name and current time to a log file at /var/log/usage.log, and configure logrotate to rotate that log daily keeping 7 days of history; provide exact commands for creating the user, script, timer/unit files, and logrotate config?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Databricks","Discord","LinkedIn"]},{"id":"q-3832","question":"On a RHEL 9 host, a kernel update crashes a critical service. Implement an auditable rollback workflow: 1) pin the problematic kernel with dnf versionlock, 2) keep older kernel installed, 3) configure GRUB to boot the older kernel by default, 4) provide a one-shot rollback script that switches GRUB default to the older entry and reboots, 5) ensure all actions are logged in /var/log/rollback.log. Include exact commands, files, and a minimal test?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Airbnb","Goldman Sachs","LinkedIn"]},{"id":"q-3996","question":"On a fresh RHEL 9 server, configure firewalld to allow SSH access only from subnet 10.0.0.0/24, ensure the rule is permanent and persists after reboot, and verify by attempting to connect from an allowed host and a disallowed host. Provide exact commands to install and start firewalld, create the permanent rule, reload, and verify with firewall-cmd and nc?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Apple","Meta","Microsoft"]},{"id":"q-4058","question":"On a fresh RHEL 9 server, create a non-login user 'web' and serve a static site from /srv/www on port 8080 using a per-service systemd unit that runs as 'web'. Provide exact commands to create the user, prepare the site, write the unit file at /etc/systemd/system/website.service, enable and start the service, and adjust the firewall to allow 8080; verify with curl http://localhost:8080 and confirm the process runs as user 'web'?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Discord","MongoDB","Netflix"]},{"id":"q-4074","question":"On a RHEL 9 host, configure rsyslog to forward all application logs to a TLS-enabled remote collector at loghub.example.com:10514. Enforce certificate validation, use a persistent disk-backed queue to retain logs for 30 days if the network is down, and verify with a test log entry. Provide exact rsyslog.conf fragments, TLS options, and rollback steps?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["Citadel","Google","Hashicorp"]},{"id":"q-4084","question":"On a RHEL 9 host, deploy a new service in its own network namespace with a dedicated veth pair, assign 192.168.100.2/24 inside, and 192.168.100.1/24 on the host. Configure NAT and per-namespace traffic logging using nftables. Provide exact commands to create the netns, veth pair, move one end to the namespace, set addresses, enable IP forwarding, and a minimal systemd unit to run the service inside the namespace?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Airbnb","Meta","Snap"]},{"id":"q-4115","question":"On a RHEL 9 host running PostgreSQL with the data directory on an LV mounted at /var/lib/pgsql/data, design and implement a zero-downtime migration to a new XFS LV mounted at /srv/pgsql, preserving SELinux contexts and existing client URLs. Provide exact commands for LV creation, filesystem setup, an rsync-based data copy, a brief cutover sequence, and rollback steps?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["Airbnb","Cloudflare","Databricks"]},{"id":"q-4192","question":"On a RHEL 9 host, deploy a small service at /opt/websvc that runs as user websvc, listens on port 8443, and writes data to /var/lib/websvc/data. The default SELinux policy blocks both data access and network binding. Provide exact commands and steps to (1) create a minimal systemd unit, (2) label the directories, and (3) implement a dedicated SELinux module (websvc_t) that allows websvc_t to read /opt/websvc, write to /var/lib/websvc/data, and bind to 8443, plus firewall adjustments and a test plan?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["IBM","Plaid"]},{"id":"q-4217","question":"Scenario: A RH9 host runs MongoDB with its data directory on an LV mounted at /data/mongo (XFS). During a maintenance window migrate the data dir to a new LV data_mongo_new of equal size mounted at /data/mongo2 preserving SELinux contexts and keeping client access seamless with no downtime. Provide exact commands to create the LV, format copy data update fstab and SELinux contexts perform the swap with minimal downtime and rollback steps if needed?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["MongoDB","Netflix"]},{"id":"q-4276","question":"On a fresh RHEL 9 server, deploy a tiny HTTP server that serves static files from /srv/www on port 8080, running as a non-root user 'webstatic'. Implement a systemd service to keep it alive and a separate timer that runs a health-check every 5 minutes and restarts the service if needed. Configure log rotation for /var/log/webstatic.log. Provide exact commands for user creation, the wrapper script, the service unit, the health-check script, the timer, and the logrotate entry?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Anthropic","Meta"]},{"id":"q-4334","question":"On a RHEL 9 host running Apache with TLS, perform a patch upgrade of OpenSSL with minimal downtime across a single host. Provide exact commands to verify the current version, apply the patch while keeping configuration intact, validate TLS handshakes post-upgrade, and a rollback procedure in case the new libcrypto breaks compatibility (including rpm history and reversion)?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["Adobe","Discord","OpenAI"]},{"id":"q-4361","question":"On a fresh RHEL 9 server, implement a small audit task: create a non-login user 'auditor', a script /usr/local/bin/collect-system-uptime.sh that appends date and uptime to /var/log/auditor/uptime.log, and schedule it with a systemd timer to run hourly. Ensure log rotation via /etc/logrotate.d/auditor and verify that file permissions and SELinux contexts allow the script to write under /var/log/auditor without enabling login. How would you implement this end-to-end?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Citadel","OpenAI","Square"]},{"id":"q-4381","question":"On a RHEL 9 host, deploy a Go API service (port 8443) in a Podman container, behind an Nginx reverse proxy with TLS termination. The API must run as user 'apiuser', store data under /var/lib/api/data, and SELinux must allow network bind and file access. Implement a blue/green deployment on the same host with zero downtime and a rollback plan, including exact Podman unit files, nginx site config, SELinux policy snippet, and data-migration strategy?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Airbnb","Instacart","OpenAI"]},{"id":"q-4490","question":"On a fresh RHEL 9 server, implement a tiny static web server that serves /srv/www on port 8080, running as a non-root user via systemd, with access logs written to /var/log/webmon/access.log and rotated. Provide exact commands to create the user, the server script, the systemd unit, SELinux labeling for /srv/www, and a logrotate config; ensure firewall allows 8080?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Anthropic","Hugging Face"]},{"id":"q-4615","question":"On a RHEL 9 host, TLS terminates at Nginx in front of a backend app. Certificates expire in 24 hours. Implement a zero-downtime certificate rotation strategy: (1) renew and install new certs to a separate path, (2) swap a symlink used by ssl_certificate to point to the new cert, (3) gracefully reload Nginx, (4) verify handshakes and SELinux contexts. Provide exact commands and validation steps?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Salesforce","Square","Two Sigma"]},{"id":"q-4788","question":"On a RHEL 9 host running three Podman containers (web, api, db) attached to a dedicated bridge, design and implement an nftables firewall that: (1) blocks inbound to the host except SSH and per-container ports limited to container IPs; (2) allows outbound DNS and HTTPS; (3) isolates containers from each other; (4) logs denied attempts; (5) provides /etc/nftables.conf and a rollback plan with verification steps. What exact configuration would you use?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["Lyft","MongoDB","Uber"]},{"id":"q-933","question":"On a fresh RHEL 9 installation with a 120 GB disk, implement an LVM layout: ROOT 40G, HOME 40G, VAR 40G, all using XFS. Create PV, VG, and LVs, format, and mount at /, /home, /var with fstab. Enable and configure firewalld to allow http and https. Ensure SELinux is enforcing. Create a non-root user 'dev' and add to the wheel group with sudo privileges. Show commands and rationale?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["Adobe","IBM","Microsoft"]},{"id":"gh-24","question":"What is DevSecOps and how does it differ from traditional DevOps security approaches?","channel":"security","subChannel":"application-security","difficulty":"advanced","tags":["security","devsecops"],"companies":["Amazon","Coinbase","Google","Microsoft","Uber"]},{"id":"gh-44","question":"How do you implement a comprehensive API security strategy that protects against common vulnerabilities while maintaining developer productivity?","channel":"security","subChannel":"application-security","difficulty":"beginner","tags":["api","service-mesh"],"companies":["Amazon","Microsoft","Morgan Stanley","PayPal","Stripe"]},{"id":"gh-69","question":"How does Zero Trust Security implement identity-based access control with micro-segmentation using modern cloud infrastructure and identity providers?","channel":"security","subChannel":"application-security","difficulty":"advanced","tags":["security","network"],"companies":null},{"id":"q-230","question":"How would you implement a Content Security Policy (CSP) with nonce-based inline script protection to prevent XSS while maintaining compatibility with third-party analytics?","channel":"security","subChannel":"application-security","difficulty":"intermediate","tags":["xss","csrf","sqli","ssrf"],"companies":["Airbnb","Google","Microsoft","Stripe","Uber"]},{"id":"q-276","question":"How would you design a secure job scheduling system for a microservices environment that prevents privilege escalation while ensuring reliable execution across distributed services?","channel":"security","subChannel":"application-security","difficulty":"advanced","tags":["systemd","cron","users","permissions"],"companies":null},{"id":"q-283","question":"What is the difference between XSS and CSRF attacks?","channel":"security","subChannel":"application-security","difficulty":"beginner","tags":["xss","csrf","sqli","ssrf"],"companies":["Amazon","Google","Meta"]},{"id":"q-359","question":"You discover a reflected XSS vulnerability in a search feature. The search term is displayed back to the user without sanitization. How would you fix this, and what's the difference between reflected XSS and stored XSS in terms of impact and remediation?","channel":"security","subChannel":"application-security","difficulty":"intermediate","tags":["xss","csrf","sqli","ssrf"],"companies":["Fortinet","Google","Tesla"]},{"id":"q-404","question":"You're building a financial trading platform at Jane Street. How would you design a secure authentication and authorization system that prevents XSS, CSRF, SQLi, and SSRF attacks while maintaining high performance for real-time trading data?","channel":"security","subChannel":"application-security","difficulty":"advanced","tags":["xss","csrf","sqli","ssrf"],"companies":["Canva","Jane Street","Miro"]},{"id":"q-423","question":"You discovered a critical security vulnerability in your team's production system that could expose customer data, but fixing it requires delaying a major product launch. How would you handle this situation, considering CVSS scoring, stakeholder communication, and risk-benefit analysis?","channel":"security","subChannel":"application-security","difficulty":"advanced","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-202","question":"How do passkeys implement passwordless authentication using public-key cryptography?","channel":"security","subChannel":"authentication","difficulty":"beginner","tags":["mfa","passkeys","zero-trust"],"companies":["Apple","Google","Meta","Microsoft","Okta"]},{"id":"q-241","question":"How would you implement JWT authentication with RS256 signing and refresh token rotation to prevent token replay attacks?","channel":"security","subChannel":"authentication","difficulty":"intermediate","tags":["jwt","oauth2","oidc","saml"],"companies":["Airbnb","Amazon","Apple","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-302","question":"Explain the technical differences between OAuth 2.0 authorization flows and OpenID Connect authentication, including token structures, validation patterns, and security considerations?","channel":"security","subChannel":"authentication","difficulty":"beginner","tags":["jwt","oauth2","oidc","saml"],"companies":["Amazon","Google","Meta","Microsoft","Salesforce","Stripe"]},{"id":"q-324","question":"How would you design a zero-trust video conferencing platform using WebAuthn passkeys with continuous authentication for enterprise users?","channel":"security","subChannel":"authentication","difficulty":"advanced","tags":["mfa","passkeys","zero-trust"],"companies":["Snowflake","Spotify","Zoom"]},{"id":"q-387","question":"Design a zero-trust authentication system for Snowflake's data warehouse that supports MFA, passkeys, and handles 100M+ daily auth requests. How would you prevent replay attacks while ensuring sub-100ms latency?","channel":"security","subChannel":"authentication","difficulty":"advanced","tags":["mfa","passkeys","zero-trust"],"companies":["Infosys","New Relic","Snowflake"]},{"id":"gh-70","question":"How does the TLS 1.3 handshake establish secure communication and what cryptographic mechanisms ensure perfect forward secrecy?","channel":"security","subChannel":"encryption","difficulty":"advanced","tags":["security","network"],"companies":["Amazon","Cloudflare","Google","Hashicorp","Netflix","Square"]},{"id":"q-179","question":"Explain how Perfect Forward Secrecy (PFS) works in TLS, describe the ECDHE key exchange mechanism, and analyze the security trade-offs compared to RSA key exchange?","channel":"security","subChannel":"encryption","difficulty":"intermediate","tags":["encryption","crypto"],"companies":["Amazon","Apple","Cloudflare","Google","Microsoft","Stripe"]},{"id":"q-295","question":"How does AES-256-GCM provide both confidentiality and integrity in a single cryptographic operation?","channel":"security","subChannel":"encryption","difficulty":"advanced","tags":["aes","rsa","tls","hashing"],"companies":["Amazon","Google","Meta"]},{"id":"q-310","question":"How does TLS 1.3 improve security compared to TLS 1.2?","channel":"security","subChannel":"encryption","difficulty":"intermediate","tags":["aes","rsa","tls","hashing"],"companies":["Amazon","Google","Meta"]},{"id":"q-337","question":"Design a secure key exchange system for autonomous vehicle communication between cars and infrastructure. How would you handle forward secrecy and key rotation in a high-mobility environment?","channel":"security","subChannel":"encryption","difficulty":"advanced","tags":["aes","rsa","tls","hashing"],"companies":["Apple","Cruise","Uber"]},{"id":"q-348","question":"You're designing a secure booking system for Expedia that handles payment data. How would you implement a hybrid encryption scheme using RSA for key exchange and AES-256-GCM for data encryption, and what specific security considerations would you address for PCI DSS compliance?","channel":"security","subChannel":"encryption","difficulty":"advanced","tags":["aes","rsa","tls","hashing"],"companies":["Amazon","Apple","Expedia","Microsoft","OpenAI","PayPal","Square","Stripe"]},{"id":"q-1234","question":"You operate a CDN edge platform that lets customers deploy WebAssembly modules for request processing. Outline a practical, auditable approach to securely load, validate, and sandbox these modules, covering authentication (signatures/attestation), host-function access, resource quotas, revocation, and incident response?","channel":"security","subChannel":"general","difficulty":"advanced","tags":["security"],"companies":["Apple","Cloudflare","Microsoft"]},{"id":"q-2613","question":"In a real-time trading platform, how would you implement low-latency, per-request authorization for microservices and WebSocket streams to prevent token replay and privilege escalation, while ensuring global revocation is scalable and auditable?","channel":"security","subChannel":"general","difficulty":"advanced","tags":["security"],"companies":["LinkedIn","Robinhood","Snap"]},{"id":"q-2683","question":"You're building a beginner-friendly secure data ingestion portal where CSV files uploaded by partners feed a Databricks Delta table. Outline a minimal, practical security flow to ensure the upload cannot execute code, is stored securely, and triggers a Databricks job safely. Include concrete steps and a simple code snippet for validating file type and preserving audit trails before queuing the Databricks job?","channel":"security","subChannel":"general","difficulty":"beginner","tags":["security"],"companies":["Databricks","Snap"]},{"id":"q-2802","question":"You manage OTA updates for a fleet of autonomous vehicles. Create an end-to-end secure supply chain plan: reproducible builds and artifact signing with Sigstore (Cosign/Fulcio/Rekor), SBOMs, offline/remote attestation on constrained ECUs, Vault-based secret management, GitOps deployment, and rollback + anomaly detection. Include concrete steps, artifacts, and trade-offs?","channel":"security","subChannel":"general","difficulty":"advanced","tags":["security"],"companies":["Hashicorp","Lyft","Tesla"]},{"id":"q-2878","question":"In a production analytics pipeline (Kafka -> Flink -> ClickHouse), you need to protect PII with differential privacy. Propose concrete steps to implement per-event DP budgets, where to inject noise, how to enforce budgets, and how to validate privacy guarantees in staging. Include concrete config hooks and trade-offs between accuracy and privacy?","channel":"security","subChannel":"general","difficulty":"intermediate","tags":["security"],"companies":["Apple","Instacart","Snap"]},{"id":"q-2991","question":"Design a cryptographic agility plan for a multi-tenant SaaS deployed across two public clouds, covering envelope encryption, key management, per-tenant data keys, cross-region availability, and a migration path to quantum-safe algorithms without downtime. Include tenant data separation and strict access controls?","channel":"security","subChannel":"general","difficulty":"advanced","tags":["security"],"companies":["Airbnb","Microsoft","NVIDIA"]},{"id":"q-3031","question":"You're deploying a fleet of edge devices (kiosks/chargers) with intermittent connectivity. Design a secure firmware/config update pipeline delivering signed deltas, boot-time attestation, atomic swap with rollback, and rapid key-compromise containment. Detail cryptographic schemes, trust anchors, orchestration, and end-to-end testing?","channel":"security","subChannel":"general","difficulty":"advanced","tags":["security"],"companies":["Airbnb","Apple","Tesla"]},{"id":"q-3169","question":"You're building a fintech profile-update API (Express/Node) used by Coinbase-like customers. Outline a beginner-friendly security plan that covers authentication, authorization, input validation, rate limiting, and audit logging. Include concrete, implementable steps and minimal code/config snippets for common tools such as JWT check, Joi validation, and AWS WAF rule?","channel":"security","subChannel":"general","difficulty":"beginner","tags":["security"],"companies":["Coinbase","Oracle","Two Sigma"]},{"id":"q-3248","question":"In a fast-moving fintech platform running Kubernetes on AWS, you discover that a shared base image in your CI/CD pipeline included a poisoned dependency, and builds signed with a compromised key made it into production. Explain end-to-end how you would detect the breach, contain it, and prevent recurrence using SBOMs, reproducible builds, image signing (Sigstore), and policy gates (OPA/Gatekeeper). Include concrete steps and trade-offs?","channel":"security","subChannel":"general","difficulty":"intermediate","tags":["security"],"companies":["Amazon","Goldman Sachs","Google"]},{"id":"q-3388","question":"Design a secure OTA update workflow for a fleet of edge devices running a security agent. Updates arrive from an online channel and could be tampered with. Describe how you would implement dual signing, hardware-backed attestation, SBOM verification, reproducible builds, delta updates, secure boot, rollback, and proactive key rotation. Include trade-offs and validation steps?","channel":"security","subChannel":"general","difficulty":"intermediate","tags":["security"],"companies":["Apple","NVIDIA","Scale Ai"]},{"id":"q-3479","question":"Design a secure file attachment workflow for a Discord/Slack-like chat app. From upload to rendering, specify validation steps, malware scanning, metadata sanitization, size/type limits, isolation of renderers, and how you would prevent remote code execution and data exfiltration when users share files. How would you test and validate this?","channel":"security","subChannel":"general","difficulty":"beginner","tags":["security"],"companies":["Discord","Google","Slack"]},{"id":"q-3543","question":"Design a secure, scalable flow for granting temporary API access to a partner service in a multi-tenant platform. Tokens must be short-lived, revocable, and auditable. Describe issuance (OIDC/OAuth2), scopes, IP restrictions, JWT structure, introspection, key rotation, revocation lists, anomaly detection, and a rollback-safe revocation workflow. Include concrete trade-offs?","channel":"security","subChannel":"general","difficulty":"intermediate","tags":["security"],"companies":["Instacart","Plaid","Twitter"]},{"id":"q-3647","question":"You're building a fintech service that talks to multiple partners via webhooks and mobile clients. Design a secure webhook verification and session management strategy using rotating keys (JWKS), strict claims (iss, aud, exp), nonce/replay protection, and automated key distribution. Compare HMAC vs RSA for signing, and outline zero-downtime key rotation and incident response steps?","channel":"security","subChannel":"general","difficulty":"intermediate","tags":["security"],"companies":["Plaid","Robinhood","Uber"]},{"id":"q-3742","question":"In a Twitter/Amazon-scale microservices stack on Kubernetes with a service mesh (Istio) and SPIFFE identities, design a defense-in-depth strategy to prevent token leakage, misissuance, and lateral movement when an edge node is compromised. Specify controls for (a) token binding and short-lived credentials, (b) per-service JWKS rotation and revocation, (c) secret management and least-privilege policies, and (d) detection/response with telemetry and playbooks?","channel":"security","subChannel":"general","difficulty":"advanced","tags":["security"],"companies":["Amazon","Twitter"]},{"id":"q-4321","question":"In a Netflix-scale, multi-tenant Kubernetes platform, design a security solution that guarantees only images signed with a trusted Sigstore issuer and with verifiable SBOM attestations are deployed, while preserving developer velocity. Outline the CI/CD signing flow, admission controls (OPA/Kyverno), runtime attestation (SPIRE), secret management (Vault), rollback strategy, and measurement of effectiveness?","channel":"security","subChannel":"general","difficulty":"advanced","tags":["security"],"companies":["Lyft","MongoDB","Netflix"]},{"id":"q-4433","question":"You're integrating a third-party webhook (e.g., payment notifications) into a service used by users and apps. Describe a secure webhook verification pipeline using HMAC signatures and replay protection, including secret management, timestamp checks, nonce handling, and how you would validate payload integrity across deployments and secret rotations?","channel":"security","subChannel":"general","difficulty":"beginner","tags":["security"],"companies":["Apple","Coinbase","Meta"]},{"id":"q-4501","question":"Describe a beginner-friendly approach to implementing tamper-evident security event logging for authentication across a microservices-based travel platform. Specify what data to log, how to prevent tampering (append-only storage, cryptographic signing), how to validate logs during an incident, and how to handle privacy/PII?","channel":"security","subChannel":"general","difficulty":"beginner","tags":["security"],"companies":["Airbnb","Bloomberg"]},{"id":"q-4548","question":"You're building a webhook receiver for a payment processor in a microservice. Describe a minimal, production-ready verification workflow: (a) signature validation with the processor's public key, (b) replay-attack prevention with a timestamp window or nonce, (c) idempotency using a delivery-id store, (d) redaction of PII in logs, (e) key rotation and monitoring tests?","channel":"security","subChannel":"general","difficulty":"beginner","tags":["security"],"companies":["Citadel","Plaid"]},{"id":"q-4572","question":"You're adding a REST API that serves PII for three brands (MongoDB-backed user data, Plaid-like payments, Lyft-like rides). Create a practical, beginner-friendly security plan for: 1) client authentication; 2) data protection in transit and at rest; 3) tamper resistance and replay protection; and 4) key management and rotation. Include concrete steps and minimal example configs?","channel":"security","subChannel":"general","difficulty":"beginner","tags":["security"],"companies":["Lyft","MongoDB","Plaid"]},{"id":"q-4753","question":"Design a secure runtime attestation and supply-chain verification for a multi-cloud microservices platform spanning AWS, GCP, and on-prem edge. Each service builds from a central repo and uses third-party libraries. Propose concrete architecture and artifacts to prevent a compromised dependency from running in production, including build signing, runtime attestation, key management, revocation, and incident response. Mention tools and workflows (Sigstore, Rekor, Fulcio, OIDC, KMS)?","channel":"security","subChannel":"general","difficulty":"advanced","tags":["security"],"companies":["Apple","Instacart","Netflix"]},{"id":"q-476","question":"How would you prevent SQL injection in a web application and what are the common attack vectors?","channel":"security","subChannel":"general","difficulty":"beginner","tags":["security"],"companies":["Lyft","Netflix"]},{"id":"q-505","question":"You're building a payment processing API that must handle PCI compliance. How would you design the architecture to ensure sensitive card data never touches your servers while maintaining low latency for payment validation?","channel":"security","subChannel":"general","difficulty":"advanced","tags":["security"],"companies":["Hugging Face","Microsoft","Square"]},{"id":"q-534","question":"How would you implement secure session management in a distributed web application to prevent session hijacking and fixation attacks?","channel":"security","subChannel":"general","difficulty":"intermediate","tags":["security"],"companies":["Airbnb","Google","Two Sigma"]},{"id":"q-561","question":"How would you implement secure session management for a web application using JWT tokens, and what are the key security considerations?","channel":"security","subChannel":"general","difficulty":"intermediate","tags":["security"],"companies":["Discord","MongoDB"]},{"id":"gh-71","question":"How does a Web Application Firewall (WAF) protect against OWASP Top 10 attacks at the application layer?","channel":"security","subChannel":"owasp","difficulty":"advanced","tags":["security","network"],"companies":["Akamai","Amazon","Cloudflare","Google","Microsoft"]},{"id":"q-255","question":"How would you implement OWASP ASVS L3 input validation for a REST API endpoint that accepts JSON payloads with nested objects?","channel":"security","subChannel":"owasp","difficulty":"intermediate","tags":["top10","asvs","samm"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-373","question":"How would you implement a comprehensive defense-in-depth strategy to prevent SQL injection attacks in a modern web application following OWASP Top 10 guidelines?","channel":"security","subChannel":"owasp","difficulty":"beginner","tags":["top10","asvs","samm"],"companies":["Amazon","Cloudflare","Google","Meta","Microsoft","Netflix"]},{"id":"q-1086","question":"Design a Snowflake CDC pattern for an SCD2 customer_dim using a stream on the source table. Explain how to implement an idempotent upsert via MERGE, how deletes are represented, and how to handle late-arriving changes to preserve history?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Google","NVIDIA"]},{"id":"q-1152","question":"Design a least-privilege access layer in Snowflake for a multi-tenant data lake spanning five domains (marketing, sales, finance, analytics, operations). Describe how you would implement ROW ACCESS POLICIES and MASKING POLICIES on a payments table (columns: id, customer_id, amount, card_number, region) to restrict data by region and user role, and how you would audit access?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Adobe","Databricks","Lyft"]},{"id":"q-1239","question":"You’re building a beginner Snowflake task: a large SALES table is routinely queried with date-range filters. Propose a minimal clustering solution to improve pruning. Include exact commands to add a single clustering key on SALE_DATE, how to monitor its effectiveness, and how to decide if reclustering is needed. Keep changes focused and explain validation steps with a simple test?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Hugging Face","PayPal"]},{"id":"q-1337","question":"You're ingesting data daily into table sensor_readings(device_id STRING, ts TIMESTAMP_NTZ, reading FLOAT). You frequently query last 24 hours per device. Propose a beginner-friendly optimization path, choosing clustering key, automatic clustering, or a materialized view, and include an exact query to compute per-device count and average reading for the last 24 hours?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["PayPal","Plaid","Tesla"]},{"id":"q-1475","question":"You have a Snowflake table raw_events with columns file_name STRING, payload VARIANT containing an array at payload:'events'. Each event is an object { event, timestamp, user: { id }, region, amount }. Design a beginner-friendly approach to compute daily total_amount and per-user purchases for the last 7 days using a temporary VIEW and LATERAL FLATTEN. Provide the exact query you would run?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Citadel","Cloudflare","Oracle"]},{"id":"q-1499","question":"You're building a Snowflake-based telemetry store: table `raw.events` (device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) ingests 50M+ rows daily. BI dashboards query per-device event_count and median reading for last 7 days and last 24 hours. Propose a pragmatic optimization, selecting between clustering, automatic clustering, and a rolling aggregate MV/table. Include exact commands to implement your approach and how you would validate improvements?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Databricks","Tesla","Zoom"]},{"id":"q-1567","question":"You're operating a Snowflake telemetry store with table raw.events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) ingesting 100M+ rows daily. BI requires per-device metrics: (a) last hour event_count, (b) last 24h median of payload.metrics.reading. Propose a production-ready strategy: data model, clustering choices vs automatic clustering vs materialized views, and how to validate performance. Include exact commands to implement and how to verify improvements?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Lyft","MongoDB"]},{"id":"q-1605","question":"Design a Snowflake-based near real-time anomaly-detection pipeline for telemetry events. Ingested table: raw_events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). Ingests 200M+ rows/day. You need to detect spikes in a metric inside payload (e.g., latency) on a per-device basis with a 5-minute window and surface alerts to an alerts table when a spike exceeds a dynamic threshold. Describe the architecture using Streams, Tasks, and possibly a Snowflake procedure; include DDLs to create the stream, a task schedule, sample SQL for the rolling compute, and how you would validate the pipeline end-to-end?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Airbnb","Scale Ai","Stripe"]},{"id":"q-1678","question":"You're provisioning a Snowflake warehouse for a multi-tenant SaaS app. All tenant data sits in one table: raw_events (tenant_id STRING, event_ts TIMESTAMP_NTZ, event_type STRING, payload VARIANT) ingesting 40M+ rows/day. Propose a hybrid approach: (1) add a composite clustering key (tenant_id, event_ts) for pruning, and (2) a rolling 7-day materialized view with per-tenant metrics. Include exact SQL commands to implement clustering, MV, and a validation query showing the improvement?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Cloudflare","Goldman Sachs","Snowflake"]},{"id":"q-1711","question":"In a Snowflake telemetry store with table raw.events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) ingesting 50M+ rows daily, implement a production-grade data retention pipeline that archives data older than 365 days to an external stage and keeps only the last 365 days in the main table, while preserving query performance on recent data. Describe the architecture and provide exact SQL commands to create the stage, stream, task, and archive procedure, plus a validation plan?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Microsoft","Oracle","Snap"]},{"id":"q-2024","question":"Implement row-level security for regional data access in Snowflake. Table `customer.sales` has `customer_id`, `region`, `amount`, `last_purchase_ts`. Roles `SA_US` and `SA_EU` should only see rows for their region. Describe and implement a Snowflake ROW ACCESS POLICY using a role-context and attach it to the table, then provide a sample query that would be allowed for role `SA_US` and a test plan to verify enforcement?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Apple","Hugging Face","Microsoft"]},{"id":"q-2048","question":"You're building a multi-tenant analytics warehouse in Snowflake. Ingested events go into raw.events(tenant_id STRING, event_ts TIMESTAMP_NTZ, payload VARIANT). BI needs per-tenant metrics: (a) count of events in the last 15 minutes, (b) 95th percentile of latency from payload.metrics.latency in the last 24 hours. Propose a production-ready architecture: data model (partitioning, clustering), streaming vs batch paths (streams/tasks vs MV), decision between automatic clustering and materialized views, and a concrete validation plan with exact SQL commands?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Amazon","Scale Ai","Square"]},{"id":"q-2229","question":"You're operating a Snowflake event store table raw.events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). Implement GDPR-like retention: keep 90 days of live data, purge older rows by archiving to raw.events_archive and deleting from live daily via a Snowflake Task, while preserving time travel history and audits. How would you design the architecture, what SQL would you use, and how would you validate?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Anthropic","Apple","DoorDash"]},{"id":"q-2277","question":"In Snowflake, ingest telemetry into RAW.events (device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) from two sources: real-time stream and nightly batch. You must deliver per-device 15-minute latency metrics for the last hour (avg_reading, cnt). Propose an end-to-end architecture using a Stream, a Materialized View (MV) or incremental aggregation, and a TASK scheduled every 15 minutes to refresh. Include exact SQL to create the stream, MV, task, and a validation query to prove latency targets?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Discord","Slack","Square"]},{"id":"q-2303","question":"Scenario: a Snowflake account holds prod_sales.customers with PII. External partners at Oracle, Microsoft, and Nvidia need a sanitized subset for analytics. Propose a beginner-friendly data-share plan: masking policies for PII, a region-limited secure view, and an account-based share. Include commands to implement?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Microsoft","NVIDIA","Oracle"]},{"id":"q-2371","question":"You're building a Snowflake-based telemetry store loaded into a shared table raw.events (tenant_id STRING, ts TIMESTAMP_NTZ, device_id STRING, payload VARIANT). Design a production-ready, multi-tenant design ensuring strict data isolation and fast dashboards for last 30 days across hundreds of tenants. Outline RAP rules, masking, secure views or materialized aggregates, and a scalable data-sharing approach. Include concrete SQL patterns and validation steps?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Google","PayPal","Two Sigma"]},{"id":"q-2483","question":"Intermediate Snowflake-core: Ingest 200M rows daily into raw.sessions(user_id STRING, session_id STRING, started_at TIMESTAMP_NTZ, ended_at TIMESTAMP_NTZ, events VARIANT). Need per-user risk scores updated every 5 minutes and dashboards for (a) count of high-risk sessions in last 15 minutes, (b) average duration of high-risk sessions, (c) top 10 user cohorts by risk. Propose a production plan covering data model (raw vs curated), clustering, streaming vs materialized views, schema evolution, validation, and rollback strategy with exact SQL commands?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Airbnb","Amazon"]},{"id":"q-2522","question":"Snowflake-core intermediate: You're ingesting telemetry into Snowflake with raw.events(tenant_id STRING, user_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). In a multi-tenant SaaS, implement per-tenant latency metrics (p95 over last 30 min), dynamic masking of sensitive payload fields for dashboards, and fast per-user drilldown without scanning raw payloads. Propose a production plan: curated schema, partitioning, streaming vs materialized path, VARIANT schema evolution, validation, and rollback with exact commands?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Oracle","Robinhood"]},{"id":"q-2591","question":"You're designing a multi-tenant analytics layer in Snowflake. A single table raw.events stores events for all tenants (tenant_id, ts, payload). BI dashboards must be isolated per tenant and support sub-second queries for the last 7 days. Propose and implement a scalable access-control strategy that avoids per-tenant copies, considering growth to hundreds of tenants. Include how you'd implement Row Access Policies, a Secure View, and validation steps with concrete SQL commands?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Adobe","Discord","DoorDash"]},{"id":"q-2592","question":"You operate a multi-tenant Snowflake data lake: table raw.orders(tenant_id STRING, order_id STRING, amount NUMBER, credit_card VARIANT). Implement per-tenant data isolation using row access policies and a masking policy for credit_card so tenants see only last 4 digits. Provide exact SQL to create masking policy, row access policy, a secure view that enforces both policies, and grants. Include how you'd test with two tenants?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["LinkedIn","Robinhood"]},{"id":"q-2657","question":"Beginner Snowflake-core: daily load from staged.orders_stage into production.orders, dedup by order_id. Columns: order_id STRING, customer_id STRING, amount NUMBER, load_date DATE. Write a single MERGE UPSERT that keeps only the latest load_date per order_id. Include exact MERGE SQL and a quick verify query?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["IBM","Oracle","Plaid"]},{"id":"q-2692","question":"You're designing a beginner Snowflake data-curation task: given a raw table RAW.events(device_id string, event_ts timestamp_ntz, payload variant) ingesting 50M rows daily with a JSON payload containing fields event_type, region, and email (PII). Propose a concrete end-to-end approach to surface a curated table ANALYTICS.curated.events_daily with columns device_id, event_type, region, user_email where user_email is masked for external queries via a masking policy. Include exact SQL statements to (1) create the curated table, (2) extract fields, (3) define and apply masking policy on user_email, and (4) create a daily REFRESH TASK at 01:00 UTC that recomputes the curated table from the last 24 hours?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Cloudflare","Discord","NVIDIA"]},{"id":"q-2729","question":"Mid-level Snowflake-core: In a multi-tenant telemetry store, raw.events(tenant_id STRING, device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) feeds dashboards showing per-tenant events last 24h. Implement a Slowly Changing Dimension for tenants (tenants_dim) as Type 2 using Streams and Tasks, and a curated per-tenant 24h rollup without scanning payload. Provide exact SQL to (1) create SCD Type 2, (2) propagate changes, (3) build the 24h rollup, and (4) validation steps. Discuss trade-offs?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Discord","Tesla"]},{"id":"q-2752","question":"Snowflake-core intermediate: You manage a multi-tenant analytics layer. Raw data lives in `raw.transactions` and `raw.users`. Design an end-to-end plan to expose BI-ready data per-tenant with: (a) a curated layer, (b) masking policies for PII (email, phone, card), (c) a ROW ACCESS POLICY tied to a per-tenant session context, (d) a SECURE VIEW for BI access, (e) incremental refresh via STREAMS/TASKS, and (f) validation and rollback. Include exact SQL commands for the masking policy, the row access policy, and a sample curated secure view?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Amazon","Cloudflare","Microsoft"]},{"id":"q-2790","question":"Beginner Snowflake-core: daily ingest loads 1M rows into `staging.image_events` (image_id STRING, ts TIMESTAMP_NTZ, status STRING, size_bytes NUMBER). Propose a minimal ETL to populate a curated table `analytics.image_events` with columns `image_id STRING, day DATE, status STRING, size_kb NUMBER`, where day = DATE(ts) and size_kb = size_bytes/1024. Provide exact SQL for table creation and a single MERGE to upsert the batch. Include how you'd validate row counts and data types?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Citadel","Cloudflare","Robinhood"]},{"id":"q-2801","question":"Intermediate Snowflake-core: You’re building a per-tenant feature-flag telemetry store. Ingest ~5M events per day into raw.flags(tenant_id STRING, feature_id STRING, user_id STRING, eval_ts TIMESTAMP_NTZ, value VARIANT). Implement strict tenant isolation, per-tenant latency metrics for flag evaluation (p95 over last 5 minutes), and avoid scanning payloads for dashboards. Propose a production plan detailing schema, streams, tasks, RBAC + masking, auditing, validation, and rollback?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Bloomberg","NVIDIA","Robinhood"]},{"id":"q-2836","question":"Design a Snowflake data-sharing solution for partner analytics. In RAW.partner_logs(partner_id STRING, ts TIMESTAMP_NTZ, payload VARIANT), expose per-partner curated analytics while preventing data leakage. Outline architecture using Streams, Tasks, and a curated view; implement masking for PII, per-partner access controls, and quotas. Provide concrete SQL for creating the share, roles, masking policy, and a test plan to verify isolation?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Microsoft","NVIDIA","Twitter"]},{"id":"q-2870","question":"You have a table raw.clicks(user_id STRING, click_ts TIMESTAMP_NTZ, price NUMBER) ingesting 20M rows/day. Build a daily aggregation stored in dashboard.daily_user_engagement(user_id STRING, day DATE, total_spend NUMBER, clicks INT) and refresh it every day at 02:00 UTC using a Snowflake Stream and Task. What are the exact steps and SQL commands you would run to implement this end-to-end?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Coinbase","Meta"]},{"id":"q-2998","question":"You operate a Snowflake data warehouse for a multi-tenant SaaS product ingesting telemetry in a table raw.events(tenant_id STRING, device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). Design a production plan to enforce per-tenant isolation in dashboards while still enabling cross-tenant analytics like top tenants by event count in the last 24 hours. Specify concrete SQL commands for row access policies, secure views, masking, and testing?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Apple","Goldman Sachs","Instacart"]},{"id":"q-3022","question":"Design an end-to-end Snowflake architecture to securely share a telemetry store across regional accounts while enabling automated per-device anomaly scoring every 5 minutes. Specify data modeling (raw vs curated), clustering strategy, use of streams and tasks, cross-region data sharing with a reader account, row-level access controls, and a practical validation plan for correctness, latency, and cost?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Google","Slack","Snowflake"]},{"id":"q-3091","question":"You're running a Snowflake-based analytics platform used by multiple squads. Design a cost-governance system that enforces per-squad monthly credit budgets, auto-suspends idle warehouses, and dynamically prioritizes critical ETL workloads during business hours using RESOURCE_MONITORS, ACCOUNT_USAGE, and a quotas table. Provide concrete SQL for monitors, a JavaScript stored procedure to estimate remaining credits, a daily TASK to refresh quotas, and Slack alert examples?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Google","Twitter"]},{"id":"q-3170","question":"In a Snowflake data lake for an online retailer, raw.orders(order_id STRING, customer_id STRING, order_ts TIMESTAMP_NTZ, items VARIANT, total DECIMAL) ingested 80M rows daily via Snowpipe. Propose a production plan for end-to-end data quality: non-null and unique checks, item totals reconcile to total, automated quarantine for failures, streaming + batch QA, and exact SQL snippets plus rollback/alert procedures?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["IBM","Oracle","Square"]},{"id":"q-3236","question":"Intermediate Snowflake-core: You run a ML feature store for a fintech app. Ingest 1B feature rows daily into RAW.FEATURES(feature_id STRING, feature_value VARIANT, feature_ts TIMESTAMP_NTZ, customer_id STRING, model_id STRING). Build CURATED.FEATURES with versioning and a SERVING.FEATURES path for online inference with sub-50 ms latency. Provide concrete SQL for ingestion, version upserts (MERGE), MV/secure views, per-model RBAC, and data quality tests, plus a rollback plan?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["MongoDB","Square","Stripe"]},{"id":"q-3370","question":"Global Snowflake analytics pipeline: Ingest 100M events/day into raw.events(account_id STRING, user_id STRING, event_type STRING, ts TIMESTAMP_NTZ, metadata VARIANT). Design for (a) last-15-minute per-segment user counts from metadata, (b) cohort LV by region, (c) share a governed subset with a partner via secure data sharing while masking PII. Include data model, ingestion, MV vs table, access controls, validation, and rollback. End with a practical SQL snippet?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Cloudflare","Microsoft","Twitter"]},{"id":"q-3386","question":"Design an idempotent ingest pipeline for Snowflake where JSON events land in S3 and are loaded via Snowpipe into RAW.STAGING_EVENTS(event_id, device_id, ts, payload); ensure duplicates are prevented and late/out-of-order data is handled; provide schema, streams, MERGE logic, a TASK, and validation steps with exact commands?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Amazon","Google","Two Sigma"]},{"id":"q-3424","question":"In a Snowflake analytics layer ingesting raw.events(tenant_id STRING, device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT), implement per-tenant data isolation and PII masking at query time. Outline a scalable approach using ROW ACCESS POLICY and MASKING POLICY, surface a tenant-scoped view, and provide exact SQL to bootstrap the policies, create the view, and validate separation for two roles?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Microsoft","Netflix","Uber"]},{"id":"q-3496","question":"You're adding two new fields to a Snowflake telemetry table raw.events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT): source STRING and ingested_by STRING. Propose a zero-downtime schema evolution strategy using zero-copy cloning, time travel, and a view-based interface. Provide exact SQL to (1) evolve the schema safely, (2) create a curated view for BI that remains backward-compatible, (3) perform a safe backfill and validate with test queries, and (4) outline rollback steps?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Airbnb","Coinbase","OpenAI"]},{"id":"q-3559","question":"Beginner Snowflake-core: You have prod_db.sales with tables customers and orders. A dev sandbox is needed without copying data. Using zero-copy cloning, outline exact steps and provide commands to (1) clone prod_db.sales into dev_db.sales_clone, (2) restrict writes on prod, (3) grant only read access to the clone for role DEV, (4) set data retention on the clone to 7 days, and (5) verify isolation so changes in the clone do not affect prod. What commands would you run and why?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Cloudflare","LinkedIn","Tesla"]},{"id":"q-3657","question":"Scenario: In Snowflake you ingest 100M events/day into raw.telemetry(tenant_id STRING, device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). You must generate near real-time alerts for anomalous temperature spikes per device with <= 2 minutes latency, while preserving per-tenant isolation for dashboards. Design a pipeline using Streams and Tasks (and decide on Snowpipe vs auto-ingest), specify idempotent processing, dead-letter handling, monitoring, and provide concrete SQL patterns for alert generation and backfill?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Discord","Tesla"]},{"id":"q-3718","question":"Design a near-real-time anomaly detection pipeline in Snowflake for telemetry data: raw.events(tenant_id, device_id, ts, value). Propose data model, a streaming ingestion, and a scheduled task that recomputes per-tenant z-scores over the last 5 minutes every minute, flags |z|>3, and writes to curated.anomalies; expose a secure view for dashboards with per-tenant filtering and discuss latency, cost, and failure handling?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Amazon","Discord","MongoDB"]},{"id":"q-3791","question":"You operate a Snowflake data warehouse for a multi-tenant platform ingesting 100M events/day into raw.events(tenant_id STRING, device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). Partners require a secure data share of a masked subset. Design a production plan that covers: (a) handling payload schema drift with semi-structured data, (b) per-tenant isolation for dashboards, (c) a rolling 24h MV for event type counts, (d) secure, revocable data sharing with masking, (e) validation, rollback, and alerting. Include concrete SQL for masking rules and a sample secure view?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Hugging Face","Instacart","NVIDIA"]},{"id":"q-3911","question":"Beginner Snowflake-core: You need a dev sandbox for prod_sales that mirrors production but excludes PII and minimizes cost. Outline a practical plan using (1) cloning the schema, (2) masking policies for SSN and email, (3) a read-only data share to the sandbox, and (4) a nightly refresh. Include the exact SQL commands you would run for each step?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Adobe","Databricks","Instacart"]},{"id":"q-3938","question":"Beginner Snowflake-core: Onboard a new partner to run analytics on sanitized sales data without touching PII or prod tables. Design a minimal reproducible setup using (1) zero-copy cloning for a dev sandbox, (2) a masking policy on email and phone in a curated view, (3) a secure view enforcing partner_role access, (4) a read-only data share to the partner’s account, and (5) a daily refresh pipeline with Snowflake Tasks. Provide exact SQL for each step?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Google","Hugging Face","Salesforce"]},{"id":"q-4079","question":"Beginner Snowflake-core: daily ingestion of 1M rows into RAW_TRANSACTIONS (id STRING, user_id STRING, amount NUMBER, status STRING, txn_date DATE). Implement a data quality guard using a STREAM on RAW_TRANSACTIONS and a TASK every 6 hours to check (1) non-null critical fields, (2) amount >= 0, (3) user_id exists in DIM_USERS. On failures, log into QA_LOG (txn_id, issue, ts). Provide exact SQL for: a) create stream on RAW_TRANSACTIONS, b) create QA_LOG, c) create a stored procedure or SQL block that runs the checks, d) create the task?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Discord","Oracle","Tesla"]},{"id":"q-4182","question":"Intermediate Snowflake-core: Build a multi-tenant analytics pipeline for a SaaS platform where each customer publishes events into raw.events (tenant_id STRING, user_id STRING, event_type STRING, ts TIMESTAMP_NTZ, payload VARIANT). Implement per-tenant isolation with Row Access Policies, create curated views per tenant, enable masked sharing to a partner via secure data sharing, and set up a 5-minute refresh using Snowflake Tasks. Provide exact SQL for: (a) a tenancy-aware Row Access Policy, (b) a curated per-tenant aggregate view, (c) provisioning a secure data share to a partner, and (d) a 5-minute refresh Task?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Apple","Cloudflare","Meta"]},{"id":"q-4218","question":"Beginner Snowflake-core: you need a nightly data-quality run for RAW.SALES (sale_id STRING, amount NUMBER, sale_ts TIMESTAMP_NTZ, customer_email STRING). Validate: sale_id IS NOT NULL, amount > 0, sale_ts within last 24h, and customer_email looks like an email. Outline a practical plan using (a) a lightweight QC query, (b) a TASK to run nightly, (c) a QA.SALES_QC results table, and (d) a simple alert when any check fails. Include exact SQL commands for each step?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Goldman Sachs","Scale Ai"]},{"id":"q-4337","question":"Snowflake-core: Ingest 1B events/day into RAW.events(sensor_id STRING, metric STRING, value FLOAT, ts TIMESTAMP_NTZ). Build an hourly rollup DEVICE_HOURLY(sensor_id STRING, metric STRING, hour_ts TIMESTAMP_NTZ, avg_value FLOAT, max_value FLOAT, min_value FLOAT) using streams and tasks. Address late data (up to 2 hours), out-of-order events, and schema evolution (add battery_level STRING) without downtime. Provide exact SQL for the stream, the hourly task, the rollup (MV or table), a rollback strategy using TIME_TRAVEL, validation checks, and a simple alert. End with a concrete health-check query?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["LinkedIn","Meta","Tesla"]},{"id":"q-4392","question":"Design and implement a multi-tenant access pattern in Snowflake for analytics on RAW.events(tenant_id STRING, user_id STRING, event_time TIMESTAMP_NTZ, payload VARIANT). Each tenant must only see their own data in BI dashboards. Provide concrete SQL for: (a) a ROW ACCESS POLICY on RAW.events using a session-context tenant_id_ctx, (b) a SECURE VIEW CURATED.events that enforces the policy, (c) a provisioning script to grant access when a new tenant is added, (d) a validation test showing TenantA cannot read TenantB data. Include exact commands and sample outputs?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Slack","Snap"]},{"id":"q-4474","question":"Beginner Snowflake-core: You have RAW.SALES(sale_id STRING, amount NUMBER, sale_ts TIMESTAMP_NTZ, customer_email STRING, region STRING) shared for two tenants. Outline exact SQL to (1) create a masking policy on customer_email that returns NULL for TENANT_B_ROLE, (2) create a row access policy on region that filters REGION by role (TENANT_A_ROLE sees US, TENANT_B_ROLE sees EU), (3) create a secure view SANDBOX.TENANT_A_SAFE_SALES exposing only non-PII fields, and (4) grant access to TENANT_A_ROLE with SELECT on the view. Include the exact SQL for each step?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Cloudflare","Databricks","MongoDB"]},{"id":"q-4727","question":"Intermediate Snowflake-core: Ingest transactions from three sources into RAW.SALES_TXN (txn_id STRING, amount NUMBER, currency STRING, ts TIMESTAMP_NTZ, customer_id STRING). Build a CURATED.SALES_TXN that (a) converts amount to USD using a daily rates table RATES(currency, rate_to_usd, effective_date), (b) flags anomalies (negative or null amount, unknown currency, ts older than 30 days, missing txn_id), (c) provides a secure, read-only view for a partner via a secure data share, and (d) includes a nightly rollback plan for any QC failure. Include exact SQL snippets for ingestion, conversion, QC checks, view creation, and rollback?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Google","Two Sigma"]},{"id":"q-868","question":"Design a cross-account data-sharing solution in Snowflake for a multinational fintech requiring regional affiliates to access a shared dataset containing PII. How would you implement Secure Data Sharing, dynamic data masking, region-specific RBAC, and auditable access, while enabling updates to masking policies without breaking consumer queries?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Coinbase","Microsoft"]},{"id":"q-981","question":"How would you configure Snowflake so regional analysts can run ad-hoc queries on a shared dataset while ensuring isolation, predictable performance, and cost control? Include concrete settings for: (a) warehouse topology (min/max clusters, auto-suspend/resume), (b) RBAC (roles, grant scopes on databases/schemas/tables), (c) cost governance (resource monitors and credit caps), (d) a sample GRANT script giving REGIONAL_ANALYST access to only SALES and EVENTS schemas, and (e) auditing and reproducibility considerations?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Anthropic","Cloudflare","Robinhood"]},{"id":"q-305","question":"How would you determine the required capacity for a service expecting 10x traffic growth during a product launch?","channel":"sre","subChannel":"capacity-planning","difficulty":"beginner","tags":["forecasting","autoscaling","load-testing"],"companies":["Amazon","Google","Meta"]},{"id":"sr-131","question":"You're managing a microservices platform with 50 services. Service A has a 95th percentile latency of 200ms and handles 10,000 RPS. It calls Service B (50ms, 5,000 RPS) and Service C (100ms, 3,000 RPS). During Black Friday, you expect 5x traffic. Service A's CPU utilization is currently 60%, memory at 70%. How do you plan capacity to maintain <500ms 95th percentile end-to-end latency?","channel":"sre","subChannel":"capacity-planning","difficulty":"advanced","tags":["capacity","scaling"],"companies":["Amazon","Google","Meta","Microsoft","Uber"]},{"id":"sr-143","question":"Your web application currently handles 1000 requests per minute during peak hours. Each request takes an average of 200ms to process. If you expect traffic to double in the next 6 months, how many additional server instances do you need if each server can handle 50 concurrent requests?","channel":"sre","subChannel":"capacity-planning","difficulty":"beginner","tags":["capacity","scaling"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"sr-149","question":"You're designing capacity planning for a microservices platform handling 10M daily active users. Each user generates 50 API calls/day with 80% during peak hours. How many instances do you need for each service?","channel":"sre","subChannel":"capacity-planning","difficulty":"advanced","tags":["capacity","scaling"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-218","question":"How would you design a chaos engineering experiment to test database failover while maintaining transaction consistency across a microservices architecture?","channel":"sre","subChannel":"chaos-engineering","difficulty":"advanced","tags":["chaos-monkey","litmus","gremlin"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-399","question":"You're using Litmus Chaos to test a microservices application. One of your chaos experiments is causing unexpected cascading failures across multiple services. How would you debug this issue and what specific steps would you take to limit the blast radius?","channel":"sre","subChannel":"chaos-engineering","difficulty":"intermediate","tags":["chaos-monkey","litmus","gremlin"],"companies":["Airtable","Fortinet","Tempus"]},{"id":"sr-146","question":"Design a chaos engineering experiment to test the resilience of a microservices-based e-commerce platform during a database partition event. How would you ensure the experiment doesn't cause customer data loss while still providing meaningful insights?","channel":"sre","subChannel":"chaos-engineering","difficulty":"advanced","tags":["chaos","resilience"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"sr-150","question":"You're implementing chaos engineering for a distributed payment system processing $10M daily transactions. Design a chaos experiment to test resilience against Byzantine failures where 30% of payment validation nodes provide conflicting consensus results. How would you ensure financial accuracy while testing system behavior under adversarial conditions?","channel":"sre","subChannel":"chaos-engineering","difficulty":"advanced","tags":["chaos","resilience"],"companies":["Amazon","Coinbase","Microsoft","Netflix","Stripe"]},{"id":"sr-153","question":"You're implementing chaos engineering for a microservices architecture. Your payment service has a 99.9% SLA. During a chaos experiment, you inject 500ms latency into 20% of requests to the database. The service starts timing out after 1 second. What's the most critical metric to monitor first, and what would indicate the experiment should be stopped immediately?","channel":"sre","subChannel":"chaos-engineering","difficulty":"intermediate","tags":["chaos","resilience"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-477","question":"You're running a chaos experiment in production. How do you determine the blast radius and ensure you don't impact customer experience while still getting meaningful failure data?","channel":"sre","subChannel":"general","difficulty":"intermediate","tags":["sre"],"companies":["Goldman Sachs","Google","LinkedIn"]},{"id":"q-506","question":"You're on-call and receive an alert that a critical service is experiencing 99% error rate. What are your immediate first steps and how do you approach incident response?","channel":"sre","subChannel":"general","difficulty":"beginner","tags":["sre"],"companies":["IBM","Scale Ai"]},{"id":"q-535","question":"You're an SRE at Netflix and notice your CDN cache hit ratio dropped from 95% to 70% during peak hours. How would you diagnose and resolve this issue?","channel":"sre","subChannel":"general","difficulty":"advanced","tags":["sre"],"companies":["Discord","Netflix","Snowflake"]},{"id":"q-590","question":"How would you design a canary deployment strategy for a microservice handling 10K RPS with 99.99% SLA requirements?","channel":"sre","subChannel":"general","difficulty":"advanced","tags":["sre"],"companies":["Bloomberg","Google","Twitter"]},{"id":"gh-65","question":"What is Mean Time to Recovery (MTTR), how do you calculate it, and what specific strategies would you implement to optimize it for SRE teams?","channel":"sre","subChannel":"incident-management","difficulty":"beginner","tags":["metrics","kpi"],"companies":null},{"id":"gh-97","question":"How do you design incident response playbooks that balance automation with human oversight for SRE teams?","channel":"sre","subChannel":"incident-management","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-262","question":"Describe a critical production outage you managed during peak traffic. How did you coordinate the response, communicate with stakeholders, and implement both immediate fixes and long-term preventive measures?","channel":"sre","subChannel":"incident-management","difficulty":"advanced","tags":["situation","task","action","result"],"companies":["Amazon","Cloudflare","Google","Microsoft","Netflix","Oracle"]},{"id":"q-319","question":"You are on-call and receive a high-severity PagerDuty alert for a production service degradation. What are your immediate steps and how do you coordinate with the team?","channel":"sre","subChannel":"incident-management","difficulty":"advanced","tags":["pagerduty","runbooks","postmortem"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Servicenow","Stripe","Wipro"]},{"id":"q-367","question":"You're managing a multi-cluster GitOps setup at Warner Bros with 50+ microservices. ArgoCD suddenly starts showing 'Unknown' sync status for critical services during peak traffic. How would you diagnose and resolve this production incident while ensuring zero downtime?","channel":"sre","subChannel":"incident-management","difficulty":"advanced","tags":["argocd","flux","declarative"],"companies":["Amazon","Google","Hashicorp","LinkedIn","Microsoft","Netflix","Salesforce","Warner Bros"]},{"id":"q-368","question":"You're on-call at Tesla when the vehicle telemetry pipeline shows 95% packet loss. Your PagerDuty alert shows the Kafka cluster is healthy, but the downstream processing service is crashing. What's your immediate triage process and how do you determine if this is a network, application, or data format issue?","channel":"sre","subChannel":"incident-management","difficulty":"intermediate","tags":["pagerduty","runbooks","postmortem"],"companies":["Discord","Tesla","Zscaler"]},{"id":"sr-126","question":"How would you design and implement a comprehensive blameless postmortem process that includes incident response coordination, root cause analysis using 5 Whys and fishbone diagrams, and actionable improvement tracking?","channel":"sre","subChannel":"incident-management","difficulty":"advanced","tags":["incident","postmortem"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"sr-142","question":"You receive a PagerDuty alert at 3 AM: 'Production API is returning 500 errors'. What are your first three steps in handling this incident, and what specific tools and metrics would you use to assess impact and coordinate response?","channel":"sre","subChannel":"incident-management","difficulty":"beginner","tags":["incident","postmortem"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"gh-19","question":"What is monitoring in DevOps and how does it differ from observability?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["observability","monitoring","logging"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-20","question":"Design a comprehensive logging architecture using the ELK Stack with File Beats for a high-traffic e-commerce platform processing 50,000 requests per minute. How would you ensure data integrity and real-time monitoring?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["observability","monitoring","logging"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"gh-21","question":"How does Prometheus implement a pull-based monitoring system, and what are the key components in its architecture?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["observability","monitoring","logging"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-22","question":"What is Grafana and how does it integrate with different data sources for monitoring and visualization?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["observability","monitoring","logging"],"companies":["Airbnb","LinkedIn","Microsoft","Stripe","Uber"]},{"id":"gh-23","question":"Explain the key differences between monitoring and logging in DevOps, and when would you use each?","channel":"sre","subChannel":"observability","difficulty":"intermediate","tags":["observability","monitoring","logging"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-61","question":"What are Service Level Indicators (SLIs) and how do they differ from SLOs?","channel":"sre","subChannel":"observability","difficulty":"intermediate","tags":["sre","reliability"],"companies":["Amazon","Google","Meta"]},{"id":"gh-77","question":"How would you design a comprehensive monitoring strategy for a distributed system, including tool selection, SLI/SLO definition, and alerting implementation?","channel":"sre","subChannel":"observability","difficulty":"intermediate","tags":["monitoring","infra"],"companies":["Amazon","Cloudflare","Google","Meta","Microsoft","Netflix"]},{"id":"gh-78","question":"How would you design a comprehensive monitoring strategy for a production microservices system, including SLI/SLO definitions and alerting thresholds?","channel":"sre","subChannel":"observability","difficulty":"intermediate","tags":["monitoring","infra"],"companies":["Amazon","Cloudflare","Google","Microsoft","Netflix","Stripe"]},{"id":"gh-79","question":"What is Application Performance Monitoring?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["monitoring","infra"],"companies":["Amazon","Datadog","Google","Microsoft","Splunk"]},{"id":"gh-95","question":"What is a Service Level Indicator (SLI)?","channel":"sre","subChannel":"observability","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Citadel","Goldman Sachs","Google","Microsoft"]},{"id":"gh-99","question":"What is Tracing in Observability?","channel":"sre","subChannel":"observability","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Goldman Sachs","Netflix","Stripe","Uber"]},{"id":"q-192","question":"How would you implement OpenTelemetry instrumentation to capture RED metrics (Rate, Errors, Duration) for a microservice using Prometheus as the backend?","channel":"sre","subChannel":"observability","difficulty":"intermediate","tags":["prometheus","grafana","opentelemetry"],"companies":["Chronosphere","Datadog","Grafana Labs","Microsoft","New Relic"]},{"id":"q-244","question":"What is the difference between metrics, logs, and traces in observability, and how do OpenTelemetry collectors correlate them?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["prometheus","grafana","opentelemetry"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-345","question":"You're monitoring a streaming service that suddenly experiences 500 errors. How would you use Prometheus and Grafana to quickly identify the root cause?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["prometheus","grafana","opentelemetry"],"companies":["Amazon","Cloudflare","Google","Infosys","Microsoft","Netflix","Warner Bros"]},{"id":"q-382","question":"You're the SRE lead for a rocket launch telemetry system. Prometheus is showing high memory usage on your OpenTelemetry collector during peak launch events, causing metric loss. How would you architect a solution to handle 100K+ metrics/second while ensuring zero data loss during critical launch windows?","channel":"sre","subChannel":"observability","difficulty":"advanced","tags":["prometheus","grafana","opentelemetry"],"companies":["Notion","OpenAI","SpaceX"]},{"id":"q-391","question":"You're an SRE at HashiCorp and your Prometheus alerts are firing every 5 minutes due to a memory leak in a Go service using OpenTelemetry. How would you debug this using the observability stack?","channel":"sre","subChannel":"observability","difficulty":"intermediate","tags":["prometheus","grafana","opentelemetry"],"companies":["Hashicorp","Instacart","Western Digital"]},{"id":"q-411","question":"You're on-call and receive an alert: 'API response time increased from 200ms to 2s over the last 5 minutes'. Using Prometheus, Grafana, and OpenTelemetry, how would you diagnose this issue?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["prometheus","grafana","opentelemetry"],"companies":["Amazon","Cloudflare","Google","Intel","Microsoft","Netflix","Palo Alto Networks","Stripe"]},{"id":"sr-124","question":"How would you implement the four golden signals of monitoring in a production microservices architecture, and what trade-offs would you consider when designing your observability strategy?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["metrics","monitoring"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"sr-133","question":"How do you implement the three pillars of observability (logs, metrics, traces) in a microservices architecture, and what are the key trade-offs between them?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["metrics","monitoring"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Snowflake"]},{"id":"sr-155","question":"What is the difference between metrics, logs, and traces in observability, and when would you use each?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["metrics","monitoring"],"companies":["Amazon","Bloomberg","Datadog","Goldman Sachs","Google","Microsoft","Netflix","New Relic","Splunk","Uber"]},{"id":"sre-1","question":"How would you design and implement SRE monitoring with SLIs, SLOs, and SLAs for a high-traffic e-commerce platform? What specific metrics would you track and how would they drive engineering decisions?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["metrics","policy","definitions","observability"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"gh-103","question":"What is a Self-Healing System and how does it work in distributed architectures?","channel":"sre","subChannel":"reliability","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-35","question":"Design a backup and disaster recovery strategy for a high-availability e-commerce platform processing 10,000 transactions/minute with 99.99% uptime SLA. What are your RTO/RPO targets and how would you implement multi-region failover?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["backup","dr"],"companies":["Amazon","Google","Meta"]},{"id":"gh-59","question":"What is Site Reliability Engineering and how does it differ from traditional operations?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["sre","reliability"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-60","question":"How do you design and implement Service Level Objectives (SLOs) with proper SLI definitions, error budgets, and monitoring strategies?","channel":"sre","subChannel":"reliability","difficulty":"intermediate","tags":["sre","reliability"],"companies":null},{"id":"gh-62","question":"What is an Error Budget and how does it impact SRE decision-making?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["sre","reliability"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-63","question":"What is Toil in Site Reliability Engineering and how should SREs approach managing it?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["sre","reliability"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-93","question":"How do you implement and monitor Service Level Agreements (SLAs) in a distributed system, including specific metrics, tools, and alerting strategies?","channel":"sre","subChannel":"reliability","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Cloudflare","Datadog","Google","Microsoft","Netflix"]},{"id":"gh-94","question":"What is a Service Level Objective (SLO) and how does it differ from an SLA?","channel":"sre","subChannel":"reliability","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-270","question":"Your microservice has a 99.9% availability SLO over 30 days with a 1-hour burn rate alert threshold. If you experience a 10-minute outage at 10% traffic, how much error budget remains and what's the burn rate? Should you alert?","channel":"sre","subChannel":"reliability","difficulty":"intermediate","tags":["slo","sli","error-budget"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"q-290","question":"Explain the relationship between SLIs, SLOs, and SLAs in reliability engineering, including how you would implement error budgets and monitor burn rate?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["slo","sli","error-budget"],"companies":["Amazon","Apple","Cloudflare","Google","Microsoft","Netflix"]},{"id":"q-333","question":"Your SLO for API response time is 99.9% with a 500ms threshold. You're at 99.7% and the error budget is exhausted. The product team wants to ship a new feature that will increase traffic by 20%. How do you handle this situation?","channel":"sre","subChannel":"reliability","difficulty":"intermediate","tags":["slo","sli","error-budget"],"companies":["Atlassian","Databricks","Unity"]},{"id":"q-355","question":"Your SLO is 99.9% for API latency (p95 < 200ms). You're at 99.85% and have 15% error budget remaining. A critical security patch requires 30% traffic shift to new version with unknown latency characteristics. How do you proceed while maintaining service reliability?","channel":"sre","subChannel":"reliability","difficulty":"advanced","tags":["slo","sli","error-budget"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"sr-130","question":"Your web service has an SLO of 99.9% availability over 30 days. You've had 3 outages: 45 minutes, 20 minutes, and 15 minutes. What's your current availability, error budget status, and what immediate actions would you take to prevent SLO breach?","channel":"sre","subChannel":"reliability","difficulty":"intermediate","tags":["slo","sli","error-budget"],"companies":["Amazon","Cloudflare","Google","Meta","Microsoft","Netflix"]},{"id":"sr-147","question":"Your distributed system has 5 microservices with the following failure rates: Service A (0.1%), Service B (0.2%), Service C (0.05%), Service D (0.15%), Service E (0.25%). Design a fault-tolerant architecture to achieve 99.5% SLO with specific implementation details?","channel":"sre","subChannel":"reliability","difficulty":"advanced","tags":["reliability","incident"],"companies":["Amazon","Databricks","Google","Meta","Microsoft","Netflix"]},{"id":"sr-154","question":"Your API serves 10M requests/day with a 99.9% availability SLO and 30-day error budget. After a 4-hour outage affecting 100% of traffic, calculate the remaining error budget and explain how you'd handle post-incident SLO adjustments, error budget recovery strategies, and burn rate monitoring?","channel":"sre","subChannel":"reliability","difficulty":"intermediate","tags":["slo","sli","error-budget"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"sr-169","question":"Your API service has an SLO of 99.9% availability. If you have 5 incidents this month with downtimes of 10min, 5min, 15min, 8min, and 12min, did you meet your SLO and what's the remaining error budget for the rest of the month?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["slo","sli","error-budget"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"sre-2","question":"How do you calculate and manage Error Budgets for a microservices architecture with multiple SLOs, and what strategies do you use for burn rate monitoring and recovery?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["management","concept","risk"],"companies":["Adobe","Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-45","question":"How do rate limiting algorithms like Token Bucket and Leaky Bucket control API request flow and what are their trade-offs?","channel":"system-design","subChannel":"api-design","difficulty":"beginner","tags":["api","service-mesh"],"companies":["Amazon","Google","Microsoft","Stripe","Uber"]},{"id":"q-406","question":"Design a REST API for a cryptocurrency exchange that handles 100,000 trades per day with real-time price updates. How would you ensure data consistency and handle high-frequency trading requests?","channel":"system-design","subChannel":"api-design","difficulty":"beginner","tags":["api","rest","grpc","graphql"],"companies":["Coinbase","Oracle","Twilio"]},{"id":"q-424","question":"Design a RESTful API for a hotel booking system. What endpoints would you create and how would you handle concurrent bookings for the same room?","channel":"system-design","subChannel":"api-design","difficulty":"beginner","tags":["api","rest","grpc","graphql"],"companies":["Airbnb","Amazon","Booking.com","Google","Microsoft"]},{"id":"q-507","question":"Design a unified API gateway for Databricks that supports REST, gRPC, and GraphQL with protocol translation, rate limiting, and unified authentication?","channel":"system-design","subChannel":"api-design","difficulty":"advanced","tags":["api","rest","grpc","graphql"],"companies":["Databricks","IBM"]},{"id":"sy-151","question":"Design a rate limiting API for a multi-tenant SaaS platform where different customers have different rate limits (free: 100 req/hour, premium: 1000 req/hour, enterprise: custom). How would you design the API endpoints and data structures to efficiently track and enforce these limits?","channel":"system-design","subChannel":"api-design","difficulty":"intermediate","tags":["api","rest"],"companies":["Amazon","Google","Microsoft","Stripe","Uber"]},{"id":"q-604","question":"Design a rate limiting system for a public API that can handle 10,000 requests per second with different rate limits for free and paid tiers (100 requests/minute for free, 1000 requests/minute for paid). How would you implement this to ensure fairness and prevent abuse?","channel":"system-design","subChannel":"api-rate-limiting","difficulty":"intermediate","tags":["rate-limiting","api-design","distributed-systems","redis","token-bucket"],"companies":["Twitter","Stripe","GitHub","Google","Amazon"]},{"id":"q-1622","question":"Design a distributed caching system for a high-traffic e-commerce platform that handles 10,000 requests per second with 99.9% availability. How would you handle cache invalidation, consistency, and failover?","channel":"system-design","subChannel":"cache-architecture","difficulty":"intermediate","tags":["distributed-caching","redis","consistency","scalability","high-availability"],"companies":["Amazon","Google","Meta","Netflix","Uber","Airbnb"]},{"id":"q-597","question":"Design a distributed caching system for a global e-commerce platform that handles 100,000 requests per second with 99.9% availability. How would you handle cache consistency, invalidation strategies, and failover across multiple geographic regions?","channel":"system-design","subChannel":"cache-architecture","difficulty":"advanced","tags":["distributed-systems","caching","redis","high-availability","consistency","scalability"],"companies":["Amazon","Google","Meta","Netflix","Uber","Airbnb","Spotify","Twitter"]},{"id":"q-603","question":"Design a distributed caching system for a global e-commerce platform that serves 10 million daily active users. The system must handle product catalog caching with 99.99% availability, sub-millisecond latency for hot items, and cache consistency across multiple data centers.","channel":"system-design","subChannel":"cache-architecture","difficulty":"advanced","tags":["distributed-systems","caching","redis","high-availability","consistency"],"companies":["Amazon","Netflix","Google","Meta","Microsoft","Uber","Airbnb"]},{"id":"q-621","question":"Design a distributed caching system for a social media platform that needs to handle 10 million active users with 99.9% availability. How would you ensure cache consistency across multiple data centers while minimizing latency?","channel":"system-design","subChannel":"cache-architecture","difficulty":"intermediate","tags":["distributed-systems","caching","consistency","scalability","high-availability"],"companies":["Facebook","Twitter","LinkedIn","Reddit","Instagram"]},{"id":"q-634","question":"Design a distributed caching system for a global e-commerce platform that serves 10 million requests per second with 99.99% availability. The system must handle cache consistency across multiple data centers, support cache warming for popular products, and provide graceful degradation when cache nodes fail.","channel":"system-design","subChannel":"cache-architecture","difficulty":"advanced","tags":["distributed-systems","caching","consistency","high-availability","scalability"],"companies":["Amazon","Netflix","Google","Meta","Microsoft","Uber"]},{"id":"q-169","question":"Design a caching strategy for a high-traffic e-commerce platform handling 10,000 RPS. Compare cache-aside vs read-through patterns, including write-through considerations, consistency guarantees, and performance implications. When would you choose each pattern and what are the trade-offs?","channel":"system-design","subChannel":"caching","difficulty":"beginner","tags":["cache","redis"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-213","question":"Design a multi-tier caching strategy for a 99.9% availability e-commerce platform handling 10M requests/day with 100ms P99 latency. How would you implement cache warming, invalidation, and fallback mechanisms?","channel":"system-design","subChannel":"caching","difficulty":"advanced","tags":["cache","redis","memcached","cdn"],"companies":null},{"id":"q-231","question":"How would you design a multi-region CDN cache purging system that guarantees content propagation within 5 seconds while handling 10,000 concurrent invalidations per second?","channel":"system-design","subChannel":"caching","difficulty":"intermediate","tags":["edge","caching","purging"],"companies":["Amazon","Cloudflare","Google","Meta","Microsoft","Netflix"]},{"id":"q-299","question":"How would you design a caching layer for a high-traffic e-commerce website?","channel":"system-design","subChannel":"caching","difficulty":"beginner","tags":["cache","redis","memcached","cdn"],"companies":["Amazon","Google","Meta"]},{"id":"q-392","question":"Design a distributed caching layer for Fortinet's threat intelligence system that serves 50M security devices with real-time malware signatures and threat data. How would you ensure cache consistency across global edge locations while maintaining sub-50ms response times?","channel":"system-design","subChannel":"caching","difficulty":"advanced","tags":["cache","redis","memcached","cdn"],"companies":["Fortinet","Microsoft","Tesla"]},{"id":"q-417","question":"Design a distributed caching strategy for a global e-commerce platform handling 10M daily users with frequent price/inventory updates. How would you ensure cache consistency, handle invalidation, and optimize performance across regions?","channel":"system-design","subChannel":"caching","difficulty":"intermediate","tags":["cache","redis","memcached","cdn"],"companies":null},{"id":"q-441","question":"Design a distributed caching layer for a social media feed serving 10M DAU with 99.9% availability. How would you handle cache invalidation across multiple data centers?","channel":"system-design","subChannel":"caching","difficulty":"advanced","tags":["cache","redis","memcached","cdn"],"companies":["Apple","LinkedIn","Two Sigma"]},{"id":"q-478","question":"Design a caching layer for a product catalog API serving 10K requests/second with 100K products. How would you handle cache invalidation and ensure data consistency?","channel":"system-design","subChannel":"caching","difficulty":"beginner","tags":["cache","redis","memcached","cdn"],"companies":["Instacart","Plaid"]},{"id":"q-601","question":"Design a communication strategy for a microservices-based e-commerce platform where the Order Service needs to notify the Inventory Service, Payment Service, and Notification Service when a new order is placed. How would you handle communication failures and ensure data consistency?","channel":"system-design","subChannel":"distributed-communication","difficulty":"intermediate","tags":["microservices","event-driven","distributed-systems","message-brokers","saga-pattern"],"companies":["Amazon","Netflix","Uber","Spotify","Airbnb"]},{"id":"q-625","question":"Design a distributed rate limiter for a high-traffic API that can handle 10,000 requests per second with per-user, per-IP, and per-endpoint limits. How would you ensure accuracy and prevent race conditions across multiple servers?","channel":"system-design","subChannel":"distributed-rate-limiting","difficulty":"intermediate","tags":["rate-limiting","distributed-systems","api-gateway","redis","token-bucket"],"companies":["Google","Amazon","Meta","Microsoft","Netflix"]},{"id":"gh-43","question":"Design an API Gateway for a high-traffic e-commerce platform handling 10M daily requests. How would you implement rate limiting, circuit breakers, and service discovery while ensuring 99.9% availability and sub-100ms latency?","channel":"system-design","subChannel":"distributed-systems","difficulty":"intermediate","tags":["api","service-mesh"],"companies":null},{"id":"q-189","question":"How would you design a distributed transaction system using the Saga pattern for an e-commerce platform handling inventory, payment, and shipping services, ensuring exactly-once processing and eventual consistency?","channel":"system-design","subChannel":"distributed-systems","difficulty":"beginner","tags":["saga","cqrs","event-sourcing"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-238","question":"How does Raft consensus algorithm ensure leader election and log replication in distributed systems?","channel":"system-design","subChannel":"distributed-systems","difficulty":"beginner","tags":["dist-sys","cap-theorem","consensus"],"companies":["Airbnb","Amazon","Apple","Cockroach Labs","Etcd","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-260","question":"Design a scalable Selenium Grid architecture to handle 10,000 concurrent test sessions with 99.9% uptime, ensuring zero memory leaks through automatic session lifecycle management, real-time monitoring, and graceful node failure recovery across multiple data centers?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["selenium","webdriver","grid"],"companies":["Amazon","Google","Microsoft","Netflix","Salesforce","Snowflake"]},{"id":"q-282","question":"Design an event sourcing system for a high-throughput e-commerce platform handling 10,000 orders/second with 99.99% availability. How would you implement the event store, handle versioning, and ensure event ordering while supporting replay and recovery?","channel":"system-design","subChannel":"distributed-systems","difficulty":"intermediate","tags":["event-sourcing","distributed-systems","architecture","cqrs","immutability"],"companies":["Amazon","Databricks","Microsoft","Netflix","Salesforce","Stripe"]},{"id":"q-313","question":"How would you design a distributed chat system like Slack that handles real-time messaging with strong consistency guarantees across global deployments?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["dist-sys","cap-theorem","consensus"],"companies":null},{"id":"q-352","question":"Design a distributed order processing system using Saga pattern for a high-frequency trading platform. How would you handle compensation transactions when a market data feed fails mid-transaction?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["saga","cqrs","event-sourcing"],"companies":["Citadel","Tcs","Western Digital"]},{"id":"q-435","question":"You're building a ride-sharing service similar to Lyft. How would you design the database architecture to handle 10,000 concurrent rides with real-time location updates? What sharding strategy would you use?","channel":"system-design","subChannel":"distributed-systems","difficulty":"beginner","tags":["scaling","sharding","replication"],"companies":["Amazon","Google","Lyft","Meta","Netflix","Salesforce","Uber"]},{"id":"q-536","question":"Design a distributed consensus service for a ride-sharing platform handling 10M concurrent rides with real-time location updates. How do you ensure consistency across geo-distributed data centers while maintaining <100ms latency?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["dist-sys","cap-theorem","consensus"],"companies":["LinkedIn","Lyft","Meta"]},{"id":"q-612","question":"How would you design a communication pattern for a microservices architecture where services need to maintain real-time data consistency while handling high-throughput requests?","channel":"system-design","subChannel":"distributed-systems","difficulty":"intermediate","tags":["microservices","communication-patterns","event-driven","consistency","message-queues"],"companies":["Netflix","Uber","Amazon","Twitter"]},{"id":"sd-2","question":"Design a distributed caching system using Consistent Hashing. How would you handle node failures, load balancing, and ensure minimal data movement when scaling from 10 to 100 nodes?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["hashing","dist-sys","caching"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"sd-3","question":"Explain the CAP Theorem. Can you really 'choose two' and what are the practical tradeoffs?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["theory","dist-sys","database"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"sd-4","question":"Design a database sharding strategy for a social media platform with 100M+ users. How would you handle data distribution, cross-shard queries, and rebalancing?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["db","scale","architecture"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"sd-5","question":"Design a distributed rate limiter for a microservices API handling 10,000 RPS with 99.9% availability using Redis Cluster. How would you handle cache invalidation, circuit breakers, and multi-region consistency?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["security","api","algorithms"],"companies":["Amazon","Cloudflare","Google","Meta","Netflix","Stripe"]},{"id":"sy-132","question":"Design a distributed rate limiting system that can handle 1M+ requests per second across multiple data centers while maintaining consistency and low latency. How would you handle burst traffic, different rate limiting algorithms (token bucket, sliding window), and ensure fair distribution across users?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["api","rest"],"companies":["Amazon","Google","Meta","Microsoft","Uber"]},{"id":"sy-137","question":"Design a distributed system that provides exactly-once processing guarantees for event streams with out-of-order delivery and network partitions. How would you handle idempotency, deduplication, and causal consistency across multiple processing nodes?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["dist-sys","architecture"],"companies":["Goldman Sachs","LinkedIn","Netflix","Stripe","Uber"]},{"id":"sy-138","question":"Design a distributed rate limiting system that can handle 10M requests per minute across 100+ microservices with different rate limit policies per service. How would you ensure high availability, consistency, and sub-millisecond latency while handling failures and scaling?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["api","rest"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"sy-139","question":"Design a rate limiting system for a multi-tenant API serving 100M+ daily calls across 5 regions, supporting tiered rate limits (1000-100K RPS), burst capacity (3x sustained rate), sub-50ms latency, and 99.99% availability using distributed token bucket algorithm?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["api","rest"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"sy-140","question":"Design a rate limiting service that can handle 10 million requests per second with distributed consistency across multiple data centers. The service should support multiple rate limiting strategies (token bucket, sliding window, fixed window) and provide sub-millisecond latency. How would you architect this to handle bursts, prevent thundering herd problems, and ensure accurate global rate limits?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["api","rest"],"companies":["Amazon","Google","Meta","Microsoft","Stripe"]},{"id":"sy-141","question":"Design a globally distributed serverless platform for real-time collaborative document editing with offline support and conflict resolution. How would you handle data consistency, versioning, and low-latency synchronization across AWS regions while maintaining sub-50ms response times?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["infra","scale"],"companies":["Amazon","Dropbox","Google","Meta","Microsoft"]},{"id":"sy-158","question":"Design a distributed rate limiter that can handle 1M requests/second across 100 data centers with <10ms latency. How do you ensure accurate rate limiting while avoiding coordination overhead?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["dist-sys","architecture"],"companies":["Amazon","Google","Meta","Microsoft","Uber"]},{"id":"q-622","question":"How would you design communication between microservices when you need to ensure data consistency across multiple services, and what patterns would you consider?","channel":"system-design","subChannel":"distributed-transactions","difficulty":"intermediate","tags":["microservices","distributed-systems","data-consistency","saga-pattern","event-driven"],"companies":["Netflix","Amazon","Uber","LinkedIn","Spotify"]},{"id":"gh-14","question":"How would you design a scalable cloud platform architecture that integrates compute, storage, networking, and database services?","channel":"system-design","subChannel":"infrastructure","difficulty":"beginner","tags":["cloud","aws","azure","gcp"],"companies":["Amazon","Google","Meta"]},{"id":"gh-84","question":"Design a cloud-native modernization strategy for a 10M-user monolithic e-commerce platform requiring 99.9% uptime. How would you migrate to microservices while maintaining business continuity and optimizing costs?","channel":"system-design","subChannel":"infrastructure","difficulty":"beginner","tags":["migration","cloud"],"companies":null},{"id":"gh-91","question":"How would you design a comprehensive feature flagging system that supports both server-side and client-side flags with proper performance considerations?","channel":"system-design","subChannel":"infrastructure","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-265","question":"How would you design a unified system monitoring dashboard that aggregates real-time process metrics, system call tracing, and network connection data from htop, strace, and lsof?","channel":"system-design","subChannel":"infrastructure","difficulty":"intermediate","tags":["top","htop","strace","lsof"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-316","question":"How would you design a database architecture to handle 10 million users with 99.99% uptime? What sharding and replication strategies would you use?","channel":"system-design","subChannel":"infrastructure","difficulty":"beginner","tags":["scaling","sharding","replication"],"companies":["Chime","Salesforce","Snowflake"]},{"id":"q-327","question":"Design a simple API rate limiter that can handle 10,000 requests per second. How would you prevent abuse while ensuring legitimate users aren't blocked?","channel":"system-design","subChannel":"infrastructure","difficulty":"beginner","tags":["infra","scale","distributed"],"companies":["Salesforce","Square","Supabase"]},{"id":"q-562","question":"Design a real-time vehicle telemetry system for Tesla's fleet of 10M cars collecting sensor data at 100Hz?","channel":"system-design","subChannel":"infrastructure","difficulty":"advanced","tags":["infra","scale","distributed"],"companies":["Apple","Tesla"]},{"id":"q-619","question":"What is an ambient mesh and how does it differ from traditional service mesh architectures?","channel":"system-design","subChannel":"infrastructure","difficulty":"intermediate","tags":["service-mesh","kubernetes","infrastructure","networking"],"companies":["Google","IBM","Microsoft"]},{"id":"sy-169","question":"Design a URL shortening service that handles 1 billion URLs with 10M daily requests, achieving 99.99% uptime. How would you architect the system for high availability and scalability?","channel":"system-design","subChannel":"infrastructure","difficulty":"beginner","tags":["infra","scale"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-607","question":"Design a communication strategy for a microservices architecture where you have 10+ services that need to exchange data. What patterns would you use and why?","channel":"system-design","subChannel":"inter-service-communication","difficulty":"intermediate","tags":["microservices","communication-patterns","system-design","event-driven","api-gateway"],"companies":["Netflix","Amazon","Uber","Spotify","Airbnb"]},{"id":"q-617","question":"Design a communication pattern for a microservices architecture where an order service needs to notify inventory, payment, and shipping services when a new order is placed. Discuss the trade-offs between synchronous REST calls, message queues, and event streaming.","channel":"system-design","subChannel":"inter-service-communication","difficulty":"intermediate","tags":["microservices","communication-patterns","distributed-systems","message-queues","event-streaming"],"companies":["Netflix","Amazon","Uber","LinkedIn","Spotify"]},{"id":"gh-33","question":"How do different load balancing algorithms distribute traffic across servers, and what are the trade-offs between performance and resource utilization?","channel":"system-design","subChannel":"load-balancing","difficulty":"advanced","tags":["scale","ha"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Netflix"]},{"id":"q-285","question":"How would you design a load balancer that handles 1M concurrent connections using NGINX vs HAProxy?","channel":"system-design","subChannel":"load-balancing","difficulty":"intermediate","tags":["lb","traffic","nginx","haproxy"],"companies":["Amazon","Google","Meta"]},{"id":"q-376","question":"Design a load balancing system for a global e-commerce platform handling 50M concurrent users during Black Friday sales. How would you ensure zero downtime while handling 10x traffic spikes?","channel":"system-design","subChannel":"load-balancing","difficulty":"advanced","tags":["lb","traffic","nginx","haproxy"],"companies":["Hashicorp","Thoughtworks","Workday"]},{"id":"q-393","question":"Design a global load balancer for Google Cloud that handles 10M concurrent connections with sub-10ms failover across 5 regions. How would you ensure zero-downtime deployments while maintaining 99.999% availability?","channel":"system-design","subChannel":"load-balancing","difficulty":"advanced","tags":["lb","traffic","nginx","haproxy"],"companies":null},{"id":"q-591","question":"How would you design a load balancer for a microservices architecture handling 10,000 requests per second with 99.99% uptime?","channel":"system-design","subChannel":"load-balancing","difficulty":"intermediate","tags":["lb","traffic","nginx","haproxy"],"companies":["Anthropic","Oracle","Tesla"]},{"id":"q-598","question":"Design a load balancer for a high-traffic e-commerce platform that must handle 100,000 requests per second with 99.99% uptime. Explain how you would choose between round-robin, least connections, and weighted round-robin algorithms, and describe your failover strategy.","channel":"system-design","subChannel":"load-balancing-algorithms","difficulty":"intermediate","tags":["load-balancing","system-design","scalability","high-availability","algorithms"],"companies":["Amazon","Google","Meta","Netflix","Microsoft","Uber","Airbnb"]},{"id":"q-605","question":"Design a load balancer for a high-traffic e-commerce website that must handle 100,000 requests per second with 99.99% uptime. Explain which load balancing algorithm you would choose and why, considering factors like session persistence, health checks, and failover mechanisms.","channel":"system-design","subChannel":"load-balancing-algorithms","difficulty":"intermediate","tags":["load-balancing","system-design","scalability","high-availability","algorithms"],"companies":["Amazon","Google","Netflix","Meta","Microsoft","Apple"]},{"id":"q-615","question":"Design a load balancer for a high-traffic e-commerce website. Which load balancing algorithm would you choose and why? How would you handle session persistence and failover?","channel":"system-design","subChannel":"load-balancing-algorithms","difficulty":"intermediate","tags":["load-balancing","system-design","high-availability","scalability","algorithms"],"companies":["Amazon","Google","Meta","Netflix","Uber","Airbnb"]},{"id":"q-609","question":"Design a load balancer for a high-traffic e-commerce platform. Which load balancing algorithm would you choose and why? Consider factors like session persistence, server health, and traffic distribution.","channel":"system-design","subChannel":"load-balancing-strategies","difficulty":"intermediate","tags":["load-balancing","algorithms","system-design","high-availability","scalability"],"companies":["Amazon","Google","Netflix","Meta","Microsoft","Apple"]},{"id":"q-610","question":"Design a load balancer for a high-traffic e-commerce platform. Which load balancing algorithm would you choose and why? Consider the trade-offs between different algorithms.","channel":"system-design","subChannel":"load-balancing-strategies","difficulty":"intermediate","tags":["load-balancing","algorithms","system-design","scalability","high-availability"],"companies":["Amazon","Google","Netflix","Meta","Microsoft"]},{"id":"q-266","question":"Design a distributed message queue system that handles 1M events/sec with exactly-once delivery, sub-second latency, and 99.99% availability. How would you ensure data consistency across partitions while handling consumer failures and network partitions?","channel":"system-design","subChannel":"message-queues","difficulty":"intermediate","tags":["kafka","rabbitmq","sqs","pubsub"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-361","question":"Design a distributed message queue system for processing 10M financial transactions per hour with exactly-once delivery guarantees across multiple data centers. How would you handle message ordering, deduplication, and cross-region consistency?","channel":"system-design","subChannel":"message-queues","difficulty":"advanced","tags":["kafka","rabbitmq","sqs","pubsub"],"companies":["Amazon","Broadcom","Google","Netflix","PayPal","Robinhood","Stripe","Western Digital"]},{"id":"q-432","question":"Design a food delivery app's order processing system using message queues to handle 10,000 orders per minute with exactly-once processing?","channel":"system-design","subChannel":"message-queues","difficulty":"beginner","tags":["kafka","rabbitmq","sqs","pubsub"],"companies":["Lyft","Microsoft","Oracle"]},{"id":"q-623","question":"Design a communication strategy for a microservices architecture where some services need real-time updates while others can tolerate eventual consistency. What patterns would you use and how would you handle service discovery and load balancing?","channel":"system-design","subChannel":"microservices-architecture","difficulty":"intermediate","tags":["microservices","communication-patterns","service-discovery","load-balancing","consistency"],"companies":["Netflix","Amazon","Uber","Spotify","Airbnb"]},{"id":"q-616","question":"Design a CI/CD pipeline for a microservices application with 10 services, each with its own repository. The pipeline must support parallel deployments, canary releases, automated testing, and rollback capabilities. How would you ensure zero-downtime deployments and maintain consistency across services?","channel":"system-design","subChannel":"pipeline-architecture","difficulty":"advanced","tags":["cicd","microservices","kubernetes","devops","gitops"],"companies":["Google","Netflix","Amazon","Microsoft","Uber","Airbnb"]},{"id":"q-631","question":"Design a CI/CD pipeline for a microservices application with 10 services. Each service has its own repository and needs automated testing, security scanning, and deployment to both staging and production environments. How would you ensure fast feedback loops while maintaining quality and security?","channel":"system-design","subChannel":"pipeline-architecture","difficulty":"intermediate","tags":["cicd","microservices","devops","security","automation"],"companies":["Google","Amazon","Microsoft","Netflix","Spotify","Uber"]},{"id":"q-626","question":"Compare and contrast synchronous vs asynchronous communication patterns in microservices, and when would you choose one over the other?","channel":"system-design","subChannel":"service-communication","difficulty":"intermediate","tags":["microservices","communication-patterns","system-design","architecture"],"companies":["Netflix","Amazon","Uber","Spotify","Airbnb"]},{"id":"q-632","question":"Design a communication strategy for a microservices architecture where you have an Order Service, Payment Service, and Inventory Service. The Order Service needs to coordinate with both Payment and Inventory services to process an order. What communication patterns would you use and why?","channel":"system-design","subChannel":"service-communication","difficulty":"intermediate","tags":["microservices","communication-patterns","system-design","async-messaging","api-design"],"companies":["Netflix","Amazon","Uber","Spotify","Airbnb"]},{"id":"q-1343","question":"How would you design a distributed caching system for a real-time analytics platform that processes streaming data from millions of IoT devices, requiring sub-second query responses and handling high write throughput with eventual consistency?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["distributed-caching","real-time-analytics","iot-streaming","eventual-consistency","write-throughput"],"companies":[]},{"id":"q-1621","question":"Design a rate limiting system for a video streaming platform that needs to enforce different limits based on content type and user subscription tier: free users (5 videos/hour, 100MB bandwidth), premium users (50 videos/hour, 1GB bandwidth), and enterprise users (unlimited videos, 10GB bandwidth). How would you implement a multi-dimensional rate limiter that tracks both request count and data transfer while preventing abuse and ensuring fair resource allocation?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","multi-dimensional","video-streaming","redis","sliding-window"],"companies":[]},{"id":"q-1623","question":"Design a rate limiting system for a video streaming platform that needs to handle different rate limits for various user tiers and content types: free users (5 video plays/hour), premium users (50 video plays/hour), and content creators (1000 API calls/hour for upload management). How would you implement a hybrid rate limiter that combines user-based and content-based limits while preventing abuse during peak viewing hours?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","redis","video-streaming","user-tiers","hybrid-rate-limiting"],"companies":[]},{"id":"q-1624","question":"Design a rate limiting system for a video streaming platform that needs to handle different rate limits for various user tiers and content types: free users (5 video plays/hour), premium users (unlimited plays), and API access (1000 req/hour). How would you implement a hybrid rate limiting approach that combines user-based and content-based limits while preventing abuse during peak events like live streams?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","redis","video-streaming","hybrid-limits","adaptive-throttling"],"companies":[]},{"id":"q-1625","question":"How would you design a communication pattern for a microservices architecture where services need to handle both high-volume batch processing and low-latency interactive requests, while ensuring message ordering and exactly-once processing guarantees?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["microservices","message-queues","exactly-once","batch-processing","low-latency"],"companies":[]},{"id":"q-2122","question":"Design a rate limiting system for a mobile banking API that must handle different limits based on transaction type and user risk level: low-risk users (1000 req/day), high-risk users (100 req/day), with stricter limits for sensitive operations like money transfers (10/hour) vs. balance checks (100/hour). How would you implement a dynamic rate limiter that adapts to real-time fraud detection signals?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","redis","fraud-detection","mobile-banking","sliding-window"],"companies":[]},{"id":"q-2123","question":"How would you design a distributed caching system for a real-time analytics platform that processes streaming data from millions of IoT devices, requiring sub-100ms query response times for time-series aggregations while handling high write throughput and data freshness requirements?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["distributed-caching","time-series","iot-analytics","real-time","cache-consistency"],"companies":[]},{"id":"q-2124","question":"Design a rate limiting system for a financial trading platform that needs to enforce different limits based on market volatility: normal markets (1000 req/sec), high volatility (100 req/sec), and flash crash scenarios (10 req/sec). How would you implement a dynamic rate limiter that can automatically adjust limits based on real-time market conditions while preventing manipulation and ensuring fair access for all clients?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","financial-systems","dynamic-algorithms","market-data","circuit-breaker"],"companies":[]},{"id":"q-2618","question":"How would you design a communication pattern for a microservices architecture where services need to handle both high-frequency small messages (like telemetry data) and large payload transfers (like file processing), while optimizing for network bandwidth and preventing message storms?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["microservices","communication-patterns","backpressure","message-queuing","gRPC","rate-limiting"],"companies":[]},{"id":"q-3111","question":"How would you design a communication pattern for a microservices architecture where services need to maintain backward compatibility during gradual protocol migrations, ensuring zero-downtime deployments while supporting both legacy and new message formats?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["microservices","protocol-migration","backward-compatibility","zero-downtime","api-versioning"],"companies":[]},{"id":"q-3652","question":"How would you design a communication pattern for a microservices architecture where services need to maintain backward compatibility during gradual protocol migrations, ensuring zero-downtime deployments while supporting both old and new message formats?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["microservices","api-versioning","zero-downtime","schema-evolution","backward-compatibility"],"companies":[]},{"id":"q-4123","question":"How would you design a hybrid communication pattern for a microservices architecture where services operate across multiple cloud regions and need to maintain data consistency while handling network partitions and varying latency between regions?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["multi-region","network-partitions","conflict-resolution","eventual-consistency"],"companies":[]},{"id":"q-4125","question":"How would you implement adaptive load balancing that dynamically adjusts algorithms based on real-time traffic patterns and server performance metrics?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["adaptive-load-balancing","algorithm-selection","performance-monitoring","dynamic-algorithms"],"companies":[]},{"id":"q-4126","question":"How would you design a distributed caching strategy for a real-time collaborative editing platform like Google Docs, where multiple users simultaneously edit the same document and changes must be reflected across all clients within 100ms?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["distributed-caching","real-time-collaboration","crdt","cache-coherence","conflict-resolution"],"companies":[]},{"id":"q-4594","question":"How would you design a distributed caching strategy for a video streaming platform's content delivery network, where you need to cache video segments at multiple edge locations while ensuring that updated content (like deleted videos or modified metadata) propagates within 30 seconds across all edge nodes?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["cdn","edge-computing","cache-invalidation","video-streaming","multi-tier-architecture"],"companies":[]},{"id":"q-639","question":"How would you design a distributed caching system for a real-time analytics platform that processes streaming event data from millions of IoT devices, requiring sub-10ms query latency for time-series aggregations while handling high write throughput and ensuring data freshness?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["distributed-caching","time-series","real-time-analytics","iot","write-through"],"companies":[]},{"id":"q-645","question":"Design a distributed caching system for a real-time collaborative editing platform (like Google Docs) that must handle concurrent edits from thousands of users on the same document while maintaining strong consistency and sub-10ms latency. How would you handle conflict resolution, version control, and cache invalidation when multiple users edit the same document simultaneously?","channel":"system-design","subChannel":"system-design","difficulty":"advanced","tags":["distributed-caching","real-time-collaboration","conflict-resolution","operational-transformation","strong-consistency"],"companies":[]},{"id":"q-646","question":"Design a rate limiting system for a real-time chat application that needs to handle different types of messages with varying limits: text messages (100/min), file uploads (10/min), and API calls (1000/min). How would you implement a multi-tier rate limiter that prevents spam while ensuring critical messages (like emergency alerts) are always delivered?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","real-time-systems","token-bucket","redis","priority-queues"],"companies":[]},{"id":"q-647","question":"Design a rate limiting system for a video streaming platform that needs to limit API calls based on both user subscription tier (free: 1000 calls/day, premium: 10000 calls/day) AND content type (video uploads: 10/hour, metadata queries: 100/minute). How would you implement multi-dimensional rate limiting while ensuring fair usage and preventing abuse?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","redis","multi-dimensional","api-gateway","token-bucket"],"companies":[]},{"id":"q-649","question":"How would you design a communication pattern for a microservices architecture where services need to handle both request-response interactions and event-driven updates, while ensuring backward compatibility during API version transitions?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["microservices","api-versioning","hybrid-communication","backward-compatibility","system-design"],"companies":[]},{"id":"q-650","question":"Design a rate limiting system for a video streaming platform that needs to limit API calls based on both user tier (free/premium) and content type (metadata vs video transcoding requests). How would you implement a multi-dimensional rate limiter that can handle 100K concurrent users with different limits per content type while preventing abuse and ensuring fair resource allocation?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","redis","multi-dimensional","user-tiers","content-aware"],"companies":[]},{"id":"q-658","question":"How would you design a multi-tier distributed caching strategy for a microservices architecture where different services have varying access patterns and consistency requirements?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["distributed-caching","microservices","cache-tiers","consistency-patterns"],"companies":[]},{"id":"q-764","question":"Design a rate limiting system for a content delivery network (CDN) that needs to enforce different limits based on content type and geographic region. Static assets (images, CSS) can serve 10,000 req/min globally, while dynamic API endpoints have stricter limits: 1,000 req/min per region for North America, 500 req/min for Europe, and 200 req/min for Asia-Pacific. How would you implement a hierarchical rate limiter that balances global fairness with regional capacity constraints?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","cdn","geo-distribution","redis","hierarchical-limits"],"companies":[]},{"id":"q-1015","question":"Design a TensorFlow 2.x data pipeline for a document-classification model trained on 8 GPUs with MirroredStrategy. Data comes from two sources: TFRecords with image_raw and a CSV with per-record numeric metadata. Build a single tf.data pipeline that yields a dict {'image': image_tensor, 'meta': meta_tensor}, with image decoded and resized to 224x224 and scaled to [0,1], metadata normalized, deterministic per-epoch shuffling with a fixed seed, interleaving sources with parallelism, caching, and prefetching. Then implement gradient accumulation to reach a global batch size of 1024 while per-replica batch size is 128, and outline reproducibility checks and simple throughput measurements. Provide key code blocks?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Bloomberg","Meta","Microsoft"]},{"id":"q-1072","question":"You're deploying a TensorFlow 2.x recommender with dense features and a massive sparse feature 'item_id'. The item vocabulary comes from Redis and updates in real time without downtime. Describe a practical serving approach that keeps latency under 20 ms, handles unseen IDs gracefully, and updates embeddings without restarting the service. Include a minimal code sketch showing how to load a Redis-backed vocabulary into a tf.lookup MutableHashTable and map incoming IDs to embeddings inside a tf.function?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Adobe","Tesla"]},{"id":"q-1103","question":"Scenario: You have a dataset of short audio clips stored as WAV files in data/train/{class}/*.wav. As a beginner TensorFlow developer, implement a minimal end-to-end solution: (1) a tf.data pipeline that reads file paths and infers the label from the parent directory, (2) loads WAVs as mono, (3) pads/trims to 16000 samples, (4) normalizes to [-1,1], (5) shuffles with a fixed seed, (6) caches and prefetches, (7) batches 32. Then define a tiny Conv1D classifier for 2 classes and show how to train with model.fit using the pipeline. Include only the essential code blocks?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Meta","MongoDB"]},{"id":"q-1168","question":"You’re building a TensorFlow model that jointly processes images and captions stored in a CSV with columns: image_path, caption, label. Implement an efficient tf.data pipeline that (1) reads the CSV, (2) loads and decodes images from disk with aspect-ratio-preserving resize to 224x224, (3) tokenizes captions using a saved BPE tokenizer loaded from a file, (4) pads captions to the max length within each batch, (5) caches, (6) shuffles with a fixed seed, (7) batches 64, (8) runs under a multi-GPU distribution strategy. Provide the core code blocks and discuss performance trade-offs?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Amazon","Google"]},{"id":"q-1252","question":"You are training a large Transformer-based recommender model on multiple GPUs with a custom training loop in TensorFlow 2.x. The per-device batch is 256, but you want an effective global batch of 4096. Describe and implement how to use gradient accumulation to achieve this, including how to adjust the learning rate, BN handling, and mixed-precision considerations?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["NVIDIA","Snap"]},{"id":"q-1384","question":"Describe how to deploy a TensorFlow model that accepts variable-length input sequences in production without a fixed max_length. Include input signatures, RaggedTensor usage, encoder compatibility, dynamic batching (bucketed), memory/latency considerations, and validation strategy with latency and throughput benchmarks?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Microsoft","Snowflake"]},{"id":"q-1422","question":"You have a local dataset of 50k JPEG images organized as train/{class} and val/{class}. Build a beginner-friendly TensorFlow 2.x data pipeline that trains a simple CNN on CPU. Describe and implement reading files with tf.data, decoding JPEG, resizing to 128x128, applying basic augmentations, and batching with prefetch. Include a method to verify input throughput keeps the trainer busy?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Amazon","Google"]},{"id":"q-1454","question":"Given a directory of JPEG images and a CSV file with two columns (path, label) for a 2-class image classification task, design a beginner-friendly tf.data pipeline that keeps the GPU busy on a single GPU. What steps would you include and provide a minimal code snippet using cache, shuffle, batch, and prefetch?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Amazon","Apple","Zoom"]},{"id":"q-1510","question":"You're deploying a browser-based image classifier in a product used by large-scale apps (e.g., Discord, Instacart, Lyft). Describe end-to-end how to convert a trained Keras MobileNetV2 model to TensorFlow.js, including quantization choices and asset packaging, and how to serve it from a static site. Then provide a minimal JavaScript snippet to load the model, preprocess a 224x224 HTMLImageElement, and output the top-3 class labels?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Discord","Instacart","Lyft"]},{"id":"q-1518","question":"On a workstation with 4 GPUs, train a multi-modal model (image + text) using a shared backbone and two heads. How would you implement a single training loop that (1) accumulates gradients to emulate a larger global batch, (2) applies per-branch loss weights to form a stable total loss under mixed-precision, and (3) keeps data sharding synchronized across GPUs? Provide a minimal code sketch?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["OpenAI","Snowflake","Tesla"]},{"id":"q-1629","question":"You are building a real-time fraud-detection model in TensorFlow 2.x. Data arrives as streaming JSON with dense features and a high-cardinality categorical field. Design a streaming tf.data pipeline that hashes the categorical feature, caches preprocessed tensors, and trains with focal loss on a single-GPU setup. Provide a minimal, runnable dataset builder and focal loss snippet that demonstrates the end-to-end flow?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["IBM","Salesforce","Tesla"]},{"id":"q-1739","question":"You’re deploying a multi-tenant vision model behind TensorFlow Serving on Kubernetes. Tenants share a single model graph but require different input normalization pipelines (e.g., color space, resizing strategy, and augmentation). How would you design a solution that isolates per-tenant preprocessing without duplicating the model, keeps latency under 50 ms per inference, and allows updating tenant parameters without redeploying the model?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Adobe","Google","Oracle"]},{"id":"q-1752","question":"In training a Transformer language model on 4 GPUs with mixed precision using tf.distribute.MirroredStrategy, how would you implement gradient accumulation to simulate a larger global batch while keeping updates stable? Provide a minimal code snippet showing an accumulation loop and the cadence for applying the optimizer?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["DoorDash","Microsoft","MongoDB"]},{"id":"q-1840","question":"How would you implement a multi‑objective training loop in TensorFlow 2.x that trains a model with a primary cross-entropy loss and an auxiliary contrastive loss under tf.distribute.MultiWorkerMirroredStrategy, ensuring stable convergence via dynamic loss weighting (GradNorm), per-batch gradient normalization, and proper sequence masking for variable-length inputs?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["IBM","NVIDIA","Twitter"]},{"id":"q-1859","question":"You are training a graph neural network for molecular property prediction with graphs of varying sizes. The dataset is stored as sharded TFRecord files on GCS and you want multi-GPU training (2–4 GPUs). Propose a tf.data pipeline that buckets graphs by node count, pads to the bucket max, preserves per-epoch shuffling, and integrates with tf.distribute.Strategy. Describe the approach and provide a minimal batching sketch?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Cloudflare","Google","Lyft"]},{"id":"q-2081","question":"In a distributed training setup with tf.distribute.MultiWorkerMirroredStrategy across 8 workers, how would you guarantee deterministic sharding and data order for a long-text Transformer training run? Include how to set global_batch_size, per_replica_batch_size, dataset sharding via input_context/experimental_distribute_dataset, and seeds/flags to ensure reproducibility; avoid hidden data leakage across workers?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Citadel","Cloudflare"]},{"id":"q-2098","question":"Using TensorFlow 2.x, implement a gradient accumulation training loop inside tf.distribute.MirroredStrategy to achieve an effective batch size of 1024 with per-step batch 64 on multi-GPU. Include how you would apply loss scaling for mixed precision, and how to accumulate and apply gradients every 'accum_steps' steps. Provide skeleton code for the train_step and train_loop, and explain memory and determinism considerations?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Discord","Google"]},{"id":"q-2161","question":"You're deploying a Transformer-based sequence model for a real-time recommender in TensorFlow Serving behind a microservice API. Clients send requests with variable-length sequences up to 1024 tokens. You need dynamic batching, mixed precision, and memory-efficient attention to meet P95 latency under 30 ms at 1k RPS, plus canary rollouts. Outline the end-to-end approach: preprocessing, model packaging, dynamic batching strategy, memory management, and benchmarking. Which TF APIs and Serving config would you use, and what are the trade-offs?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Databricks","DoorDash"]},{"id":"q-2258","question":"You're deploying a real-time sentence-embedding model in TensorFlow 2.x behind TensorFlow Serving on Kubernetes for a high-throughput API. How would you architect deterministic dynamic batching to coalesce requests with varying sequence lengths, ensuring tail latency stays under 20 ms while preserving embedding quality, including choices between TF Serving batching versus a custom batching layer, handling bucketing/padding, and validation/rollback plans?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Goldman Sachs","Meta","Oracle"]},{"id":"q-2283","question":"You have a small image classifier trained in Keras on 28x28 grayscale images and you need to deploy on-device with limited compute. Describe a concrete, end-to-end plan to convert the model to TensorFlow Lite, choose between post-training quantization and quantization-aware training, and how you would validate that accuracy loss on-device remains acceptable after quantization?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Discord","Lyft","Scale Ai"]},{"id":"q-2437","question":"You run a real-time multi-tenant anomaly-detection service for network traffic. Each tenant can generate up to 1k events/sec; you need strong isolation and predictable tail latency. How would you architect the inference path in TensorFlow Serving to support per-tenant routing, tenant-specific weights, dynamic batching, and model versioning in production?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Cloudflare","Coinbase"]},{"id":"q-2512","question":"In a production TensorFlow 2.x service, you need to feed a model with high-throughput image batches stored in a remote bucket. The pipeline must support deterministic preprocessing, robust caching, and minimal CPU-GPU contention. Describe how you would construct a tf.data pipeline using interleave/map with num_parallel_calls, cache, shuffle, batch, prefetch, and AUTOTUNE. Include a minimal example demonstrating the pipeline for 256x256 RGB images and explain how you would measure throughput and tune parameters?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["LinkedIn","Microsoft","Salesforce"]},{"id":"q-2528","question":"You're deploying a streaming text-generation Transformer behind TensorFlow Serving; latency budget is 50 ms per token under burst traffic. Outline a practical approach to streaming inference in TensorFlow that reuses attention keys/values across tokens, chooses decoding strategy (greedy, top-k/top-p) with cache, and maintains per-session state across requests. Include concrete components, data shapes, and potential pitfalls like cache invalidation and memory growth?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Hugging Face","Instacart","Lyft"]},{"id":"q-2543","question":"You are training a simple image classifier in TensorFlow 2.x. The training loop stalls because the input pipeline is the bottleneck; dataset is 1M TFRecord images on GCS. How would you construct a tf.data pipeline with parallel reads, caching, and prefetch to keep GPUs busy? Provide a code snippet showing the pipeline steps and parameter choices?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["DoorDash","Oracle","Snowflake"]},{"id":"q-2651","question":"Given a 10k-image dataset and a simple CNN in TensorFlow 2.x, describe a reproducible training setup on a single GPU that enforces determinism across runs, including seed initialization, environment flags for deterministic ops, and pragmatic caveats; provide a minimal code-free plan and an outline of what you'd verify in tests?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Anthropic","Lyft"]},{"id":"q-2768","question":"During TFRecord-based training in TF 2.x, some records become corrupted and crash the tf.data pipeline, stalling training. Design a robust ingestion strategy using tf.data.experimental.ignore_errors(), with minimal slowdown, plus per-epoch accounting and a simple retry/recovery path. Include a minimal code snippet demonstrating the setup and discuss monitoring?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Databricks","Meta","Two Sigma"]},{"id":"q-2811","question":"Scenario: CTR model with a 100M-item embedding trained on 8 GPUs suffers embedding hot-spotting. Describe a practical, production-ready approach to shard the embedding across devices, implement a Keras layer that performs distributed lookups, and ensure gradient updates synchronize under tf.distribute.Strategy. Include a minimal code skeleton showing a partitioned embedding and how IDs are routed to shards?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Databricks","IBM"]},{"id":"q-2889","question":"You have a CSV dataset stored at gs://data/reviews.csv with columns 'text' and 'label'. Design a beginner-friendly TensorFlow 2.x pipeline that uses a TextVectorization layer inside the model to tokenize and feed into an Embedding + GlobalAveragePooling + Dense classifier. Include the tf.data steps (read CSV, batch, cache, prefetch) and show a minimal training snippet?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Cloudflare","LinkedIn","Lyft"]},{"id":"q-2941","question":"Design a minimal custom training loop in TensorFlow 2.x to train a CNN on a toy 28x28 grayscale dataset. Use tf.GradientTape, an Adam optimizer, and per-batch logging to TensorBoard. Implement a simple early stopping based on validation loss and save the best model with a Checkpoint. Provide a compact train_step and loop skeleton showing these components?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Adobe","Meta","Tesla"]},{"id":"q-3164","question":"Design an inference path for a Transformer-based model served via TensorFlow Serving behind a REST API where input sequences vary in length from 1 to 1024 tokens. To meet bounded latency under burst traffic, describe how you would implement dynamic batching with a fixed window, sequence-length bucketing, and a warm cache. Include concrete parameters and where the logic lives (serving config vs client)?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Plaid","Salesforce"]},{"id":"q-3321","question":"In a production TensorFlow Serving setup, design a single endpoint that can route requests to either a small text classifier or a large document classifier while sharing a common embedding layer. Describe the model wrapper, how you determine routing (e.g., by token count or a flag), how you expose appropriate signatures, and how you keep latency predictable under burst traffic. Provide a minimal code sketch of the wrapper with two heads and routing logic?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Anthropic","Bloomberg","Lyft"]},{"id":"q-3482","question":"You’re deploying a TensorFlow Serving model for real-time image classification on Kubernetes; traffic is bursty and latency SLOs must be met. Describe how you would configure dynamic batching in TF Serving, including batching_parameters_file settings (max_batch_size and batch_timeout_millis) and how you would validate throughput and latency under bursty load?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Amazon","IBM","Microsoft"]},{"id":"q-3519","question":"Advanced: You deploy a large multilingual Transformer in TensorFlow 2.x on Kubernetes with TensorFlow Serving. Inputs are variable-length sequences; latency must stay under 20 ms per request during bursts. Outline end-to-end design for preprocessing, dynamic batching, model optimization (quantization/TF-TRT), and zero-downtime updates with versioned models, plus monitoring?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Hugging Face","Microsoft"]},{"id":"q-3581","question":"You have a local CSV dataset with 100k rows, 8 numeric features, 1 categorical feature named 'cat' with 4 categories, and a binary label 'target'. Build a minimal TensorFlow solution: 1) tf.data pipeline to parse CSV, apply per-feature normalization for numeric features, encode 'cat' via StringLookup + Embedding, 2) a small feed-forward network that takes the concatenated features, 3) train with validation split and EarlyStopping, 4) export the trained model as a SavedModel. Include code and discuss trade-offs?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Bloomberg","Goldman Sachs","Uber"]},{"id":"q-3710","question":"Design and outline the end-to-end deployment of a multilingual Transformer for real-time inference behind TensorFlow Serving on Kubernetes, with variable-length inputs and burst latency ≤25 ms, supporting dynamic batching, mixed-precision, TF-TRT optimizations, and zero-downtime updates via versioned SavedModels. What are your concrete preprocessing, batching, optimization, observability, and rollout plans?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Citadel","Google","Microsoft"]},{"id":"q-3735","question":"You’re deploying a TensorFlow 2.x time-series anomaly detector that accepts variable-length sequences (up to 2048 timesteps) from edge sensors. Real-time latency target: 15 ms average, with spikes up to 1000 rps. Describe a serving design that uses tf.data preprocessing to bucket by sequence length (min padding), caching, and prefetch; handling RaggedTensor vs padded tensors; and a minimal tf.data snippet for sequences with 16 features. Include how you would measure latency and tune batch sizing and thread pools?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Cloudflare","Microsoft"]},{"id":"q-3819","question":"You have a dataset of 28x28 grayscale images stored as TFRecord files on GCS for a 2-class classifier. Build a tf.data pipeline that reads the records in parallel, parses features to (image, label), casts and scales image to [0,1], applies random horizontal flip as augmentation, shuffles with a 1000-element buffer, batches 64, caches in memory, and uses prefetch. Provide a minimal training snippet using a small Keras CNN?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Apple","Bloomberg","Discord"]},{"id":"q-3951","question":"Advanced: Training a multilingual seq2seq Transformer in TensorFlow 2.x with tf.distribute.MultiWorkerMirroredStrategy on Kubernetes. Per-epoch training time drifts due to dynamic padding causing load imbalance across workers. Propose a concrete data-pipeline strategy to fix this: implement bucketization by source/target lengths with tf.data.bucket_by_sequence_length, pad within buckets, balance batch sizes per bucket, and validate with micro-benchmarks. Include a minimal code snippet showing bucket_by_sequence_length usage?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Goldman Sachs","Instacart","Netflix"]},{"id":"q-4003","question":"Build a 28x28 grayscale image classifier in TensorFlow 2.x that trains a small CNN on CPU using Keras, then convert the trained model to TensorFlow Lite with post-training quantization; outline the data pipeline, model, training, and TFLite conversion steps, and explain how you would verify the quantized model's accuracy?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Goldman Sachs","Meta","Stripe"]},{"id":"q-4024","question":"You have a TensorFlow 2.x image classifier served with TensorFlow Serving on Kubernetes. During burst traffic, GPU memory fragmentation causes sporadic OOMs despite a fixed model size. Propose a concrete, implementable plan to mitigate without changing the model. Include deployment/config changes (dynamic batching, memory growth, concurrency, threads), monitoring, and a brief staging validation plan?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Amazon","Oracle","Square"]},{"id":"q-4105","question":"You have a 100k-sentence sentiment dataset with labels: positive, negative, neutral. Using TensorFlow 2.x, design a practical preprocessing and model-training pipeline that uses a TextVectorization layer (vocab size 10000, sequence length 100, OOV token, standardization: lowercase and punctuation removal) and a small embedding-based classifier. How would you train, validate, and evaluate it? Provide a minimal code outline?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["LinkedIn","Oracle","Snap"]},{"id":"q-4173","question":"In TensorFlow 2.x, you have a text dataset of 50k short customer reviews and need a lightweight binary sentiment classifier. Build a minimal tf.keras pipeline using TextVectorization (max_tokens=20000, output_mode='int', standardize='lower_and_strip_punctuation'), followed by Embedding(64), GlobalAveragePooling1D, and Dense(1, activation='sigmoid'). Show model construction and a train call. Discuss adjustments for 1M samples and latency goals?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Apple","Plaid"]},{"id":"q-4239","question":"In a TensorFlow 2.x model served behind a gRPC API on Kubernetes, bursts cause latency spikes due to input length variance. How would you configure TensorFlow Serving's dynamic_batching, handle variable-length inputs, ensure deterministic generation with a fixed seed, and validate SLA adherence under burst traffic with a synthetic workload?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Databricks","Microsoft","PayPal"]},{"id":"q-4257","question":"You have a multi-input TensorFlow 2.x model (image + text) deployed behind a microservice with bursty traffic. The data lives in GCS and preprocessing is CPU-bound; latency spikes under load. How would you design a scalable, deterministic input pipeline using tf.data service, per-replica batching, and caching to maintain throughput? Provide a concise plan plus a minimal code sketch?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Bloomberg","PayPal","Snap"]},{"id":"q-4314","question":"You're deploying a real-time anomaly detector in TensorFlow 2.x that ingests a telemetry stream of variable-length events. Design a serving path with a SavedModel that (a) uses dynamic batching to meet sub-5ms latency at 1k req/s, (b) accepts RaggedTensor inputs (with in-graph padding/masking to a max_len), (c) includes preprocessing in the graph, (d) optionally uses TF‑TRT or XLA, and (e) supports zero-downtime updates with versioned models and rollback. Outline input signature, batching policy, monitoring, and rollback criteria?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Amazon","Apple"]},{"id":"q-4441","question":"You are deploying a 50M-parameter Transformer for real-time inference in Kubernetes with CPU-only nodes. Provide a concrete end-to-end design to meet tail latency p95/p99 of 25 ms under bursts. Cover: input preprocessing and caching, dynamic batching strategy with latency budgets, model optimizations (quantization, pruning, kernel tuning), serving topology with versioned models and canary rollout, and an observability plan with latency validation under load. Include concrete parameter choices and trade-offs?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Robinhood","Scale Ai"]},{"id":"q-4483","question":"You have a transformer-based text classifier served via TensorFlow Serving on Kubernetes. To handle bursts with latency under 20 ms, design an end-to-end serving approach using dynamic batching, on-the-fly padding, and a small cache for recent embeddings. Include how you'd configure the model server, enable zero-downtime updates with versioning, and how you'd monitor tail latency. What would be your concrete plan?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Coinbase","Google","Uber"]},{"id":"q-4551","question":"You’ve trained a small image classifier in TF 2.x and want to deploy it with TensorFlow Serving via REST. Provide a minimal serving function that accepts a batch of images [B,224,224,3], runs the model, and returns a dict {'probs': ...} of class probabilities. Show how to wrap it with tf.function(input_signature=...) and save as a SavedModel exposing the 'serving_default' signature. Include a concise code snippet?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Citadel","Twitter"]},{"id":"q-4621","question":"You are building a beginner TensorFlow 2.x text classifier that reads sentences from a file and labels are provided separately. How would you implement a tf.data pipeline that tokenizes each line on whitespace, maps tokens to integer IDs using a StaticVocabularyTable built from the dataset, pads sequences to a fixed length, and batches for training? Include a minimal code snippet showing vocab creation, tokenization, and batching?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Goldman Sachs","Hashicorp","MongoDB"]},{"id":"q-4763","question":"In a Kubernetes deployment using TensorFlow Serving, you run a multi-tenant inference API where each tenant's models have separate versions and possibly different input shapes. Design an end-to-end solution that routes requests by tenantId to the correct model version, supports per-tenant latency targets, uses dynamic batching efficiently, handles model warmup/canary rollout, and provides monitoring/rollbacks. Be concrete about TF Serving config, gateway, and observability?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Hashicorp","Slack","Tesla"]},{"id":"q-857","question":"You’re deploying a multimodal TensorFlow 2.x Keras model that consumes an image [N,224,224,3] and a text embedding [N,128] to TensorFlow Serving on Kubernetes. Explain how to export a SavedModel with a serving_default signature that accepts a dict input {'image': ..., 'text': ...} and a separate 'predict_dense' signature for A/B testing. Include concrete input signatures, how to create concrete_functions, and how to manage versioning for backward compatibility?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["DoorDash","Meta","Slack"]},{"id":"q-943","question":"You're running distributed TensorFlow training with tf.distribute.MultiWorkerMirroredStrategy across 8 workers. Intermittent batch loss suggests non-deterministic per-worker data sharding and uneven batch boundaries. Describe a concrete fix: deterministic sharding, fixed seeds, per-replica batch sizing, and validation steps; specify exact API calls, TF_CONFIG handling, and how you'll verify convergence is repeatable?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Airbnb","Citadel"]},{"id":"q-966","question":"How would you deploy a text classifier in TF2 that must support vocab expansion without retraining? Provide a single SavedModel with two signatures: 'predict' for input {'texts': tf.Tensor<String>} and 'extend_vocab' for {'new_tokens': tf.Tensor<String>, 'vectors': tf.Tensor<float>}. Explain embedding resizing, token→id mapping, stateful management, and versioning; include a minimal code outline?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Citadel","Hugging Face","Oracle"]},{"id":"q-992","question":"You’re building a beginner TensorFlow image classifier. The dataset sits under data/train with subfolders per class (e.g., cat/dog). Write a minimal tf.data pipeline that (1) reads image files with automatic label inference, (2) decodes and resizes to 224x224, (3) scales pixels to [0,1], (4) shuffles with a fixed seed for reproducibility, (5) batches 32, and (6) caches to speed training. Include the key code blocks?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Anthropic","Snowflake","Tesla"]},{"id":"do-3","question":"What is Infrastructure as Code (IaC) and why is Terraform preferred over manual infrastructure management?","channel":"terraform","subChannel":"basics","difficulty":"beginner","tags":["infra","automation","terraform"],"companies":["Airbnb","Amazon","Google","Meta","Microsoft","Netflix","Stripe","Uber"]},{"id":"gh-17","question":"What is Terraform and how does it implement Infrastructure as Code (IaC) workflows?","channel":"terraform","subChannel":"basics","difficulty":"beginner","tags":["iac","terraform","ansible"],"companies":["Airbnb","Databricks","Goldman Sachs","Microsoft","Snowflake"]},{"id":"de-137","question":"You have a Terraform configuration that creates an AWS S3 bucket. After running 'terraform apply', you realize you need to add versioning to the bucket. What's the safest way to modify your existing infrastructure?","channel":"terraform","subChannel":"best-practices","difficulty":"beginner","tags":["terraform","iac"],"companies":["Amazon","Google","Hashicorp","Microsoft","Netflix"]},{"id":"q-272","question":"How would you implement a DRY Terraform configuration using Terragrunt and Atlantis for multi-environment deployments?","channel":"terraform","subChannel":"best-practices","difficulty":"intermediate","tags":["dry","terragrunt","atlantis"],"companies":["Amazon","Google","Netflix","Stripe"]},{"id":"q-284","question":"Design a production-grade Terraform architecture for a multi-environment AWS infrastructure with 100+ resources, including state management, CI/CD integration, and security controls. How would you handle state locking, workspace strategy, and deployment validation?","channel":"terraform","subChannel":"best-practices","difficulty":"advanced","tags":["infrastructure-as-code","automation","best-practices"],"companies":["Amazon","Google","Hashicorp","Microsoft","Netflix","Snowflake"]},{"id":"q-1049","question":"In a multi-account AWS setup, a core Terraform module is versioned in a private registry and consumed by 12 workspaces. A regional failover requires a safe rollback to the previous core module version without drift. Describe the end-to-end strategy, including version pinning, CI validation, and state/rollback mechanisms?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["Citadel","Goldman Sachs","Lyft"]},{"id":"q-1197","question":"In a multi-account AWS setup, a single Terraform repo provisions VPCs and IAM roles per environment using provider aliases. A governance rule requires per-environment tagging and automatic drift detection that blocks non-Terraform changes. Describe a concrete pattern to enforce per-account isolation, tagging, and drift guardrails, including provider aliasing, remote state per environment, and a PR-based drift test workflow?","channel":"terraform","subChannel":"general","difficulty":"intermediate","tags":["terraform"],"companies":["Citadel","Twitter"]},{"id":"q-1271","question":"You have a Terraform project that provisions an AWS VPC and a small app stack. You want developers to run the same config against their own environments using per-environment secrets (DB_PASSWORD, APP_SSH_KEY) that are never stored in git. Outline a minimal structure (files, vars, and commands) to supply these secrets safely, and explain how you prevent secrets from triggering plan changes or drift?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["LinkedIn","Microsoft","Twitter"]},{"id":"q-1615","question":"You have a VPC with public and private subnets and an Internet Gateway. You want to optionally provision a NAT Gateway in the public subnet based on a boolean var create_nat_gateway (default true). How would you implement conditional creation of the Elastic IP, NAT Gateway, and the private route to 0.0.0.0/0 using Terraform 0.12+ syntax? Explain how you handle plan stability for existing deployments when the flag toggles?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Meta","Plaid","Snap"]},{"id":"q-1666","question":"You manage a Terraform project that already provisions a VPC with a public subnet, a private subnet, and an EC2 instance in the private subnet via a NAT Gateway. Add a feature flag to optionally create an RDS instance in the private subnet, but only when var.create_rds is true. Ensure running 'terraform apply' in non-prod environments does not touch the RDS resource. Describe the exact Terraform changes you would make, including the variable declaration, the RDS resource, and any dependencies, with a minimal disruption?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["NVIDIA","Two Sigma"]},{"id":"q-2099","question":"Create a minimal Terraform setup using provider aliases for Cloudflare and IBM Cloud to provision a single IBM Cloud VM and a Cloudflare A record for example.com, such that the A record always points to the VM's public IP. Outline folder structure, how to reference outputs, and how updates occur with no downtime?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Cloudflare","IBM","Stripe"]},{"id":"q-2255","question":"Design a scalable onboarding workflow for per-tenant environments in Terraform that provisions AWS VPCs via a shared module while giving each tenant an isolated Terraform Cloud workspace and remote state. Explain how you'd enforce per-tenant tagging, IAM least privilege, and network boundaries, and how you handle tenant retirement with drift-aware teardown?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["Discord","Microsoft","Snap"]},{"id":"q-2330","question":"You have a Terraform project that currently stores state locally; describe how you would switch to an S3 backend with DynamoDB locking, migrate the existing state safely, and adjust team workflows to prevent drift during the transition?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Citadel","Databricks","Meta"]},{"id":"q-2499","question":"In a Terraform project spanning IBM Cloud resources with a shared network module across three environments dev staging prod, implement per environment state isolation and drift aware deployments. Describe a backend strategy using remote backends per environment, provider aliases for IBM Cloud, and CI gates with Open Policy Agent that block applies when drift is detected or policies fail. Include concrete backend config and a minimal drift check approach?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["IBM","Meta"]},{"id":"q-2626","question":"You have a Terraform repo that uses a private git module for VPC networking across two environments. A new module tag was pushed, but CI fails due to registry access. Explain how you'd pin module versions, add a local fallback path, and validate with 'terraform init' and 'terraform plan' for both environments. Provide a minimal config snippet showing module source and version pinning?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Microsoft","Snap","Uber"]},{"id":"q-3029","question":"You maintain two Terraform repos: network (AWS VPC and subnets) and app (ECS service). The app module consumes VPC outputs via terraform_remote_state from network. Upstream changes to the network (CIDR or subnets) would break app networking. Describe a practical workflow to ensure per-environment isolation, safe cross-repo state access, and drift-free promotions, including a concrete backend config and a data source usage sketch?","channel":"terraform","subChannel":"general","difficulty":"intermediate","tags":["terraform"],"companies":["DoorDash","Snap","Twitter"]},{"id":"q-3080","question":"You're consolidating two Terraform environments (prod and non-prod) into a single repo with a shared VPC module. You want true per-environment state isolation and safe promotions to prod via PR gates. Describe a practical workflow: (1) concrete backend config per environment (S3 + DynamoDB locking), (2) provider aliasing and environment-specific backend selection, (3) how CI gates enforce a plan+policy check before applying to prod, and (4) how to share outputs without touching prod state?","channel":"terraform","subChannel":"general","difficulty":"intermediate","tags":["terraform"],"companies":["Anthropic","Slack","Tesla"]},{"id":"q-3098","question":"You're tasked with gradually adopting Terraform for a prod AWS VPC that currently has hundreds of resources not managed by Terraform. Describe a concrete, incremental plan: per-env backends, an import workflow for the VPC and subnets, state moves into a central module, drift checks, and gating before apply. How would you implement this end-to-end?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["Airbnb","Scale Ai","Tesla"]},{"id":"q-3201","question":"You manage two Terraform repos (network and app) and must add a new environment (dev) with per-environment state isolation using a single backend and minimal code changes. Describe a beginner-friendly plan to implement this, including backend key layout, a wrapper module for env context, and a GitHub Actions gate that runs 'terraform plan' for dev and fails if plan contains changes outside a defined whitelist. Include concrete backend config snippet and a basic automation sketch?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Netflix","Scale Ai"]},{"id":"q-3233","question":"In a Terraform Cloud/Enterprise setup with dozens of teams and modules, design a policy-as-code gated workflow to enforce security and cost constraints across all workspaces. Explain how you would structure policy packs (Sentinel or OPA), gate per environment, handle computed attributes in plans, and provide a concrete policy snippet that blocks public S3 buckets and enforces KMS CMKs. How would you test and rollback?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["Coinbase","Tesla","Uber"]},{"id":"q-3256","question":"You're building a tiny Terraform setup to provision an AWS S3 bucket for static assets and a DynamoDB table for locking, using a single backend. Describe a beginner-friendly plan that includes: (1) a local backend snippet for development, (2) a small reusable module that exposes bucket_name and locks_table_arn, and (3) a gating workflow (CI) that runs fmt, validate, and plan, failing if bucket-level changes are attempted outside the module?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Goldman Sachs","OpenAI","Stripe"]},{"id":"q-3272","question":"You maintain a Terraform module that provisions an AWS S3 bucket with an optional versioning flag (var enable_versioning bool). Propose a beginner-friendly plan to add automated tests using Terratest or kitchen-terraform to verify the bucket exists, versioning toggles correctly, and a policy is attached. Outline the minimal test layout and CI steps to run tests?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Databricks","Tesla"]},{"id":"q-3311","question":"Scenario: a small Terraform repo provisions an AWS S3 bucket and an IAM role. You want to enforce naming conventions and required tags across environments without duplicating code: use variable validation for bucket_name and a module-level requirement for Environment/Owner tags. In CI gate on 'terraform plan' and run a lightweight script to verify tags exist on all resources. Provide minimal code snippets and a CI sketch?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Citadel","Slack"]},{"id":"q-3342","question":"Design a Terraform plan for a multi-cloud messaging layer provisioning AWS SQS and GCP Pub/Sub in prod and staging using a single shared module with provider aliases and per-env backends. Describe provider wiring, module boundaries, state, and drift/policy checks (OPA/Sentinel). Include a minimal code sketch for both resources in one module and a CI gate that blocks apply unless the plan matches an approved whitelist?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["Adobe","Airbnb","Meta"]},{"id":"q-3368","question":"Design a Terraform CI/CD plan for a multi-account AWS setup with two modules (network and apps) and three environments (dev, staging, prod). Use per-env remote backends (S3+DynamoDB), provider aliases for cross-account refs, and a gating policy (OPA) to enforce allowed regions and mandatory tags. Include backend snippet, an OPA policy example, and a minimal GitHub Actions step that runs 'terraform plan -out=plan.tfplan' and validates against the policy before permitting merge. Provide concrete details?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["DoorDash","NVIDIA","Zoom"]},{"id":"q-3437","question":"Build a Terraform cross-account deployment provisioning an AWS EKS cluster in dev and prod. Implement a policy-as-code gate that blocks plans creating unencrypted EBS volumes or privileged pods, and add drift checks across accounts. Explain provider aliases and module boundaries, and provide a concrete OPA policy snippet enforcing encryption and non-privileged contexts; outline CI steps for plan + policy execution?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["PayPal","Scale Ai","Uber"]},{"id":"q-3531","question":"Design a practical, scalable approach to manage Terraform for a multi-tenant SaaS on AWS where each tenant's resources (VPC, subnets, services) live in separate accounts but state is centralized in a single backend. Provide (a) a reusable tenant module with per-tenant provider aliases and a backend key derivation, (b) a wrapper that injects tenant context, (c) CI gating that rejects plans touching resources outside the tenant, and (d) drift checks and promotion workflow. Include concrete backend config snippet and a sketch of the wrapper module?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["LinkedIn","Netflix","Slack"]},{"id":"q-3624","question":"You're consolidating two AWS accounts (prod and sandbox) into a single Terraform repo with minimal code changes. Outline a beginner-friendly plan to achieve per-account state isolation using a single backend. Include provider aliases, a wrapper module for env context, per-account backend keys or workspaces, and a CI gate that runs plan for sandbox and blocks changes targeting prod resources?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Amazon","Meta","Snowflake"]},{"id":"q-3638","question":"You're deploying a small fleet of EC2 instances into an existing AWS VPC that is not Terraform-managed. Describe a beginner-friendly plan to reference the VPC and its subnets via data sources, place the resources under a stable module boundary, and enable per-environment isolation with a single backend. Include concrete data source usage, a minimal wrapper module, and a CI gate that only allows changes to compute resources, failing if the plan touches VPC or subnets?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["NVIDIA","OpenAI","Scale Ai"]},{"id":"q-3903","question":"You're tasked with building a reusable Terraform module that provisions an Aurora Global Database across multiple AWS regions: a primary region and at least one secondary region. Describe how you'd design provider aliases, per-environment state isolation, and cross-region dependencies. Include how you'd implement safe updates, drift checks, and CI gating with Terratest or kitchen-terraform. Provide minimal backend and provider snippets and outline the testing plan?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["Hugging Face","Square"]},{"id":"q-4154","question":"Design a beginner-friendly Terraform module that provisions a minimal network for either AWS (VPC) or Azure (VNet) using the same interface. Describe input shapes (region, environment, tags), provider aliasing, and conditional resources to support both clouds with a single repo. Include minimal cloud-specific examples and a test plan for parity?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Airbnb","PayPal"]},{"id":"q-4179","question":"Design a practical plan to migrate three Terraform repos to a single Terraform Cloud backend with per-env workspaces (dev, staging, prod) while keeping per-repo modules. Include workspace structure, backend config sketch, a CI gate for plan approval, and a minimal policy snippet that blocks prod deployments if drift is detected or there are non-prod changes pending?","channel":"terraform","subChannel":"general","difficulty":"intermediate","tags":["terraform"],"companies":["Hashicorp","Robinhood","Slack"]},{"id":"q-4219","question":"In a multi-tenant deployment, a single Terraform module provisions VPC, IAM roles, and Lambda permissions used by multiple environments via separate Terraform workspaces and a central backend. Upstream changes to module inputs can break downstream environments. Describe a practical workflow to isolate environments, pin provider versions, and enforce drift/upgrade gates, including: 1) backend/key layout per env, 2) provider version constraints and a data source pattern to fetch shared config, 3) a CI gate that blocks upgrades outside a whitelist, and 4) a minimal snippet showing required_providers constraints and a data source?","channel":"terraform","subChannel":"general","difficulty":"intermediate","tags":["terraform"],"companies":["Airbnb","Meta"]},{"id":"q-4259","question":"In a Terraform-managed deployment across two AWS accounts and three regions, describe a canary-based rollout pattern for a new service version. How would you structure modules, per-region backends, and CI gates to deploy first to a canary slot, validate with health checks, and progressively promote—while ensuring drift detection and safe rollback if the canary fails?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["DoorDash","Snap","Two Sigma"]},{"id":"q-4496","question":"You manage a shared Terraform repo across multiple AWS accounts that provisions many S3 buckets. A strict security policy requires every bucket to have server-side encryption with a KMS key and to block public access, with no exceptions. Propose a concrete, end-to-end approach to enforce this policy at plan and apply time, including: 1) a guardrail mechanism (OPA, custom plan checks, or Terraform validations), 2) how to integrate it into CI/CD with a gate that fails on non‑compliance and reports actionable findings, 3) how you handle exceptions and drift, 4) a minimal code example and pipeline snippet to illustrate your approach?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["Databricks","Goldman Sachs"]},{"id":"q-4520","question":"You're managing two Terraform repos: network (VPC/subnets) and app (container service). A new requirement is for a centralized monitoring stack to consume VPC/subnet IDs as read-only data from the network repo without creating deployment coupling, while dev/prod remain isolated. Describe a concrete implementation plan detailing per-environment backends, remote state data sources to feed app and monitoring repos, and a CI gate to enforce a whitelist of plan changes before promotion?","channel":"terraform","subChannel":"general","difficulty":"intermediate","tags":["terraform"],"companies":["Plaid","Snap"]},{"id":"q-4602","question":"You're centralizing AWS Transit Gateway management in Terraform across three accounts (dev, stage, prod). A core TG lives in the network account and each environment attaches its VPCs via separate modules. Changes to attachments must be safe, idempotent, and avoid cross-env drift. Propose a concrete strategy using provider aliases, a shared core module, per-env wrappers, and a deterministic create_before_destroy sequence. Include a minimal code snippet showing alias usage and a dependent attachment?","channel":"terraform","subChannel":"general","difficulty":"intermediate","tags":["terraform"],"companies":["MongoDB","Tesla","Zoom"]},{"id":"q-4647","question":"You're building a Terraform platform that spans AWS and GCP with per-environment state isolation using a single central backend and a policy gate to block prod-destructive changes. Describe the architecture, including provider aliases, cross-cloud data sharing via outputs, drift detection, and a gating CI step that runs init, plan -out, and apply only after approval. Include a concrete backend config snippet and a guardrail rule example?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["Bloomberg","DoorDash","Twitter"]},{"id":"q-4725","question":"You're maintaining a Terraform multi-cloud foundation (AWS, GCP, IBM) where teams deploy per-cloud resources independently but governance must prevent cross-cloud CIDR overlaps and ensure auditable changes. Describe the architecture: module boundaries, provider aliases, backends, and a policy gate (Sentinel or OPA) that runs on plan, including concrete data sources for existing networks and a sample gating rule?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["Amazon","Google","IBM"]},{"id":"q-479","question":"You're managing a multi-region infrastructure with 50+ Terraform modules. How would you design a strategy to handle state locking, drift detection, and safe deployments across regions while minimizing downtime?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["Microsoft","Uber"]},{"id":"q-4809","question":"You're maintaining a private Terraform registry with two modules: network (VPC/subnets) and app (service). A new requirement is to publish a read-only data-sharing module that consumes network outputs without deployment coupling, while keeping dev/prod isolated via per-env backends. Describe a concrete plan: (a) backend layout for environments, (b) data sharing approach (registry-driven module outputs vs remote state) for app and a new observability repo, and (c) a CI gate that enforces a whitelist of plan changes before promotion. Include backend config snippets and example data usage?","channel":"terraform","subChannel":"general","difficulty":"intermediate","tags":["terraform"],"companies":["Apple","Oracle"]},{"id":"q-4815","question":"Design a multi-account Terraform baseline across dev, stage, and prod using a shared-services account for read-only data. Explain per-env backends, a wrapper module, cross-account data sharing via data sources, drift detection, and a CI gate with a whitelist before apply. Provide a minimal repo layout and concrete code snippets for backend config and data access?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["Netflix","Salesforce"]},{"id":"q-508","question":"You have a Terraform configuration that creates multiple EC2 instances across different availability zones. How would you implement a blue-green deployment strategy using Terraform workspaces and what are the key considerations?","channel":"terraform","subChannel":"general","difficulty":"intermediate","tags":["terraform"],"companies":["Citadel","PayPal","Scale Ai"]},{"id":"q-563","question":"You're deploying a simple web application using Terraform. How would you create an AWS EC2 instance with a security group that allows HTTP traffic on port 80?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Coinbase","Discord","Slack"]},{"id":"q-592","question":"How would you use Terraform variables to manage different environments (dev/staging/prod) while keeping your configuration DRY?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Oracle","Snowflake","Stripe"]},{"id":"q-854","question":"In a Terraform Cloud setup spanning AWS and GCP, you must enforce a cross-cloud policy: every resource must carry a non-empty 'cost-center' tag and new regions must not auto-create default VPCs. How would you implement drift detection, policy gating, and automatic remediation across workspaces without downtime?","channel":"terraform","subChannel":"general","difficulty":"intermediate","tags":["terraform"],"companies":["Amazon","Discord","Google"]},{"id":"q-983","question":"In a Terraform project that provisions an AWS S3 bucket, add a new boolean variable enable_sse to toggle server-side encryption; when enable_sse is true, the bucket should have server-side encryption AES256 enabled. How would you implement this in the bucket resource using Terraform 0.12+ syntax, ensuring existing deployments remain stable and the plan doesn't force unnecessary changes?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Microsoft","Tesla"]},{"id":"gh-105","question":"What is Infrastructure Drift and how do you detect and prevent it?","channel":"terraform","subChannel":"state-management","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Stripe"]},{"id":"q-175","question":"You have a Terraform configuration with multiple developers working on the same infrastructure. How would you implement remote state locking to prevent state corruption and enable team collaboration?","channel":"terraform","subChannel":"state-management","difficulty":"intermediate","tags":["state","backend"],"companies":["Amazon","Google","Microsoft","Stripe","Uber"]},{"id":"q-221","question":"How would you implement a zero-downtime blue-green deployment strategy using Terraform workspaces, remote state locking, and Atlantis for production-scale microservices?","channel":"terraform","subChannel":"state-management","difficulty":"advanced","tags":["dry","terragrunt","atlantis"],"companies":["Amazon","Google Cloud","Microsoft","Stripe","Uber"]},{"id":"q-247","question":"How does Terraform remote state prevent conflicts when multiple team members work on the same infrastructure, and what are the key mechanisms involved?","channel":"terraform","subChannel":"state-management","difficulty":"beginner","tags":["remote-state","locking","workspaces"],"companies":["Amazon","Hashicorp","Microsoft","Netflix","Stripe"]},{"id":"q-1009","question":"Scenario: a single repo provisions per-tenant AWS resources for many tenants. To isolate state without workspaces, use a shared S3 backend with a DynamoDB lock table and a tenant-scoped key. Describe the backend config (bucket, region, dynamodb_table, key pattern per tenant), how you detect drift across tenants, and CI gating for dev-to-prod promotions (plan -out, tests, approve, apply)?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Amazon","Coinbase","Databricks"]},{"id":"q-1154","question":"In a Terraform module that provisions an AWS RDS instance, operators sometimes alter maintenance_window directly in AWS, creating drift. You want Terraform to ignore external changes to maintenance_window but still apply code-driven updates (e.g., allocated_storage). How would you implement this using a lifecycle block? Provide a minimal aws_db_instance snippet showing ignore_changes for maintenance_window and outline tradeoffs?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Salesforce","Scale Ai","Tesla"]},{"id":"q-1172","question":"In AWS us-east-1, you must provision 3 private subnets in a single VPC across 3 AZs using one module. Define a map variable with AZs and CIDRs, create the subnets with for_each, attach appropriate tags, and output the subnet IDs. Then show how to reference these IDs in a resource that requires subnet_id (eg NAT Gateway) and justify for_each vs count?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Netflix","Oracle"]},{"id":"q-1202","question":"Scenario: you must bring under Terraform management a set of existing AWS S3 buckets across teams. Some buckets already exist and must be imported; you will manage encryption and versioning with a single module using for_each over a bucket map. How would you import, structure, and promote changes safely via CI?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Amazon","NVIDIA","Robinhood"]},{"id":"q-1338","question":"Scenario: You maintain a Terraform repo that provisions an AWS RDS primary in us-east-1 and an optional cross-region read replica in us-west-2 controlled by a feature flag var.enable_replica. If the flag is false, Terraform should destroy the replica on apply. How would you implement the cross-region provider, conditional resource creation, and safe destruction without leaving dangling resources? Provide code patterns for the provider alias and the replica resource?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Hugging Face","NVIDIA","Netflix"]},{"id":"q-1367","question":"You\\'re managing 100 AWS subnets defined in a single Terraform module using for_each. A drift occurred in one subnet\\'s route_table_id because it was changed outside Terraform. Describe exactly how you\\'d detect the drift and fix only that resource with zero-drift impact, using Terraform CLI commands. Include the exact commands to init plan, taint the resource, apply with -target, and re-run a full plan to confirm drift-free state?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Databricks","IBM","Meta"]},{"id":"q-1584","question":"You are refactoring a Terraform repo and moving an existing AWS S3 bucket resource from the root module into a submodule named storage. Describe exact steps and commands to relocate the resource in state without recreation, including the state mv addresses and a follow-up plan?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Coinbase","Instacart","Zoom"]},{"id":"q-1655","question":"You discover an AWS VPC and related resources created outside Terraform and want to bring them under a new network module without recreating. Describe exact steps to import the VPC, its subnets (for_each), and a peering connection into the module, including the resource addresses you’ll use, how to handle multiple subnets, and how to verify with a plan that nothing changes?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Lyft","PayPal"]},{"id":"q-1740","question":"Advanced cross-repo Terraform: network state is stored in a dedicated repo/backends across envs. The application repo imports VPC IDs and subnets via data terraform_remote_state. Explain how you would structure backends and modules to avoid drift, ensure isolation between dev/staging/prod, and orchestrate safe promotions from dev to prod with exact commands for init/plan/apply per environment. Include how you would test drift and rollback?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Adobe","Databricks"]},{"id":"q-1749","question":"You manage a single Terraform repo deploying a shared 'network' module into two AWS accounts (prod and dev) using provider aliases. Describe how you structure the provider blocks, module calls, and var-files to deploy both environments without duplicating code. Include exact commands to init, plan, and apply for each environment, ensuring isolation and no cross-env state changes?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Robinhood","Slack","Snap"]},{"id":"q-1805","question":"How would you implement input validation in a Terraform module to enforce that a variable instance_count is between 1 and 5 and that a string region is one of an allowed set? Provide the approach and expected Terraform behavior during plan and apply?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Netflix","Square","Tesla"]},{"id":"q-1836","question":"Scenario: A Terraform repo uses a module named network that creates subnets via for_each over a map of subnet_id to name. Three subnets were created manually (subnet-aaa, subnet-bbb, subnet-ccc) and must be brought under Terraform control without changing the rest. Describe exact steps and commands to import only these existing subnets into the module state, update the for_each map to include their IDs, and verify drift with a full plan. Include init/plan/apply commands and how to handle dependent resources (route tables, tags)?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["NVIDIA","Plaid"]},{"id":"q-1888","question":"Scenario: You manage 120 Cloudflare DNS records in a single Terraform module with for_each. A drift occurred in one record's IP address due to a manual edit in Cloudflare. Describe a surgical plan to detect drift and fix only that resource with zero-drift impact, using Terraform CLI commands. Include the exact commands to init, plan, taint the resource, apply with -target, and re-run a full plan to confirm drift-free state?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Citadel","Microsoft","Plaid"]},{"id":"q-1948","question":"You’re extending a VPC Terraform module to support both a managed NAT Gateway and a NAT instance via a single input variable named nat_type with default 'gateway'. How would you implement this with conditional resources, including the variable, two resource blocks, and outputs? Provide the minimal code and explain how you’d validate with terraform plan to ensure no changes when nat_type remains 'gateway'?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Google","PayPal","Twitter"]},{"id":"q-1976","question":"In a monorepo, a Terraform module provisions a multi-tenant AWS VPC setup with per-tenant isolation via tenant_id. A new tenant must be added without touching existing ones. Describe exact steps and commands to add the tenant using a fresh state segment, ensure no drift to others, and verify with targeted and full plans. Include backend-key organization and apply sequencing?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Goldman Sachs","LinkedIn"]},{"id":"q-2037","question":"You're provisioning a private API gateway via a custom Terraform provider. Implement a blue/green deployment pattern with two gateway instances and an atomic traffic switch using a single Terraform plan. Describe the exact structure (for_each, modules, and a central traffic_split map), how to prevent unintended recreation, and the precise steps and commands to drift-check and promote from blue to green in prod with minimal blast radius?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Netflix","PayPal"]},{"id":"q-2049","question":"You maintain a per-environment security module that provisions an AWS Security Group with environment-scoped rules (dev, stg, prod). An external admin altered prod inbound 22 to 0.0.0.0/0, causing drift. Describe exact steps to detect the drift and fix prod only, using Terraform CLI with init/plan/apply and a -target run, then re-run a full plan to confirm no drift. Provide the exact resource address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Meta","Tesla"]},{"id":"q-2079","question":"In a Terraform repo using a single VPC module across AWS prod and canary via provider aliases, a CIDR change would recreate many subnets. Describe a blue/green deployment strategy that updates production with zero downtime. Include exact commands to initialize, create/apply canary resources with targeted -target, run drift checks, then migrate canary state to prod with terraform state mv and finalize with a full plan?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Bloomberg","Netflix"]},{"id":"q-2127","question":"You manage a Terraform repo with a Google Cloud IAM bindings module using for_each over projects. A drift occurred when a member was removed in prod outside Terraform. Describe exact steps to detect the drift and fix only prod binding using Terraform CLI with init/plan/apply and a -target run, then re-run a full plan to confirm no drift. Include the exact resource address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Adobe","Google","Slack"]},{"id":"q-2168","question":"You find an AWS IAM role that was created outside Terraform and you want to bring it under Terraform control without changing its attached policies—what exact steps and commands would you use to import it, reconcile the state, and apply only the intended configuration?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Adobe","Cloudflare","Coinbase"]},{"id":"q-2212","question":"Scenario: You manage a single Terraform module that provisions a Google Cloud DNS managed zone with multiple A records across environments (dev, staging, prod) using a map variable and a for_each on google_dns_record_set. A drift occurred in prod: the IP of prod A record was manually changed outside Terraform. Describe exact steps to detect drift and fix only prod's A record using Terraform CLI with init/plan/apply and a -target run, then re-run a full plan to confirm drift-free state. Include the exact resource address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Databricks","Meta","NVIDIA"]},{"id":"q-2238","question":"Scenario: A Terraform module manages an AWS VPC with subnets created outside Terraform. The module uses an aws_vpc.main and a for_each map aws_subnet.subnets keyed by environment (prod, staging). Provide exact commands to import the existing VPC and each subnet without recreation, including their addresses, then run terraform plan to confirm no drift and describe handling if drift is detected?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Hashicorp","Snowflake"]},{"id":"q-2337","question":"You maintain a Terraform repo that builds a multi-account AWS network using a root module with per-account submodules. In prod, a single security group rule was manually changed outside Terraform. Describe exact steps to detect drift, fix only prod with a targeted apply, and then re-run a full plan. Include the exact resource address to target and how to ensure prod drift is isolated from other accounts?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Meta","Zoom"]},{"id":"q-2345","question":"A Terraform repo provisions a VPC, subnets, and an EC2 instance. A central tagging policy is required via a local default_tags map and an optional input var.tags, applied to all resources without duplicating tags. Describe exact steps to implement this, including how to merge tags in each resource, how to handle resources that cannot be tagged, and how to validate with a full plan before apply. Include a minimal code snippet showing how to declare the variables and apply tags to the VPC and EC2 instance?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Hashicorp","LinkedIn"]},{"id":"q-2470","question":"You find an existing Google Compute Engine instance named \\\"web-1\\\" that wasn't created by Terraform but should be managed by the Terraform module at path modules/infra with a for_each over a map of instances. Describe exact steps to import that instance into state, including the resource address you would use for the import, what to add to the code to match the real-world instance, and how you verify drift with a full terraform plan after the import (no recreation)?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Google","MongoDB"]},{"id":"q-2563","question":"You manage a Terraform repo deploying a shared AWS network module across three accounts with for_each over accounts to create security groups. In account \"acct-b\" a security group rule’s CIDR was manually changed outside Terraform. Describe exact steps to detect drift and fix only that resource with zero-drift impact, using Terraform CLI init/plan -detailed-exitcode, then a -target run, and finally a full plan. Include the exact resource address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Amazon","Meta","Twitter"]},{"id":"q-2610","question":"In a Terraform repo with two modules, networking (producing vpc_id and public_subnet_ids as outputs) and app (consuming them), configure and use a remote state data source to read networking's outputs from a remote backend and reference them in app. Provide exact code for data 'terraform_remote_state' and show how to use outputs.vpc_id and outputs.public_subnet_ids in the app module. Assume AWS S3 backend?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Apple","Databricks","Slack"]},{"id":"q-2800","question":"You maintain a Terraform repo that provisions a shared VPC, subnets, and compute instances for a LinkedIn-like service. You want to deploy dev and prod from the same codebase with isolated state using Terraform workspaces and a remote S3 backend. Describe exact backend config, workspace setup, and the precise command sequence to initialize, create/select the dev and prod workspaces, and apply environment-specific var-files without state leakage. Include sample backend config and commands?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["LinkedIn","Lyft"]},{"id":"q-2854","question":"You're managing a Terraform repo that provisions GCP organization policies across 30 projects using google_organization_policy with for_each on project IDs. In prod, a policy constraint was loosened manually for one project. Describe exact, repeatable steps to detect this drift, identify the impacted project, and fix only that project's policy using Terraform CLI (init/plan/apply) with a targeted -target, followed by a full plan to verify no drift. Include the exact resource address you would target?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Google","PayPal","Twitter"]},{"id":"q-2874","question":"Context: You deploy Snowflake warehouses via Terraform provider. In prod, a manual change increased max_cluster_count from 4 to 8. How would you detect drift and fix only prod using Terraform CLI (init, plan, apply) with a -target run, then re-run a full plan to confirm drift-free state? Include the exact resource address to target: snowflake_warehouse.prod_warehouse[\\\"prod\\\"]?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Google","Snowflake"]},{"id":"q-2946","question":"You manage a Terraform repo deploying a multi-region VPC via a network module applied with for_each over regions. In eu-west-1 a route in the VPC's route_table was manually changed outside Terraform. Describe exact steps to detect drift and fix only that region's route resource with zero-drift impact, using Terraform init/plan -detailed-exitcode, then terraform apply -target=module.network[\"eu-west-1\"].aws_route_table.this, and a final full plan. Include the exact resource address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Adobe","Anthropic","Cloudflare"]},{"id":"q-3035","question":"Across three AWS accounts (dev, staging, prod), a shared VPC module creates per-account security groups with for_each over accounts and a nested for_each over rules. In prod, an inbound rule for 203.0.113.0/24 on TCP 22 was removed via manual change. Describe exact steps to detect the drift and fix only prod binding using Terraform CLI with init/plan/apply and a -target run, then re-run a full plan to confirm no drift. Include the exact resource address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Amazon","Meta","Salesforce"]},{"id":"q-3105","question":"Scenario: A Terraform repo manages multi-region Kubernetes clusters via a module using for_each over regions. In prod, a Kubernetes deployment manifest (kubernetes_manifest) was manually modified outside Terraform, creating drift only in prod. Describe exact steps to detect drift with terraform plan -detailed-exitcode, fix only prod by applying a targeted change (terraform apply -target=module.k8s[\"prod\"].kubernetes_manifest[\"app-deploy\"]), then run a full plan to confirm no drift. Include the exact resource address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Bloomberg","MongoDB","PayPal"]},{"id":"q-3278","question":"Context: Terraform repo deploys Azure Linux VMs across regions via a module with for_each over regions. In prod, a single VM size was manually changed outside Terraform, causing drift for prod only. Describe exact steps to detect drift, identify prod VM, and fix only prod VM with a targeted change, including the exact resource address to target (e.g., module.compute[\\\"prod\\\"].azurerm_linux_virtual_machine[\\\"web01\\\"]). Also outline verification with a final full plan?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Airbnb","Databricks"]},{"id":"q-3334","question":"In a multi-env AWS deployment, a data source feeds a dynamic IP allowlist used by aws_security_group_rule resources. A drift occurs in prod when the allowlist updates. Explain how to implement lifecycle { replace_triggered_by = [data.external.allowlist.version] } in prod rules to force replacement only on allowlist changes, and show exact commands to detect drift (terraform plan -detailed-exitcode) and apply a targeted change with terraform apply -target=module.network[\"prod\"].aws_security_group_rule[\"sg-prod-https\"], then re-run a full plan to confirm no drift. Include an address pattern example?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["MongoDB","Netflix"]},{"id":"q-3407","question":"You're maintaining a Terraform repo that uses a top-level module with for_each over environments (prod, staging) and two provider aliases (aws.prod, aws.staging). In prod, a security group rule allowing 0.0.0.0/0 ingress on port 22 was changed manually. Describe exact steps to detect drift and fix only prod drift using Terraform CLI: init, plan -detailed-exitcode, apply -target=module.network[\"prod\"].aws_security_group.this, then a full plan to confirm no drift. Include the exact resource address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Anthropic","Databricks","Hashicorp"]},{"id":"q-3578","question":"Scenario: A Terraform repo provisions cross-account AWS IAM roles via a module named network using for_each over accounts. In prod, an IAM role tenant_access exists in AWS but drift is detected due to a manual policy change. Describe exact steps to detect drift with terraform plan -detailed-exitcode, then fix prod by importing the drifted resource into state and applying a targeted update using terraform apply -target=module.network[\"prod\"].aws_iam_role[\"tenant_access\"] followed by a full plan. Include the exact resource address to target: module.network[\"prod\"].aws_iam_role[\"tenant_access\"]?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Cloudflare","Databricks","Goldman Sachs"]},{"id":"q-3613","question":"In a Terraform repo that uses a module named network and for_each to create per-environment NAT Gateways across prod and staging, the prod NAT Gateway was created manually outside Terraform. Describe exact steps to detect drift with terraform plan -detailed-exitcode, import the prod NAT Gateway into the correct module address to bring it under Terraform control, run a targeted apply to refresh state, and then run a full plan to ensure no drift remains. Include the exact resource address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Discord","PayPal"]},{"id":"q-3727","question":"In a Terraform repo using module network with for_each over environments and provider aliases (aws, azurerm) to manage multi-cloud resources, prod Azure Virtual Network azurerm_virtual_network.prod_vnet exists outside Terraform. Describe exact steps to detect drift with terraform plan -detailed-exitcode, import the drifted resource into state, then run a targeted apply to bring prod in line (terraform apply -target=module.network[\"prod\"].azurerm_virtual_network[\"prod_vnet\"]), followed by a full plan. Include the exact address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Oracle","Square"]},{"id":"q-3848","question":"Scenario: A Terraform repo uses a module named dns with for_each over domains to manage Cloudflare DNS records. In prod, a CNAME record api_prod.example.com was manually added outside Terraform, causing drift. Describe exact steps to detect drift with terraform plan -detailed-exitcode, fix prod by importing the drifted Cloudflare record into the state under module.dns[\"prod\"], then run a targeted apply to refresh: terraform apply -target=module.dns[\"prod\"].cloudflare_record[\"api_prod\"], followed by a full plan to confirm no drift. Include the exact resource address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Instacart","Netflix","Square"]},{"id":"q-3963","question":"Scenario: A Terraform repo uses a module named database with for_each over environments to create AWS RDS instances. In prod, an RDS instance was manually modified outside Terraform. Describe exact steps to detect drift with terraform plan -detailed-exitcode, import the drifted RDS into state, run a targeted apply to bring prod back in line (terraform apply -target=module.database[\\\"prod\\\"].aws_db_instance[\\\"prod\\\"]), then run a full plan to confirm no drift. Include the exact address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["LinkedIn","Oracle","Twitter"]},{"id":"q-3992","question":"Scenario: A Terraform repo uses a Cloudflare module with for_each over environments (prod, staging, dev) to manage DNS records. In prod, a DNS A record for webapp.example.com was created manually outside Terraform. Describe exact steps to detect drift with terraform plan -detailed-exitcode, import the drifted record into state at the correct address, run a targeted apply to bring prod in line (terraform apply -target=module.cloudflare[\"prod\"].cloudflare_record[\"webapp\"]), and then run a full plan to verify no drift. Include the exact resource address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Apple","Cloudflare","Google"]},{"id":"q-4047","question":"Scenario: A Terraform repo uses a module named cloud-network with for_each over environments to manage Google Cloud firewall rules. In prod, firewall rule prod-allow-internal was manually changed outside Terraform. Describe exact steps to detect drift with terraform plan -detailed-exitcode, fix prod only via a targeted apply (module.cloud-network[\"prod\"].google_compute_firewall[\"prod-allow-internal\"]), and then run a full plan to confirm no drift. Include the exact resource address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Amazon","Google"]},{"id":"q-4101","question":"A Terraform repo uses module storage with for_each over environments (dev, prod) to create S3 buckets in AWS. Prod uses a separate AWS account via provider alias aws.prod. Describe exact steps to configure provider aliases, assign the correct provider to each env, and validate changes with plan/apply so prod changes are isolated?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Adobe","Coinbase","Tesla"]},{"id":"q-4117","question":"Scenario: A Terraform repo uses module \"network\" with for_each over environments (prod, staging) to create AWS subnets. In prod, a subnet was renamed outside Terraform from aws_subnet[\"prod_subnet\"] to aws_subnet[\"prod-private\"]. The AWS subnet remains, state holds the old address. Describe exact steps to detect drift, migrate state with terraform state mv and/or import, and a targeted apply to bring prod in line without touching staging. Include the exact addresses to target?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Apple","Meta","PayPal"]},{"id":"q-4275","question":"Scenario: A Terraform repo uses module 'network' with for_each over regions and a prod account provider alias. In prod, an aws_vpc_peering_connection named \\\"prod_to_shared\\\" was created manually outside Terraform. Describe exact steps to detect drift with terraform plan -detailed-exitcode, import the drifted resource into the correct module address (e.g., module.network[\\\"prod\\].aws_vpc_peering_connection[\\\"prod_to_shared\\\"]), perform a targeted apply to bring prod in line, then run a full plan. Include the exact resource address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Airbnb","Meta","Plaid"]},{"id":"q-4291","question":"You want to run dev and prod using a single Terraform repo with a single backend, using workspaces to isolate state. You deploy an AWS S3 bucket and a DynamoDB table for state locking. Describe the minimal steps to structure the code so that bucket name is workspace specific, how to configure backend to use the workspaces, how to switch workspaces, and how to verify that plans apply only to the active workspace?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Databricks","Microsoft","Square"]},{"id":"q-4419","question":"In a Terraform repo with a monitoring module that creates multiple Datadog monitors per environment using a for_each over environments, prod drift exists because a monitor was created outside Terraform. Describe exact steps to detect drift with terraform plan -detailed-exitcode, import the drifted monitor into the module address, run a targeted apply to refresh state, and then run a full plan to confirm no drift. Include the exact resource address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Citadel","Netflix","Salesforce"]},{"id":"q-4771","question":"In a Terraform repo using a single backend with multiple workspaces (dev, staging, prod), prod has drift after a manual firewall rule change in a VPC module. Describe exact steps to detect drift with plan -detailed-exitcode, switch to the prod workspace, import the drifted resource into state if needed, perform a targeted apply to reconcile prod only (provide the exact address to target), then run a full plan to confirm no drift?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Cloudflare","LinkedIn","Tesla"]},{"id":"q-864","question":"In Terraform, you need to manage two AWS accounts in a single repo: prod (provider aws.prod) and audit (provider aws.audit). Create an S3 bucket in prod and a cross-account IAM policy in audit that grants read access to that bucket. How do you configure aliased providers, reference the prod bucket ARN from the audit module, and enforce deterministic apply order (e.g., data fetch before policy) with minimal risk? Include a minimal config outline?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Snap","Snowflake","Square"]},{"id":"q-960","question":"In a single Terraform repo that provisions prod, staging, and dev AWS environments, how would you configure a single S3 backend to isolate state for each environment without using separate Terraform workspaces? Provide the exact backend config (bucket, region, dynamodb_lock_table, key per env) and explain how you would promote changes from dev to prod, including drift handling and CI plan/apply steps?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Bloomberg","Google","Hashicorp"]},{"id":"q-1033","question":"You build a small service that fetches user profiles via GET /api/users/{id} and caches results in memory for 5 minutes. Write concrete tests that verify: (1) a cache miss calls the API, stores the result with TTL; (2) a cache hit returns cached value without API call; (3) TTL expiry triggers a fresh API call to refresh cache. Provide example test code?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["Google","Lyft","Netflix"]},{"id":"q-1076","question":"You implement a debounce utility in frontend code: debounce(fn, wait) returns a wrapper that ensures fn is called at most once per wait ms, using the last invocation's arguments. Write a focused test that demonstrates rapid successive calls do not trigger fn more than once, and that a final call after waiting triggers with the latest args?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["DoorDash","Instacart","Oracle"]},{"id":"q-1287","question":"Design a testing strategy for a real-time data pipeline built with Apache Flink processing millions of events per second, ensuring exactly-once semantics across sources and sinks, handling out-of-order and late data, with stateful operators and checkpointing. Outline how you'd structure unit, integration, and end-to-end tests, simulate late data and failures, verify sink idempotence and recovery guarantees, and specify concrete tools and success criteria?","channel":"testing","subChannel":"general","difficulty":"advanced","tags":["testing"],"companies":["Amazon","Google","Scale Ai"]},{"id":"q-2150","question":"You're deploying a Delta Lake ingestion pipeline on Databricks: a Spark Structured Streaming job reads from a Kafka topic (Avro, Schema Registry), upserts into a partitioned Delta table, and downstream queries rely on the latest state. Design a test plan that (1) proves exactly-once semantics and idempotent upserts under retry/replay, (2) validates safe schema evolution without breaking downstream code, and (3) detects data quality regressions (missing keys, late data) within a 24h window. Include artifacts, environments, and metrics?","channel":"testing","subChannel":"general","difficulty":"advanced","tags":["testing"],"companies":["Databricks","IBM","Scale Ai"]},{"id":"q-2219","question":"Design a practical test plan for a new endpoint '/upload-csv' that accepts CSV files up to 5MB. The CSV must have **headers**; on success, records are inserted into PostgreSQL table `users(id, name, email)` and 201 is returned. Provide concrete unit tests, integration tests, and end-to-end tests. Include edge cases: empty file, invalid header, invalid email, duplicates, and simulate **DB** outage?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["LinkedIn","Snowflake"]},{"id":"q-2419","question":"In a MongoDB-backed user service, introduce an optional field 'lastLogin' (timestamp) to user documents without breaking existing clients. Provide a concrete, beginner-friendly test plan covering: (1) ensuring existing docs are untouched, (2) new docs set lastLogin correctly, (3) queries that filter on lastLogin behave as expected, (4) rollback path if migration fails. Include test data, tooling, and a minimal CI flow?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["IBM","MongoDB","Twitter"]},{"id":"q-2601","question":"You're adding a multilingual contact form to a React app that saves submissions to a REST API. The UI has client-side validation and the API may respond with 400 or 429 errors. Describe a beginner-friendly, concrete test plan to verify correctness, localization, and resilience in CI. Include test types, data sets, and example test cases?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["IBM","Microsoft"]},{"id":"q-2649","question":"You're implementing a background data export feature: when a user clicks Export, the system queues a request to generate a CSV of that user's data and deliver it to a downstream REST API. Propose a beginner-friendly, concrete test plan that covers correctness, idempotency, retry/backoff behavior, and data privacy (redaction of PII). Include test types, data sets, and example test cases?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["Google","IBM","Square"]},{"id":"q-2736","question":"In a multi-service streaming platform, a new feature flag routes 25% of events through a costly enrichment path. Design a practical end-to-end test plan to validate observability: (a) metrics coverage and SLO mapping in Prometheus, (b) canary rollout strategy with Grafana dashboards, and (c) alerting correctness and log correlation using Loki and PagerDuty. Include concrete steps, data generation, and acceptance criteria?","channel":"testing","subChannel":"general","difficulty":"advanced","tags":["testing"],"companies":["Meta","Tesla","Uber"]},{"id":"q-2867","question":"You operate a real-time fraud-detection inference service for an e-commerce platform. It exposes REST/GRPC endpoints, retrieves online features from Feast (Redis) with a model registry, and supports canary rollouts for new models. Design a practical end-to-end test strategy that validates latency under burst, correctness of scored outputs across two model versions, safe canary rollout with rollback, and drift/alerting thresholds. Include concrete data schemas, test data, and CI integration steps?","channel":"testing","subChannel":"general","difficulty":"intermediate","tags":["testing"],"companies":["Google","Microsoft","NVIDIA"]},{"id":"q-2904","question":"Design a cross-layer test plan for a drone flight-control stack with an edge RTOS and cloud advisory service. Use hardware-in-the-loop plus a deterministic flight simulator to replay sensor streams, inject faults (IMU drift, GPS loss, lidar drop), and simulate intermittent network outages. Validate fail-safe states, latency budgets, recovery times, and regulatory-aligned test cases; add chaos tests to confirm resilience?","channel":"testing","subChannel":"general","difficulty":"advanced","tags":["testing"],"companies":["Apple","Tesla"]},{"id":"q-2957","question":"You maintain a Node.js service with endpoint POST /process that selects between algorithm A and B based on a feature flag controlled by an environment variable USE_NEW_ALGO. Design a beginner-friendly, concrete test plan to verify correctness for both flag states, prevent regressions when flipping flags, and validate CI/CD canary safety in deployments. Include test types, sample payloads, and example cases?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["Apple","Cloudflare","MongoDB"]},{"id":"q-3007","question":"You’re building a background job processor that retries failed external API calls using exponential backoff with jitter. Design a beginner-friendly test plan to validate enqueueing, backoff timing, max retries, and error handling. Include how you'd mock the broker and time, and the concrete test cases you would write to cover these aspects?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["Airbnb","Amazon","Google"]},{"id":"q-3147","question":"You're building a checkout aggregator in a PayPal/Instacart-like system that routes a single card payment through multiple providers (Visa, MasterCard, PayPal) based on real-time risk signals and provider SLAs. Design a concrete testing plan that includes provider simulators for contract tests, idempotent retry logic, circuit breakers, and end-to-end scenarios with mixed success/failure, latency, and currency conversions. Include data sets and failure models?","channel":"testing","subChannel":"general","difficulty":"intermediate","tags":["testing"],"companies":["Instacart","PayPal"]},{"id":"q-3304","question":"You maintain a small Python service that processes a Redis-backed queue of user actions every minute and writes results to PostgreSQL. In CI, tests occasionally fail due to timing and race conditions, even though the code is correct. Design a beginner-friendly test plan to improve CI reliability: specify test types, data seeding, environment isolation, and how to detect and prevent flakes?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["IBM","Two Sigma"]},{"id":"q-3515","question":"You built a search input that debounces user input by 300ms and cancels any in-flight fetch requests when a new query is typed. Describe a concrete beginner-friendly test plan to verify debouncing, request cancellation, result rendering, and error handling in CI. Include test types, data sets, and example test cases?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["Coinbase","Discord","IBM"]},{"id":"q-3530","question":"You’re adding an event-sourced catalog service where each state change is stored as an event in a distributed log (e.g., Kafka). The system must tolerate out-of-order events, support replay from snapshots, and handle shard rebalancing without breaking invariants. Outline a practical testing strategy that covers idempotency, correctness, and latency at scale, including mutation testing, fault injection, and real-world failure scenarios?","channel":"testing","subChannel":"general","difficulty":"advanced","tags":["testing"],"companies":["Google","Microsoft","Oracle"]},{"id":"q-3584","question":"You're running a MongoDB-backed service that consumes writes and exposes real-time analytics via Change Streams across a multi-region replica set. Design a practical test plan to validate correctness, ordering guarantees, and failover recovery under network partitions and replica reconfigurations, while maintaining high throughput?","channel":"testing","subChannel":"general","difficulty":"intermediate","tags":["testing"],"companies":["IBM","Microsoft","MongoDB"]},{"id":"q-3701","question":"Design a testing plan for a canary rollout of a MongoDB-backed feature flag in a large-scale service (e.g., DoorDash/Discord) that modifies the user profile schema in-place. Include migration tests, backward/forward compatibility, end-to-end feature gating, cross-region replica data integrity checks, and precise rollback criteria?","channel":"testing","subChannel":"general","difficulty":"advanced","tags":["testing"],"companies":["Discord","DoorDash","MongoDB"]},{"id":"q-3844","question":"You're integrating a feature-flag service into a frontend app used by Oracle and Nvidia. Flags update at runtime and the app should gracefully fall back if the service is unavailable. Design a beginner-friendly test plan to verify default behaviors, UI changes driven by flags, caching, and resilience across dev/stage/prod with concrete test cases and data?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["NVIDIA","Oracle"]},{"id":"q-3887","question":"You're adding a feature flag 'newCheckout' controlled by a remote flag service to an existing React/Node checkout flow. Describe a beginner-friendly, concrete test plan to verify that: (1) the UI routes to the old flow when the flag is false, (2) the UI uses the new flow when true, (3) when the flag service is slow or unavailable, the app gracefully falls back to the old flow, and (4) CI tests cover both flag values across common user segments and simulate retries?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["Airbnb","Google","Meta"]},{"id":"q-4206","question":"You're building a GitOps-driven multi-region deployment with Terraform modules for AWS across multiple accounts. How would you design a testing strategy that covers unit tests for modules, integration tests for cross-module interactions, drift detection, policy-as-code (OPA), and canary rollouts, including how you'd simulate outages and verify safe rollback before gate release?","channel":"testing","subChannel":"general","difficulty":"advanced","tags":["testing"],"companies":["Apple","Cloudflare","Hashicorp"]},{"id":"q-4265","question":"You’re introducing a multi-tenant feature flag service with per-user regional overrides and canary rollouts across three regions. Design a concrete, end-to-end test strategy that verifies correct flag resolution, isolation between tenants, rollback safety, and performance under burst traffic. Specify test types, data sets, environment setup, and observable metrics with example checks?","channel":"testing","subChannel":"general","difficulty":"intermediate","tags":["testing"],"companies":["Google","Hashicorp","Zoom"]},{"id":"q-4511","question":"You're adding a per-user feature flag system to a REST API used by mobile apps for A/B testing. Outline a beginner-friendly, concrete test plan to verify correctness, rollback safety, and observability in CI/CD. Include test types, data sets, and example test cases?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["Goldman Sachs","Tesla"]},{"id":"q-4539","question":"In a real-time analytics pipeline ingesting events from three products (payments, messaging, ads) via Kafka with Avro schemas that evolve over time, how would you design a test plan to validate backward and forward compatibility, safe schema evolution during rolling deployments, and correct aggregations in both the data lake and live dashboards across 100 tenants?","channel":"testing","subChannel":"general","difficulty":"intermediate","tags":["testing"],"companies":["Meta","PayPal","Slack"]},{"id":"q-4608","question":"You’re building a streaming ingestion path that reads JSON events from Kafka, enriches with a batch lookup, and upserts into Delta Lake with watermarking. Describe a concrete CI/CD test plan to verify end-to-end correctness, schema evolution, late data handling, and fault tolerance. Include test data schemes, mocks, upsert validation, and replay checks?","channel":"testing","subChannel":"general","difficulty":"intermediate","tags":["testing"],"companies":["Databricks","Robinhood","Two Sigma"]},{"id":"q-4695","question":"You're implementing a background image-processing pipeline: messages in a queue trigger a worker that downloads the image, resizes it, and uploads to storage. The worker must be idempotent, handle retries with backoff, and deduplicate messages using a per-message id. Describe a beginner-friendly test plan to verify correctness, retry behavior, and eventual consistency in CI. Include test data, mocked services, and concrete test cases?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["Robinhood","Snowflake","Square"]},{"id":"q-480","question":"How would you design a comprehensive testing strategy for a distributed microservices architecture handling 10M requests/day, ensuring 99.9% uptime while maintaining fast CI/CD pipelines?","channel":"testing","subChannel":"general","difficulty":"advanced","tags":["testing"],"companies":["Bloomberg","LinkedIn","NVIDIA"]},{"id":"q-509","question":"How would you test a REST API endpoint that creates a user account, including validation, error handling, and database integration?","channel":"testing","subChannel":"general","difficulty":"intermediate","tags":["testing"],"companies":["Amazon","Coinbase","Tesla"]},{"id":"q-537","question":"You're testing a real-time chat application that uses WebSockets. How would you design a test strategy to verify message ordering, connection resilience, and concurrent user scenarios?","channel":"testing","subChannel":"general","difficulty":"intermediate","tags":["testing"],"companies":["NVIDIA","Twitter","Two Sigma"]},{"id":"q-593","question":"How would you test a function that makes HTTP requests to an external API? What testing strategies would you use?","channel":"testing","subChannel":"general","difficulty":"intermediate","tags":["testing"],"companies":["Apple","Microsoft"]},{"id":"q-990","question":"You maintain a Node.js API function getUser(userId) that reads from MongoDB via Mongoose and caches the result in an in-memory TTL cache (60s). Write a practical test plan and code to verify: (1) first call hits DB and caches, (2) second call returns cached value, (3) after TTL expires a new DB hit occurs and cache updates, (4) DB error propagates to caller. Use Jest with minimal mocks and demonstrate time-control?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["Airbnb","MongoDB"]},{"id":"q-259","question":"How would you design integration tests for a Saga pattern implementation across 5 microservices to ensure exactly-once transaction processing and proper compensation handling during partial failures?","channel":"testing","subChannel":"integration-testing","difficulty":"advanced","tags":["api-testing","database-testing","mocking"],"companies":["Airbnb","Amazon","LinkedIn","Netflix","Spotify","Twitter","Uber"]},{"id":"q-207","question":"How would you implement a test-driven development workflow for a REST API endpoint using Jest and Supertest, following the red-green-refactor cycle with proper test organization and mocking strategies?","channel":"testing","subChannel":"tdd","difficulty":"intermediate","tags":["test-driven","red-green-refactor","test-first"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-360","question":"You're building a simple calculator class. Write a failing test first, then implement the add method using TDD. What's the red-green-refactor cycle?","channel":"testing","subChannel":"tdd","difficulty":"beginner","tags":["test-driven","red-green-refactor","test-first"],"companies":["Amazon","Anthropic","Google","Meta","Microsoft","Salesforce","Stripe"]},{"id":"q-405","question":"You're building a real-time collaborative drawing feature where multiple users can simultaneously edit a canvas. How would you apply TDD to test the conflict resolution mechanism when two users edit the same element at the same time?","channel":"testing","subChannel":"tdd","difficulty":"intermediate","tags":["test-driven","red-green-refactor","test-first"],"companies":["Canva","Unity","Zoom"]},{"id":"q-234","question":"How would you design a scalable test architecture for a microservices application handling 10,000+ concurrent tests across multiple environments while ensuring test isolation, performance, and CI/CD integration?","channel":"testing","subChannel":"test-strategies","difficulty":"advanced","tags":["jest","mocha","pytest","junit"],"companies":["Amazon","Google","Microsoft","Netflix","Salesforce","Stripe"]},{"id":"q-278","question":"How would you design a comprehensive testing strategy for a microservices architecture that scales to handle millions of requests per second while ensuring 99.99% availability?","channel":"testing","subChannel":"test-strategies","difficulty":"advanced","tags":["test-pyramid","coverage","mutation-testing"],"companies":["Amazon","Google","Meta","Netflix"]},{"id":"q-325","question":"How would you implement mutation testing to validate your test suite's quality and what's the relationship between mutation testing and code coverage?","channel":"testing","subChannel":"test-strategies","difficulty":"advanced","tags":["test-pyramid","coverage","mutation-testing"],"companies":["Epic Systems","Jane Street","Western Digital"]},{"id":"q-349","question":"You're building a distributed event streaming platform similar to Kafka. How would you design a comprehensive testing strategy that ensures message ordering guarantees, exactly-once semantics, and fault tolerance across a cluster of brokers?","channel":"testing","subChannel":"test-strategies","difficulty":"advanced","tags":["test-pyramid","coverage","mutation-testing"],"companies":["Confluent","Epic Games","Meta"]},{"id":"q-374","question":"You're testing a ServiceNow form validation module. How would you structure your test pyramid and what coverage metrics would you track?","channel":"testing","subChannel":"test-strategies","difficulty":"beginner","tags":["test-pyramid","coverage","mutation-testing"],"companies":["Amazon","Google","Microsoft","Netflix","Okta","Salesforce","Servicenow"]},{"id":"q-416","question":"You're building a React component library. How would you structure your test pyramid and what specific coverage metrics would you target for each layer?","channel":"testing","subChannel":"test-strategies","difficulty":"beginner","tags":["test-pyramid","coverage","mutation-testing"],"companies":["Broadcom","Hugging Face","Meta"]},{"id":"q-296","question":"In Jest, how would you implement advanced mocking patterns including sequential return values, async behavior, and proper mock lifecycle management for comprehensive test coverage?","channel":"testing","subChannel":"unit-testing","difficulty":"intermediate","tags":["jest","mocha","pytest","junit"],"companies":["Google","Meta","Netflix","Salesforce","Stripe"]},{"id":"q-311","question":"How do you mock a function in Jest that's called within another function under test?","channel":"testing","subChannel":"unit-testing","difficulty":"advanced","tags":["jest","mocha","pytest","junit"],"companies":["Amazon","Google","Meta"]},{"id":"q-338","question":"You're testing a React component that fetches user data from an API. How would you write a unit test using Jest to mock the API call and verify the component renders the user's name correctly?","channel":"testing","subChannel":"unit-testing","difficulty":"beginner","tags":["jest","mocha","pytest","junit"],"companies":["Cisco","Hulu","Postman"]},{"id":"q-388","question":"You're testing a REST API endpoint that returns user data. Write a basic unit test using Jest that verifies the endpoint returns a 200 status code and the response contains a 'name' field. What's the most important assertion to include?","channel":"testing","subChannel":"unit-testing","difficulty":"beginner","tags":["jest","mocha","pytest","junit"],"companies":["Postman","Retool","Supabase"]},{"id":"q-1002","question":"In a Unix environment logs are stored in /var/log/app/*.log with lines formatted as timestamp|user|action|resource. Write a practical one-liner using standard UNIX tools to output the top 5 users by total actions in the last 24 hours. Explain how you would handle log rotation and malformed lines?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Amazon","Robinhood"]},{"id":"q-1032","question":"How would you capture stdout and stderr of a simple shell command into separate log files while still displaying live output in the terminal? Provide a concrete Bash command and brief justification?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Databricks","LinkedIn","Uber"]},{"id":"q-1069","question":"In a Unix environment, logs live under /var/log/metrics/*.log and are hourly rotated to .log and .log.gz. Each line is like [YYYY-MM-DD HH:MM:SS] LEVEL: message. Propose a robust, portable approach (one-liner preferred) to output the number of ERROR events per hour for the last 6 hours, handling missing files and rotations without external dependencies?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Adobe","Meta","Plaid"]},{"id":"q-1123","question":"In a Unix environment, multiple services write JSON logs under /var/log/diag/*.log and rotated hourly to *.log.gz. Each line is a JSON object like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"ERROR\",\"msg\":\"...\"}; Propose a robust one-liner (no external dependencies beyond standard UNIX tools) to output the number of ERROR events per hour for the last 4 hours, aggregated across all services, and tolerant of missing files and rotated archives?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Apple","OpenAI","Snap"]},{"id":"q-1139","question":"Scenario: JSON logs at /var/log/diag/*.log, rotated hourly to *.log.gz. Each line: {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"ERROR\",\"msg\":\"...\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) that outputs the per-hour 99th percentile of message length for the last 4 hours, across all services, deduplicating identical messages per hour, and tolerant of missing files and gz archives?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["OpenAI","Plaid","Uber"]},{"id":"q-1224","question":"Scenario: In a Unix cluster, logs are emitted as JSON lines under /var/log/cluster/*/*.log and rotated hourly to *.log.gz. Each line contains {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"tenant\":\"tenant-id\",\"svc\":\"service\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}; Propose a robust one-liner (no external deps beyond standard UNIX tools) to identify the top 3 tenants by error rate (ERROR / total events) for the last 6 hours, aggregated across all services, tolerant of missing files and gz archives?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Apple","Coinbase","PayPal"]},{"id":"q-1241","question":"Scenario: In a multi-user Unix workspace, /home contains subdirectories per user with various files. Some are large, some are temporary. Provide a robust one-liner (no external dependencies) that lists the five largest regular files under /home (recursively), excluding hidden files and symlinks, printing each as 'size<TAB>path' with sizes in human-readable form. Explain how you ensure safety against spaces and newlines in filenames?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Databricks","Lyft","Tesla"]},{"id":"q-1492","question":"Scenario: multiple services log JSON lines to /var/log/app/*.log, rotated hourly to *.log.gz. Each line looks like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lat_ms\":123}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to compute the per-service average lat_ms in the last 2 hours, across all logs, tolerant of missing and gzipped files, ignoring malformed lines. Output: 'service avg_ms'?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["PayPal","Salesforce"]},{"id":"q-1809","question":"Scenario: A Linux host logs auth events into /var/log/auth/{service}/*.log and rotates hourly to *.log.gz. Each line is JSON like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"user\":\"alice\",\"action\":\"LOGIN\",\"result\":\"FAIL\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to print the top 5 users by failed login rate in the last 2 hours, aggregated across all services, tolerant of missing files and gz archives, and resilient to malformed lines?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Databricks","Robinhood","Tesla"]},{"id":"q-1903","question":"Propose a robust one-liner to compute the number of ERROR lines per hour for the previous 12 hours, aggregating across all logs in /var/log/app and across both uncompressed (*.log) and rotated/compressed files (*.log.*, *.log.gz). The one-liner must tolerate missing files and gz archives and rely only on standard UNIX tools?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Databricks","Slack","Stripe"]},{"id":"q-1975","question":"In a Unix host, logs live in /logs/web/*.log and are rotated hourly to *.log.gz. Each line is a JSON object like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"ip\":\"1.2.3.4\",\"method\":\"GET\",\"path\":\"/api\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to output the top 5 IPs by request count in the last 4 hours, aggregating across all files and rotations, tolerant of missing files and compressed archives?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Google","Snap","Twitter"]},{"id":"q-2094","question":"Scenario: In a Unix CI environment, logs are written by build agents under /build/logs/*/*.log and rotated hourly to *.log.gz. Each line is BUILDID|TIMESTAMP|STEP|STATUS|MESSAGE. Propose a robust one-liner (no external deps beyond standard UNIX tools) to report, for the last 24 hours, per BUILDID and per hour, the count of failed steps (STATUS != 'SUCCESS'), aggregating across all agents and rotations and tolerating missing files and compressed archives?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Apple","IBM"]},{"id":"q-2253","question":"Scenario: /var/run contains *.pid per daemon (service name == filename). Some daemons crash leaving stale PIDs. Propose a robust one-liner (no external deps) that lists service:pid pairs for all PID files whose PID is not running, tolerant of missing files and spaces in names?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Coinbase","Microsoft"]},{"id":"q-2293","question":"In a Unix environment, logs live under /var/log/app-logs/*.log and rotations *.log.gz. Each line is a JSON object like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"dur_ms\":1234,\"ev\":\"RUN\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to compute, per service, the mean and 95th percentile of dur_ms for the last 2 hours, aggregating across all files and rotations, tolerating missing files and archives?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Google","IBM","Zoom"]},{"id":"q-2396","question":"In a Unix host, multiple services write JSON log lines to /logs/app/*/*.log, rotated hourly to *.log.gz. Each line is {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) that outputs a per-minute histogram of WARN or ERROR events for the last 60 minutes, aggregated across all files and rotations, tolerant of missing files and gz archives, and resilient to out-of-order timestamps within the minute bucket?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Apple","Databricks","PayPal"]},{"id":"q-2420","question":"Scenario: In a Unix host, logs live under /var/log/app-logs with daily rotation and filenames like app-<service>-YYYYMMDD.log. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a total count of lines containing the word 'ERROR' across all log files modified in the last 3 days. The command must gracefully skip missing/unreadable files and work across many services?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Coinbase","MongoDB","Tesla"]},{"id":"q-2520","question":"In a fleet of Linux hosts, log files sit under /var/log/relay/*/*.log and rotated to *.log.gz. Each line is a JSON object with fields ts (YYYY-MM-DDTHH:MM:SSZ), svc, lat_ms. Write a robust one-liner (no external deps beyond standard UNIX tools) that outputs each service's 95th percentile latency over the last 15 minutes, aggregating across all files and rotations, tolerant of missing/unreadable files and out-of-order lines?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Google","Lyft","Twitter"]},{"id":"q-2624","question":"In a Linux host running dozens of worker processes inside containers, build a POSIX-friendly watchdog using only standard UNIX tools to detect any worker that has spent uninterruptible sleep (D state) for more than 60 seconds within a 10-minute window and restart such workers with exponential backoff. Use /proc to derive each worker's startup command from /proc/$pid/cmdline and /proc/cgroups to avoid system processes?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Amazon","Google","Zoom"]},{"id":"q-2738","question":"In a Unix host, per-job logs are under /srv/ci/jobs/<job_id>/logs, rotated daily with filenames log-YYYYMMDD.log. A symlink log-current.log points to the current day’s log file for each job. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the count of lines containing 'STATUS=FAILED' across all current-day log files for all jobs, gracefully handling missing files and broken links?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Lyft","Tesla"]},{"id":"q-2781","question":"In a Unix host, logs live under /var/log/services/*/*.log and are rotated hourly; some files are .log, others .log.gz. Each line is of the form [YYYY-MM-DD HH:MM:SS] <service>: <message>. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-service histogram of the number of lines containing the word ERROR in the last 24 hours, aggregating across all files and rotations. Must handle missing or unreadable files and gzipped archives, and work with spaces in paths?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Adobe","Robinhood","Uber"]},{"id":"q-3086","question":"In a Unix host, a policy requires that any file within data/ under /shared/projects must be group-writable and have the group set to the project's primary group, but only for files changed in the last 24 hours. Write a robust one-liner (no external deps beyond standard UNIX tools) to identify files that violate this policy, ignoring unreadable files and handling spaces in names?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Cloudflare","MongoDB","Snowflake"]},{"id":"q-3134","question":"In a Unix host, backups live under /srv/backups; each backup is named backup-YYYYMMDD.tar.gz and contains a data/ directory. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the total number of unique .log files across all backups modified in the last 7 days. The command should gracefully skip unreadable archives and ignore non-log files?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Airbnb","Google","Two Sigma"]},{"id":"q-3206","question":"In a Unix host, logs live under /var/log/ops/* with hourly rotation to *.log and sometimes *.log.gz. Lines are either plain text LEVEL: message or JSON like {ts: ISO8601, svc: name, lvl: ERROR}, and multi-line messages may follow a line that starts with a timestamp. Propose a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-minute histogram of ERROR events for the last 90 minutes, aggregating across all files and rotations, tolerant of missing files and gz archives, and robust to mixed formats?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Amazon","Microsoft","Snap"]},{"id":"q-3443","question":"In a Unix host, logs live under /var/log/trace/* with hourly rotation; some files end in .log, others .log.gz. Each line is either [YYYY-MM-DD HH:MM:SS] svc: duration=NNNms or JSON {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"dur_ms\":NNN}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to produce a per-service histogram of total request duration binned into 2-minute intervals for the last 60 minutes, aggregating across all files and rotations, tolerant of missing/unreadable files and out-of-order timestamps?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["NVIDIA","Netflix","Two Sigma"]},{"id":"q-3499","question":"In a Unix host, multiple services write JSON log lines under /logs/app/*/*.log and are rotated hourly to *.log.gz. Each line is {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"<name>\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-service idle time metric: time since last log line for each service within the last hour, flagging services idle for more than 2 minutes. Handle gz archives, unreadable files, and out-of-order timestamps?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Anthropic","Netflix","Square"]},{"id":"q-3573","question":"You are deploying a per-host log-tailer for thousands of services across a fleet. Logs live under /var/log/services/<svc>/*.log and rotate daily; older files may be *.log.gz. The on-host agent must stream new lines to a central sink with at-least-once delivery, tolerate missing/unreadable files, and stay correct during concurrent rotations. Describe design, data flow, and failure modes; outline an implementation strategy using only standard UNIX tools?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Anthropic","NVIDIA","Robinhood"]},{"id":"q-3583","question":"Scenario: A Unix host stores executables under /opt/apps with nested folders. Some files have setuid/setgid bits. Write a robust one-liner (no external deps beyond standard UNIX tools) that lists all files modified in the last 24 hours with setuid or setgid bits, printing per file: permissions, owner:group, and full path. The command must skip unreadable files and handle spaces in paths?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Apple","Bloomberg","Snap"]},{"id":"q-3617","question":"On a Linux host with many long-running services, write a robust one-liner (no external deps beyond standard UNIX tools) that lists the top 5 processes by number of open file descriptors. Only include processes that started at least 24 hours ago. Output: PID, UID, user, FD count, executable path, and a human-friendly age (e.g., 3d4h)? Must handle unreadable pids and spaces in cmdlines. Use /proc and standard tools?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Hugging Face","Oracle","Square"]},{"id":"q-3765","question":"In a Unix host, there is a directory /var/experiments containing subdirectories for each experiment. Each subdir may contain a file metrics.txt with lines that are either PASS or FAIL; some subdirs may contain metrics.txt.gz instead. Write a robust one-liner (no external dependencies beyond standard UNIX tools) that prints per-experiment fail rate as: <experiment>: <fails>/<total> (<percent>%), handling missing files and gzipped files, and across many experiments (spaces in names)?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Amazon","Google","Hugging Face"]},{"id":"q-3836","question":"Scenario: A Unix host runs dozens of services that log to daily rotated plain-text logs under /var/log/app-logs/app-<service>-YYYYMMDD.log, plus a real-time stream via a named pipe at /var/log/app-logs/pipe.log which receives JSON entries like {ts: ISO8601, svc: name, lvl: ERROR, msg: ...}. Some services also compress old logs to .gz. Write a robust one-liner (no external deps beyond standard UNIX tools) that outputs a per-minute histogram of ERROR events for the last 60 minutes, aggregating across both file logs and the live pipe, tolerant of missing/unreadable files and gzipped archives, and resilient to mixed formats (plain text and JSON)?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Apple","Meta","PayPal"]},{"id":"q-3904","question":"On a Unix host, multiple services write logs under /var/log/app-logs/service-*/ and a central JSON log at /var/log/central/combined.log.gz. Each plain-text line in app-logs uses [YYYY-MM-DD HH:MM:SS] <service>: <msg>, while the central log has JSON lines like {\"ts\":\"ISO\",\"svc\":\"name\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a per-service count of ERROR events in the last 2 hours, aggregating across all files and rotations, tolerating missing/unreadable files and gz archives, and handling mixed formats?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Meta","MongoDB","Snowflake"]},{"id":"q-3976","question":"Scenario: On a Unix host, logs sit in /var/log/app-logs with two formats: app-<service>-YYYYMMDD.log and rotated .log.gz. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the total number of non-empty lines across all files modified today. The command should gracefully skip unreadable files and work across many services?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Databricks","Google","Scale Ai"]},{"id":"q-4011","question":"In a Unix host, logs live under /logs/platform/*/*.log and are rotated hourly into *.log.gz. Each line is a JSON object: {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}. Provide a robust one-liner (no external deps beyond standard UNIX tools) that outputs a per-service, per-minute histogram of ERROR events for the last 60 minutes, aggregating across all files and rotations, tolerant of missing/unreadable files and gz archives, and deduplicating lines that may appear in multiple rotated files via inode tracking?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Adobe","Anthropic","Tesla"]},{"id":"q-4028","question":"In a Unix host, logs live under /logs/** and rotate hourly into *.log and *.log.gz. Each line is either 'YYYY-MM-DD HH:MM:SS <LEVEL>: <text>' or JSON with fields ts, lvl, msg. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the number of unique ERROR messages observed in the last 24 hours across all files, across both formats, tolerant of missing/unreadable files and gz archives?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Bloomberg","LinkedIn","Microsoft"]},{"id":"q-4175","question":"Scenario: A Unix host aggregates logs for dozens of services under /var/log/services/<service>/. Logs rotate hourly; some are plain .log and others .log.gz. Lines are either a plain timestamped message like [YYYY-MM-DD HH:MM:SS] LEVEL: text or a JSON object {\"ts\": \"YYYY-MM-DDTHH:MM:SSZ\", \"svc\": \"service\", \"lvl\": \"ERROR\", \"msg\": \"...\"}. A per-service live feed is written to /var/log/services/<service>/live.log as JSON lines. Write a robust pipeline (standard UNIX tools only) that outputs a per-service, per-minute histogram of ERROR events for the last 60 minutes, aggregating across files, archives, and the live feed, tolerating unreadable files and spaces in paths?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Google","Oracle"]},{"id":"q-4213","question":"In a Unix host, /data/projects contains thousands of files across nested dirs; some filenames contain spaces. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the five most common file extensions among files modified in the last 7 days, printing extension and count, ignoring directories and unreadable files?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["IBM","LinkedIn","Snap"]},{"id":"q-4271","question":"In a Unix host, logs live under /var/logs/services/*/*.log and *.log.gz, rotated hourly. Two formats exist: (a) plain lines 'YYYY-MM-DD HH:MM:SS LEVEL: message', (b) JSON lines '{\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}'. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints the top 3 services by total count of ERROR messages in the last 12 hours, aggregating across both formats and all rotated files, and gracefully skipping unreadable files?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Hugging Face","Meta"]},{"id":"q-4344","question":"On a Linux host serving many services, dangling (deleted) file handles can cause resource leaks. Create a robust one-liner (no external deps beyond standard UNIX tools) that prints, per process, any open file descriptor that points to a path that no longer exists on disk. Output: PID, process name, FD number, and the target path as read by readlink (including ' (deleted)' if present). Handle spaces in names and permissions gracefully?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["DoorDash","Hashicorp","Netflix"]},{"id":"q-4605","question":"In a Unix host, logs live under /srv/logs/app/ with files app-YYYY-MM-DD.log and nightly rotation to app-YYYY-MM-DD.log.gz. Each line is either a plain log line like YYYY-MM-DD HH:MM:SS - svc: message or a JSON object {\"ts\":\"YYYY-MM-DD HH:MM:SS\",\"svc\":\"name\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints, for every service, the number of unique error messages seen in the last 60 minutes, across all files and rotations, deduplicating identical messages per service, and tolerant of unreadable files?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Google","Hashicorp","Hugging Face"]},{"id":"q-4649","question":"Context: A multi-tenant Unix host under heavy load shows sporadic 'Too many open files' errors. Without external tooling, design a robust approach (one or two small scripts) using only standard UNIX utilities to identify the top 5 processes by open file descriptors in the last 60 minutes. Output must include: PID, user, CMD, fd count, and a sample FD target path (including deleted files). Address unreadable /proc entries and long paths?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Apple","Coinbase","Databricks"]},{"id":"q-4697","question":"On a Unix host, logs are under /logs/app-logs with hourly rotation into app-<service>-YYYYMMDD.log and gzipped as *.log.gz. A real-time stream provides JSON lines appended to /logs/app-logs/stream.log: {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"ERROR\",\"msg\":\"...\"}. Write a robust one-liner (no external deps beyond standard UNIX tools) that outputs the per-service moving average of ERROR events per minute over the last 15 minutes, across both file-based logs and the live stream, tolerant of missing/unreadable files and gz archives, and avoiding double-counting lines that appear in both current and rotated copies?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Instacart","Lyft","MongoDB"]},{"id":"q-4777","question":"In a Unix host, directory tree under /projects contains files some world-writable and some recently modified. Write a robust one-liner (no external deps beyond standard UNIX tools) that lists each world-writable file modified in the last 24 hours, printing path, owner, and permission bits. Ensure it handles spaces in filenames and large trees, skipping unreadable dirs?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Citadel","Google","Scale Ai"]},{"id":"q-481","question":"You're debugging a production system where processes are hanging. Using only Unix tools, how would you identify which processes are blocked on I/O, what they're waiting for, and safely terminate them without causing data corruption?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Snap","Snowflake"]},{"id":"q-4824","question":"Write a robust one-liner (no external deps beyond standard UNIX tools) to print the five most frequent ERROR messages observed in the last 2 hours across logs under /logs/app/** and /var/log, where lines are either 'YYYY-MM-DD HH:MM:SS <LEVEL>: <text>' or JSON lines {'ts':'YYYY-MM-DDTHH:MM:SSZ','lvl':'LEVEL','msg':'...'}, and support gzip-compressed files (*.log.gz)?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Lyft","Slack","Twitter"]},{"id":"q-510","question":"You're debugging a production issue where a process is stuck in uninterruptible sleep (D state). How would you identify and handle this situation?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["OpenAI","Tesla"]},{"id":"q-538","question":"You notice a process is consuming excessive CPU on a production server. How would you diagnose and troubleshoot this issue using Unix commands?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Citadel","Goldman Sachs","Microsoft"]},{"id":"q-564","question":"You're debugging a production system where processes are hanging. Using only Unix tools, how would you identify which processes are stuck in uninterruptible sleep (D state) and what could be causing this?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Adobe","OpenAI","Square"]},{"id":"q-894","question":"On a Linux host, /var/log/myapp.log is written by multiple processes. Implement a robust log rotation that triggers when the file reaches 100MB, keeps 7 rotated files, compresses older logs, and ensures no log loss while writers continue. Describe the approach, commands, and failure modes for concurrent writers?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Coinbase","Discord","OpenAI"]},{"id":"q-264","question":"How do Unix pipes enable inter-process communication and what are their performance implications?","channel":"unix","subChannel":"system-programming","difficulty":"beginner","tags":["posix","signals","pipes","sockets"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"q-1014","question":"Scenario: A multinational SaaS runs Vault with cross-region replication. Tenants use Vault's DB Secrets Engine for Postgres with per-tenant roles and short TTLs. Explain how you would enforce per-tenant rotation and immediate revocation on disable, while preventing cross-tenant leakage during DR failover?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Adobe","Goldman Sachs","MongoDB"]},{"id":"q-1054","question":"Scenario: A multi-tenant SaaS stores per-tenant API keys in Vault KV v2 with versioning. A rotation accidentally overwrote the previous key. Describe exactly how you would recover the previous version, which Vault paths and commands you'd use, and what policy controls you would enforce to prevent accidental deletions and ensure auditability?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Coinbase","Hugging Face","Tesla"]},{"id":"q-1094","question":"Design a per-tenant envelope encryption workflow using Vault Transit: each tenant has a dedicated Transit key; generate a per-tenant DEK, encrypt data with the DEK, and store the ciphertext. How would you rotate the Transit key without downtime, rewrap existing ciphertext to the new version, handle tenant disablement (revocation), and maintain end-to-end auditability? Include specific Vault paths and commands?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Hashicorp","Instacart","Twitter"]},{"id":"q-1174","question":"In a multi-tenant SaaS, you store per-tenant data encryption keys in Vault Transit with a dedicated key per tenant and daily rotation. Describe how you would implement per-tenant key lifecycle (creation, rotation without downtime, data rewrap for existing data, and immediate revocation when a tenant is disabled) and how you would monitor/audit across services?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Hugging Face","MongoDB","Stripe"]},{"id":"q-1229","question":"You’re building a multi-tenant SaaS and plan to encrypt data at rest using Vault's Transit engine. Describe how you would enable Transit, create per-tenant keys, implement encryption/decryption via the API, and perform zero-downtime key rotation while preserving ciphertext integrity. Include policy considerations to prevent data leakage and how you validate rotation?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Hugging Face","Meta","Tesla"]},{"id":"q-1357","question":"In a microservices setup, Vault's PKI engine issues short-lived TLS certs for mTLS. Describe end-to-end: enable PKI at path pki, create a role microservice-tls with allowed_domains='service.local', allow_subdomains=true, max_ttl='24h', issue a cert for 'auth-service.service.local', and revoke certificates immediately when a service is decommissioned and audit issuance?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Bloomberg","Lyft","MongoDB"]},{"id":"q-1405","question":"In a multi-DC Vault deployment used by a large enterprise, explain how to provision ephemeral SSH access to hosts using Vault's SSH secrets engine. Include how to establish a per-user role, certificate TTLs, revocation on logout or compromise, host-key rotation, and how to audit all SSH sessions end-to-end?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["LinkedIn","Uber"]},{"id":"q-1446","question":"You operate a multi-tenant app using Vault's Transit engine to encrypt per-tenant data with keys at transit/tenant-{tenant_id}. When retiring a tenant, you must rotate its key, re-encrypt historical data, and keep live traffic unaffected. Describe a concrete plan for key rotation, re-encryption strategy, audit logging, and rollback safeguards to preserve decryptability?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Snowflake","Tesla","Twitter"]},{"id":"q-1504","question":"Scenario: A project alpha needs read-only access to its KV v2 secrets at secret/data/projects/alpha and secret/metadata/projects/alpha. Define a policy 'alpha-read' granting read to those endpoints with renewable=false and a max TTL of 1h, and show the token issuance command to grant this policy. How would you verify expiry and revoke immediately if the project ends?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["NVIDIA","Snowflake"]},{"id":"q-1569","question":"Describe how you would configure Vault audit logging and per-tenant access controls for a SaaS that issues per-tenant Snowflake credentials via Vault's database secrets engine. Include audit setup, tenant-scoped policies, how to attach tenant metadata to credentials, and a test plan to verify instant revocation and tenant-specific logs?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Meta","NVIDIA","Snowflake"]},{"id":"q-1676","question":"You manage a SaaS that grants per-tenant SSH access to a fleet using Vault's SSH Secret Engine. Each tenant receives ephemeral SSH credentials valid for 24 hours, scoped to a per-tenant username and a dedicated user CA. Outline the operational steps to configure the engine, define a per-tenant role, enforce 24h TTLs, and ensure immediate revocation on de-provisioning, including how you would issue credentials and revoke them?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Airbnb","Anthropic","Two Sigma"]},{"id":"q-1699","question":"Design a practical key-rotation workflow using Vault Transit to rotate a dataset encryption key (DEK) for a Databricks workflow that writes Parquet files to S3, ensuring zero-downtime encryption for new data, safe rewrap of existing ciphertext, and immediate revocation if a cluster is decommissioned. Include exact Vault commands and rollout plan?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Databricks","Zoom"]},{"id":"q-1855","question":"You run a multi-tenant Databricks data platform using Vault's database secret engine to issue per-tenant Snowflake credentials with short TTLs. Design a rotation strategy that rotates credentials across 1000+ concurrent jobs with zero downtime, and enables immediate revocation when a tenant is disabled. Include Vault role config, rotation/renewal workflow, and how to propagate changes to active notebooks/clusters?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Databricks","Two Sigma"]},{"id":"q-1894","question":"You're operating a Slack/Anthropic-scale SaaS using Vault for **Kubernetes** workloads. Each namespace gets a dedicated Vault role via the Kubernetes auth method, issuing per-pod credentials with a TTL of 15 minutes. Describe how you would implement scalable rotation and safe renewal to avoid expired secrets, including: (a) agentless vs agent-based rotation strategies, (b) pre-rotation and lease renewal coordination across many pods, (c) handling long-running jobs that outlive their pods, and (d) auditing, revocation on namespace deletion, and potential upgrade paths. Include concrete Vault commands and config snippets?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Anthropic","Slack"]},{"id":"q-1896","question":"Design an end-to-end workflow using Vault's SSH secrets engine to issue ephemeral SSH credentials for a multi-tenant Linux fleet. Include per-tenant roles under ssh/roles/tenant-*, TTL 10m, a renewal approach that preserves live sessions, and an immediate revocation path on tenant disable. Include audit scopes and concrete Vault commands?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Bloomberg","LinkedIn","NVIDIA"]},{"id":"q-2007","question":"In a multi-tenant SaaS running on Kubernetes, each tenant's app pods authenticate to Vault via Kubernetes auth. Design an end-to-end plan to ensure per-tenant secret isolation, automatic rotation with no downtime, and immediate revocation when a tenant is disabled. Include: (1) Identity/Group mapping per tenant, (2) per-tenant DB role in the Vault DB secret engine with TTL, (3) rotation flow and how apps will access new creds without downtime, and (4) tenant disable revoke process and audit considerations. Provide sample policies and a rollout plan?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Oracle","Plaid","Square"]},{"id":"q-2023","question":"You’re deploying a small SaaS that requires per-service mTLS authentication. Using Vault's PKI engine, design an issuance flow to deliver short-lived client certificates (TTL 15m) to services, with immediate revocation on termination. Specify role config, issuance path, revocation process, and how you'd validate revocation in a running cluster?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Instacart","OpenAI","Snap"]},{"id":"q-2108","question":"Design a PCI-DSS compliant multi-tenant Vault deployment across three regions. Each tenant retrieves per-tenant DB credentials via Vault with strict isolation, rotation, and revocation. Provide an isolation strategy (namespaces vs path-based), a cross-region DR plan, a zero-downtime rotation workflow, and concrete Vault commands and policies for rollout and rollback?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Goldman Sachs","LinkedIn","Tesla"]},{"id":"q-2156","question":"You're running a multi-region Vault deployment for a SaaS with tenants isolated in namespaces. Plan a cross-region DR replication that preserves tenant isolation and minimizes downtime. Include: 1) namespace/policy design across regions, 2) replication topology and sync behavior, 3) tenant token/lease revocation paths during failover, 4) a failover testing plan with realistic success metrics?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Apple","Coinbase","Discord"]},{"id":"q-2215","question":"In a multi-tenant Vault Enterprise deployment with per-tenant namespaces, a tenant’s OIDC token was compromised. Outline a practical incident response plan to immediately revoke leases, rotate the tenant’s DB and KV secrets, and restore access with minimal downtime. Include concrete Vault commands, rollback checks, and validation steps?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Adobe","NVIDIA"]},{"id":"q-2281","question":"Design a per-tenant SSH dynamic credential workflow using Vault's SSH Secrets Engine for a multi-tenant SaaS with a shared bastion host. Include per-tenant roles, TTL controls, instant revocation on tenant disable, and robust audit/logging considerations?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Google","LinkedIn","Salesforce"]},{"id":"q-2299","question":"You’re running a multi-tenant data processing platform used by Zoom and Airbnb. Vault's AWS secret engine issues per-tenant ephemeral IAM credentials for short-lived compute jobs in Kubernetes. Explain how you would: 1) map tenants to Vault roles, 2) enforce 15–30 minute TTLs with rotation without downtime, 3) revoke immediately when a tenant is disabled, and 4) audit and validate end-to-end?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Airbnb","Zoom"]},{"id":"q-2401","question":"You’re running a multi-tenant Vault Enterprise deployment with per-tenant namespaces. Data is envelope-encrypted with Vault Transit using a per-tenant key set. A tenant requests a routine key rotation due to suspected compromise. Provide a concrete, low-downtime rotation plan that (1) uses Transit key versions and rewrap, (2) ensures in-flight requests finish safely, (3) validates data integrity after rotation, and (4) defines rollback steps if issues are detected. Include concrete Vault commands and timing guidelines?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Google","MongoDB"]},{"id":"q-2575","question":"In a Vault Enterprise deployment with per-tenant namespaces, design a secure workflow using the SSH secret engine to grant ephemeral SSH access to a fleet of jump hosts for each tenant. Host keys rotate every 24 hours. Describe (a) per-tenant ssh roles and constraints, (b) the rotation flow that avoids breaking in-flight sessions, including how to handle known_hosts and key distributions, (c) immediate revocation when a tenant is disabled, and (d) validation of tenant isolation after rotation. Include concrete Vault paths and commands?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Adobe","DoorDash","Two Sigma"]},{"id":"q-2595","question":"Scenario: You run Vault KV v2 under path secret/ with per-tenant data: secret/data/tenants/{tenant}/api-key. Implement a low-downtime rotation for a tenant's API key: enable KV v2, rotate by creating a new version, ensure clients fetch the latest by default, and securely destroy older versions when a tenant is disabled. Include concrete Vault commands and policy considerations?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Hugging Face","Scale Ai","Twitter"]},{"id":"q-2641","question":"In a Vault Enterprise deployment with per-tenant namespaces on Kubernetes, each tenant's workloads authenticate via Kubernetes auth and obtain per-tenant PostgreSQL credentials through the DB secrets engine. A tenant is disabled while pods may be running. Describe a concrete, low-downtime rotation plan that revokes all tenant leases, rotates credentials, ensures in-flight requests complete safely, and validates post-rotation isolation. Include concrete Vault commands and timing guidelines?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Robinhood","Snowflake","Uber"]},{"id":"q-2707","question":"You're setting up Vault in a multi-tenant environment. Beginner task: enable a file audit device to log all requests to /var/log/vault_audit.log in JSON format, set up OS logrotate to safely rotate the audit log without losing data, and demonstrate how to verify in the audit log that a tenant's read of their own secret is recorded without exposing secret contents. Include exact Vault commands for enabling the audit, a sample logrotate config snippet, and a sample CLI session that would generate an audit entry?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Amazon","Citadel","Twitter"]},{"id":"q-2812","question":"Scenario: A multi-tenant SaaS uses Vault with per-tenant namespaces, Transit for envelope encryption, and MongoDB for tenant data. A tenant reports credential compromise. Provide an end-to-end plan to isolate the tenant, revoke access, rotate per-tenant Transit keys, rewrap existing data, and prove there’s zero cross-tenant access, including concrete Vault commands and timing?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["MongoDB","NVIDIA"]},{"id":"q-2822","question":"In a Vault Enterprise deployment with per-tenant namespaces, describe a concrete, low-downtime plan to issue per-tenant PostgreSQL credentials via the database secret engine, rotate them with 1h TTL, and revoke immediately when a tenant is disabled, ensuring no downtime for running apps. Include namespace/role setup, renewal/rotation mechanics, and rollback steps with concrete Vault commands?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Apple","MongoDB","Slack"]},{"id":"q-2887","question":"In a Plaid-like fintech SaaS using Vault with per-tenant namespaces, dynamic AWS credentials are issued per tenant via the AWS secrets engine. Describe in detail how you would: (1) design per-tenant AWS roles with short TTLs, (2) rotate credentials with zero downtime, and (3) revoke all tenant credentials immediately when a tenant is disabled, including concrete Vault commands and sample policies?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Plaid","Tesla"]},{"id":"q-2978","question":"In a multi-tenant Vault Enterprise deployment using PKI to issue TLS certificates for tenant services, a root CA rotation is required with zero downtime. Design a concrete, tenant-aware plan to rotate the root certificate and re-issue in-flight certificates, including Vault paths, commands, timing, and rollback checks?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Amazon","Google","PayPal"]},{"id":"q-3008","question":"In a multi-region Vault Enterprise deployment serving multiple tenants, design a tenant-aware ephemeral SSH access flow using the SSH Secrets Engine to grant jump-access to production hosts. Specify per-tenant roles (e.g., tenant-<id>-admin), TTLs, how certs are issued/validated, cross-region consistency, and a rollback plan if a tenant is disabled or the SSH CA is compromised. Include concrete Vault paths and commands?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Coinbase","Meta","MongoDB"]},{"id":"q-3081","question":"In a small team, a service needs dynamic PostgreSQL credentials from Vault. Design and implement a minimal AppRole workflow: policy allowing access to database/creds/role-a only; a database secret engine role role-a with 30m TTL; an AppRole service-a bound to that policy; a runnable script that authenticates with AppRole and prints the generated connection string. Include exact Vault commands and a runnable example script?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Amazon","Scale Ai"]},{"id":"q-3145","question":"In a multi-tenant Vault Enterprise deployment with per-tenant namespaces, a tenant wants to migrate workloads from AppRole to Kubernetes Auth with Vault Agent sidecars. Design a zero-downtime migration plan that preserves active leases, binds tokens to the new auth method, updates policies, and validates isolation post-migration. Include concrete Vault paths, commands, timing, and rollback checks?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Anthropic","OpenAI","Robinhood"]},{"id":"q-3324","question":"Describe a Terraform-driven onboarding workflow for Vault Enterprise that provisions per-tenant namespaces, policies, and an AppRole bound to a dedicated DB secret engine role with TTLs under 1 minute. Include how to apply changes safely in a live cluster, ensure strict tenant isolation, perform post-apply validation, and implement rollback via lease revocation and audit checks?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Discord","Scale Ai","Stripe"]},{"id":"q-3387","question":"Design a multi-tenant PKI workflow using Vault's PKI engine in a Vault Enterprise deployment. Each tenant should receive a subordinate CA and be allowed to issue client certificates with a TTL of 24 hours. Include isolation guarantees, certificate revocation and CRL distribution, and a rolling root CA rotation plan with zero-downtime, plus validation and rollback steps. Provide concrete Vault commands and verification steps?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Meta","PayPal"]},{"id":"q-3488","question":"In a multi-tenant Vault Enterprise deployment with per-tenant namespaces, a tenant’s Transit key appears compromised. Describe a concrete, low-downtime plan to rotate only that tenant’s Transit key and rewrap all its data envelopes in place, while preserving isolation for others; include exact Vault commands, how to handle in-flight requests, how to verify data integrity, and rollback steps if issues arise?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Amazon","Apple"]},{"id":"q-3526","question":"You're onboarding a new tenant in a multi-tenant Vault cluster that uses Kubernetes auth. Describe the minimal steps to: 1) create a per-tenant policy that allows read on secret/data/tenants/tenantA/*; 2) map a tenantA-sa service account in namespace tenantA to that policy via Vault's Kubernetes auth role; 3) verify isolation by attempting reads from tenantA and tenantB namespaces. Include exact Vault and kubectl commands?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Discord","Meta","OpenAI"]},{"id":"q-3542","question":"Given a SaaS app on Kubernetes, onboard a new tenant with Vault. Describe a beginner-friendly, production-safe flow to: 1) create a per-tenant namespace and policy, 2) bind a Kubernetes auth role for that tenant, 3) configure a dedicated database secret engine role with a short TTL (e.g., 15 minutes), 4) perform safe live changes with a canary path, 5) validate post-apply checks and rollback via lease revocation and audit checks?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Goldman Sachs","Hashicorp","MongoDB"]},{"id":"q-3668","question":"Onboard a new tenant in a multi-tenant Vault using the Transit secrets engine for per-tenant envelope encryption. Provide concrete steps to create a per-tenant transit key, bind it to a lightweight auth path (AppRole or Kubernetes), configure 24h automatic rotation, and validate with an end-to-end test; include a rollback plan if rotation introduces issues?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["PayPal","Plaid","Tesla"]},{"id":"q-3771","question":"You operate a multi-tenant SaaS that uses Vault PKI to issue per-tenant TLS certificates for service-to-service mTLS. Design a scalable workflow where each tenant has a dedicated PKI role, cert TTLs are <= 15 minutes, CA certificates rotate every 24 hours, and tenant deprovision triggers immediate revocation and clean-up. Describe roles, issuance, rotation, revocation, audit, and testing steps for live clusters?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Airbnb","Hashicorp","Plaid"]},{"id":"q-3818","question":"In Vault Enterprise with per-tenant namespaces, a tenant uses the database secret engine to obtain Postgres creds. A security incident requires rotating only that tenant's credentials without downtime. Outline a concrete tenant-scoped rotation workflow: which Vault paths and commands to issue a new credential, how to keep old leases valid during validation, TTL guidance, rollback checks, and audit verification?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Coinbase","Google","Oracle"]},{"id":"q-3874","question":"Design a tenant-aware regional DR failover test for a Vault Enterprise deployment with per-tenant namespaces (ns/tenant-<id>). Provide a concrete runbook that promotes a standby region to active, preserves tenant isolation during the switch, and verifies post-failover credentials and leases for a sample tenant. Include exact Vault paths, representative commands, TTL considerations, and a rollback/audit plan?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Microsoft","Tesla"]},{"id":"q-3959","question":"Implement a tenant-aware envelope encryption workflow using Vault Transit in a multi-tenant Enterprise deployment. Outline per-tenant namespaces ns/tenant-<id> and key tenant-<id>-enc, rotation via transit/keys/<name>/rotate, data-key rotation and rewrap steps, how clients switch to the new key with zero downtime, and rollback/audit checks for cross-region DR?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Cloudflare","Instacart","LinkedIn"]},{"id":"q-4068","question":"Given a Vault Enterprise cluster with per-tenant namespaces ns/tenant-<id> and a PostgreSQL database secret engine mounted at database/config/pg, design a beginner onboarding workflow for a new tenant that: - creates a policy restricting access to secret/data/tenant-<id>/* and to database/creds/tenant-<id>-pg; - configures a PostgreSQL DB role tenant-<id>-pg with ttl 20m for credentials; - creates an AppRole bound to that policy and DB role; - demonstrates a quick login to fetch credentials via AppRole and a read of database/creds/tenant-<id>-pg; - provides a rollback/offboarding procedure revoking leases and deleting the AppRole and policy if needed. Include exact Vault paths and sample commands?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Citadel","Instacart","MongoDB"]},{"id":"q-4147","question":"In a Vault Enterprise cluster with per-tenant namespaces ns/tenant-<id>, design a tenant-scoped encryption strategy using Transit. Decide between per-tenant Transit mounts vs a shared mount with per-tenant keys, and justify. Provide a concrete plan for tenant-42: 1) policy and AppRole bindings to ns/tenant-42/transit/keys/tenant-42-key for encrypt/decrypt; 2) rotation strategy using transit/keys/tenant-42-key/rotate with zero downtime; 3) cross-tenant isolation tests and audit checks; 4) rollback steps if tenant-42 is disabled, including lease revocation and AppRole/policy removal?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Adobe","Databricks","Square"]},{"id":"q-4235","question":"In a Vault Enterprise cluster with per-tenant namespaces ns/tenant-<id>, design a PKI workflow that mounts a tenant-specific intermediate CA at pki/tenant-<id>, issues short-lived client certificates TTL 10m for a TLS service, and automatically revokes them when a tenant is disabled. Include per-tenant role, CSR validation, CRL distribution, post-issue checks, and a rollback plan with lease revocation and audit verification?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Anthropic","Apple","NVIDIA"]},{"id":"q-4328","question":"Advanced Vault Enterprise multi-tenant SSH access: In ns/tenant-<id>, implement per-tenant SSH dynamic credentials using the SSH secrets engine. Each tenant must access only a bucket of hosts (hostA, hostB) with TTLs <= 2 minutes. Provide exact Vault paths, a tenant-<id>-ssh role, an AppRole bound to that role, a concrete usage flow (login, request creds, SSH to host), and an offboarding plan revoking leases and auditing. Ensure strict isolation?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Airbnb","Apple","Meta"]},{"id":"q-4412","question":"Within Vault Enterprise, each tenant has a namespace. Design a tenant-scoped SSH access flow using the SSH secrets engine to issue ephemeral certs with a max TTL of 10 minutes for tenant engineers to access production hosts in the tenant's allowed list. Include Vault paths, role config, identity binding (OIDC/AppRole), host constraints, revocation strategy on tenant disable, and a test plan for validation and rollback?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Airbnb","IBM","OpenAI"]},{"id":"q-4458","question":"In a Vault Enterprise deployment with per-tenant namespaces, how would you implement multi-region disaster recovery (DR) for tenant secrets while preserving tenant isolation, including replication strategy, failover orchestration, data consistency guarantees, lease/token handling during failover, and post-failover validation?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Citadel","Google","MongoDB"]},{"id":"q-4609","question":"Beginner onboarding: for a new tenant, use Vault's SSH secret engine to issue ephemeral SSH credentials for a per-tenant bastion host. Isolate by namespace ns/tenant-<id>. Create role tenant-<id>-ssh with a 5m default TTL and 10m max TTL; attach a policy permitting ssh/creds/tenant-<id>-ssh and access to bastion. Provide exact Vault paths and sample commands?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Airbnb","Lyft","Meta"]},{"id":"q-4779","question":"In a multi-tenant Vault setup with per-tenant namespaces ns/tenant-<id>, describe a beginner onboarding workflow for a new tenant that uses an AppRole bound to a dedicated database secret engine role with TTLs of 15m. Include policy scoping, AppRole bindings, exact Vault paths and sample commands, an initial login, and a safe offboarding plan with lease revocation?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Amazon","Google","Robinhood"]},{"id":"q-971","question":"In a PCI-compliant SaaS, each tenant uses Vault's database secret engine with a dedicated role to issue per-tenant PostgreSQL credentials. TTLs are short (1h). Describe how you would configure per-tenant roles, handle rotation without downtime, and ensure immediate revocation when a tenant is disabled?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Anthropic","Google","Stripe"]}]