[{"id":"q-1158","question":"How would you implement a health check mechanism for a load balancer that uses exponential backoff for failed servers, and how does this approach prevent cascading failures during partial outages?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["load-balancing","health-checks","exponential-backoff","fault-tolerance","cascading-failures"],"companies":[]},{"id":"q-2121","question":"How would you implement a load balancer that uses predictive scaling based on request patterns and historical data, and what machine learning techniques would you use to forecast traffic spikes?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["predictive-scaling","machine-learning","time-series-forecasting","load-balancing"],"companies":[]},{"id":"q-653","question":"How would you implement a consistent hashing load balancer that handles server additions and removals with minimal key remapping? What data structures would you use and how would you handle virtual nodes?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["consistent-hashing","load-balancing","distributed-systems","hash-ring","virtual-nodes"],"companies":[]},{"id":"q-659","question":"How would you implement a consistent hashing load balancer and what advantages does it provide over traditional hash-based load balancing when servers are added or removed?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["load-balancing","consistent-hashing","distributed-systems","scalability"],"companies":[]},{"id":"q-767","question":"Design a load balancer that implements adaptive load balancing using real-time server metrics. How would you collect and weight server performance data, and what algorithm would you use to dynamically adjust traffic distribution?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["adaptive-load-balancing","performance-monitoring","dynamic-weighting","real-time-metrics"],"companies":[]},{"id":"al-1","question":"When would you choose a Linked List over an Array and what are the key trade-offs for each data structure?","channel":"algorithms","subChannel":"data-structures","difficulty":"beginner","tags":["struct","comparison","basics"],"companies":["Adobe","Amazon","Apple","Google","Meta","Microsoft"]},{"id":"al-165","question":"Implement a Trie data structure for efficient prefix search with insert, search, and startsWith operations. What are its advantages over hash maps for autocomplete systems, and what are the trade-offs?","channel":"algorithms","subChannel":"data-structures","difficulty":"intermediate","tags":["struct","basics"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"q-187","question":"How would you implement a thread-safe LRU cache using a HashMap and DoublyLinkedList, considering eviction policy and O(1) operations?","channel":"algorithms","subChannel":"data-structures","difficulty":"intermediate","tags":["arrays","linkedlist","hashtable","heap"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Uber"]},{"id":"q-277","question":"How would you efficiently process a 50GB log file to extract the top 10 most frequent IP addresses from millions of entries while handling memory constraints and optimizing for performance?","channel":"algorithms","subChannel":"data-structures","difficulty":"advanced","tags":["find","xargs","cut","sort"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Snowflake"]},{"id":"q-377","question":"Implement a min-heap using an array that supports insert, extractMin, and peek operations in O(log n) time. Include time/space complexity analysis and edge cases?","channel":"algorithms","subChannel":"data-structures","difficulty":"beginner","tags":["arrays","linkedlist","hashtable","heap"],"companies":null},{"id":"q-407","question":"Given a stream of log events with timestamps, design an algorithm to find the top K most frequent error messages in the last N minutes using O(K) space, where each event contains timestamp, error type, and message?","channel":"algorithms","subChannel":"data-structures","difficulty":"intermediate","tags":["arrays","linkedlist","hashtable","heap"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-418","question":"Design a data structure that supports range sum queries and point updates on a dynamic array with O(log n) operations. How would you implement this using a segment tree, and what are the trade-offs compared to a Binary Indexed Tree?","channel":"algorithms","subChannel":"data-structures","difficulty":"advanced","tags":["bst","avl","trie","segment-tree"],"companies":null},{"id":"q-425","question":"Given an array of integers and a target sum, find two numbers that add up to the target. How would you implement this efficiently and what's the time complexity?","channel":"algorithms","subChannel":"data-structures","difficulty":"beginner","tags":["arrays","linkedlist","hashtable","heap"],"companies":["Adobe","Amazon","Two Sigma"]},{"id":"q-442","question":"Given a stream of user actions with timestamps, design a system to find the top K most frequent actions in the last N minutes using O(1) time per query?","channel":"algorithms","subChannel":"data-structures","difficulty":"intermediate","tags":["arrays","linkedlist","hashtable","heap"],"companies":["LinkedIn","OpenAI"]},{"id":"q-565","question":"Given a stream of timestamped events, find the maximum number of concurrent events at any time?","channel":"algorithms","subChannel":"data-structures","difficulty":"advanced","tags":["arrays","linkedlist","hashtable","heap"],"companies":["Salesforce","Slack","Square"]},{"id":"al-152","question":"You have a staircase with n steps. You can climb 1, 2, or 3 steps at a time. How many distinct ways can you reach the top? Implement a solution with O(n) time and O(1) space?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"intermediate","tags":["dp","optimization"],"companies":null},{"id":"al-166","question":"Given a string, find the minimum cost to transform it into a palindrome where insertions cost 2 and deletions cost 1. What is the optimal dynamic programming approach?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"intermediate","tags":["dp","optimization"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"al-167","question":"Given a target sum n, count the number of ways to reach it using dice rolls where each roll can be 1-6. Return the result modulo 10^9+7. Optimize for O(n) time and O(1) space?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"intermediate","tags":["dp","optimization"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"al-170","question":"Given an array of integers where each element represents the maximum number of steps you can jump forward from that position, find the minimum number of jumps required to reach the last index. If it's not possible to reach the end, return -1. How would you implement this efficiently?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"advanced","tags":["dp","optimization"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"al-3","question":"What is Dynamic Programming and how does it differ from plain recursion? When would you choose one over the other?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"advanced","tags":["dp","optimization","theory"],"companies":["Amazon","Google","Meta"]},{"id":"q-328","question":"Given a grid of size m x n where each cell contains a non-negative integer representing the cost to enter that cell, find the minimum cost path from the top-left corner (0,0) to the bottom-right corner (m-1,n-1) moving only right or down. Return both the minimum cost and the path itself?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"intermediate","tags":["dp","memoization","tabulation"],"companies":["Amazon","Apple","Google","Meta","Microsoft","NVIDIA"]},{"id":"q-440","question":"Given a string s and dictionary wordDict, return all possible sentences where s can be segmented into space-separated words from wordDict. Handle overlapping subproblems efficiently?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"advanced","tags":["dp","memoization","tabulation"],"companies":["Adobe","Google","NVIDIA"]},{"id":"q-540","question":"Given a string s and a dictionary wordDict, return all possible sentences formed by inserting spaces in s such that each word exists in wordDict. Use DP with memoization to avoid exponential recomputation?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"advanced","tags":["dp","memoization","tabulation"],"companies":["Goldman Sachs","LinkedIn","Slack"]},{"id":"q-214","question":"Given a directed weighted graph with up to 10^6 edges and frequent edge weight updates, design a data structure that supports dynamic shortest path queries with sub-millisecond response time?","channel":"algorithms","subChannel":"graphs","difficulty":"advanced","tags":["bfs","dfs","dijkstra","topological"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Uber"]},{"id":"q-286","question":"Explain the difference between BFS and DFS and when would you use each?","channel":"algorithms","subChannel":"graphs","difficulty":"intermediate","tags":["bfs","dfs","dijkstra","topological"],"companies":["Amazon","Google","Meta"]},{"id":"q-350","question":"Given a directed graph representing city intersections and one-way streets, implement a function to find if there's a valid route from point A to point B using BFS. Return the shortest path distance or -1 if no route exists?","channel":"algorithms","subChannel":"graphs","difficulty":"beginner","tags":["bfs","dfs","dijkstra","topological"],"companies":["Amazon","Apple","Cruise","Google","Meta","Microsoft","Netflix","Vercel"]},{"id":"q-394","question":"Given a directed acyclic graph representing task dependencies where each task takes 1 unit of time and you have unlimited workers, what is the minimum time to complete all tasks?","channel":"algorithms","subChannel":"graphs","difficulty":"advanced","tags":["bfs","dfs","dijkstra","topological"],"companies":null},{"id":"q-662","question":"Given a directed graph with N nodes and M weighted edges (positive weights) and a source s, describe an implementation to (1) identify nodes reachable from s via BFS, (2) compute shortest distances dist[] to all nodes with Dijkstra, and (3) count the number of distinct shortest s→v paths for every v (mod 1e9+7). How would you build a DAG of edges on shortest paths and perform a topological DP over dist-ordered nodes to obtain path counts?","channel":"algorithms","subChannel":"graphs","difficulty":"advanced","tags":["bfs","dfs","dijkstra","topological"],"companies":["Anthropic","NVIDIA","Robinhood"]},{"id":"q-596","question":"Explain the differences between round-robin, least connections, and IP hash load balancing algorithms. When would you choose each one?","channel":"algorithms","subChannel":"load-balancing-algorithms","difficulty":"intermediate","tags":["load-balancing","algorithms","networking","scalability","system-design"],"companies":["Google","Amazon","Meta","Netflix","Microsoft","Twitter","Uber","Airbnb"]},{"id":"q-627","question":"Explain the difference between round-robin and weighted round-robin load balancing algorithms. When would you choose one over the other?","channel":"algorithms","subChannel":"load-balancing-algorithms","difficulty":"intermediate","tags":["load-balancing","algorithms","distributed-systems","networking"],"companies":["Amazon","Google","Microsoft","Netflix","Facebook","Twitter"]},{"id":"al-163","question":"You have an array where each element appears twice except one element that appears once. Sort the array in O(n) time without using extra space for sorting. How would you approach this?","channel":"algorithms","subChannel":"sorting","difficulty":"intermediate","tags":["sort","complexity"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"al-2","question":"Compare QuickSort, MergeSort, and Timsort. When would you choose each algorithm and what are their key trade-offs in production systems?","channel":"algorithms","subChannel":"sorting","difficulty":"intermediate","tags":["sort","recursion","complexity"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-300","question":"Explain the difference between quicksort and mergesort, including their time and space complexities?","channel":"algorithms","subChannel":"sorting","difficulty":"beginner","tags":["quicksort","mergesort","complexity"],"companies":["Amazon","Google","Meta"]},{"id":"q-362","question":"Given an array of integers, implement quicksort with proper partitioning, explain its O(n log n) average vs O(n²) worst-case complexity, and compare with mergesort in terms of stability, space usage, and practical performance?","channel":"algorithms","subChannel":"sorting","difficulty":"beginner","tags":["quicksort","mergesort","complexity"],"companies":null},{"id":"q-433","question":"Implement quicksort and explain when you'd choose it over mergesort. What's the worst-case scenario and how do you avoid it?","channel":"algorithms","subChannel":"sorting","difficulty":"beginner","tags":["quicksort","mergesort","complexity"],"companies":["Hashicorp","Oracle","Snowflake"]},{"id":"q-167","question":"Write a function to find the maximum depth of a binary tree using both recursive DFS and iterative BFS approaches. Discuss time/space complexity and handle edge cases?","channel":"algorithms","subChannel":"trees","difficulty":"beginner","tags":["tree","binary"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-314","question":"Given a binary search tree with n nodes, find the kth smallest element where 1 ≤ k ≤ n. Discuss both recursive and iterative approaches with their time and space complexities?","channel":"algorithms","subChannel":"trees","difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"companies":null},{"id":"q-340","question":"Given a BST that may have duplicate values, implement a function to find the kth smallest element considering duplicates. What's the time complexity and how would you handle edge cases?","channel":"algorithms","subChannel":"trees","difficulty":"intermediate","tags":["bst","avl","trie","segment-tree"],"companies":["Hrt","New Relic","Sap"]},{"id":"q-451","question":"Given a BST, write a function to find the kth smallest element using O(h) space and O(n) time, where h is height and n is nodes?","channel":"algorithms","subChannel":"trees","difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"companies":["Anthropic","Cloudflare"]},{"id":"q-660","question":"You’re building a chat app that autocompletes words as users type. Implement a Trie with insert(word), search(word), and startsWith(prefix). Provide a compact JavaScript class with these methods, assuming lowercase a-z. After inserting 'apple','app','application', does search('app') return true and does startsWith('appl') return true?","channel":"algorithms","subChannel":"trees","difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"companies":["Cloudflare","IBM","Slack"]},{"id":"q-258","question":"How would you design a reactive Android ViewModel using StateFlow with sealed classes to handle network API responses, ensuring proper error handling and loading states?","channel":"android","subChannel":"architecture","difficulty":"intermediate","tags":["coroutines","flow","sealed-classes"],"companies":["Airbnb","Google","Meta","Microsoft","Uber"]},{"id":"q-1022","question":"You're building an Android field-inspection app where users collect forms and photos offline. When connectivity returns, design a robust sync engine that uploads only new or updated items, resolves conflicts via last_modified, and handles intermittent networks. Use WorkManager with network constraints and a foreground service for long syncs; include backoff and tests?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Airbnb","Goldman Sachs","Scale Ai"]},{"id":"q-1127","question":"Design a beginner Android feature: a simple offline notes app. Notes stored in Room with fields id, text, tag, last_modified. Provide a tag-based search, and a background export to cloud via WorkManager that runs only on WiFi and while charging. Ensure deduplication by last_modified, handle restarts, and outline a minimal test plan?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Anthropic","Robinhood"]},{"id":"q-1143","question":"You’re building a chat-like Android app. Messages are stored in Room with fields: id (UUID), text (String), timestamp (Long), status (pending, sending, sent, failed). When sending, insert a pending message and enqueue a WorkManager task to upload unsent messages when online, using exponential backoff. On success, save serverId and set status to sent; on failure, keep failed with a retriable option. Outline the data flow, DAO/Worker skeleton, and offline→online test plan?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Snowflake","Twitter"]},{"id":"q-1267","question":"You're building an Android offline-first app that records field observations (id, timestamp, value) in Room. Changes are queued when offline and synced to a REST backend when online. Implement a robust sync with versioning, conflict resolution (server-wins, client-wins, or merge), and tombstones. Describe data flow, DAO/Repository, a WorkManager worker, and a testing strategy for edge cases like concurrent edits and delayed pushes?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Cloudflare","Hugging Face","Tesla"]},{"id":"q-1388","question":"Implement a field survey photo capture in Android: tap Capture to take a photo, request CAMERA permission via the Activity Result API, use TakePicturePreview to obtain a Bitmap, save it to internal storage with a timestamped filename, and display a preview in an ImageView. Ensure rotation safety and a graceful denial/retry flow. What code would you write?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Adobe","Bloomberg","Google"]},{"id":"q-1715","question":"You're building an Android module to support background, continuous BLE logging from an OBD-II dongle for a fleet-tracking app. Requirements: maintain a stable BLE connection with automatic reconnect and backoff; buffer data locally in Room during disconnect; periodically upload batched logs via WorkManager with exponential backoff; use a ForegroundService while in the background; respect Doze/App Standby; outline data flow, architecture, and a minimal skeleton of the BLE manager, DAO, and Worker; include a test plan for connectivity changes and battery constraints?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Databricks","Lyft"]},{"id":"q-1880","question":"You’re building an Android app that streams live telemetry video from a drone's camera over an unstable network. Using CameraX, encoding with H.264, and a hybrid UDP/TCP transport, design a solution that includes buffering and backpressure, a foreground service with Doze-friendly behavior, adaptive bitrate, and robust error handling. Provide data flow, key classes (StreamingManager, PacketSender), and a skeleton implementation plus an end-to-end test plan for packet loss and latency?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Cloudflare","Oracle"]},{"id":"q-2076","question":"You're building a beginner Android app that logs field visits. The main screen shows a list of visits (id, siteName, timestamp). Implement adding a visit via a dialog, persist with Room, and a DataStore-backed sort toggle (timestamp ascending/descending) that preserves the choice across rotations. Describe data flow, DAO/Repository, ViewModel, and a minimal UI test plan?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Amazon","Salesforce","Uber"]},{"id":"q-2143","question":"You're building a beginner Android checklist app: store items in Room (id, title, completed, dueDate), display them in a RecyclerView, and support add/edit/delete. Implement a one-time reminder using WorkManager that triggers a Notification at dueDate, with a backoff if scheduling fails or the device is asleep. Describe data flow (DAO/Repository), Worker implementation, and a test plan for offline item creation and overdue reminders?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Apple","Meta"]},{"id":"q-452","question":"How would you implement a RecyclerView with multiple view types while maintaining smooth scrolling performance on large datasets?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Adobe","Citadel","Google"]},{"id":"q-482","question":"How would you handle Activity lifecycle when screen rotates and you need to preserve user input data?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Amazon","Google","Oracle"]},{"id":"q-512","question":"How would you implement a simple RecyclerView in Android to display a list of user profiles with name and email?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["MongoDB","Tesla","Two Sigma"]},{"id":"q-541","question":"How would you implement a RecyclerView with ViewHolder pattern to display a list of user profiles efficiently?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Databricks","Goldman Sachs","Tesla"]},{"id":"q-976","question":"You are building an Android app that tracks a delivery ride; location updates every 5 seconds; battery life; Doze; Provide plan using FusedLocationProvider, ForegroundService, and WorkManager; include backoff; testing; intermittent connectivity?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Goldman Sachs","Oracle","Uber"]},{"id":"q-205","question":"How would you implement Compose Navigation with nested graphs, shared ViewModels, configuration change handling, and deep linking in a production Android app?","channel":"android","subChannel":"jetpack-compose","difficulty":"intermediate","tags":["composables","state","navigation"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-182","question":"What is the first lifecycle method called when an Android Activity is created, and what critical initialization tasks must be performed within it?","channel":"android","subChannel":"lifecycle","difficulty":"beginner","tags":["lifecycle","components"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-236","question":"How would you implement a comprehensive contract testing strategy using MSW (Mock Service Worker) with OpenAPI to ensure frontend API mocks stay synchronized with backend specifications, including CI/CD integration and drift detection?","channel":"api-testing","subChannel":"contract-testing","difficulty":"intermediate","tags":["wiremock","mockserver","msw"],"companies":["Amazon","Microsoft","Netflix","Salesforce","Square","Stripe"]},{"id":"q-1030","question":"Design a test strategy for an API gateway that enforces per-tenant sliding-window rate limits with dynamic quotas updated via admin API. Outline how you'd simulate high-concurrency traffic, verify quota propagation across nodes, validate headers and 429 responses, and test failure modes (Redis outage or misconfig). Include concrete test cases and tooling suggestions?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Citadel","Google","Meta"]},{"id":"q-1050","question":"Design an automated test plan for a REST + streaming API: /inventory/{sku}/status returns current stock via a streaming endpoint /inventory/stream (Server-Sent Events). The plan should cover stream resilience, event deduplication, per-warehouse aggregation under bursts, and failure modes when downstream storage becomes partially unavailable. Provide concrete test cases, tooling suggestions, and expected outcomes, with emphasis on realism for high-scale retail backends?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Amazon","Apple","DoorDash"]},{"id":"q-1061","question":"Design a practical test plan for a GraphQL API that aggregates data from products, pricing, and reviews. Include how you validate query depth limits, detect N+1 issues, test caching and cache invalidation under high concurrency, and ensure partial responses when some downstream services fail. Provide concrete test cases and tooling?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Instacart","NVIDIA","PayPal"]},{"id":"q-1166","question":"You manage a public REST API with versioning: /v1/... and /v2/... Design a practical, automated test plan to validate backward compatibility as v2 introduces a new field (tags) and changes a field type (price from int to decimal). Include concrete test cases, OpenAPI contract checks, cross-version schema validation, and how to verify deprecation behavior via a warnings header and 429s for old clients. Outline tooling and steps?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["Google","Lyft","NVIDIA"]},{"id":"q-1251","question":"You're testing an inventory REST API for a global retail platform. `/inventory/{sku}` returns current stock and scheduled restock ETA from a separate availability service that uses eventual consistency and a message bus (Kafka). Design a practical test plan to verify consistency windows, eventual accuracy under bursts, and cross-region cache coherency, including late-arriving events, out-of-order messages, and failure modes of Kafka and Redis caching. Include concrete test cases, tooling, and expected outcomes?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Discord","Square"]},{"id":"q-1284","question":"You maintain a simple REST API with POST /checkout that creates a payment intent for a cart. As a beginner, outline an end-to-end test plan to verify input validation, idempotent retries, and error handling under transient failures. Include concrete test cases, tooling suggestions, and expected responses?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["DoorDash","MongoDB"]},{"id":"q-1720","question":"Design a test strategy for a multi-tenant GraphQL API exposing persisted queries. Include N+1 detection, depth limits, per-tenant access controls, cross-tenant isolation, mutation cache invalidation, and canary schema rollout?","channel":"api-testing","subChannel":"general","difficulty":"advanced","tags":["api-testing"],"companies":["Google","Hashicorp","Microsoft"]},{"id":"q-1744","question":"Design a test for a bulk user import API: POST /imports/users accepts up to 100k records, uses an idempotency-key, and publishes per-record results to a downstream analytics queue. Specify test data, idempotency scenarios, partial downstream failures, retry/backoff, and observability. Include concrete cases, tooling, and expected outcomes?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Airbnb","Hashicorp","Meta"]},{"id":"q-1957","question":"Design a concrete, end-to-end automated test plan for an asynchronous data-processing API: POST /data/process returns 202 with an operation-id; messages flow via Kafka to downstream workers; results exposed at GET /data/process/{operationId}/result. Include idempotency tests (Idempotency-Key), eventual consistency with versioning, and fault-injection scenarios (Kafka outage, consumer restart, DB outage) with concrete test cases and tooling recommendations?","channel":"api-testing","subChannel":"general","difficulty":"advanced","tags":["api-testing"],"companies":["Google","Plaid","Two Sigma"]},{"id":"q-2017","question":"Design a beginner-friendly test plan for an API endpoint GET /products/{id} that uses a real-time feature flag to toggle response fields. Validate that additional fields (e.g., discount, promotionTag) appear when the flag is ON and disappear when OFF; cover admin-API downtime fallback; and test mid-request toggles with concurrent requests to ensure each response matches its observed flag. Include tooling suggestions?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["Adobe","Meta","Plaid"]},{"id":"q-453","question":"You're testing a REST API that returns paginated results. The endpoint has a rate limit of 100 requests per minute and sometimes returns 500 errors under load. How would you design a comprehensive test strategy?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Airbnb","Google","Lyft"]},{"id":"q-483","question":"You're testing a REST API that returns paginated results. How would you design a comprehensive test strategy to verify pagination works correctly across different page sizes and edge cases?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Discord","Netflix","Salesforce"]},{"id":"q-513","question":"How would you test a REST API endpoint that returns user data, including both success and error scenarios?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["Bloomberg","OpenAI"]},{"id":"q-542","question":"You're testing a payment API that processes transactions. How would you design test cases to verify idempotency, and what specific HTTP status codes would you expect for duplicate requests?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Cloudflare","NVIDIA","Stripe"]},{"id":"q-566","question":"How would you design a comprehensive API testing strategy for a machine learning model deployment pipeline that handles real-time inference requests?","channel":"api-testing","subChannel":"general","difficulty":"advanced","tags":["api-testing"],"companies":["Goldman Sachs","Hugging Face","Snap"]},{"id":"q-909","question":"Design a practical test plan for an asynchronous data ingestion API: POST / ing est accepts CSV payload and returns 202 with a job_id. It enqueues to a queue, then a worker writes to storage and updates a /status/{job_id} endpoint. Some tenants require PII redaction controlled by a tenant flag; a 'force' param bypasses CSV schema validation. Outline concrete test cases to verify correctness, privacy, idempotency, race conditions, and failure modes when queue or storage fail. Include sample CSV payloads and expected outcomes?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Robinhood","Snowflake","Two Sigma"]},{"id":"q-209","question":"How would you design a REST API testing framework that handles rate limiting, circuit breaking, and distributed tracing for microservices with 10,000+ concurrent requests?","channel":"api-testing","subChannel":"rest-testing","difficulty":"advanced","tags":["postman","rest-assured","supertest"],"companies":["Amazon","Goldman Sachs","Microsoft","Netflix","Stripe"]},{"id":"gh-12","question":"What are the three main service models of cloud computing and how do they differ?","channel":"aws","subChannel":"compute","difficulty":"beginner","tags":["cloud","aws","azure","gcp"],"companies":["Amazon","Google","Meta"]},{"id":"gh-13","question":"What is AWS (Amazon Web Services)?","channel":"aws","subChannel":"compute","difficulty":"beginner","tags":["cloud","aws","azure","gcp"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Netflix"]},{"id":"gh-15","question":"Compare AWS IaaS, PaaS, and SaaS service models with specific examples and use cases?","channel":"aws","subChannel":"compute","difficulty":"intermediate","tags":["cloud","aws","azure","gcp"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-34","question":"How would you design an Auto Scaling configuration for a high-traffic e-commerce application that handles 10,000 RPS with 99.99% availability, including scaling policies, health checks, and cost optimization?","channel":"aws","subChannel":"compute","difficulty":"advanced","tags":["scale","ha"],"companies":null},{"id":"gh-57","question":"What is Cloud Cost Optimization and what are the key strategies to reduce cloud spending in production environments?","channel":"aws","subChannel":"compute","difficulty":"beginner","tags":["finops","cost"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-58","question":"What are AWS Reserved Instances and how do they compare to On-Demand pricing?","channel":"aws","subChannel":"compute","difficulty":"intermediate","tags":["finops","cost"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Uber"]},{"id":"gh-83","question":"How do you evaluate cloud services for business needs using TCO analysis, SLA metrics, and migration strategies?","channel":"aws","subChannel":"compute","difficulty":"advanced","tags":["migration","cloud"],"companies":["Amazon","Google","IBM","Microsoft","Oracle","Salesforce"]},{"id":"gh-85","question":"How do cloud migration tools automate application and data transfer between on-premise and cloud environments, and what are the key technical challenges in ensuring data consistency and minimal downtime?","channel":"aws","subChannel":"compute","difficulty":"intermediate","tags":["migration","cloud"],"companies":["Amazon","Citadel","Goldman Sachs","Google","Microsoft"]},{"id":"gh-87","question":"How would you implement a multi-cloud cost allocation system using tagging strategies and automation APIs?","channel":"aws","subChannel":"compute","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Stripe","Uber"]},{"id":"q-174","question":"You have an EC2 instance that suddenly becomes unresponsive. What step-by-step troubleshooting methodology would you follow, which specific AWS tools and commands would you use at each stage, and how would you handle different instance states and recovery scenarios?","channel":"aws","subChannel":"compute","difficulty":"intermediate","tags":["ec2","compute"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-321","question":"You have a containerized web application that needs to handle variable traffic loads. When would you choose ECS Fargate over EKS and what are the key trade-offs?","channel":"aws","subChannel":"compute","difficulty":"beginner","tags":["ec2","ecs","eks","fargate"],"companies":["Cloudflare","Figma","MongoDB"]},{"id":"q-216","question":"How would you design an eventual consistency strategy for a multi-region DynamoDB application using Global Tables to handle write conflicts, ensure data convergence, and minimize latency?","channel":"aws","subChannel":"database","difficulty":"intermediate","tags":["mongodb","dynamodb","cassandra","redis"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-357","question":"You're designing a security monitoring system that needs to store 10M+ events per day with millisecond read latency. How would you choose between DynamoDB, Aurora, and ElastiCache, and what's your data partitioning strategy?","channel":"aws","subChannel":"database","difficulty":"intermediate","tags":["rds","aurora","dynamodb","elasticache"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix","Palo Alto Networks"]},{"id":"q-401","question":"You're designing a real-time analytics dashboard for Scale AI that needs to handle 10,000 events/second. Your team is debating between using DynamoDB with DAX vs. Aurora with ElastiCache. What are the key trade-offs you'd consider, and which would you choose for this use case?","channel":"aws","subChannel":"database","difficulty":"intermediate","tags":["rds","aurora","dynamodb","elasticache"],"companies":["Cohere","Oscar Health","Scale Ai"]},{"id":"q-413","question":"You're designing a real-time analytics dashboard for an IoT application that receives 10,000 events per second. The dashboard needs to show current metrics and historical trends. How would you design the database architecture using AWS services, and what caching strategy would you implement?","channel":"aws","subChannel":"database","difficulty":"intermediate","tags":["rds","aurora","dynamodb","elasticache"],"companies":["Amazon","Apple","Databricks","Google","Micron","Microsoft","Netflix","Snowflake"]},{"id":"q-1256","question":"You're operating a global real-time analytics pipeline on AWS: data streams from mobile apps ingest via Kinesis Data Streams, processed by Lambda, stored in DynamoDB and S3 Parquet. A new release causes timeouts and duplicate processing under peak load. Propose a concrete plan to fix cold starts and throttling, ensure exactly-once semantics, and safely deploy with minimal data loss. Include services, config values, and rollout steps?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Amazon","Robinhood","Two Sigma"]},{"id":"q-454","question":"You need to host a static website with high availability and low latency globally. How would you configure AWS S3 and CloudFront to achieve this?","channel":"aws","subChannel":"general","difficulty":"beginner","tags":["aws"],"companies":["Cloudflare","Google","Netflix"]},{"id":"q-484","question":"You're designing a real-time ML inference pipeline on AWS that must process 10,000 requests/second with sub-100ms latency. How would you architect this using serverless components, and what trade-offs would you consider?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Databricks","Hugging Face","Snowflake"]},{"id":"q-514","question":"You're building a serverless application that needs to process user uploads. How would you design an architecture using S3, Lambda, and API Gateway to handle file uploads securely and efficiently?","channel":"aws","subChannel":"general","difficulty":"beginner","tags":["aws"],"companies":["OpenAI","Stripe"]},{"id":"q-543","question":"You're deploying a microservices application on AWS ECS. One service is experiencing intermittent 503 errors during peak traffic. How would you diagnose and resolve this issue?","channel":"aws","subChannel":"general","difficulty":"intermediate","tags":["aws"],"companies":["Apple","Bloomberg","Meta"]},{"id":"q-567","question":"How would you design a multi-region serverless architecture for a real-time chat application using AWS services, ensuring low latency and high availability?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Slack","Tesla"]},{"id":"q-220","question":"How would you design a multi-AZ VPC architecture with Route53 latency-based routing to CloudFront, ALB, and private EC2 instances while ensuring failover within 30 seconds?","channel":"aws","subChannel":"networking","difficulty":"intermediate","tags":["vpc","route53","cloudfront","alb"],"companies":["Amazon","Databricks","Goldman Sachs","Microsoft","Netflix"]},{"id":"q-384","question":"You're designing a multi-region SaaS application with users in North America and Europe. How would you configure Route53, CloudFront, ALB, and VPC to ensure low latency and high availability? What are the key trade-offs?","channel":"aws","subChannel":"networking","difficulty":"intermediate","tags":["vpc","route53","cloudfront","alb"],"companies":["Airtable","Cisco","Epic Games"]},{"id":"gh-66","question":"How does serverless computing abstract infrastructure management and what are its key execution characteristics?","channel":"aws","subChannel":"serverless","difficulty":"beginner","tags":["serverless","lambda"],"companies":["Airbnb","Amazon","Google","Microsoft","Uber"]},{"id":"q-246","question":"How would you design a serverless order processing workflow using AWS Step Functions with Lambda functions, implementing specific retry patterns, error handling, and state management?","channel":"aws","subChannel":"serverless","difficulty":"intermediate","tags":["lambda","api-gateway","step-functions"],"companies":["Amazon","Google","Microsoft","Netflix","Salesforce","Stripe"]},{"id":"q-292","question":"How would you design a data lifecycle strategy for a media company storing petabytes of video content requiring immediate access, archiving, and cost optimization across AWS storage services?","channel":"aws","subChannel":"storage","difficulty":"advanced","tags":["s3","ebs","efs","glacier"],"companies":["Meta","Netflix","Youtube"]},{"id":"q-307","question":"What are the key differences between S3, EBS, and EFS in terms of performance, scalability, and use cases?","channel":"aws","subChannel":"storage","difficulty":"intermediate","tags":["s3","ebs","efs","glacier"],"companies":["Amazon","Google","Meta"]},{"id":"q-370","question":"You're designing a file storage system for Canva's design assets. Users upload large PSD files (up to 10GB) that need versioning and quick access. How would you architect this using AWS storage services, considering cost, performance, and durability?","channel":"aws","subChannel":"storage","difficulty":"intermediate","tags":["s3","ebs","efs","glacier"],"companies":["Affirm","Booking.com","Canva"]},{"id":"q-1001","question":"You're building a beginner-friendly AWS-only pipeline to ingest customer chat transcripts (text files up to 50 KB) uploaded to S3. Design how to automatically redact PII using Amazon Comprehend PII detection, store the redacted transcript back to S3, and index metadata in DynamoDB. Include data flow, IAM permissions, error handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Snowflake","Zoom"]},{"id":"q-1058","question":"Design a cross region, multi account AI inference platform for real time pricing and risk scoring in a fintech setting. Ingest streaming data, enforce per tenant data residency, and meet sub 100 ms latency. Describe data flow, services, IAM boundaries, model registry, feature store, drift monitoring, error handling, and cost controls?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Coinbase","NVIDIA","Stripe"]},{"id":"q-1259","question":"Design an end-to-end AWS-native real-time fraud detection pipeline for a global e-commerce platform. Ingest event streams (Kinesis Data Streams), redact PII, create real-time features stored in SageMaker Feature Store (online) and offline store, with governance, lineage, access control, and cost constraints. Include data flow, IAM, retry logic, backpressure, testing, and incident response?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Airbnb","Cloudflare","Salesforce"]},{"id":"q-1292","question":"Design a real-time fraud-detection pipeline on AWS for a FinTech use-case. Ingest streaming transactions via Kinesis Data Streams, preprocess with Lambda, and invoke a SageMaker endpoint for real-time scores. Persist results to DynamoDB with audit logs in S3. Address latency (<200 ms), data privacy (KMS, VPC endpoints), IAM, drift monitoring, error handling, and cost control. Provide concrete components and trade-offs?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Citadel","Coinbase"]},{"id":"q-1313","question":"You're designing a beginner-friendly AWS-only pipeline for user-submitted PDFs (max 5 MB) uploaded to S3. Build an end-to-end workflow that uses Textract to extract text, runs a lightweight topic model via Comprehend or a tiny SageMaker endpoint to derive a topic label, stores a compact summary and a searchable index in DynamoDB, and exposes a read path via API Gateway + Lambda. Include data flow, IAM roles, error handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Google","Snap"]},{"id":"q-1366","question":"You're operating a real-time text classifier API on a SageMaker hosting endpoint behind API Gateway. You need a canary deployment strategy using CodeDeploy for SageMaker endpoints, with 2% of traffic to the new version, ramping to 50% over 2 hours, then full if no regressions. Describe data flow, required services, IAM, drift detection via Model Monitor, and rollback/failover logic, plus privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["NVIDIA","Netflix","Salesforce"]},{"id":"q-1437","question":"You're building a beginner-friendly, AWS‑only pipeline to process user-submitted emails stored as JSON in S3 (max 20 KB per file). Design an end-to-end workflow that detects language, translates to English, analyzes sentiment, and stores results in DynamoDB, with a simple read path via API Gateway + Lambda. Include data flow, services, error handling, IAM roles, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["LinkedIn","Robinhood","Tesla"]},{"id":"q-1460","question":"Design an advanced, low-latency fraud-detection pipeline for real-time payments in AWS. Ingest events via Kinesis Data Streams, derive features into SageMaker Feature Store, train/deploy with SageMaker Pipelines, score via a real-time SageMaker Endpoint called from Lambda, and log audits in CloudTrail. Enforce KMS encryption, data minimization/retention policies, IAM least privilege, and a rollback/failover plan. Include data flow, IAM, privacy, and failure handling?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Bloomberg","PayPal"]},{"id":"q-1482","question":"Design a production-ready, AWS-native content moderation pipeline for a multi-tenant SaaS platform handling images and captions uploaded to S3. Must route per-tenant policies to a custom SageMaker multi-model endpoint, apply per-tenant threshold overrides, enforce data isolation, log audit trails, and keep costs predictable with auto-scaling and caching. Describe data flow, IAM, encryption, error handling, and privacy implications?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Adobe","Cloudflare","MongoDB"]},{"id":"q-1552","question":"You're deploying a privacy-preserving AI rating model for financial support tickets on AWS. Incoming tickets arrive as JSON (up to 10 KB) in S3; design an end-to-end pipeline that uses SageMaker for inference, writes per-ticket audit logs to DynamoDB, encrypts data at rest with SSE-KMS, and supports reliable, rate-limited processing with error handling and drift monitoring. How would you implement this?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["PayPal","Scale Ai"]},{"id":"q-1583","question":"You're running a real-time fraud-detection model for payments. Data arrives in a Kinesis stream; design an end-to-end AWS-native pipeline that performs per-record inference, monitors drift and bias with SageMaker Clarify/Model Monitor, archives data in S3 with metadata, and auto-triggers a rollback to a previous model version via Step Functions if drift thresholds are exceeded. Include data flow, IAM, encryption, backpressure handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Amazon","Anthropic","Snowflake"]},{"id":"q-1627","question":"You're building an AWS-native ML platform that serves a global analytics product with data residing in Snowflake and Databricks across regions. Design a pipeline that ingests from both sources, uses SageMaker Feature Store, trains/registers with a human approval, serves real-time inference, and uses HashiCorp Vault for secrets. Include data lineage, IAM, encryption, drift monitoring, rollback policy, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Databricks","Hashicorp","Snowflake"]},{"id":"q-1750","question":"You're building a beginner-friendly AWS-only pipeline for multilingual customer recordings. Audio files (up to 90 seconds) are uploaded to S3. Design an event-driven flow that uses Amazon Transcribe to produce a transcript in the source language, Amazon Translate to produce a Spanish version, stores both transcripts in S3, and writes a compact index in DynamoDB with fields like customerId, fileKey, sourceLang, translatedKey. Include data flow, IAM, error handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Anthropic","PayPal"]},{"id":"q-1789","question":"Design an AWS-native, real-time fraud-detection pipeline: ingest streaming transactions from Kinesis Data Streams, score risk with a SageMaker Inference endpoint (Transformer-based), store scores in DynamoDB, and trigger a Step Functions workflow for high-risk events. Include data flow, IAM, privacy controls, drift monitoring, and cost governance?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["DoorDash","Goldman Sachs"]},{"id":"q-1800","question":"You're building a compliant multi-tenant, multilingual content moderation service on AWS. Users upload video to S3; extract audio, transcribe with Amazon Transcribe, run a SageMaker classifier to flag policy-violating content, redact PII, and store transcripts and decisions, indexing per-tenant metadata in DynamoDB. Design an end-to-end, event-driven pipeline with data isolation, data sovereignty, error handling, and privacy controls?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Google","Square"]},{"id":"q-1831","question":"You're building a beginner AWS-only pipeline to process user-uploaded audio stories (up to 120 seconds). When an audio file lands in S3, design an event-driven flow that uses Amazon Transcribe to produce a transcript, Amazon Comprehend to extract keywords, and a SageMaker endpoint to classify genre. Save transcript and metadata to S3 and index in DynamoDB. Include data flow, IAM permissions, error handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Adobe","Anthropic","Plaid"]},{"id":"q-1902","question":"You're building a beginner AWS-only pipeline to moderate user-uploaded profile pictures (JPEG/PNG, up to 2MB) in S3; on upload, design an event-driven flow using Rekognition to detect unsafe content and top labels, write a JSON report to S3, and upsert a summary in DynamoDB with fields like userId, objectKey, safeFlag, topLabel. Include data flow, IAM, error handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Databricks","MongoDB"]},{"id":"q-1935","question":"Design a real-time, multi-tenant fraud-detection pipeline for a SaaS platform. Ingestion uses Kinesis Data Streams per tenant; a central SageMaker detector scores each event; PII is redacted before storage in per-tenant S3 buckets; an index is kept in DynamoDB with fields tenantId, sessionId, eventTime, fraudScore. Explain data isolation, cross-account IAM, error handling, privacy controls, and idempotency/replay safety?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Adobe","PayPal"]},{"id":"q-2018","question":"Design an end-to-end, event-driven AWS pipeline for a regulated financial analytics service. Tenants upload encrypted JSON trade logs (up to 5 MB) to S3. Build per-tenant, region-isolated processing: decrypt with KMS, run a SageMaker multi-tenant analytics model, redact PII with Comprehend and regex, and store outputs in region-scoped S3 prefixes. Maintain an immutable audit trail in DynamoDB with versioning, implement retry and DLQ, and enforce strict IAM boundaries?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Adobe","Discord","Uber"]},{"id":"q-2045","question":"Design a real-time, AWS-only PII redaction pipeline for streaming chat messages. Ingest messages enter regional Kinesis Data Streams with tenant isolation. Use SageMaker endpoint (or Comprehend) to detect PII, redact spans, store redacted messages in S3, and index per-tenant audit logs in DynamoDB (tenantId, messageId, detectedPII, redacted, timestamp). Include data flow, IAM, error handling, privacy controls, and rate-limiting?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["DoorDash","Lyft","Netflix"]},{"id":"q-2075","question":"Design a real-time fraud detection pipeline for a high-volume platform used by Lyft, PayPal, and Snap. Ingest payment events into Kinesis, enrich with per-tenant risk data from DynamoDB via Lambda, compute a fraud score with a SageMaker endpoint, and enforce decisions in DynamoDB while routing high-risk events to a Step Functions human-in-the-loop workflow and notifying security via SNS. Include data isolation, sub-200 ms latency budget, and privacy controls with PII masking and audit trails?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Lyft","PayPal","Snap"]},{"id":"q-951","question":"You're building a low‑ops internal voice assistant for support scripts. Audio files up to 60 seconds are uploaded to S3. Design an AWS‑only pipeline that transcribes, analyzes sentiment, and stores a brief summary plus an index in DynamoDB, with minimal cost and ops. Include data flow, services, error handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["MongoDB","Snap"]},{"id":"q-1336","question":"In a multi-account AWS data lake used by a global product analytics team, ingest semi-structured data from several SaaS APIs into S3, enforce schema evolution and data quality checks, and expose curated tables with time-travel queries. Describe the end-to-end design including data formats, upsert strategy, and governance controls you would apply using AWS services?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Amazon","Apple"]},{"id":"q-1419","question":"Design an advanced, multi-region data ingestion and governance pipeline for a global fintech platform. Data from partners arrives as JSON and Parquet; must enforce per-tenant isolation, support near real-time analytics, and GDPR erasure. Outline architecture using AWS Kinesis (streams), DMS for CDC, Glue (ETL), Iceberg on S3 for schema-evolving tables, Lake Formation/IAM for access, and Athena/Redshift Spectrum for queries. Include format choices, CDC vs batch mix, lineage, and failure modes?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Meta","Stripe"]},{"id":"q-1487","question":"You operate an AWS data lake where streaming user activity flows from Kinesis Data Streams into S3 via Firehose, with Glue Data Catalog, Athena queries, and dashboards. Schemas evolve and data quality is mission-critical. Design a real-time data quality and drift detection framework: schema drift, per-record quality checks, quarantine failing files, alert owners, and preserve cross-account auditability?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Adobe","Coinbase","Oracle"]},{"id":"q-1506","question":"Scenario: A partner API provides daily batches of image assets with evolving metadata. Build an end-to-end ingest to a multi-region data lake: landing in S3, metadata in a versioned, upsertable table, idempotent processing, schema evolution, and governance across accounts. Describe data formats, partitioning, dedupe strategy, and cross-account orchestration?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Instacart","Slack","Stripe"]},{"id":"q-1521","question":"You manage a global, multi-tenant data platform where streaming events bucket into S3 and downstream engines like Redshift and Athena rely on per-tenant schemas. Design a resilient end-to-end architecture that versions datasets, enforces per-tenant schema contracts, detects drift, quarantines bad records, and supports rollback without downtime. Specify services, data formats, governance, and observability?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Adobe","Google","NVIDIA"]},{"id":"q-1543","question":"Design an event-driven data platform to ingest order and driver events from regional services into a centralized data lake for near-real-time analytics. Data arrives as evolving JSON schemas; must handle schema evolution, efficient partitioning, cross-account access, and cost. Which AWS services and data design patterns would you implement, and how would you validate data quality and enable fast ad-hoc queries?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["DoorDash","Square"]},{"id":"q-1608","question":"Scenario: A new source streams JSON events from a mobile app via Kinesis Data Streams. Design a beginner-level ingestion pipeline to land data in S3 as Parquet with daily partitions, using AWS Glue for ETL and cataloging. Include services, data formats, schema evolution approach, basic data quality checks, and how you would test end-to-end before analytics?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["NVIDIA","Netflix","Zoom"]},{"id":"q-1631","question":"You receive a daily nested JSON file and a separate CSV in an S3 bucket. Design a beginner-level ingestion using AWS Glue Studio to flatten the JSON, normalize types, parse the CSV, and write Parquet under a date-partitioned path s3://data-lake/events/date=YYYY-MM-DD/. Register a Glue catalog table, outline simple schema-drift handling, and include basic data-quality checks plus a quick end-to-end test plan with sample files?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["Instacart","Oracle","Twitter"]},{"id":"q-1654","question":"New data source writes daily CSVs to S3. Design a beginner pipeline to validate a defined schema, fail-fast on missing required fields, and route bad records to a quarantine bucket with a reason. Store valid data as Parquet in S3 with daily partitions, and catalog via Glue so Athena can query. Include a basic end-to-end test plan?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["Plaid","Two Sigma","Zoom"]},{"id":"q-1785","question":"Two-account AWS data lake (us-east-1 and eu-west-1) ingests partner JSON events via API into S3. Design an end-to-end pipeline using Glue for ETL and cataloging, store Parquet with daily partitions by country/date, enable Athena queries with Lake Formation access controls, and handle schema evolution, data quality, late-arrivals, and cross-region replication trade-offs?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Lyft","Snap"]},{"id":"q-1816","question":"You collect protobuf-encoded telemetry from thousands of IoT devices via AWS IoT Core; design an end-to-end ingestion that lands as daily-partitioned Parquet in S3, uses AWS Glue Schema Registry for evolving schemas, and implements idempotent deduplication plus a 3-sigma anomaly check. Include data formats, partition keys, testing strategy?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Amazon","Meta","Square"]},{"id":"q-1883","question":"Design a streaming data contract and quality workflow for events arriving from multiple teams via MSK and Kinesis. Use AWS Glue Schema Registry to enforce evolving Avro/JSON schemas with versioning and compatibility (backward/forward). Describe publishing schemas, validating payloads at ingest, storing Parquet in S3 with daily partitions, and monitoring for drift and quality across regions?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Cloudflare","Google"]},{"id":"q-1968","question":"Design an end-to-end streaming-to-lake pipeline: ingest real-time events from AWS MSK, consolidate into Apache Iceberg tables on S3 with daily partitions, support CDC-style upserts/deletes, and implement schema evolution. Include governance with Lake Formation/IAM, testing strategy (canaries, synthetic data), and data quality/lineage monitoring?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Anthropic","Google"]},{"id":"q-1981","question":"Design a data lake ingestion pipeline on S3 for semi-structured SaaS data that must support upserts and deletes with time-travel queries; choose between Apache Iceberg, Hudi, or Delta Lake on AWS (Glue, EMR, Athena) and justify: schema evolution, compaction, CDC handling, partitioning, and data quality checks; include testing plan?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Hashicorp","Instacart","Two Sigma"]},{"id":"q-2019","question":"Scenario: IoT sensors on factory floors emit JSON telemetry with evolving schemas (new metrics appear over time). Design a streaming ingestion pipeline that reads from Kinesis Data Streams, writes to S3 as Parquet with daily partitions, and uses an Iceberg catalog (Glue-backed) to handle schema evolution. Compare upsert strategies for late data (Iceberg MERGE INTO vs Hudi), outline data quality checks, and a testing plan including sandbox data and end-to-end validation. Include governance via Lake Formation?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Citadel","IBM","Tesla"]},{"id":"q-2060","question":"Design a multi-region, streaming-plus-batch ingestion pipeline for a mobile app that emits JSON events at high volume. Ingest via Kinesis Data Streams to S3 Parquet using Apache Iceberg tables with a Glue catalog, supporting upserts, deletes, and schema evolution. Include multi-source CDC, data quality checks, governance via Lake Formation, and end-to-end testing?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["LinkedIn","Meta","PayPal"]},{"id":"q-2095","question":"You receive a financial dataset hourly from a partner API in JSON with nested fields. Design a beginner-level ingestion pipeline to land data in S3 as Parquet with hourly partitions, using AWS Glue for ETL and cataloging. Include how you flatten nested JSON, define a stable schema, enable basic data quality checks, ensure encryption with KMS, and implement least-privilege IAM roles and Lake Formation permissions. Outline how you'd test end-to-end before analytics?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["Robinhood","Snap"]},{"id":"q-2144","question":"Design a real-time fraud signals pipeline: ingest 50 banks via Kinesis Data Streams, use Glue Schema Registry for evolving Avro schemas, run Spark Streaming to land Parquet in S3 partitioned by date and merchantId, with a raw zone and a masked curated zone. Apply IP masking and immutability retention; catalog with Glue Data Catalog; enforce access via Lake Formation; emit lineage/audit logs. Include failover tests and latency considerations?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Plaid","Square"]},{"id":"q-886","question":"Scenario: you manage a financial data lake with multiple transactional sources feeding S3 via DMS. Stakeholders require upserts, full history for compliance, fast dashboards using a latest-state table, and seamless schema evolution. Compare Iceberg, Hudi, and Delta Lake for this scenario and outline a concrete pipeline (CDC, ETL, compaction, governance)?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Goldman Sachs","Tesla"]},{"id":"q-948","question":"In a multi-tenant data lake on S3 across two AWS accounts, each tenant's data must be isolated, with auditable access, cost accounting, and fast analytics for dashboards. Design a solution using AWS Lake Formation for governance, S3 prefixes per tenant, and Athena/Glue for analytics. Explain cross-account sharing, tag-based access, and data lineage, plus failure modes?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Apple","Discord","Netflix"]},{"id":"q-1074","question":"Scenario: A global time-series platform ingests 1M events/hour in us-west-2; dashboards in eu-central-1 and ap-southeast-2 need sub-200ms reads on the latest window. Data must be immutable for 90 days for compliance. Compare DynamoDB Global Tables with DAX vs Aurora PostgreSQL Global Database with cross-region backups. Provide topology, replication, PITR/backup plans, and RPO/RTO targets?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Apple","Microsoft"]},{"id":"q-1131","question":"**Hybrid Analytics Path for Multiregion Aurora**\n\nYou're running an Aurora PostgreSQL OLTP cluster with tenant isolation via RLS in us-east-1. A regulatory BI team in eu-west-1 requires near real-time analytics with masked PII. Design a hybrid analytics path using Aurora Global Database for OLTP replicas and a CDC-based analytic store (Redshift or DynamoDB+Lambda) in eu-west-1. Describe data flow, masking strategy, encryption, failover, and how to meet RPO 5s and RTO 60s, including cost considerations?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Coinbase","Goldman Sachs","Two Sigma"]},{"id":"q-1279","question":"In a multi-tenant SaaS on AWS, run a single Aurora PostgreSQL cluster with per-tenant schemas and RLS to isolate data. An analytics team in eu-west-1 requires cross-tenant BI with masked PII in near real-time dashboards. Design a cost-aware architecture that delivers masking, auditing, and SLA, comparing per-tenant schemas in a single cluster vs separate clusters per tenant. Include data flow, backup, and failover?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Amazon","Hugging Face","Robinhood"]},{"id":"q-1303","question":"In a multi-tenant SaaS using Aurora PostgreSQL with Global Database spanning us-west-2 and us-east-1, tenants must have isolated data access and BI dashboards must mask PII in real time. Propose an end-to-end design using per-tenant RLS, dynamic masking for BI, and a separate analytics store fed by CDC (DMS/Debezium). Include cross-region DR with RPO <5s and RTO <60s, data flow, encryption, backups, and a concrete sizing plan (replicas, window, network)?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Databricks","Lyft","Tesla"]},{"id":"q-1314","question":"Design a GDPR-compliant data deletion strategy for a multi-region Aurora PostgreSQL Global Database that uses us-east-1 as the writer and replicas in multiple regions. How would you implement Right-to-Erasure for tenant data, propagate deletions with minimal latency, handle referential integrity, and maintain an auditable trail while meeting RPO/RTO targets? Include practical steps and trade-offs?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Google","Hashicorp","Lyft"]},{"id":"q-1382","question":"In an Aurora PostgreSQL Global Database with a writer in us-east-1 and replicas in eu-west-1, a financial balance update must be atomic across regions. Explain why cross-region distributed transactions are not supported and propose a practical pattern to achieve atomic-ish behavior with low latency, including data flow, failover handling, and cost/latency trade-offs?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Airbnb","Stripe"]},{"id":"q-1441","question":"In Aurora PostgreSQL (us-east-1) with tenant isolation via RLS, design a near real-time analytics path for a eu-west-1 consumer needing masked data and <5s lag. Use Aurora Global Database for OLTP and a CDC store in eu-west-1 (Redshift or DynamoDB+Lambda). Explain data flow, masking/encryption, consistency, failover, and cost with concrete config sketches?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Amazon","Microsoft"]},{"id":"q-1447","question":"You run a high-volume ecommerce on Aurora PostgreSQL Global Database with a single writer in us-east-1 and read replicas in eu-west-1. An outage in us-east-1 requires routing writes to eu-west-1 within 60s while ensuring RPO<5s, idempotent writes, and no double billing. Design the architecture and concrete configuration (replica counts, failover procedures, analytics CDC path, masking, PITR) to meet these goals?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Airbnb","Hugging Face"]},{"id":"q-1550","question":"In a two-region deployment with a single writer in us-east-1 and analytic reads in eu-west-1, design a CDC pipeline to keep a near real-time analytic store updated within 5 seconds of commits, while masking per-tenant data and enforcing encryption at rest and in transit. Compare AWS DMS, Debezium/Kafka, and native Aurora logical replication, and provide concrete configuration (engine, instance types, replica counts, PITR, KMS keys, VPC endpoints, and network topology) to meet RPO 5s and RTO 60s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Discord","DoorDash","Snowflake"]},{"id":"q-1747","question":"You run a multi-region SaaS with Aurora PostgreSQL as the OLTP in us-east-1 and read replicas in eu-west-1. A new requirement enforces strict per-tenant data isolation via Row-Level Security and data residency controls for backups. Design a concrete approach: RLS policy skeletons for all tables, session-based tenant_id from authentication, per-tenant restore strategy, cross-region backup copy schedule, and a testing/validation plan that proves no cross-tenant leakage under burden. Include concrete config knobs and a sample policy?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["MongoDB","Slack"]},{"id":"q-1775","question":"In a multi-region Aurora PostgreSQL Global Database setup (writer in us-east-1; readers in eu-west-1 and ap-south-1) with strict tenant-level data residency, design a scalable architecture that provides sub-50ms reads for hot paths in each region while ensuring RPO <= 5s and RTO <= 60s, using row-level security and a CDC-based analytic store; explain data partitioning, access controls, and failover strategy, plus cost trade-offs?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Adobe","Zoom"]},{"id":"q-1882","question":"Design a cross-region analytic path for a SaaS app with an Aurora PostgreSQL OLTP cluster in us-east-1 as the single writer and regional read replicas in us-west-2. The goal: near real-time analytics with masked PII in the analytics store. Propose a CDC-based pipeline (Aurora CDC, DMS, Debezium, or Kinesis) to load into Redshift or DynamoDB in us-west-2, choose masking strategy, encryption, data freshness target (RPO), failover plan, and cost considerations. Include concrete config choices (instance types, retention, network, and security)?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Snowflake","Tesla"]},{"id":"q-2139","question":"You run OLTP in Aurora PostgreSQL us-east-1 and need near real-time BI in us-west-2. Design a CDC pipeline: enable a dedicated logical replication slot in Aurora; use DMS in CDC mode to stream changes to Redshift in us-west-2 via a staging S3 bucket; apply PII masking at BI layer; enable PITR and cross-region backups; target end-to-end latency ~2s and RTO <60s. Include data flow, failover, and cost trade-offs?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Google","Netflix"]},{"id":"q-2163","question":"A multi-region SaaS app needs sub-20ms reads for hot tenants across three continents, writes allowed in any region, and PCI-DSS data residency constraints. Design a data-layer using AWS: compare Aurora PostgreSQL Global Database with DynamoDB Global Tables plus CDC to an analytics store, including consistency, DR, backups, and concrete configurations to meet RPO 5s and RTO 60s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Databricks","Google"]},{"id":"q-851","question":"Two-region OLTP SaaS with a single writer in us-east-1 and read replicas in eu-west-1. Compare Aurora Global Database (PostgreSQL) vs DynamoDB Global Tables for this workload: latency targets, consistency model, failover behavior, and cost. Which approach would you pick and why, and what concrete configuration (replica count, failover window, write routing) would you implement to meet RTO < 60s and RPO < 5s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Robinhood","Snowflake"]},{"id":"q-871","question":"Migration plan: An OLTP app runs on Aurora PostgreSQL provisioned; traffic is bursty; you want to evaluate Aurora Serverless v2. Provide a concrete plan to migrate, including: (1) start/stop criteria and scaling configuration; (2) handling of long-running transactions and prepared statements; (3) how to keep reads consistent during scaling; (4) testing approach for failover/RTO targets; (5) cost considerations and potential pitfalls with Serverless v2?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Hugging Face","IBM","Uber"]},{"id":"q-895","question":"Your multi-region SaaS needs an audit-friendly cross-tenant analytics store with writes transactional in us-east-1 and analytics queries in eu-west-1 under GDPR. Compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables for this workload, focusing on transactional integrity, analytics capability, PITR/retention, cross-region latency, and cost. Recommend a concrete configuration (writer region, replica counts, PITR window, tenant isolation, ETL approach) to meet RPO 15 minutes and RTO 1 hour?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Google","Slack","Square"]},{"id":"q-956","question":"For a real-time fraud graph application needing sub-100ms neighbor lookups across two AWS regions, compare Amazon Neptune Global Database with DynamoDB (using graph patterns and DAX) for this workload. Writer region us-east-1; readers in eu-west-1; assess graph traversal latency, consistency guarantees, failover behavior, and total cost. Provide a concrete setup (cluster engine and size, replica counts, PITR window, backup schedule, and network/config) to meet an RPO of 5s and an RTO of 60s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Meta","NVIDIA","Snap"]},{"id":"q-964","question":"You run an Amazon RDS PostgreSQL in **us-east-1** with automated backups. A regional outage blocks access from that region. How would you achieve **RPO ≤ 60s** and **RTO ≤ 15 minutes** by restoring to **eu-west-1**? Compare cross-region read replicas, backup copy, and Aurora Global Database, and outline concrete steps, knobs, and caveats?","channel":"aws-database-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-database-specialty"],"companies":["LinkedIn","Slack","Tesla"]},{"id":"q-1183","question":"Design a cross-account AWS CI/CD flow for a real-time analytics platform with data plane in Account A, model training in Account B, and API endpoints in Account C. Implement Terraform-driven IaC, policy-driven approvals, drift detection, canary promotion with synthetic monitoring, and automated rollback. How do you enforce least privilege, secret rotation, and auditable artifact pipelines across accounts?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Coinbase","Microsoft","MongoDB"]},{"id":"q-1416","question":"Design a beginner-friendly AWS CI/CD pipeline for a Dockerized REST API deployed to ECS Fargate behind an ALB. Source from GitHub; CodeBuild runs pytest and a Trivy container image scan; pushes image to ECR; deploys with a blue/green traffic shift across ECS target groups. Explain IAM least-privilege and Secrets Manager usage; include manual Prod approval and basic observability?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Airbnb","Microsoft","Stripe"]},{"id":"q-1553","question":"Design a cross-region, risk-aware deployment for a real-time streaming data platform (Kinesis Data Streams → Lambda → DynamoDB) using CodePipeline/CodeBuild and CDK. Implement blue/green rollout across us-east-1 and us-west-2, with synthetic canary tests, latency/SLA-based automatic rollback, and drift detection. Explain IAM roles, Secrets rotation, and audit logging?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Robinhood","Square","Zoom"]},{"id":"q-1613","question":"Design a Git-driven CI/CD workflow to deploy a two-region microservice API on EKS (us-east-1, us-west-2) with Argo Rollouts canary, Terraform IaC, and DynamoDB Global Tables. Include region-specific config via AppConfig, IAM least privilege, drift detection, and automated rollback on latency or error-rate spikes?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Airbnb","Netflix","Zoom"]},{"id":"q-1728","question":"Design a beginner-friendly AWS CI/CD workflow that builds a Docker image from a GitHub repo, runs unit tests, runs a static security scan, pushes to ECR, and deploys to an ECS Fargate service using CodeDeploy blue/green with a canary (15%). Include rollback on failure, health checks, and IAM least-privilege. Explain staging vs prod isolation and how you’d structure permissions and secrets?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["NVIDIA","Snowflake","Two Sigma"]},{"id":"q-1811","question":"Design a GitHub-driven CI/CD pipeline that builds microservices to ECR, tests in CodeBuild, and deploys via ArgoCD to per-service namespaces on EKS. Store per-service configs in Secrets Manager/SSM; enforce IAM least privilege with scoped roles; implement canary rollouts with synthetic checks and automatic rollback on failure. Include artifact/config flow and audit strategy?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Google","Oracle","Square"]},{"id":"q-1822","question":"Design a cross-account, multi-region deployment for a real-time fraud-detection platform: data plane in Account A (Kinesis/S3), model training in Account B (SageMaker), live REST endpoints in Account C (API Gateway/Lambda/DynamoDB). Provide a concrete plan using Terraform, CodePipeline, cross-account roles, drift checks, end-to-end tracing, and canary promotions with automated rollback. Include IAM, secret rotation, and success criteria?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Coinbase","Databricks","Scale Ai"]},{"id":"q-1865","question":"You have a small containerized web app in GitHub, deployed to AWS ECS Fargate behind an Application Load Balancer. Design a beginner CI/CD pipeline using GitHub Actions to build and push a Docker image to ECR, register a new ECS task definition, update the staging service, run a smoke test against the staging ALB, and require a manual approval before production. How would you implement IAM least-privilege and secrets management (Secrets Manager or SSM) for this flow?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Amazon","OpenAI","Robinhood"]},{"id":"q-1895","question":"In a three-region deployment for a real-time personalization service using Lambda@Edge and CloudFront, design a CI/CD workflow that sources code from Git, provisions infra with Terraform, and coordinates region-local deployments via CodePipeline. Implement per-region canaries with traffic weights, automated rollback on synthetic checks, Secrets Manager rotation, KMS key rotation, and auditable artifacts. Separate staging and prod accounts and validate edge caching with synthetic tests?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Salesforce","Scale Ai","Snap"]},{"id":"q-1907","question":"Design a beginner-friendly AWS DevOps pipeline for a small Node.js API containerized app deployed to ECS Fargate behind an Application Load Balancer. Source from GitHub; CodeBuild runs npm test and a lightweight security scan (Trivy); build Docker image and push to ECR; deploy via CodePipeline using ECS blue/green (CodeDeploy) with canary shifts across dev, stage, prod. Explain IAM least privilege and Secrets Manager use for env vars, plus rollback triggers?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Meta","Microsoft","Slack"]},{"id":"q-1923","question":"Design a two-account AWS CI/CD pipeline for a multi-tenant SaaS app deployed to ECS Fargate, sourced from GitHub. Use Terraform for IaC, enforce per-tenant isolation via tag-based deployment, and AppConfig feature flags to enable tenant-specific features. Implement cross-account artifact sharing, canary promotions with synthetic monitoring, and automatic rollback on latency or error-rate thresholds. Explain IAM guardrails and auditability across accounts?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Anthropic","Databricks","Zoom"]},{"id":"q-1960","question":"You’re deploying a simple REST API on AWS Lambda behind API Gateway with a GitHub source. Implement a runtime feature flag using AWS AppConfig that can be toggled without redeploying. Describe the minimal IAM permissions, how Lambda fetches the flag on cold starts and keeps it refreshed, and a safe canary rollout workflow with rollback. Include a brief code snippet in your preferred language?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Apple","Lyft"]},{"id":"q-1984","question":"Design a two-region CodePipeline per region (us-east-1, eu-west-1) for a three-service ECS/Fargate app behind an ALB. Source from GitHub; CodeBuild runs unit/integration tests plus Trivy/CodeQL scans; artifacts encrypted in S3; deploy via CDK to StackSets; blue/green per service; Route 53 latency routing; automatic rollback on health checks; drift detection with AWS Config; Secrets in Secrets Manager; enforce least-privilege IAM?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Airbnb","OpenAI","Stripe"]},{"id":"q-1998","question":"You're deploying a small REST API to AWS ECS Fargate in us-east-1 with GitHub as the source and a basic CodePipeline. Implement a beginner-friendly disaster-recovery workflow: cross-region deployment to us-west-2, Route 53 failover, and S3 artifact replication. Show minimal IaC (CloudFormation), a simple health-check-based failover policy, and how you test failover?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Citadel","OpenAI"]},{"id":"q-2029","question":"Design a GitOps-based CI/CD workflow for a microservices app running on two AWS accounts with an EKS cluster in each (staging and prod), using FluxCD to deploy Helm charts, IRSA for service accounts, and OPA Gatekeeper for policy enforcement, plus Istio for canary promotions and automatic rollback on 5xx errors or latency spikes. Include repo structure, IAM roles, and example configurations?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Goldman Sachs","Google","PayPal"]},{"id":"q-2058","question":"Design a multi-region, zero-downtime deployment for a serverless REST API using AWS Lambda, API Gateway, and DynamoDB, with cross-region replication, automated canary shifts, and regional failover testing. Provide Terraform that wires cross-account roles, Secrets Manager, and CloudWatch logs, plus a rollback plan?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Google","OpenAI","PayPal"]},{"id":"q-2115","question":"You're deploying a small REST API container to AWS ECS Fargate in us-east-1 behind an Application Load Balancer, with GitHub as the source and a basic CodePipeline. Implement a beginner-friendly blue/green deployment using CodeDeploy for ECS: provide a minimal CloudFormation snippet to enable CODE_DEPLOY deployments, configure a 10% canary start, include a simple health check path, and outline rollback criteria if health checks fail or error rate exceeds threshold. How would you test failover and enforce least-privilege IAM roles?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Coinbase","Square","Twitter"]},{"id":"q-2142","question":"Design a beginner-friendly AWS CI/CD pipeline for a small ECS Fargate app where the Infrastructure as Code is written in Terraform and source is GitHub. Implement a remote Terraform backend using S3 with DynamoDB locking, and a CodePipeline that runs `terraform init` and `terraform plan` on pushes, requiring a manual approval before `terraform apply`. Add drift-detection that halts deployments when `terraform plan` shows changes, and outline a safe rollback mechanism to the previous Terraform state. Include minimal Terraform snippets for the S3 backend and a pipeline step?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Adobe","Robinhood"]},{"id":"q-676","question":"You have a Node.js app deployed on EC2 instances behind an Application Load Balancer in a private VPC. You want a beginner-friendly, repeatable CI/CD pipeline that triggers on git pushes, runs tests, and safely deploys with rollback. Describe a practical setup using AWS CodePipeline, CodeBuild, and CodeDeploy (blue/green) to auto-build, test, and deploy with artifact flow in S3, and IAM roles with least privilege, including how to isolate staging vs production using separate target groups?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Cloudflare","Discord","Google"]},{"id":"q-890","question":"Design an AWS-based CI/CD pipeline for a data platform (S3 data lake, Lambda ETL, ECS) that sources from Git, runs unit tests and data quality checks (Great Expectations) on staging data, then plans/applies Terraform in staging and promotes to production with canary deployment and traffic shift. Explain IAM least-privilege, S3 artifact flow, and staging vs prod isolation?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Anthropic","Citadel","Databricks"]},{"id":"q-924","question":"Design a beginner-friendly AWS CI/CD pipeline for a serverless REST API deployed to AWS Lambda behind API Gateway. Source from GitHub; CodeBuild runs unit tests with pytest and a basic security scan with bandit; artifacts stored in S3; deployment uses SAM/CFN to Lambda with separate stages dev/stage/prod and a canary traffic shift via Lambda aliases. Explain IAM least-privilege and secure env vars (Secrets Manager or SSM)?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Microsoft","Square","Uber"]},{"id":"q-998","question":"In a 3-account AWS setup (Dev, SecProd, Prod), you need a Git-driven CI/CD that builds a container image in Dev, promotes IaC and app config via Terraform, and signals canary tests in Prod before full rollout. Explain cross-account CodePipeline stages, IAM roles with least privilege, Secrets Manager rotation, and canary deployment with rollback triggers. Include artifact flow and auditing considerations?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Apple","Databricks"]},{"id":"q-1025","question":"In a multi-account AWS DVA data platform, streaming IoT telemetry into Kinesis Data Firehose feeding an S3 data lake with Glue catalog. Data schemas evolve and backfills are needed without full reprocessing. Design an end-to-end approach for schema evolution, idempotent writes, and backfill using Glue Schema Registry, Iceberg on S3, and partition pruning. Include data validation, DLQ, and observability?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Goldman Sachs","Instacart","Snap"]},{"id":"q-1268","question":"Design an AWS data lake pattern for multi-tenant analytics where each tenant's data sits under /tenants/{tenantId} in S3 and is exposed to Athena and QuickSight. How would you implement strict tenant isolation, least-privilege access, and automated policy-driven discovery and auditing using Lake Formation, IAM, CMKs, and SCPs? Include governance, testing, and performance considerations?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Discord","Oracle","Salesforce"]},{"id":"q-1381","question":"A serverless data ingestion pipeline: an S3 PUT triggers a Lambda that transforms JSON logs into CSV and writes to a separate bucket; failed records go to a DLQ. Explain how you implement idempotent writes, choose between DLQ mechanisms (Lambda DLQ vs SQS), and set up minimal monitoring/alerts to catch processing failures?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Discord","OpenAI"]},{"id":"q-1436","question":"In a beginner AWS DVA workflow, JSON logs arrive to S3 at s3://data-logs/raw/. Propose a minimal pipeline where a Lambda validates each record against a JSON schema, writes valid records as Parquet to s3://data-logs/processed/YYYY/MM/DD/, and routes invalid ones to a DLQ. Explain idempotent writes, choose between Lambda DLQ vs SQS, and basic monitoring setup?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["LinkedIn","Meta","Salesforce"]},{"id":"q-1549","question":"Design a secure, scalable cross-account analytics pattern for a multi-tenant data lake. Data for tenants live at /tenants/{tenantId}/ in S3 and must be queryable via Athena/QuickSight with strict isolation. Explain how Lake Formation, per-tenant LF permissions, and cross-account IAM roles control access from a central analytics account. Include encryption (CMKs), cross-account RAM/trust, schema evolution handling, and a testing plan for isolation and governance?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["LinkedIn","Meta","Slack"]},{"id":"q-1575","question":"In a multi-tenant data platform on AWS, tenants stream JSON events into a single Kinesis Data Stream; you aggregate into per-tenant Parquet files in S3 using Firehose. Design an end-to-end pipeline with idempotence, schema validation, and auditing. Include DLQ handling and isolation via Lake Formation. What are your concrete steps?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Hugging Face","Lyft","Salesforce"]},{"id":"q-1617","question":"Design a per-tenant data lake access system on AWS where billions of Parquet files in S3 are consumed by Athena/Glue; describe how you would implement tenant isolation, masking, and row-level security using Lake Formation, and how you would validate auditing and performance under burst workloads?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Discord","IBM","Robinhood"]},{"id":"q-1637","question":"In a beginner AWS DVA ingestion pipeline, a CSV file uploads to s3://data/tenant-{tenantId}/uploads/YYYY/MM/DD/file.csv triggers a Lambda that validates the header has exactly [timestamp, tenant_id, metric], converts to Parquet, and writes to s3://data/tenant-{tenantId}/processed/YYYY/MM/DD/file.parquet; design for idempotent writes (no duplicates), choose a DLQ strategy, and add minimal CloudWatch alarms for failures. How would you implement this end-to-end, and why?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Adobe","LinkedIn","Salesforce"]},{"id":"q-1706","question":"Design a beginner-friendly, end-to-end data quality check for a daily Parquet dataset stored in S3: s3://telemetry/processed/YYYY/MM/DD/. The data is produced by a Glue job from JSON input. Propose a minimal workflow (using Lambda, Glue, or Athena) that validates schema conformance, computes a day-over-day row-count delta, and emits a quality score JSON to s3://telemetry/quality-reports/YYYY/MM/DD/. Include how you would trigger, idempotency, and basic monitoring?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Amazon","Discord","DoorDash"]},{"id":"q-1772","question":"In a multi-account AWS data lake, ingested data lands in s3://lake/raw from several tenants via Kinesis Firehose into a shared account. Propose an end-to-end DVA pipeline that enforces per-tenant isolation, uses Lake Formation for access control, handles schema evolution with Parquet, and provides tenant-aware lineage and auditing. Include how you test isolation and how you monitor for cross-tenant data leakage?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Discord","Google","Snowflake"]},{"id":"q-1801","question":"Ingest JSON events from multiple payment rails into a single streaming layer, partitioned by exchange, and store per-exchange Parquet data in an Iceberg-backed table on S3. Describe the end-to-end design focusing on idempotent writes, late-arriving data, schema evolution, and auditability with time travel. Include concrete steps and trade-offs between Iceberg vs Glue Catalog?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Coinbase","Square","Stripe"]},{"id":"q-1821","question":"In a real-world data platform on AWS, streaming JSON events from many producers land in a single Kinesis Data Stream and are ingested into per-tenant Parquet files in S3. Over time the schema evolves and late data arrives. Describe an end-to-end approach that ensures idempotent writes, supports schema evolution, isolates tenants with Lake Formation, and provides reliable auditing and monitoring. Include specific services, data formats, and trade-offs?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Apple","Databricks","Goldman Sachs"]},{"id":"q-1881","question":"Design an advanced cross-region data ingestion and governance pattern on AWS for a high-volume streaming platform. In us-east-1 raw JSON events arrive via Kinesis Firehose into S3, then replicate to us-west-2 with minimal latency. Propose an architecture that guarantees exactly-once ingestion, supports schema evolution via Iceberg, and enforces per-tenant isolation with Lake Formation. Explain idempotent writes, cross-region replication, date-based partitioning, and robust monitoring (lag, drift, backfills) with minimal tenant impact?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["MongoDB","NVIDIA"]},{"id":"q-1925","question":"You're building a beginner AWS DVA pipeline: 2,000 devices emit JSON telemetry to S3 as daily JSONL; a Lambda validates lines and writes valid records as Parquet to a partitioned dataset, while invalid lines go to a DLQ. Propose a minimal approach to ensure idempotent processing, schema evolution, and late-data handling, with concrete services, data formats, and a simple monitoring plan?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Apple","Meta"]},{"id":"q-2001","question":"Design a cross-account, multi-region ingestion pipeline: devices in Account A stream telemetry via Kinesis to a central data lake in Account B. Use Firehose to write Parquet to S3, Glue Catalog for schema, and Lake Formation for access control. Ensure idempotent writes with a dedup key and conditional Put, support schema evolution via Glue Schema Registry, handle late data with watermarks, and implement observability with CloudWatch metrics and a DLQ?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["LinkedIn","Lyft","Stripe"]},{"id":"q-2064","question":"In a cross-region telemetry pipeline for millions of devices, ensure exactly-once processing, schema evolution with optional fields, and late-arriving data backfill within 24 hours. Propose a concrete architecture using AWS DVA primitives (Kinesis Data Streams, Lambda/Fargate, S3, Glue, Athena, DynamoDB), describe idempotent writes, partitioning, late data handling, monitoring, and failure plans. Include trade-offs?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Meta","Netflix","Zoom"]},{"id":"q-2113","question":"Design a minimal AWS DVA ingestion for 2,000 devices publishing JSON to a Kinesis Data Stream. Implement a Lambda dedupe layer using DynamoDB (keyed by deviceId and eventId), then write validated records as Parquet to S3 partitioned by date/deviceId. Use Glue Schema Registry for optional fields and versioning. Backfill late data within 24 hours; include a basic monitoring plan and DLQ for invalid records?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Google","Robinhood","Slack"]},{"id":"q-674","question":"You're building an AWS DVA data-analytics pipeline ingesting telemetry from 2000 devices/sec via Kinesis Data Streams. A Spark/Glue path processes windows and writes Parquet to S3; aggregation state in DynamoDB. Data loss or duplicates occur during retries and outages; costs spike at peak. Design a resilient, cost-efficient approach: shard sizing, processing path, idempotent writes, error handling with DLQ, and observability. What would you implement and why?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Microsoft","OpenAI"]},{"id":"q-916","question":"In an AWS-based DVA pipeline ingesting telemetry from 2000 devices/sec via Kinesis Data Streams, with a Spark/Glue path writing Parquet to S3 and cataloged in Glue, design an end-to-end strategy for robust schema evolution, data validation, and partitioning that minimizes reprocessing and supports backfills. Include schema registry, idempotence, DLQ, and observability?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["MongoDB","Snowflake","Tesla"]},{"id":"q-1228","question":"Design a drift-aware continuous training and multi-region deployment workflow for a fraud-detection model, using SageMaker Model Monitor, Pipelines, and Model Registry. Explain how you detect data and feature drift (PSI/KS against baselines), retrain triggers, versioning, canary validation, rollback, and how cross-region consistency is maintained?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Amazon","Bloomberg","Square"]},{"id":"q-1297","question":"You're deploying a multilingual sentiment-analysis model for a global customer-support chatbot. To minimize downtime when updating language adapters, design a SageMaker-based deployment with per-language variants, Model Registry, and canary rollouts that preserve latency SLAs and isolate traffic. Describe autoscaling, traffic routing, validation, and rollback criteria with concrete values?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Amazon","Google"]},{"id":"q-1324","question":"In a two-region SageMaker real-time inference setup for fraud detection, data drift is likely between regions and latency targets are strict. Outline a concrete canary deployment with per-region endpoint configs, Drift Detection thresholds, and Feature Store versioning/replication. Include traffic split, rollback criteria, validation plan, and monitoring strategy?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Cloudflare","LinkedIn","Twitter"]},{"id":"q-1378","question":"You’re building a SageMaker ML workflow that validates incoming data in a processing step before training. Design a minimal Processing Job using Python to check (i) all required features exist, (ii) numeric columns have ≤5% missing values, (iii) categoricals are within allowed sets. How would you trigger it from a SageMaker Pipeline, store results, and surface metrics? Include concrete resource choices?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Adobe","Instacart"]},{"id":"q-1457","question":"You're building a real-time risk-scoring model for a multi-region e-commerce platform. The model consumes streaming events, uses SageMaker Feature Store for features, and is deployed as a real-time endpoint with cross-region routing. Describe a concrete end-to-end deployment and monitoring design that ensures deterministic latency, supports feature versioning, detects data drift, and handles canary rollouts with rollback triggers; include governance, cost controls, and testing strategy?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Amazon","MongoDB"]},{"id":"q-1564","question":"You're deploying a global churn-prediction model for a SaaS app that requires real-time scoring in-app and nightly analytics reports, while complying with data residency rules. Propose an end-to-end AWS pattern using SageMaker real-time endpoints for live inference, Batch Transform for nightly analytics, per-region Feature Store isolation, and a governance framework with Model Registry versioning, drift detection, automated rollback, and cost controls. Include testing and validation steps?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Amazon","Google"]},{"id":"q-1572","question":"Design a multi-tenant, per-tenant inference service on SageMaker for a financial risk model where each client has isolated data, separate feature store namespace, and per-tenant model version, yet share a common endpoint. Describe the architecture, how you isolate data and billing, how you route requests by tenant_id, how you handle feature/version drift, and how you implement canary rollouts and rollback?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Coinbase","Hugging Face","Tesla"]},{"id":"q-1618","question":"You're deploying a predictive maintenance model to 10,000 industrial edge devices using SageMaker Edge Manager. Outline a phased OTA rollout with canary groups, offline devices, and automated rollback triggers based on telemetry. Specify packaging and signing process, versioned Edge Manifest in S3, device-grouping strategy, and how you monitor model accuracy, latency, and update success; discuss cost controls and governance?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Adobe","IBM"]},{"id":"q-1690","question":"You're deploying a SageMaker real-time endpoint for a financial risk model that ingests customer PII from EU and US users. Propose a compliant deployment pattern that enforces data locality (EU data stays EU), supports active-active regional endpoints, and provides GA-ready drift and privacy controls. Include resource layout, data flow, encryption, IAM/KMS, auditing, canaries, and cost controls?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Robinhood","Slack"]},{"id":"q-1725","question":"Design an end-to-end, multi-account, multi-region real-time fraud-detection pipeline on AWS. The model is deployed as SageMaker endpoints in two regions with cross-region routing. Provide concrete choices for endpoint configuration, SageMaker Feature Store versioning, canary rollout, drift detection, monitoring, autoscaling, governance, and cost controls; include validation steps and rollback criteria?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Amazon","Salesforce","Square"]},{"id":"q-1761","question":"You're building a privacy-preserving, multi-tenant real-time inference platform on AWS where each tenant's data must stay isolated (data locality + encryption) and costs are allocated per tenant. Propose an architecture using SageMaker Endpoints behind PrivateLink, per-tenant Feature Store versions, and a tenant-scoped Model Registry with canary rollouts. Explain how you validate latency, monitor drift, trigger retraining via SageMaker Pipelines, and enforce governance and per-tenant cost controls?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Google","Meta","Stripe"]},{"id":"q-1868","question":"Design a hybrid on-prem plus AWS inference workflow for a regulated financial service where customer data must never leave the on-prem site, but model updates are deployed from SageMaker. Propose an architecture using SageMaker Edge Manager for edge endpoints, PrivateLink to AWS backends, and a regulated Model Registry with per-tenant access controls. Include latency targets, canary rollouts to edge devices, drift detection, retraining triggers, and auditability?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Microsoft","Square","Uber"]},{"id":"q-1887","question":"Design a beginner-friendly SageMaker multi-model endpoint setup that serves two small text classifiers from a single endpoint. Route requests by a tenant_id included in the JSON input, ensuring models load on demand, monitor latency with CloudWatch, and implement a simple canary switch to compare model A vs B for a subset of tenants before full rollout. Include basic file structure, IAM roles, and a minimal test plan?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Bloomberg","Discord","IBM"]},{"id":"q-1913","question":"You’re deploying a beginner-friendly real-time sentiment moderation model for a global social app on SageMaker. End-user data must stay in one region and be routed through PrivateLink. Propose a concrete deployment: a single SageMaker endpoint behind PrivateLink, basic drift and bias checks with SageMaker Clarify, and a versioned Feature Store for user interactions, plus a simple canary and rollback plan. Include latency targets, retraining triggers, and governance basics?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Bloomberg","Snap","Uber"]},{"id":"q-2039","question":"You're deploying a beginner-friendly text classification service with SageMaker Serverless Inference for a low-traffic social app. Data must remain in-region, and endpoint credentials must rotate every 90 days. Propose a concrete setup: packaging and artifact storage, a single variant serverless endpoint, monthly drift-driven retraining triggers, a rollback plan, and basic monitoring/alerts for latency and errors?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Cloudflare","Twitter"]},{"id":"q-2052","question":"Design an automated rollout/rollback strategy for a multi-tenant real-time SageMaker inference platform with per-tenant Feature Store variants and cross-region canaries. How would you implement retraining triggers, drift validation, and per-tenant cost controls? Include concrete thresholds, duration, and rollback criteria?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Goldman Sachs","Meta","Snowflake"]},{"id":"q-2078","question":"Design a multi-tenant, privacy-preserving inference path on AWS that returns per-prediction explanations without leaking tenant data. Propose using SageMaker Endpoints behind PrivateLink, SageMaker Clarify explainability, per-tenant Feature Store versions, and a tenant-scoped Model Registry with canaries. Include drift detection, retraining triggers, and auditability; specify latency targets and rollback criteria?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["IBM","Plaid","Two Sigma"]},{"id":"q-2096","question":"Design an end-to-end deployment for a real-time anomaly detection pipeline across three data-residency regions with strict data sovereignty. Raw data must stay on-prem; only aggregated signals may traverse to AWS. Propose an architecture using SageMaker Endpoints in each region, PrivateLink to on-prem data sources, and a Global data-plane that routes latency-critical inferences. Include canary rollouts, drift detection, automated retraining, and rollback criteria?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Google","Meta","Twitter"]},{"id":"q-2151","question":"Design a data-quality and feature-drift monitoring plan for a real-time fraud-detection inference service deployed as SageMaker Endpoints across four Regions using PrivateLink. Outline detection of shifts in feature distributions before inference, retraining triggers via SageMaker Pipelines, and canary rollouts with rollback criteria. Include data provenance, access control, and governance?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Instacart","MongoDB","Zoom"]},{"id":"q-876","question":"You're deploying a SageMaker real-time endpoint for a model expected to see bursty, unpredictable traffic. Propose a concrete autoscaling setup using AWS Application Auto Scaling that keeps latency under a target while never scaling to zero. Specify min and max instances, the metric and target value (latency or invocations), the policy type, and cooldowns; discuss validation steps?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Bloomberg","Lyft"]},{"id":"q-896","question":"You run a SageMaker real-time endpoint serving a risk-scoring model for payments. After a drift alert, outline a canary deployment plan using endpoint variants and the Model Registry to shift 20% of traffic to a new version while preserving latency and safety. Describe how you automate metric validation (latency, error rate, and drift), rollback triggers, and guardrails, and how you promote a stable canary to baseline?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Bloomberg","PayPal","Robinhood"]},{"id":"q-969","question":"In a production AWS ML pipeline, you must serve multiple fraud-detection models across two regions using a SageMaker Multi-Model Endpoint (MME). Propose a concrete deployment and autoscaling strategy that keeps p95 latency under 200 ms during peak, prevents cold starts, and optimizes memory by loading only active models. Describe per-model versioning with SageMaker Model Registry, traffic routing, canary validation, rollback triggers, cost implications, and cross-region consistency?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Bloomberg","Netflix","Tesla"]},{"id":"q-1321","question":"In a multi-account, multi-VPC setup in us-west-2, VPCs A and B must reach external SaaS apps via private endpoints only, while all outbound internet egress is inspected by a centralized firewall in VPC C. Propose a beginner-friendly design using Transit Gateway, PrivateLink, and Route 53 Resolver DNS. Describe routing, DNS resolution, and observability?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["LinkedIn","Meta","Square"]},{"id":"q-1349","question":"Two AWS accounts across us-east-1 and us-west-2 host a data science platform with multiple VPCs. Design a scalable, secure network that minimizes cross-region data transfer, keeps east-west private, and supports easy onboarding of new accounts. Requirements: hub Transit Gateway across regions, centralized outbound egress via a shared firewall stack, private access to S3/data lake using PrivateLink with Private DNS, IPv6 dual-stack readiness, and automatic account onboarding. Explain architecture, routing, firewall policy model, cost controls, and observability?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["NVIDIA","Snowflake"]},{"id":"q-1433","question":"In a single AWS account, two VPCs exist: a private app VPC (no Internet gateway) and a separate VPC hosting the patch bucket. You want the private VPC to pull software updates from S3 in the same region without any public Internet access, and you must restrict access to only the vendor-patches bucket. Design and describe the steps to implement an S3 VPC Endpoint with an endpoint policy, route table changes, and any security considerations?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Coinbase","IBM","Robinhood"]},{"id":"q-1466","question":"Design a two-region hub-and-spoke network for a SaaS platform with 3 AWS accounts per region and 4 VPCs per region. They require private east-west traffic between services, deterministic private access to S3/DynamoDB via PrivateLink, centralized firewall egress, and easy onboarding of new tenants via an account factory. Propose a design using Transit Gateway, PrivateLink, and centralized firewalling; detail routing, tenancy isolation, scaling, and observability?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Instacart","Meta"]},{"id":"q-1469","question":"Design a two-region, multi-tenant SaaS network: per-tenant VPCs in Region A and Region B, all funneling to a centralized Transit Gateway and a regional Service VPC hosting API gateways and Network Firewall. Use PrivateLink for S3/DynamoDB, Route 53 private DNS for discovery, and an automated account factory for onboarding. Explain tenancy isolation, onboarding, cross-region failover, and observability?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["NVIDIA","Salesforce","Slack"]},{"id":"q-1545","question":"In two AWS accounts with two VPCs in a single region, internal services require private DNS resolution for api.internal and auth.internal without public endpoints. Design a simple cross‑account Route 53 Resolver setup using a central DNS hub VPC with inbound/outbound endpoints and forwarding rules, and outline onboarding for new VPCs?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Meta","Robinhood","Tesla"]},{"id":"q-1576","question":"In a two-region deployment (Region A and Region B) across two AWS accounts per region, tenants are isolated in VPCs attached to regional Transit Gateways. Design a DR-ready network that preserves private east–west traffic, ensures tenant isolation during regional outage, and enables automatic failover of control-plane services to the DR region. Propose TGW topology with a DR region, synchronized PrivateLink services, replicated firewall rules, and a testing/validation plan with telemetry?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Google","Instacart","LinkedIn"]},{"id":"q-1600","question":"Design a DR-ready, multi-region network for a SaaS platform with 3 tenants per region, each tenant in its own VPC attached to a regional Transit Gateway. Propose topology to keep private east-west traffic between tenants and control plane across regions, enable automatic failover of the control-plane to DR region using PrivateLink, support tenant onboarding, and specify telemetry and testing plans?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Google","Hashicorp"]},{"id":"q-1820","question":"Beginner-level: You operate a SaaS API in Account A, Region us-east-1. You must share the API with a partner in Account B using PrivateLink, with automatic DR failover to DR region us-west-2 and DNS-based failover via Route 53 private hosted zones. Design the minimal topology, specify resources (PrivateLink service, VPC endpoints, private DNS records) and a basic testing plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Anthropic","Lyft","Twitter"]},{"id":"q-1835","question":"Design a DR-ready topology for a SaaS platform with per-tenant VPCs attached to regional Transit Gateways in two AWS regions. Ensure private east-west traffic, tenant isolation during regional outage, and automatic control-plane failover to the DR region. Explain TGW topology, PrivateLink replication, DNS failover, cross-region storage replication, and telemetry-based validation. **Edge cases** considered?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Microsoft","NVIDIA"]},{"id":"q-1862","question":"Design a cross-region, multi-account SaaS network using AWS VPC Lattice for per-tenant service discovery and authorization across Region A and Region B. Tenants reside in isolated VPCs yet must privately reach a central API service in a hub region. Outline: tenant-to-service mapping with Lattice; inter-region PrivateLink; centralized firewall strategy; DR with DNS failover and telemetry; onboarding and revocation processes?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["MongoDB","Slack"]},{"id":"q-1908","question":"Scenario: A SaaS company hosts 5 tenants in 5 VPCs in Region us-east-1; all attach to a central Transit Gateway in a shared-services account. Each tenant must access a per-tenant PrivateLink API service while keeping strict east-west isolation. Design the topology, attach VPCs, implement per-tenant firewall rules, and provide telemetry to verify isolation and scalability. Include onboarding a new tenant?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Google","Scale Ai","Uber"]},{"id":"q-2158","question":"In a two-region deployment for a multi-tenant SaaS, each region houses 4 tenant VPCs attached to a regional Transit Gateway, plus a central Service VPC hosting API gateways and a Firewall fleet. Design a DR-ready topology that keeps tenant traffic private to the local region, ensures isolation during regional outages, and enables automatic control-plane failover to the DR region via PrivateLink. Include routing, PrivateLink replication, firewall policy synchronization, tenant onboarding, telemetry, and a DR validation plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Hugging Face","OpenAI","Oracle"]},{"id":"q-902","question":"A multinational bank operates 6 VPCs across 3 AWS accounts in two regions. They want to centralize north-south internet egress for security inspection, keep east-west traffic private, minimize inter-region data transfer costs, and streamline onboarding of new accounts. Propose a hub-and-spoke design using AWS Transit Gateway, Network Firewall, and VPC Endpoints. Describe routing, policy model, and observability considerations?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Goldman Sachs","Google","Tesla"]},{"id":"q-923","question":"In a multi-account AWS environment with four accounts (prod, dev, staging, shared-services) and two regions, internal services (e.g., app.internal, db.internal) must resolve to private IPs across accounts, with public DNS unchanged for customers. Propose a scalable private DNS design using a private hosted zone and Route 53 Resolver (inbound/outbound as needed). Explain setup steps, minimal cross-account boundaries, and basic validation methods?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["DoorDash","Hashicorp","Meta"]},{"id":"q-939","question":"An enterprise runs 4 AWS accounts across 2 regions with 10 VPCs. They require centralized north-south internet egress through a firewall inspection stack, private east-west traffic, cross-region replication, and scalable SaaS access via PrivateLink. Design an end-to-end network using Transit Gateway, VPC Endpoints, Private DNS, and optional Direct Connect/Interconnect. Describe routing, policy model, failover, and observability considerations?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["DoorDash","Microsoft","Square"]},{"id":"q-1047","question":"Design a multi-tenant analytics data lake on AWS for 3,000 tenants with strict data isolation. Propose an architecture using a shared S3 data lake with per-tenant prefixes, Lake Formation permissions, and ABAC via tenant tags. Outline governance (IAM, KMS, RAM), onboarding/offboarding automation, cost accounting, and cross-region DR. Include testing strategies to validate isolation and detect privilege escalation?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Cloudflare","Robinhood","Snowflake"]},{"id":"q-1203","question":"A web app runs in a private subnet in a VPC with no Internet access. It must fetch a config.json from a single S3 bucket owned by the same account. Design the minimal IAM role for the EC2 instance, a bucket policy, and a VPC endpoint setup to allow access via the VPC endpoint while denying access to other buckets. What steps and policies would you implement?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Cloudflare","DoorDash","Snowflake"]},{"id":"q-1318","question":"A Secrets Manager secret in account A stores DB credentials used by an app in account B. You need a Lambda in account B to rotate the secret daily. Describe the cross-account access setup, including the secret's resource policy, the Lambda's execution role, and the minimal privileges on the DB user, plus a simple test plan?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Cloudflare","Coinbase"]},{"id":"q-1390","question":"In a managed SaaS environment, each customer gets its own AWS account under a shared Organization. You require SSO-based access to their account via IAM Identity Center, provisioning per-customer roles via SCIM, with temporary credentials; ensure strict tenant isolation and prevent cross-tenant access. Describe the IAM role structure, trust/policy design, SSO mappings, and use of SCPs, tagging, and audit strategy. Include how you'd validate isolation and detect misconfigurations?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["DoorDash","Lyft","Tesla"]},{"id":"q-1402","question":"Scenario: You operate a multi-tenant SaaS on AWS. Each tenant stores its data in separate accounts with per-tenant prefixes in S3 and DynamoDB. Partners require selective data exports to their accounts for analytics. Design a scalable, secure data export and access model that guarantees tenant isolation, supports per-tenant export scopes, uses least-privilege cross-account roles, per-tenant KMS keys, and catalog governance (Lake Formation/Data Catalog). Include onboarding/offboarding, cost controls, monitoring, and testability?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Airbnb","Databricks","Netflix"]},{"id":"q-1425","question":"In a multi-tenant SaaS on AWS with per-tenant accounts and a central analytics account, design secure cross-account access so tenants can query their data without seeing others. Describe the cross-account role structure, trust policies, session controls, least-privilege, and validation/audit approach for onboarding/offboarding?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Microsoft","Salesforce","Scale Ai"]},{"id":"q-1515","question":"Scenario: A Lambda function in a VPC must securely retrieve a DB password from Secrets Manager to connect to an RDS instance. The secret is encrypted with a customer-managed CMK in the same account. Describe the exact IAM policy statements for the Lambda execution role to permit GetSecretValue and DescribeSecret on that secret, and kms:Decrypt on the CMK; include any needed resource-based or key policies and how you’d validate least privilege and rotation compatibility?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Databricks","Meta","Snowflake"]},{"id":"q-1532","question":"Design a per-tenant, region-aware analytics pipeline for a multi-account AWS SaaS where each tenant's data residency must stay in its region. Explain how you’d enforce data locality, isolate tenant data, share insights back to a central analytics account, and test DR?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Bloomberg","Google"]},{"id":"q-1566","question":"In a multi-tenant SaaS on AWS, each tenant's data sits in a separate prefix under a shared data lake. Propose a beginner-friendly, cost-conscious approach to let tenants access their own analytics dashboards through a central BI tool without cross-tenant data exposure. Specify IAM patterns, data access controls, and how you'd validate isolation and cost?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Microsoft","Robinhood"]},{"id":"q-1592","question":"An analytics SaaS runs in a single AWS region with private data stores. You must enable cross‑region DR with automated failover to a secondary region within 30 minutes, preserving tenant isolation and continuous client access. Design the architecture using Aurora Global Database, S3 cross‑region replication, Route 53 failover, per‑tenant data partitioning, and regionally isolated KMS keys. Include data‑sync, cutover automation, and a testing plan?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Databricks","Microsoft"]},{"id":"q-1778","question":"You run a multi-tenant SaaS on AWS with a central control plane and per-tenant accounts. On signup, you must provision a dedicated VPC Endpoint service and per-tenant IAM roles with least privilege for cross-account access, revocable within 60 seconds. Describe the architecture, trust policies, RAM/PrivateLink usage, SCIM provisioning, and how you would automate validation and rollback with EventBridge and Step Functions?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Cloudflare","Instacart","NVIDIA"]},{"id":"q-1796","question":"Scenario: A SaaS runs per-tenant accounts across AWS Organizations. A tenant's IAM role is suspected to be compromised, enabling sideways access to the central control plane. Propose a zero-downtime incident response workflow that (a) detects via CloudTrail/GuardDuty/EventBridge, (b) revokes the compromised role’s trust, (c) isolates the tenant from cross-account access, and (d) notifies security and customer teams within 60 seconds. Include workflow steps and rollback?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Adobe","Goldman Sachs","Google"]},{"id":"q-1826","question":"Design a self-serve cross-account restore workflow: a tenant requests data restore from a central backup vault into their own AWS account. Outline the per-tenant cross-account role with TTL, trust policy, least-privilege permissions, and how you orchestrate AWS Backup restore, validation, and rollback within SLA?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Airbnb","Apple","Oracle"]},{"id":"q-1866","question":"You operate a multi-tenant SaaS on AWS with per-tenant data stored in separate accounts. Upon signup, you need to provision an isolated workspace with a per-tenant VPC, a dedicated KMS CMK for data at rest, and strictly limited cross-account access for support tooling that can revoke within 60 seconds. Describe the end-to-end architecture, including IAM/RAM roles, SCPs, KMS key management, S3 and DynamoDB data isolation, and how you test onboarding/rollback and audits at scale?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Salesforce","Twitter"]},{"id":"q-1920","question":"You operate a multi-tenant SaaS on AWS with per-tenant accounts under an Organization. A tenant triggers suspicious activity suggesting cross-account data exfiltration. Design a zero-downtime incident response that detects via CloudTrail/EventBridge/GuardDuty, revokes trust and rotates CMKs, quarantines the tenant with SCPs, and notifies teams within 60 seconds. Include rollback steps and testing strategy?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Oracle","Tesla","Two Sigma"]},{"id":"q-1974","question":"You run a multi-tenant SaaS on AWS that provisions per-tenant data pipelines (Glue/EMR) on signup. Tenants upload ETL scripts; you must sandbox execution with strict runtime and cost caps, guarantee per-tenant data isolation, and enable revocation within 60 seconds. Describe the control plane, roles, policies, data isolation, and rollback/audit strategy?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Salesforce","Scale Ai"]},{"id":"q-2056","question":"Scenario: A multi-tenant SaaS stores each customer's data in a regional S3 bucket owned by that customer, with your service ingesting data via cross-account roles. On signup you must provision a per-tenant IAM role, per-tenant KMS CMK, and per-tenant Lake Formation permissions, with revocation within 60 seconds if policy is violated. Describe the architecture, trust policies, and automation (EventBridge, Step Functions, RAM) to onboard, rotate keys, and revoke access?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Coinbase","MongoDB","Square"]},{"id":"q-2149","question":"In a two-account setup (dev and prod), how would you grant a developer permission to deploy to prod S3 and prod ECR via a cross-account role with MFA, using minimal permissions and fast revocation? Describe the trust policy, role inline permissions, and a basic CI check to verify access?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Hugging Face","LinkedIn","NVIDIA"]},{"id":"q-672","question":"Design a scalable, cost-aware data ingestion and processing pipeline on AWS for 1 TB/day of log data arriving from multiple on-prem and cloud sources. The pipeline must deliver raw data immutably for 90 days, provide near-real-time enrichment within 5 seconds, and support cross-region failover. Specify services, data flows, constraints, and trade-offs?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Hashicorp","IBM"]},{"id":"q-930","question":"An existing web service runs on EC2 in a single VPC with an ALB. Traffic surges cause latency spikes and occasional outages during AZ failures. Propose a beginner-friendly, cost-conscious HA setup using an Application Load Balancer, Auto Scaling across at least two AZs, and a relational database option. Include networking, health checks, a scaling policy, and a basic DB deployment choice with trade-offs. What would you implement first and why?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Amazon","MongoDB"]},{"id":"q-944","question":"Design a multi-tenant SaaS on AWS that ingests telemetry from thousands of tenants daily, enforces strict data isolation, and provides cross-region DR. Outline data partitioning, access control, encryption strategy, immutable logging, and DR failover. Include services, trade-offs, and how you test isolation?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Anthropic","Meta","Snap"]},{"id":"q-989","question":"Design a centralized security telemetry pipeline for 1,000 AWS accounts to detect anomalies in near real-time. Ingest VPC Flow Logs, CloudTrail, and GuardDuty findings into a central security account using AWS Organizations, implement least-privilege cross-account roles, normalize data into a common schema in S3, partition by source account and region, apply Lake Formation permissions, and set up alerting with EventBridge and security findings. Include scaling, cost, DR, and testing?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Google","Plaid"]},{"id":"q-1005","question":"You operate SAP S/4HANA on AWS with analytics in Snowflake and must implement a data masking/tokenization pipeline so analytics do not expose PII. Design end-to-end data flow, masking rules by field, latency (<5 minutes), and governance using KMS/IAM. Include auditing, rollback, and failover considerations?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Oracle","Snap","Snowflake"]},{"id":"q-1011","question":"How would you implement an automated cross-region DR for SAP HANA on AWS? Use SAP HANA System Replication with Region A as primary and Region B as hot standby, orchestrated by AWS Step Functions; back up to EBS/S3 with Data Lifecycle Manager and cross-region KMS keys; ensure automated DR tests, rollback playbooks, and meet RPO <5 minutes, RTO <15 minutes?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["PayPal","Uber"]},{"id":"q-1059","question":"Design an advanced SAP S/4HANA on AWS architecture using SAP HANA MDC on EC2 across 3 AZs, with analytics separated into a data lake. Propose a rolling OS/kernel/SAP patching and upgrade strategy that preserves near-zero downtime, enables automated cross-region DR testing, and guarantees RPO <2 minutes and RTO <5 minutes; specify automation, services, failure modes, and rollback?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Airbnb","Hashicorp","Stripe"]},{"id":"q-1179","question":"Design an automated, auditable patching workflow for SAP S/4HANA on EC2 across multiple AWS accounts and regions. Use AWS Systems Manager Patch Manager to patch OS and SAP kernel updates with rolling upgrades, pre/post checks, and automated rollback if SLA drift occurs. Include governance, approvals, testing, and validation of success before go-live?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Apple","Citadel","Plaid"]},{"id":"q-1186","question":"Design a multi-account SAP S/4HANA on AWS with SAP HANA MDC across 3 AZs and a cross-region DR setup. Route SAP system and security logs to a centralized, cross-account S3 data lake with Object Lock (WORM) and cross-region replication. Use AWS Glue/Data Catalog and Lake Formation for lineage and access control; enforce least privilege with SCPs. Automate DR tests and integrity checks?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Instacart","Lyft","Plaid"]},{"id":"q-1328","question":"Design a cross-account SAP S/4HANA MDC deployment where production data sits in Account A and an analytics MDC mirrors in Account B. Use near real-time CDC data replication (e.g., DMS) into a centralized S3 data lake and Glue catalog, with strict IAM governance, private networking, and automated drift checks. Include rollback and DR testing plan; target RPO <60s, RTO <5m. How would you implement it?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Microsoft","Netflix"]},{"id":"q-1404","question":"Design an advanced SAP S/4HANA on AWS architecture for a global user base focused on latency and data residency. Use SAP HANA MDC on EC2 across 2 AZs in Region A with a cross-region MDC replica in Region B for DR; enforce EU data residency with local S3 buckets and KMS keys; implement IAM/SCP least privilege; outline automated DR tests, failover/failback, and rollback?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Bloomberg","Discord","Slack"]},{"id":"q-1434","question":"Scenario: You operate SAP S/4HANA on SAP HANA MDC on EC2 across two AWS regions (Region A as primary, Region B as hot standby). Design an automated DR test plan that uses SAP HANA System Replication, AWS Step Functions, and AWS Fault Injection Simulator to perform regular, reportable failover tests without impacting production. Define test cadence, success criteria (RPO <2m, RTO <10m), rollback procedures, and how to prove continuity to stakeholders with logs, metrics, and cross-region backups?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Google","MongoDB"]},{"id":"q-1494","question":"You manage SAP HANA MDC on EC2 in a single AWS region. Design a beginner-friendly, automated backup strategy using only AWS native services (EBS snapshots, S3, AWS Backup, KMS) with cross-region replication to a warm standby. Include backup frequency, retention, encryption, and a reproducible restore test plan with objective to meet RPO 15 minutes and RTO 1 hour?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Adobe","Hashicorp"]},{"id":"q-1559","question":"In a single-region AWS SAP deployment (SAP NetWeaver + SAP HANA on EC2), propose a beginner-friendly automated health-check using only AWS native services. The check runs daily, verifies SAP instance status, HANA availability, and key OS metrics, writes a JSON report to S3, and triggers an SNS alert on any failure. Include data flow, AWS services used, and a minimal script snippet the Lambda would run via SSM Run Command?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Adobe","Airbnb","Hugging Face"]},{"id":"q-1606","question":"In a single-region SAP HANA on EC2 deployment, design a beginner-friendly incident-response automation using only AWS-native services to detect an outage within 5 minutes, isolate the SAP subnet, perform a warm-standby failover, and notify stakeholders via SNS. Include data flow, services used, and a minimal Lambda snippet that checks SAP status via SSM Run Command?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Hashicorp","Two Sigma"]},{"id":"q-1709","question":"You operate SAP HANA on EC2 with a parallel SAP ABAP stack in a multi-AZ VPC. Propose a concrete AWS-native auto-scaling and DR plan to handle quarterly peak workloads with zero SAP downtime. Include (1) app-tier scaling strategy (ASG, launch templates), (2) HANA scale-out readiness and data protection, (3) storage IOPS and sizing choices, (4) cross-region DR with replication and failover testing cadence, (5) rollback criteria and metrics?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Discord","Google","PayPal"]},{"id":"q-1779","question":"In an SAP HANA on EC2 landscape spanning three AWS environments (dev, stage, prod), design a policy-driven security hardening and continuous compliance workflow using only AWS-native services. Include OS baselines, SAP user and kernel parameters, and file permissions; enable drift detection, automated remediation via SSM Run Command, and audit reports to S3 + Glue catalog. Provide architecture and a runnable Lambda snippet to enforce the baseline?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["IBM","Meta","Snowflake"]},{"id":"q-1814","question":"In a two-region AWS SAP HANA deployment (NetWeaver + S/4HANA on EC2), design an observability-driven auto-remediation plan to detect SAP performance anomalies (e.g., high ABAP work process waits, HANA stalls, long SAP responses) using only AWS-native services. Define metrics, thresholds, data flow, remediation actions (auto-scale, restart SAP components via SSM Run Command), and how you validate success?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Adobe","Snowflake"]},{"id":"q-1838","question":"Design a beginner-friendly cost governance workflow for SAP HANA on EC2 in a single AWS region that uses only native AWS services: enforce SAP-resource tagging with Environment and Owner, set a monthly spend budget, and trigger an SNS alert if forecasted spend exceeds 80% of the budget. Describe data flow, services used, and provide a minimal CloudFormation snippet to create the budget and a Lambda skeleton to respond to forecast notifications?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Bloomberg","MongoDB"]},{"id":"q-1905","question":"In a multi-region SAP HANA on EC2 deployment, design an automated upgrade path for SAP NetWeaver stack from 7.50 to 7.52 with zero downtime using only AWS-native services. Include blue/green deployment via CodePipeline/CodeDeploy and Route 53, SSM Automation for upgrade steps, rollback triggers, and post-cutover SAP health verification?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Airbnb","Anthropic","OpenAI"]},{"id":"q-1942","question":"You manage an SAP HANA MDC deployment on EC2 across two AWS regions and two accounts. Design a policy-driven, automated patch rollout for SAP kernel and HANA patches using only AWS-native tools, with zero-downtime, predefined maintenance windows, and automated rollback. Include staging, validation, auditing, and a runnable Step Functions workflow that triggers SSM Run Command patches and reports success?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Hashicorp","Scale Ai","Snowflake"]},{"id":"q-2067","question":"Design a daily cost-performance monitoring solution for SAP HANA on EC2 using AWS-native services that alerts when savings potential exceeds 20% or utilization thresholds are breached for consecutive days?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Anthropic","Apple","Oracle"]},{"id":"q-2071","question":"Scenario: You operate a single-region SAP NetWeaver + HANA on EC2. A quarterly kernel upgrade and patch cycle must be performed with minimal downtime. Design a blue/green rollout using only AWS-native services: two identical stacks (Blue/Green) across the same VPC/AZs, Route 53 weighted routing, SSM automation for kernel patching, CloudFormation templates for reproducibility, and automated health checks (SAP status, HANA availability, sample ABAP batch). Include rollback steps and metrics?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Meta","Microsoft"]},{"id":"q-2148","question":"Design a beginner-friendly, AWS-native license-compliance workflow for SAP HANA on EC2 in a single region. Use AWS License Manager to model SAP licenses, collect SAP and OS license data via a daily SSM Run Command, compare against entitlements, and publish a JSON report to S3 with an SNS alert on non-compliance. Include data flow, services, and a minimal Lambda snippet to fetch License Manager data and SAP status?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Adobe","Microsoft","Snowflake"]},{"id":"q-673","question":"You manage a small SAP NetWeaver footprint on AWS using EC2 for the app tier and a separate DB tier on HANA. Describe a practical single-region HA and backup plan to meet an RPO of 15 minutes and an RTO of 60 minutes. Include chosen services, EC2 sizing approach, storage strategy (EBS/S3), backup schedule, and a cost-conscious trade-off you’d consider?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Microsoft","Robinhood"]},{"id":"q-900","question":"Scenario: You manage a single-region SAP NetWeaver deployment on AWS with a separate SAP HANA DB on EC2. You need a beginner-friendly, cost-conscious maintenance workflow that automates OS patching and SAP kernel upgrades using only AWS native services, with minimal downtime. Outline the steps, services, and a sample two-hour weekly maintenance window, including how you validate success and perform rollback?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Cloudflare","Hashicorp","Zoom"]},{"id":"q-1302","question":"A data pipeline in Account A must read daily compressed data from a bucket in Account B, with no Internet access and least privilege. Propose a practical cross-account access pattern using a role in Account B that can be assumed by a service role in Account A, a bucket policy, and a CMK policy. Include how you would validate that only authorized principals can access and that encryption keys are used and rotated?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Hashicorp","NVIDIA","Uber"]},{"id":"q-1345","question":"In a multi-account setup, a Lambda in Account B must read a secure string from Parameter Store in Account A, encrypted with a CMK in Account A. The environment has no Internet access. Outline a concrete cross-account pattern: (1) IAM role trust, (2) Parameter Store policy, (3) CMK policy, and (4) plan to verify access and CMK rotation?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Citadel","Cloudflare"]},{"id":"q-1428","question":"Scenario: A SaaS app serves 100 tenants. Each tenant stores data in its own S3 bucket encrypted with a per-tenant CMK (SSE-KMS). The backend uses a per-tenant role to access both the bucket and key. Design the IAM roles, bucket policies, and KMS key policies to enforce strict per-tenant isolation, automatic CMK rotation, and auditable access. Include how you’d validate no cross-tenant access and rotate status?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Google","Hugging Face","Snowflake"]},{"id":"q-1574","question":"In a multi-account, multi-region data lake, design a zero-trust cross-account analytics boundary that ensures only approved principals can read S3 data via private endpoints, while data at rest is encrypted with a CMK in a centralized security account that rotates automatically. Include CMK and bucket policies, Lake Formation permissions, VPC endpoints/PrivateLink use, cross-account roles, and how you would validate no privilege creep and immutable logs?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Lyft","Two Sigma"]},{"id":"q-1593","question":"In a cross-account ETL job: a service in Account A needs to copy only files tagged ETL=Allowed from a bucket in Account B, over a private VPC endpoint, with data at rest encrypted by a central CMK. Design the least-privilege pattern: trust policy for a cross-account role, bucket and KMS policies, and how you would enforce per-object tag-based access. Include how you’d validate no privilege creep and that logs are immutable?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Google","NVIDIA","Uber"]},{"id":"q-1612","question":"In a multi-account setup, a worker in Account A must read API credentials stored in Secrets Manager in Account B, with automatic rotation and centralized auditing. Propose a concrete pattern using a cross-account Secrets Manager resource policy, an IAM role in Account A, a CMK in a Security account with rotation, and a least-privilege policy for the worker. Describe validation steps to detect privilege creep and ensure immutable logs?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Bloomberg","Microsoft","Uber"]},{"id":"q-1792","question":"Scenario: A data processor in Account A must read encrypted inputs from bucket 'data-inbound' in Account B, decrypt with a CMK in the Security account, and write results to 'data-out' in Account B. No Internet access. Propose exact cross-account roles, trust policies, bucket policies, and KMS key policy to grant least privilege; show how you would test that only the intended principals can access, and how to ensure immutable auditing logs?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Bloomberg","Discord","Microsoft"]},{"id":"q-1870","question":"Design a federation pattern for querying a shared S3 data lake from multiple AWS accounts using Athena workgroups, cross-account IAM roles, and Lake Formation with ABAC-based dynamic data masking. Ensure least privilege, private connectivity (VPC endpoints), central CMK encryption with rotation in the Security account, immutable audit logs, and automated drift/privilege creep detection. Include IAM roles, bucket policies, Lake Formation permissions, KMS key policy, and a test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Databricks","Lyft","Meta"]},{"id":"q-1897","question":"Scenario: An ETL job in Account A reads from a shared data lake in Account B and writes results to a bucket in Account C. Design an ephemeral, least-privilege cross-account access pattern using STS AssumeRole, ABAC tags, Lake Formation permissions, VPC endpoints, and a centralized CMK in the Security account with rotation. Include trust, session policies, encryption, audit, and a test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Databricks","PayPal"]},{"id":"q-1929","question":"You need to share a subset of an S3 data lake between two AWS accounts. Design a least-privilege cross-account pattern that grants read access to specific prefixes via a dedicated role, enforces access only over S3 VPC Endpoints, and logs access with immutable logs; include bucket policy, IAM role trust, and a test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Apple","Cloudflare","Hugging Face"]},{"id":"q-2012","question":"Design a secure cross-account data-processing pattern for a multi-account AWS setup where data flows from a private S3 data lake in Account A to Account B for processing, with a central Security account handling encryption keys. Include ABAC-based access via STS AssumeRole, Lake Formation permissions, VPC endpoints, and immutable logs. Consider an EventBridge trigger from a partner account and private connectivity, with least privilege and automatic key rotation?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Coinbase","Microsoft","MongoDB"]},{"id":"q-2043","question":"A Lambda-backed API in Account A reads DB credentials from AWS Secrets Manager. Design a minimal, auditable approach to rotate that secret automatically with zero downtime. Include cross-account access if the DB is in another account, the required IAM roles/policies, and a concrete test plan to validate rotation without API downtime?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Hashicorp","Oracle","Stripe"]},{"id":"q-2068","question":"Design a beginner-friendly secure CI/CD pipeline for deploying Lambda functions across three AWS accounts (dev, stage, prod) using GitHub Actions. Enforce least privilege with per-environment execution roles, separate cross-account deployment roles, and approval gates. Include IAM trust policies, role permissions boundaries, artifact handling, Secrets rotation, and a basic test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Adobe","Microsoft","Tesla"]},{"id":"q-2112","question":"Design an on-demand data science workspace using AWS SageMaker notebooks for researchers in a multi-account data lake. Enforce ABAC with data-tag constraints, Lake Formation permissions, per-notebook IAM roles, and cross-account trusts. Require private connectivity via VPC endpoints, centralized CMK rotation, immutable audit logs, and automated drift/privilege creep detection. Provide IAM roles, KMS, Lake Formation, and a test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Adobe","Instacart","LinkedIn"]},{"id":"q-853","question":"In a multi-account AWS setup, centralize a logs bucket in Account A that stores CloudTrail and VPC Flow Logs from Accounts B and C. All objects must be encrypted at rest with a CMK in Account A that rotates automatically. Design the KMS key policy, bucket policy, and cross-account IAM roles to allow Account B/C services to encrypt, while preventing decryption except via a centralized IAM role in Account A. Include how you would validate encryption, rotation status, and auditability?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Citadel","IBM"]},{"id":"q-867","question":"Design cross-account data sharing using Lake Formation and S3 that lets Account B write to a shared data lake in Account A and Account C read it via a service role, with a CMK in A that rotates automatically and multi-region, tamper-evident audit logs. Provide IAM/Lake Formation/KMS policies, cross-account trust, and a validation plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Bloomberg","MongoDB","Snap"]},{"id":"q-938","question":"In a two-account data lake, enforce immutable data retention with S3 Object Lock across regions. Buckets in Account A replicate to Account B via CRR. Design the retention policy (COMPLIANCE vs GOVERNANCE), enable automatic CMK rotation, and set cross-account IAM trust for replication. Explain how you'd validate retention, replication integrity, and rotation status?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["DoorDash","MongoDB","Scale Ai"]},{"id":"q-1004","question":"Scenario: A serverless API stack (API Gateway, Lambda, DynamoDB, S3, CloudFront) runs in us-east-1 with a DR region eu-west-1. Propose an automated DR plan using AWS Global Accelerator and Route 53 health checks to failover within 15 minutes. Include data replication choices, Lambda versioning strategy, S3 replication mode, and a safe rollback/verification approach that avoids production impact during tests?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Databricks","Google"]},{"id":"q-1291","question":"In a 12-account AWS Organization, you must implement near real-time central audit logging for VPC Flow Logs, Lambda logs, and RDS logs in a dedicated Logging account. Design the end-to-end mechanism to ship logs from all member accounts to the central account, ensuring secure cross-account access, encryption, and resilience across Regions. What is your approach?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["MongoDB","Salesforce","Scale Ai"]},{"id":"q-1481","question":"In a multi-account AWS Organization for a global SaaS app, mandate data residency: S3 buckets and DynamoDB tables used by customer data must not store or replicate data outside the designated region per account. Design an automated governance solution using SCPs, AWS Config, IAM Roles, EventBridge, and Lambda to enforce this, with testing and alerting?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Google","IBM","Snowflake"]},{"id":"q-1536","question":"Design a DR orchestration for a global app with primary S3 (with Object Lock) and DynamoDB in us-east-1 and a DR site in us-west-2. Require 15 min RTO and 5 min RPO, data residency, automated backups via AWS Backup, cross-region replication that respects residency, and Route 53 failover. Describe a Step Functions workflow to automate failover, validation, and rollback without production impact, with testing cadence and success criteria?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Cloudflare","Hashicorp","Tesla"]},{"id":"q-1542","question":"In a three-account AWS Organization (prod, staging, dev) for a global SaaS app, enforce cost governance by requiring that all new resources carry the tags Owner and Environment in prod, with automated remediation for noncompliant resources and separate budgets/alerts per account. Design an end-to-end process using SCPs, AWS Config rules, EventBridge, Lambda, and AWS Budgets to detect, remediate, and alert. Include testing steps?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Citadel","DoorDash","Stripe"]},{"id":"q-1591","question":"In a multi-account AWS Organization hosting a global service, a misconfigured VPC peering or transit gateway leaks a route to the internet, risking data exposure and egress costs. Design an automated, end-to-end remediation and validation workflow (detection, isolation, rollback) using AWS Config and Config Rules, EventBridge, Lambda, IAM roles, and CloudWatch alarms, plus a controlled traffic test before rollback. What steps and artifacts would you implement?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Amazon","Instacart","Netflix"]},{"id":"q-1634","question":"How would you implement a guardrail so every new EC2 instance in a single AWS account for a mobile backend launches only in Prod-Subnet, carries Owner and Environment tags, and uses IAM role MobileBackendRole, with automated remediation and alerts via AWS Config, Lambda, EventBridge, and SNS while providing a testing plan?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Cloudflare","DoorDash"]},{"id":"q-1672","question":"Across a three-account AWS Org (prod, staging, dev) hosting a Databricks-powered data lake on S3, design an automated end-to-end remediation to ensure all compute resources access S3 only via VPC Endpoints, enforce private DNS, and block public S3 access. Include SCPs, AWS Config rules, EventBridge, Lambda, and GuardDuty findings with testing steps and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Databricks","Hashicorp"]},{"id":"q-1765","question":"Within a multi-account AWS environment across six regions hosting a critical financial service, implement automated detection and remediation of IAM role trust policy misconfigurations that could expose cross-account access. Use AWS Config, Lambda, EventBridge, IAM Access Analyzer, and SCPs in a central Governance account. Outline controls, testing, and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Apple","Goldman Sachs","Lyft"]},{"id":"q-1886","question":"In a 3-account AWS Organization (prod, staging, dev) for a global SaaS app, enforce a global IAM password policy across all accounts: min length 14, require uppercase, lowercase, number, symbol, password age 90 days, and forbid reuse of last 5. Design an end-to-end automation using SCPs, an AWS Config custom rule, EventBridge, Lambda, and AWS Budgets to detect drift, remediate, and alert. Include testing steps?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Hugging Face","Oracle"]},{"id":"q-1934","question":"In a 5-account, multi-region AWS setup (prod, prod-sec, staging, dev, shared) spanning 3 regions, design an automated disaster recovery plan for a critical app running on **EKS** and **RDS**. The DR must auto‑failover **RDS** to cross‑region replicas, switch **Route 53** DNS, re‑sync **EKS** state with **ArgoCD**, rotate encryption keys, and enforce **SCPs** for DR accounts. Include testing, rollback, and post‑drill validation?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["OpenAI","Snowflake"]},{"id":"q-1949","question":"In a 3-account AWS Organization (prod, staging, dev) for a global SaaS app, configure automatic idle-instance remediation. If an EC2 instance in prod has the tag AutoStop=true and CPUUtilization < 5% for 24h, stop it at 02:00 local time. Use AWS Config to detect noncompliance, EventBridge/Lambda to stop, and a Config rule to enforce tag presence. Include testing steps?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Cloudflare","Two Sigma"]},{"id":"q-2009","question":"In a single-region AWS setup hosting a small SaaS app, design a weekly automated DR test that validates restoring an RDS snapshot and at least one EBS volume, then runs a basic end-to-end check against the app. Outline practical steps using AWS Config, EventBridge, Lambda, and a separate DR bucket/account for test artifacts, including rollback steps and how you verify success?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Google","Hashicorp","Square"]},{"id":"q-2046","question":"In a multi-region AWS deployment (prod across 4 regions) with accounts prod, security, infra, and audit, design an automated containment workflow triggered by GuardDuty findings of UnauthorizedAccess or PrivilegeEscalation. Automatically quarantine affected EC2s by swapping in a deny-all security group and routing traffic to a quarantine subnet, while persisting original SGs for rollback. Restoration requires security approval. Include cross-region EventBridge routing, Lambda orchestration, IAM permissions, rollback, and testing?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Coinbase","Meta","Two Sigma"]},{"id":"q-2136","question":"In a 3-account AWS Organization (prod, staging, dev) for a global SaaS app, enforce a secure S3 baseline in prod: no public access, SSE-KMS with a central CMK, and versioning enabled on all buckets. Use AWS Config rules (e.g., s3-bucket-public-access-prohibited, s3-bucket-server-side-encryption-enabled, s3-bucket-versioning-enabled), EventBridge, Lambda, and SCPs to detect, remediate, and alert. Include testing steps and rollback plan?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Instacart","Oracle","Uber"]},{"id":"q-675","question":"You manage a two-region AWS deployment (us-east-1, us-west-2) behind an ALB with private subnets, NAT gateway, and RDS in us-east-1. During business hours, us-west-2 exhibits spike in 5xx errors and higher latency. Outline immediate incident triage steps, AWS CLI commands to run, how you’d identify root causes (NAT saturation, DNS routing, cross-region replication lag), and both short- and long-term mitigations with verification steps?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Apple","Snowflake"]},{"id":"q-982","question":"Design an automated cross-region disaster recovery plan for a globally distributed web app currently active in us-east-1 with a DR site in us-west-2, covering RDS, DynamoDB, S3, and ALB-backed frontend. Specify data synchronization, failover steps, testing, and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Anthropic","Google"]},{"id":"q-1036","question":"Scenario: A new external vendor needs read-only access to a single Azure AD-secured web portal (SaaS app) via B2B. Create a guest user, assign them to a dedicated App Role of the portal, enforce MFA and device-compliant access with a Conditional Access policy restricted to the office IP range, and outline post-grant validation and rollback?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Citadel","NVIDIA","PayPal"]},{"id":"q-1109","question":"How would you enable Self-Service Password Reset (SSPR) for a 20-user Azure AD group, enforce MFA during resets, and ensure auditable reset events with a simple rollback plan? Include licensing notes, enrollment flow, verification steps, and failure handling?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["MongoDB","Stripe"]},{"id":"q-1191","question":"Scenario: A data platform CI/CD pipeline deploys to Data Lake Gen2, Synapse, and a storage account across three subscriptions using a non-interactive service principal. Design a secure, rotating, auditable auth model: App Registration with certificate-based OAuth, Key Vault-stored certs rotated every 30 days, per-resource RBAC with least privilege, automatic revocation on build failure, and validation steps. Include how you would test access during runs and how to roll back changes if needed?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Databricks","Google","PayPal"]},{"id":"q-1304","question":"Scenario: a startup with 5–15 users is moving to Azure AD for the first time. You need to enable basic identity services: (a) create users and groups, (b) provide SSO for a SaaS app using SAML, (c) enforce MFA, (d) automatically assign Office 365 licenses via group membership, and (e) enable self-service password reset for all users. Outline an end-to-end setup plan and a minimal validation checklist prior to go-live?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Discord","IBM","Lyft"]},{"id":"q-1363","question":"Design a cross-tenant temporary access workflow for a contractor to a SaaS portal secured by Azure AD, leveraging Azure Lighthouse, B2B guest accounts, PIM with Just-In-Time activation, Conditional Access (MFA, device compliance, IP restrictions), and an access-review auto-revoke policy. Detail validation during activation and rollback?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Anthropic","Apple"]},{"id":"q-1380","question":"Scenario: Implement Just-In-Time elevation for Azure AD roles using Privileged Identity Management (PIM) to grant temporary Global Administrator and Privileged Role Administrator access. Include activation workflows, MFA requirements, approval routing, auto-expiry, and post-activation auditing. How would you validate elevated access usage and ensure revocation, including staging validation prior to production?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Instacart","LinkedIn","Two Sigma"]},{"id":"q-1427","question":"Implement Just-In-Time privileged access for production admin roles using Azure AD PIM. Requirements: MFA, approval workflow, time-bound activation, auto-expiry, and audit trails. Describe end-to-end steps: role setup, activation process, testing, monitoring, and how to safely handle emergency bypasses?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Adobe","Apple","Cloudflare"]},{"id":"q-1448","question":"How would you architect an end-to-end cross-tenant access governance flow that grants a partner contractor temporary access to Snowflake via Azure AD B2B and Entitlement Management, with MFA, sign-in risk conditions, an approvals workflow, auto-revocation after 10 days, and automated access reviews before renewal?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Coinbase","Netflix","Snowflake"]},{"id":"q-1561","question":"Design a beginner-friendly plan to enforce conditional access for a single Azure AD‑secured internal portal used by contractors. Include which policies you would create (MFA, device-compliance, IP ranges), how you would test rollout with a small pilot, and how you would validate access during the pilot and after, with minimal automation?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Airbnb","Discord","DoorDash"]},{"id":"q-1589","question":"Scenario: You run Azure AD for a multinational org with internal staff and external partners. Secure access to a sensitive SharePoint Online library and a CI/CD portal. Design a concrete end-to-end plan using: (a) Conditional Access with device compliance and sign-in risk, (b) Entitlement Management for guest access with time‑boxed access and automated reviews, (c) Privileged Identity Management for service accounts with Just‑In‑Time, approvals, and MFA, and (d) comprehensive auditing and alerting. Include validation steps and rollback?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Google","Meta","Stripe"]},{"id":"q-1753","question":"Scenario: A regulated fintech uses Azure AD Entitlement Management to grant external QA vendors episodic access to five internal SaaS apps. Design an Access Package strategy: define apps/roles, set time-limited assignments, enforce MFA, approval workflows, and auto-expiry with revocation. Outline validation across apps during the window and post-expiry?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Bloomberg","Robinhood","Snap"]},{"id":"q-1784","question":"Design a risk‑aware Azure AD access plan for two high‑risk apps (CI/CD portal and HR app) in a multi‑tenant setup, using Conditional Access with sign‑in risk, device compliance, and trusted IPs; implement auto‑remediation via Access Reviews and PIM for admins; outline testing, rollback, and auditing while minimizing automation?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["DoorDash","Salesforce"]},{"id":"q-1824","question":"Pilot passwordless sign-in for 10 users accessing a single internal Azure AD‑secured portal. Describe exact steps to enable passwordless (FIDO2 or Windows Hello), enroll devices, register credentials, enforce a Conditional Access policy for that app, run a 2-week pilot, and validate success and rollback procedures if issues arise?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Cloudflare","Google"]},{"id":"q-1876","question":"In a Cloudflare/NVIDIA-style Azure AD tenant, a partner needs access to a single Azure AD-secured portal for 30 days. Outline a concrete plan to grant time-bound access using Entitlement Management (Access Packages): define the package scope and roles, configure approvals, enforce auto-expiry, and audit/validate during the window and after expiry with minimal scripting. Include pilot testing steps?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Cloudflare","NVIDIA"]},{"id":"q-1985","question":"Scenario: You need to onboard external vendors to access a single Azure AD‑secured SaaS app via Azure AD B2B. Draft a beginner‑friendly, end‑to‑end plan to (1) grant guest access through a dedicated security group, (2) auto‑assign the required SaaS license via group membership, (3) enforce MFA for guests with Conditional Access, and (4) set a 14‑day expiry with Access Reviews and a rollback path. Include validation steps during the window and after expiry?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Coinbase","Google","Lyft"]},{"id":"q-2042","question":"Scenario: A regulated fintech team requires time-bound admin access to production Azure AD and resources via Privileged Identity Management (PIM) for a 72-hour window each quarter. Outline an end-to-end plan to (1) assign and activate a PIM-eligible role, (2) require MFA and Just-In-Time activation, (3) implement approvals and alerts, (4) auto-expire and enable post-activation reviews, and (5) validate access during and after the window with minimal scripting?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Anthropic","Coinbase","Snowflake"]},{"id":"q-2110","question":"Architect an identity governance pattern to manage admin access to multiple critical apps (both SaaS and internal portals) in Azure AD across a multi-tenant org. Design an integrated solution using Identity Governance features: Entitlement Management with Access Packages, Access Reviews with auto-remediation, Privileged Identity Management for JIT elevation, and Conditional Access signals to enforce device compliance and MFA. Include lifecycle flows, automation touchpoints, testing, and rollback plan?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Databricks","LinkedIn","NVIDIA"]},{"id":"q-855","question":"Your organization needs a temporary access workflow: grant a contractor read/write access to a single Blob Storage container for 30 days using Azure AD groups and RBAC, and automatically revoke access at day 30. How would you implement this end-to-end?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["DoorDash","IBM","Stripe"]},{"id":"q-897","question":"Scenario: A contractor needs access to a single Azure AD‑secured app (CI/CD portal) for 14 days. Outline an end‑to‑end approach using a dedicated Azure AD security group, app RBAC, and an Access Review to auto‑revoke access at day 14. Include how you would validate access during the window and after expiry, with minimal automation and no custom scripts?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Airbnb","Twitter","Two Sigma"]},{"id":"q-929","question":"Scenario: A multinational bank needs external data scientists to access multiple Azure resources (Data Lake Gen2, Synapse, and a storage account) for a 6-week analytics project. Propose an end-to-end access model using Azure AD Entitlement Management, Access Reviews, B2B collaboration, and Privileged Identity Management (PIM) for Just-In-Time role activations, with cross-subscription considerations, least privilege, and auditability. Include how you enforce MFA, token lifetimes, revocation at end, and validation steps?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Goldman Sachs","Netflix","PayPal"]},{"id":"q-942","question":"Scenario: A vendor must run nightly ingestion pipelines against Data Lake Gen2, a storage account, and Synapse in a shared Azure AD tenant for 10 days using a single service principal. Propose an end-to-end access model using an App Registration with scoped RBAC, Just-In-Time activation (PIM) for the service principal, Entitlement Management or Access Reviews, and automatic revocation at expiry. Include how you would validate access during the window and after expiry with minimal automation and no custom scripts?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Instacart","NVIDIA"]},{"id":"q-1053","question":"You’re deploying a multi-tenant Azure OpenAI-powered customer-support assistant for a global marketplace. Describe an end-to-end plan for runtime isolation and governance: prevent prompt injection, redact PII before OpenAI calls, enforce per-tenant quotas, maintain data lineage in Purview, ensure regional residency, and implement drift alerts via Azure Monitor plus a lightweight detector in Azure ML?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Cloudflare","DoorDash","Snowflake"]},{"id":"q-1134","question":"You're building a beginner-level Azure OpenAI-powered chat assistant for a rideshare service that serves clients in two regions. Outline a concrete data path and a minimal routing implementation that ensures user messages and model outputs stay in-region. Include a TypeScript function that selects the regional OpenAI endpoint based on client region, a latency fallback policy, and basic in-region logging to Azure Monitor. Provide testing approaches?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Citadel","Google","Uber"]},{"id":"q-1330","question":"Design a region-agnostic, tenant-aware GenAI service on Azure OpenAI that updates safety and governance policies at runtime via a central config store (Azure App Configuration) without redeploying prompts. Include how you route queries, enforce per-tenant data residency, and measure latency under 200ms for common tasks?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Hashicorp","Snowflake","Square"]},{"id":"q-1360","question":"You're building a real-time, multi-tenant Azure OpenAI-powered support chatbot for PayPal, MongoDB, and Two Sigma. You want canary deployments of two model variants (v1, v2) with automatic rollback on degradation, regional routing, and per-tenant quotas. Describe a concrete end-to-end approach: how you implement versioned deployments, traffic routing, governance, latency/quality monitoring, and rollback triggers using Azure OpenAI, API Management, Functions, Front Door, and Monitor. Include data flow and concrete metrics?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["MongoDB","PayPal","Two Sigma"]},{"id":"q-1414","question":"Scenario: You're deploying a multilingual, multi-tenant fintech chat assistant on Azure OpenAI Service. Each tenant's data must remain in their region, with per-tenant quotas, prompt-injection defenses, and automatic redaction before OpenAI calls. Outline a concrete deployment and testing plan using Azure API Management, Azure Functions, Text Analytics, Purview, regional OpenAI endpoints, and Azure Monitor. What edge cases exist and how would you verify data residency and drift monitoring across languages?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Meta","Scale Ai","Two Sigma"]},{"id":"q-1468","question":"You’re building an Azure OpenAI-powered support assistant for a payments platform that must isolate tenant data regionally, enforce per-tenant quotas, and attach citations to every answer. Design the end-to-end architecture, data paths, and testing plan using Azure API Management, regional OpenAI deployments, Redis quotas, Azure Cognitive Search with vector store, Purview, and residency checks. What are the key trade-offs?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["PayPal","Twitter"]},{"id":"q-1509","question":"Design a regional, policy-driven retrieval-augmented generation (RAG) pipeline on Azure for a global platform. Enforce per-tenant data residency, block secrets in prompts, redact PII automatically. Implement per-tenant quotas and budget alarms; emit audit trails to Purview. Route via APIM to regional OpenAI endpoints and a regional vector store (Cognitive Search or Cosmos DB) with drift monitoring and rollback?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Airbnb","MongoDB","Twitter"]},{"id":"q-1597","question":"You're building a beginner-friendly SaaS chat API powered by Azure OpenAI. It should be fronted by Azure API Management and implemented with an Azure Function backend. Implement a per-tenant rate limit of 60 requests per minute using an API Management policy and log usage to Azure Monitor. Describe the end-to-end setup, plus a minimal policy snippet and a tiny function wrapper showing the data flow. How would you validate it?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Instacart","NVIDIA"]},{"id":"q-1641","question":"Scenario: A Meta/Anthropic-scale social platform needs an Azure-hosted, multi-tenant content moderation bot. It must support multilingual queries, preserve tenant data residency in their regions, harden against prompt injection, redact PII before any OpenAI calls, enforce per-tenant quotas, and provide drift alerts. Outline an end-to-end pipeline using Azure API Management, Azure Functions, Azure OpenAI Service (regional endpoints), Text Analytics for PII, Purview for data lineage, and Azure Monitor plus a lightweight detector in Azure ML. Include a minimal policy snippet and a tiny function wrapper to illustrate data flow?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Anthropic","Meta"]},{"id":"q-1650","question":"Scenario: You are building a beginner-level multi-tenant Azure OpenAI chat assistant that uses a versioned prompt catalog stored in Azure Blob. API Management selects the tenant’s prompt version, enforces a per-tenant quota, redacts PII before calling OpenAI, and falls back to a default prompt if the catalog fetch fails. Outline the end-to-end flow, a minimal policy snippet, a simple function wrapper, and a test/failover plan?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Coinbase","Databricks"]},{"id":"q-1762","question":"Design a private, per-tenant Azure OpenAI-powered assistant exposed via API Management with strict data residency and governance. Describe end-to-end architecture using Private Link to OpenAI, tenant-scoped API Management gateways, per-tenant Managed Identities to call the private endpoint, regional OpenAI endpoints, Key Vault for secret rotation, Purview auditing, per-tenant quotas, and a drift detector in Azure ML. Include deployment, data flow, failure modes, and testing?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Amazon","Hashicorp"]},{"id":"q-1777","question":"Design an Azure OpenAI code-assistant workflow for a SaaS platform where developers paste private repository snippets to generate patches. Implement hard secret redaction before OpenAI calls, integrate Azure Key Vault for secret lookups with RBAC, enable per-tenant data isolation, and log all prompts/responses to Purview. How would you route, guard, and verify outputs end-to-end?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Cloudflare","Microsoft"]},{"id":"q-1833","question":"Design a real-time, search-augmented chat assistant on Azure OpenAI Service for a video conferencing platform. It must query a region-bound Azure Cognitive Search index, redact PII before OpenAI calls, enforce per-tenant quotas, and provide per-tenant explainability of retrieved docs. Outline the end-to-end data path (ingestion, vector search, redaction, prompt assembly, streaming response), a minimal API Management policy snippet, and a tiny backend snippet illustrating the flow and versioning strategy?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Cloudflare","Hugging Face","Zoom"]},{"id":"q-1891","question":"You're deploying an Azure OpenAI-powered enterprise assistant for a multi-tenant SaaS app. Each tenant's data must never mix, residency must be region-bound, and prompts must be safeguarded against leakage via memory or external tools. Outline a concrete data path and governance using Azure OpenAI Service (private endpoint), Azure Functions, API Management, per-tenant Cosmos DB for embeddings, and per-tenant quotas, with a lightweight detector for leakage. Include an example policy snippet and a test plan?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Apple","Goldman Sachs","Meta"]},{"id":"q-1898","question":"Design a privacy-preserving, multi-tenant AI data enrichment pipeline on Azure that uses Azure OpenAI Service for generation and embeddings to augment customer data with external sources, while enforcing per-tenant data residency, PII redaction, drift detection, and auditable governance via Purview; outline architecture, data flows, and testing edge cases for latency and cost under burst traffic?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Citadel","Plaid","Uber"]},{"id":"q-1933","question":"You're deploying a privacy-preserving Azure OpenAI-powered analytics assistant for a fleet of autonomous vehicles with edge-to-cloud hybrids. Tenants must keep data in their own stores, even at the edge, and APIs are exposed via APIM. Propose a concrete pipeline using Azure Arc-enabled OpenAI, Private Endpoints, Functions, Purview, and Monitor, detailing data flow, tenancy isolation, data residency, offline mode and drift monitoring. Include testing and edge-case notes?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Slack","Snowflake","Tesla"]},{"id":"q-1958","question":"Design a multilingual customer feedback analytics pipeline that runs on Nvidia edge GPUs at retail kiosks and pairs with Azure OpenAI for summarization. Ingests local text, translates to English, analyzes sentiment, flags PII locally, then pushes aggregated metrics to Azure for long-term storage. Describe the data path, services involved, governance, and testing approach?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Goldman Sachs","NVIDIA","Snowflake"]},{"id":"q-1994","question":"Beginner-level: You have a multi-service Azure OpenAI chat pipeline: APIM -> Function wrapper -> OpenAI. Latency is variable. Describe a minimal end-to-end tracing plan using Application Insights, OpenTelemetry, and correlation IDs. Show how you would propagate a trace across APIM policy, Function, and OpenAI call, and outline a simple test to reproduce and measure end-to-end latency?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Coinbase","MongoDB","Salesforce"]},{"id":"q-2066","question":"Scenario: You run a multi-tenant Azure OpenAI-powered assistant where each tenant supplies per-tenant safety policies and tone guides that must be enforced before any OpenAI call. Design an end-to-end path using Azure API Management, Azure Functions, a lightweight policy engine, regional OpenAI endpoints, and Purview for data lineage. Explain policy evaluation, tenant isolation, drift handling, and testing with realistic workloads?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Coinbase","IBM","Lyft"]},{"id":"q-2116","question":"Design a multi-tenant Azure OpenAI-powered support bot for fintech apps with per-tenant regional residency, live moderation gating, and PII redaction before OpenAI calls. Outline the end-to-end data path, components, and a testing plan; include edge cases and how residency and drift are validated?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["PayPal","Stripe","Uber"]},{"id":"q-2162","question":"You're building a multi-tenant Azure OpenAI-powered analytics assistant that ingests user-uploaded CSVs and returns AI-generated insights via chat. Each tenant requires data isolation, per-tenant model versions, and privacy controls enforcing in-region processing and PII redaction in outputs. Design an end-to-end pipeline: user → API gateway → function orchestrator → per-tenant data store in Data Lake (Purview-tagged) → OpenAI inference with tenant-specific prompts and versioned deployments. Include canary rollouts, tests for data leakage, and drift monitoring?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Amazon","Google","MongoDB"]},{"id":"q-866","question":"You're deploying a multi-tenant chat assistant on Azure OpenAI Service for a rideshare company. PII must never be sent to OpenAI and responses must redact sensitive data before delivery. Outline a practical, beginner-friendly data path using Azure API Management, Azure Functions, Text Analytics for PII detection, and a regional OpenAI deployment. Include a simple data flow?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["MongoDB","Tesla","Uber"]},{"id":"q-963","question":"You're building a beginner-friendly customer support bot on Azure OpenAI Service. How would you design a lightweight API boundary policy (Azure API Management) to rate-limit per user, cap monthly spend, and gracefully fall back to a rule-based reply if OpenAI is unavailable? Describe the data flow from API call through OpenAI or fallback, and include a minimal policy snippet?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Apple","Bloomberg","Square"]},{"id":"q-1029","question":"In a global IoT telemetry pipeline ingesting 100k events/s per region into ADLS Gen2 and Databricks Delta Lake, implement schema drift tolerant ingestion, end-to-end data lineage via Purview, RBAC, and cross-region DR replication. How would you architect the data contracts, partitioning, watermarking, retention, and governance to meet data residency and fault-tolerance requirements?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Cloudflare","Twitter","Uber"]},{"id":"q-1043","question":"Design an end-to-end Azure data-ecosystem pipeline that ingests incremental changes from a MongoDB Atlas collection into Delta Lake on ADLS Gen2, preserving SCD Type 2 history for a customers dimension, and handling schema drift. Include data lineage to Azure Purview, late-arriving updates, and on-demand reprocessing without data loss. Which components and config would you choose, and why?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["MongoDB","NVIDIA","Robinhood"]},{"id":"q-1311","question":"Design an end-to-end incremental ELT pipeline on Azure that ingests 2 TB/day of nested JSON telemetry from Event Hubs into a Delta Lake on ADLS Gen2, then loads a star schema in Azure Synapse. Requirements: (1) schema drift tolerant writes and schema evolution, (2) upserts by a composite key (tenant_id, event_id), (3) end-to-end data lineage to Purview, (4) late-arriving data support via watermarking, (5) partitioning by region and day with file-size/compaction strategies. Which components and patterns would you use, and why? Compare Delta Lake MERGE vs Delete+Insert for upserts?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["Bloomberg","Google","Snowflake"]},{"id":"q-1326","question":"Design an end-to-end pipeline that streams user activity from Azure Event Hubs into Delta Lake on ADLS Gen2, supports schema evolution, and updates an SCD Type 2 customer dimension in Synapse. Include exactly-once guarantees, late-arriving data handling, idempotent sinks, and end-to-end data lineage via Purview in a hybrid estate. List components, data flows, and governance approach?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["IBM","Snap"]},{"id":"q-1453","question":"You are starting a beginner-friendly Azure data engineer task: daily telemetry logs arrive in ADLS Gen2 under /raw/telemetry/ in mixed formats (CSV and nested JSON). Design an event-driven pipeline using Azure Data Factory that fires on blob creation, ingests files, flattens nested structures to a canonical schema, handles optional fields and few schema drift cases, and loads into a simple star schema in Azure Synapse Analytics. Include incremental loading with a watermark and basic data quality checks. What components and steps would you implement?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Airbnb","Google"]},{"id":"q-1474","question":"Design a nightly Azure data pipeline that ingests delta updates from a SaaS source into ADLS Gen2, handles schema drift with automatic inference, and materializes a star schema in Azure Synapse. Ensure end-to-end data lineage, idempotent upserts, and reliable rollback after failures. Which services and patterns would you deploy, and how would you verify correctness and lineage end-to-end?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["Oracle","Plaid","Two Sigma"]},{"id":"q-1524","question":"You operate a global event lake: multiple producers push JSON events into Event Hubs. Ingest to ADLS Gen2, apply drift-tolerant transformations, and publish a canonical star schema in Azure Synapse with incremental loads. Propose a production-ready architecture leveraging Data Factory, Databricks Delta Lake, and Purview; detail schema evolution, late-arriving data handling, data quality gates, and privacy masking?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["LinkedIn","Scale Ai","Square"]},{"id":"q-1563","question":"Daily JSON feed arrives in ADLS Gen2. Design a beginner Data Factory pipeline to stage, flatten nested fields, validate data quality, and write a partitioned Parquet table in Azure Synapse. Handle minor schema drift by defaults and ignoring new fields. Which components and steps would you use?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Bloomberg","DoorDash","Twitter"]},{"id":"q-1638","question":"Design a streaming ingestion from an on-prem ERP into ADLS Gen2 for real-time analytics. Use Delta Lake with Auto Loader-like ingestion, bronze-silver-gold data path, exact-once delivery, and schema-drift tolerant processing. The silver layer feeds a star schema in Azure Synapse. Ensure end-to-end lineage in Purview, data quality gates, and robust late-arriving data handling. Describe architecture, contracts, and rollback strategy?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["IBM","Snowflake","Two Sigma"]},{"id":"q-1682","question":"Design an end-to-end near-real-time data pipeline for a global adtech dataset with mixed sources (on-prem SQL Server, Azure SQL, and SaaS APIs). Implement CDC, incremental loads, and schema drift tolerance, load into Delta Lake on Synapse, and enforce end-to-end lineage in Purview. Include data masking for PII, data quality gates, and a GitOps workflow for pipelines?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Amazon","Scale Ai"]},{"id":"q-1782","question":"Design a beginner-level Azure data ingestion task: In a fintech scenario, ingest daily transaction rows from an on-premises SQL Server into Azure Data Lake Storage Gen2 as Parquet files using Azure Data Factory. Include steps to handle date partitioning, security, and reliable runs. What would your pipeline look like, and what minimal code or configuration would you provide?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Citadel","Coinbase"]},{"id":"q-1916","question":"You manage a multi-region analytics platform with data from Salesforce, Workday, and IoT devices. Data lands in ADLS Gen2; you need to apply policy-driven redaction for PII, support incremental loads, enforce schema drift tolerance, and publish end-to-end lineage to Azure Purview and downstream consumers (Power BI, Synapse). Propose an Azure-native pipeline design using Data Factory, Databricks Delta Live Tables, Purview, and Synapse; highlight design decisions, governance, and potential pitfalls?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["LinkedIn","MongoDB","Snap"]},{"id":"q-1979","question":"In Azure, design a production-grade data pipeline for streaming ecommerce telemetry from Azure Event Hubs into ADLS Gen2, with downstream star schema in Synapse, enabling incremental loads, automatic schema drift handling, and end-to-end data lineage via Purview. Include details on Delta Lake usage, MERGE-based upserts, late-arriving data handling, and a minimal Spark code snippet for a MERGE with schema drift tolerance?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Databricks","OpenAI"]},{"id":"q-2102","question":"A batch of daily JSON click events arrives in ADLS Gen2 under /raw/events/ from an on‑prem source via Self-Hosted IR. Build a beginner Data Factory pipeline that (1) copies raw JSON to /staging/events/, (2) uses a schema-drift-tolerant mapping to normalize fields and cast ts to timestamp, (3) writes to a daily-partitioned Parquet table in Azure Synapse, and (4) loads incrementally by tracking max ts in a control table and filtering ts > last_ts. Include a basic data quality check for userId?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Airbnb","NVIDIA","Uber"]},{"id":"q-940","question":"Daily CSV exports arrive in an ADLS Gen2 container at /incoming/sales/. Files may evolve over time as columns drift. Build a beginner pipeline using Data Factory to stage raw data, infer/handle schema changes, and load a simple star schema in Azure Synapse Analytics. Include a watermark-based incremental load and basic scheduling. Which services and steps would you implement?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Adobe","Amazon","Zoom"]},{"id":"q-980","question":"Design a global streaming-to-batch data pipeline for a PayPal/Adobe/Netflix-scale analytics platform: ingest real-time clickstream from Event Hubs into ADLS Gen2, maintain a Delta Lake with drift-tolerant schema, mask PII at ingestion, expose masked aggregates via serverless SQL pool, and enforce end-to-end lineage with Purview while supporting cross-region DR and data residency. Which Azure components and patterns would you use, and how would you handle schema evolution and failure modes?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["Adobe","Netflix","PayPal"]},{"id":"q-1006","question":"Design a real-time telemetry ingestion pipeline for a fleet of autonomous vehicles on Azure. Events arrive at high volume per region; you must store compact per-vehicle summaries in Cosmos DB and archive raw events to Data Lake Gen2. How would you achieve exactly-once processing for aggregates, sub-200 ms latency, and zero data loss on transient failures? Propose architecture using Event Hubs, Functions, Databricks, and cross-region replication; justify idempotency and retry strategies?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Lyft","Slack","Tesla"]},{"id":"q-1136","question":"Design an end-to-end telemetry ingestion pipeline for 1M devices/min delivering messages {vehicleId, ts, lat, lon, speed}. Ingest via HTTPS into Event Hubs with vehicleId as partition key, process with a Function app (Event Hubs trigger) using batchSize=100; deduplicate per vehicle with Durable Entity and upsert to Cosmos DB multi-region. Explain data model, idempotency, Change Feed, backpressure, and monitoring?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Airbnb","Amazon","Tesla"]},{"id":"q-1248","question":"Design an end-to-end Azure ingestion pipeline for multi-tenant IoT events: thousands of devices per region send JSON to a gateway, per-tenant aggregates stored in Cosmos DB, raw data archived to Data Lake Gen2. Explain chosen services (Event Hub, Function/Durable Function, Cosmos DB with TTL, Data Lake), how you enforce per-tenant isolation and auditability, and how you achieve exactly-once processing and retry semantics?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Amazon","Citadel","Google"]},{"id":"q-1277","question":"Design a real-time multi-tenant feature-store pipeline on Azure for a high-velocity AI platform. Ingest telemetry events via Event Hubs (tenantId, featureName, value, ts). Build end-to-end streaming with exactly-once semantics, isolation by tenant, and low-latency online reads. Specify concrete components (Event Hubs, Spark Structured Streaming, Cosmos DB with tenantId partition, Redis online store), auditability, TTL, and testing strategy?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Google","Hugging Face","OpenAI"]},{"id":"q-1359","question":"A small API running on Azure Functions must securely retrieve a database connection string from Azure Key Vault at startup and refresh it periodically without restarting the function. Propose a beginner-friendly, low-latency approach using Managed Identity and Key Vault, including caching strategy, rotation handling, and error fallback?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["OpenAI","Salesforce","Snowflake"]},{"id":"q-1397","question":"Design a multi-region, per-tenant Azure API with data residency, deterministic retries, and exactly-once semantics at scale. Propose services (APIM, Functions + Durable Functions, Cosmos DB per-tenant, Front Door, Private Endpoints) and explain how per-tenant isolation, data residency, and retry determinism are achieved?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Goldman Sachs","Oracle","Plaid"]},{"id":"q-1539","question":"Design a real-time, multi-tenant analytics pipeline for a chat platform: events arrive at 5-10k msgs/sec total via Azure Event Hubs; process with Azure Databricks on Delta Lake stored in ADLS Gen2; governance via Unity Catalog; ensure strict per-tenant isolation, auditability, and exactly-once processing; TTL/retention; cross-region read; cost constraints. Describe architecture decisions, data layouts, and failure scenarios?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Databricks","Discord","Microsoft"]},{"id":"q-1565","question":"Design a zero-trust, per-tenant access model to a private Azure SQL Database from a microservices mesh. Use Azure AD, Managed Identities, Row-Level Security, and dynamic data masking. Explain how you enforce least privilege, tenant isolation, auditing, and how you validate access policies in CI/CD?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Discord","Google","Plaid"]},{"id":"q-1594","question":"You're building a real-time, multi-tenant feature-flag platform on Azure to serve traffic across three regions with sub-50ms evaluation latency. Each tenant has per-flag rules, canary/A/B experiments, and strict audit requirements. Outline end-to-end design: data model for flags and experiments, evaluation path, storage choices (Cosmos DB vs SQL), caching, event sourcing, cross-region synchronization, security (Managed Identities, Key Vault, RBAC), rollback strategy, and failure modes. Include how you'd test canary safety and ensure tenant isolation?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Citadel","DoorDash"]},{"id":"q-1620","question":"You're architecting a **multi-region telemetry ingestion** pipeline for a real-time fraud-detection service. Edge devices per region push JSON events to **Azure IoT Hub**; you must ingest, partition by tenant, store raw and processed results with exact-once semantics, and enforce strict per-tenant isolation and auditability within tight cost constraints. Propose architecture choices (IoT Hub, **Event Hubs**, **Delta Lake**, **Unity Catalog**, **Cosmos DB**), data layout, and failure modes; how would you test end-to-end dedup and cross-region replayability?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Google","Hugging Face"]},{"id":"q-1668","question":"You’re building a beginner Azure Function (HTTP trigger) that calls a 3rd‑party API. Do not store API keys in code or app settings. How would you securely fetch the API key at runtime from Azure Key Vault using a managed identity? Outline the steps to enable the identity, grant access, and provide a minimal code snippet to read the secret?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["LinkedIn","Slack"]},{"id":"q-1727","question":"You're building a polyglot Azure-based service with HTTP APIs, background workers, and a data lake. You need end-to-end tracing across Functions, AKS, and Databricks jobs using OpenTelemetry and a single trace across services. Describe how you'd implement tracing, propagate context, and collect/export to Azure Monitor, including sampling strategy and validation steps?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Databricks","Meta","Zoom"]},{"id":"q-1791","question":"You're building an Azure-native, multi-tenant data platform for real-time payments used by PayPal and Tesla. Ingest via Event Hubs, process with AKS and Functions, store in Delta Lake on ADLS Gen2, expose a data API. How would you enforce strict per-tenant isolation, achieve end-to-end exactly-once semantics across services, and implement a shadow-traffic ML model deployment with safe rollback and audit trails? Include architecture choices, data layouts, and failure modes?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["PayPal","Tesla"]},{"id":"q-1839","question":"You're building an Azure IoT telemetry platform for a global fleet. Devices send 20–30k events/sec to IoT Hub. You need per-tenant isolation, real-time enrichment (geolocation, device type), and fan-out to three sinks: Delta Lake on ADLS Gen2, Azure Data Explorer dashboards, and an AI inference service on AKS. Must guarantee at-least-once semantics, handle out-of-order data, and support cross-region DR with measurable RPO. Compare two architectures: (A) serverless micro-batch: IoT Hub → Event Hubs → Functions for lightweight enrichment; Spark Structured Streaming writes to Delta Lake on ADLS Gen2; sinks feed Data Explorer and AKS inference. (B) pure streaming: Event Hubs → Spark Structured Streaming with larger cluster, stricter SLAs, and end-to-end exactly-once semantics. Explain data models, failure modes, and governance?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Amazon","Google","Netflix"]},{"id":"q-1962","question":"You’re building a beginner Azure Function (HTTP trigger) that receives event payloads and stores them in a data container. Design a lightweight, auditable approach to log every write without slowing latency. Include how you would generate an immutable audit trail and what storage pattern you’d use. Provide a minimal code snippet to append an audit line with timestamp, eventId, and userId derived from Authorization header?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["Google","Tesla"]},{"id":"q-1995","question":"Describe how you would implement a timer-triggered Azure Function that runs every 15 minutes to poll an on‑prem REST endpoint and write a daily aggregation blob to Azure Blob Storage. How would you guarantee idempotent writes to avoid duplicates across retries, including blob naming strategy and a minimal check-then-write code pattern?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["Cloudflare","Plaid","Uber"]},{"id":"q-2059","question":"Design an Azure-native data export service for a fintech that must export per-customer data on demand and on a schedule, with strict data residency, consent checks, and an audit trail. Use ADLS Gen2, Cosmos DB, Event Grid, Durable Functions or Functions, and Key Vault. Explain data partitioning, encryption, access control, idempotency, and failure modes including retries, outages, and rollback. Provide a concrete data model and flow?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Goldman Sachs","Robinhood","Stripe"]},{"id":"q-2109","question":"You're building a real-time moderation pipeline for a global social app on Azure. Ingest flows via Azure Event Hubs at 20-40k msgs/sec, then you apply NLP classification in a chain of Functions (or Durable Functions) to flag policy-violating messages, store results in Cosmos DB with per-tenant isolation, and index metadata in Azure Cognitive Search for fast queries. How would you design for latency under 150ms per event, strict per-tenant data isolation, idempotent retries, and auditable trails across services? Include error handling and rollback strategies?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Airbnb","Amazon","Plaid"]},{"id":"q-2155","question":"You're building a tenant-aware API gateway on Azure that serves dozens of microservices for hundreds of tenants. Implement per-tenant quotas, latency budgets, and canary rollouts. Describe a concrete architecture using Azure API Management, Azure Front Door, Redis for rate-limiting, and per-tenant state in Cosmos DB or Redis; explain failure modes, rollback, and testing strategies under traffic spikes?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["IBM","Uber"]},{"id":"q-891","question":"Design a beginner-friendly serverless image-upload API on Azure: clients POST JPEGs to an HTTP-triggered Function, which saves the file to Blob Storage, enqueues a processing job, and stores a metadata record in Cosmos DB. How would you ensure idempotency, handle retries with back-off, and keep costs predictable on a Consumption plan?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["Robinhood","Salesforce","Tesla"]},{"id":"q-973","question":"Case: You’re building a beginner-friendly Azure API that accepts events from mobile apps. Each event includes userId, eventType, and timestamp. The API should write a compact summary to Cosmos DB and stream raw events to Event Hubs for analytics. On a Consumption plan, outline the minimal architecture, bindings, and error handling to ensure low latency, safe retries, and no data loss during transient outages?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["PayPal","Scale Ai"]},{"id":"q-1063","question":"With a 60-service monorepo deployed to Azure subscriptions across three regions, design an end-to-end release strategy that builds per-service artifacts, promotes to dev/stage/prod environments, gates each promotion with environment approvals tied to region-owner groups, and enforces that PRs link to an Azure Boards item. Include a minimal YAML template and gating approach?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Airbnb","Cloudflare"]},{"id":"q-1135","question":"Design an Azure DevOps multi-tenant canary deployment pipeline for a SaaS service that promotes per-tenant changes to prod only after a staged rollout window, uses tenant-scoped feature flags, enforces per-tenant approvals before prod, and rolls back automatically if telemetry thresholds are exceeded; outline the pipeline structure, environment gates, and auditing approach?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-devops-engineer"],"companies":["LinkedIn","Stripe"]},{"id":"q-1255","question":"Scenario: You’re configuring a new Azure DevOps project with Repos and Boards for a small service. How would you implement beginner-friendly PR governance to ensure PRs into main are linked to a Boards work item, the PR title includes the work item ID, a PR validation build runs and passes, and an automatic staging deployment with a manual prod gate? Outline exact steps and considerations?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Microsoft","Robinhood"]},{"id":"q-1329","question":"In Azure DevOps, design an end-to-end pattern to provide per-PR ephemeral environments in AKS for a microservices app. Use a single AKS cluster with namespace-per-PR, Helm for deployments, Azure Key Vault for per-PR secrets, and a TTL-based teardown. Describe the pipeline steps, gating, security, costs, and how to isolate test data and telemetry. End with ?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["DoorDash","LinkedIn"]},{"id":"q-1355","question":"Configure a PR-only security gate for a Node.js service: trigger on PRs to main (trigger: none; pr: branches include: main) and skip pushes. Add a security job that runs npm ci, npm audit --json, and fail if any advisories of severity high or critical exist, printing a concise summary. This blocks the PR until issues are addressed?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Google","Robinhood"]},{"id":"q-1423","question":"In Azure DevOps for a monorepo with multiple npm packages, design a PR validation that runs tests only for changed packages, builds per-package artifacts, and gates PR merge until all relevant tests pass; include a minimal YAML snippet showing path-based triggers, npm workspaces, and artifact publishing?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Databricks","MongoDB","Twitter"]},{"id":"q-1463","question":"How would you implement an Azure Pipelines YAML that tests a Node.js app against Node.js 14 and 16 using a matrix, caches npm dependencies with Cache@2 to speed up PR builds, and invalidates the cache when package-lock.json changes? Provide the YAML snippet and explain the key derivation?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["IBM","PayPal"]},{"id":"q-1486","question":"Design a blue/green canary rollout in AKS for a critical microservice using Azure Pipelines. Use Istio for traffic splitting, separate namespaces for canary and stable, and Azure Key Vault for per-environment secrets. Describe pipeline structure, gating with approvals, health checks, telemetry correlation across versions, auto-rollback, and cost controls?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Databricks","Microsoft","Scale Ai"]},{"id":"q-1513","question":"Design a scalable Azure Pipelines pattern for a multi-tenant SaaS app where each tenant has its own Azure Key Vault and database shard. Outline a per-tenant environment bootstrap, secret injection into deployments, governance checks (Azure Policy, approvals), telemetry tagging to prevent cross-tenant leakage, TTL-based teardown, and a robust rollback path with canary moves. How would you implement this end-to-end?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Apple","Bloomberg"]},{"id":"q-1535","question":"You have a small Python service in Azure Repos. Create a beginner CI pipeline in Azure Pipelines that: (1) runs on push/PR, (2) installs Python 3.11, (3) runs pytest with coverage, (4) builds a wheel and publishes it as a build artifact, and (5) gates release to Prod with a manual approval and a built-in security scan before promotion. How would you implement this?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Discord","MongoDB","Snap"]},{"id":"q-1644","question":"In Azure DevOps, you manage a beginner Node.js microservice stored in Azure Repos. Create a practical CI/CD flow (YAML) that: 1) triggers on push and PR, 2) uses Node.js 18, 3) runs npm ci and npm test -- --coverage, 4) builds a Docker image and pushes to a private Azure Container Registry, 5) updates a Helm chart in the repo and promotes to a staging environment with a manual approval gate and a basic readiness check. What files and steps would you implement?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Google","LinkedIn","Oracle"]},{"id":"q-1670","question":"Design a robust multi-region release strategy in Azure DevOps for a real-time collaboration service deployed to AKS. The pipeline must support per-region canary rollouts using Istio, region-scoped feature flags in Azure App Configuration, per-region secrets in Azure Key Vault, auto-rollback on SLA deviations, and cost controls via HPA and budget alerts. Describe the YAML structure, gating approvals, health checks, telemetry correlation across regions, and rollback triggers. What files and steps would you implement?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-devops-engineer"],"companies":["Microsoft","Netflix","Zoom"]},{"id":"q-1770","question":"Design a beginner CI/CD workflow in Azure DevOps for a Go microservice stored in Azure Repos, containerized and deployed to AKS. Requirements: 1) Triggers on push and PR; 2) go test ./... -cover and build the binary; 3) multi-stage Dockerfile to build and publish image to ACR; 4) Helm upgrade dev with values.dev.yaml; 5) manual approval gate before prod upgrade with values.prod.yaml; 6) readinessProbe and livenessProbe in deployment; 7) health endpoint for readiness and rollout status check in pipeline. What files and steps would you implement?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Hashicorp","Lyft","Oracle"]},{"id":"q-1850","question":"Design an auditable, region-aware release workflow in Azure DevOps for a global payment service deployed to AKS. The pipeline must promote a single immutable container image across regions (EU, US, APAC) in sequence, generate SBOMs, run security scans, enforce region-specific approvals, implement per-region health checks and rollback triggers, and record full provenance in Azure Artifacts. Describe the YAML structure, gating, and rollback strategy?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["IBM","Meta","Stripe"]},{"id":"q-1911","question":"Design a cross-region release pipeline in Azure DevOps for a high-availability service deployed to AKS across two regions. Use Istio for traffic splitting, per-region Key Vault secrets, and region-scoped feature flags via Azure App Configuration. Implement a single reusable YAML template shared by regions, with region-specific approvals, health checks, telemetry correlation, auto-rollback on cross-region latency SLA breach, and cost controls via HPA and budget alerts. What files and steps would you implement?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Lyft","Meta"]},{"id":"q-1927","question":"Design a tenant-aware release pipeline for a multi-tenant SaaS service deployed to AKS in two regions. Include per-tenant RBAC with Azure AD, per-tenant secrets in Key Vault, per-tenant feature flags in App Configuration, and per-tenant canary traffic routed via Front Door. Architect a policy-driven gate that rejects a release if any tenant SLA is missed, and implement auto-rollback on regional drift. Provide files, stages, and gates you would implement?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Airbnb","Cloudflare"]},{"id":"q-1955","question":"Design an Azure Pipelines release for an AKS-hosted data ingestion service that reads from MongoDB Atlas and writes to Snowflake. Roll out a new transformer to three regions (US, EU, APAC) with per-region approvals, blue/green deployment, and a runtime feature flag to switch transformers. Include data-quality gates, idempotent Snowflake upserts, telemetry lineage, and auto-rollback on ingestion errors within 24 hours. Provide YAML structure and gating strategy?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-devops-engineer"],"companies":["MongoDB","Snowflake"]},{"id":"q-1977","question":"Design an incident-driven rollback for a real-time analytics service deployed on AKS via Azure DevOps. When production alerts breach SLA (latency > 500ms or error rate > 2%), automatically rollback to the previous stable revision, shift traffic back using Istio, pause the Canary, and run post-rollback health checks and synthetic tests before resuming promotion. Describe pipeline changes, alert integrations, gating, rollback criteria, and telemetry correlation?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Citadel","Cloudflare"]},{"id":"q-2103","question":"Design a PCI-DSS compliant CI/CD workflow in Azure DevOps for a payments microservice deployed to AKS, with per-tenant data isolation, image signing, SBOM generation, policy gates, and attestation-based deployments. Describe YAML structure, Key Vault secrets management, runtime scanning, rollback criteria, and how you’d prove compliance. What files and steps would you implement?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Lyft","Tesla"]},{"id":"q-2138","question":"Design a zero-downtime fleet upgrade for 5,000 Azure IoT Edge devices across regions using **Azure Pipelines** to build and sign Edge modules, publish to a registry, and use **Device Update for IoT Hub** to orchestrate per-group rollouts. Include per-group approvals, region-specific configuration, telemetry correlation, and automatic rollback if update failures exceed threshold. How would you structure the pipeline, gating, and rollback criteria?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Apple","Plaid"]},{"id":"q-1007","question":"A web app hosted in Azure App Service must securely call a private API hosted in an Azure Function behind a VNet. How would you implement private connectivity so traffic never leaves the Azure backbone? Include which resources you’d use, how to create a Private Endpoint for the API, DNS configuration, and how to restrict public access?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["DoorDash","Lyft","Salesforce"]},{"id":"q-1083","question":"An enterprise web app must enforce data residency per region, minimize egress costs, and meet real-time latency targets. Design an Azure-native, region-aware architecture that enforces residency, uses regional storage and a global entry point, and supports auditing and DR. Outline services, data replication, access control, and failover strategy?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Coinbase","Oracle","Scale Ai"]},{"id":"q-1110","question":"Scenario: A web API running in Azure App Service must persist user uploads to an Azure Storage account. Public access to the storage is disabled. How would you implement a Private Endpoint so the App Service talks to Storage over a private network, including enabling VNet integration, creating the Private Endpoint in the storage subnet, DNS configuration for privatelink, and any trade-offs?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Citadel","Coinbase","Goldman Sachs"]},{"id":"q-1212","question":"Scenario: a new web app stores and serves user-uploaded images globally. To keep costs predictable and delivery fast, which Azure services would you pair to store, serve, and secure access to images, and what trade-offs would you consider for storage tiering, CDN option, and access control?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Adobe","Hugging Face"]},{"id":"q-1242","question":"A startup runs a web app that stores user-uploaded images in Azure Blob Storage and serves them globally via CDN. Describe a practical storage design to minimize costs and latency: container layout, default and lifecycle tiers, lifecycle rules to auto-tier/delete, access control (RBAC vs SAS), and how you’d wire in CDN caching. Include concrete steps and example commands?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Cloudflare","IBM","Two Sigma"]},{"id":"q-1276","question":"Design an end-to-end Azure architecture for a globally distributed analytics platform servicing EU customers with strict data residency. Your plan should cover: data at rest with customer-managed keys, cross-region disaster recovery, private networking (Private Link/Endpoints), least-privilege access, encryption key rotation, and continuous policy compliance checks. Explain key services and trade-offs?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Hugging Face","Oracle"]},{"id":"q-1386","question":"You're building a real-time telemetry pipeline in Azure. Ingest devices via IoT Hub, route events to a Function App for normalization, and persist to an ADLS Gen2 data lake. Compare Event Grid vs Event Hubs for this pub-sub pattern, and justify your choice, including at-least-once delivery, backoff strategy, and observability requirements?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Microsoft","Square"]},{"id":"q-1491","question":"In a global fleet telemetry use-case, events from devices arrive across regions at high volume. Propose an end-to-end Azure pipeline that achieves sub-200 ms latency, ingests 200k events/sec per region, and writes to Delta Lake on ADLS Gen2 with exactly-once semantics and cross-region DR. Which components would you choose (Event Hubs, Functions/Durable Functions, Databricks or Synapse, Delta Lake, identity/security), and how would you implement idempotent sinks, retries, and cross-region failover?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Airbnb","Databricks","Discord"]},{"id":"q-1516","question":"A startup hosts a static frontend in Azure Blob Storage and a small API in Azure Functions. They want low costs, automatic scaling, and simple CI/CD using GitHub Actions. Design a beginner-level end-to-end setup: storage for static site, enable static website hosting, configure Functions with a Consumption plan, set up GitHub Actions for deployment, and outline basic security considerations?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["DoorDash","NVIDIA","OpenAI"]},{"id":"q-1580","question":"You have a REST API backend hosted in a private subnet (AKS/VM) that must be exposed to external partners. Requirements: IP allowlisting, DDoS protection, WAF, token-based authentication via Azure AD, and per-client rate limiting. Which Azure services would you assemble, and what specific configurations would you apply to meet these requirements while keeping backend private?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Coinbase","PayPal"]},{"id":"q-1710","question":"For a new Azure project with a static site on Blob Storage and a REST API on Azure Functions (Consumption), outline a beginner-friendly, least-privilege deployment setup: create an Azure AD service principal for GitHub Actions, assign minimal roles to deploy both resources, and use Azure Key Vault to store API keys. Include concrete roles and example CLI commands?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Citadel","Meta","Netflix"]},{"id":"q-1743","question":"Design a beginner-friendly, low-cost Azure webhook receiver that ingests JSON POSTs, runs in a Consumption-plan Function, validates an HMAC signature with a shared secret, stores each event in Azure Table Storage with PartitionKey as source and RowKey as GUID, and emits a basic success/failure metric in Application Insights. Outline steps and provide a minimal code snippet for signature verification in JavaScript?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Google","Microsoft","Snap"]},{"id":"q-1818","question":"A multi-tenant SaaS app must enforce strict per-tenant data isolation while serving a global user base with sub-100ms latencies. Propose an Azure-based storage and caching design (e.g., Cosmos DB with per-tenant partitioning vs relational sharding, caching strategy) and justify data residency, consistency, and failover choices. Include how you'd scale and monitor?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["DoorDash","Google","Hugging Face"]},{"id":"q-1829","question":"Advanced Azure Fundamentals: Design a cross-region real-time telemetry pipeline for a global IoT fleet (300k events/sec, 5 regions). Ingest with Event Hubs, process with Databricks Structured Streaming, and write to Delta Lake on ADLS Gen2 with exactly-once semantics. Implement geo-replication to a DR region, enforce data residency via Azure Policy and Purview, and ensure idempotent sinks with a tested failover process. What components and trade-offs would you choose, and how would you validate DR?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["IBM","MongoDB","NVIDIA"]},{"id":"q-1921","question":"In a multinational SaaS app where data residency is user-controlled, design an end-to-end Azure architecture that auto-scales with demand, minimizes latency across regions, and enforces per-tenant data isolation. Include data stores, messaging, compute, routing, security, DR, and a plan for idempotent processing with exact-once semantics?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Google","Netflix","Slack"]},{"id":"q-1983","question":"Design a beginner-level, end-to-end global web API deployment using two Azure regions. The API serves mobile clients worldwide, requires low latency, automatic regional failover, and basic security. Which Azure services would you use (Front Door, Traffic Manager, App Service, WAF), and outline the minimal wiring: two regional API endpoints, Front Door with a backend pool and health probes, WAF policy, and DNS configuration?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Citadel","MongoDB","Tesla"]},{"id":"q-859","question":"You have a REST API hosted on Azure App Service that processes user uploads and stores them in a separate Azure Blob Storage account. To avoid embedding secrets, how would you securely grant the API read/write access to the blob container using a managed identity? Outline the exact steps and roles you would apply, and mention any network considerations?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Amazon","MongoDB","Two Sigma"]},{"id":"q-865","question":"An IoT platform deployed in Azure collects about 5 million device events per minute from global regions. You must build an ingestion and processing pipeline that guarantees at-least-once delivery with idempotent writes to Cosmos DB, tolerates regional outages, and minimizes cost. Describe the end-to-end architecture, data flow, dedup strategy, and monitoring?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Discord","Scale Ai"]},{"id":"q-932","question":"Scenario: a multi-tenant SaaS app runs on Azure App Service and stores per-tenant files in separate containers in Azure Blob Storage. To avoid hard-coded keys, describe a secure pattern using managed identities and RBAC to grant the app the correct container access while ensuring tenant isolation and minimal permission surface. Outline steps, roles, and any network considerations (e.g., Private Endpoint)?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["DoorDash","Oracle"]},{"id":"q-984","question":"In a globally distributed API using Azure Functions and Cosmos DB, implement per-tenant data isolation with minimal cross-region data transfer. How would you design the architecture, enforce least-privilege access via managed identities and RBAC, and ensure data residency with Private Endpoints and cross-region replication policies?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Bloomberg","MongoDB","Square"]},{"id":"q-1322","question":"Scenario: You operate a multi-tenant Azure landing zone with AKS clusters and storage across three tenants. Implement scalable tenant isolation and policy-driven security without per-VM NSG churn, while enabling selective cross-tenant analytics via Private Link. Propose a concrete design using Azure Firewall Manager, Private Endpoints, managed identities, and Azure Policy, plus a scale-test plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Google","Snap"]},{"id":"q-1358","question":"In a three-tenant Azure deployment, implement Just-In-Time privileged access for security admins across tenants using Lighthouse. Outline how you would provision roles, enforce time-bound activations, integrate approval workflows, and ensure continuous auditing and cross-tenant access reviews. Include a concrete rollout plan and rollback safety checks?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Apple","DoorDash","Oracle"]},{"id":"q-1411","question":"In a three-tenant Azure deployment hosting critical workloads (AKS and serverless), design a cross-tenant threat-hunting workflow that federates Defender for Cloud and Azure Sentinel telemetry into a shared analyst workspace using Lighthouse. Specify data schema, access controls, data residency, rollback steps if ingestion fails, rollout milestones, and success criteria?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Cloudflare","Lyft","Uber"]},{"id":"q-1421","question":"Design an end-to-end security model for a serverless data ingestion pipeline in Azure: per-tenant telemetry from IoT devices arrives at IoT Hub, then lands in a Gen2 Data Lake with per-tenant partitions. Outline tenant isolation, identity (RBAC, PIM), data encryption at rest (CMK), network controls (Private Link), and auditing. Include rollout steps and rollback checks?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Coinbase","LinkedIn","Meta"]},{"id":"q-1496","question":"In a three-tenant Azure deployment hosting microservices in AKS and tenant storage, design a scalable zero-trust inter-service access model that avoids NSG churn on every VM. Use Azure AD workload identity, Istio mTLS, Private Endpoints, and ABAC with policy-driven access. Explain how you would enforce least privilege, audit access, and plan rollout with rollback steps?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Amazon","Snap"]},{"id":"q-1538","question":"Scenario: in a three-tenant Azure deployment with shared automation pipelines and cross-tenant data access, design a scalable, auditable model to grant and revoke time-bound automation access without leaking credentials. Leverage Lighthouse as a central RBAC anchor, Azure AD app roles, PIM for Just-In-Time elevation, ABAC with claims, and CI/CD policy gates. Include rollout, rollback, and telemetry strategy?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Microsoft","Netflix"]},{"id":"q-1582","question":"Scenario: A single Azure subscription hosts an App Service and an Azure SQL Database. Harden security with Defender for Cloud, Private Endpoints for Storage, RBAC via two Azure AD groups (Developers and Admins), and basic access reviews and alerts. Provide concrete steps, a minimal RBAC mapping, and a rollout plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["IBM","Instacart"]},{"id":"q-1616","question":"In a tri-tenant Azure deployment (tenants A, B, C) hosting microservices in AKS and serverless functions, design a cross-tenant secret management strategy using Azure Key Vault. Include per-tenant vaults plus a central SharedKV, implement 90‑day secret rotations with automatic versioning, enforce cross-tenant access via Azure AD B2B and access reviews, and specify auditing, rollout, and rollback steps?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Google","Microsoft","Oracle"]},{"id":"q-1647","question":"In a two-tenant Azure deployment, Tenant Alpha hosts a data lake (ADLS Gen2) and Synapse pipelines; Tenant Beta hosts identity and apps. Design a concrete, auditable cross-tenant threat containment strategy to prevent data exfiltration and lateral movement during peak load. Include per-tenant Private Endpoints, Conditional Access, cross-tenant RBAC, Defender for Cloud alerts, Key Vault key rotation (90 days), and an automated rollback plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["IBM","Oracle","Twitter"]},{"id":"q-1656","question":"In a single Azure subscription with dev/test/prod resource groups, a CI/CD pipeline currently uses secrets in code. Propose a beginner-friendly, concrete strategy to manage secrets and identities using Azure Key Vault, managed identities, and least-privilege RBAC. Include steps for retrieval at deploy time, rotation readiness, and rollback checks?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Adobe","LinkedIn","Meta"]},{"id":"q-1689","question":"In a beginner Azure Security Engineer scenario, you manage a single subscription hosting an App Service and a SQL Database. Outline a concrete, auditable process to grant a developer time-limited access to both resources using Azure RBAC and Privileged Identity Management (PIM). Include onboarding, role provisioning, activation policy, approval workflow, logging, and a rollback plan if access is no longer needed?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Apple","Cloudflare","DoorDash"]},{"id":"q-1726","question":"In a three-tenant Azure deployment hosting microservices in AKS across tenants A, B, and C, design a scalable, zero-trust API security model where a central API gateway (in Tenant A) issues short-lived access tokens for per-tenant services, using OAuth2/OIDC with Azure AD, cross-tenant service principals, and mTLS between gateway and services. Include token lifecycle, per-tenant scopes, automatic revocation, cross-tenant auditing with Azure Monitor/Sentinel, and a rollback plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Databricks","Salesforce","Uber"]},{"id":"q-1773","question":"In a two-tenant Azure deployment hosting analytics workloads across TenantA and TenantB (ADLS Gen2 and Synapse), design a zero-trust data access model with per-tenant data boundaries and a central broker in TenantA. Explain how you would implement RBAC and CMK, cross-tenant Data Share approvals, managed identities, Private Endpoints, and centralized monitoring with Sentinel/ Defender, plus automated access reviews and anomaly alerts. Include rollout and rollback steps?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Airbnb","Citadel"]},{"id":"q-1857","question":"In a three-tenant Azure landing zone, implement centralized drift detection for security baselines across tenants using Azure Policy, Azure Monitor, and Azure Arc-enabled resources. Propose per-tenant policy scopes, remediation tasks, cross-tenant alerting, and a rollback plan including safe rollback for policy changes and resource remediation. Include a concrete rollout plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Microsoft","Tesla"]},{"id":"q-1931","question":"In a tri-tenant Azure landing zone hosting workloads on AKS and App Services, design a cross-tenant incident response workflow leveraging Azure Sentinel, Azure Lighthouse, and Defender for Cloud. Include roles, cross-tenant playbooks, containment steps, and rollback tests; specify automated evidence collection, cross-tenant alert correlation, and post-incident reporting. Outline rollout plan and rollback safety checks?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Netflix","Snap","Twitter"]},{"id":"q-2074","question":"In a two-subscription Azure environment (AppInfra and DataInfra) with CI/CD pipelines deploying to both, how would you implement service principal credential hygiene to prevent credential leakage? Outline steps to (1) create a dedicated SPN with least privilege, (2) rotate credentials automatically, (3) enforce ephemeral credentials for pipelines via Azure DevOps service connections with Managed Identity, (4) implement auditing via Defender for Cloud and Azure AD sign-in logs, and (5) a rollback plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Databricks","NVIDIA","Tesla"]},{"id":"q-2111","question":"In a tri-tenant Azure deployment (Tenants A, B, C) hosting AKS and serverless services, design a cross-tenant software supply chain security workflow that enforces artifact signing, image provenance, and runtime integrity. Specify the integration points across Azure DevOps, Azure Container Registry with content trust, Azure Policy, and Lighthouse for cross-tenant approvals; include per-tenant gates, rollback safety checks, and a concrete rollout plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Instacart","Lyft","Tesla"]},{"id":"q-898","question":"In a multi-cluster AKS deployment with rapid scale-out, design a workload-based segmentation pattern that avoids per-VM NSGs. Use Kubernetes NetworkPolicy/Calico, central allowlists, and Azure Policy for drift remediation. Include how to enforce per-service identity, automate policy checks, and detect violations with Defender for Cloud?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Bloomberg","Google","NVIDIA"]},{"id":"q-925","question":"In a multi-subscription Azure deployment with ephemeral, autoscaled workloads across AKS and VMs, how would you achieve scalable, workload-oriented segmentation without updating NSG rules on every VM? Propose a concrete design using Azure Firewall Manager, managed identities, Private Link, and policy-driven groupings, plus a plan to test at scale?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Discord","Microsoft"]},{"id":"q-968","question":"In a global Azure deployment with microservices spread across AKS clusters and VM Scale Sets, design a workload-based segmentation that avoids touching NSG rules on every VM. Outline a concrete end-to-end approach using Azure Firewall Manager, policy-based groups, Private Endpoints, workload tags, and Managed Identities. Include deployment steps, scale testing, and violation monitoring?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["MongoDB","Snap","Uber"]},{"id":"q-1046","question":"You're building a regulated, multi-tenant analytics platform on Azure that ingests IoT and application logs from customers across three continents. Customers demand regional data residency while analytics must be global for cross-tenant benchmarks. Propose a practical, cost-conscious architecture that enforces per-tenant data isolation (at rest and in transit), regional ingestion, geo-redundant storage, cross-region analytics, and auditable access control using Azure native services. Include data plane vs control plane separation, and show how you'd satisfy RPO/RTO targets and regulatory requirements?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Databricks","Google"]},{"id":"q-1181","question":"You operate a fintech SaaS platform serving tenants across US, EU, and APAC. Each tenant's data must reside regionally at rest, yet global analytics require anonymized cross-tenant insights. Describe an Azure-native architecture that (1) enforces per-tenant data isolation in storage and processing, (2) supports real-time ingestion of fraud/transaction events, (3) enables cross-region analytics without tenant leakage, (4) meets DR targets with RPO <15 minutes and RTO <5 minutes, and (5) provides end-to-end auditing and governance. Include components, data flows, trade-offs, and a concrete failover test plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Plaid","Snap"]},{"id":"q-1305","question":"You manage a global healthcare analytics platform on Azure. Regulations require that **PHI** stays in-country while **non-PHI** can aggregate regionally. Propose an end-to-end data pipeline using **Azure Data Lake Storage Gen2**, **Data Factory**/Synapse, and **Purview** to enforce residency, enable regional analytics, and provide auditable data lineage and masking. Include encryption, governance, and failover strategies across regions?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Google","Instacart","OpenAI"]},{"id":"q-1365","question":"Design an Azure-based, EU-resident real-time trading analytics pipeline for a regulated fintech platform that must achieve sub-100 ms end-to-end latency, robust multi-region DR, and strict auditability. Outline the services, data flows, data residency, encryption (BYOK), governance, and cost controls you would implement?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Coinbase","Netflix","Robinhood"]},{"id":"q-1391","question":"You are tasked with building a EU-resident, real-time analytics platform with strict data residency and disaster recovery. Ingest events via Azure Event Hubs in the EU, land in a Data Lake Gen2 in the EU, process with Azure Synapse and Azure Databricks, and expose global analytics through external tables or Synapse Link. Include governance with Purview, BYOK via Key Vault, and private connectivity via Private Link/ExpressRoute. Design DR: RPO <5m, RTO <15m, and cost controls; outline testing plan (blue/green, chaos) and tradeoffs?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Hugging Face","Slack","Square"]},{"id":"q-1432","question":"For a new EU-resident SaaS app serving multiple tenants, you choose data isolation in Azure SQL Database. Compare using a single database with Row-Level Security (RLS) vs separate contained databases per tenant, focusing on cost, governance, backup/restore, and scale. Propose a concrete decision and outline the basic migration steps, including how you’d implement BYOK with Key Vault for per-tenant encryption and Azure AD authentication?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Google","Instacart"]},{"id":"q-1477","question":"For a EU-resident, multi-tenant SaaS app deployed in Azure, data residency requires tenant data to remain in EU while global analytics runs from a separate region. Design an end-to-end architecture that enforces per-tenant residency, enables cross-geo analytics, uses Arc-enabled data services, Cosmos DB, Synapse, and Purview, implements BYOK and private connectivity, and achieves DR with RPO<5m and RTO<15m. Include data flows, governance model, and testing plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Salesforce","Tesla"]},{"id":"q-1541","question":"Design a two-region, Azure-native DR for a beginner-friendly SaaS API with EU residency; propose a minimal architecture using Azure Front Door, App Service, and Azure SQL with geo-replication to achieve sub-minute RTO and RPO under 15 minutes; outline the failover process and a practical testing plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Hugging Face","Uber"]},{"id":"q-1642","question":"Design a cost-conscious EU-resident SaaS API hosted in Azure: propose a minimal architecture using Azure App Service, Azure SQL Database, and a public endpoint with autoscale and a regional failover to a secondary Azure region; detail traffic routing, RTO/RPO targets, and a practical testing plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Apple","Google","Netflix"]},{"id":"q-1652","question":"Design a beginner-friendly, Azure-native, event-driven ingestion path for a multi-tenant SaaS that streams user actions to analytics. Use a single Azure Event Hub, a Function with Event Hub trigger, and ADLS Gen2 as the sink. Include idempotent processing, dedup, backoff retries, and a simple tenant-scoped data schema. Outline validation steps to prove end-to-end latency under 2 minutes and zero data loss?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["LinkedIn","Oracle","Salesforce"]},{"id":"q-1696","question":"You’re designing a EU-resident, multi-tenant analytics platform that ingests in near real-time and serves tenant-isolated dashboards. Propose a cost-conscious Azure-based lakehouse architecture using ADLS Gen2, Event Hubs, Data Factory, and Synapse, detailing data isolation, per-tenant encryption with BYOK in Key Vault, and Azure AD-based access control; include a pragmatic migration path from a single-tenant baseline?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Adobe","Robinhood","Salesforce"]},{"id":"q-1730","question":"As the Azure Solutions Architect for a Zoom-scale platform, design an Azure-native, EU-resident data pipeline that ingests telemetry from MongoDB Atlas (Change Streams), streams it with minimal data loss, processes it in near real-time, and serves dashboards without EU data leaving the region. Outline data flow, services, DR, encryption (BYOK), and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["MongoDB","Zoom"]},{"id":"q-1781","question":"Design a beginner-friendly EU-resident telemetry pipeline for a gaming platform: ingest per-session data via Azure Event Hubs, process with Functions, store per-tenant data in Cosmos DB with Row-Level Security, and surface per-tenant dashboards in Power BI. Ensure data stays in the EU, implement BYOK, and outline DR and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Google","NVIDIA"]},{"id":"q-1797","question":"Design an Azure-native, EU-resident, cross-tenant data platform that ingests telemetry from an on-prem gateway fleet into Azure, processes it in near real-time while guaranteeing data sovereignty (EU only), uses BYOK with Key Vault, and supports tenant-level dashboards with RBAC. Outline data model, services, DR, and cost controls?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Adobe","Apple"]},{"id":"q-1841","question":"Design a beginner-friendly Azure-native telemetry pipeline for a fleet of IoT devices used by a mobile app, with EU residency constraints. Ingest device telemetry via Azure IoT Hub, preprocess at the edge with IoT Edge (sampling and filtering), route to Azure Functions for enrichment, and store per-tenant aggregates in Azure SQL with Row-Level Security. Expose dashboards in Power BI; include BYOK with Key Vault, DR planning, and cost levers; keep data in the EU region?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Airbnb","MongoDB","Robinhood"]},{"id":"q-1860","question":"Design an EU-resident Azure-native architecture for a real-time streaming recommendations engine serving EU users; telemetry is generated globally and must be processed entirely within the EU with no data leaving the region. Propose ingestion, real-time processing (sub-second latency), state storage, and serving path, tenant isolation, BYOK with Key Vault, private endpoints, and a regional DR plan with automated failover. Include concrete services and trade-offs?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["MongoDB","Netflix","PayPal"]},{"id":"q-1919","question":"EU-resident, regulated fintech SaaS: design a fully Azure-native, event-driven analytics platform that ingests on-prem transaction streams from a gateway into EU-region data plane; ensure tenants are isolated, data never leaves the EU, with BYOK in Key Vault, and automatic regional failover to a secondary EU region. Choose services (Event Hubs, Functions/Stream Analytics, Cosmos DB multi-tenant with per-tenant containers or databases, ADLS Gen2, Synapse), network controls (Private Link, VNets), security, cost levers, and migration steps. Provide data flow, DR plan, testing plan, and governance considerations?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Goldman Sachs","Hugging Face","Meta"]},{"id":"q-1943","question":"You're building an EU-resident telemetry observability layer for a real-time game platform. Telemetry ingested through EU-based Event Hubs is processed by Functions and stored per-tenant in Cosmos DB; dashboards display in Power BI. Propose an end-to-end observability design that enables per-tenant debugging without exporting data outside the EU. Include logs, tracing, retention, and testing?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["NVIDIA","Plaid"]},{"id":"q-1947","question":"Scenario: design a beginner-friendly **EU-resident** analytics pipeline for an Instacart-like app focusing on **support tickets** and **in-app events**. Ingest via **Azure Event Hubs** in the EU, enrich with **Functions**, store per-tenant metrics in **Cosmos DB** with **RLS**, and visualize in **Power BI**. Keep data EU-only, enforce **BYOK** via **Key Vault**, propose a 30-day retention and a purge workflow, plus a minimal **DR** plan and cost levers. Provide end-to-end data path and governance touches?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Instacart","Lyft"]},{"id":"q-2035","question":"You're building a EU-resident, multi-tenant fintech SaaS with strict data isolation. Propose a 2-region EU Azure-native data fabric that keeps each tenant's data isolated, supports real-time analytics and ML scoring, and enforces BYOK. Include data flow, services, governance, DR, and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Google","Hashicorp","Robinhood"]},{"id":"q-887","question":"You’re building a multi-tenant analytics platform on Azure for a consumer-brand SaaS product. Each tenant must have isolated data processing, with per-tenant data lake isolation, on-demand Spark/notebook compute that auto-suspends, and cost governance at the tenant level. Propose an architecture using Azure Data Lake Storage Gen2, Unity Catalog or RBAC, Synapse or Databricks, private endpoints, and auditing. How do you ensure data isolation, prevent cross-tenant leakage, and meet compliance while keeping ops simple?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Instacart","Microsoft","Snap"]},{"id":"q-606","question":"How would you implement a rate limiter for a REST API to prevent abuse while ensuring legitimate users aren't blocked? Describe the algorithm and data structures you would use.","channel":"backend","subChannel":"api-design","difficulty":"intermediate","tags":["rate-limiting","api-design","redis","distributed-systems","backend"],"companies":["Google","Amazon","Twitter","Stripe","GitHub"]},{"id":"q-614","question":"How would you implement API rate limiting for a high-traffic service that needs to handle millions of requests per minute? Discuss the trade-offs between different algorithms and your approach for distributed systems.","channel":"backend","subChannel":"api-gateway","difficulty":"intermediate","tags":["rate-limiting","api-design","distributed-systems","redis","token-bucket","scalability"],"companies":["Google","Meta","Twitter","Stripe","Amazon","Netflix"]},{"id":"q-624","question":"How would you implement API rate limiting in a distributed system to prevent abuse while ensuring fair usage across multiple servers?","channel":"backend","subChannel":"api-infrastructure","difficulty":"intermediate","tags":["rate-limiting","redis","distributed-systems","api-design","scalability"],"companies":["Stripe","Twitter","GitHub","Google","Amazon"]},{"id":"q-611","question":"How would you implement API rate limiting to prevent abuse while ensuring fair usage for legitimate clients?","channel":"backend","subChannel":"api-middleware","difficulty":"intermediate","tags":["rate-limiting","api-design","middleware","redis","security"],"companies":["Twitter","GitHub","Stripe","Google","Amazon"]},{"id":"gh-46","question":"How would you design comprehensive API documentation that ensures smooth developer integration and reduces support overhead?","channel":"backend","subChannel":"apis","difficulty":"beginner","tags":["api","service-mesh"],"companies":["GitHub","LinkedIn","Microsoft","Postman","Stripe"]},{"id":"q-267","question":"Compare REST, GraphQL, and gRPC performance characteristics and identify optimal use cases for each protocol in modern microservices architecture?","channel":"backend","subChannel":"apis","difficulty":"beginner","tags":["rest","graphql","grpc","openapi"],"companies":["Amazon","Google","Microsoft","Netflix","Square","Stripe"]},{"id":"q-396","question":"You're building a microservice that needs to expose both REST and GraphQL endpoints for the same data model. How would you design the architecture to avoid code duplication while maintaining optimal performance for each query type?","channel":"backend","subChannel":"apis","difficulty":"intermediate","tags":["rest","graphql","grpc","openapi"],"companies":["Amazon","Booking.com","Citadel"]},{"id":"q-515","question":"You're building a REST API for a payment service. How would you design the endpoint for processing a payment, and what HTTP status codes would you return for different scenarios?","channel":"backend","subChannel":"apis","difficulty":"beginner","tags":["rest","graphql","grpc","openapi"],"companies":["PayPal","Twitter"]},{"id":"q-539","question":"What is dependency injection in Spring and how does it improve application design?","channel":"backend","subChannel":"apis","difficulty":"intermediate","tags":["spring","dependency-injection","ioc","design-patterns","java"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-1196","question":"In a production backend with multiple IdPs (OIDC providers and a SAML bridge), design a token validation strategy to prevent replay and bind tokens to a device/session. Outline how you would implement: (a) JWKS caching and per-provider key rotation, (b) replay protection using jti stored in a distributed cache with TTL, (c) token binding via mTLS or client certificate binding, and (d) cross-provider revocation propagation and token lifecycle (short-lived access tokens with refresh tokens)?","channel":"backend","subChannel":"authentication","difficulty":"intermediate","tags":["jwt","oauth2","oidc","saml"],"companies":["Goldman Sachs","Hashicorp","Twitter"]},{"id":"q-1240","question":"Within an enterprise multi-IdP setup (OIDC and SAML), you run an API gateway that issues short-lived JWTs for a service mesh. Propose a concrete bridge design that: (a) supports converting SAML assertions and OIDC tokens into token-bound JWTs with audience and scope constraints, (b) binds tokens to a device fingerprint and a one-time nonce, (c) supports PKCE-backed mobile/native flows and refresh token rotation, (d) provides revocation and token introspection across regions, and (e) prevents token replay in a globally distributed environment. Explain data flows, token formats, and security checks?","channel":"backend","subChannel":"authentication","difficulty":"advanced","tags":["jwt","oauth2","oidc","saml"],"companies":["Anthropic","MongoDB"]},{"id":"q-342","question":"You're implementing OAuth2 for a SaaS product. A user reports their access token works but refresh token fails. What are the top 3 causes and how would you debug each?","channel":"backend","subChannel":"authentication","difficulty":"intermediate","tags":["jwt","oauth2","oidc","saml"],"companies":["Cohere","Hulu","Spotify"]},{"id":"q-455","question":"Design a secure authentication system for a microservices architecture that supports JWT, OAuth2, and SAML. How would you handle token rotation, session management, and prevent token replay attacks across multiple services?","channel":"backend","subChannel":"authentication","difficulty":"advanced","tags":["jwt","oauth2","oidc","saml"],"companies":["OpenAI","Twitter"]},{"id":"q-544","question":"You're implementing SSO for an enterprise application using SAML 2.0. The IdP sends signed assertions but you're seeing intermittent 'Invalid Signature' errors. What are the most common causes and how would you debug them?","channel":"backend","subChannel":"authentication","difficulty":"intermediate","tags":["jwt","oauth2","oidc","saml"],"companies":["Adobe","Hashicorp"]},{"id":"q-1340","question":"How would you implement a tiered rate limiting system that provides different limits for free, premium, and enterprise customers while preventing users from bypassing limits by creating multiple accounts?","channel":"backend","subChannel":"backend","difficulty":"intermediate","tags":["rate-limiting","authentication","abuse-prevention","distributed-systems"],"companies":[]},{"id":"q-1116","question":"You're building a highly cached backend for a social app. **Redis** stores user profiles and feeds; **Memcached** caches post details. On a user profile edit, describe a precise, scalable strategy for **cache invalidation** that prevents stampedes, maintains consistency, and minimizes stale reads. Include data structures, TTLs, invalidation triggers, and atomic operations across **Redis** and **Memcached**, with concrete commands or pseudo-code?","channel":"backend","subChannel":"caching","difficulty":"advanced","tags":["redis","memcached","cache-invalidation"],"companies":["Apple","Google","Snap"]},{"id":"q-427","question":"You're building a user profile service that caches frequently accessed profiles. How would you implement cache invalidation when a user updates their profile, and what trade-offs would you consider between Redis and Memcached?","channel":"backend","subChannel":"caching","difficulty":"beginner","tags":["redis","memcached","cache-invalidation"],"companies":["Airbnb","Amazon","Google","Microsoft","Netflix","Snowflake","Stripe","Zoom"]},{"id":"q-443","question":"You're building a user profile API that caches user data in Redis. How would you implement cache invalidation when a user updates their profile, and what's the difference between using TTL vs explicit invalidation?","channel":"backend","subChannel":"caching","difficulty":"beginner","tags":["redis","memcached","cache-invalidation"],"companies":["Meta","MongoDB","NVIDIA"]},{"id":"q-330","question":"You're building a collaborative whiteboard app like Miro. When a user drags a shape, you need to update the UI immediately and persist the change. How would you implement this using CQRS?","channel":"backend","subChannel":"microservices","difficulty":"beginner","tags":["saga","cqrs","event-sourcing"],"companies":["Miro","Slack","Snowflake"]},{"id":"q-364","question":"You're building an order management system using CQRS with microservices architecture. How would you ensure data consistency between the write and read models when a command to create an order is processed, considering network partitions and potential service failures?","channel":"backend","subChannel":"microservices","difficulty":"beginner","tags":["saga","cqrs","event-sourcing"],"companies":null},{"id":"q-379","question":"You're building a distributed order processing system using the Saga pattern. How would you handle compensation when a payment service fails after inventory has been reserved?","channel":"backend","subChannel":"microservices","difficulty":"beginner","tags":["saga","cqrs","event-sourcing"],"companies":["Elastic","Epic Systems","Oscar Health"]},{"id":"q-666","question":"How would you implement a **saga**-driven checkout across services using **CQRS** and **event-sourcing**? Provide a concrete flow for an order touching Inventory, Payment, and Shipping: what commands and events you define, orchestration vs choreography, idempotency, compensating actions, and how read models are projected and kept consistent. Include reliability patterns like outbox and retries to ensure at-least-once delivery?","channel":"backend","subChannel":"microservices","difficulty":"intermediate","tags":["saga","cqrs","event-sourcing"],"companies":["DoorDash","OpenAI","Oracle"]},{"id":"q-667","question":"In a microservices backend for a retail platform, design a saga-driven workflow using CQRS and event sourcing across Order, Inventory, Payment, and Shipping. When an order is created, reserve inventory and authorize payment; on success, create shipping and complete the order. If inventory or payment fails, apply compensations (InventoryRelease, RefundPayment). Detail the event/command sequence, data in the event store, idempotency strategy, and orchestration vs choreography trade-offs?","channel":"backend","subChannel":"microservices","difficulty":"advanced","tags":["saga","cqrs","event-sourcing"],"companies":["Meta","Snowflake"]},{"id":"q-1095","question":"Design a globally distributed event store for a chat app where user_id determines the shard via consistent hashing. Each shard has 3 replicas in distinct regions; ingestion writes go to a leader replica and durably commit to all replicas using a 2-of-3 quorum. Reads are served from any replica with read-your-writes guarantees. Explain shard rebalancing without downtime, hot shard mitigation, cross-region replication lag, and failure recovery strategies?","channel":"backend","subChannel":"server-architecture","difficulty":"advanced","tags":["scaling","sharding","replication"],"companies":["Discord","MongoDB","Tesla"]},{"id":"q-249","question":"How would you implement a connection pool manager for aiohttp that handles graceful degradation under high load and connection timeouts?","channel":"backend","subChannel":"server-architecture","difficulty":"advanced","tags":["asyncio","aiohttp","concurrency"],"companies":["Airbnb","Amazon","Google","Meta","Microsoft","Netflix","Stripe","Uber"]},{"id":"q-485","question":"You're designing a distributed database for a fintech platform handling 10M transactions/day. How would you implement sharding and replication to ensure strong consistency while maintaining 99.99% availability?","channel":"backend","subChannel":"server-architecture","difficulty":"advanced","tags":["scaling","sharding","replication"],"companies":["Amazon","Coinbase","Plaid"]},{"id":"q-568","question":"How would you design a database schema for a user authentication system that needs to handle 1 million users with proper indexing and sharding considerations?","channel":"backend","subChannel":"server-architecture","difficulty":"beginner","tags":["scaling","sharding","replication"],"companies":["Citadel","LinkedIn","Tesla"]},{"id":"q-1648","question":"In a scenario with a two-week deadline for a critical feature, you mediate between a senior engineer pushing for scope expansion and a PM pressing to cut scope to meet the deadline. Describe your mediation steps, the decision framework you use, and how you communicate the final plan and follow-ups?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"advanced","tags":["negotiation","mediation","feedback"],"companies":["Bloomberg","Meta","Twitter"]},{"id":"q-185","question":"Describe a specific situation where you had to resolve a technical disagreement with a difficult team member. What conflict resolution techniques did you use, and what was the measurable outcome?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"intermediate","tags":["communication","collaboration"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-312","question":"Tell me about a time you had to negotiate a solution between two team members with conflicting approaches?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"beginner","tags":["negotiation","mediation","feedback"],"companies":["Amazon","Google","Meta"]},{"id":"q-326","question":"Tell me about a time you had a conflict with a team member. How did you handle it and what was the outcome?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"beginner","tags":["negotiation","mediation","feedback"],"companies":["Crowdstrike","Salesforce","Tesla"]},{"id":"q-439","question":"Tell me about a time you had to mediate a conflict between two senior engineers who disagreed on a critical technical approach for a high-stakes project with a tight deadline?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"advanced","tags":["negotiation","mediation","feedback"],"companies":["Amazon","Apple","Google","Instacart","Meta","Microsoft","Netflix"]},{"id":"q-446","question":"Tell me about a time you had a disagreement with a teammate about how to approach a project. How did you handle it?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"beginner","tags":["negotiation","mediation","feedback"],"companies":["IBM","OpenAI"]},{"id":"q-486","question":"Tell me about a time you had to mediate a conflict between two senior engineers with opposing technical approaches. How did you handle it?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"advanced","tags":["negotiation","mediation","feedback"],"companies":["Microsoft","OpenAI","Square"]},{"id":"q-1039","question":"Describe a real incident in a fintech app where a release caused a customer-visible outage during market hours. Outline the explicit ownership and the immediate bias-for-action decision (hotfix vs rollback) with severity criteria, and how to ensure customer-obsessed restitution (transparent status updates, potential compensation, postmortem, and preventive steps)?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"beginner","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Google","Robinhood"]},{"id":"q-1243","question":"You're inheriting a mission-critical data pipeline that powers dashboards for a global customer; a nightly ETL job is failing in one region. Describe exactly how you would take ownership, act with urgency, and prioritize fixes while keeping customers informed. What trade-offs would you make and how would you measure success?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"advanced","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Airbnb","NVIDIA","Snowflake"]},{"id":"q-1969","question":"How would you own a cross‑functional initiative to ship a high‑impact feature under a tight deadline when early signals favor a different direction than customer feedback suggests? Describe your approach to ownership, bias-for-action, and customer obsession, including decision criteria, quick experiments, and stakeholder communication?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"intermediate","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Coinbase","Databricks"]},{"id":"q-2063","question":"In a Netflix-like app, a low-severity but high-visibility startup latency bug is reported by many users. Describe a concrete, end-to-end plan showing ownership, bias-for-action, and customer obsession: triage steps, quick hotfix, monitored rollout, rollback criteria, and a plan for a permanent fix with clear metrics and communication?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"beginner","tags":["ownership","bias-for-action","customer-obsession"],"companies":["NVIDIA","Netflix"]},{"id":"q-431","question":"Tell me about a time when you noticed a small issue that others overlooked. How did you take ownership and what was the impact?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"beginner","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Hugging Face","IBM","Tesla"]},{"id":"q-516","question":"Tell me about a time you had to make a critical decision with incomplete data. How did you balance speed vs accuracy?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"intermediate","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Cloudflare","Coinbase","PayPal"]},{"id":"q-1273","question":"Describe a time you faced a technical disagreement in a cross-functional project and successfully influenced a decision without direct authority. What was the conflict, what steps did you take to communicate your view, what data or patterns supported you, and what was the outcome?","channel":"behavioral","subChannel":"soft-skills","difficulty":"beginner","tags":["communication","collaboration","influence"],"companies":["NVIDIA","Tesla"]},{"id":"q-1282","question":"Describe a time when influence changed a project's approach after data showed the initial plan underperformed. What data did you present, what communication channels did you use, how did you secure collaboration, and what was the outcome and learning?","channel":"behavioral","subChannel":"soft-skills","difficulty":"beginner","tags":["communication","collaboration","influence"],"companies":["Google","Tesla","Twitter"]},{"id":"q-2082","question":"Describe a time when a project failed due to miscommunication across cross-functional teams (engineering, product, and QA). What happened, how did you identify the root cause, what concrete steps did you take to restore alignment, and what would you do differently to prevent recurrence?","channel":"behavioral","subChannel":"soft-skills","difficulty":"beginner","tags":["communication","collaboration","influence"],"companies":["Citadel","Google","Oracle"]},{"id":"q-2154","question":"Describe a time when a project faced scope drift due to a miscommunication across teams. What steps were taken to clarify priorities, align stakeholders, and avoid future breakdowns? Include the channels you used, artifacts (like a decision log or a RACI matrix), and the outcome?","channel":"behavioral","subChannel":"soft-skills","difficulty":"beginner","tags":["communication","collaboration","influence"],"companies":["Google","IBM","Snap"]},{"id":"q-297","question":"Tell me about a time you had to influence a senior stakeholder who disagreed with your technical approach. How did you handle it?","channel":"behavioral","subChannel":"soft-skills","difficulty":"advanced","tags":["communication","collaboration","influence"],"companies":["Amazon","Google","Meta"]},{"id":"q-913","question":"Tell me about a time you persuaded a cross-functional team to adopt a non-obvious technical approach. What was the situation, how did you influence without direct authority, what data or experiments did you rely on, what trade-offs did you communicate, and what was the result?","channel":"behavioral","subChannel":"soft-skills","difficulty":"intermediate","tags":["communication","collaboration","influence"],"companies":["Bloomberg","Google","Salesforce"]},{"id":"q-212","question":"How would you structure a STAR method response when describing a time you resolved a technical conflict between frontend and backend teams over API design, and what specific communication strategies did you employ?","channel":"behavioral","subChannel":"star-method","difficulty":"beginner","tags":["situation","task","action","result"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Postman"]},{"id":"q-375","question":"Tell me about a time when you had to make a critical technical decision with incomplete data that impacted production systems. What was the situation, what data did you have, what decision did you make, and what was the result?","channel":"behavioral","subChannel":"star-method","difficulty":"advanced","tags":["situation","task","action","result"],"companies":["Amazon","Apple","Coinbase","Google","Meta","Microsoft","Netflix"]},{"id":"q-389","question":"Tell me about a time when you had to convince your team to adopt a new technology or approach that they were initially resistant to. What was the situation, what did you do, and what was the outcome?","channel":"behavioral","subChannel":"star-method","difficulty":"intermediate","tags":["situation","task","action","result"],"companies":["Expedia","IBM","Shopify"]},{"id":"q-569","question":"Tell me about a time you had to make a difficult technical decision with incomplete information under extreme pressure?","channel":"behavioral","subChannel":"star-method","difficulty":"advanced","tags":["situation","task","action","result"],"companies":["Citadel","Netflix","Tesla"]},{"id":"q-1073","question":"Implement transpose64(uint64_t x) by treating x as an 8x8 bit matrix in row-major order (bits 0-7 are row 0, 8-15 row 1, etc.). Return the transposed matrix (bit (r,c) moves to (c,r)). Use only bitwise ops and shifts; no loops or conditionals. Provide signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Cloudflare","Coinbase","Hashicorp"]},{"id":"q-1216","question":"Implement reverse128 for a 128-bit value stored as two 64-bit halves hi and lo. Provide a function:\n\nvoid reverse128(uint64_t hi, uint64_t lo, uint64_t *out_hi, uint64_t *out_lo);\n\nThe reversal should map bit i to bit 127 - i, using only bitwise operations and shifts (no loops or conditionals). Include a brief justification and 1-2 quick test examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Google","Snowflake","Stripe"]},{"id":"q-2040","question":"Implement countTrailingZeros64(uint64_t x) using only bitwise operations and shifts (no loops or conditionals). Return 64 if x == 0. Use a De Bruijn sequence with a 64-entry lookup table. Provide the function signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Oracle","Twitter","Uber"]},{"id":"q-680","question":"Given a 32-bit unsigned int n, implement a function hasAdjacentOnes(n) that returns true if n contains any two consecutive 1 bits (for example 0b1100100 has adjacent ones). Use only bitwise operations, no loops or lookups. Explain the core trick in a sentence?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Cloudflare","Oracle","Two Sigma"]},{"id":"q-689","question":"Given a 32-bit unsigned integer n, implement a function isSingleEvenBitSet(n) that returns true if exactly one bit is set and that bit lies at an even index (0, 2, 4, ...). Use only bitwise operations, no loops or built-in helpers. Provide the expression and a brief justification, plus a couple of quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["LinkedIn","MongoDB","Snowflake"]},{"id":"q-698","question":"Implement a 32-bit unsigned integer function popcount32(n) that returns the number of set bits in n using only bitwise operations and shifts, with no loops or built-ins. Use the SWAR (SIMD Within A Register) technique: apply a sequence of masks and shifts (0x55555555, 0x33333333, 0x0F0F0F0F) and a final multiply to consolidate counts. Provide the function and a brief justification, plus a couple of quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Google","Two Sigma"]},{"id":"q-705","question":"Implement reverseBits32(n) that reverses all 32 bits in a 32-bit unsigned integer using only a fixed sequence of bitwise operations (no loops or conditionals). Use a SWAR-style approach with masks 0x55555555, 0x33333333, 0x0F0F0F0F and 0x00FF00FF, finishing with a 16-bit half-swap. Provide the function signature and a brief justification, plus a couple of quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Coinbase","Snap"]},{"id":"q-711","question":"In a network packet parser, a 32-bit field uses trailing-zero count to encode the length of a value. Given a 32-bit unsigned n, implement countTrailingZeros32(n) that returns the number of trailing zero bits (0-32). If n==0, return 32. Use only bitwise operations, shifts, and basic arithmetic, no loops or built-ins. Provide signature, brief justification, and 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Amazon","Google","Uber"]},{"id":"q-724","question":"In memory allocation, implement nextPowerOfTwo32(uint32_t n) that returns the smallest 32-bit unsigned power-of-two >= n, given n > 0, using only bitwise operations with no loops or conditionals. If the result would overflow 32 bits, return 0. What is the correct implementation signature and approach?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Apple","IBM","Uber"]},{"id":"q-730","question":"Given a 32-bit unsigned integer n, implement swapAdjacentBits32(n) that returns a new 32-bit value with every adjacent bit pair swapped (bits 0-1, 2-3, ..., 30-31). Use only bitwise operations, no loops or conditionals. Provide the function signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Amazon","Plaid","Twitter"]},{"id":"q-738","question":"Implement a 64-bit bit reversal function reverse64(uint64_t n) that returns the bitwise reversal of n (bit 0 becomes bit 63, bit 63 becomes bit 0). Use only bitwise operations and shifts, no loops or conditionals. Provide the function signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Lyft","MongoDB","PayPal"]},{"id":"q-747","question":"Implement isBinaryPalindrome32(uint32_t n) that returns 1 if the 32-bit binary representation of n is a palindrome (bit0 equals bit31, bit1 equals bit30, etc.), and 0 otherwise. Use only bitwise operations and shifts; no loops or conditional branches. Provide the function signature and a brief justification, plus 1-2 quick examples.\n\nExamples:\n- 0x80000001 is a palindrome\n- 0xA5A5A5A5 is not a palindrome?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Coinbase","Google","Netflix"]},{"id":"q-751","question":"Implement popcount64(uint64_t x) that returns the number of set bits in x using only bitwise operations and shifts, with no loops and no built-in popcount. Use a fixed SWAR approach with masks 0x5555555555555555ULL, 0x3333333333333333ULL, 0x0F0F0F0F0F0F0F0FULL and a final multiply/shift step. Provide the function signature and a brief justification, plus 1-2 quick examples of inputs and outputs?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Lyft","MongoDB","Snap"]},{"id":"q-757","question":"Given a 32-bit unsigned integer n, implement maskBelowLSB32(n) that returns a 32-bit mask with all bits at positions <= the least significant set bit of n turned on; if n is 0 return 0. Use bitwise operations and shifts (and optional arithmetic). Provide the signature, justification, and 1-2 examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Meta","Microsoft","Zoom"]},{"id":"q-776","question":"Implement a 64-bit bitboard function knightAttacks64(n) that returns a 64-bit mask of all squares attacked by knights, given a 64-bit bitboard n where 1s indicate knight positions. Use only bitwise operations and shifts, no loops or conditionals. Provide the signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Bloomberg","Netflix","Plaid"]},{"id":"q-783","question":"Given a 32-bit unsigned n, implement hasPattern101(n) that returns true if there exists any i such that bits i, i+1, i+2 form 101 (n_i=1, n_{i+1}=0, n_{i+2}=1). Use only bitwise operations and shifts, no loops or conditionals. Provide the function signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Apple","Plaid","Slack"]},{"id":"q-791","question":"Implement parity32(n) that returns 1 if the number of set bits in a 32-bit unsigned n is odd, otherwise 0. Do not use loops or built-ins; only bitwise ops and shifts. Provide function signature parity32(uint32_t n) and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Databricks","DoorDash"]},{"id":"q-798","question":"Implement nextHigherWithSamePopcount64(uint64_t n) that returns the smallest integer greater than n with the same number of 1-bits. Use only bitwise operations and shifts; no loops or conditionals. If no such number exists, return 0. Provide the function signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Google","Two Sigma"]},{"id":"q-807","question":"Implement rotateLeft128 by k on a 128-bit value stored as two 64-bit words (hi, lo). The function rotates within the 128-bit boundary by k bits (0 <= k < 128) using only bitwise operations and shifts, no loops or conditionals. Provide the signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Google","Meta","Twitter"]},{"id":"q-814","question":"Implement interleave16(uint16_t a, uint16_t b) that returns a 32-bit value with bits interleaved as a0 b0 a1 b1 ... a15 b15, where a0 is LSB of a and b0 is LSB of b. Use only bitwise operations and shifts (no loops or conditionals). Provide the function signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Adobe","Snowflake","Zoom"]},{"id":"q-823","question":"Implement rotateLeft32(uint32_t n, unsigned int k) that returns the 32-bit value formed by rotating n left by k bits. Constraints: no loops or conditionals; handle k >= 32 by using k % 32. Provide the function signature and a brief justification, and give 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Citadel","LinkedIn","Stripe"]},{"id":"q-830","question":"Implement xor32(uint32_t a, uint32_t b) that returns a ^ b without using the ^ operator. Use only &, |, ~ and shifts. Provide the function signature and a brief justification, plus 1-2 quick examples to verify correctness?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Instacart","Netflix"]},{"id":"q-1091","question":"Scenario: An OTA firmware update causes GPS altitude drift in a subset of IoT devices across regions. Design a CAPA program to detect, collect evidence, and prevent recurrence. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API for CAPAs with device logs, 4) region/device-type metrics (recurrence rate, MTTR, false positives), 5) an RCA template and a canary rollback plan for OTA updates?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Plaid","Snowflake","Tesla"]},{"id":"q-1111","question":"Scenario: Production ML feature store drift after a data refresh degrades latency and CTR across regions due to stale features and late streaming data. Propose a CAPA program: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPAs, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region-aware metrics to prove containment and recurrence, 5) an RCA template and a canary rollout plan to prevent recurrence during future refreshes?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["LinkedIn","Tesla"]},{"id":"q-1132","question":"Scenario: batch ingestion misses PII masking in two tenants across regions, raising privacy risk. Design a CAPA program: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) tenant/region-aware metrics to prove containment and recurrence (MTTD, MTTR, false positives), 5) an RCA template and a canary rollout plan to validate policy enforcement before global deployment?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Coinbase","MongoDB","Salesforce"]},{"id":"q-1156","question":"Scenario: A distributed streaming analytics pipeline processes click events for an online marketplace. A recently deployed shard rebalancing causes out-of-order events in two regions, leading to incorrect revenue attribution and fraud alerts. Design a CAPA program that covers: 1) a CAPA data model that captures evidence (events, traces, timestamps, orderings) and artifacts (config, manifest, canary results), 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces and event metadata, 4) region- and shard-aware metrics to prove containment (recurrence rate, MTTR, misattribution rate), 5) an RCA template and a canary-rollback plan for shard rebalancing?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Amazon","Apple"]},{"id":"q-1200","question":"Scenario: A global real-time telemetry platform for autonomous vehicles experiences intermittent PII exposure due to a log-redaction misconfiguration after a software update in two regions. Design a CAPA program to detect, document, and prevent recurrence. Include: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) tenant- and region-aware metrics (recurrence rate, MTTR, false positives), 5) an RCA template and a canary-based rollback/patch plan, 6) a policy gate preventing deployment until redact coverage is above threshold?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["MongoDB","Tesla"]},{"id":"q-1426","question":"Scenario: A multi-tenant SaaS billing system release unintentionally changes pricing rules for a subset of tenants, triggering incorrect invoices and revenue churn. Design a beginner CAPA program to address this. Your task: 1) propose a CAPA data model capturing evidence (invoices, logs, traces) and artifacts (config, feature flags, pricing rules); 2) define a lifecycle state machine for CAPA progression; 3) outline a minimal REST API to create/update CAPAs with linked billing events; 4) specify tenant- and plan-level metrics to prove containment (recurrence rate, MTTR, false positives); 5) provide an RCA template and a canary-based patch plan before full rollout; 6) draft a simple policy gate that prevents deployment until pricing rules pass a preflight check?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Hashicorp","Zoom"]},{"id":"q-1450","question":"Scenario: A global multi-tenant data platform implements a new consent-logging feature. After rollout, a bug in the consent service causes PII redaction failures in two regions, risking data exposure and compliance violations. Design a CAPA program to detect, collect evidence, and prevent recurrence. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region- and tenant-aware metrics (recurrence rate, MTTR, false positives), 5) an RCA template and a canary rollout plan to validate fixes before global deployment?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Citadel","LinkedIn","Stripe"]},{"id":"q-1485","question":"**Scenario**: A cross-tenant data export feature in a multi-tenant SaaS app accidentally exposes data from unrelated tenants during a canary rollout in two regions. Design a beginner **CAPA** program to detect, document, and prevent recurrence. Your task: 1) propose a CAPA data model framing evidence and artifacts, 2) define a lifecycle state machine, 3) outline a minimal REST API to create/update CAPAs with linked logs/traces, 4) specify tenant-scoped metrics for containment, 5) provide an RCA template and a canary-based fix plan to validate before broader rollout?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Salesforce","Twitter"]},{"id":"q-1502","question":"Scenario: A new analytics feature ingests raw customer IDs into a 3rd-party BI dataset due to a masking policy misconfiguration in the data pipeline. Design a CAPA program to detect, document, and prevent recurrence. Your tasks: 1) propose a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) tenant-scoped metrics for containment and recurrence, 5) an RCA template and a canary-based remediation plan to validate before rollout?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Oracle","PayPal","Tesla"]},{"id":"q-1528","question":"Scenario: A vendor data feed for pricing and product metadata experiences occasional delays and duplicates during a quarterly refresh, causing price mismatches in two regions. Design a beginner CAPA program to address this. Your tasks: 1) propose a CAPA data model capturing evidence and artifacts, 2) define a lifecycle state machine, 3) outline a minimal REST API to create/update CAPAs with linked feed records, 4) specify region- and tenant-aware metrics to prove containment, 5) provide an RCA template and a canary-based preventive action plan to validate before broader rollout?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Adobe","Coinbase","Microsoft"]},{"id":"q-1556","question":"Two regions saw bids inflated after a cache invalidation caused an expired pricing model to be applied in an ad-bidding pipeline. Design a CAPA program to detect, document, and prevent recurrence. Your task: 1) CAPA data model for evidence and artifacts, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with logs/traces, 4) region-asset metrics for containment (recurrence rate, MTTR, false positives), 5) RCA template and a canary cache-invalidation plan before rollout?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Discord","DoorDash","LinkedIn"]},{"id":"q-1635","question":"A distributed cache layer across two regions intermittently serves stale reads after a deployment; design a beginner CAPA program to detect, document, and prevent recurrence?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Microsoft","MongoDB"]},{"id":"q-1745","question":"Scenario: In a multi-tenant payments platform serving PayPal, Lyft, and Coinbase, a policy change to transaction masking and logging fails to propagate to streaming and analytics pipelines in two regions, raising privacy risk and audit gaps. Design a CAPA program to address this: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces and policy versions, 4) region-aware metrics to prove containment (recurrence rate, MTTR, false positives), 5) an RCA template and a canary rollback plan for policy changes?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Coinbase","Lyft","PayPal"]},{"id":"q-1848","question":"Design a CAPA for drift in an ML fraud detector across regions: 1) CAPA data model with driftIndex, affectedFeatures, modelVersion, evidence and artifact links; 2) lifecycle: detected → investigating → containment → RCA → closed; 3) minimal REST API to create/update CAPAs with logs and model snapshots; 4) region metrics: drift magnitude, MTTR, FPR/TPR changes; 5) RCA and canary rollback plan?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Cloudflare","Netflix","Uber"]},{"id":"q-1899","question":"Scenario: A real-time analytics service relies on a 3rd-party enrichment API. Under peak load, enrichment latency spikes cause backpressure and data loss in two regions. Design a beginner CAPA program to address this. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with evidence, 4) region-aware metrics to prove containment, 5) an RCA template and a canary-based fallback plan?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Meta","Netflix","Twitter"]},{"id":"q-1952","question":"Scenario: A multi-tenant data platform powering market data feeds experiences cross-tenant data leakage under peak load. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model capturing tenantId, policyId, incidentId, evidence and artifact links; 2) a lifecycle state machine; 3) a minimal REST API to create/update CAPAs with tenant-scoped logs and traces; 4) tenant-aware metrics (recurrence, MTTR, spillover rate); 5) RCA template and a canary policy rollout plan?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Meta","MongoDB","Robinhood"]},{"id":"q-2132","question":"Scenario: A multi-region real-time bidding system processes ad events via Kafka across us-east-1 and eu-west-1. A recently deployed bid normalization microservice causes timeouts and mispricing, leading to revenue variance and increased false positives in two regions. Design a CAPA program that covers: 1) a CAPA data model capturing evidence, artifacts, and cross-region traces; 2) a lifecycle state machine for CAPA progression; 3) a minimal REST API to create/update CAPAs with linked logs/traces; 4) region-aware metrics to prove containment (recurrence rate, MTTR, revenue delta, false-positive rate); 5) an RCA template and a canary rollout plan for the normalization service?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Amazon","Salesforce","Slack"]},{"id":"q-2174","question":"Scenario: A data-access policy violation occurs when a misconfigured feature-flag rollout temporarily allows cross-tenant data access in two regions. Design a CAPA program to detect, document, and prevent recurrence. Include: 1) a CAPA data model capturing lineage, access changes, and remediation artifacts; 2) a lifecycle state machine; 3) a minimal REST API to create/update CAPAs with linked logs/traces and policy changes; 4) region- and tenant-scoped metrics; 5) an RCA template and a canary-based remediation plan?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Microsoft","Twitter"]},{"id":"q-841","question":"Design a CAPA workflow for a high-volume platform (Airbnb/LinkedIn scale). The system must log incidents, perform RCA, implement corrective and preventive actions, and verify outcomes before closing. Provide: 1) a CAPA data model, 2) a lifecycle state machine, 3) an API surface to create/update CAPAs, 4) metrics to prove effectiveness (recurrence rate, time-to-close)?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Airbnb","LinkedIn"]},{"id":"q-937","question":"Scenario: A post-rollout incident caused latency spikes and higher error rates for a subset of regions when a new feature flag was enabled. Design a beginner-friendly CAPA to address this. Your task: 1) propose a CAPA data model, 2) define a lifecycle state machine, 3) outline a minimal REST API to create/update CAPAs with evidence, 4) specify practical metrics to prove effectiveness, 5) provide a simple RCA template and a canary-based preventive action you would test before full rollout?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["DoorDash","Twitter"]},{"id":"q-965","question":"Scenario: A multilingual moderation model update causes spikes in unsafe content in two locales. Design a beginner CAPA plan focusing on locale-scoped evidence, drift checks, and a safe rollback with feature flags. Include: 1) a CAPA data model, 2) a lifecycle machine, 3) a minimal REST API to capture CAPAs with evidence, 4) locale-specific success metrics, 5) an RCA template and a locale-specific canary plan for preview before global rollout?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Apple","Hugging Face"]},{"id":"q-986","question":"Scenario: After a schema evolution in the event ingestion pipeline, latency spikes and incorrect bids appear in two regions. Design a CAPA program with: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region-aware metrics to prove effectiveness (recurrence rate, mean time to containment, false-positive rate), 5) an RCA template and a canary rollout plan to validate before global deployment?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Citadel","Oracle","Uber"]},{"id":"q-1020","question":"You’re evaluating a beginner feature: a daily market digest in a Robinhood-like app. Do a practical cost‑benefit analysis: state assumptions, estimate data/API costs, storage and engineering time, quantify benefits (retention lift, ARPU), and compute break-even time. Provide a rough calculation and rationale?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["MongoDB","NVIDIA","Robinhood"]},{"id":"q-1052","question":"Scenario: Real-time ingestion service receives JSON events from edge devices. Guarantee per-user throughput while allowing bursts in a distributed cluster. Design and implement a practical throttling mechanism, specify data structures, atomicity (e.g., Redis Lua script), failure modes, testing strategy, and observability?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["NVIDIA","OpenAI","Tesla"]},{"id":"q-1057","question":"In a real-time feed system using a contextual bandit with attention weighting (CBA), design a policy that balances short-term CTR and long-term engagement. Explain your reward decomposition, exploration strategy, and handling of non-stationarity. How would you validate offline with CPE and ramp online safely? Provide a concise update rule?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Hugging Face","Microsoft","Uber"]},{"id":"q-1084","question":"Given a large social network planning to adopt a real-time feature flag evaluation service that runs on a hybrid stream/batch pipeline. Current pipeline: 1.2M events/sec, median latency 50 ms, 5% tail. New service promises 20–30% latency reduction and 25% cost increase, plus migration risk. Perform a cost-benefit analysis: quantify costs, benefits, risks, horizon (12 months), and decision rule with sensitivity ranges?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["LinkedIn","Meta","Snowflake"]},{"id":"q-1114","question":"You’re migrating a real-time feature flag engine (multi-tenant SaaS) to reduce tail latency and cost. Propose a three-phase migration: centralized eval, regional edge gateway, then hybrid routing with per-tenant caches. Specify success metrics, rollback criteria, and a 12-month plan?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["DoorDash","Hashicorp","Two Sigma"]},{"id":"q-1151","question":"In a real-time analytics system for engagement on a large social app, design a privacy-preserving cohort analytics pipeline that ingests ~3M events/sec with sub-100 ms latency per region. Requirements: data residency, differential privacy for cohort counts, delta- or exact-once streaming state, drift detection, and cost-conscious multi-region deployment. Outline architecture, data contracts, and trade-offs?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Coinbase","Meta","Snap"]},{"id":"q-1209","question":"You’re building an offline-first version of a daily digest app for low-connectivity users. Outline a minimal data model, eviction policy, and a practical plan to quantify cost savings from reduced network usage versus increased storage and complexity; provide a concrete example with rough numbers?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Adobe","DoorDash","Goldman Sachs"]},{"id":"q-1238","question":"You're evaluating a beginner feature: a daily 'Portfolio Health Snap' panel that assigns a risk score to each user based on volatility of top holdings. Do a practical cost-benefit analysis: state assumptions, data/API costs, storage, and engineering time; quantify benefits (retention lift, ARPU) and compute break-even time. Provide rough calculations and rationale?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Adobe","Square","Two Sigma"]},{"id":"q-1274","question":"You're assessing migrating a high-volume telemetry pipeline from a centralized data warehouse to a lakehouse with streaming ingestion and on-demand materialized views. Current throughput 5M events/sec, latency 5–7 min; target latency 2–3 min, 25% cost increase. Build a 12-month cost-benefit model: incremental storage/compute, streaming infra, data egress, drift/rollback costs; specify decision rules and sensitivity ranges?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Bloomberg","Citadel","Slack"]},{"id":"q-1403","question":"Design a real-time, fault-tolerant order processing pipeline for a high-traffic ecommerce system. Partition Kafka by user_id, use transactional producers for exactly-once ingestion, and idempotent consumers keyed by event_id. Maintain a durable log, emit to a warehouse via an idempotent sink, and support replay via event sourcing and schema evolution. Include chaos and backpressure tests?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Goldman Sachs","Instacart","Square"]},{"id":"q-1472","question":"Design and implement a cost-benefit analysis module for a feature-flag rollout in a global streaming platform. Given 1000 edge regions, 1M users, latency budget 50ms, and known incremental costs, specify a data model, metrics to collect, uplift estimation, and a core function that returns whether to roll out. Include rollout policy and plan for validation via A/B tests in production?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Bloomberg","Microsoft","Netflix"]},{"id":"q-1508","question":"You're deploying a real-time personalization pipeline where a new ML model runs behind a feature flag with a 1% canary. If live drift is detected against offline benchmarks and latency deviates beyond a threshold, outline a concrete plan for rollout control, rollback, and data integrity checks that minimizes user impact while preserving auditability?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Airbnb","Apple","Hashicorp"]},{"id":"q-1531","question":"Context: A streaming feature store for real-time personalization must decide between two deployment models under GDPR/CCPA constraints: (A) cloud-region-centric with EU data replicated to a central US region for global analytics, delivering tail latency 30–70 ms; (B) EU-first on-prem data plane with a lightweight cloud cache for global lookup, targeting 50–120 ms tail latency. Provide a 12–18 month cost model including data residency compliance, data transfer, storage, compute, tooling, outage risk, and a decision rule with sensitivity ranges?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Databricks","Google","Two Sigma"]},{"id":"q-1724","question":"You operate a real-time feature store for a global e-commerce platform. Ingested events arrive from multiple regions with clock skew and occasional late arrivals. You must deliver online feature values with tail latency under 100 ms while ensuring correctness when late data retroactively updates aggregates (e.g., cart_value_last_24h). Propose the architecture, covering data versioning, watermarking strategy, exactly-once processing, per-tenant isolation, and testing approach. Include a concrete late-event example and the system’s expected behavior?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Coinbase","IBM","Meta"]},{"id":"q-1817","question":"Design and describe a scalable, fault-tolerant real-time collaboration pipeline: ingest per-document operations via a durable queue (Kafka), assign strict sequence numbers, and ensure exactly-once processing with idempotent workers. Explain data model, ordering guarantees, CRDTs vs OT, cross-region replication, failure modes, tests, and rollback strategy. How would you implement end-to-end?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Adobe","Google","Zoom"]},{"id":"q-1874","question":"You’re evaluating a beginner feature: an in-app guided onboarding widget for a CRM product. Do a practical cost-benefit analysis: state assumptions, estimate data/hosting costs, content creation time, estimate uplift in activation and paid conversion, and compute break-even time. Provide a rough calculation and rationale?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["PayPal","Salesforce"]},{"id":"q-1963","question":"How would you design a practical 1-day task to ingest daily payments from an Oracle source CSV into Snowflake, ensuring idempotent loads, proper schema mapping, and data quality checks, with a minimal Python MERGE-based load skeleton by transaction_id and a quick test plan?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Oracle","Snowflake","Square"]},{"id":"q-1989","question":"You're evaluating replacing a centralized analytics pipeline with a federated learning-based recommendation model deployed on-device across 3 regions for 5M MAU; build a 2-year cost-benefit model including capex, opex, data-transfer savings, regulatory risk penalties, and uplift in engagement/ARPU, and specify the break-even horizon and go/no-go criteria?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Google","Twitter"]},{"id":"q-2005","question":"In a 3-site edge anomaly-detection rollout for 10k sensors per site (roughly 100 GB/day), design a 2-year cost-benefit analysis for an on-device autoencoder with federated updates to a central model. Include capex for edge hardware, opex for cloud training and data transfer, and quantified benefits from reduced downtime, maintenance savings, and MTBF uplift. Provide break-even horizon and go/no-go criteria?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Amazon","PayPal","Tesla"]},{"id":"q-2057","question":"You’re evaluating a starter feature named 'Retention Policy Engine' for a Slack/HashiCorp-like chat app that auto-purges messages older than a per-channel window, with per-user holds and legal holds; design a 2-year cost-benefit model, including storage savings, indexing/compute overhead, engineering time, potential compliance penalties avoided, and a go/no-go break-even horizon?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Hashicorp","Slack"]},{"id":"q-2086","question":"You're evaluating a privacy-preserving on-device NLP summarizer that runs on user devices with federated updates to a central model via secure aggregation. Design a 2-year cost-benefit model for deploying across Android and iOS in 4 regions with ~25M MAU. Include capex for secure enclaves and on-device storage, opex for federated aggregation and governance, update bandwidth, regulatory penalties for leakage, and uplift in engagement. Provide break-even and go/no-go criteria?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Meta","MongoDB","OpenAI"]},{"id":"q-843","question":"Given a CSV file with columns user_id, action, timestamp, write a Python function using only the standard library that returns the most recent action per user by deduplicating on user_id and keeping the latest timestamp; describe time complexity and edge cases?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["OpenAI","PayPal","Twitter"]},{"id":"q-872","question":"You are evaluating two deployment options for a new AI inference service under budget and latency constraints. Outline a practical, end-to-end cost-benefit analysis framework to decide which to deploy in production. Include: data you would collect (traffic, latency, SLA penalties, accuracy), metrics (NPV, ROI, payback), horizons, discount rate, handling uncertainty (scenarios), and a concrete calculation workflow you would run?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Meta","Scale Ai"]},{"id":"q-1010","question":"You're building a GPU-accelerated graph analytics pipeline that streams 1e9 edges. Design a cache coherence protocol (CCA) to keep per-vertex state consistent across 8 GPUs via a central directory. Choose directory vs snooping, invalidation vs update, and granularity. Describe data layout, coherence transitions, and a minimal update protocol with atomic operations; include trade-offs and performance tips?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Amazon","LinkedIn","NVIDIA"]},{"id":"q-1102","question":"You’re building a real-time 'cca' analytics service ingesting 10k events/sec from multiple services; it must provide low latency, deduplicate, and support backfill. Describe the architecture, data model, and exactly-once strategy, including how you’d implement dedup, transactional writes, and testing under node failures. What trade-offs do you consider?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Lyft","Netflix"]},{"id":"q-1106","question":"Design an end-to-end CDC pipeline that ingests change events from Salesforce and MongoDB and publishes to downstream consumers with at-least-once delivery. Explain your transport choice, deduplication, ordering across partitions, schema evolution, and strategies for backfills, replay, and rollbacks. Include monitoring, testing, and failover plans?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["MongoDB","Salesforce"]},{"id":"q-1190","question":"You're building a real-time collaborative whiteboard for a chat/video platform at Discord/Airbnb/Netflix scale. Each of 5–10k rooms can have up to 200 concurrent editors and must stay highly available with <100 ms latency. Explain your stack decisions: transport (WebSocket vs gRPC streaming), per-room state partitioning, operation encoding, and conflict resolution (CRDT vs OT). How would you handle exactly-once delivery and failure recovery?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Airbnb","Discord","Netflix"]},{"id":"q-1211","question":"In a multi-region service, each region maintains a local L1 cache and a shared L2 cache. How would you implement a robust cache coherence protocol to prevent stale reads while keeping latency low during write-heavy workloads? Include data paths, an invalidation strategy (push vs TTL), race-condition handling, and testing approaches?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Bloomberg","Microsoft"]},{"id":"q-1260","question":"You're building a real-time cca analytics service that ingests 20k-50k events/sec from multiple microservices and external partners. It must deliver per-user engagement scores with sub-second latency, handle out-of-order and late data, deduplicate events, and support backfill. Describe the end-to-end architecture, data model, and exactly-once strategy, including how you'd implement dedup, transactional writes, watermarking, and backfill testing under network partitions and clock skew?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Google","Uber","Zoom"]},{"id":"q-1296","question":"Design a privacy-conscious extension of a real-time cca analytics pipeline that computes per-user engagement scores across multiple geo-regions for three partner firms (Lyft, NVIDIA, Instacart). The extension must minimize PII exposure, support synthetic data feeds for testing without leaking real PII, provide auditable data events for compliance, and preserve correctness under backpressure, partition rebalancing, and clock skew. Describe architecture, data model changes, masking strategies, and how you’d validate with synthetic data?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Instacart","Lyft","NVIDIA"]},{"id":"q-1333","question":"Design a beginner-friendly data quality and observability pattern for a cca event ingestion pipeline. Ingest 1000–2000 events/sec from mobile and web sources. Specify a lightweight schema: user_id, event_type, ts, event_id. Implement at ingest: schema validation, DLQ for invalid records, per-field quality metrics, and a 60s watermark for late data. Describe implementation details and a concrete test plan with synthetic late and malformed events?","channel":"cca","subChannel":"general","difficulty":"beginner","tags":["cca"],"companies":["Apple","Google"]},{"id":"q-1344","question":"You're building a privacy-preserving, cross-tenant event ingestion and analytics service for a media analytics platform. Ingest 40k-120k events/sec from partner APIs and mobile SDKs, including PII fields. Design the end-to-end pipeline to enforce per-tenant isolation, field-level consent-based access, and auditability while preserving low-latency analytics. Include data model, masking, consent revocation handling, schema evolution, and testing strategy?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Anthropic","Meta","Snowflake"]},{"id":"q-1372","question":"Design a beginner-friendly, end-to-end pipeline to generate a daily per-user cca_score from 20-50k events/day across services. Each event has event_id, user_id, type (view, click, purchase), ts. Weights: view 0.2, click 0.5, purchase 2.0. Handle out-of-order data with a 4-hour watermark, deduplicate by event_id, and support day-level corrections (if a repair event arrives, recompute that day and upsert). Describe data schema, ETL steps, dedup strategy, and a minimal test plan?","channel":"cca","subChannel":"general","difficulty":"beginner","tags":["cca"],"companies":["NVIDIA","Stripe","Two Sigma"]},{"id":"q-1464","question":"You’re building a real-time cca scoring pipeline ingesting 30k–80k events/sec from multiple vendors and regions. A feature update changes the engagement formula and must be reproducible for historical backfills without mutating past results. Design end-to-end data lineage, feature versioning, and deterministic backfill workflows: how to version features, catalog definitions, replay with identical inputs, handle non-determinism, and test under clock skew and partitions?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Citadel","Microsoft"]},{"id":"q-1489","question":"You're building an advanced real-time cca analytics platform for a multi-tenant product used by Zoom and Microsoft. The pipeline must respect per-tenant data residency, apply on-the-fly PII masking, and support per-user consent states while still delivering sub-second per-user scores. Explain the end-to-end architecture, privacy controls, and test strategy for backfill and audit trails?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Microsoft","Zoom"]},{"id":"q-1585","question":"You're running a real-time cca analytics pipeline ingesting 20k-50k events/sec from multiple partners. Beyond latency and dedup, design a GDPR/CCPA-compliant data erasure and retention mechanism: when a user requests deletion, purge all derived scores and raw events across online stores, backfills, and audit logs within sub-second latency. Describe architecture, data model, and guarantees, plus testing under partitions and clock skew?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Netflix","Oracle","PayPal"]},{"id":"q-1607","question":"You're operating an advanced streaming cca scoring pipeline where a new feature-weighting model must be rolled out with minimal disruption. Design a canary rollout strategy that guarantees deterministic routing, comparability of scores, and safe rollback under drift or latency spikes. What data-plane changes, testing plans, and rollback criteria would you implement?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Google","Two Sigma"]},{"id":"q-1662","question":"You're operating a privacy-preserving, real-time cca scoring service for a multi-region delivery platform with strict tenant isolation. Design an architecture that guarantees per-tenant data isolation, deterministic routing for canary vs production, and safe rollback under drift or latency spikes. Include data partitioning, encryption, feature gating, testing plan, and rollback criteria?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["DoorDash","Meta"]},{"id":"q-1684","question":"You're building a production-grade per-user cca_score service that spans multiple regions and partner integrations. Design a hybrid real-time/batch pipeline to compute and refresh cca_score with model versioning, ensuring region-local data processing, strict tenant isolation, dedup and exactly-once semantics, and safe backfill testing under clock skew and partitions. Include data model, streaming windowing, incremental scoring, canary rollout, rollback criteria, and test plan?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["DoorDash","Twitter","Uber"]},{"id":"q-1717","question":"You're building a production-grade real-time cca scoring service with tenant data residency rules. Some tenants require EU-only storage and compute, others are global. Design an architecture that (a) deterministically routes requests by tenant and user, (b) version-controls per-tenant models, and (c) supports zero-downtime hot-swapping with safe rollback under drift. Include data model, isolation, testing, and rollback criteria?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Instacart","Lyft","Twitter"]},{"id":"q-1769","question":"You're building a region-scoped, multi-tenant cca_score streaming system with regional data residency guarantees for each tenant. The pipeline ingests 15k–60k events/sec from partner feeds, computes per-tenant cca_scores with model_versioning, and must support cross-region dedup, exactly-once semantics, and safe backfill during partitions. Describe end-to-end architecture, data model, and deployment strategy to meet residency, isolation, and rollback guarantees?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Meta","Plaid"]},{"id":"q-1991","question":"Design a privacy- and compliance-focused real-time cca_score pipeline for a multi-tenant enterprise. The system must enforce per-tenant data isolation, immutable audit logs, and an on-demand privacy mode that halts ingestion and model updates for regulatory checks. Describe architecture, data lineage, deletion policies (Right-to-Deletion), drift detection, and rollback criteria under partitions and clock skew. How would you implement this?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Anthropic","Instacart","Microsoft"]},{"id":"q-2062","question":"You're building a beginner-friendly per-user cca_score API for a mobile app used worldwide. Design a minimal, region-aware scoring service with a 2-feature linear model, cache results in Redis (TTL 15m), and deduplicate requests via a request_id. Use Postgres for user features, gate features by region to meet privacy rules, and keep writes idempotent. Provide endpoints, data schemas, and a simple test plan with canary rollout, clock skew, and backfill considerations?","channel":"cca","subChannel":"general","difficulty":"beginner","tags":["cca"],"companies":["Apple","IBM","NVIDIA"]},{"id":"q-840","question":"In a secure messaging service, ciphertexts are decrypted by a server with a decryption oracle exposed to clients, creating a potential CCA risk. Design a practical IND-CCA secure scheme for message confidentiality using existing primitives (e.g., OAEP, AES-GCM, MACs). Explain how you prevent chosen-ciphertext attacks, outline a concrete protocol, and discuss trade-offs?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Microsoft","Oracle","Stripe"]},{"id":"q-879","question":"Describe a cross-region user preferences syncing protocol using MongoDB that tolerates regional partitions. Specify the data model (per-field version stamps), conflict resolution policy, and read/write configurations. Provide a concrete merge approach and an example conflict scenario?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Coinbase","MongoDB","Uber"]},{"id":"q-970","question":"You're shipping an E2E chat feature for an internal Meta–Microsoft product. An attacker may access a decryption oracle. Outline a concrete IND-CCA2 secure scheme for message exchange using public-key crypto, detailing padding (OAEP), a KEM/DEM split or AEAD wrapper, ephemeral keys, and how you bind metadata (timestamps, sender IDs) to prevent malleability. What are the failure modes and mitigations?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Meta","Microsoft"]},{"id":"q-1026","question":"Design a CGOA wrapper for a C library that provides two symbols: int* generate_seq(int n) which allocates an int array of length n via malloc filled with 0..n-1, and void free_seq(int* p) to free it. Provide the C header, the Go binding using CGO, and a small Go program that concurrently requests arrays of sizes 4 and 8, validates contents, and frees them. Include exact build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Bloomberg","Coinbase","Oracle"]},{"id":"q-1099","question":"Given a C library with an asynchronous API: \n\n- void run_async(const char* input, void (*cb)(int status, const char* data, void* user), void* user);\n\nwhere the callback runs on a worker thread and data is malloc-allocated or NULL on error. Design a CGO-based Go wrapper that exposes RunAsync(input string, cb func(status int, data string, err error)). The wrapper must: manage the Go callback safely across C boundaries, free C data, map non-zero status to errors, and handle thread attachment; provide header + binding + minimal test harness to demonstrate safety?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Google","LinkedIn","PayPal"]},{"id":"q-1207","question":"Design a CGO bridge for a C EventLib that exposes a function: void register_event_source(int stream_id, void (*cb)(int, const char*)); void start_event_loop(); Build a Go binding that lets two independent streams subscribe and receive events via a single exported Go callback, using a C shim to bridge into Go. Describe memory management and thread-safety; provide header, Go binding, and a small Go program demonstrating two streams; include build steps?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Hashicorp","Twitter","Two Sigma"]},{"id":"q-1235","question":"Implement a CGOA binding for an opaque C Counter handle. API: typedef struct Counter Counter; Counter* CounterNew(int); void CounterInc(Counter*, int); int CounterValue(Counter*); void CounterFree(Counter*); In Go wrap as type Counter with NewCounter, Inc, Value, Close. Show two goroutines each creating its own Counter, incrementing independently, and printing values to verify isolation and no data races. Include header, binding, and a minimal program with build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Adobe","Instacart","Tesla"]},{"id":"q-1294","question":"Implement a CGOA bridge that lets C trigger a Go callback asynchronously when an external sensor fires events. Provide a C header and stub, a CGO binding in Go that registers a Go callback and routes events through an exported Go function, and a safe cleanup mechanism to release resources when producers stop?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Anthropic","Lyft","NVIDIA"]},{"id":"q-1371","question":"Implement a CGO binding for a C function that computes sum and mean of an int array. Provide header and C source with int compute_stats(const int* values, size_t n, long long* sum, double* mean); 0 on success, -1 if n==0. In Go, implement ComputeStats(values []int) (int64, float64, error) calling the C function via CGO, converting types. Add a small main.go that runs two goroutines, each calling ComputeStats on its own slice. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Lyft","Two Sigma"]},{"id":"q-1396","question":"Implement a CGO bridge for a C event source that runs callbacks on its own threads and delivers events to Go via a callback of the form void cb(int code, const char* msg, void* ctx). Expose StartEventSource(cb, ctx) -> handle and StopEventSource(handle). Provide a C header, the Go binding using CGO (with a Go-exported callback and pointer-ownership strategy), and a small Go program that starts two sources and validates receipt of events through a single Go channel. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Instacart","PayPal","Slack"]},{"id":"q-1548","question":"Implement a CGO wrapper for a C library that registers a Go callback and invokes it from a C event loop. Provide a C header and implementation for: - void register_callback(void (*cb)(int)); - void trigger(int value); Write Go bindings using CGO to pass a Go function as the callback, ensuring safe cross-language invocation, correct Go runtime considerations, and demonstrate with two goroutines each registering and triggering events. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Databricks","Meta","NVIDIA"]},{"id":"q-1659","question":"Design a CGOA wrapper to let a Go function be registered as a callback for a C API that emits events asynchronously on external threads. The C API exposes: typedef void (*cb_t)(int eventCode, void* context); void register_cb(cb_t cb, void* ctx); void emit_event(int code); Implement header, CGO binding, and a minimal program that registers a Go callback, triggers an event, and exits safely. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Google","Scale Ai","Twitter"]},{"id":"q-1758","question":"You have a C library exposing void start_event_loop(void (*cb)(int event, void* ctx), void* ctx) and void stop_event_loop(). Implement a CGO bridge in Go that safely delivers events to per-stream Go handlers without passing Go pointers to C, supports multiple concurrent streams, and clean teardown. Provide C header, a thin C wrapper to register a per-stream callback, and a Go binding plus a small program that starts two streams and stops them?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Lyft","Netflix","Snap"]},{"id":"q-1815","question":"Design and implement a CGOA bridge for a C streaming library that emits integers via a callback. Expose a Go API StartStream() (<-chan int, func Stop()) using cgo.Handle to pass the Go-side context. Ensure thread-safe delivery of values to Go, proper cleanup with finalizers, and robust error reporting across the boundary. Provide C header, Go binding, and a minimal caller?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Cloudflare","Google","Plaid"]},{"id":"q-1845","question":"Implement a CGOA wrapper around a C function that returns a heap-allocated string: char* greet(const char* name). Provide a header with greet and void free_string(char*). Write a Go binding using CGO to call greet from multiple goroutines and free the result with free_string to avoid leaks. Include the header, a Go binding file, and a small Go program that calls greet(\"Alice\"), greet(\"Bob\"), greet(\"Carol\") concurrently and prints results. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["MongoDB","NVIDIA","Snowflake"]},{"id":"q-1910","question":"Design a CGOA exercise: Create a C header exposing void scale_and_offset(const float* input, int n, float* output, float scale, float offset); Implement a Go binding using CGO that wraps ScaleAndOffset and returns a slice of float results. Provide a minimal C implementation, a Go binding, and a short Go program that launches two goroutines, each feeding distinct input arrays to scale_and_offset concurrently? Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Bloomberg","Instacart","PayPal"]},{"id":"q-1993","question":"Design a CGO bridge for a C API that uses a progress callback and supports cancelable work. The C API exposes: typedef void (*progress_cb)(int percent, void* user); void register_progress(progress_cb cb, void* user); int do_work(int steps); The callback may be invoked on worker threads. Implement: (1) a C header that defines the callback type and registration; (2) a Go binding using CGO that exposes a Go channel-based Progress stream; (3) a small Go program that starts do_work in a goroutine and prints progress, with a context-based cancellation. Ensure memory and lifecycle safety across boundaries?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Cloudflare","Google","Stripe"]},{"id":"q-2011","question":"Implement a beginner CGO binding that exposes a C API void compute_stats(const double* data, size_t n, double* mean, double* stddev) to Go. Provide a C header, a Go binding using CGO, and a small Go program that launches 4 goroutines, each calling ComputeStats on its own data slice concurrently. Ensure memory safety, zero-copy access where possible, and that data races are avoided. Include explicit build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Airbnb","Meta","Square"]},{"id":"q-2036","question":"Your C library exposes a function int start_task(int task_id, void (*progress_cb)(int, const char*)); void stop_task(int task_id); The callback is invoked from worker threads and passes a progress value and a C string message. Design a CGO-based Go binding that allows registering a Go callback per task, safely translates C strings to Go strings, guarantees thread-safe delivery of progress events to Go, and supports cancellation via stop_task while ensuring resources are freed. Include a header, the Go binding, and a minimal Go program launching two concurrent tasks with per-task callbacks. Provide build steps?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Google","NVIDIA","Robinhood"]},{"id":"q-842","question":"You have a Go service using cgo to wrap a C API. A C function 'char* fetch_data(int id)' returns a malloc-allocated string or NULL on error. Design a safe Go wrapper that converts the result to a Go string, ensures the C allocation is freed, handles NULL with a meaningful error, and notes CGO thread-safety considerations. What implementation would you write?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Meta","PayPal","Snowflake"]},{"id":"q-870","question":"Design a Go wrapper for a C API with malloc’d results that must be freed, exploring a new angle: ensure thread-safe, single-point ownership transfer for each call and robust error handling when the C call returns a non-zero code or NULL. API: typedef struct { int code; const char* msg; } ScanResult; ScanResult* perform_scan(const char* query); void free_scan_result(ScanResult*); Implement function: func Scan(query string) (string, error)?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Instacart","Netflix"]},{"id":"q-914","question":"Using cgo, how would you wrap a C function that allocates a string (char*) and returns it, ensuring memory is freed by Go code without leaks, and provide a minimal working example?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Instacart","Snap","Twitter"]},{"id":"q-950","question":"Implement a minimal CGOA wrapper that exposes a C function int add(int a, int b) to Go. Provide the C header, the Go binding using CGO, and a small Go program that concurrently calls Add from two goroutines to demonstrate thread safety. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Google","Snap","Twitter"]},{"id":"q-978","question":"Design a CGO bridge for a C streaming API that delivers chunks via a callback: void stream_data(int id, void (*chunk_cb)(const char* data, size_t len, void*), void* ctx). Implement a Go wrapper StreamFromC(id int) (io.Reader, error) that buffers chunks into an io.Pipe and exposes a safe reader, supporting concurrent streams and proper backpressure. Include a C header, a Go CGO binding, and a simple consumer showing two goroutines reading from the reader?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Goldman Sachs","Microsoft","NVIDIA"]},{"id":"q-987","question":"Implement a CGO binding for a C function char* greet(const char* name) that returns a newly allocated string. Provide the C header and implementation, a Go binding using CGO that wraps greet in a safe Go function returning (string, error), and a small Go program that concurrently calls the binding from multiple goroutines and frees the allocated memory. How would you handle NULL returns and memory deallocation robustly?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Google","MongoDB","Snap"]},{"id":"cissp-communication-network-1768227886993-3","question":"An organization wants to ensure the integrity and authenticity of DNS responses to prevent cache poisoning, while not necessarily encrypting DNS query payloads. Which mechanism provides this guarantee?","channel":"cissp","subChannel":"communication-network","difficulty":"intermediate","tags":["AWS","DNS","Terraform","certification-mcq","domain-weight-13"],"companies":null},{"id":"q-1097","question":"An enterprise with microservices deployed across AWS, Azure, and GCP stores API keys and credentials in Kubernetes secrets and environment variables. After an audit finds stale keys and overly broad service accounts, design a defense‑in‑depth credential management strategy: architecture changes, tooling choices (Vault, AWS Secrets Manager, KMS, Kubernetes CSI or equivalent), rotation cadence, access controls, and how you would validate effectiveness?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Apple","Discord","Meta"]},{"id":"q-1113","question":"In a multi-tenant ML feature store on AWS, a compromised notebook can access all tenants due to broad IAM policy. Propose a concrete mitigation plan to enforce tenant isolation and data protection: separate namespaces, least-privilege roles with explicit ARNs, ABAC tags (TenantID/DataClass), SCPs to block cross-tenant actions, VPC isolation with PrivateLink, per-tenant KMS keys, and centralized logging with CloudTrail, Config, and GuardDuty. Include testing steps?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Scale Ai","Twitter"]},{"id":"q-1373","question":"In a global organization with Kubernetes clusters on AWS and GCP, CI/CD pipelines currently pull secrets from plaintext files. Design a CISSP-aligned secrets management solution: vault choice (HashiCorp Vault or cloud-native), access controls, dynamic Secrets, short‑lived tokens, rotation cadence, audit logging, break-glass, and CI/CD integration with GitHub Actions/Jenkins. Include trade-offs?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Adobe","Bloomberg"]},{"id":"q-1407","question":"Scenario: A data platform uses MongoDB Atlas for operational data and Snowflake for analytics across three regions; admins use SSO, but access is excessive and logs are incomplete. Design a defense-in-depth plan that enforces least privilege, KMS-backed encryption, and robust auditing. Include concrete steps for MongoDB Atlas and Snowflake: provisioning, key management, logging, data masking for PII, and verification?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["MongoDB","Snowflake"]},{"id":"q-1480","question":"In a multinational fintech operating across Google Cloud, AWS, and on-prem environments, PCI data flows through a centralized data lake used by data scientists for analytics. Propose a data-centric security design that ensures PCI data never leaves regulated regions while enabling cross-cloud analytics. Include data tagging, tokenization or masking strategy, key management, and telemetry/audit hooks. What pattern do you choose and why, and outline concrete steps to implement in the next sprint?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Bloomberg","Google","PayPal"]},{"id":"q-1493","question":"An international company runs CI/CD pipelines in GitHub Actions that assume cross‑cloud roles across AWS and GCP. Secrets are stored in plaintext, and pipeline logs may expose credentials. Propose a CISSP-aligned remediation: pick an access model (e.g., short‑lived credentials + Just‑In‑Time access), specify token lifetimes, rotation, logging/auditing, and CI/CD integration checks. How would you validate fixes in production-like staging and what trade-offs exist?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Airbnb","Discord","Tesla"]},{"id":"q-1558","question":"Design a CISSP-aligned access-control plan for a multinational org running data science workloads across AWS and GCP. After a contractor's credentials were compromised, enforce Just-In-Time access, short‑lived tokens, and auditable trails. Specify tooling (AWS STS, Vault, GCP IAM, OPA, Cloud Audit Logs), break-glass process, and how you validate the policy before production rollout?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["LinkedIn","NVIDIA","OpenAI"]},{"id":"q-1619","question":"A multinational fintech runs microservices on AWS EKS and Azure AKS with a shared Snowflake data lake. Credential theft enables pivot via cross-cloud service accounts. Design a CISSP-aligned control set: (1) cross-cloud Just-In-Time access with tokens from AWS STS, Azure AD, and GCP IAM; (2) ABAC/RBAC policy-as-code with OPA; (3) service-mesh mTLS and micro-segmentation; (4) audit trails; (5) break-glass and staging purple-team validation. Specify acceptance criteria and evidence for production rollout?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["IBM","Meta"]},{"id":"q-1633","question":"A global retailer stores customer data in AWS S3, Google Cloud Storage, and an on-prem Hadoop cluster. Data is classified as public, internal, or restricted. Design a CISSP-aligned data-classification and access-control policy using ABAC with tags to enforce least privilege and enable auditable access across all platforms. Include example tags, roles, token lifetimes, and a minimal policy snippet?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Hashicorp","Two Sigma"]},{"id":"q-1660","question":"Scenario: A multinational runs workloads in AWS and IBM Cloud with Cloudflare Zero Trust. An insider exfiltrates data using DNS tunneling to a rogue domain. Propose a concrete, CISSP‑level detection and containment plan that preserves legitimate traffic. Include monitoring sources, detection signatures, tools, and IR steps across the three platforms?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Amazon","Cloudflare","IBM"]},{"id":"q-1704","question":"Design a CISSP-aligned zero-trust admin access model for remote production sessions across a hybrid cloud environment (AWS/Azure) with Windows/Linux endpoints. Include device posture checks, MFA, ephemeral credentials, just-in-time roles, jump-host/RDP/SSH brokering, session recording, break-glass controls, and auditable logging. Provide a concrete deployment plan and a safe red-team validation approach?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Meta","Tesla"]},{"id":"q-1798","question":"Design a CISSP-aligned admin-access governance model for a global platform with multi-cloud assets (AWS, GCP, Azure) and microservices. A contractor needs short-term admin rights across prod and staging; outline controls, tooling (IdP, Vault, OPA, Cloud Audit/Logging, Kubernetes RBAC), break-glass, and how you would validate the policy before production rollout?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Google","Slack","Tesla"]},{"id":"q-1892","question":"An organization runs production apps across AWS, on‑prem, and Azure. A compromised developer workstation exposed a long‑lived API key used by a microservice to access production data. Outline a CISSP‑aligned incident response plan: containment, eradication, recovery; specify evidence sources (CloudTrail, VPC flow logs, EDR), credential rotation (short‑lived tokens), revocation (IAM/Vault), network segmentation, and post‑incident hardening. Include a Vault runbook snippet to revoke the compromised token and issue a new one?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Microsoft","Tesla"]},{"id":"q-1904","question":"Describe a CISSP-aligned secure software supply chain plan for a new product shipped via CI/CD pipelines across AWS and GitHub Actions. Include SBOM generation/verification, code signing and binary attestation, secrets management in pipelines, and auditable logs plus incident response. Provide concrete steps and tooling?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["NVIDIA","PayPal","Scale Ai"]},{"id":"q-1980","question":"A multi-tenant delivery platform serving Microsoft and DoorDash customers processes sensitive order data across microservices in AWS. Propose a CISSP-aligned data protection plan covering data-in-transit and data-at-rest, key management (rotation, separation of duties, access controls), and auditing/incident response. Include concrete tools and steps?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["DoorDash","Microsoft"]},{"id":"q-2051","question":"Scenario: a junior developer accidentally commits a cloud service key to a public repo, exposing AWS and GCP access. Outline a CISSP-aligned, beginner-friendly containment and recovery plan: immediate revocation of exposed keys, rotate credentials, enforce MFA, apply least-privilege IAM changes, enable cross-cloud audit logs, and run a quick tabletop exercise to validate readiness. Which steps and tools would you use?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Amazon","Google","IBM"]},{"id":"q-2083","question":"In a global fintech with services on AWS and GCP plus on‑prem, a contractor's workstation is compromised and used to harvest ephemeral credentials authenticating to production apps. Propose a CISSP‑aligned containment, eradication, and recovery plan covering cross‑cloud token revocation, key rotation, Just‑In‑Time access, vault integration, break‑glass, and auditable logging. Include tool‑specific steps and a runbook snippet to revoke and reissue tokens across AWS IAM, GCP IAM, and Vault?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["LinkedIn","Robinhood","Snap"]},{"id":"q-2118","question":"In a multi-cloud dev sandbox spanning AWS, Azure, and GCP, service accounts for CI deployments have overly broad admin rights, risking secret exposure in production. Provide a CISSP-aligned, beginner-friendly containment and remediation plan: inventory affected identities, revoke excessive privileges, implement least-privilege roles per cloud, enable just-in-time access, rotate keys, enable cross-cloud audit logs, and validate with a tabletop exercise. Which steps and tools would you use?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Apple","Coinbase","Databricks"]},{"id":"q-2137","question":"Scenario: a multinational platform operates across AWS, Azure, and on‑prem storage. A CSV containing customer PII was exposed publicly due to a drift in IAM/RBAC policies and a misconfigured data catalog. Outline a CISSP‑aligned incident response playbook: detection signals, containment (isolate storage, revoke tokens), eradication (remediate drift, enforce ABAC), recovery, and post‑incident audit across clouds; include tooling, roles, and timing?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["LinkedIn","MongoDB","Tesla"]},{"id":"q-2172","question":"In a data lake spanning AWS S3, GCP Cloud Storage, and on-prem HDFS, design a CISSP-aligned, beginner-level plan to enforce least privilege for 120 analysts. Describe an RBAC/ABAC approach, just-in-time elevation, access revocation, and auditable logging, plus onboarding and withdrawal workflows and pre-deployment validation?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Apple","Meta","Two Sigma"]},{"id":"q-958","question":"Scenario: A fintech startup uses cloud IAM for multi-cloud apps. Admins use weak passwords and MFA is not enforced; API keys are shared in chat and stored insecurely. You’re asked to pick one first CISSP-aligned control to reduce risk while enabling operations. Which baseline control should be implemented first and why?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Coinbase","Google","Meta"]},{"id":"q-1087","question":"You're running a 5-node HA Kubernetes control plane (3 in AZ-a, 2 in AZ-b) with a 3-member etcd cluster. After a regional outage, etcd loses quorum. Describe exact, command-level steps to restore quorum, rejoin the third member, and validate API availability across both AZs, including risk notes and DR readiness checks?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Cloudflare","Scale Ai","Snap"]},{"id":"q-1368","question":"In a 3-node etcd-backed Kubernetes cluster, one node loses network connectivity and becomes partitioned while the other two remain healthy. How do you preserve availability and data integrity, avoid split-brain, and recover the partitioned member? Outline health checks, how you isolate the partition, recovery steps, and verification?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Netflix","Tesla"]},{"id":"q-1395","question":"In a 3-node Kubernetes cluster, deploy a stateless web app using a Deployment with 3 replicas and a ClusterIP Service. Include readiness and liveness probes, CPU/memory requests and limits, and populate APP_MODE via a ConfigMap and DB_PASSWORD from a Secret. Describe the steps to perform a rolling update to 4 replicas with zero downtime and how you would verify the rollout across nodes?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Adobe","Snap"]},{"id":"q-1418","question":"Enable at-rest encryption for Kubernetes Secrets using a KMS (envelope) provider on an existing 3-control-plane cluster. Describe exact steps to configure the EncryptionConfig, rotate KEKs without downtime, trigger re-encryption of existing Secrets, and verify that new and existing Secrets are stored encrypted at rest (without decrypting at-rest data). Include concrete commands and caveats?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Google","Robinhood","Scale Ai"]},{"id":"q-1452","question":"You operate a 3-node Kubernetes cluster with a StatefulSet of 3 replicas backed by PVs. A critical fix requires upgrading to image app:v2 with zero downtime. Outline a precise upgrade plan: set a PodDisruptionBudget to minAvailable 2, apply a RollingUpdate with partition=2, ensure readiness probes tolerate brief pod restarts, and verify data integrity during rollout with concrete kubectl commands?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Google","Oracle","Uber"]},{"id":"q-1562","question":"How would you design a Kubernetes Job named app-init-seed that seeds app data on first run? The Job should use an Alpine-based image, mount a PVC at /data, load a script from a ConfigMap at /scripts/seed.sh, skip if /data/.seeded exists, optionally fetch seed.json from https://example.com using API key from a Secret, and write a log and the marker file upon success; include full YAML and apply steps?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Adobe","Salesforce"]},{"id":"q-1737","question":"You're deploying a 3-replica Deployment in Kubernetes for a payment app. Suddenly 2 pods crash with CrashLoopBackOff. Describe a practical debugging workflow to identify if the issue is container startup, config, resources, or image, and outline the exact kubectl commands and file checks you would perform. Include how you would propose a minimal fix and a rollback plan?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Apple","Robinhood"]},{"id":"q-1764","question":"Advanced: In a 5-node Kubernetes cluster where etcd memory usage has spiked after a noisy deployment, outline a production-safe remediation plan: verify health with etcdctl, diagnose via metrics, perform defrag/compact, take a snapshot, and adjust API watch load while preserving API server availability. What steps would you take?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Google","Microsoft","Snowflake"]},{"id":"q-1810","question":"You're operating a 5-node Kubernetes cluster with a validating webhook named cfg.example.com enforcing a label requirement on Deployments in all namespaces. A critical patch must be applied in prod-adv while the webhook service is temporarily unavailable. Outline a safe, auditable plan to bypass the webhook with exact kubectl commands to identify, disable, apply, and revert?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["DoorDash","Google","Meta"]},{"id":"q-1885","question":"In a high-traffic delivery platform, you deploy a Kubernetes-based worker pool for async order processing. Explain how you would implement a robust, rate-limited queue with backpressure, idempotent workers, and at-least-once delivery, using Kubernetes primitives and common tools. Include how you handle spikes, retries, and data-store consistency?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["DoorDash","Two Sigma"]},{"id":"q-1932","question":"Describe a **concrete, production-ready plan** for zero-downtime deployments in a Lyft-scale Kubernetes cluster with ~40 microservices, multiple data stores, and strict uptime. How would you implement **blue/green or canary releases**, traffic shaping, and automated rollbacks? Include rollout strategy, readiness checks, RBAC, DR, and a compact manifest example comparing Istio and Linkerd approaches?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Lyft","Tesla","Twitter"]},{"id":"q-1946","question":"On a 3-node Kubernetes cluster, a Deployment’s Pods stay Pending while 2 nodes are Ready. The scheduler shows a NoSchedule taint on node-1. Explain exact commands to identify the taint, check the pod’s tolerations, and either remove the taint or add a matching toleration so the workload schedules. How do you validate after changes?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Amazon","Apple","Uber"]},{"id":"q-1972","question":"You maintain a Node.js app deployed as a 3-replica Kubernetes Deployment behind a service. A security patch requires updating the container image from v1.0.1 to v1.0.2 with zero downtime. List exact kubectl steps to perform a safe rolling update, how you confirm readiness, and how you rollback if the rollout fails?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Microsoft","Netflix"]},{"id":"q-2027","question":"You're managing a shared Kubernetes cluster with namespaces prod, analytics, and dev. A nightly analytics batch job sometimes starves frontend pods during peak hours, triggering evictions. Design an end-to-end remediation plan: tune pod requests/limits, enforce defaults with a LimitRange, cap total usage with a ResourceQuota, ensure guaranteed QoS, evaluate VerticalPodAutoscaler vs HorizontalPodAutoscaler, and outline validation steps to prevent regressions?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["DoorDash","Netflix"]},{"id":"q-2134","question":"You manage a Databricks/OpenAI-like platform on Kubernetes. A Spark streaming job processes 1TB/day and faces sporadic node preemption causing data loss without checkpoints. Design a fault-tolerant deployment (manifests and configurations) and explain trade-offs between Kubernetes Job vs. StatefulSet, backoff strategies, taints/tolerations, and Spark dynamic allocation. What would you implement and why?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Databricks","OpenAI"]},{"id":"q-2152","question":"On a Kubernetes cluster running Spark jobs via Spark on Kubernetes, a 12-pod job reports intermittent OOM errors during shuffle-heavy stages on medium-sized datasets. Provide a concrete debugging plan: how you verify memory limits, adjust executor/driver memory and overhead, tune JVM GC, and validate changes with metrics and a controlled benchmark run?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Databricks","Oracle"]},{"id":"q-874","question":"In a Kubernetes cluster used by Salesforce/Cloudflare/Snap engineers, a Deployment's startup latency rose from 1–2s to 6–8s after introducing an initContainer that runs a health check before the application starts. Describe how you would diagnose, what metrics/logs to collect, and concrete fixes (e.g., moving checks to readiness, caching, parallel init, or canary rollout). Include rollback and validation steps?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Cloudflare","Salesforce","Snap"]},{"id":"q-893","question":"You manage a 3-node Kubernetes control plane backed by an etcd cluster. After a power outage, one etcd member reports corruption. Describe the exact steps to detect the corrupted member, restore from a known-good snapshot, rejoin the cluster, and validate API availability. Include concrete commands, risk notes, and how you would verify DR readiness?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Cloudflare","IBM","Netflix"]},{"id":"q-936","question":"A 3-control-plane Kubernetes cluster on AWS experiences API server latency spikes after a webhook deployment. The admission webhook is malfunctioning and causing slow requests; outline precise steps to identify the failing webhook, safely disable it to restore API responsiveness, validate cluster availability, and prepare a rollback plan with minimal downtime?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Airbnb","Databricks","Google"]},{"id":"q-1066","question":"Inside namespace analytics, schedule a daily batch to process a CSV: use a CronJob (02:00 UTC) to start a Job that runs a Python script from a ConfigMap, reads input from a ConfigMap, uses a Secret for DB credentials to insert results into Postgres service, writes output to a PVC, runs as non-root with a readOnlyRootFilesystem, with resource limits, a backoffLimit of 3, and a 15-minute activeDeadlineSeconds; ensure proper probes?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Goldman Sachs","Microsoft","Snowflake"]},{"id":"q-1312","question":"In a Kubernetes CKAD scenario, you have a Deployment named 'web-server' in namespace 'prod' running a Node.js app. During rolling updates, some pods terminate and restart slowly, causing request timeouts. Outline concrete changes to implement startupProbe, adjust readiness and liveness probes, and add a preStop hook to drain existing connections. Provide a minimal manifest patch showing startupProbe, a readinessProbe, a livenessProbe, and a preStop lifecycle hook that ensures graceful shutdown. How would you approach this?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Discord","MongoDB","Snap"]},{"id":"q-1352","question":"You have a small Python API app to run in Kubernetes. Write a minimal manifest that creates a Deployment with 3 replicas using image myregistry/api:1.0, a readinessProbe httpGet /health on port 8080, a livenessProbe with initialDelaySeconds 15 and periodSeconds 10, and a ClusterIP Service exposing port 8080. Inject config via ConfigMap app-config with LOG_LEVEL. How would you verify and how would you scale to 5 replicas?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Google","Meta","OpenAI"]},{"id":"q-1393","question":"Design a Kubernetes manifest that deploys a stateless app with 3 replicas, uses a ConfigMap and a Secret, attaches a 1Gi PVC for data, includes a HorizontalPodAutoscaler, exposes via ClusterIP, and enforces a NetworkPolicy restricting egress to api.internal.example.com:443. Provide YAML fragments and discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["OpenAI","Two Sigma"]},{"id":"q-1459","question":"Create Kubernetes manifests for a simple API: Deployment using image 'my-api:1.0' that reads PORT from a ConfigMap via env, a ConfigMap with PORT and APP_MODE, readinessProbe and livenessProbe for /healthz on that port, and resource requests/limits. Expose with a Service on port 80 targeting 3000. Describe a rolling update plan?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["IBM","Meta","NVIDIA"]},{"id":"q-1488","question":"Given a Deployment named 'image-processor' in namespace 'prod' with 6 replicas processing images from a Redis queue, design a practical patch to ensure graceful shutdown of in-flight tasks during rollouts, prevent simultaneous pod terminations, and maintain availability during node drains; include a minimal manifest patch adding a preStop script, terminationGracePeriodSeconds, readiness and liveness probes, and a PodDisruptionBudget targeting the deployment. What steps would you take to validate under load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Adobe","Airbnb","Snap"]},{"id":"q-1547","question":"Configure a NetworkPolicy to restrict egress from prod/checkout-service to only reach payments/payment-processor in the payments namespace on port 443, blocking all other egress. Provide a minimal manifest patch and a test strategy to validate allowed and blocked traffic inside the cluster?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Netflix","Oracle"]},{"id":"q-1609","question":"You're deploying inventory-service on Kubernetes. Write YAML to (1) deploy 3 replicas with a ConfigMap for API_ENDPOINT and a Secret for DB_PASSWORD, (2) an InitContainer that runs migrations, (3) readiness and liveness probes, resource requests/limits, and a RollingUpdate strategy with maxUnavailable: 1, (4) a Job to run migrations before the first pod starts, (5) a sidecar log-shipper. Include the key fragments and rationale?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["DoorDash","NVIDIA","Slack"]},{"id":"q-1626","question":"Blue/Green rollout for a CKAD production web service: in namespace 'prod', a Deployment 'web-app' with 3 replicas serves traffic via the Service 'web-app'. Introduce a canary path with a second Deployment 'web-app-canary' and a canary route (via canary Ingress or a second Service) to validate with 10–20% traffic before full switch. Provide a minimal manifest patch showing resource requests/limits, readinessProbe, and a livenessProbe for both deployments, plus how to switch traffic and rollback. Include validation steps under load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["NVIDIA","OpenAI","Oracle"]},{"id":"q-1707","question":"Explain a progressive canary rollout for a 4-replica frontend behind an NGINX Ingress. Use a stable Deployment and a canary Deployment (image frontend:1.2-canary, replicas:1). Patch Ingress to route 10% to canary; later 50% and 100%. Include minimal YAML patches for deployments and a canary Ingress with canary-weight; how would you monitor health and rollback?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Coinbase","Discord"]},{"id":"q-1767","question":"Scenario: In a CKAD scenario, you must roll out a new image for a stateless API 'inventory-api' in namespace 'prod' with 2% traffic to the canary, using vanilla Kubernetes (no service mesh). Outline a practical canary strategy with two Deployments and two Services, explain how you split traffic without a mesh, and provide minimal manifests for the canary Deployment and a canary-facing Service, plus a rollback plan and how you'd validate under load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Google","NVIDIA"]},{"id":"q-1776","question":"Design and implement a CKAD deployment for a 3-replica web app: use a ConfigMap for env vars, define resource requests/limits, add readiness and liveness probes, and configure an HPA with min 3, max 10 and targetCPUUtilizationPercentage 50. Provide the YAML and show verification steps?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Meta","Oracle"]},{"id":"q-1879","question":"You are deploying a Node API behind a Service in Kubernetes with Redis and PostgreSQL. Provide manifests to run API with 4 replicas, Secret for DB creds, ConfigMap for flags, readiness/liveness probes, resource requests/limits, and a RollingUpdate with maxUnavailable=25%, maxSurge=25%. Include how you would test zero-downtime and how HPA would be wired?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Amazon","Discord","Google"]},{"id":"q-1922","question":"CKAD intermediate: In a prod namespace, a Deployment named 'orders-api' with 3 replicas experiences brief outages during image upgrades. Provide a concrete patch for zero-downtime upgrades: set rollingUpdate strategy (maxUnavailable: 0, maxSurge: 1), add a PreStop hook for graceful shutdown, and create a PodDisruptionBudget to protect at least 2 healthy pods during maintenance. Include minimal Deployment and PDB manifests and describe validation under maintenance-like load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Google","Square"]},{"id":"q-1959","question":"In a Kubernetes CKAD scenario, design a real-time log-processor that consumes from Kafka, writes results to Cassandra, restarts gracefully on failure, and preserves at-least-once delivery. Provide a concrete deployment design with InitContainer, ConfigMap, Secret, Probes, HPA, DLQ strategy, and a minimal YAML skeleton. Explain trade-offs and monitoring hooks?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Meta","NVIDIA","Two Sigma"]},{"id":"q-1992","question":"Scenario: A 3-replica Deployment for a Kubernetes CKAD exercise uses a ConfigMap for APP_PORT and LOG_LEVEL and a Secret for DB_PASSWORD. How would you structure manifests to ensure zero-downtime updates, proper health checks, and correct secret/config usage? Provide concrete manifest fragments and a rollout strategy?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Apple","IBM","Meta"]},{"id":"q-2015","question":"Design a CKAD-grade, multi-tenant API gateway canary rollout: implement two Deployments (stable and canary) for api-gateway, share a Service, and use an Ingress canary annotation to route 20% traffic to canary. Use a ConfigMap flag newFeature to toggle the new code path; store TLS certs in a Secret; include readiness/liveness probes, resource requests/limits, and a minimal YAML skeleton. Explain how you would observe traffic split and rollback?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Meta","Uber"]},{"id":"q-2093","question":"You deploy a Kubernetes Deployment named web-app using image registry.example.com/web-app:v1.2 and expose port 8080. Provide a YAML manifest snippet that adds: livenessProbe for /health on 8080 with initialDelaySeconds: 5 and periodSeconds: 10; readinessProbe for /ready with timeoutSeconds: 3; resources: requests cpu: 250m memory: 256Mi; limits cpu: 500m memory: 512Mi; a ConfigMap mounted at /etc/config providing APP_MODE and an env var APP_MODE sourced from that ConfigMap?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Discord","Salesforce"]},{"id":"q-2145","question":"CKAD intermediate: In namespace retail, a Deployment 'inventory' (3 replicas) talks to Postgres service 'inventory-db'. You need a one-time seed of lookup data at Pod startup without delaying traffic. Provide a minimal patch that uses an InitContainer to run a seed SQL script against inventory-db with idempotent INSERTs (ON CONFLICT DO NOTHING), and add resource requests/limits, a readiness probe, and a liveness probe. Explain how you'd validate under load and re-seed behavior on restarts?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Discord","Microsoft","PayPal"]},{"id":"q-858","question":"In a Kubernetes CKAD scenario, you have a Deployment named 'web-app' in namespace 'prod' with 3 replicas; pods frequently OOMKilled under load. Describe a practical debugging plan and provide a minimal manifest patch showing resource requests/limits, a readiness probe, and a liveness probe. Include scaling considerations and how you'd validate the fix under load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Hugging Face","LinkedIn","Snowflake"]},{"id":"q-1180","question":"Design a CKNE-aware per-tenant admission control for a multi-tenant real-time analytics gateway. Downstream CKNE health signals (queue depth, latency, error rate) are exposed via metadata. Propose a per-tenant health score, a dynamic token-bucket policy, and a cross-tenant shedding strategy that preserves fairness and SLA compliance. Include payload schemas, a minute-by-minute control loop, and a minimal sample payload?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Goldman Sachs","Meta","Snap"]},{"id":"q-1198","question":"Design a CKNE-aware per-tenant traffic shaping policy for a real-time collaboration platform (gateway -> engine -> persistence) servicing thousands of tenants with different SLAs. Edge CKNE health signals drive a minute-by-minute token-bucket shedding policy that prioritizes high-SLA tenants while gracefully degrading others; specify payload schemas and provide a minimal test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Discord","Snap"]},{"id":"q-1214","question":"Design a CKNE-aware data lineage policy for a three-stage ETL pipeline (ingest → transform → load) servicing thousands of tenants. Each hop attaches CKNE health in trace metadata. Propose a per-tenant degradation policy that preserves auditability for high‑SLA tenants while shedding heavy lineage data during degradation. Include payload schema, a minute-by-minute decision loop, and a minimal payload example?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Anthropic","Google","Scale Ai"]},{"id":"q-1280","question":"Scenario: A serverless workflow (API gateway -> orchestrator -> worker) serves thousands of tenants. Design a CKNE-aware tracing approach where every hop propagates a CKNE health signal in trace metadata and implement a per-tenant adaptive sampling policy that starts at 3% and scales to 25% during degradation. Include payload schemas, per-tenant health aggregation at the orchestrator, strategies to preserve trace fidelity during micro-bursts, and a minute-by-minute loop mapping health to sampling for the next window; provide a minimal payload example and a test plan?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Discord","Microsoft","Netflix"]},{"id":"q-1289","question":"Design a CKNE-aware canary rollout strategy for a multi-tenant image-resize API (ingest -> process -> deliver). Each tenant's requests carry CKNE health in headers. Propose a per-tenant rollout policy that starts at 5% canary, scales to 40% during healthy conditions, and reverts on degradation, with a minute-by-minute control loop. Include payload schemas, edge aggregation, and a minimal payload example?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Amazon","Meta","NVIDIA"]},{"id":"q-1377","question":"Design a CKNE-aware cross-region cache strategy for a multi-tenant real-time feed service (ingest -> compute -> deliver). Each tenant emits a CKNE health signal attached to requests. Propose per-tenant cache admission, TTLs, and prefetching depth that adapt minute-by-minute based on CKNE health, edge burst traffic, and tenant SLAs. Include payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Instacart","Slack"]},{"id":"q-1442","question":"Context: a multi-tenant mobile app with gateway -> dispatcher -> worker. Each tenant emits CKNE health in requests; design a CKNE-aware per-tenant notification dispatcher that throttles messages by a simple score-to-drop policy: score < 0.7 drops 10%, score < 0.5 drops 30%, otherwise none. Provide payload schemas, minute-by-minute decision loop, a minimal payload example, and a unit test to validate the policy?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Google","NVIDIA","Uber"]},{"id":"q-1554","question":"Design a CKNE-aware per-tenant event routing policy for a real-time analytics pipeline (ingest -> stream-processor -> dashboard) servicing thousands of tenants with varying SLAs. Each hop propagates a CKNE health signal. Propose a per-tenant routing policy that uses CKNE health, queue depth, and SLA tier to determine dynamic fan-out limits, with a minute-by-minute control loop and tenant-aware backpressure. Include payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Citadel","Coinbase","Meta"]},{"id":"q-1595","question":"Design a CKNE-aware per-tenant circuit-breaker policy for a real-time order routing system (gateway -> routing -> fulfillment) serving thousands of merchants. Each hop propagates a CKNE health signal. Propose a per-tenant policy that activates a progressive circuit breaker during degradation based on CKNE health, queue depth, and tenant SLA, with a minute-by-minute control loop; include payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["DoorDash","Tesla"]},{"id":"q-1632","question":"Design a CKNE-aware per-tenant cache TTL policy for a multi-tenant CDN path (gateway -> edge-cache -> origin) where each hop propagates CKNE health signals. Propose how TTLs and cache invalidation granularity vary by tenant health and SLA, with a minute-by-minute control loop. Include payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["OpenAI","PayPal","Snap"]},{"id":"q-1663","question":"Design a CKNE-aware multi-tenant ingestion pipeline (ingest → stream-processor → store) where edge CKNE health signals gate per-tenant throughput: batch size, forwardRaw vs enriched, and backpressure to enrichment services. Provide a minute-by-minute decision loop, per-tenant SLA handling, payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Apple","Databricks","Snowflake"]},{"id":"q-1755","question":"Design a CKNE-aware multi-tenant caching layer for a real-time recommendations pipeline (ingest -> rec-service -> storefront). Edge nodes propagate CKNE health; implement adaptive per-tenant TTLs, prefetch, and anti-stampede guards. Provide payload schemas, minute-by-minute decision loop, and a minimal payload example; include a test plan?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Meta","Slack"]},{"id":"q-1843","question":"Design a CKNE-aware privacy-gating policy for a multi-tenant streaming analytics pipeline (ingest -> processor -> store) that handles PII with per-tenant redaction levels tied to CKNE health. When health degrades, progressively increase redaction, throttle nonessential fields, and gate exports to dashboards. Describe payload schemas, a minute-by-minute control loop, and provide a minimal payload example?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["IBM","MongoDB","Oracle"]},{"id":"q-2169","question":"Design a CKNE-aware per-tenant rate-limiter for a real-time multi-tenant ingestion pipeline (ingest -> transform -> store). Edge CKNE health signals drive per-tenant quotas and burst handling. Propose how quotas are computed, payload schemas, a minute-by-minute control loop, and a minimal payload example; include a test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Lyft","Meta","Snowflake"]},{"id":"q-846","question":"Design a real-time CKNE failure detector for a distributed microservice mesh. Specify the data pipeline, latency budget, how you compute p95 latency and error rate, and how you implement replay and backpressure for fault tolerance. Include testing strategies and production validation to demonstrate correctness and resilience?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Coinbase","Meta","NVIDIA"]},{"id":"q-861","question":"Design an adaptive CKNE-aware tracing and sampling strategy for a real-time order-processing pipeline in a multi-tenant mesh. Explain how CKNE health signals influence sampling decisions, how you preserve trace fidelity under bursts, and how you quantify the overhead and impact on latency. Include concrete data structures and an example workflow?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["PayPal","Snap","Tesla"]},{"id":"q-954","question":"Scenario: A three-service order flow (API gateway -> inventory -> payment) runs in one region. Design a CKNE-aware tracing approach where each service propagates a CKNE health signal in trace metadata and employs an adaptive sampling policy: base 10% with a health-adjusted factor that can raise sampling to 50% during degradation. Specify data structures for per-service health, the trace metadata payload, and a minute-by-minute workflow to compute health and adjust sampling for the next window. Provide a minimal code snippet showing the payload and health update logic?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Goldman Sachs","IBM"]},{"id":"q-991","question":"Design a CKNE-aware tracing strategy for a real-time ad bidding pipeline (gateway -> bidding service -> settlement) servicing multi-tenant advertisers. Each leg propagates a CKNE health signal; implement an adaptive sampling policy that scales from 5% baseline to 60% during degradation, with per-tenant health aggregation at the edge. Specify payload schemas, how to preserve trace fidelity under micro-burst traffic, and a minute-by-minute workflow for health-to-sampling decisions; provide a minimal payload example and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["NVIDIA","Netflix","Stripe"]},{"id":"q-1121","question":"Scenario: You operate a shared Kubernetes cluster serving multiple product teams. You must prevent cross-namespace data leakage and enforce least-privilege access while remaining auditable and scalable. Describe a concrete strategy using either OPA Gatekeeper or Kyverno for admission control (with at least two constraints), implement namespace RBAC boundaries, apply Calico NetworkPolicy for namespace isolation, and outline a monitoring/audit plan with tests and runbooks. Include example policies and a minimal test commands snippet?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Google","IBM","Snap"]},{"id":"q-1130","question":"You're running a Kubernetes cluster for a web app. A Pod mounting hostPath and running as root was detected in dev. Outline a practical plan to enforce least privilege across namespaces (Baseline/Restricted) using a policy engine (Kyverno or OPA Gatekeeper) and show how you would validate enforcement without disrupting workloads. What steps and files would you use?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["Amazon","Google"]},{"id":"q-1167","question":"Scenario: You operate a multi-cluster Kubernetes data platform (cloud+on‑prem) where a Spark job can access customer data. Design an end-to-end approach to detect, prevent, and respond to data exfiltration attempts from pods across clusters. Include policy design, telemetry signals, enforcement, and incident runbooks; discuss trade-offs?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["Bloomberg","Instacart"]},{"id":"q-1278","question":"Scenario: A fintech data platform runs a multi-tenant data lake on Kubernetes. Each data job uses per-job ServiceAccounts to access restricted cloud storage. A rogue pod tries to exfiltrate data via the bucket. Propose a security approach that binds each pod to a dedicated cloud IAM role (workload identity), enforces namespace-scoped permissions, and provides tamper-evident audit trails. Include detection and response for abnormal egress and a safe rotation plan. What trade-offs?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["DoorDash","Robinhood"]},{"id":"q-1301","question":"You're debugging a Kubernetes deployment in a multi-tenant environment where one namespace's pods delay startup by several minutes. Provide a practical, beginner-friendly diagnostic flow focusing on pod events, init containers, image pulls, and config maps. List concrete kubectl commands you would run and how you’d determine the root cause?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["Anthropic","Netflix","Twitter"]},{"id":"q-1323","question":"In a Kubernetes cluster deploying an ML inference service, models and weights live in a private registry. Outline a practical plan to sign models with cosign, publish attestations, and enforce runtime verification so only attested models can be deployed via GitOps (Argo CD) and an enforcement policy (OPA Gatekeeper or Kyverno). Include concrete commands and sample configuration?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Hugging Face","PayPal","Uber"]},{"id":"q-1356","question":"You manage a Kubernetes cluster hosting regulated data across tenants. Design a practical end-to-end plan to enable at-rest encryption with envelope encryption using a cloud KMS, protect both API server data and etcd data, rotate keys safely, and prove compliance via CI checks. Include concrete commands and sample manifests?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Amazon","Google","Uber"]},{"id":"q-1374","question":"Scenario: A multi-cloud platform runs Kubernetes on EKS and serverless runtimes; ensure only cosign-signed images with verifiable attestations can be deployed across all targets. Describe an end-to-end plan to enforce cross-registry provenance, automate Rekor attestations, and integrate with a GitOps workflow (Argo CD), including concrete commands and sample policy rules?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Discord","Meta","Tesla"]},{"id":"q-1398","question":"In a multi-tenant Kubernetes cluster with images stored in a private OCI registry, design an end-to-end workflow to sign images with cosign, publish attestations, and enforce that only attested images are deployed. Include concrete commands, CI hints, and an admission control policy snippet?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Cloudflare","Discord","Google"]},{"id":"q-1444","question":"Scenario: A poly-cloud serverless stack runs AWS Lambda, GCP Cloud Functions, and Azure Functions. CI/CD signs each function package and its dependencies with cosign and publishes SLSA attestations to a central registry. Deployments must be allowed only if attested across all clouds. Describe an end-to-end plan, including concrete commands and sample configs for signing, attesting, and cross-cloud enforcement?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Amazon","LinkedIn","Netflix"]},{"id":"q-1522","question":"Scenario: In a multi-tenant SaaS, a central migration service applies Flyway SQL scripts to dozens of PostgreSQL instances. How would you implement a concrete plan to sign each migration with cosign in CI, publish attestations to a registry, and enforce at runtime that only attested migrations are executed? Include concrete signing commands, attestation storage approach, and a sample enforcement policy (Kyverno/OPA) plus integration steps with Flyway?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Databricks","Slack","Uber"]},{"id":"q-1551","question":"In a Kubernetes-based data platform hosting a multi-tenant ML feature store exposed via a high-volume API, design a privacy-preserving, end-to-end audit trail to support forensics without exposing PII. Specify architecture, RBAC/ABAC controls, OpenTelemetry instrumentation, log pipeline (Fluentd/Loki), encryption, data redaction, retention, and how you’d run production-scale incident drills. What would you monitor first and why?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["Cloudflare","LinkedIn","Two Sigma"]},{"id":"q-1691","question":"Scenario: A Terraform-driven multi-tenant cloud platform provisions resources across clouds. You must sign every Terraform plan in CI with cosign, publish an attestation to a central registry, and enforce at runtime that only attested plans are applied by the GitOps flow. Describe an end-to-end approach with concrete signing commands, attestation storage layout, an sample OPA policy, and integration steps with the deployment pipeline?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Netflix","Stripe"]},{"id":"q-1734","question":"In a multi-cloud Terraform deployment across AWS, GCP, and Azure, a central registry hosts official modules. Propose a concrete plan to: - sign every module and its dependencies with cosign during CI, - publish attestations to a provenance registry, - enforce at plan/apply time that only attested modules are used via an OPA policy or equivalent gate, including concrete signing commands, storage layout for attestations, and a sample enforcement policy with integration steps?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Airbnb","Apple","Salesforce"]},{"id":"q-1846","question":"In a Tesla-scale Databricks lakehouse on AWS, with Unity Catalog and a Kubernetes data plane, contractors routinely export aggregated datasets to external S3 buckets. Design a detection and response mechanism that distinguishes legitimate exports from exfiltration attempts. Include telemetry (Unity Catalog audit logs, IAM activity, Delta table operations), thresholds, alerting, and an incident playbook?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["Databricks","Tesla"]},{"id":"q-1964","question":"In a multi-tenant notebook service on Kubernetes, each notebook runs in a transient pod and pulls dependencies from a private registry. Design a concrete plan to sign the notebook artifact (notebook.ipynb) and its dependencies with cosign, publish SLSA attestations to a central registry, and enforce at runtime that only attested notebooks and dependencies are allowed to run. Include concrete signing commands, attestations storage, and a sample Kyverno/OPA policy plus integration steps with the notebook runner?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Anthropic","Citadel","Microsoft"]},{"id":"q-2092","question":"In a Kubernetes-based data-analytics platform with two namespaces, dev and prod, design a practical RBAC and Pod Security setup for a new microservice 'data-processor' to ensure it can only read its own ConfigMaps and Secrets, runs as a non-root user, and cannot escalate privileges. Provide concrete manifest fragments (RBAC, ServiceAccount, RoleBinding, PodSecurityContext) and describe how to validate at admission time that all pods comply?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["Adobe","Databricks","DoorDash"]},{"id":"q-2128","question":"In a high-sensitivity environment spanning on-prem and cloud, design a tamper-evident Kubernetes audit logging and incident-response pipeline that preserves evidence from the API server, etcd, and kubelets while enabling automated triage and containment during a breach. Specify data formats, forwarding targets, non-repudiation measures, and failure modes?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["Adobe","PayPal","Tesla"]},{"id":"q-920","question":"In a real-time chat service like Discord, deployed on Kubernetes with NVIDIA GPUs for video processing, you introduce a third-party plugin system that runs as WebAssembly modules to apply custom video filters. How would you design a secure plugin sandbox and runtime attestation to prevent leakage of streams or keys, ensure isolation from other plugins, and enable rapid rollback if a plugin behaves unexpectedly in production? Provide concrete approaches and trade-offs?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Discord","NVIDIA","Zoom"]},{"id":"q-959","question":"Scenario: A service executes user-provided Python plugins inside a container. Design a concrete runtime hardening plan using Linux namespaces, a minimal seccomp profile, and capability bounding, ensuring plugins cannot access host files or network directly while preserving IPC with a controlled channel. Outline exact steps and validation tests?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["PayPal","Tesla"]},{"id":"q-967","question":"Scenario: You manage a microservice app deployed to Kubernetes with CI/CD; you need to prevent tampered container images. **Describe a practical, beginner-friendly plan** to implement image signing and verification using **cosign**, integrate it into a GitHub Actions workflow, and enforce verification at deployment (registry or admission webhook). Include concrete commands?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["Apple","NVIDIA","Stripe"]},{"id":"q-994","question":"Scenario: A Kubernetes-based ML platform serves multiple teams; outbound data exfiltration is a breach risk. Propose a concrete, end-to-end control plane approach to prevent unauthorized data egress using policy-as-code, Kubernetes NetworkPolicy, and a centralized egress gateway. Include a sample Rego policy for Gatekeeper that enforces a namespace label data-export=allowed and an annotation egress-proxy=https://proxy.internal, and outline testing and GitOps integration?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["OpenAI","Zoom"]},{"id":"q-1019","question":"You're operating a CNF-based API gateway cluster that terminates TLS for thousands of tenants across 3 regions. A mandate requires migrating all TLS to post-quantum algorithms with per-tenant keys sourced from an HSM, while delivering zero-downtime upgrades, per-tenant key isolation, and a rollback plan. Outline an end-to-end rollout including (a) inventory and compatibility checks, (b) PQC algorithm and certificate strategy, (c) HSM PKCS#11 integration and key rotation, (d) canary/traffic-mirror rollout and drift detection, (e) observability and rollback criteria?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Goldman Sachs","LinkedIn","PayPal"]},{"id":"q-1040","question":"You're deploying a CNF-based UDP gateway across four data centers. A CVE requires hardware-backed attestation before image execution. Outline a concrete end-to-end rollout plan that (a) signs CNF images with Cosign and SBOMs, (b) enforces attestation via TPM 2.0-based attestation bundles and ImagePolicyWebhook, (c) rolls out region-by-region with traffic mirroring and per-tenant quotas, (d) implements drift detection and automated rollback on attestation failure, and (e) provides observability for attestation metrics and rollback triggers?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Adobe","OpenAI"]},{"id":"q-1071","question":"You're deploying a CNF-based API gateway across 2 Kubernetes clusters. A recent upgrade causes a cross-tenant data bleed under load due to a shared in-memory cache. Outline a beginner-friendly, end-to-end plan to fix and roll out safely: (a) reproduce in staging with two tenants and isolated traffic, (b) implement tenant-scoped cache keys and per-tenant isolation checks, (c) add unit/integration tests for isolation, (d) perform blue/green canary rollout with tenant-based traffic splits, (e) observability: per-tenant SLA metrics and automatic rollback if bleed is detected. Include a minimal code snippet for tenant-scoped cache key?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Discord","Stripe","Two Sigma"]},{"id":"q-1081","question":"You're deploying a CNF-based API gateway that uses workload identities for tenants. Describe an end-to-end plan to enforce identity attestation and per-tenant isolation using SPIRE for SPIFFE IDs, OPA Gatekeeper for policy decisions, and a local Kind testbed. Include (a) issuing and rotating SVIDs, (b) encoding tenant permissions in policies, (c) testing isolation with two tenants, and (d) observability hooks for attestation and policy evaluations?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Lyft","Meta"]},{"id":"q-1107","question":"You're deploying a CNF-based API gateway serving two tenants with strict data-retention and per-tenant log-redaction requirements. Outline an end-to-end plan using SPIRE for workload identities and OPA Gatekeeper for policy decisions to enforce data handling rules while enabling zero-downtime upgrades in a local Kind testbed. Include (a) SVID issuance/rotation tied to tenant IDs, (b) tenant-scoped policies for retention windows and redaction, (c) runtime redaction checks on logs/traces, (d) cross-tenant isolation testing under load, and (e) observability hooks for attestation, policy decisions, and redaction misses?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Google","Meta","Twitter"]},{"id":"q-1187","question":"A beginner CNF certification scenario: you implement a gated CI/CD pipeline for a CNF gateway image. Outline an end-to-end workflow to ensure image provenance before deployment: (a) sign CNF images with Cosign using a KMS-backed key, (b) generate and publish SBOMs, (c) enforce signatures via ImagePolicyWebhook, and (d) surface observability in the monitoring stack for signing success/failure and rollback signals. Provide concrete steps and minimal config snippets?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Instacart","LinkedIn","Tesla"]},{"id":"q-1221","question":"You're operating a CNF-based API gateway deployed across three regions behind a service mesh. Propose a practical upgrade workflow that enforces runtime integrity along with image attestations: (a) sign images with Cosign using a KMS-backed key and publish SBOMs, (b) require TPM/measured-boot attestation plus runtime integrity checks for CNFs, (c) roll out region-by-region with per-tenant canaries and live connection migration, (d) implement drift detection and automatic rollback on attestation/runtime mismatch, (e) surface observability for sign-off, attestation, and rollback triggers. Provide minimal config references?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Netflix","Plaid"]},{"id":"q-1347","question":"Advanced CNF certification scenario: deploy a CNF gateway across three air‑gapped data centers. Design an end‑to‑end plan to ensure image provenance and runtime integrity for updates, covering (a) offline Cosign signing with a TPM‑backed key, (b) SBOM generation and private catalog, (c) per‑tenant policy enforcement via OPA/Gatekeeper, (d) edge‑site offline attestation broker, (e) drift detection with automated rollback, and (f) observability hooks. Include concrete config examples?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Apple","Citadel","Oracle"]},{"id":"q-1511","question":"Beginner CNF certification: You manage a CNF API gateway in a single Kubernetes namespace with a private registry and no external access. Outline an end-to-end plan to enforce SLSA provenance for each image before deployment, including (1) build and sign with Cosign using a KMS-backed key, (2) generate and attach SPDX SBOMs, (3) verify SLSA provenance via ImagePolicyWebhook, (4) canary upgrades with rollback on provenance failure, (5) observability for signing events, SBOM validity, and rollback triggers?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Databricks","Hashicorp","Twitter"]},{"id":"q-1526","question":"In a CNF gateway spanning multiple tenants, implement runtime **per-tenant feature flags** controlled by a central policy server. Describe a concrete plan using SPIRE for tenant identities, OPA for policy evaluation, and a sidecar that hot-reloads policies from a central repo. Include how you detect flag revocation, perform zero-downtime updates, and observability hooks (SBOM provenance, traces, metrics)?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Netflix","Oracle"]},{"id":"q-1544","question":"You're managing a CNF-based API gateway deployed in two regions, serving three tenants with a single global CI/CD pipeline. Propose an end-to-end upgrade and rollback strategy that delivers zero downtime while guaranteeing strict tenant isolation during updates. Include deployment approach (canary vs blue-green), tenant-aware routing, per-tenant metrics and logging, rollback triggers, and how you would validate the plan in staging that mirrors production. Cite concrete tooling choices (Argo Rollouts, Istio/Envoy, OPA, Prometheus) and touchpoints?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["DoorDash","Snowflake"]},{"id":"q-1571","question":"You’re deploying a CNF API gateway across 6 regions and need strict per-tenant isolation with dynamic rate limits and auditable policy changes. Propose an end-to-end plan that uses SPIRE for per-tenant identities, OPA for policy decisions, Envoy with hot xDS updates, and a shared policy catalog. Include how you push canary upgrades, rollback on drift, and observability for quota hits and policy eval latency?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["IBM","Stripe","Uber"]},{"id":"q-1599","question":"Outline a beginner-friendly, end-to-end workflow to enforce image provenance and runtime integrity per-tenant before deployment, using Cosign signing, SPDX SBOMs, and admission controls (ImagePolicyWebhook + Gatekeeper/OPA). Provide concrete steps and minimal config examples?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Apple","IBM"]},{"id":"q-1674","question":"Advanced CNF scenario: deploy a CNF API gateway across three geographies with a private registry and multi-tenancy. Propose an end-to-end plan to enforce per-tenant image provenance and runtime integrity, including (i) per-tenant Cosign signing keys stored in KMS/HSM, (ii) per-tenant SBOMs, (iii) admission controls using ImagePolicyWebhook/OPA, (iv) tenant-aware drift detection and rollback, and (v) observability for signing, attestation, and rollback signals. Provide concrete steps and minimal config examples?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Hashicorp","IBM","Snowflake"]},{"id":"q-1735","question":"Beginner CNF certification: In a multi-tenant Kubernetes cluster hosting CNF gateways in separate namespaces, design an end-to-end image provenance gate: (1) sign images with Cosign using a KMS-backed key and attach SPDX SBOMs, (2) enforce with an ImagePolicyWebhook that requires both signature and SBOM presence, (3) surface a per-tenant visibility badge on deployments, (4) provide minimal config snippets and commands to validate end-to-end in one namespace before rolling out to others?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Google","Hashicorp","Snowflake"]},{"id":"q-1793","question":"In a multi-tenant cluster hosting CNF gateway images, design a beginner end-to-end flow to enforce per-tenant image provenance using Cosign signing with tenant keys, SPDX SBOMs, and a Gatekeeper constraint that reads a per-tenant policy from a ConfigMap. Include minimal YAML for the ConstraintTemplate and Constraint, plus compliant vs noncompliant Deployment manifests, and the exact kubectl steps to validate end-to-end in a single namespace first?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Coinbase","Snap"]},{"id":"q-1812","question":"In a beginner CNF certification scenario, you operate a multi-tenant CNF gateway platform on Kubernetes with a private image registry. Describe a concrete end-to-end workflow to enforce provenance and runtime integrity for every pull: (1) embed build provenance into image labels (commit SHA, CI job, build ID), (2) sign with Cosign using a KMS-backed key and attach SPDX SBOMs, (3) enforce per-tenant policy via Gatekeeper/OPA that rejects images missing provenance or SBOM, and (4) provide a minimal test plan and config snippets to validate in a single namespace before rolling out to others?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Cloudflare","Coinbase","Square"]},{"id":"q-1914","question":"In a multi-tenant CNF gateway managed by a central control plane, design an end-to-end upgrade workflow that uses per-tenant feature flags and a canary rollout with Istio, while enforcing runtime policy with OPA Gatekeeper and SPIRE-based identity. Include: (a) how CNF versions and flags are modeled in CRDs, (b) traffic splitting and health checks for safe canaries, (c) per-tenant policy evaluation, and (d) drift detection with automatic rollback and observability?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Cloudflare","NVIDIA","Twitter"]},{"id":"q-2030","question":"Beginner CNF certification: How would you design an end-to-end workflow in a multi-tenant Kubernetes cluster to enforce SBOM freshness and key-rotation awareness for CNF images, so that when a Cosign key rotates SBOMs are re-generated and re-attested, and a Kyverno policy rejects images signed with old keys or missing SBOMs, with a minimal namespace-scoped test?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Anthropic","Cloudflare","NVIDIA"]},{"id":"q-847","question":"You're deploying a CNF gateway (e.g., NGINX CNF) on a 50-node Kubernetes cluster handling streaming traffic. A node eviction hits during peak load. Outline a concrete plan to maintain streaming availability, focusing on (1) graceful drain with preStop, (2) health checks/readiness/liveness, and (3) traffic affinity and pod topology, and (4) observability and rollout strategy?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Netflix","Plaid","Tesla"]},{"id":"q-918","question":"You're rolling out a CNF-based NAT gateway across a 3-region multi-cluster Kubernetes setup. A policy change must be applied without disrupting live traffic. Outline a concrete, end-to-end rollout plan emphasizing (a) shadow canaries with traffic mirroring, (b) region-by-region rollout with per-region SLI targets, (c) policy-state reconciliation and drift detection, (d) rollback conditions and observability instrumentation?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Citadel","Coinbase"]},{"id":"q-935","question":"You manage CNF-based gateways across 4 regions. A suspected supply-chain compromise requires enforcing in-cluster image attestations before rollout without downtime. Outline an end-to-end plan to (a) sign CNF images with Cosign and SBOMs, (b) enforce signatures via ImagePolicyWebhook, (c) roll out region-by-region with traffic mirroring, (d) implement drift detection and automatic rollback on attestation failure. Include observability?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["IBM","Tesla","Two Sigma"]},{"id":"q-985","question":"Design a zero-downtime, CNF-based API gateway rollout where per-tenant routing rules update live without dropping connections. Outline end-to-end steps: (a) safe rule distribution, (b) canary-ingress slicing with weighted traffic, (c) drift detection between desired and active routes, (d) observability and rollback?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Goldman Sachs","Meta","Tesla"]},{"id":"q-1051","question":"CNPA stack: HTTP API writes to PostgreSQL and emits Kafka events. A hot, large users table needs a non-blocking schema change (e.g., adding a new NOT NULL column with default). Propose a production-grade online migration plan that minimizes downtime, keeps Kafka in sync, handles backfill, and describes rollback and validation steps?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Amazon","Instacart","Plaid"]},{"id":"q-1150","question":"In a CNPA stack: HTTP API ingests events, writes to PostgreSQL, and publishes to Kafka; during spikes, retries duplicate processed events. Propose a concrete, end-to-end plan to guarantee idempotent processing and prevent duplicates under retry storms. Include idempotency key strategy, dedup enforcement point, Kafka/DB coordination, and validation?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Netflix","Snap","Twitter"]},{"id":"q-1157","question":"CNPA pipeline uses an HTTP API -> PostgreSQL -> Kafka with Avro schemas in a Schema Registry. A new optional field is added to the events, and some consumers crash when they see older versions. Provide a concrete, zero-downtime plan for schema evolution, including compatibility mode, rollout strategy, topic/consumer changes, backfill approach, rollback, and validation?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Anthropic","Hugging Face","Salesforce"]},{"id":"q-1188","question":"CNPA stack: an HTTP API writes to PostgreSQL and publishes to Kafka. During peak load, duplicate events may be produced due to retries and at-least-once semantics. Describe a concrete end-to-end plan to enforce idempotent processing across HTTP, DB, and Kafka, including dedupe strategy, upsert/constraints, transactional writes, offset tracking, and how you’d validate correctness under load?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Meta","PayPal","Tesla"]},{"id":"q-1283","question":"CNPA stack with HTTP API, PostgreSQL, Kafka, and Redis: a Redis-based rate limiter fronting the API causes legitimate bursts to be 429-throttled during a promo, despite normal traffic. Provide a concrete debugging plan to isolate whether latency or errors come from Redis Lua script, Redis network, the HTTP handler, or downstream services, with exact metrics and concrete fixes and how you’d validate impact?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["DoorDash","PayPal"]},{"id":"q-1383","question":"CNPA stack with HTTP API writing to PostgreSQL, publishing to Kafka, and Elasticsearch dashboards. A nightly backfill misses events, causing dashboards to report incorrect counts. Describe a concrete debugging plan to isolate whether loss occurs in HTTP write/transaction, Postgres-to-Kafka CDC, or Kafka-to-Elasticsearch sink, with exact metrics, sampling, and concrete fixes (idempotent sinks, transactional writes, producer retries, dedup IDs) and how you would verify end-to-end?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Databricks","Robinhood"]},{"id":"q-1401","question":"CNPA stack: HTTP API writes events to PostgreSQL, publishes to Kafka with event_time metadata, and a downstream analytics service reads from Kafka to produce 5-minute windowed counts. After a release, a dashboard shows both late counts and misaligned windows. Provide a concrete debugging plan to determine if the issue is event-time timestamps, Kafka timestamps, consumer windowing, or clock skew across services, including exact metrics, sampling, and fixes (re-timestamp, watermarking, idempotent sinks) and how you would validate end-to-end correctness?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Amazon","Apple"]},{"id":"q-1440","question":"In a CNPA stack with an HTTP API writing to PostgreSQL and publishing to Kafka, add an optional field customer_segment; design a zero-downtime schema evolution and payload versioning plan, including DB changes, Kafka message formats, consumer upgrades, backfill, and validation?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Apple","NVIDIA","Zoom"]},{"id":"q-1586","question":"In a CNPA stack with an HTTP API writing to PostgreSQL, publishing to Kafka, and Elasticsearch/Redis downstream, a burst causes duplicate rows in Postgres and delayed dashboard freshness. Propose a concrete end-to-end exactly-once plan: transactional Kafka producers, Postgres outbox, idempotent Elasticsearch sinks, Redis invalidation, and verification steps, plus rollback if needed?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Airbnb","IBM","Snowflake"]},{"id":"q-1630","question":"In a CNPA stack with an HTTP API, PostgreSQL, Kafka, and Redis, dashboards display stale data for a cohort after a deployment; propose a concrete debugging plan to determine whether drift originates from writes to Postgres, the Kafka sink, or Redis caching, including exact metrics to collect, sampling strategy, and concrete fixes (transactional outbox, idempotent sinks, Redis invalidation, cache-warming) and how you would verify impact?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["NVIDIA","Plaid"]},{"id":"q-1716","question":"CNPA stack: an HTTP API writes to PostgreSQL and publishes to Kafka. A schema evolution adds a new optional field to the event payload stored in Postgres and emitted to Kafka. Propose a concrete migration plan that preserves compatibility, uses a versioned envelope, coordinates writes and reads, validates with end-to-end tests, and provides a safe rollback. Include concrete steps, metrics, and rollback strategy?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Databricks","Google"]},{"id":"q-1732","question":"CNPA stack: HTTP API writes to PostgreSQL, emits to Kafka, and a Redis-backed read cache. A schema change adds a new optional field to the event payload; rollout under peak load leads to some consumers crashing due to compatibility. Provide a concrete, practical rollout and debugging plan to ensure no data loss or outages, including steps, metrics, and concrete changes (schema registry, backward/forward compatibility tests, dual-write, feature flags, cache invalidation) and how you would verify success?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Adobe","Coinbase","MongoDB"]},{"id":"q-1751","question":"CNPA stack: HTTP API → PostgreSQL → Kafka → stream processor → Redis dashboards. A new audit requires end-to-end latency visibility for late events during windows; latency spikes 2–3 minutes. Provide a concrete debugging plan to pinpoint whether the delay lies in HTTP ingress, DB writes, Kafka publish, stream windowing, or Redis caching. Include exact metrics, sampling, and fixes (idempotent sinks, transactional outbox, watermark tuning, checkpointing) and how you’d verify impact?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Goldman Sachs","Instacart","Snowflake"]},{"id":"q-1794","question":"In a CNPA stack—HTTP API, PostgreSQL, Kafka, Redis—latency spikes appear on a new endpoint that touches all components. Outline a practical plan to implement end-to-end tracing with a correlation_id: where to instrument, which metrics to collect (end-to-end latency, per-service latency, queue time, DB time, cache misses), and concrete changes (propagate correlation_id in HTTP, persist it in DB, attach it to Kafka messages, emit trace spans). How would you validate in staging before production?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Bloomberg","Google","Hugging Face"]},{"id":"q-1851","question":"In a CNPA stack: HTTP API ingests events, writes to PostgreSQL, and publishes to Kafka. A new enrichment step guarded by a feature flag calls a 3rd-party service. Design a concrete, end-to-end rollout plan that minimizes risk: per-tenant flag rollout, fallback behavior when the service is down, tracing with correlation IDs, circuit breaker, and backpressure; metrics to monitor (p50/p95 latency, error rate, backlog), rollback criteria, and validation steps?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Discord","Salesforce"]},{"id":"q-1875","question":"CNPA stack: HTTP API writes to PostgreSQL and publishes to Kafka; downstream analytics reads from Kafka and loads into a data warehouse. During deployment, dashboards drift and latency tails widen. Create a concrete, end-to-end debugging plan to isolate whether the root cause is HTTP serialization, DB write latency, Kafka publish, consumer, or ETL/warehouse load, with exact metrics, sampling, and concrete fixes (outbox pattern, idempotent sinks, backpressure, staged deploy) and verification steps?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Hashicorp","Slack","Snowflake"]},{"id":"q-1956","question":"In a CNPA stack: HTTP API ingests events, writes to PostgreSQL, and publishes to Kafka. A new optional field customer_region must be added without downtime or data loss. Describe a concrete, end-to-end migration plan: schema changes, producer/consumer compatibility, backfill strategy, validation checks, and rollback. Include metrics and canary signals to verify success?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["DoorDash","Oracle","Two Sigma"]},{"id":"q-2041","question":"In a CNPA stack (HTTP API -> Postgres -> Kafka -> Redis) you notice tail latency spikes during peak hours. Outline a concrete, beginner-friendly plan to diagnose end-to-end latency using distributed tracing. Include what to instrument, where to insert spans (HTTP handler, DB query, Kafka producer/consumer, Redis ops), how to propagate trace context, minimal metrics, and a simple verification checklist to confirm fixes?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Hashicorp","Lyft","Snap"]},{"id":"q-845","question":"In a MongoDB-backed service, read latency tails spike during peak hours. Provide a concrete, practical debugging plan to determine whether the bottleneck is network, driver, query plan, or index design. Include exact steps, metrics to collect, and concrete changes (indexes, readConcern, pooling) you would apply, plus how you would verify impact?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Anthropic","Lyft","MongoDB"]},{"id":"q-873","question":"A CNPA service receives events over HTTP, writes to PostgreSQL, and publishes to Kafka. During peak hours, read latency spikes. Describe a concrete debugging plan to determine whether the bottleneck is the HTTP handler, the DB query, or the Kafka producer, including exact steps, metrics to collect, and concrete changes (indexes, pooling, prepared statements, idempotent producer) and how you would verify impact?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Databricks","Instacart","NVIDIA"]},{"id":"q-903","question":"In a CNPA stack, an HTTP API writes to Postgres, publishes to Kafka, and a Redis-backed dashboard consumes the stream. During peak load, HTTP latency tails spike. Provide a concrete debugging plan to isolate whether the bottleneck is HTTP, DB, Kafka, producer, consumer, or Redis, with exact metrics, sampling, and concrete changes (pooling, prepared statements, acks, batch sizes, caching strategies) and how you would verify impact?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Bloomberg","Meta","Tesla"]},{"id":"q-928","question":"In a CNPA stack consisting of an HTTP API, PostgreSQL, Kafka, and Redis, latency tails spike under peak load. Provide a concrete, beginner-friendly plan to enable end-to-end tracing with OpenTelemetry to pinpoint the bottleneck. Include trace propagation, spans for HTTP handler, DB query, Kafka publish/consume, Redis access, and verification steps with a sample end-to-end trace?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Discord","Hashicorp","Netflix"]},{"id":"q-957","question":"CNPA stack with HTTP API, PostgreSQL, Kafka, and Redis dashboards requires a schema evolution: add a new optional field region_id to event records without downtime or breaking producers/consumers. Describe a practical, step-by-step migration plan: DB changes, schema registry versioning, producer/consumer updates, data backfill, testing, and rollback strategies, ensuring end-to-end consistency?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["PayPal","Scale Ai"]},{"id":"q-1193","question":"You have an array A of n positive integers and a threshold T. For a given window length L, define ok(L) as: there exists a subarray of length L with sum <= T. Design an O(n) check for ok(L) using a sliding window, then outline how to find the maximum L with binary search over [1..n], and analyze total time and space. Include edge-case handling and practical optimizations?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Airbnb","Snowflake","Tesla"]},{"id":"q-1254","question":"You're given a DAG G=(V,E) with N nodes and M edges. Edges can be inserted online in batches of size B. Design a dynamic transitive-closure using bitsets to answer reachability queries in O(1). Provide initialization, amortized per-batch update time, and memory. Include two practical optimizations and compare to recomputing the closure after each batch?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Goldman Sachs","Square","Two Sigma"]},{"id":"q-2159","question":"Scenario: In a weighted directed graph with nonnegative weights, you must maintain approximate betweenness centrality for all nodes under batches of edge insertions of size B. Propose a concrete, implementable scheme that (i) initializes estimates, (ii) updates after each batch with amortized cost, (iii) bounds memory, and (iv) provides guaranteed error bounds ε. Include two practical optimizations and compare to re-running Brandes' algorithm after every batch?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["LinkedIn","Snowflake","Two Sigma"]},{"id":"q-678","question":"In a directed acyclic graph with N nodes and M edges, all edge costs are nonnegative. Compute the minimum-cost path from S to T. Costs can decrease online; design a strategy to maintain shortest paths with updates, aiming for sublinear re-computation on average. Provide initial complexity and amortized update complexity, plus memory usage and practical optimizations?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Lyft","Meta","Oracle"]},{"id":"q-690","question":"Design a data structure to support two online operations on an integer array A of length N: 1) rangeAdd(l, r, delta) adds delta to A[i] for l <= i <= r, 2) queryMaxSubarray() returns the maximum subarray sum of the current A. Provide a structure that supports both operations in O(log N) time, describe what to store per node, how to merge children, and how to apply a lazy add. Include correctness and complexity considerations?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Goldman Sachs","Microsoft"]},{"id":"q-700","question":"You're building a real-time analytics dashboard that shows the top-k most frequent event types from a high-volume log stream (e.g., clicks, errors). Each event has a string type. Design a data structure and algorithm to maintain the current top-k frequencies with online increments, aiming for roughly O(log k) update time and O(n) memory. Explain how you handle ties and memory growth, and compare with a naive approach that re-sorts after every insert?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Bloomberg","Google"]},{"id":"q-704","question":"Scenario: a directed graph with nonnegative weights and a fixed source S. Each batch updates up to B edges (increases or decreases). Propose a practical dynamic data structure to maintain exact distances from S to all nodes and answer distance queries S→T in polylog time, with sublinear amortized update time. Compare to rerunning Dijkstra after every batch; include expected bounds, memory usage, and practical heuristics?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Salesforce","Snap"]},{"id":"q-709","question":"Given a directed graph with nonnegative weights, a fixed source S, and a stream of online edge weight updates (both increases and decreases), design a dynamic SSSP data structure that maintains exact distances dist(S, v) for all v after each update. Aim for sublinear amortized update time per edge change; specify initial preprocessing, worst-case vs amortized bounds, memory usage, and practical optimizations. Provide a plan for applying this in a traffic-graph scenario with frequent but localized updates?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["OpenAI","PayPal"]},{"id":"q-721","question":"Given a fixed directed graph with nonnegative weights and a single source S, handle a batch of edge-weight decreases (no insertions/deletions). Design a dynamic algorithm to update the exact S→v distances after each batch with sublinear amortized per-edge cost. Specify data structures, provide an amortized bound, and discuss memory and practical optimizations for real-time traffic networks?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Lyft","NVIDIA","Two Sigma"]},{"id":"q-732","question":"Scenario: A data stream yields integers. At each time step, a new value enters a sliding window of fixed size W, and the oldest value leaves. Design a solution to maintain the top-2 most frequent values in the current window with fast updates. Compare a naive O(W) recompute to an augmented structure using a frequency map and a max-heap with lazy deletions. Provide update and query complexities and memory usage?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Meta","Twitter","Two Sigma"]},{"id":"q-740","question":"Scenario: An edge CDN collects response times in milliseconds for every request. Design a beginner-friendly online algorithm to maintain the median latency as new times arrive, using only inserts. Explain the data structure, update steps, and time/space bounds, assuming up to 1e6 entries?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Cloudflare","Coinbase","Oracle"]},{"id":"q-742","question":"In a DAG with N nodes and M edges, nonnegative edge weights. You maintain shortest-path distances from source S to a fixed set of target nodes {T1,...,Tk}. Edge weights can only decrease over time due to updates. After a batch of updates, you should update only the target distances that can improve, avoiding full re-relaxation. Propose a practical algorithm that lazily propagates decreases using the DAG’s topological order, such that total work across updates is sublinear on average. Provide update and query steps, concrete time bounds, and memory usage, plus optimizations?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Microsoft","Robinhood"]},{"id":"q-755","question":"You're maintaining real-time travel times in a citywide road network modeled as a weighted directed graph with nonnegative costs. Costs can only decrease as new data arrives. Design an incremental algorithm to keep the shortest-path distances from a fixed hub S to all nodes up-to-date after each edge-cost decrease, aiming for sublinear amortized update work. Provide initial SSSP complexity, amortized per-decrease update, memory usage, and practical optimization strategies?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Discord","Google","Zoom"]},{"id":"q-766","question":"## Prompt\n\nIn a dynamic directed graph G=(V,E) with nonnegative weights, edge latencies only decrease in batches. Design a practical, production-ready algorithm to maintain a (1+ε)-approximate SSSP tree from a source S under these updates, enabling distance queries dist(S,v) in O(log|V|) time. Target sublinear amortized update in |E| for batch updates, and linear space. Explain data structures, update bounds, and how you bound cascade effects?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Discord","Meta","OpenAI"]},{"id":"q-770","question":"Given a directed graph G=(V,E) with |V|=N and |E|=M, nonnegative weights, support online operations: 1) decreaseWeight(u,v,delta) with delta>0, 2) queryShortestPath(S,T) returning current shortest path length. Updates and queries are interleaved. Propose a data-structure and algorithm that achieves sublinear amortized reprocessing per update, justify amortized bounds, and discuss space and practical optimizations for massive graphs (N up to 1e6, M up to 1e7)?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Citadel","Lyft"]},{"id":"q-779","question":"You're building a real-time analytics component for a fintech platform. You must maintain the number of distinct values in the most recent W events in a streaming fashion. Implement two operations: append(v) to push a new event value, and distinctCount() to return the number of unique values among the last W events. Assume values are integers up to 1e9 and W can be large. Provide a simple approach with its time/memory costs, then describe an amortized-constant-time solution using a hashmap plus a circular buffer, and discuss edge cases (e.g., large W, many duplicates). How would you implement and analyze it?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Databricks","LinkedIn","Robinhood"]},{"id":"q-787","question":"You maintain N players with integer scores in the range 0..10000. You must support two online operations: 1) update(i, s) — set player i's score to s; 2) countLE(X) — return how many players have score <= X. Propose a data structure and algorithm to support both in O(log V) time per operation, where V=10001, and analyze space usage. Include initialization and a brief correctness sketch?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Apple","Cloudflare","Snowflake"]},{"id":"q-796","question":"Given an array A of length N with integers in range [0, R). You implement a function to count distinct values by inserting each A[i] into a hash set, then return its size. 1) What is the time and space complexity in terms of N and D (distinct values)? 2) If R is small, propose a memory-efficient alternative and analyze its tradeoffs?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["DoorDash","Netflix","PayPal"]},{"id":"q-803","question":"In a directed graph G=(V,E) with N nodes, M edges and nonnegative weights, a fixed source S, and an SSSP tree T. Edges can change weight online (increase or decrease), but no edges are added or removed. Propose a concrete, implementable strategy to maintain the SSSP efficiently, including data structures, update rules, and expected time bounds. Provide preprocessing, amortized per-update, and memory usage, plus practical optimizations and a concrete scenario where it shines (e.g., streaming latency updates)?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Instacart","LinkedIn"]},{"id":"q-808","question":"Dynamic path counting in a DAG: maintain the number of S→T paths of length at most L under online edge insertions and deletions. Propose a data structure and amortized update time bound in terms of N, M, L and #updates; discuss memory usage and how to handle large L and modulo arithmetic in practice?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Citadel","Snowflake","Zoom"]},{"id":"q-817","question":"Design a dynamic, multi-source shortest-path maintenance scheme for a directed graph with nonnegative weights. A fixed set of K hub nodes H must always have exact shortest-path distances to all nodes. Edges can be inserted or weights decreased online, in batches of size at most B. Provide initial preprocessing and a full-update algorithm, with (i) initial time, (ii) amortized update time per batch, and (iii) memory usage. Include two practical optimizations and compare to recomputing from scratch after each batch?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Bloomberg","PayPal","Uber"]},{"id":"q-827","question":"Design a dynamic distance labeling scheme for an undirected weighted graph that supports edge insertions and deletions online. Use a fixed hub set H to enable dist(u,v) queries via dist(u,v)=min_h dist(u,h)+dist(h,v) only if H covers all shortest paths. Explain maintenance of hub distances under updates, and bound update/query times and memory usage. Provide two optimizations and a comparison to recomputing from scratch?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Google","Meta","Microsoft"]},{"id":"q-831","question":"In an undirected weighted graph G=(V,E) with nonnegative weights, design a (1+ε)-approximate distance oracle based on a fixed landmark set L (|L|=k). Edges are only inserted online in batches of size B; no deletions. After each batch, specify: (i) preprocessing time and space to build distances from every landmark, (ii) amortized update time per batch to update the oracle, (iii) query time for dist(u,v), (iv) total memory, (v) two practical optimizations, (vi) a comparison to rebuilding all-pairs distances after each batch. Provide concrete asymptotics in terms of n=|V|, m=|E|, k, ε, B?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Instacart","Microsoft","NVIDIA"]},{"id":"q-1028","question":"Your team runs a multi-tenant SaaS analytics platform on Kubernetes in AWS. Each tenant lives in an isolated namespace with 100+ microservices. Explain how you enforce least privilege RBAC, ephemeral credentials with automatic rotation, secrets management, mTLS/workload identity (SPIFFE/SPIRE), policy-driven runtime enforcement, and a playbook for detecting and responding to cross-tenant data exfiltration?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Airbnb","Google","Meta"]},{"id":"q-1034","question":"In a multi-region SaaS platform running 3,000 microservices on Kubernetes in AWS with a shared data lake, design a secure data-access flow that enforces least privilege, uses workload identity (SPIFFE/SPIRE), ephemeral credentials with rotation, and policy-driven runtime checks. Include tenant isolation, secret management, and a playbook for cross-tenant data exfiltration?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Netflix","PayPal","Zoom"]},{"id":"q-1079","question":"In a multi-tenant rideshare analytics platform with per-tenant clusters and a shared data lake, design a zero-trust data-access pattern that enforces least privilege, uses SPIFFE/SPIRE for workload identity, ephemeral credentials with rotation, and policy-driven runtime checks. Include tenant isolation, secret management, cross-tenant data-exfiltration playbooks, and a DR/IR plan?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Lyft","Snap"]},{"id":"q-1315","question":"Scenario: A Databricks-based analytics platform on AWS feeds Slack alerts. Engineers share notebooks and data assets across teams with minimal controls. Propose a practical, beginner-friendly security plan that enforces least privilege using Unity Catalog RBAC, secrets management with rotation, and a CI check to block secrets in code, plus a 48-hour incident playbook for credential leakage or cross-tenant exposure?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Databricks","Slack"]},{"id":"q-1362","question":"Scenario: A multi-tenant data platform serving Apple, Adobe, and Snowflake runs data jobs in a shared Kubernetes cluster with per-tenant namespaces. An operator could pivot roles to access other tenants. Design a practical access-control plan enforcing least privilege via ABAC, dynamic approvals for sensitive actions, policy-drift detection, and a 24–72 hour rollback/incidence playbook. Include tools like OPA, AWS IAM, Kubernetes RBAC, Vault?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Adobe","Apple","Snowflake"]},{"id":"q-1379","question":"Scenario: A multi-tenant SaaS on Kubernetes in AWS uses a shared MongoDB Atlas cluster and Vault-managed secrets; Oracle reports data. Design a practical plan to enforce least privilege, issue per-tenant Vault DB credentials with short lifetimes, bound to tenant service accounts, include CI checks to block secrets in code, and a 48-hour IR playbook for credential leakage?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Hashicorp","MongoDB","Oracle"]},{"id":"q-1406","question":"In a multi-tenant data platform on AWS with a shared data lake and event bus, outline a practical plan to prevent cross-tenant leakage as data and events flow through data-plane and message-plane. Include (1) ABAC with OPA, (2) SPIRE-like identities and mutual TLS, (3) ephemeral credentials with rotation to data stores and topics, (4) drift detection/remediation, and (5) a 72-hour IR playbook for credential leakage or cross-tenant exposure, with concrete tool choices and trade-offs?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Hashicorp","Plaid","Twitter"]},{"id":"q-1501","question":"In a Slack-like multi-tenant platform deployed on Kubernetes in AWS with a central analytics data lake, a leaked tenant admin token is used to attempt cross-tenant data access via the data API. Design a practical, intermediate-level test plan to validate and harden per-tenant isolation. Include concrete abuse vectors to test, enforcement via RBAC/ABAC/OPA, SPIRE/mTLS, ephemeral credentials, and per-tenant secrets; outline CI checks, observability gates, and a concise IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Citadel","Slack","Square"]},{"id":"q-1529","question":"In a Kubernetes-based multi-tenant analytics platform on AWS where data and logs traverse a shared data plane to a central observability stack, design a production-ready plan to prevent cross-tenant data leakage via log/metric pipelines. Include enforcement with RBAC/ABAC/OPA, SPIRE/mTLS, ephemeral credentials, and per-tenant secrets; detail CI checks, observability gates, and an IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Lyft","Meta","MongoDB"]},{"id":"q-1686","question":"Context: A beginner security analyst joins a multi-tenant SaaS deployed on Google Cloud. The platform runs on GKE and a central data lake; tenants are isolated by per-tenant namespaces with restricted data access. Secrets live in Secret Manager and workloads use Cloud IAM + Workload Identity; CI/CD has a secret-scanning step. Design a practical, beginner-friendly security plan to enforce least privilege, implement secrets rotation, add a CI check to block secrets in code, and prepare a 48-hour incident playbook for credential leakage or cross-tenant exposure. Specify concrete steps, tooling, and success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Google","Instacart","Robinhood"]},{"id":"q-1889","question":"In a global multi-tenant analytics platform on Kubernetes with a shared data lake, design an advanced, concrete plan to verify and harden per-tenant isolation against cross-tenant data access, focusing on dynamic ABAC with OPA, SPIRE/mTLS, ephemeral credentials, and per-tenant secrets. Include abuse vectors, CI gates, observability checks, and an IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Cloudflare","Meta","Zoom"]},{"id":"q-1938","question":"In a beginner security role for a multi-tenant delivery platform on AWS, tenants store order data in per-tenant prefixes under a shared data lake. A developer accidentally attaches a Lambda execution role with broad access to all buckets. Propose a practical, beginner-friendly security plan to enforce least privilege with per-tenant IAM roles and bucket policies, implement per-tenant key rotation via Secrets Manager, and add a CI check to block broad policies before merges, plus a 48-hour incident playbook for credential leakage or cross-tenant exposure. Include concrete steps and success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["DoorDash","Hashicorp","Instacart"]},{"id":"q-2000","question":"In a Kubernetes cluster utilizing MIG partitions for shared NVIDIA GPUs, how would you enforce per-tenant GPU isolation and protect models and data from cross-tenant leakage? Outline architecture, policy controls (RBAC/ABAC/OPA), SPIRE/mTLS, ephemeral credentials, per-tenant secrets; include CI tests, observability gates, and a concise IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["NVIDIA","Square"]},{"id":"q-2091","question":"Design a practical per-tenant data-sharing broker for a SaaS analytics platform that must allow approved external collaborators to access only specific tenant datasets for a limited time. Propose the key-management model (per-tenant CMKs and envelope keys), a policy-driven issuance flow (OPA), service identities (SPIRE), and short-lived credentials. Include rotation cadence, audit/logging, and a concise IR/DR plan with success criteria. Give concrete steps and success metrics?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Amazon","Bloomberg","Databricks"]},{"id":"q-2114","question":"In a multi-tenant SaaS on AWS with a Kubernetes cluster and a shared artifact store, how would you harden the CI/CD pipeline to prevent tenant-confusion and supply-chain abuse during builds and releases? Include architecture: per-tenant runners, SPIRE/mTLS, Vault tenant roles, image signing, OPA policy checks, SBOM validation, per-tenant secret rotation, plus concrete test vectors and an IR/DR plan with measurable success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Apple","Snowflake"]},{"id":"q-2141","question":"In a cloud-native platform deployed to Kubernetes with GitOps pipelines, design an intermediate-level test plan to secure the software supply chain end-to-end, including SBOM provenance (SLSA), per-tenant secrets, and policy enforcement with OPA; outline CI gates, runtime observability, and an IR/DR playbook for a compromised dependency or insertions into the build process?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Cloudflare","Microsoft","Tesla"]},{"id":"q-883","question":"Describe a practical, scalable secure software supply-chain workflow for an automotive platform with OTA-enabled ECUs, leveraging SBOMs, code signing, and runtime attestation. Include concrete steps for CI/CD isolation, secret management, artifact signing, and incident response?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Hashicorp","Salesforce","Tesla"]},{"id":"q-974","question":"Given a global fintech platform delivering real-time risk analytics via a fleet of edge appliances and cloud microservices, outline a practical, end-to-end security workflow for secure software and firmware updates that ensures provenance, integrity, and trust. Include: SBOMs, code signing, runtime attestation for both containers and devices; CI/CD isolation and secrets management; artifact signing and key rotation; secure OTA rollout with canarying and rollback; and incident response playbooks?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Goldman Sachs","Meta","Tesla"]},{"id":"q-988","question":"Scenario: A fintech app runs microservices in Kubernetes on AWS and Cloud Run on GCP. You must implement least privilege and dynamic secrets for a data-access service. Outline a practical, beginner-friendly workflow to give the service temporary credentials to a Postgres DB, with a choice between Vault or cloud-native secret managers. Include CI/CD integration, rotation, RBAC, and auditability?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Bloomberg","Citadel","Cloudflare"]},{"id":"q-1018","question":"You’re building a mobile camera app that auto-captures frames from a live video feed. Describe a practical, beginner-friendly pipeline to decide whether a frame is usable by applying two simple checks: (1) sharpness via variance of Laplacian, (2) exposure via histogram-based brightness. Explain how thresholds would be chosen, how you'd adapt them across lighting, and provide a minimal code snippet illustrating the core checks?","channel":"computer-vision","subChannel":"general","difficulty":"beginner","tags":["computer-vision"],"companies":["Goldman Sachs","LinkedIn","NVIDIA"]},{"id":"q-1093","question":"Design a privacy-preserving, real-time hand-gesture recognition system for video calls on consumer laptops (720p camera) that distinguishes a small set of gestures (hand-raise, thumbs-up, peace) without exposing facial details. Must run on-device at 30–60 FPS, handle lighting/occlusion, and support federated fine-tuning with differential privacy. Outline architecture, data strategy, and evaluation plan?","channel":"computer-vision","subChannel":"general","difficulty":"intermediate","tags":["computer-vision"],"companies":["Hashicorp","Tesla","Zoom"]},{"id":"q-1108","question":"Design a real-time, on-device hand pose/gesture system for air-drawing in a video-conferencing app. From a monocular 1080p60 stream, infer 2D/3D hand pose with <40ms latency on a CPU, robust to occlusion and varied skin tones, and support at least 5 gestures (draw, erase, next, previous, pointer). Outline data needs, model architecture, latency optimizations, temporal consistency, and evaluation plan?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Adobe","Plaid","Zoom"]},{"id":"q-1270","question":"Design a real-time monocular 3D detector for an assembly line that estimates 6-DoF pose of tools from a single RGB camera, achieving sub-50ms per frame on embedded hardware. Use a lightweight backbone with self-supervised pretraining plus a small labeled set; recover pose via differentiable PnP from 2D-3D correspondences with a temporal filter and reprojection losses. Monitor drift with streaming per-frame errors?","channel":"computer-vision","subChannel":"general","difficulty":"intermediate","tags":["computer-vision"],"companies":["Goldman Sachs","Hashicorp","MongoDB"]},{"id":"q-1417","question":"Given a single RGB camera mounted on an autonomous delivery drone operating in urban environments, design a real-time system to detect and track pedestrians and other vulnerable actors at 30 fps under nighttime and rain conditions. Propose data strategy (synthetic rain + real), model backbone, temporal fusion and latency targets, and safety/failover mechanisms?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Amazon","Cloudflare","Uber"]},{"id":"q-1435","question":"For aerial inspection of solar farms, design a CV system to detect and grade micro-cracks on solar panels from drone video with limited labeled data. Specify an architecture that detects tiny defects, data-augmentation strategies (synthetic crack overlays, texture randomization), domain adaptation, and an edge-friendly output (box, segmentation mask, and severity score). Include evaluation protocol and latency targets?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Bloomberg","MongoDB","Scale Ai"]},{"id":"q-1861","question":"You're given an overhead RGB image of a table with scattered coins. Design a beginner-friendly pipeline to count the number of coins and estimate their approximate denomination from a single image. Include preprocessing, circle/contour detection, radius-based grouping to separate coin types, handling shadows, and a minimal code sketch using OpenCV to detect circular shapes?","channel":"computer-vision","subChannel":"general","difficulty":"beginner","tags":["computer-vision"],"companies":["Goldman Sachs","Instacart","Snowflake"]},{"id":"q-1945","question":"Design a real-time anomaly-detection pipeline for a single RGB camera monitoring an industrial warehouse. Propose a memory-augmented autoencoder approach that runs on an edge device at 15–20 FPS, uses frame-wise reconstruction error plus optical-flow residuals, maintains a fixed-size normal-pattern memory, and includes a drift-adaptation strategy with minimal labeling?","channel":"computer-vision","subChannel":"general","difficulty":"intermediate","tags":["computer-vision"],"companies":["Hashicorp","Hugging Face"]},{"id":"q-2131","question":"In a factory setting, design a real-time 2-view RGB tool-pose tracking system that achieves sub-60ms per frame on edge hardware. Use two synchronized cameras, a lightweight backbone, and a differentiable PnP with 2D-3D correspondences; include cross-view fusion, a temporal filter, and a self-supervised pretraining strategy with synthetic data. How would you validate drift and occlusion resilience?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Adobe","OpenAI","Two Sigma"]},{"id":"q-456","question":"How would you design a real-time object detection system for a social media platform that processes 10M images/day with 99.9% accuracy and <100ms latency?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Microsoft","Salesforce","Twitter"]},{"id":"q-487","question":"Design a real-time object detection system for DoorDash delivery vehicles that must identify packages, license plates, and traffic signs in varying weather conditions. How would you handle model optimization for edge deployment and ensure 99% accuracy?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["DoorDash","Google"]},{"id":"q-517","question":"Design a real-time object detection system for cryptocurrency trading terminals that must detect and classify multiple monitor types, trading interfaces, and unauthorized screen recording devices with <100ms latency. How would you optimize YOLOv8 for this specific use case?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Amazon","Coinbase","Meta"]},{"id":"q-545","question":"How would you detect if an image contains a face using basic computer vision techniques?","channel":"computer-vision","subChannel":"general","difficulty":"beginner","tags":["computer-vision"],"companies":["Airbnb","NVIDIA","Netflix"]},{"id":"q-570","question":"How would you design a real-time object detection system for Airbnb's property listing photos that can identify amenities and safety violations while processing 10,000 images per hour?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Airbnb","Instacart"]},{"id":"q-274","question":"How would you implement a hybrid CNN architecture combining ResNet residual connections with EfficientNet compound scaling for production image classification?","channel":"computer-vision","subChannel":"image-classification","difficulty":"intermediate","tags":["cnn","resnet","efficientnet"],"companies":["Amazon","Google","Meta","Microsoft"]},{"id":"q-253","question":"How does YOLO implement real-time object detection using grid-based prediction and what are the key components of its architecture?","channel":"computer-vision","subChannel":"object-detection","difficulty":"beginner","tags":["yolo","rcnn","detr"],"companies":["Amazon","Google","Meta","Microsoft","NVIDIA","Tesla"]},{"id":"q-200","question":"How does U-Net's skip connection architecture enable precise medical image segmentation?","channel":"computer-vision","subChannel":"segmentation","difficulty":"beginner","tags":["unet","mask-rcnn","sam"],"companies":["Amazon","Google","Meta"]},{"id":"q-228","question":"How would you optimize a real-time medical image segmentation pipeline using SAM with 100ms latency constraint on edge devices?","channel":"computer-vision","subChannel":"segmentation","difficulty":"advanced","tags":["unet","mask-rcnn","sam"],"companies":["Apple","Google","Meta","Microsoft","NVIDIA"]},{"id":"q-1080","question":"In a log-processing pipeline, multiple producers enqueue log entries into a bounded, rate-limited work queue. Implement a beginner-friendly token-bucket rate limiter that allows enqueuing only when a token is available; a background timer refills tokens at a fixed rate up to a max. Producers block when tokens==0; workers process items from the queue. Provide a concrete Go/Python/Java solution and discuss testing?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Bloomberg","Lyft","NVIDIA"]},{"id":"q-1204","question":"In a real-time notification system with thousands of tenants, each tenant's events must be delivered to their own handler in order, while cross-tenant processing happens in parallel. Design a bounded, multi-queue architecture with per-tenant in-order guarantees, backpressure, and graceful shutdown. Compare two backpressure strategies (per-tenant token buckets vs. global credits) and discuss testing and failure scenarios. Provide runnable sketch in Go or Rust?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Scale Ai","Tesla"]},{"id":"q-2080","question":"In a real-time chat system, multiple producers enqueue messages into per-room bounded queues and multiple consumers deliver to connected clients. Design a beginner-friendly concurrency solution (Go, Java, or Python) that guarantees producers block when a room's queue is full and consumers block when the queue is empty, while preserving per-room isolation and simple backpressure. Compare a mutex/condition-based queue versus a channel-based approach for bounded queues?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Discord","Hashicorp"]},{"id":"q-682","question":"In a service handling image uploads, each file triggers a resize and thumbnail generation pipeline. Design a bounded producer-consumer queue in Python using asyncio. Use a Queue with maxsize, a fixed number of worker coroutines, and backpressure so producers await when full. Include clean shutdown and error handling. Provide a runnable minimal example showing enqueue, worker loop, and cancellation?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Discord","Lyft","Snap"]},{"id":"q-687","question":"Write a small Python snippet: create a shared counter initialized to 0, spawn 4 threads that increment it 1000 times each, using a Lock to protect the increment. After joining, print the final value. Explain what happens if the lock is removed and how atomicity is ensured. What value do you expect and why?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Google","Tesla","Two Sigma"]},{"id":"q-696","question":"Context: in a real-time analytics pipeline for a video-conference system, dozens of producers emit events into a shared queue and multiple workers consume them. Implement a bounded, multi-producer, multi-consumer queue with capacity 1024. It must not drop messages while not full, block producers when full, support concurrent consumers, and support a clean shutdown. Describe API, invariants, and a simple synchronization strategy?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Hugging Face","Zoom"]},{"id":"q-708","question":"Design a bounded, concurrent, multi-priority work scheduler for a rendering service: 3 priority levels (0 highest). Producers enqueue tasks into per-priority queues with backpressure; workers drain from the highest non-empty queue (0→1→2). Include aging to prevent starvation and a graceful shutdown sentinel. Provide a runnable minimal Java example?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Adobe","Salesforce","Snap"]},{"id":"q-713","question":"You’re building a web service that enqueues tasks from many request handlers into a shared, fixed-capacity queue consumed by a single worker. Implement a thread-safe bounded queue with a fixed circular buffer, a mutex, not_full and not_empty condition variables, and a shutdown signal. Show enqueue and dequeue semantics and how shutdown behaves?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Adobe","Cloudflare","Tesla"]},{"id":"q-720","question":"Design a bounded worker pool for a high-throughput API gateway that queues work with backpressure. Use a lock-free ring buffer, N workers, and blocking enqueue when full. Include per-task cancellation, timeouts, and metrics. Compare Go, Rust, and Java trade-offs and explain how you’d debug data races and starvation under burst traffic?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Hugging Face","Square"]},{"id":"q-726","question":"Design a concurrent event broker for a live chat service that guarantees per-user in-order delivery while allowing parallel processing across users, using a bounded buffer; describe backpressure handling when the buffer fills, and compare locking versus lock-free approaches in your language of choice?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Meta","Microsoft","Zoom"]},{"id":"q-735","question":"Design a concurrency-safe per-channel message queue for a Discord-like chat service: multiple producers push messages, a single consumer persists them to storage; implement bounded capacity, preserve per-channel order, and handle backpressure. Which synchronization primitives would you use and how would you test edge cases?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Discord","Snowflake"]},{"id":"q-748","question":"You're building a real-time analytics pipeline with many shards. Propose a concurrent, bounded path that preserves per-shard in-order processing while enabling cross-shard parallelism. Design a data structure and protocol (enqueuing, routing, backpressure, and shutdown). Provide a runnable sketch in Go or Rust showing enqueue, per-shard consumer loop, and graceful shutdown?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["NVIDIA","Snap","Twitter"]},{"id":"q-756","question":"Implement a concurrent task-graph executor with dependencies. Tasks form a DAG; a task runs only after all its prerequisites complete. Use a bounded in-flight task pool, per-worker work stealing, and deadlock/backpressure handling. Provide a runnable Go or Rust sketch showing submission, ready-state transitions, worker loop, and graceful shutdown?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Lyft","NVIDIA"]},{"id":"q-759","question":"In a multi-threaded microservice, there is a shared in-memory counter for total processed events. Provide a concrete, beginner-friendly approach to implement a thread-safe increment using language primitives (Java, Go, or Python) and discuss the trade-offs between lock-based vs lock-free solutions when scaling across cores?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Bloomberg","Coinbase","MongoDB"]},{"id":"q-773","question":"In a Go HTTP server, you maintain a per-user quota counter in memory. Multiple requests for different users should advance concurrently, but updates to the same user must be serialized. Design a striped lock (a fixed set of mutexes) to guard a map[string]int, map userID to a bucket via hashing, and explain how you minimize contention, handle hash collisions, and ensure correctness when entries may be evicted?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["MongoDB","NVIDIA","Plaid"]},{"id":"q-780","question":"Implement a lock-free, bounded multi-producer, multi-consumer pipeline with per-symbol partitions and fixed-capacity queues. Producers must acquire credits to enqueue; downstream workers release credits on completion. Explain memory visibility, false sharing avoidance, and a clean shutdown. How do you guarantee in-order per partition and exactly-once processing on failure?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Bloomberg","Microsoft","PayPal"]},{"id":"q-788","question":"In a streaming data pipeline, multiple producers generate frames and a bounded buffer sits between the producer stage and a consumer stage. Implement a backpressure-enabled producer-consumer pair in JavaScript (Node.js) using an async bounded queue that supports multiple producers and multiple consumers. It must guarantee no data loss, bounded memory, and graceful shutdown. How would you test under varying production/consumption rates?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Citadel","Tesla"]},{"id":"q-795","question":"Design a dynamic, concurrency-safe worker pool in Go for a real-time analytics pipeline. Each task type has its own queue; enforce backpressure with bounded channels and implement a central scaler that adjusts worker counts based on observed tail latency and throughput. Provide a runnable skeleton showing enqueue, worker loops, and graceful shutdown?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Databricks","Meta","Microsoft"]},{"id":"q-804","question":"You're building a video-frame ingestion service where frames from thousands of users arrive on a shared input channel. Design a bounded, concurrent dispatcher that routes frames to per-user in-order queues, enabling parallel processing across users. Provide backpressure handling when the global buffer fills, a strategy for reordering out-of-order frames, and a graceful shutdown. Sketch the core data structures and synchronization in Rust or C++ and explain trade-offs?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Hugging Face","Netflix","Snap"]},{"id":"q-811","question":"In a real-time analytics service, multiple producers enqueue data into a fixed-size circular buffer consumed by multiple workers. Design a beginner-friendly concurrency solution in Java, Go, or Python that guarantees producers block when the buffer is full and consumers block when empty, while maintaining correctness under concurrent access; compare lock-based vs channel-based approaches for this bounded buffer?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Databricks","Oracle","Two Sigma"]},{"id":"q-818","question":"In a build pipeline, N compile tasks run in parallel and must all finish before the linker runs. Design a barrier that blocks each task at barrier.wait() until all N tasks have arrived, then releases them to proceed. Implement two beginner-friendly approaches in Go (or Java/Python): (A) locks/condition variables; (B) channels/futures. Ensure the barrier is reusable for repeated builds, avoids deadlocks, and explain correctness and trade-offs?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Discord","Two Sigma"]},{"id":"q-825","question":"In a real-time chat moderation pipeline for a platform like Discord/LinkedIn, design a dynamic Go worker pool that scales from minWorkers to maxWorkers based on a bounded task queue. The queue should backpressure producers, guarantee per-user fairness, support graceful shutdown, and tolerate occasional misbehaving workers (timeouts). Provide runnable code showing enqueue, worker loop, and scaler?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Discord","LinkedIn"]},{"id":"q-835","question":"Design a multi-tenant dispatcher for a streaming event bus. Producers from each tenant push events into per-tenant input channels; a central dispatcher interleaves tenants fairly, enforces per-tenant rate limits using a token-bucket, and enforces a maximum in-flight events per tenant. Implement in Go using channels; ensure backpressure propagates to producers when quotas or in-flight limits are reached, and support graceful shutdown. Include a runnable minimal example with: per-tenant limiter, dispatcher loop, producer(s), and cancellation?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Airbnb","NVIDIA","Slack"]},{"id":"q-1027","question":"You're running a mixed Consul Connect mesh with Kubernetes services in DC1 and VM-based services in DC2. A new API service in DC1 calls a legacy VM backend behind a firewall via a mesh gateway, but TLS handshakes intermittently fail after a CA rotation. Propose a zero-downtime plan to diagnose, implement automatic CA rotation, and validate end-to-end, including config changes, monitoring, and rollback steps?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Cloudflare","LinkedIn","Microsoft"]},{"id":"q-1178","question":"Across a tri-cloud Consul Connect mesh, a new microservice 'payments-api' in namespace 'payments' must reach a legacy data service 'orderdb' in namespace 'legacy' via a mesh gateway. Propose an end-to-end pattern that enforces strict identity via namespace-scoped Intention defaults, per-service tokens, and gateway ACLs, including resource definitions, deployment steps, and rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Anthropic","DoorDash","Tesla"]},{"id":"q-1293","question":"In a hybrid setup with Consul across two Kubernetes clusters (AWS) and VM-based services on-prem, how would you design cross-cluster service authentication and discovery using Consul Connect with mesh gateways, Namespaces, and ACLs to enforce zero-trust policy and automatic token rotation while preserving DNS-based discovery?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Microsoft","Snowflake"]},{"id":"q-1334","question":"Scenario: You operate a mixed environment with Consul Connect across Kubernetes namespaces dev (auth-service) and prod (user-service). How would you implement an Intentions policy that allows auth-service to call user-service only on port 8080, with default deny for all other traffic, and enable audit logging? Describe the commands, tokens, and how you would verify enforcement from a small client pod?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["NVIDIA","Robinhood","Snap"]},{"id":"q-1348","question":"Scenario: A global application runs in three environments (prod, staging, dev) across Kubernetes in GCP and legacy VM services on‑prem, all using Consul Connect. Design a zero-trust mesh that uses Vault for dynamic secrets, namespace-scoped ACL tokens, and mesh gateways for cross-cluster traffic. Explain how you would rotate credentials automatically, enforce service-to-service ACLs, and validate safety during partial outages?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["IBM","MongoDB"]},{"id":"q-1369","question":"Three-datacenter Consul Connect mesh (DC1 Kubernetes, DC2 VM, DC3 on‑prem) needs zero-downtime TLS credential rotation using Vault PKI and Consul CA integration. Describe a concrete plan including Vault roles, TTLs, renewal cadence, root CA distribution, per-service identity, and a rollback procedure with deployment steps and rollback commands?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Google","MongoDB"]},{"id":"q-1408","question":"Design a zero-downtime certificate rotation strategy for a Consul Connect mesh spanning Kubernetes and VM-based services across two clusters. How would you coordinate CA rotation, per-service identity tokens, and mesh gateway trust so that mutual-TLS remains valid during rotation and new services automatically trust the updated CA without downtime?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Google","Twitter","Two Sigma"]},{"id":"q-1429","question":"In a Consul Connect mesh spanning two Kubernetes clusters, a frontend service in cluster A must call a private external billing API behind a VPC. The API requires mTLS and tenant-scoped API keys that rotate every 24 hours. Design a scalable egress pattern using a Mesh Gateway for outbound traffic, per-service Intentions with explicit allow rules, TLS credential rotation via Vault, and a rollback plan. Include sample manifests, deployment steps, and failure recovery?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Discord","Twitter"]},{"id":"q-1465","question":"In a Consul Connect mesh spanning Kubernetes and VM workloads, an external analytics service must fetch data from internal microservices via mesh gateways but cannot run a sidecar. Design a secure bridge pattern: per-request identity, TLS with a bridge certificate minted by Consul, short-lived credentials rotated automatically, and revocation without downtime. Include concrete resource definitions, deployment steps, and rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Citadel","NVIDIA","Two Sigma"]},{"id":"q-1470","question":"In a multi-cluster Consul Connect mesh spanning Kubernetes and VMs, design a tenant-aware, dynamic access control model where new tenants join/leave at runtime without service downtime. How would you implement per-tenant namespaces, ephemeral tokens, and policy synchronization to enforce zero-trust across tenant boundaries? Include concrete steps and constraints?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Goldman Sachs","Square","Tesla"]},{"id":"q-1525","question":"In a Consul Connect mesh, two services communicate over gRPC. One is in Kubernetes, the other is VM-based without a sidecar. Design a secure bridge via a mesh gateway to enable mutual-TLS and per-request identity, including how to configure the gRPC wiring, gateway policy, and a minimal manifest for the gateway and client. Include a rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Airbnb","IBM","Meta"]},{"id":"q-1669","question":"In a single cluster with two namespaces, dev and prod, a frontend service in dev must call a backend service in prod through a Consul mesh gateway. Design a beginner-friendly end-to-end setup: register both services, enable a mesh gateway in prod, add a route, ensure mTLS with short-lived certs, and include a rollback plan if the gateway fails. Include concrete YAML fragments and commands?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Hashicorp","Hugging Face","Robinhood"]},{"id":"q-1693","question":"In a Consul Connect mesh spanning Kubernetes and VM workloads, how would you implement scalable, per-service access with automatic token rotation and zero-downtime revocation for an external analytics app calling internal services via mesh gateways? Include identity model (SPIFFE IDs), Vault-based token lifecycle, per-service ACLs, mesh-gateway configuration, and a rolling update plus rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Apple","Scale Ai","Square"]},{"id":"q-1795","question":"In a Consul Connect-enabled Kubernetes cluster, a new Python gRPC service must call a legacy on-prem TCP service via a mesh gateway using TCP mode (no HTTP). Draft a minimal setup: (1) service definitions and ACLs, (2) mesh gateway TCP config, (3) per-service identity, tokens, and rotation, (4) a rollback plan if handshake fails?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Oracle","Tesla","Twitter"]},{"id":"q-1849","question":"In a Consul Connect mesh spanning Kubernetes in DC1 and VM-based services in DC2, a compromised service identified by SPIFFE ID spiffe://example.org/compromised-frontend.dc1 must be revoked mesh-wide within minutes. Design a reproducible, low-downtime remediation workflow: revoke tokens, rotate TLS certs, refresh CA bundles on mesh gateways, enforce temporary ACL lockdown, and verify no unauthorized calls remain. Include concrete resource definitions, rotation steps, and rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Apple","Two Sigma"]},{"id":"q-2031","question":"Design a three-region Consul Connect mesh where 500+ services span Kubernetes clusters in US/EU and VM-based services in APAC, with a partner-facing API exposed via a mesh gateway. Describe your approach to: (1) identity and ACL strategy across regions, (2) automatic TLS certificate rotation backed by Vault, (3) WAN federation topology, (4) zero-downtime token revocation, and (5) testing/rollout plan with rollback steps?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Two Sigma","Uber"]},{"id":"q-2065","question":"Scenario: a Kubernetes-based checkout calls inventory. A new canary version inventory-v2 is deployed on both Kubernetes and VM-backed services. Implement a 20% canary split using Consul Connect service-router, gate by a health check, and allow quick rollback to 0% canary. Provide minimal service-router config, deployment labels, and a rollback process with commands to reweight or disable v2?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Amazon","DoorDash","Oracle"]},{"id":"q-2126","question":"Design a per-tenant isolation in a mixed Kubernetes/VM Consul Connect mesh with two tenants (alpha, beta). Implement namespace-scoped ACLs and per-tenant tokens so payments-alpha can access ledger-alpha only, and payments-beta ledger-beta only. Provide service-router rules using identity, a short-lived token rotation plan, and an automated test that verifies cross-tenant access is denied and a rollback path to re-allow access if misconfig is detected?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Snowflake","Tesla","Twitter"]},{"id":"q-880","question":"In a Consul Connect-enabled dev cluster, two services run in namespace dev: 'reviews' and 'ratings'. You want to enforce that only reviews can call ratings via the mesh, with all other cross-service calls denied by default. Describe the minimal service-defaults and service-intentions changes needed, and how you would verify using a small client container in dev?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Amazon","Google","Square"]},{"id":"q-919","question":"You're operating a multi-datacenter Consul Connect mesh. A new API service in DC1 must reach a legacy monolith in DC2 that cannot run a sidecar. Design a cross-datacenter connectivity pattern using a mesh gateway, per-service Intentions with explicit allow rules, and TLS credential rotation. Include resource definitions, deployment steps, and rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Anthropic","Hugging Face","PayPal"]},{"id":"q-999","question":"Design a cross-datacenter Consul Connect pattern in a three-datacenter setup where api-service.dc1 must reach legacy-db.dc3 (no sidecar). Use a MeshGateway to bridge DC1↔DC3, per-service Intentions with explicit allow rules, and TLS credential rotation. Include concrete resource definitions, deployment steps, and a rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["NVIDIA","Salesforce","Two Sigma"]},{"id":"q-1077","question":"Design a data ingestion and processing pipeline for a global ride-hailing platform that ingests 5 TB/day of operational events from multiple regional Kafka topics and batch feeds. Requirements: idempotent upserts into a table (Iceberg/Delta), handle late-arriving events, schema evolution, and partition pruning by country/date. Compare Flink vs Spark for streaming, and outline testing, monitoring, and data quality checks?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Amazon","LinkedIn","Lyft"]},{"id":"q-1140","question":"Given daily 1 GB of web logs in JSON lines stored on S3 with fields user_id, timestamp, path, status, and optional referrer, design a beginner-friendly pipeline (Python or Node.js) that deduplicates by timestamp+user_id+path, validates required fields, normalizes timestamp to UTC, and writes date-partitioned Parquet to a data lake; include basic tests and monitoring?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Discord","NVIDIA","Stripe"]},{"id":"q-1272","question":"Design a data pipeline to ingest 2M GPU telemetry events per minute from a global fleet of AI training clusters into a data lake and a feature store. Events include host_id, region, timestamp, metric_type, and value. Requirements: immutable raw Parquet storage partitioned by region/hour; near-real-time metrics and anomaly alerts (1–2 minute latency) via a streaming engine; idempotent upserts into a feature store; schema evolution handling; late-arriving data; cost-aware storage/compute; monitoring and tests; compare Spark vs Flink for streaming components?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["Apple","NVIDIA"]},{"id":"q-1498","question":"Design a global data ingestion and governance pipeline for a real-time ad-tech platform that processes 200k events/sec from 3 cloud regions into a centralized lakehouse. Each event has event_id, tenant_id, timestamp, event_type, and payload. Requirements: enforce data contracts via a central registry (schema + compatibility rules), support schema evolution with automatic catalog updates, and ensure multi-tenant data isolation and access control. Implement partitioning by tenant/date, handle late-arriving data within a 1–2 minute SLA, ensure data lineage and quality checks, and provide rollback semantics for contracts. Compare Iceberg vs Delta as the storage layer, outline testing/monitoring, and describe concrete example schemas and contract definitions. Include how you'd validate end-to-end?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Scale Ai","Tesla"]},{"id":"q-1692","question":"In a social app generating 100 GB/day of newline-delimited JSON events across 3 regions stored in S3, design a beginner-friendly batch pipeline that validates fields (event_id, user_id, timestamp, event_type), derives date, deduplicates by event_id, hashes user_id for privacy, and writes Parquet partitioned by date to a data lake. Compute a daily data-quality score (0-1) based on missing/invalid fields and store it in a metadata table. Outline tooling, testing, and monitoring?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Anthropic","PayPal","Square"]},{"id":"q-1701","question":"Ingest 3 TB/day of regional event data from Kafka (EU/US/APAC) with user_id, session_id, event_type, timestamp, and attributes. Design a privacy-first analytics pipeline: per-region salt pseudonymization of user_id before cross-region joins, idempotent upserts into Apache Iceberg, event-time processing with 15-minute tumbling windows and 1-hour late data, and immutable audits and data contracts. Compare Spark Structured Streaming vs Flink for the streaming layer, and specify GDPR masking after aggregation?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Google","Lyft"]},{"id":"q-1928","question":"Design a cost-aware real-time ingestion pipeline that processes 50 TB/day of clickstream data from multiple sources (Kafka topics and batch feeds) into a lakehouse. Ensure idempotent upserts into an Iceberg/Delta table, handle late-arriving events, and support schema evolution with partition pruning by date and region. Compare Spark Structured Streaming vs Flink for the path and outline data quality checks, testing, and monitoring strategies?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["NVIDIA","Robinhood","Two Sigma"]},{"id":"q-1965","question":"Design a GDPR/CCPA-compliant, multi-source data pipeline for an e-commerce analytics platform. Ingest CDC from OLTP databases plus batch feeds, preserve raw data immutably, and enable per-tenant data isolation. Implement lineage via a metadata catalog, apply privacy techniques (PII redaction and differential privacy for aggregates), and support schema evolution. Compare Flink vs Spark for streaming, outline tests, monitoring, and cost implications across regions?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Google","LinkedIn","Robinhood"]},{"id":"q-1971","question":"You run a product analytics pipeline for a global iOS/Android app (telemetry: user_id, device_id, ts, event, properties). Data must be ingested from a MongoDB Atlas source into a lakehouse on S3 using Apache Iceberg. Design an end-to-end pipeline that supports idempotent upserts, late-arriving events, and schema evolution, while enforcing PII masking and GDPR data deletion requests. Compare using Flink vs Spark for streaming, outline testing, monitoring, and data-quality checks?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["Apple","MongoDB"]},{"id":"q-457","question":"You need to process 10GB of CSV files daily and load them into a PostgreSQL database. The files contain user activity logs with timestamps, user IDs, and event types. How would you design an efficient ETL pipeline using Python?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Anthropic","Discord","MongoDB"]},{"id":"q-488","question":"You're building a real-time analytics pipeline for a food delivery app. How would you design a data pipeline to process 1M events/day with 5-minute latency, considering data quality, schema evolution, and cost optimization?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["Adobe","Google","Instacart"]},{"id":"q-518","question":"You have a 10GB CSV file with user activity logs that needs to be processed daily. The file contains user_id, timestamp, action_type, and metadata. How would you design a data pipeline to efficiently process this file and load it into a data warehouse?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Adobe","Airbnb","Oracle"]},{"id":"q-571","question":"How would you design a data pipeline to process 1M ride events per minute from Uber's real-time streaming system?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Meta","Twitter","Uber"]},{"id":"q-885","question":"You operate a multi-tenant SaaS analytics platform ingesting per-tenant event streams from Kafka into Snowflake. Each tenant has different event schemas that can evolve independently. Design a data pipeline to enforce per-tenant data contracts, support late-arriving events, and minimize schema drift while controlling storage costs. Include schema versioning, validation, and deployment safety steps?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["NVIDIA","Snowflake","Stripe"]},{"id":"q-915","question":"You ingest 200k newline-delimited JSON app events daily into S3. Each event has event_id, user_id, timestamp, event_type, and attributes. Design a beginner-friendly pipeline to deduplicate by event_id, hash user_id for privacy, validate required fields, and write Parquet data partitioned by date in a data lake. Address simple schema drift and testing?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Discord","MongoDB","Snap"]},{"id":"q-993","question":"Design a global, multi-tenant data ingestion system for a ride-hailing platform with streams for billing, trips, safety, and promotions. Each tenant defines a data contract; schemas evolve independently; late-arriving events up to 15 minutes must be accepted. Describe architecture using Apache Kafka, Schema Registry (Avro/JSON), an Iceberg/Delta lake sink, and a streaming processor (Flink/Spark). Include data model, schema evolution, backfill handling, testing, and observability?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Coinbase","DoorDash","Meta"]},{"id":"q-176","question":"How would you design a data pipeline that handles both batch and streaming workloads for real-time analytics?","channel":"data-engineering","subChannel":"streaming","difficulty":"beginner","tags":["streaming","kafka"],"companies":["Amazon","Google","Meta","Netflix","Uber"]},{"id":"q-222","question":"How would you design a Kafka Streams application to handle exactly-once processing with stateful aggregations while maintaining sub-second latency during peak loads of 100K events/sec?","channel":"data-engineering","subChannel":"streaming","difficulty":"advanced","tags":["kafka","flink","kinesis"],"companies":["Amazon","Confluent","Netflix","Stripe","Uber"]},{"id":"q-248","question":"How would you implement exactly-once processing in a data pipeline when both source (Kafka) and sink (database) can fail, ensuring no duplicate data or data loss during network partitions and system crashes?","channel":"data-engineering","subChannel":"streaming","difficulty":"intermediate","tags":["dag","orchestration","scheduling"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Snowflake"]},{"id":"q-1038","question":"Design a stack that supports push(x), pop(), top(), and getMin() in O(1) time per operation. Duplicates allowed. Explain the approach, discuss invariants and space usage, and provide a compact code sketch (Python or Java)?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Apple","Databricks","Tesla"]},{"id":"q-1213","question":"Design a dynamic autocomplete data structure for a code search tool that stores terms with frequencies; implement insertWord(word), eraseWord(word), and querySuggestions(prefix, k) returning up to k completions starting with prefix, ordered by descending frequency then lexicographically. Discuss a Trie-based design with per-node top-k structures and update costs?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Hashicorp","Hugging Face"]},{"id":"q-2010","question":"Design a dynamic session tracker: each session has an id, a score, and a lastActive timestamp. Support insertSession(id, score, ts), updateScore(id, delta), expireSessionsBefore(ts) removing expired sessions, topK(window, k) returning the k highest scores among active sessions (active = ts >= now - window) breaking ties by id, and medianScore(window) returning the median score among active sessions. Target near O(log n) per update and O(k log n) for topK. Explain data structures and tradeoffs?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Anthropic","Tesla"]},{"id":"q-2087","question":"Design a dynamic leaderboard data structure to support a game with many players: addOrUpdatePlayer(playerId, score), removePlayer(playerId), getTopK(k), and getPlayerRank(playerId). Achieve O(log n) updates and O(log n + k) to fetch the top-k. Explain how you handle duplicates (equal scores) and score changes?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Databricks","Lyft","Plaid"]},{"id":"q-677","question":"Design a data structure to maintain dynamic counters for items, supporting add(key, delta), get(key), and getTopK(k) returning the k keys with highest counts, ties broken by key. In a real-time analytics scenario, such as tracking top active users by event count, describe the structure, updates, and complexity trade-offs?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Bloomberg","Discord","Netflix"]},{"id":"q-686","question":"Design a data structure that maintains a multiset of integers with insert(x), erase(x) for one occurrence, findMedian(), and findKthSmallest(k). Achieve O(log n) time per operation on average. Explain data layout, invariants, and handling duplicates and lazy deletions. For example: insert 1,2,3,4,5; erase 3; what is median and findKthSmallest(2)?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Apple","Oracle","Twitter"]},{"id":"q-697","question":"Design a time-weighted event multiset data structure. Supports insertEvent(ts, w), eraseEvent(ts, w) removing one occurrence, rangeSum(a, b) returning the sum of weights for events with timestamps in [a, b], and findKthWeightInRange(a, b, k) returning the k-th smallest weight among events with timestamps in [a, b]. Target O(log^2 n) per operation; handle duplicate timestamps and weights; discuss memory and trade-offs?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["DoorDash","Plaid","Scale Ai"]},{"id":"q-701","question":"Design a dynamic multiset of 2D points (duplicates allowed) with insertPoint(x,y), erasePoint(x,y) removing one occurrence, rangeCount(x1,y1,x2,y2), and rangeKthX(x1,y1,x2,y2,k) returning the k-th smallest x in the rectangle (tie by y). Target O(log^2 n) per op. Propose a segment tree over x; each node stores an ordered multiset of (y, unique_id). Explain duplicates handling, updates, rangeCount, and rangeKthX via binary search over x?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Apple","Discord","Snap"]},{"id":"q-710","question":"Design a data structure that maintains a dynamic multiset of strings with operations insert(word), erase(word) removing a single occurrence, countWithPrefix(prefix) returning how many words start with the prefix, and mostFrequentWithPrefix(prefix) returning the most frequent word among those starting with the prefix (tie-break lexicographically). What data structure would you implement, and what are the time complexities and invariants?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Apple","Scale Ai","Slack"]},{"id":"q-722","question":"Design a data structure for an array of integers that supports point updates update(i, val) and range maximum subarray sum queries maxSubarray(l, r) in O(log n). Explain the segment tree node data (sum, bestPref, bestSuff, bestSub) and the merge logic, including tie-breaking for leftmost subarray and handling entirely negative ranges. Example: start with [1,-2,3,4,-1], update index 2 to 5, query maxSubarray(0,4)?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Apple","Bloomberg"]},{"id":"q-727","question":"Design a dynamic data structure for weighted 3D points (x,y,t) with weight w. Support insertPoint(x,y,t,w) and erasePoint(x,y,t,w) (duplicates allowed), rangeSum3D(x1,x2,y1,y2,t1,t2) for total weight in the 3D box, and findKthLargestInRange(x1,x2,y1,y2,t1,t2,k) for the k-th largest weight inside the box. Target O(log^3 n) per op; O(log W * log^3 n) for kth; discuss coordinate compression, memory trade-offs, and handling duplicates?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Airbnb","Google","Meta"]},{"id":"q-739","question":"Design a dynamic 2D weighted point data structure that supports insertPoint(x,y,w), erasePoint(x,y,w) (duplicates allowed), rangeSum2D(x1,x2,y1,y2) for the total weight in the rectangle, and kthLargestInRectangle(x1,x2,y1,y2,k) for the k-th largest weight inside the rectangle. Aim for average O(log^2 n) per operation; discuss coordinate compression, memory trade-offs, and duplicates handling?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Citadel","Hugging Face","Snap"]},{"id":"q-746","question":"Design a dynamic multiset of integers that supports insert(x), erase(x) (one occurrence), countInRange(l, r) for the number of elements in [l, r], and kthSmallestInRange(l, r, k) returning the k-th smallest value among elements in [l, r]. Aim for expected O(log n) updates and O(log n * log U) queries. Explain data structure, balance, and duplicates handling?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Airbnb","Google","PayPal"]},{"id":"q-749","question":"Design a dynamic weighted string multiset with insertWord(word, w), eraseWord(word, w) (duplicates allowed), sumByPrefix(prefix) returning total weight of words starting with prefix, and kthLargestWeightInPrefix(prefix, k) returning the k-th largest weight among those words. Outline the data structure, invariants, and expected complexities; discuss handling of long words and memory trade-offs?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Google","IBM"]},{"id":"q-761","question":"Design a dynamic word-frequency histogram for a text stream. Implement addWord(word) to increment frequency, eraseWord(word) to decrement (removing word when count hits zero), getFrequency(word), and topKWords(n) returning the n most frequent distinct words (ties broken lexicographically). Target average O(log m) per update and O(k log m) for topK, where m is the number of distinct words. Explain approach and data structures you would use?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Adobe","Scale Ai"]},{"id":"q-774","question":"Design a data structure to manage a dynamic multiset of weighted intervals on the real line. Each interval is [l, r] with weight w; duplicates allowed. Implement insertInterval(l, r, w), eraseInterval(l, r, w) (one occurrence), rangeSumAt(x) returning the total weight of all intervals covering point x, and kthLargestWeightAt(x, k) returning the k-th largest weight among intervals covering x. Target average O(log n) update and O(log n) query; discuss coordinate compression, memory trade-offs, and handling duplicates?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Hashicorp","Slack"]},{"id":"q-782","question":"Design a fully dynamic data structure to maintain a set of linear cost functions y = m_i x + b_i. Support: addLine(id, m, b), removeLine(id) (one occurrence per id), queryMin(x) returning the minimum cost at x across active lines, and queryKthMin(x, k) returning the k-th smallest cost at x. Domain x in [Xmin, Xmax]. Aim for near O(log X) per operation; discuss how deletions are handled, precision, and memory trade-offs?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Apple","DoorDash","Tesla"]},{"id":"q-790","question":"Design a persistent 2D dynamic weighted point data structure that supports insertPoint(x,y,w) and erasePoint(x,y,w) (duplicates allowed). Extend with rangeSum2D(x1,x2,y1,y2,version) and kthLargestInRectangle(x1,x2,y1,y2,k,version). Each insertion creates a new version; queries run in O(log^2 n) time. Explain coordinate compression, memory management, and how duplicates are handled across versions?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Amazon","Databricks","Google"]},{"id":"q-801","question":"Design a fixed-window streaming data structure: push(x) appends an integer to the window; if the window exceeds size W, remove the oldest value. Implement getKthSmallest(k) to return the k-th smallest value in the current window in O(log W). Propose a concrete structure (e.g., an order-statistics tree with duplicates) and outline push/evict/getKthSmallest, including how duplicates and memory are handled?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Cloudflare","Plaid","Twitter"]},{"id":"q-805","question":"Design a dynamic forest data structure supporting: addNode(id, value), link(childId, parentId) to attach a child under a parent, cut(childId) to detach a subtree, update(id, delta) to adjust a node's value, pathQuery(u, v) for the sum of values along the path from u to v, and subtreeQuery(u) for the sum of values in the subtree rooted at u. Target amortized O(log n) per operation. Which approach would you pick (Link-Cut Tree vs Euler Tour Tree), and how would you handle edge cases like root changes and duplicate node values?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Lyft","Meta","Netflix"]},{"id":"q-815","question":"Design a dynamic data structure for a bipartite graph with left indices [1..N] and right indices [1..M]. Each edge (u,v) has weight w; duplicates allowed. Support: - addEdge(u,v,w), eraseEdge(u,v,w) (one occurrence) - rangeSum(uL,uR,vL,vR) total weight of edges with u in [uL,uR] and v in [vL,vR] - kthLargestEdgeWeight(uL,uR,vL,vR,k) the k-th largest edge weight in that submatrix. Aim for ~O(log N log M) per operation; discuss compression, duplicates, and memory trade-offs?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Apple","NVIDIA","Snowflake"]},{"id":"q-820","question":"Design a dynamic sequence structure 'RopeArray' storing an integer array supporting insertAt(i, val), eraseAt(i), rangeSum(l, r), rangeMax(l, r), and getAt(i). How would you implement it to achieve average O(log n) per operation using a balanced tree with implicit keys and augmented fields (size, sum, max), and how would you test with an example sequence?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Amazon","Google","Microsoft"]},{"id":"q-832","question":"Design a data structure to maintain a dynamic multiset of words with three operations: insertWord(word), eraseWord(word) (one occurrence), and getAnagrams(word) that returns all current words that are anagrams of the given word (including duplicates). Explain how you would store the words, how to compute the canonical signature, and the expected time complexity for updates and queries. For example, after insertWord('listen') and insertWord('silent'), getAnagrams('tinsel') should return ['listen','silent']?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["NVIDIA","Snowflake","Twitter"]},{"id":"q-1160","question":"How would you implement and maintain an indexing strategy for a database that experiences frequent schema changes and evolving query patterns, while ensuring minimal performance degradation during migrations?","channel":"database","subChannel":"database","difficulty":"intermediate","tags":["database-indexing","schema-migration","performance-optimization","index-lifecycle"],"companies":[]},{"id":"q-1161","question":"How would you implement and maintain an indexing strategy for a database that experiences frequent schema changes and evolving query patterns, ensuring optimal performance without requiring constant manual intervention?","channel":"database","subChannel":"database","difficulty":"intermediate","tags":["adaptive-indexing","database-automation","performance-monitoring","schema-evolution"],"companies":[]},{"id":"q-1162","question":"How would you implement and maintain an indexing strategy for a database that experiences frequent schema changes and evolving query patterns, ensuring optimal performance without requiring constant manual intervention?","channel":"database","subChannel":"database","difficulty":"intermediate","tags":["adaptive-indexing","schema-evolution","automated-maintenance","query-optimization"],"companies":[]},{"id":"q-643","question":"How would you design an indexing strategy for a time-series database that handles both recent data queries and long-term historical analysis, considering the trade-offs between write performance and query efficiency?","channel":"database","subChannel":"database","difficulty":"intermediate","tags":["time-series","index-optimization","partitioning","performance-tuning"],"companies":[]},{"id":"q-651","question":"How would you design an indexing strategy for a table with 10 million rows that has frequent read queries with multiple WHERE conditions, occasional bulk inserts, and needs to support both exact match and range queries on different columns?","channel":"database","subChannel":"database","difficulty":"intermediate","tags":["database-indexing","query-optimization","performance-tuning","composite-indexes","bulk-operations"],"companies":[]},{"id":"q-765","question":"How would you design an indexing strategy for a multi-tenant SaaS application where each tenant has isolated data but queries frequently need to aggregate across tenants for reporting, while ensuring query performance doesn't degrade as the number of tenants scales to thousands?","channel":"database","subChannel":"database","difficulty":"intermediate","tags":["multi-tenancy","index design","performance optimization","scalability"],"companies":[]},{"id":"q-630","question":"Explain the difference between a B-tree index and a hash index, and when would you choose one over the other?","channel":"database","subChannel":"index-types","difficulty":"intermediate","tags":["database-indexing","b-tree","hash-index","query-optimization"],"companies":["Google","Amazon","Microsoft","Meta","Netflix"]},{"id":"da-125","question":"Explain database indexing and when should you use it?","channel":"database","subChannel":"indexing","difficulty":"intermediate","tags":["sql","indexing"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"db-1","question":"Explain the differences between Clustered and Non-Clustered Indexes, including their performance implications, storage characteristics, and when to choose each type in database design?","channel":"database","subChannel":"indexing","difficulty":"beginner","tags":["sql","indexing","perf"],"companies":null},{"id":"q-170","question":"When would you choose a composite index over multiple single-column indexes in a relational database?","channel":"database","subChannel":"indexing","difficulty":"intermediate","tags":["index","optimization"],"companies":["Amazon","Goldman Sachs","Google","Meta","Microsoft"]},{"id":"q-288","question":"What is the main difference between B-tree and hash index in terms of range query performance?","channel":"database","subChannel":"indexing","difficulty":"beginner","tags":["btree","hash-index","composite"],"companies":["Amazon","Google","Meta"]},{"id":"q-365","question":"You're designing a real-time analytics system for Discord that processes millions of message events per minute. Your PostgreSQL database is experiencing severe write contention on the message_events table. How would you design a partitioning strategy using declarative partitioning, and what specific index optimizations would you implement to handle both time-series queries and user-based lookups efficiently?","channel":"database","subChannel":"indexing","difficulty":"advanced","tags":["joins","indexes","normalization","postgres"],"companies":["Amazon","Discord","Google","Netflix","Palantir","Stripe","Uber"]},{"id":"q-409","question":"You're designing a database for an e-commerce platform with frequent queries on (user_id, order_date) and (product_id, category). How would you choose between B-tree and hash indexes, and what composite index strategy would optimize both query patterns?","channel":"database","subChannel":"indexing","difficulty":"intermediate","tags":["btree","hash-index","composite"],"companies":["Gitlab","Tcs","Tempus"]},{"id":"q-420","question":"You're designing a user database for a chat application with 10M users. When would you choose a B-tree index over a hash index for the 'email' column, and what are the performance implications for login queries, user search, and profile updates?","channel":"database","subChannel":"indexing","difficulty":"beginner","tags":["btree","hash-index","composite"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-489","question":"You're designing a database for LinkedIn's feed system. Posts can be queried by user_id, created_at, and engagement_score. How would you optimize the indexing strategy for high-throughput reads and writes?","channel":"database","subChannel":"indexing","difficulty":"advanced","tags":["btree","hash-index","composite"],"companies":["Anthropic","LinkedIn","NVIDIA"]},{"id":"q-572","question":"You're designing a database for a high-frequency trading system. When would you choose a B-tree index over a hash index for composite queries on (symbol, timestamp, price)? What are the specific performance implications?","channel":"database","subChannel":"indexing","difficulty":"advanced","tags":["btree","hash-index","composite"],"companies":["IBM","NVIDIA"]},{"id":"q-599","question":"When would you choose a composite index over multiple single-column indexes, and what are the trade-offs?","channel":"database","subChannel":"indexing-strategies","difficulty":"intermediate","tags":["indexing","performance","query-optimization","database-design"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-618","question":"Explain the difference between clustered and non-clustered indexes and when you would choose each type. Provide a specific example scenario.","channel":"database","subChannel":"indexing-strategies","difficulty":"intermediate","tags":["database","indexing","performance","sql"],"companies":["Google","Microsoft","Amazon","Meta","Apple"]},{"id":"da-129","question":"What is the main difference between SQL and NoSQL databases in terms of data structure?","channel":"database","subChannel":"nosql","difficulty":"beginner","tags":["nosql","mongodb"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-242","question":"How do MongoDB's document structure and SQL's table rows differ in handling user data with varying attributes, and what are the performance implications for common user operations?","channel":"database","subChannel":"nosql","difficulty":"beginner","tags":["mongodb","dynamodb","cassandra","redis"],"companies":null},{"id":"q-331","question":"You're designing a multi-region e-commerce platform using DynamoDB. Your product catalog needs to support 10M items with eventual consistency across regions, but you must handle hot partitioning during flash sales. How would you design your partition key strategy and what trade-offs would you make between read performance and write throughput?","channel":"database","subChannel":"nosql","difficulty":"advanced","tags":["mongodb","dynamodb","cassandra","redis"],"companies":["Hashicorp","Meta","Oracle"]},{"id":"q-268","question":"How would you optimize a time-series analytics query that scans 100M+ rows across multiple date partitions in PostgreSQL when the WHERE clause cannot be pruned effectively due to complex temporal conditions?","channel":"database","subChannel":"query-optimization","difficulty":"advanced","tags":["explain","query-plan","partitioning"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-303","question":"How would you optimize a slow PostgreSQL query that joins 5 tables with millions of rows?","channel":"database","subChannel":"query-optimization","difficulty":"advanced","tags":["joins","indexes","normalization","postgres"],"companies":["Amazon","Google","Meta"]},{"id":"q-343","question":"You have a PostgreSQL table with 100M rows partitioned by date. A query filtering on a specific date range is still slow. What would you check in the EXPLAIN plan and how would you optimize it?","channel":"database","subChannel":"query-optimization","difficulty":"intermediate","tags":["explain","query-plan","partitioning"],"companies":["Affirm","Amazon","Google","Jane Street","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-380","question":"You're optimizing a query that's slow due to a large time-series table. The query filters by timestamp range and device_id. How would you analyze the query plan and what partitioning strategy would you recommend?","channel":"database","subChannel":"query-optimization","difficulty":"intermediate","tags":["explain","query-plan","partitioning"],"companies":["Hrt","Stripe","Tesla"]},{"id":"q-436","question":"You have a 100M row orders table with slow queries. The query plan shows sequential scans despite indexes on customer_id and order_date. How would you diagnose and fix this performance issue?","channel":"database","subChannel":"query-optimization","difficulty":"advanced","tags":["explain","query-plan","partitioning"],"companies":["Bloomberg","Salesforce","Slack"]},{"id":"q-546","question":"You're analyzing a slow query on a partitioned table. The EXPLAIN plan shows a full table scan instead of partition pruning. What could cause this and how would you fix it?","channel":"database","subChannel":"query-optimization","difficulty":"intermediate","tags":["explain","query-plan","partitioning"],"companies":["Microsoft","Tesla"]},{"id":"da-156","question":"What are the key differences between DELETE and TRUNCATE commands in SQL, including their impact on identity columns, foreign key constraints, and performance characteristics?","channel":"database","subChannel":"sql","difficulty":"beginner","tags":["sql","indexing"],"companies":["Amazon","Google","Microsoft","Netflix","Oracle","Snowflake"]},{"id":"q-172","question":"Design a database failover strategy for a high-traffic e-commerce platform using primary-replica PostgreSQL. How would you ensure zero-downtime failover while maintaining data consistency during peak traffic of 10,000 requests/second?","channel":"database","subChannel":"sql","difficulty":"intermediate","tags":["chaos","resilience"],"companies":["Amazon","Bloomberg","Microsoft","Netflix","Uber"]},{"id":"q-458","question":"You have a PostgreSQL database with orders (10M rows) and customers (1M rows). A query joining these tables is slow. How would you optimize it?","channel":"database","subChannel":"sql","difficulty":"intermediate","tags":["joins","indexes","normalization","postgres"],"companies":["Apple","Tesla","Uber"]},{"id":"da-128","question":"You have a banking system where users can transfer money between accounts. Design a transaction to handle a transfer of $500 from Account A (balance: $1000) to Account B (balance: $200). What happens if the system crashes after debiting Account A but before crediting Account B? How would you ensure data consistency?","channel":"database","subChannel":"transactions","difficulty":"intermediate","tags":["acid","transactions"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Stripe"]},{"id":"da-134","question":"You have a banking system where Account A transfers $100 to Account B, but during the transaction, Account B gets deleted by another process. The transfer uses READ COMMITTED isolation. What happens to the $100, and how would you prevent data inconsistency?","channel":"database","subChannel":"transactions","difficulty":"advanced","tags":["acid","transactions"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Stripe"]},{"id":"da-170","question":"You're building a banking system where users can transfer money between accounts. How would you design the transaction handling to ensure no money is lost or created during transfers, especially when the system crashes mid-transfer?","channel":"database","subChannel":"transactions","difficulty":"intermediate","tags":["acid","transactions"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Stripe"]},{"id":"da-172","question":"In a distributed database system, how would you implement a two-phase commit protocol to ensure atomicity across multiple nodes, and what are the key failure scenarios and recovery mechanisms you must handle?","channel":"database","subChannel":"transactions","difficulty":"advanced","tags":["acid","transactions"],"companies":null},{"id":"db-2","question":"How do ACID properties ensure data integrity in a banking transaction where $100 is transferred from Account A to Account B?","channel":"database","subChannel":"transactions","difficulty":"intermediate","tags":["acid","transactions","theory"],"companies":["Amazon","Goldman Sachs","Google","PayPal","Stripe"]},{"id":"q-190","question":"What is the difference between READ COMMITTED and REPEATABLE READ isolation levels in database transactions, and how does MVCC implementation affect their behavior?","channel":"database","subChannel":"transactions","difficulty":"beginner","tags":["acid","isolation-levels","mvcc"],"companies":["Amazon","Databricks","Google","Microsoft","Oracle","Snowflake"]},{"id":"q-317","question":"Explain how MVCC (Multi-Version Concurrency Control) works and how it prevents lost updates in a database system?","channel":"database","subChannel":"transactions","difficulty":"intermediate","tags":["acid","isolation-levels","mvcc"],"companies":["Microsoft","Plaid","Warner Bros"]},{"id":"q-353","question":"You're building a collaborative design tool where multiple users can edit the same document simultaneously. How would you use database transactions and isolation levels to prevent conflicts while maintaining good performance?","channel":"database","subChannel":"transactions","difficulty":"beginner","tags":["acid","isolation-levels","mvcc"],"companies":["Adobe","Amazon","Canva","Epic Games","Google","Meta","Microsoft","Netflix"]},{"id":"q-397","question":"In a high-transaction payment system using PostgreSQL, how would you design a transaction isolation strategy to prevent lost updates while maintaining high concurrency for account transfers?","channel":"database","subChannel":"transactions","difficulty":"advanced","tags":["acid","isolation-levels","mvcc"],"companies":["Amazon","Google","Netflix","PayPal","Square","Stripe"]},{"id":"q-428","question":"You're building a booking system for Airbnb where multiple users can reserve the same property simultaneously. How would you design the transaction handling to prevent double bookings while maintaining high availability?","channel":"database","subChannel":"transactions","difficulty":"intermediate","tags":["acid","isolation-levels","mvcc"],"companies":["Airbnb","Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"q-519","question":"You're designing a high-frequency trading system where transactions must see consistent data snapshots. How would you implement MVCC to handle concurrent reads while preventing write skew anomalies, and what isolation level would you choose?","channel":"database","subChannel":"transactions","difficulty":"advanced","tags":["acid","isolation-levels","mvcc"],"companies":["Lyft","Snap","Tesla"]},{"id":"q-1100","question":"In a Databricks data pipeline, ingest JSON events from S3 into Delta Lake with Auto Loader. Each event has a nested user object (id, email) and an optional pages array. Some events miss user.email. How would you design the pipeline to safely flatten nested fields, handle missing values, and upsert the latest user state into a Delta Silver table using an idempotent MERGE?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["MongoDB","Oracle"]},{"id":"q-1112","question":"Scenario: In a Databricks Delta Live Tables pipeline, ingest JSON events from a Kafka source into a Bronze table, where nested fields drift over time (e.g., payload.user.locale is added later, some events miss payload.user). You then transform to a Silver table used by billing and marketing. How would you implement a robust schema-evolution strategy and gating so new fields are captured without breaking existing downstream queries, and how would you test this in a run?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["DoorDash","Microsoft","Snap"]},{"id":"q-1144","question":"In a Databricks streaming pipeline ingesting tenant-scoped events from Kafka into Delta Lake, events may arrive late by up to 10 minutes. Describe a concrete end-to-end approach to upsert the latest state per (tenant_id, user_id) into a Silver table while preserving the full event history. Include: data model, CDC logic, watermarking and late data handling, an idempotent MERGE strategy, schema evolution handling, and governance considerations with Unity Catalog RBAC and data lineage?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Airbnb","Cloudflare","Databricks"]},{"id":"q-1182","question":"In a multi-tenant Databricks lakehouse ingesting per-tenant telemetry from Kafka and S3 into a unified Silver Delta table, describe an end-to-end CDC strategy to implement SCD2-like history per tenant with idempotent MERGE, handle late data with a watermark, and enforce governance through Unity Catalog RBAC and dynamic PII masking. Include data model, CDC logic, testing plan, and how you’d validate lineage?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Discord","Google","Snowflake"]},{"id":"q-1226","question":"In a Databricks Delta Live Tables pipeline that ingests clickstream events from a cloud bucket into a Delta table, data quality checks are defined via expectations. A batch arrives with corrupted event_time values that would fail the run. Outline a concrete approach to quarantine bad data, feed downstream tables only from valid rows, and store invalids for audit, while still handling late data and deduplication?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Hashicorp","Zoom"]},{"id":"q-1264","question":"In a Databricks job ingesting daily Parquet files from S3 into Delta Lake, a new field 'campaign_id' may appear in newer files while older ones do not. Describe a concrete, beginner-friendly approach to ingest without data loss, allowing downstream joins, and handling the schema change gracefully. Include how to enable Delta schema evolution (mergeSchema/autoMerge), define a compatible target schema, and implement a minimal validation to drop rows missing essential fields?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Anthropic","Coinbase","Uber"]},{"id":"q-1320","question":"You're designing a Delta Live Tables workflow ingesting data from Kafka and an S3 landing zone, with a downstream customer dimension in Delta Lake that uses SCD Type 2. How would you implement idempotent MERGE-based upserts, handle schema drift, and preserve late-arriving data while auditing invalid events and ensuring downstream BI reads only current rows?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["NVIDIA","Robinhood","Two Sigma"]},{"id":"q-1375","question":"In a Databricks streaming job, a Kafka topic emits JSON events for many tenants. The payload schema drifts with new fields; you want a stable Silver Delta table with a canonical schema and history. Describe a beginner-friendly approach to map events to the canonical schema, handle new fields without breaking downstream joins, and perform an idempotent MERGE into Silver by (tenant_id, event_id). Include a concrete mapping rule set and a small MERGE example?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["MongoDB","Oracle","Robinhood"]},{"id":"q-1409","question":"Describe an end-to-end approach for a fraud-detection streaming pipeline using two Kafka topics (transactions, account_updates): Bronze ingest, join to a versioned SCD2 customer_dim, compute risk in Silver, upsert via MERGE with a deterministic key, watermark late data, handle schema drift with Delta Lake evolution, and enforce governance with Unity Catalog RBAC and lineage. Include testing with synthetic late-arriving data?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Hugging Face","Scale Ai","Snowflake"]},{"id":"q-1451","question":"In a Databricks notebook, you need to join a 10M-row Delta Lake 'customers' table with a 10k-row 'segments' reference table to produce a daily marketing audience feed. How would you implement an efficient join strategy in Spark/Delta to minimize shuffle and cost, ensure correctness if segments update, and maintain lineage? Include: join type and hints, caching strategy, refresh cadence, and where to store results?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Databricks","Google","Two Sigma"]},{"id":"q-1540","question":"In a Databricks streaming pipeline ingesting order events from Kafka into Delta Lake, implement a scalable SCD Type 2 for a customer_dim table to preserve history while handling late-arriving updates up to 15 minutes. Describe the data model (bronze/silver), CDC logic using MERGE, watermarking, and schema evolution, plus Unity Catalog RBAC and lineage considerations. Include a minimal code sketch of the MERGE closing an old row and inserting a new version?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Cloudflare","Hugging Face"]},{"id":"q-1555","question":"In a Databricks pipeline ingesting 20 TB/day of Parquet logs on S3 into Delta Lake, design a practical optimization plan to improve read latency for near-real-time dashboards using Delta features like OPTIMIZE, ZORDER, Data Skipping, caching, and Photon. Discuss partitioning strategy, trade-offs, and how you'd validate gains with benchmark metrics?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Databricks","IBM","Oracle"]},{"id":"q-1643","question":"In Databricks, ingest streaming data from Kafka into Delta Lake for 10k IoT devices emitting multiple sensor types (temperature, humidity, pressure). Build a Silver table with the latest per-device per-sensor-type state while preserving full history. The source schema will evolve (new sensors added, some removed). Outline a robust end-to-end approach: data model, CDC/Upsert logic, watermarking for late data, schema evolution strategy, idempotent MERGE, and governance with Unity Catalog RBAC and lineage. Include concrete examples of Bronze->Silver handoff and a dynamic per-tenant view?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Lyft","OpenAI"]},{"id":"q-1658","question":"Design a secure external data sharing workflow in a Databricks environment using Unity Catalog and Delta Sharing to expose aggregated metrics derived from production Delta tables to external partners while maintaining tenant isolation and governance. Include data model, masking strategy, per partner quotas, refresh cadence, and how to monitor and revoke access?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Amazon","Oracle","Twitter"]},{"id":"q-1683","question":"Design a multi-tenant, Databricks-based data pipeline ingesting 1 TB/day of JSON events from Kafka into Delta Lake. Tenants share storage but must be completely isolated; dashboards must mask PII fields per-tenant. Propose an end-to-end pattern using Unity Catalog RBAC, dynamic data masking, Delta Live Tables, and Photon-enabled reads. Include data model, masking rules, handling schema evolution, and how you validate governance and lineage?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Lyft","Netflix","Square"]},{"id":"q-1721","question":"Databricks beginner scenario: A streaming pipeline reads 2 TB/month of JSON events from S3 via Autoloader into Delta Lake. A downstream dashboard shows active_users by hour. Late events arrive up to 10 minutes. Design a practical plan to maintain accurate hourly active_user counts with minimal duplication, including: schema/partitioning for streaming, watermark-based late data handling, an idempotent MERGE into a Silver table, and a simple end-to-end test using synthetic late events. Also outline monitoring steps?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Adobe","Instacart","Plaid"]},{"id":"q-1844","question":"In a Databricks streaming pipeline ingesting events from Kafka into Delta Lake for a 50+ TB/day workload, design a CDC-based upsert to a Silver table with per-tenant sharding, late data handling, and schema evolution. Describe the data model, idempotent MERGE keys, watermark latency model, and Unity Catalog RBAC considerations; propose validation metrics and a minimal reproducible code skeleton?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Databricks","Instacart","MongoDB"]},{"id":"q-1853","question":"In a Databricks job ingesting 50 GB/day of Avro logs from S3 into Delta Lake, design a beginner-friendly Delta Live Tables pipeline to produce Bronze (raw) and Silver (flattened) tables. Include how you flatten the nested field user (id and tier) into Silver, enforce a simple data quality gate (NOT NULL user_id, user_tier in {'free','standard','premium'}), and choose a partitioning strategy by file_date. Explain testing and how you'll measure improvements?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Meta","Robinhood","Stripe"]},{"id":"q-1944","question":"Design a Delta Live Tables pipeline that streams per-tenant user activity from Kafka into Bronze, then updates a per-tenant Silver table implementing SCD Type 2 on a tenant-scoped customer_dim, using a deterministic MERGE for upserts. Tenants can emit out-of-order data and schema drift occurs. Propose concrete config for watermarking, per-tenant constraints, and schema evolution, plus RBAC in Unity Catalog. Include synthetic late-data tests and metrics to validate latency and correctness?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Bloomberg","Citadel","Meta"]},{"id":"q-1990","question":"Beginner: Adobe wants a minimal, end-to-end Databricks pipeline to ingest daily JSON event logs from S3 into Delta Lake. Design the workflow using Auto Loader for a Bronze table, then flatten to a date-partitioned Silver table. Include simple schema-evolution handling, a lightweight data-quality check (required fields, duplicates), and a basic unit test that validates the Silver schema and daily row count?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Adobe","Uber"]},{"id":"q-2033","question":"In a Databricks workflow, ingest 50 GB/day of JSON web logs from S3 into a Bronze Delta table. Propose a beginner-friendly pipeline to populate a Silver table that upserts the latest event per session_id, handles 2-minute late data with a watermark, and validates data quality before write. Include partitioning strategy, a MERGE-based CDC, and Unity Catalog RBAC considerations?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Amazon","Bloomberg","Google"]},{"id":"q-2070","question":"Design an end-to-end Databricks pipeline to ingest a daily JSON feed from an on-prem FTP drop into Delta Lake. Use Auto Loader with schema evolution for changing fields (user_id, event_type, event_ts, ip_address, device, etc.). Describe Bronze to Silver upserts via MERGE, data quality tests, IP masking, and Unity Catalog RBAC with lineage. Be concrete about steps and trade-offs?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["MongoDB","Snowflake"]},{"id":"q-906","question":"Scenario: A daily Delta Live Tables (DLT) pipeline ingests clickstream JSONs into a Bronze Delta table and then enriches with a users_dim to a Silver table. Build a beginner-friendly pipeline that: deduplicates by event_id, validates user_id via users_dim, filters to business hours 08:00–18:00, enriches with user fields, and writes to Silver partitioned by event_date. Outline minimal steps and rationale?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Apple","Netflix","Scale Ai"]},{"id":"q-1125","question":"You're building an incremental dbt model analytics.daily_revenue_by_product sourced from staging.sales_raw. Data can arrive late up to 2 days. How would you implement incremental upserts, reprocess late days without disturbing older data, and validate with tests and a snapshot of product price history? Provide a minimal SQL snippet illustrating the approach?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Apple","Hashicorp"]},{"id":"q-1165","question":"You're designing a dbt analytics pipeline for a ride-sharing platform. The raw events are in `staging.events_raw` with columns like `ride_id`, `city`, `ride_type`, `currency`, `amount`, `occurred_at`, and data can arrive up to 48 hours late. Build a beginner-friendly incremental model `analytics.daily_revenue` that computes USD revenue per day by `date`, `city`, and `ride_type`. Use a daily `pricing.exchange_rates` table to convert from local currency to USD. Describe how you'd implement incremental upserts, late-data handling (2-day window), schema-drift guards, and validation with tests and a snapshot. Also outline tests for late refunds and missing rates?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Google","PayPal"]},{"id":"q-1257","question":"Design a drift-aware dbt workflow across dev/staging/prod in Snowflake. How would you detect schema drift between sources and models using snapshots, tests, and sources, enforce a drift threshold, and automatically fail CI when drift exceeds the threshold? Describe the macro, tests, and alert/rollback mechanism, plus how you version thresholds?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["Goldman Sachs","Google","Meta"]},{"id":"q-1361","question":"Design a beginner-friendly incremental dbt model in Snowflake for a SaaS analytics pipeline: raw events live at staging.event_logs with user_id, event_type (signup, login, purchase), country, occurred_at. Build analytics.daily_metrics that counts distinct active users by day and event_type, enriched by dimensions.countries. Explain incremental logic, late data handling (7 days), schema drift guards, and validation with tests and a snapshot. Also outline an Exposure for the dashboard with lineage?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Snowflake","Zoom"]},{"id":"q-1523","question":"Design an incremental dbt model analytics.daily_engagement for a global multi-tenant app. Raw events sit in staging.user_events with tenant_id, user_id, email, event_type, occurred_at, privacy_flag. Build per-tenant daily distinct-user counts by event_type, redacting emails when privacy_flag is true. Describe incremental strategy, late data window, schema-drift guards, tests and snapshots, and how to expose lineage for dashboards?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Apple","IBM","Tesla"]},{"id":"q-1560","question":"In a dbt project, you ingest raw events into `staging.web_events` with columns: `event_id`, `user_id`, `event_type`, `country`, `occurred_at` (UTC). Build a beginner-friendly incremental model **analytics.daily_active_users** that reports the count of distinct `user_id`s per day, by `country` and `event_type`. Data can arrive up to 2 days late. Describe your incremental logic, how you handle late data with a rolling window, how to guard against schema drift, and how you validate with tests. Also draft an Exposure for a dashboard showing daily active users by country and event_type, including lineage?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Bloomberg","DoorDash","Oracle"]},{"id":"q-1590","question":"Design a two-model incremental dbt flow in Snowflake: 1) analytics.first_session derives first_session_at per user and seeds analytics.users(user_id, first_session_at, country_code, cohort). 2) analytics.daily_metrics counts daily active users by day and event_type, enriched with dimensions.countries. Include late-data handling (3 days), schema-drift guards, tests (not_null, unique), a snapshot, and expose lineage to dashboards?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Amazon","Oracle"]},{"id":"q-1604","question":"In a dbt pipeline powering a fintech analytics platform, raw events arrive at staging.fin_events with customer_id, txn_id, amount, currency, status, event_ts. Design an incremental analytics.transactions model that deduplicates by txn_id across sources, handles late events within a 2-day window, validates currency via a macro-based set per source, guards against schema drift with a dynamic mapping table, and snapshots customers on their first txn for dashboard lineage. Include tests and performance considerations?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["IBM","NVIDIA"]},{"id":"q-1714","question":"In a beginner dbt project for a media site, staging.events_logs(user_id, event_type, occurred_at, region_code) feeds analytics.daily_engagement that counts distinct users per date and event_type, enriched by dimensions.regions on region_code. Build the incremental model with late data window (3 days), include schema-drift guards, and tests (not_null on user_id, occurred_at; unique on (user_id, occurred_at, event_type)). Add a data-contract macro/test ensuring staging.events_logs contains required fields and types, and a snapshot on users to capture region changes. Explain approach, tests, and macro design?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Bloomberg","Cloudflare","Google"]},{"id":"q-1741","question":"You're building a multi-tenant dbt analytics pipeline on Snowflake. Raw events live in staging.events with tenant_id, user_id, event_type, occurred_at. You plan per-tenant analytics schemas (analytics.{tenant}). Design an incremental analytics.daily_metrics per tenant that counts distinct active users by day and event_type, with late data up to 2 days, joining dimensions.tenants and dimensions.countries, including schema-drift guards, tests (not_null, unique), and a snapshot for user_first_seen per tenant. Explain approach for cross-tenant lineage and tenant-scoped materializations?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["IBM","Stripe"]},{"id":"q-1757","question":"Design a dbt analytics layer for three domains: core_events (user actions), memberships (plans), and referrals. Data arrives late (up to 2 days). Propose an architecture that ensures: 1) incremental models with controlled late data backfills, 2) deterministic surrogate keys for cross-domain joins, 3) schema-drift guards via custom tests/macros, 4) automated docs with complete lineage, 5) dashboard-friendly exposure with stable lineage during backfills, and 6) performance considerations for Snowflake or BigQuery (partitioning, clustering). Include a concrete incremental SQL snippet?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["Anthropic","MongoDB","Slack"]},{"id":"q-1808","question":"Design an intermediate dbt workflow for a fintech analytics pipeline on Snowflake. Raw events are in staging.transactions (transaction_id, user_id, amount, currency, occurred_at, status) and staging.users (user_id, country_code, account_status). Build an incremental analytics.daily_finance that sums total_amount and transaction_count by day, currency, country_code, and status, with late data support up to 2 days. Add analytics.users_snapshot as a Type 2 surrogate for changes in country_code/account_status. Expose lineage and discuss a data-contract macro to validate source schemas and auto-generate tests?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Cloudflare","LinkedIn"]},{"id":"q-1823","question":"Scenario: In a beginner dbt project for a gig-economy platform, raw events arrive in staging.event_logs with user_id, session_id, event_type (visit, click, conversion), occurred_at, and region_code. Build an incremental model analytics.daily_events that counts events by day and event_type, enriched by dims.regions on region_code. Use a 2-day late data window, guard against schema drift with not_null and unique tests, and create a data-contract macro to verify staging.event_logs contains the required columns and types before run. What approach would you take?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Amazon","Uber"]},{"id":"q-1909","question":"Design a per-tenant incremental dbt flow in Snowflake that computes analytics.{tenant}.daily_event_summary from staging.events (tenant_id, user_id, event_type, occurred_at, country_code, record_hash). Include enrichment from dimensions.countries, and produce a per-tenant daily count of distinct users by event_type. Implement late data handling (2 days), schema-drift guards, tests (not_null, unique), and a snapshot for analytics.{tenant}.users to capture cohort changes. Explain how you ensure cross-tenant lineage and isolation?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["LinkedIn","Netflix","Twitter"]},{"id":"q-1988","question":"Design an incremental, per-tenant analytics.daily_metrics model in Snowflake with dbt. Staging.events has tenant_id, user_id, event_type, occurred_at, platform, revenue. Build analytics.{tenant}.daily_metrics counting distinct users per day by event_type, with late data tolerance of 2 days. Add a per-tenant feature flag in dimensions.tenants (revenue_enabled) and a macro to include revenue only when true. Enforce isolation via per-tenant schemas, add schema-drift guards, tests (not_null, unique), and a snapshot for analytics.{tenant}.users. Explain cross-tenant lineage and dashboard exposure?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Hugging Face","Snowflake"]},{"id":"q-2013","question":"Design a per-tenant incremental analytics model in Snowflake that materializes analytics.{tenant}.daily_event_engagement from staging.events (tenant_id, user_id, event_type, occurred_at). Enrich with dimensions.regions on country_code. Produce daily counts by event_type and region_name. Add a 3-day late data window, schema-drift guards, tests (not_null, unique), and a snapshot analytics.{tenant}.customers for cohort changes. Explain tenant isolation and cross-tenant lineage via macro-generated per-tenant schemas?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Goldman Sachs","LinkedIn"]},{"id":"q-2097","question":"Design a per-tenant weekly retention pipeline in Snowflake using dbt where raw events live in staging.events with tenant_id, user_id, first_seen_at, occurred_at, and an events.users table. Create analytics.tenant_retention (tenant_id, cohort_week, retention_users, total_users) that computes weekly retention by cohort (first_seen_week) with incremental loading, and late data handling up to 4 days. Also implement analytics.global_retention that aggregates per-tenant retention across tenants with tenant_dim country/plan, ensuring cross-tenant lineage and isolation. Include schema-drift guards, tests (not_null, unique), and a snapshot of analytics.tenants to track cohort definitions. Explain how you ensure isolation and lineage?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Lyft","Snowflake"]},{"id":"q-2133","question":"Scenario: A new multi-tenant event feed named staging.stream_events with columns tenant_id, user_id, event_type, occurred_at, country_code, payload (JSON). Task: implement a dbt incremental model analytics.daily_user_events that, for each day, tenant_id, and event_type, returns the count of distinct users. Enrich with dimensions.countries on country_code. Create a macro to parse payload JSON extracting device and app_version; fallback defaults if missing. Implement late data tolerance of 2 days (i.e., if occurred_at within last 2 days, process in daily batch). Add tests: not_null on tenant_id, user_id, occurred_at; unique on (tenant_id, user_id, occurred_at, event_type). Add a snapshot for analytics.users to capture cohort-country changes. Provide strategy for cross-tenant lineage and isolation in a single schema (no per-tenant schemas)?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["MongoDB","NVIDIA","OpenAI"]},{"id":"q-2173","question":"Design a per-tenant incremental dbt model for a SaaS analytics pipeline on Snowflake. Source staging.user_events (tenant_id, user_id, event_type, event_timestamp, revenue, lifecycle_stage). Build analytics.{tenant}.daily_cohort to compute daily cohorts by lifecycle_stage and event_type with a 2-day late-data window, upserting via MERGE. Add schema-drift guards, tests (not_null, unique), and a per-tenant last_seen_users snapshot. How do you enforce cross-tenant isolation and lineage?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Hashicorp","NVIDIA"]},{"id":"q-848","question":"You're designing a dbt analytics pipeline for an e-commerce platform. The 'staging.events_raw' table ingests page views and purchases and is partitioned by day. Data arrives both on time and as late as 24 hours. Design a practical incremental dbt model 'analytics.daily_sales' that aggregates revenue by day, category, and customer_segment. Explain how you'd implement incremental logic, handle late data, guard against schema drift, and validate with tests and snapshots?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["Amazon","Anthropic","Tesla"]},{"id":"q-972","question":"In a dbt analytics project deployed to Snowflake, three tenants share a common model but data is isolated by schema prefixes (tenant_a_, tenant_b_, tenant_c_). Describe how you would structure tenancy-aware tests and a test runner to prevent cross-tenant data leakage during PR runs. Include how you configure sources, seeds, and per-tenant test filtering, and how you verify isolation in CI without touching production data?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["DoorDash","Lyft","OpenAI"]},{"id":"q-1146","question":"You're building a data ingestion service that validates JSON records before persisting them. Validation steps include NonEmpty fields, Email format, and PasswordStrength. Validators must be pluggable: new validators can be added at runtime by registering them into a Registry without touching the core pipeline. Design a minimal interface and registry, and show how to compose and execute the pipeline with a sample config. Include how to add a new validator and run the pipeline, returning the first failure?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["IBM","Square","Tesla"]},{"id":"q-1201","question":"Design a runtime-pluggable per-message transformation system for a streaming pipeline. Messages include metadata that determines the transformation strategy (e.g., enrich, normalize, validate). Implement with a design pattern that lets you add new strategies without modifying the pipeline core. Provide minimal interfaces, a registry, and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["MongoDB","Plaid","Salesforce"]},{"id":"q-1236","question":"Design a runtime-pluggable, per-route transformer and throttling policy system for a real-time event router. Each route maps an event type to a transformer and a rate-limit policy; new transformers (enrich, redact, normalize) and new policies (token-bucket, fixed-window, leaky-bucket) must be addable at runtime without touching the router core. Which pattern would you adopt and how would you structure the minimal interfaces and registry?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Oracle","Snowflake"]},{"id":"q-1258","question":"Design a runtime-extensible notification dispatch system that handles multiple channels (email, SMS, push) where new transports and their backoff strategies can be added at runtime via a registry without touching the core dispatcher. Specify the minimal interfaces, how you register a new transport and a new backoff policy, and show a usage example including adding a WhatsApp transport and a geometric backoff?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Coinbase","DoorDash","Plaid"]},{"id":"q-1300","question":"Design a feature-flag evaluation engine for a large SaaS product. Flags support boolean, percentage rollout, user-segment, and A/B bucket strategies. Create a pluggable evaluator where new strategies can be added at runtime via a Registry without touching the core evaluator. Provide interfaces, a thread-safe registry, and a usage example with versioned strategy lookup?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Discord","Google","Snowflake"]},{"id":"q-681","question":"You're building a multi-tenant API gateway for a Stripe-like payments service, backed by MongoDB storage and a Twitter-like feed. Each tenant has a per-minute rate limit. Describe a concrete solution using the Decorator pattern to enforce quotas, show how you'd implement atomic Redis updates, discuss burst handling and clock drift, and outline a minimal wrapper skeleton in code?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["MongoDB","Stripe","Twitter"]},{"id":"q-688","question":"You're building a feature flag system where flags can be evaluated as a hard boolean, a percentage rollout, or a targeted user segment. Design the architecture using a design pattern that lets you add new evaluation strategies without changing the caller. Which pattern would you choose and how would you implement it in code? Provide a minimal interface and usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Amazon","Google","Hashicorp"]},{"id":"q-695","question":"You're building a real-time data ingestion pipeline that must apply a sequence of transformations to each record. New transforms (normalization, enrichment, validation, anomaly detection) should be added as plugins without touching producer/consumer code. Design a pluggable Transform pipeline with per-tenant routing and hot-reload of configuration. Provide minimal interface and a usage example, including how to configure a few plugins and compose them for a stream?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Amazon","Google","Twitter"]},{"id":"q-706","question":"You're building an extensible data ingestion framework where new data formats (JSON, Parquet, ORC) must be supported without touching the core ingestion logic. Design the architecture using a pattern that decouples format parsing from the caller and lets you add new format handlers without changing the caller. Which pattern would you choose and how would you implement it in code? Provide a minimal interface and usage example?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Amazon","Google","MongoDB"]},{"id":"q-714","question":"You're building a small HTTP client wrapper that fetches a user profile, but the server occasionally fails. The caller selects a retry policy by name (linear, exponential, jitter) and fetchUser should retry using that policy without changing fetchUser's code. Design the architecture using a design pattern that lets you add new retry strategies without modifying the caller. Provide a minimal interface and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Google","Lyft","Salesforce"]},{"id":"q-717","question":"You're building a real-time event processing pipeline that validates, enriches, and filters events before persisting. New validators, enrichers, and filters must be added at runtime without altering the core processor. Which design pattern enables this extensibility and how would you implement it? Provide minimal interfaces and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Snowflake","Stripe"]},{"id":"q-729","question":"You're building an image-processing pipeline that applies a sequence of filters (blur, brighten, sharpen) to images. The pipeline must run on both CPU and GPU backends, and new backends must be pluggable without touching filter implementations or orchestration code. Design an architecture that decouples filters from backends using a design pattern, enabling adding a backend such as Vulkan without modifying core code. Provide a minimal interface and usage example?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Adobe","NVIDIA"]},{"id":"q-737","question":"In a messaging pipeline used by Zoom and Hugging Face, the system tokenizes, normalizes, and stores messages. You want to support swapping in different normalization strategies (lowercasing, diacritic removal, profanity filtering) without changing the pipeline code. Design the architecture using a design pattern that lets you add new normalization strategies without modifying the pipeline. Provide a minimal interface and a usage example. How would you implement this pattern to make additions painless?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Hugging Face","Zoom"]},{"id":"q-741","question":"In a telemetry alerting system for autonomous fleets, each customer needs a custom alert-threshold strategy for metrics like speed or battery: static value, percentile-based, or rolling window. The aggregator should surface alerts without depending on a concrete strategy. Design the architecture using a design pattern that lets you add new threshold strategies without modifying the aggregator. Provide a minimal interface and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Tesla","Uber"]},{"id":"q-750","question":"You’re building a data export utility that must support multiple formats. New formats can come from external libraries with different APIs. Design an architecture using a pattern that lets you add new formats without changing the core exporter. Which pattern would you use and how would you implement it? Provide a minimal interface and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Anthropic","Cloudflare","Tesla"]},{"id":"q-763","question":"You're designing a data ingestion pipeline where raw inputs pass through optional enhancers (encryption, compression, watermarking) implemented as decorators around a base DataSource. You must add new decorators without touching the core pipeline or existing decorators. Which pattern would you use and how would you implement minimal interfaces to compose them? Provide a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Instacart","Microsoft","Oracle"]},{"id":"q-769","question":"In a log analytics pipeline, you must support multiple formats (JSON, CSV, Protobuf) and multiple sinks (Elasticsearch, BigQuery, S3). Design an architecture using a design pattern that allows adding new formats or sinks without touching the producer. Provide minimal interfaces and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Lyft","MongoDB","Oracle"]},{"id":"q-781","question":"You're building a CLI tool that supports commands and subcommands, forming a tree (e.g., 'git remote add'). Design a Command interface that treats leaves (actual actions) and composites (groups of commands) uniformly. Implement LeafCommand and CommandGroup using the Composite pattern so a single call can execute a command or print help for a whole subtree without changing client code. Provide a minimal interface and usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Meta","Robinhood"]},{"id":"q-785","question":"You're building a pluggable HTTP request/response transformer pipeline inside a reverse proxy. Each Transformer can add, redact, or modify headers/body. The gateway must load new Transformers at runtime by name without redeploying. Design a minimal interface and registry-driven architecture that supports adding new transformer types without touching the gateway core. Provide a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Airbnb","Cloudflare","Meta"]},{"id":"q-793","question":"You're designing a streaming data processing framework where each record passes through a configurable pipeline of transformation steps. New transformations must be added at runtime without touching the orchestrator, and jobs select steps by name. Which design pattern and minimal interfaces would you use to register, compose, and execute transformations, ensuring type-safety and low churn when adding new steps? Provide a concise usage example?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Citadel","LinkedIn","Snowflake"]},{"id":"q-802","question":"Design a unit-test framework runner that supports multiple assertion styles (classic, fluent, should). The goal is to add a new assertion style (e.g., expect) at runtime without modifying the runner core. Which design pattern would be chosen and how would the minimal interfaces and a registry be structured? Provide a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Apple","Salesforce","Scale Ai"]},{"id":"q-809","question":"Design a health-check framework for a fleet-management service. The system aggregates health from multiple subsystems (database, message broker, geolocation API). New checks (checkDiskSpace, checkApiLatency) must be added at runtime without touching the aggregator. Which pattern supports this, and how would you implement minimal interfaces and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Lyft","Salesforce"]},{"id":"q-819","question":"You are designing a log export module for a distributed service. It must support exporting archives to multiple backends (S3, GCS, and an on-prem object store). New backends should be addable at runtime without touching the exporter or consumer code. Design the architecture using a design pattern that lets you plug in new backends via a registry. Provide a minimal interface and usage example?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["DoorDash","Netflix","Uber"]},{"id":"q-824","question":"Design a pluggable text-formatting pipeline for a CLI tool. The pipeline should allow new transforms to be added at runtime via a registry, without touching the core pipeline. Use a suitable pattern to compose these transforms in order; provide a minimal interface and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Bloomberg","Hashicorp","MongoDB"]},{"id":"q-833","question":"You're building a streaming analytics dashboard where widgets render different metrics. New chart renderers can be added at runtime by third-party teams without modifying core widgets. Design the architecture using a design pattern that supports pluggable renderers via a registry. Provide a minimal interface and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["NVIDIA","Netflix","Tesla"]},{"id":"gh-16","question":"What is Infrastructure as Code and why has it become essential for modern DevOps practices?","channel":"devops","subChannel":"automation","difficulty":"beginner","tags":["iac","terraform","ansible"],"companies":["Amazon","Google","Hashicorp","Microsoft","Netflix","Salesforce"]},{"id":"gh-18","question":"What is Ansible and how does it work for infrastructure automation?","channel":"devops","subChannel":"automation","difficulty":"beginner","tags":["iac","terraform","ansible"],"companies":["Amazon Web Services","Google Cloud","Microsoft","Red Hat","Southwest Airlines"]},{"id":"gh-29","question":"What is Configuration Management?","channel":"devops","subChannel":"automation","difficulty":"beginner","tags":["config-mgmt","ansible","chef"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Netflix"]},{"id":"gh-30","question":"What is Puppet and how does it manage infrastructure configuration?","channel":"devops","subChannel":"automation","difficulty":"beginner","tags":["config-mgmt","ansible","chef"],"companies":["Bank Of America","Cisco","Google","Microsoft","Staples"]},{"id":"gh-31","question":"What is Scalability in DevOps?","channel":"devops","subChannel":"automation","difficulty":"advanced","tags":["scale","ha"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-36","question":"How do different backup strategies balance storage efficiency, backup speed, and recovery time?","channel":"devops","subChannel":"automation","difficulty":"intermediate","tags":["backup","dr"],"companies":["Amazon","Google","LinkedIn","Microsoft","Uber"]},{"id":"gh-92","question":"How does a Service Catalog enable self-service infrastructure provisioning in an Internal Developer Platform?","channel":"devops","subChannel":"automation","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Spotify"]},{"id":"q-243","question":"How would you design a zero-downtime deployment strategy using Ansible that includes blue-green infrastructure setup, traffic management, and automated rollback capabilities?","channel":"devops","subChannel":"automation","difficulty":"intermediate","tags":["ansible","puppet","chef"],"companies":["Adobe","Amazon","Google","Microsoft","Netflix","Stripe"]},{"id":"q-269","question":"Compare Ansible, Puppet, and Chef configuration management tools, focusing on their architecture, state management approaches, and ideal use cases for enterprise environments?","channel":"devops","subChannel":"automation","difficulty":"beginner","tags":["ansible","puppet","chef"],"companies":["Adobe","Amazon","Google","IBM","Microsoft","Netflix"]},{"id":"q-304","question":"How would you design a multi-environment configuration management strategy using Ansible that supports development, staging, and production environments with role-based access control?","channel":"devops","subChannel":"automation","difficulty":"advanced","tags":["ansible","puppet","chef"],"companies":["Amazon","Google","Meta"]},{"id":"q-381","question":"You have 10 web servers that all need Nginx installed and configured identically. How would you use Ansible to ensure this configuration is consistent across all servers?","channel":"devops","subChannel":"automation","difficulty":"beginner","tags":["ansible","puppet","chef"],"companies":["Deepmind","Google","MongoDB"]},{"id":"q-421","question":"You're managing infrastructure at scale with Ansible, Puppet, and Chef. How would you design a configuration management strategy that handles secret rotation across 1000+ servers while ensuring zero-downtime deployments?","channel":"devops","subChannel":"automation","difficulty":"intermediate","tags":["ansible","puppet","chef"],"companies":["Discord","Meta","Scale Ai"]},{"id":"q-437","question":"You're migrating from Puppet to Ansible for configuration management. How would you handle idempotency differences and what strategy would you use to ensure zero-downtime during the transition?","channel":"devops","subChannel":"automation","difficulty":"intermediate","tags":["ansible","puppet","chef"],"companies":["Adobe","Amazon","Cloudflare","Hashicorp","IBM","Microsoft","Netflix","Salesforce"]},{"id":"q-459","question":"You're managing infrastructure at scale with Ansible. How would you design a strategy to handle configuration drift across 1000+ servers while ensuring minimal downtime during updates?","channel":"devops","subChannel":"automation","difficulty":"intermediate","tags":["ansible","puppet","chef"],"companies":["Cloudflare","Discord","Tesla"]},{"id":"q-490","question":"You're migrating a 500-server fleet from Puppet to Ansible with zero downtime. How would you design the migration strategy to ensure configuration consistency and rollback capabilities?","channel":"devops","subChannel":"automation","difficulty":"advanced","tags":["ansible","puppet","chef"],"companies":["Adobe","Microsoft","PayPal"]},{"id":"q-573","question":"How would you design a GitOps workflow using Terraform and ArgoCD to manage infrastructure across multiple cloud providers while ensuring zero-downtime deployments?","channel":"devops","subChannel":"automation","difficulty":"intermediate","tags":["ansible","puppet","chef"],"companies":["Cloudflare","OpenAI"]},{"id":"do-2","question":"Compare Blue/Green vs Canary deployment strategies, including traffic routing, monitoring, rollback complexity, and cost implications for a microservices architecture?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["deployment","strategy","cicd","jenkins"],"companies":null},{"id":"gh-1","question":"What are the core principles and practices of DevOps, and how does it bridge the gap between development and operations teams?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["basics"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-10","question":"What is a CI/CD pipeline and how does it automate software delivery?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["cicd","automation"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-102","question":"What is GitHub Actions and how does it work?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Digital Ocean","Goldman Sachs","Google","Microsoft"]},{"id":"gh-104","question":"What is Canary Analysis and how does it work in production deployments?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-11","question":"What is Jenkins and how does it facilitate continuous integration and continuous delivery (CI/CD) in modern software development workflows?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["cicd","automation"],"companies":["Amazon","Deutsche Bank","Goldman Sachs","Microsoft","Netflix"]},{"id":"gh-2","question":"How would you design a DevOps pipeline that reduces deployment time by 60% while improving reliability and security?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["basics"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-3","question":"What is Continuous Integration and how does it improve software development quality?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["basics"],"companies":["Amazon","Google","Microsoft","Netflix","Stripe"]},{"id":"gh-64","question":"What are the four key DORA metrics for measuring DevOps performance and how are they calculated?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["metrics","kpi"],"companies":["Amazon","Google","Hashicorp","Microsoft","Netflix","Salesforce"]},{"id":"gh-67","question":"How does Database DevOps integrate database schema changes into CI/CD pipelines while ensuring data integrity and minimizing downtime?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["db","devops"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Snowflake"]},{"id":"gh-68","question":"How would you implement comprehensive security practices in a DevOps pipeline including SAST/DAST, container security, and secrets management?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["security","network"],"companies":["Amazon","Cloudflare","Google","Microsoft","Netflix","Stripe"]},{"id":"gh-74","question":"How does DevOps culture transform traditional siloed development and operations into collaborative workflows?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["culture","soft-skills"],"companies":["Amazon","Google","LinkedIn","Microsoft","Netflix"]},{"id":"gh-75","question":"What DevOps practices are essential for implementing continuous delivery and fostering team collaboration?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["culture","soft-skills"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-90","question":"What is Blue/Green Deployment?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-177","question":"Explain the key differences between model serving and model deployment in ML systems, including specific technologies, scaling considerations, and real-world implementation patterns?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["mlops","deployment"],"companies":["Amazon","Databricks","Google","Meta","Microsoft","Netflix"]},{"id":"q-194","question":"How would you design a Terragrunt + Atlantis workflow that prevents state lock contention across 50+ microservice environments while maintaining DRY principles?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["dry","terragrunt","atlantis"],"companies":["Airbnb","Coinbase","Databricks","Stripe","Uber"]},{"id":"q-298","question":"Design a large-scale enterprise CI/CD system for an AWS-based application?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["ci-cd","aws","enterprise","containers","automation"],"companies":["Amazon","Google","Microsoft"]},{"id":"q-318","question":"How would you design a GitHub Actions workflow that runs tests in parallel across multiple matrix configurations while ensuring proper artifact management and failure handling?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["github-actions","jenkins","gitlab-ci"],"companies":["DoorDash","LinkedIn","Robinhood"]},{"id":"q-332","question":"You have a GitHub Actions workflow that's failing intermittently due to race conditions when multiple PRs trigger the same deployment pipeline. How would you design a solution to prevent concurrent deployments while maintaining fast feedback for developers?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["github-actions","jenkins","gitlab-ci"],"companies":["Amazon","Hulu","Jane Street"]},{"id":"q-398","question":"You have a GitHub Actions workflow that's failing intermittently due to rate limiting on a third-party API. How would you design a robust retry mechanism with exponential backoff while ensuring the workflow completes within the 6-hour timeout limit?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["github-actions","jenkins","gitlab-ci"],"companies":["Deepmind","Elastic","Webflow"]},{"id":"q-410","question":"You're setting up a CI/CD pipeline for a microservice that needs to run security scans, build a Docker image, and deploy to staging. How would you configure GitHub Actions to fail fast if security vulnerabilities are found, while still allowing the build to proceed for testing?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["github-actions","jenkins","gitlab-ci"],"companies":["Meta","Okta","PayPal"]},{"id":"q-444","question":"You have a GitHub Actions workflow that's failing intermittently due to rate limiting. How would you design a robust CI/CD pipeline that handles API rate limits, implements proper retry logic, and ensures consistent deployments across multiple environments?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["github-actions","jenkins","gitlab-ci"],"companies":["OpenAI","Tesla","Uber"]},{"id":"q-520","question":"You have a GitHub Actions workflow that's failing intermittently due to rate limiting when pulling Docker images. How would you design a robust solution that ensures consistent builds while minimizing costs?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["github-actions","jenkins","gitlab-ci"],"companies":["Apple","Square"]},{"id":"q-1159","question":"Design a CI/CD pipeline for a containerized application that needs to support multiple environment-specific configurations (dev, staging, prod) while maintaining security best practices. How would you structure the pipeline to handle secrets management, image scanning, and environment-specific deployments without duplicating pipeline code?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["ci-cd","container-security","secrets-management","multi-environment","kubernetes"],"companies":[]},{"id":"q-1339","question":"How would you implement a custom Kubernetes scheduler to handle specialized workloads like GPU-intensive ML jobs, and what components would need to be modified compared to the default scheduler?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["kubernetes","scheduler","custom-scheduler","gpu","ml-workloads"],"companies":[]},{"id":"q-1341","question":"How does Kubernetes handle pod preemption and priority classes when scheduling, and what happens when a high-priority pod needs to be placed but all nodes are at capacity?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["kubernetes","pod-scheduling","preemption","priority-classes","resource-management"],"companies":[]},{"id":"q-640","question":"Design a multi-region CI/CD pipeline for a global SaaS application that must achieve 99.99% uptime with zero-downtime deployments. The pipeline should handle blue-green deployments across 5 regions, implement circuit breakers for regional failures, and maintain data consistency. How would you architect this pipeline and what specific tools and strategies would you use?","channel":"devops","subChannel":"devops","difficulty":"advanced","tags":["multi-region","blue-green","gitops","circuit-breaker","zero-downtime"],"companies":[]},{"id":"q-641","question":"Design a CI/CD pipeline for a monolithic application that needs to be gradually migrated to microservices. How would you structure the pipeline to support both the monolith and new microservices during the transition period, ensuring minimal downtime and feature parity?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["ci-cd","microservices","migration","pipeline-design","monolith"],"companies":[]},{"id":"q-644","question":"How would you design a CI/CD pipeline that implements feature flagging and progressive delivery to enable zero-downtime deployments for a high-traffic e-commerce platform?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["feature-flags","progressive-delivery","zero-downtime","canary-deployment","monitoring"],"companies":[]},{"id":"q-648","question":"How would you implement a custom Kubernetes scheduler to handle specialized workloads like GPU-intensive ML jobs, and what components would need to be modified compared to the default scheduler?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["kubernetes","scheduler","custom-scheduler","gpu","ml-workloads","devops"],"companies":[]},{"id":"q-652","question":"How would you design a CI/CD pipeline that implements progressive delivery with feature flags, ensuring zero-downtime deployments while maintaining data consistency across multiple database services?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["feature-flags","progressive-delivery","database-migration","zero-downtime","blue-green-deployment"],"companies":[]},{"id":"q-654","question":"How would you implement a custom Kubernetes scheduler to handle specific business requirements like cost optimization or geographic placement, and what are the key components you'd need to modify?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["kubernetes","scheduler","custom-scheduler","cost-optimization","devops"],"companies":[]},{"id":"q-655","question":"How would you design a CI/CD pipeline that implements infrastructure-as-code with blue-green deployments while ensuring zero-downtime database schema migrations?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["infrastructure-as-code","blue-green-deployment","database-migration","zero-downtime"],"companies":[]},{"id":"q-656","question":"How would you implement a custom Kubernetes scheduler to prioritize pods based on business criticality levels, and what components would you need to modify or extend?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["kubernetes","custom-scheduler","pod-priority","scheduling-framework","devops"],"companies":[]},{"id":"q-657","question":"How would you design a CI/CD pipeline that implements infrastructure-as-code with immutable infrastructure patterns, ensuring zero-downtime deployments while maintaining compliance and audit trails for a regulated industry?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["infrastructure-as-code","immutable-infrastructure","compliance","zero-downtime","audit-trails"],"companies":[]},{"id":"gh-37","question":"What is Cloud Native Architecture?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["cloud-native","microservices"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-4","question":"What is Docker and how does containerization differ from traditional virtualization in terms of architecture and resource efficiency?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["docker","containers"],"companies":["Amazon","Google","Microsoft","Netflix","PayPal","Uber"]},{"id":"gh-5","question":"Explain the Docker image and container lifecycle, including image layers, copy-on-write, container states, and resource isolation mechanisms?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["docker","containers"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"gh-6","question":"What is a Dockerfile and how does it enable containerized application deployment?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["docker","containers"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Netflix"]},{"id":"q-171","question":"You have a Docker container that keeps crashing and restarting in production. How would you systematically debug this issue without modifying the container image, and what specific Docker commands and monitoring techniques would you use?","channel":"devops","subChannel":"docker","difficulty":"intermediate","tags":["docker","containers"],"companies":["Amazon","Databricks","Google","Microsoft","Netflix","Snowflake"]},{"id":"q-191","question":"What is the purpose of a multi-stage Docker build and how does it reduce final image size?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["dockerfile","compose","multi-stage"],"companies":["Amazon","Capital One","Google","Microsoft","Uber"]},{"id":"q-289","question":"How do you implement multi-stage builds in Docker to optimize image size and security while maintaining build cache efficiency?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["dockerfile","compose","multi-stage"],"companies":["Amazon","Google","Netflix","Spotify","Uber"]},{"id":"q-344","question":"You're deploying a Node.js microservice to production and notice the Docker image is 850MB. How would you optimize it using multi-stage builds, and what are the key trade-offs between image size and build time?","channel":"devops","subChannel":"docker","difficulty":"intermediate","tags":["dockerfile","compose","multi-stage"],"companies":["Aurora","Shopify","Snowflake"]},{"id":"q-354","question":"You need to deploy a Node.js microservice to SAP's production environment. The current Dockerfile is 1.2GB and includes build tools. How would you optimize it using multi-stage builds to reduce the image size under 200MB?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["dockerfile","compose","multi-stage"],"companies":["Apple","Elastic","Sap"]},{"id":"q-670","question":"Given a monorepo with two services: a Node.js API and a Python worker, design multi-stage Dockerfiles to produce minimal production images, using BuildKit secrets for API keys at build time without bake-in. Write a docker-compose.yml to build and run both services on a shared network, mount a logs volume, and run as a non-root user. How would you implement end-to-end?","channel":"devops","subChannel":"docker","difficulty":"intermediate","tags":["dockerfile","compose","multi-stage"],"companies":["Anthropic","Google","LinkedIn"]},{"id":"q-671","question":"Given a minimal FastAPI app (main.py) with requirements.txt, write a 2-stage Dockerfile to build and run it in a slim final image. Ensure the app runs as a non-root user, and the runtime image only contains Python and the app. Create a docker-compose.yml that starts the web service and a Redis cache, exposes port 8000, and reads config from .env. How would you verify the image size and run locally?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["dockerfile","compose","multi-stage"],"companies":["Apple","Meta","Salesforce"]},{"id":"gh-27","question":"Design a Git-based collaboration system for a 50-person distributed team. How would you implement branching strategies, conflict resolution, and CI/CD integration to ensure 99.9% uptime while handling 1000+ daily commits?","channel":"devops","subChannel":"gitops","difficulty":"advanced","tags":["git","vcs"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"gh-28","question":"What is Git Branching Strategy?","channel":"devops","subChannel":"gitops","difficulty":"beginner","tags":["git","vcs"],"companies":["Amazon","Goldman Sachs","Google","Meta","Microsoft"]},{"id":"gh-53","question":"What is GitOps and how does it work in practice?","channel":"devops","subChannel":"gitops","difficulty":"beginner","tags":["automation","tools"],"companies":["Amazon Web Services","Gitlab","Google","Microsoft","Netflix"]},{"id":"gh-54","question":"What is ArgoCD and how does it implement GitOps for Kubernetes deployments?","channel":"devops","subChannel":"gitops","difficulty":"beginner","tags":["automation","tools"],"companies":["Amazon","Google","Hashicorp","IBM","Microsoft","Netflix"]},{"id":"q-217","question":"How would you design a GitOps multi-cluster deployment strategy using ArgoCD that handles blue-green deployments with zero-downtime rollback across 50+ clusters while maintaining state consistency?","channel":"devops","subChannel":"gitops","difficulty":"advanced","tags":["argocd","flux","declarative"],"companies":["Amazon","Google","Microsoft","Red Hat","Uber"]},{"id":"q-366","question":"How would you design a GitOps workflow using ArgoCD to deploy a microservices application across multiple environments?","channel":"devops","subChannel":"gitops","difficulty":"intermediate","tags":["gitops","argocd","kubernetes","deployment","automation"],"companies":["Amazon","Google","Meta"]},{"id":"q-429","question":"You're setting up GitOps for a microservices deployment. How would you configure ArgoCD to automatically sync changes from your Git repository to Kubernetes, and what's the difference between declarative and imperative approaches in this context?","channel":"devops","subChannel":"gitops","difficulty":"beginner","tags":["argocd","flux","declarative"],"companies":["Amazon","DoorDash","Google","Hashicorp","Lyft","Microsoft","Netflix"]},{"id":"q-547","question":"You're implementing GitOps for a microservices application. How would you configure ArgoCD to automatically sync changes from your Git repository to Kubernetes, and what would you set as the sync policy to ensure safe deployments?","channel":"devops","subChannel":"gitops","difficulty":"beginner","tags":["argocd","flux","declarative"],"companies":["IBM","NVIDIA","Tesla"]},{"id":"q-668","question":"Describe a practical, automated secret rotation flow in a multi-cluster GitOps setup using ArgoCD and Flux. Include how Vault, ExternalSecret/SealedSecret, and per-cluster Secrets interact, what triggers rotation, how drift is prevented, and how rollback/auditing is handled across clusters?","channel":"devops","subChannel":"gitops","difficulty":"advanced","tags":["argocd","flux","declarative"],"companies":["Apple","Plaid","Zoom"]},{"id":"q-669","question":"You manage a single service deployed to Kubernetes with declarative manifests stored in git. Using both Argo CD and Flux in a GitOps pipeline, describe a practical beginner-friendly approach to implement blue-green or canary deployment, including the minimal YAML you'd configure in Argo CD to promote from staging to prod, and how you'd handle secrets?","channel":"devops","subChannel":"gitops","difficulty":"beginner","tags":["argocd","flux","declarative"],"companies":["Amazon","DoorDash","Google"]},{"id":"q-633","question":"Design a CI/CD pipeline for a microservices application with 10 services, where each service has its own repository. The pipeline must support parallel deployments, canary releases, and automatic rollback on failure. How would you structure the pipeline and what tools would you use?","channel":"devops","subChannel":"kubernetes-devops","difficulty":"advanced","tags":["CI/CD","microservices","kubernetes","gitops","canary-deployments"],"companies":["Google","Netflix","Uber","Spotify","Airbnb"]},{"id":"q-600","question":"Design a CI/CD pipeline for a microservices application that includes automated testing, security scanning, and deployment to multiple environments (dev, staging, prod). What are the key components and how would you ensure zero-downtime deployments?","channel":"devops","subChannel":"pipeline-architecture","difficulty":"intermediate","tags":["CI/CD","microservices","docker","kubernetes","security","zero-downtime"],"companies":["Google","Amazon","Microsoft","Netflix","Uber","Spotify"]},{"id":"q-602","question":"Design a CI/CD pipeline for a microservices application with the following requirements: automated testing, containerization, blue-green deployment, and rollback capabilities. What tools and stages would you include?","channel":"devops","subChannel":"pipeline-architecture","difficulty":"intermediate","tags":["CI/CD","microservices","kubernetes","docker","devops"],"companies":["Google","Amazon","Microsoft","Netflix","Uber","Spotify"]},{"id":"q-613","question":"Design a CI/CD pipeline for a microservices application with 10 services. How would you handle deployment strategies, testing, and rollback mechanisms?","channel":"devops","subChannel":"pipeline-architecture","difficulty":"intermediate","tags":["CI/CD","microservices","kubernetes","docker","deployment"],"companies":["Google","Amazon","Microsoft","Netflix","Uber","Spotify"]},{"id":"q-629","question":"Design a CI/CD pipeline for a microservices application that includes automated testing, security scanning, and multi-environment deployments. What components would you include and how would you structure the pipeline stages?","channel":"devops","subChannel":"pipeline-architecture","difficulty":"intermediate","tags":["cicd","microservices","devops","automation","security"],"companies":["Amazon","Google","Microsoft","Netflix","Spotify"]},{"id":"q-594","question":"How does Kubernetes decide which node to schedule a pod on, and what factors can influence this decision?","channel":"devops","subChannel":"pod-scheduling","difficulty":"intermediate","tags":["kubernetes","scheduling","kube-scheduler","node-selection","resource-management"],"companies":["Google","Amazon","Microsoft","Red Hat","VMware","IBM"]},{"id":"q-608","question":"How does Kubernetes handle pod scheduling when a node becomes resource-constrained, and what mechanisms can be used to ensure critical pods remain running?","channel":"devops","subChannel":"pod-scheduling","difficulty":"intermediate","tags":["kubernetes","scheduling","resource-management","priority","preemption"],"companies":["Google","Netflix","Uber","Amazon","Microsoft"]},{"id":"q-638","question":"Explain how Kubernetes scheduler decides which node to place a pod on, and what factors can cause a pod to remain in a Pending state?","channel":"devops","subChannel":"pod-scheduling","difficulty":"intermediate","tags":["kubernetes","scheduling","pod-management","troubleshooting","resource-allocation"],"companies":["Google","Amazon","Microsoft","Red Hat","VMware"]},{"id":"q-635","question":"How does Kubernetes handle pod scheduling when a node becomes unavailable, and what mechanisms ensure high availability?","channel":"devops","subChannel":"pod-scheduling-failover","difficulty":"intermediate","tags":["kubernetes","scheduling","high-availability","pod-management","failover"],"companies":["Google","Amazon","Microsoft","Red Hat","VMware","IBM"]},{"id":"q-620","question":"Explain the difference between node affinity, node selectors, and taints/tolerations in Kubernetes pod scheduling. When would you use each?","channel":"devops","subChannel":"scheduling-mechanisms","difficulty":"intermediate","tags":["kubernetes","pod-scheduling","node-affinity","taints-tolerations","devops"],"companies":["Google","Amazon","Microsoft","Netflix","Uber"]},{"id":"q-1392","question":"In a three-node Docker Swarm spanning two data centers, deploy a stateless API across an overlay network and ensure zero-downtime upgrades via canary, using update_config (start-first, parallelism 1). Outline the exact sequence of commands to init/join the swarm, create the overlay, deploy the service with update settings, and perform a canary upgrade with health checks. Include a minimal docker-compose snippet showing update_config?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Airbnb","Cloudflare","Two Sigma"]},{"id":"q-1412","question":"Scenario: you’re building a beginner-friendly Docker Compose setup for a FastAPI microservice with PostgreSQL on a single host. Create Dockerfiles for the app and an init script, plus a docker-compose.yml with a named volume for Postgres data, healthchecks, and a startup script that waits for PostgreSQL on port 5432 before starting the app. Explain the exact files, commands, and deployment sequence?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Hugging Face","IBM","Stripe"]},{"id":"q-1443","question":"In a 3-node Swarm across DC-A and DC-B, deploy a stateless API via overlay api-net with DC-affinity (2 replicas in DC-A and 1 in DC-B). Outline the exact CLI steps to init/join, create the overlay, and deploy two services with update_config (order: start-first, parallelism: 1). Describe a canary upgrade process that gradually updates replicas across DCs and validates with health checks. Include a minimal docker-compose snippet showing the two services, the overlay network, and update_config?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Amazon","Netflix"]},{"id":"q-1530","question":"In a three-node Docker Swarm spanning two data centers, deploy a stateless API with TLS termination that uses Vault for dynamic TLS certificate rotation. Use Docker secrets to distribute certs, and implement a lightweight sidecar that refreshes certificates without dropping connections. Configure a canary-style rolling upgrade ensuring zero-downtime during cert rotations. Outline the exact steps: swarm init/join, overlay network creation, stack/deploy with secret handling, and the certificate rotation workflow with health checks?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Amazon","Meta"]},{"id":"q-1579","question":"Design a secret-rotation workflow for a Docker Swarm stack that uses Vault to rotate a TLS cert and a database credential with zero downtime. Use Swarm secrets and a rolling update (start-first). Outline exact steps: swarm init/join, overlay network, create secrets, deploy stack, Vault rotate trigger, service update commands. Include a minimal docker-compose snippet showing secret usage and update_config?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Bloomberg","Databricks","Hashicorp"]},{"id":"q-1628","question":"Scenario: In a 3-node Swarm across two DCs, deploy an API service behind Traefik with image signing enforcement (cosign/DOCKER_CONTENT_TRUST) and implement a canary upgrade to v2 with a 10% traffic split via a separate api-v2-canary service. Outline exact commands, Swarm update_config usage, and docker-compose/service definitions to achieve zero-downtime upgrade and safe rollback?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Cloudflare","IBM","MongoDB"]},{"id":"q-1653","question":"Scenario: On a single host, implement a beginner-friendly Docker Compose stack: a Node.js API that talks to Redis and a Fluent Bit logging service that reads logs from the API via a shared volume and forwards them to stdout. Provide a Dockerfile for the API, a docker-compose.yml with api, redis, fluent-bit, a logs volume, and healthchecks; explain the run steps and verification?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Hugging Face","Instacart","Salesforce"]},{"id":"q-1688","question":"Beginner-level: design a local docker-compose stack for a Node.js API that uses Redis as a cache. Provide a Dockerfile for the API, a startup script that waits for Redis to be reachable on 6379 before starting, and a docker-compose.yml with healthchecks for both services on a shared network. Include exact file contents or minimal snippets, the commands to build and run, and how to validate a cache-hit endpoint?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["DoorDash","MongoDB","Scale Ai"]},{"id":"q-1760","question":"In a three-node Docker Swarm spanning two data centers, introduce a new internal auth service that all APIs depend on. Roll it out with zero downtime and a canary, using update_config (start-first, parallelism 1), Docker Secrets, and a routing shim. Provide exact commands to init/join the swarm, create the overlay, deploy the stack, add the secret, and perform the canary upgrade with health checks?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["LinkedIn","Tesla"]},{"id":"q-1786","question":"In a 3-node Docker Swarm spanning two data centers, deploy a GPU-accelerated model-serving API (TorchServe) using the NVIDIA runtime. Expose it behind an internal overlay network and a simple LB. Ensure zero-downtime upgrades with canary traffic shifts and a pre-warm sidecar to warm the new replica without serving traffic. Outline the steps for swarm init/join, overlay creation, service spec with GPU constraints, secret handling, and a separate migration container if needed. Include a minimal docker-compose snippet showing update_config (start-first, parallelism 1)?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Databricks","Google"]},{"id":"q-1832","question":"Design and implement a zero-downtime upgrade for a 4-node Docker Swarm across two data centers hosting a stateful Redis queue and a stateless worker service. The worker consumes messages encoded in a new proto format; the cluster uses TLS mutual authentication with Vault for cert rotation and Docker Secrets for credentials. Outline exact upgrade steps, a 10% canary rollout, health checks, and a rollback plan, and include a minimal docker-compose snippet showing update_config(order: start-first, parallelism: 1)?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Amazon","Two Sigma","Zoom"]},{"id":"q-1967","question":"In a production Swarm across two data centers with four nodes, implement end-to-end image provenance using Cosign. Sign CI artifacts, publish signatures to a registry, and enforce verification before deploys. Outline the exact steps: key management, signing workflow in CI, signature verification at pull-time, updating services with image digests, and a rollback plan if verification fails. Provide concrete Cosign commands and how to incorporate into a CI/CD pipeline?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Amazon","DoorDash","Microsoft"]},{"id":"q-2016","question":"In a three-node Docker Swarm spanning two data centers, deploy a stateless API behind a Traefik ingress with a Redis-backed cache layer and mutual TLS between services. Use Docker secrets/configs for TLS certs and cache creds. Apply zero-downtime upgrades via update_config (start-first, parallelism 1) and implement a canary upgrade path with health checks and automatic rollback. Outline the exact init/join commands, overlay creation, stack file, and upgrade sequence?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Discord","Twitter"]},{"id":"q-2022","question":"Scenario: On a single host, implement a blue-green deployment for a stateless API behind an Nginx reverse proxy using docker-compose. Start with green (v1) receiving all traffic; deploy blue (v2) and switch traffic to blue only after a 30-second health-check window confirms readiness, ensuring zero downtime. Provide: a) docker-compose.yml with two API services (api-green and api-blue) and Nginx, b) nginx.conf with a switchable upstream, c) a Bash script upgrade.sh that promotes blue by reconfiguring upstream and reloading Nginx, d) exact commands to bring up green, deploy blue, run the upgrade, and verify with curl?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Adobe","Hashicorp","Snowflake"]},{"id":"q-2053","question":"In a four-node **Docker Swarm** spanning two data centers, deploy a stateless API with a Redis-backed rate limiter and a shared cache. Implement a canary upgrade of the API using update_config (start-first, parallelism 1), with a front-door that routes the canary subset using label-based routing. Outline exact commands: swarm init/join, overlay creation, stack deploy, and the health-check-driven promotion. Include a minimal docker-compose snippet showing update_config?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Instacart","Meta"]},{"id":"q-2120","question":"In a two-node Swarm, deploy a TLS-secured stateless API behind an Nginx proxy. Use Docker secrets for TLS certs, a Docker config for API settings, and a small sidecar that ships logs without affecting requests. Attach both services to a single overlay network and perform a canary upgrade with a rolling update (start-first, parallelism 1). Provide exact commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Netflix","Salesforce"]},{"id":"q-2140","question":"In a two-datacenter Docker Swarm (2 nodes per DC), deploy a stateless API behind Traefik with TLS termination on an overlay network across DCs. Implement a canary rollout for a feature-flag change using a Swarm Config and route 10% of traffic to canary via Traefik labels. Outline exact swarm init/join commands, overlay creation, stack deploys, config creation, and the canary upgrade with health checks and rollback criteria?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Apple","Meta"]},{"id":"q-856","question":"You're running a Docker Swarm with services frontend, api, and worker. A feature-flag config is provided via Docker Config mounted at /etc/flags.json in all containers. You must rotate this config weekly with zero downtime. Describe the exact sequence of commands to create a new config version, rotate the services to use it, and implement a graceful reload inside apps so the new flags are picked up without losing requests. Include any Swarm update options you would tune?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Hashicorp","Snowflake"]},{"id":"q-862","question":"In a Docker Swarm with a stateful web app that uses Postgres, you must roll out version 3.2 with a DB schema migration and zero downtime. Propose a concrete upgrade plan that uses a start-first, one-task-at-a-time update, a separate migration container, and post-migration validation. Include exact Swarm commands and a minimal docker-compose snippet showing update_config?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["IBM","PayPal"]},{"id":"q-1262","question":"You're given an array A of length n and an integer k. You must select exactly k non-overlapping, contiguous subarrays (non-empty) to maximize the sum of all selected elements. Return the maximum sum and the k subarrays (start and end indices). Propose a dynamic programming formulation with states and recurrences, include reconstruction, and discuss time/space complexity?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Hugging Face","Snap"]},{"id":"q-1872","question":"You're given an n x m grid of integers grid[i][j]. From (0,0) to (n-1,m-1) you may move only right or down. You may turn at most K times (a turn is a change between directions). Return the maximum sum along a valid path and the path coordinates. Propose a DP with states dp[i][j][t][dir], recurrences, base cases, and reconstruction, and discuss time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["LinkedIn","MongoDB","Slack"]},{"id":"q-1941","question":"You're given an array A of length n and an integer k. You must select at most k elements such that no two selected elements are adjacent. Return the maximum sum you can obtain and the list of selected indices. Propose a DP formulation with states dp[i][j], recurrences, base cases, and reconstruction?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Apple","Instacart","Meta"]},{"id":"q-2170","question":"You're given an array values[0..n-1] of positive integers. Two players take turns removing either the leftmost or rightmost value and add it to their score. Assuming both play optimally, return A's maximum guaranteed total score and the move sequence (L/R) for each turn. Provide a DP formulation with dp[i][j] as the maximum score difference for subarray i..j, the base cases, and how to reconstruct the moves?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Apple","Lyft","Tesla"]},{"id":"q-679","question":"In a warehouse grid of size n x m, each cell has a traversal cost. You can move only right or down from (0,0) to (n-1,m-1). You have a one-time token to halve the cost of exactly one visited cell. Design an O(nm) DP to compute the minimum path cost after optimally using the discount, and describe the recurrences and a space-optimized implementation?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Anthropic","Snap","Twitter"]},{"id":"q-691","question":"You're planning a delivery route along a straight street of n blocks. At block i you can advance up to jumps[i] blocks (at least 1). How many distinct routes reach block n-1 from block 0? If unreachable, return 0. Propose a dynamic-programming approach with dp[i] as ways to reach i and outline its time/space complexity, edge cases, and a brief correctness justification. How would you implement it?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["DoorDash","Meta","Square"]},{"id":"q-693","question":"You're building a daily workout planner. Over n days, you can pick Light (L) or Heavy (H) workouts, with a cooldown: after a Heavy, you must skip the next two days (no workouts). Given integers n and r, how many length-n sequences contain exactly r Heavy workouts and satisfy the cooldown rule? Provide a DP formulation with state dp[i][c][t] (days processed, cooldown days left, heavies used) and outline time/space complexity, edge cases, and a brief correctness justification?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Citadel","Google","Microsoft"]},{"id":"q-703","question":"You're navigating a warehouse grid of size n x m. Each cell (i, j) has a risk value r[i][j] ≥ 0, where higher means less safe. You may move only right or down from (0,0) to (n-1,m-1). Devise a dynamic-programming solution to minimize the maximum risk encountered on the path (i.e., minimize max(r[i][j]) along the path). Define a suitable dp[i][j], give the recurrence and base cases, describe reconstruction of the path, and analyze time/space complexity. How would you handle blocked cells by setting r[i][j] = INF?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["NVIDIA","Scale Ai","Stripe"]},{"id":"q-716","question":"You're building a text formatter. Given a list of word lengths L = [l1, l2, ..., ln] and a maximum line width W, wrap the words into lines so that all lines except the last incur a penalty of (W - usedWidth)^2, where usedWidth = sum of word lengths on the line plus spaces between words. The last line has 0 penalty. Propose a dynamic programming solution with dp[i] representing the minimum penalty for words i..n-1, specify the recurrence, reconstruction method, and time/space complexity. How would you implement it?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Anthropic","DoorDash","Square"]},{"id":"q-718","question":"You're given an array A[1..n] and an integer k. Partition into exactly k contiguous segments. The cost of a segment [t+1..i] is (sum(A[t+1..i]))^2. Return the minimum total cost and the partition indices. Propose DP: P as prefix sums, dp[i][j] = min_{t in [j-1..i-1]} dp[t][j-1] + (P[i]-P[t])^2, with base dp[0][0]=0. Explain reconstruction and discuss naive vs. optimized time, space, and edge cases?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Bloomberg","Netflix"]},{"id":"q-731","question":"You're given an array A of length n with non-negative integers representing daily story points. You must partition the days into consecutive weeks, each containing 2–7 days. The cost of a week is (sum of that week's points − W)^2 where W is a fixed target. Return the minimum total cost to cover all days or INF if impossible. Propose a DP with dp[i] as min cost for first i days; dp[i] = min_{k=2..7, i-k>=0} dp[i-k] + (sum(i-k+1..i) − W)^2, using prefix sums for O(1) range sums. Reconstruct weeks and analyze time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Databricks","Salesforce","Two Sigma"]},{"id":"q-736","question":"You're given an n x m grid. Each cell (i, j) has a color c[i][j] in [0, C-1] and a non-negative cost w[i][j]. Start at (0,0) and move to (n-1,m-1) with only right or down moves. You must visit at least one cell of every color that appears in the grid along your path. Return the minimum total cost to do so, or -1 if impossible. Describe a DP using dp[i][j][mask] where mask tracks visited colors; explain base cases, transitions from top/left, how to reconstruct the path, and complexity?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Microsoft","Netflix","Plaid"]},{"id":"q-743","question":"You're given a list of n task durations L. Partition the tasks into consecutive days (each day at least one task); the cost of a day is the maximum duration on that day. Devise a DP to minimize the total cost across all days. Define dp[i] as the minimum cost for the first i tasks, provide the recurrence, reconstruction method, and complexity. How would you implement it?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Slack","Stripe","Twitter"]},{"id":"q-754","question":"Given a circular array v[1..n] of non-negative values and an integer d≥1, pick a subset of indices such that the cyclic distance between any two chosen indices is at least d. Maximize the sum of selected values; return both the maximum sum and the number of distinct optimal subsets. Constraints: n ≤ 2e5, v[i] ≤ 1e9. Describe an O(n) DP solution with two cases for circularity and how you'd reconstruct counts?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Apple","Netflix","Uber"]},{"id":"q-768","question":"You're given an array A of length n and an integer k. Partition A into exactly k non-empty contiguous subarrays. The cost of a subarray [t+1..i] is (max(A[t+1..i])) * (i-t). Return the minimum total cost and the partition indices. Propose DP: dp[i][j] = min_{t in [j-1..i-1]} dp[t][j-1] + max(A[t+1..i]) * (i-t). Explain reconstruction, base cases, and time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Adobe","Robinhood","Salesforce"]},{"id":"q-777","question":"You're given an array A of non-negative integers representing task durations in order. You must schedule all n tasks into exactly m days. Each day can host a consecutive block with total duration <= D. The cost of a day is (sum of that day's durations)^2. Return the minimum total cost, or -1 if impossible? Propose a DP using dp[i][d] as min cost for first i tasks in d days, with transition dp[i][d] = min_{j<i, prefix[i]-prefix[j] <= D} dp[j][d-1] + (prefix[i]-prefix[j])^2. Include base cases, reconstruction, and time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Amazon","Apple","Robinhood"]},{"id":"q-786","question":"You have an array A of length n. Partition into exactly k non-empty contiguous blocks. The cost of a block [t+1..i] is (max(A[t+1..i])) * (sum(A[t+1..i])). Return the minimum total cost and the partition indices. Propose a DP formulation, reconstruction strategy, and complexity analysis. Assume 1-based indexing?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Coinbase","Scale Ai","Snap"]},{"id":"q-792","question":"You're given an array prices[0..n-1]. You may complete at most k buy-sell transactions (one share at a time, can't hold more than one). Return the maximum profit and the days of each trade. Propose a DP formulation with states cash[i][t] and hold[i][t], include recurrences, base cases, and reconstruction, and discuss time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Goldman Sachs","IBM","MongoDB"]},{"id":"q-797","question":"You have an array of task difficulties A of length n. Partition it into exactly m non-empty contiguous chapters. Each chapter cost equals the maximum difficulty within that chapter. Return the minimum total cost and a valid partition (chapter end indices). Propose a DP: dp[i][j] = min_{t in [j-1..i-1]} dp[t][j-1] + max(A[t..i-1]), with base dp[i][1] = max(A[0..i-1]). Explain reconstruction, base cases, and time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Adobe","LinkedIn","OpenAI"]},{"id":"q-806","question":"You're given an n x m grid of digits grid[i][j] in [0..9]. You may move only right or down from (0,0) to (n-1,m-1). Define a path score as the number of times the next cell's digit is strictly larger than the previous cell's digit along the path. Return the maximum score and a valid path (as coordinates or directions). Propose a dynamic programming formulation with recurrences, base cases, and reconstruction, and discuss time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Google","Hugging Face","Tesla"]},{"id":"q-813","question":"You're given a tree with N nodes (N up to 2e5). Each node i has a value val[i]. Find a connected subtree of exactly K nodes that maximizes the sum of values. Return the maximum sum and the node set. Propose a DP formulation with dp[u][s] = max sum of a connected subtree of size s that contains u and lies entirely within u's subtree when the tree is rooted at 1; include reconstruction, base cases, and discuss time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Google","Microsoft"]},{"id":"q-821","question":"You're given a string s of length n consisting of lowercase letters. Partition s into at most k non-empty contiguous substrings. The cost of a substring is the number of distinct characters in that substring. Return the minimum total cost and one valid partition (end indices). Propose a DP formulation with recurrence dp[i][t] = min_{p in [t-1..i-1]} dp[p][t-1] + cost(p, i-1) where cost(p, q) is the number of distinct letters in s[p..q]. Explain reconstruction, base cases, and time/space complexity?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Citadel","LinkedIn","Snap"]},{"id":"q-829","question":"You're given an array A of length n and an integer k. Partition A into exactly k non-empty contiguous subarrays. Each subarray's cost is max(A[l..r]) - min(A[l..r]). Return the minimum total cost and one valid partition (end indices). Propose a DP: dp[i][t] = min_{p in [t-1..i-1]} dp[p][t-1] + (max(A[p..i-1]) - min(A[p..i-1])); base: dp[i][1] = max(A[0..i-1]) - min(A[0..i-1]). Explain reconstruction, base cases, and time/space complexity?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["DoorDash","Hugging Face","Netflix"]},{"id":"q-235","question":"How do you organize Cypress fixtures for component testing, and what are the key patterns for managing test data dependencies across multiple test suites?","channel":"e2e-testing","subChannel":"cypress","difficulty":"beginner","tags":["cypress","component-testing","fixtures"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"q-1173","question":"Design an end-to-end test for GDPR data deletion in an e-grocery platform: a synthetic user requests account deletion that must purge PII from cart, catalog, checkout, loyalty services, and analytics pipelines across three regions. Describe data setup, purge verification across stores, caches, and search indexes, audit logs, and idempotency after replay; outline concrete tooling (Playwright + REST mocks + Kafka) and how you handle eventual consistency and test isolation?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Apple","DoorDash"]},{"id":"q-1335","question":"Design an end-to-end test strategy for a multi-tenant SaaS app with per-tenant data isolation, dynamic feature flags, and role-based access controls. Outline how you seed tenants, run parallel tests across tenants and roles, validate feature gating, and verify audit logs across services. Include tooling choices, data isolation strategy, and cleanup?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Adobe","Tesla"]},{"id":"q-1420","question":"Design a beginner-friendly E2E test for a CMS bulk-upload feature: a CSV with 100 rows is uploaded; validations run in the background; after completion, 100 pages are created with titles from the CSV; outline specific UI steps, how you verify progress, how you confirm data creation via API, and how you isolate the test to avoid polluting production data?","channel":"e2e-testing","subChannel":"general","difficulty":"beginner","tags":["e2e-testing"],"companies":["Adobe","Twitter"]},{"id":"q-1603","question":"Design an E2E test for a real-time collaborative post editor behind a region-specific feature flag (US vs EU) in a Twitter/Airbnb-like product. The test should cover flag gating, optimistic UI updates, WebSocket propagation across regions, eventual consistency after reconnections, and a rollback path when the flag is disabled. Which tooling and concrete steps would you use?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Airbnb","Twitter"]},{"id":"q-1713","question":"Design an end-to-end test for a Plaid-like bank-link flow embedded in a fintech app, covering OAuth-like redirects, token exchange, and account data pulls. Include how you isolate test data, simulate bank outages, verify idempotent ledger events, and ensure eventual consistency across retries?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Microsoft","Plaid"]},{"id":"q-1746","question":"Design an E2E test for an embeddable form builder widget used by partners (Salesforce, Cloudflare, Coinbase) that runs inside an iframe. It uses postMessage for cross-origin events, drafts in localStorage, and submits to a CORS API. Create an end-to-end test that validates tenant isolation, iframe messaging, draft autosave after brief network blips, and successful submission across two partner domains?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Cloudflare","Coinbase","Salesforce"]},{"id":"q-1893","question":"Design an end-to-end test for a multi-tenant data-export feature: an admin triggers an export via the UI; the export runs in the background, reads tenant-scoped data from multiple services, writes to blob storage, and emails a download link. How would you verify tenant isolation, idempotent retries, and resilience to blob outages without cross-tenant leakage? Include concrete tooling choices and test data strategy?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Airbnb","Microsoft"]},{"id":"q-449","question":"How would you design an E2E testing strategy for a distributed edge computing platform that needs to validate functionality across 100+ global data centers with varying network conditions?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Cloudflare","Tesla"]},{"id":"q-460","question":"You're testing a login form with Playwright. The form has email and password fields, and a submit button. How would you write a basic E2E test to verify successful login and redirect to dashboard?","channel":"e2e-testing","subChannel":"general","difficulty":"beginner","tags":["e2e-testing"],"companies":["Lyft","MongoDB","NVIDIA"]},{"id":"q-491","question":"How would you set up a basic E2E test for a login form using Playwright?","channel":"e2e-testing","subChannel":"general","difficulty":"beginner","tags":["e2e-testing"],"companies":["IBM","Lyft","Snowflake"]},{"id":"q-521","question":"You're testing a React app with Playwright. Some tests fail intermittently due to API delays. How would you make your e2e tests more reliable without removing the API dependency?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Google","Hugging Face"]},{"id":"q-548","question":"How would you design a scalable E2E testing strategy for a microservices architecture with 50+ services, ensuring test isolation and parallel execution while maintaining realistic user journeys?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["LinkedIn","Scale Ai"]},{"id":"q-574","question":"How would you handle flaky E2E tests in a CI/CD pipeline? What strategies would you implement to ensure reliable test execution?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["LinkedIn","Meta","Oracle"]},{"id":"q-850","question":"In a Stripe-like billing system built on an event-driven microservice architecture, design an E2E test that validates the end-to-end flow from a user initiating a purchase to invoice settlement across regionally distributed services. Include how you verify eventual consistency, idempotency, and state replay safety after a simulated regional outage, with concrete tooling choices and steps?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Google","Stripe"]},{"id":"q-901","question":"Design a beginner-friendly E2E test for a React checkout flow: user visits a product page, adds to cart, proceeds to checkout, fills shipping details, and completes a payment via a sandbox API. Explain how you would ensure test isolation and determinism (seed/reset test data, mock payment endpoint), and show a minimal Playwright script snippet that asserts successful order confirmation and a backend order record?","channel":"e2e-testing","subChannel":"general","difficulty":"beginner","tags":["e2e-testing"],"companies":["MongoDB","Tesla","Twitter"]},{"id":"q-949","question":"Design an end-to-end test plan for a Netflix/Meta-like streaming service that delivers adaptive bitrate video across 4 regions. Include how you model test content, simulate varying network conditions, verify manifest/chunk fetch, DRM/licensing checks, on-device caching, and end-to-end telemetry; specify tooling, data isolation, and how you scale across regions while minimizing flakiness?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Meta","Netflix"]},{"id":"q-979","question":"Design an end-to-end test for a multi-tenant content platform that serves regionally personalized content with feature flags and A/B tests. The platform must ensure per-tenant data isolation, correct content personalization, flag-driven UI, and during regional outages. Outline the test scope, data management, tooling, and steps to verify end-to-end delivery across tenants and regions without flakiness?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Adobe","Meta","Salesforce"]},{"id":"q-279","question":"What are the key differences between getByRole() and getByText() selectors in Playwright, and when would you choose one over the other for reliable E2E testing?","channel":"e2e-testing","subChannel":"playwright","difficulty":"beginner","tags":["playwright","browser-automation","selectors"],"companies":["Adobe","Amazon","Microsoft","Netflix","Salesforce"]},{"id":"q-208","question":"What is the difference between Selenium WebDriver and Selenium Grid, and when would you use each in your testing strategy?","channel":"e2e-testing","subChannel":"selenium","difficulty":"beginner","tags":["selenium","webdriver","grid"],"companies":["Amazon","Google","Meta"]},{"id":"q-1062","question":"You're stewarding reliability for a 24/7 payments platform with two active regions and strict latency requirements. A recent outage exposed brittle failover and slow triage. Outline a practical plan: governance model (platform vs product teams), incident playbooks, auto-remediation, SRE metrics, release controls, and how you measure ROI while preserving feature velocity?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Citadel","Microsoft","Square"]},{"id":"q-1088","question":"Two squads share a single API surface: Platform Reliability (2 senior + 1 mid) and Growth Feature (4 engineers). Platform incidents increased MTTR by 40% last quarter; Growth feature is 70% complete but depends on unstable APIs and tight external deadlines. How would you structure quarterly planning to protect reliability, reallocate capacity, set SLOs and error budgets, and implement gating (flags, canaries, contracts) to finish the Growth feature without amplifying risk?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Google","Robinhood","Uber"]},{"id":"q-1223","question":"You're steward of a shared data platform used by 8 squads across web, mobile, and ML workloads. A new event schema from one squad breaks downstream contracts and delays analytics dashboards. Propose a governance model: data contracts, versioned schemas, deprecation policy, and a cross-squad escalation process. Include concrete ownership, metrics, and a migration plan that minimizes customer impact while meeting quarterly release goals?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Anthropic","Instacart","Twitter"]},{"id":"q-1309","question":"You're leading engineering for a company with 5 teams (Auth, Billing, Search, Recommendations, Web). A directive requires end-to-end delivery quality: cut post-release hotfix rate by 40% while preserving delivery velocity. Propose a concrete plan to implement dual-track planning (feature vs reliability), allocate capacity (e.g., 60/25/15 split), designate cross-cutting owners (telemetry, incident mgmt, release engineering), and the metrics, rituals, and a 12-week rollout. Include milestones and go/no-go criteria?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Airbnb","DoorDash","Meta"]},{"id":"q-1376","question":"How would you design and implement a platform governance model for 6 product squads relying on a shared platform (auth, billing, search) with a cap of 2 engineers for platform work per sprint, ensuring API stability, a deprecation policy, telemetry, and incident response, plus a 12-week rollout and concrete success metrics? Provide the first 90-day plan and the top trade-offs?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Hugging Face","IBM","Instacart"]},{"id":"q-1614","question":"As owner of a real-time analytics platform used by three customer apps, a silent ETL failure intermittently leaves dashboards stale for 20% of users, skewing revenue KPIs. Provide a concrete remediation plan: 1) 24h incident triage, 2) redesigned monitoring with SLIs/SLOs and automated remediation, 3) cross-team ownership and gating, 4) validation and rollback, 5) a 6-week rollout with milestones and go/no-go criteria. Include concrete metrics and an initial implementation plan?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Lyft","PayPal","Robinhood"]},{"id":"q-1687","question":"You're overseeing five platform teams (Networking, Compute, Auth, Billing, Observability). EU data localization is mandated with a 9-month deadline; design a concrete program to migrate data stores and services with minimal downtime, ensure compliance and data sovereignty, and minimize feature disruption. Include ownership matrix, migration waves, rollback plan, telemetry & auditability, go/no-go criteria, and success metrics?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Cloudflare","Google","LinkedIn"]},{"id":"q-1827","question":"You're the head of platform engineering at a fintech with four squads—Payments, Identity, Analytics, and Notifications. A critical legacy event bus must be replaced with Apache Pulsar. Current availability 99.99% and latency <100 ms, ~2M messages/day, strict regulatory retention for audits. You have 6 weeks, no downtime. Outline a phased migration plan: dependency map, cutover, rollback, telemetry, governance, and success metrics. What plan would you implement to accomplish this within the constraints?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Goldman Sachs","Lyft","Stripe"]},{"id":"q-1856","question":"Oversee a data-platform initiative to consolidate 6 product domains into a centralized data catalog with standardized data contracts. Ambiguous expectations cause data quality issues and flaky dashboards. Design a governance model and a 90-day rollout: schema/versioning, tests, SLIs/SLOs for data, data-contract rituals, and cross-stakeholder engagement. How would you measure ROI and prevent contract creep?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Databricks","Meta","Robinhood"]},{"id":"q-2028","question":"How would you implement a **dual-track leadership ladder** (Technical Leader vs People Manager) in a 4-squad, multi-region organization to deepen technical depth without sacrificing delivery velocity? Propose a 90‑day pilot design, governance, mentoring cadence, and shared OKRs; specify success metrics (**velocity**, turnover, time-to-promotion) and a rollout plan?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Google","Netflix"]},{"id":"q-2047","question":"In a fast-scaling organization aiming for 10x traffic, you manage five squads: Platform, Auth, Payments, Data, and Web, plus centralized SRE. A quarterly objective mandates MTTR down 40%, lead time down 25%, and on-call toil down 20%, without sacrificing velocity. Design an org and governance model, specify roles and rituals, metrics to track, and a 12-week rollout with milestones. Include risk mitigations and rollout gates?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Amazon","Anthropic"]},{"id":"q-461","question":"How would you handle a situation where your top engineer wants to work on a different project, but you need them to complete a critical deadline?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Airbnb","Google"]},{"id":"q-492","question":"How would you handle a situation where your top engineer wants to work on a personal project during work hours, claiming it will benefit the company long-term?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Microsoft","Snap"]},{"id":"q-522","question":"You're leading a team of 5 engineers. Two team members disagree on the technical approach for a critical feature. How do you handle this situation while maintaining team morale and meeting the deadline?","channel":"engineering-management","subChannel":"general","difficulty":"beginner","tags":["engineering-management"],"companies":["Google","Instacart"]},{"id":"q-549","question":"How would you handle a situation where your top engineer wants to work on a different project than what the team needs?","channel":"engineering-management","subChannel":"general","difficulty":"beginner","tags":["engineering-management"],"companies":["Apple","Hashicorp","Scale Ai"]},{"id":"q-575","question":"How do you balance technical debt with feature delivery when managing engineering teams?","channel":"engineering-management","subChannel":"general","difficulty":"beginner","tags":["engineering-management"],"companies":["Bloomberg","PayPal"]},{"id":"q-860","question":"When onboarding new engineers to a project with a legacy codebase and a new component library, operating on a 3-week sprint with shared CI, what concrete onboarding plan and gates would you implement in the first 4 weeks to accelerate learning while preserving code quality and preventing regressions?","channel":"engineering-management","subChannel":"general","difficulty":"beginner","tags":["engineering-management"],"companies":["Meta","Stripe"]},{"id":"q-184","question":"You're managing a critical microservices migration from monolith to Kubernetes with 3 teams. Team A (backend services) is 2 weeks behind due to database connection pooling issues, Team B (frontend) is on track but blocked by API contracts, and Team C (DevOps) needs production-ready Helm charts by EOW. How do you resolve the technical dependencies and get the migration back on schedule while maintaining service availability?","channel":"engineering-management","subChannel":"project-management","difficulty":"advanced","tags":["project","planning"],"companies":["Adobe","Amazon","Google","Microsoft","Netflix","Salesforce"]},{"id":"q-211","question":"How would you implement a technical debt repayment framework using the 20% time allocation model while balancing feature delivery deadlines?","channel":"engineering-management","subChannel":"project-management","difficulty":"intermediate","tags":["delegation","mentoring","growth"],"companies":["Google","LinkedIn","Microsoft","Robinhood","Stripe"]},{"id":"q-261","question":"Design a task delegation matrix system for a 15-person engineering team that balances skill development with project delivery SLAs. Include RACI implementation, automated task assignment algorithms, and success metrics. How would you handle edge cases like skill gaps and conflicting priorities?","channel":"engineering-management","subChannel":"team-leadership","difficulty":"beginner","tags":["delegation","mentoring","growth"],"companies":["Amazon","Google","Meta","Microsoft","Salesforce","Stripe"]},{"id":"q-281","question":"How do you influence technical decisions when you're not the technical lead, and what specific strategies do you use to build technical credibility across different stakeholder groups?","channel":"engineering-management","subChannel":"team-leadership","difficulty":"intermediate","tags":["communication","collaboration","influence"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-168","question":"Explain the CSS box model and how box-sizing affects layout calculations. What's the difference between border-box and content-box?","channel":"frontend","subChannel":"css","difficulty":"beginner","tags":["css","styling"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-363","question":"You have a navigation bar with 3 items that should be evenly spaced. The middle item needs to be centered while the first and last items stick to the edges. How would you implement this using CSS Flexbox?","channel":"frontend","subChannel":"css","difficulty":"beginner","tags":["css","flexbox","grid","animations"],"companies":["Amazon","Google","Hrt","Meta","Microsoft","Netflix"]},{"id":"q-408","question":"You're building a responsive dashboard with a complex grid layout that must support dynamic widget resizing, reordering via drag-and-drop, and maintain performance with 100+ widgets. How would you architect the CSS grid system to handle these requirements while ensuring smooth animations and preventing layout thrashing?","channel":"frontend","subChannel":"css","difficulty":"advanced","tags":["css","flexbox","grid","animations"],"companies":["Affirm","Okta","Shopify"]},{"id":"q-661","question":"Design a responsive image gallery using CSS Grid that shows 4 columns on wide screens, 2 on tablets, and 1 on mobile. Each tile contains an image, title, and caption. Add a hover/focus animation that lifts the tile and deepens the shadow, and ensure keyboard accessibility and respect for reduced-motion preferences?","channel":"frontend","subChannel":"css","difficulty":"beginner","tags":["css","flexbox","grid","animations"],"companies":["Amazon","Snap","Square"]},{"id":"q-664","question":"Question: Create a responsive 3-column feature grid using CSS Grid that collapses to a single column on narrow viewports, with a hover animation that gently scales each card and reveals a caption with a slide-in effect, while keeping focus-visible styles and respecting prefers-reduced-motion?","channel":"frontend","subChannel":"css","difficulty":"beginner","tags":["css","flexbox","grid","animations"],"companies":["Apple","Meta"]},{"id":"q-1342","question":"You're building a React application where certain components need to maintain state that persists across page refreshes but doesn't need to be shared globally. How would you implement a state management solution that combines local component state with browser storage, and what are the key considerations for handling synchronization, performance, and potential race conditions?","channel":"frontend","subChannel":"frontend","difficulty":"intermediate","tags":["react-hooks","local-storage","state-persistence","custom-hooks","performance"],"companies":[]},{"id":"q-2125","question":"You're building a React form with multiple controlled inputs that need to maintain their state during component unmounting/remounting (e.g., when switching between tabs in a single-page application). How would you implement a state management solution that preserves form state across component lifecycle changes while avoiding the performance pitfalls of lifting all state to a global store, and what specific React patterns would you use to handle this efficiently?","channel":"frontend","subChannel":"frontend","difficulty":"intermediate","tags":["react-hooks","form-state","persistence","performance","component-lifecycle"],"companies":[]},{"id":"q-642","question":"You're building a React form with multiple controlled inputs that need to share validation state. How would you implement a custom hook to manage form state and validation logic efficiently, and what are the key considerations for preventing unnecessary re-renders?","channel":"frontend","subChannel":"frontend","difficulty":"intermediate","tags":["react-hooks","form-validation","performance-optimization","custom-hooks","state-management"],"companies":[]},{"id":"q-762","question":"You're building a React dashboard with multiple components that need to share and update real-time data (like stock prices or live metrics). How would you implement a state management solution that minimizes re-renders while ensuring all components receive the latest data, and what specific React patterns would you use to prevent performance bottlenecks?","channel":"frontend","subChannel":"frontend","difficulty":"intermediate","tags":["react-state-management","performance-optimization","context-api","real-time-data","memoization"],"companies":[]},{"id":"fe-2","question":"Explain the JavaScript Event Loop architecture. How do microtasks and macrotasks differ in execution order, and what are the practical implications for async/await code?","channel":"frontend","subChannel":"javascript","difficulty":"beginner","tags":["js","async","core"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"fe-3","question":"Explain JavaScript closures with a practical use case and how they're used in real applications?","channel":"frontend","subChannel":"javascript","difficulty":"intermediate","tags":["js","scope","patterns"],"companies":["Amazon","Google","Meta"]},{"id":"fr-157","question":"What is the difference between `let`, `const`, and `var` in JavaScript, and how do their scoping rules and temporal dead zone affect real-world code?","channel":"frontend","subChannel":"javascript","difficulty":"beginner","tags":["js","core"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"fr-162","question":"Explain how JavaScript's event loop handles microtasks vs macrotasks. What happens when a Promise resolves inside a setTimeout callback?","channel":"frontend","subChannel":"javascript","difficulty":"advanced","tags":["js","core"],"companies":["Airbnb","Google","Meta","Netflix","Stripe"]},{"id":"fr-173","question":"What is the output of this code and explain the event loop behavior: console.log('A'); setTimeout(() => console.log('B'), 0); Promise.resolve().then(() => console.log('C')); Promise.resolve().then(() => console.log('D')); console.log('E'); How do microtask and macrotask queues interact in JavaScript's event loop?","channel":"frontend","subChannel":"javascript","difficulty":"advanced","tags":["js","core"],"companies":["Amazon","Google","Meta","Netflix","Stripe"]},{"id":"q-240","question":"What is a closure in JavaScript and how does it enable data encapsulation?","channel":"frontend","subChannel":"javascript","difficulty":"beginner","tags":["js","es6","closures","promises"],"companies":["Airbnb","Amazon","Apple","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-351","question":"You're building a file upload component that processes multiple files in parallel. How would you implement a concurrent upload queue with a maximum of 3 simultaneous uploads using Promise.allSettled and closures?","channel":"frontend","subChannel":"javascript","difficulty":"intermediate","tags":["js","es6","closures","promises"],"companies":["Amazon","Anthropic","Microsoft"]},{"id":"q-462","question":"Implement a rate-limited API wrapper that queues requests when the limit is reached, using closures to maintain state and promises to handle request ordering?","channel":"frontend","subChannel":"javascript","difficulty":"advanced","tags":["js","es6","closures","promises"],"companies":["Google","Lyft","Scale Ai"]},{"id":"q-550","question":"Explain how closures work in JavaScript and provide a practical example of when you'd use one in a React component?","channel":"frontend","subChannel":"javascript","difficulty":"beginner","tags":["js","es6","closures","promises"],"companies":["Google","Oracle","Tesla"]},{"id":"fr-154","question":"What are the performance implications and layout shift consequences of loading large images without explicit dimensions, and how do modern CSS properties and loading strategies mitigate these issues?","channel":"frontend","subChannel":"performance","difficulty":"beginner","tags":["perf","optimization"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"fr-172","question":"How would you optimize rendering performance for a React component displaying a large list (10,000+ items) with frequent real-time updates?","channel":"frontend","subChannel":"performance","difficulty":"intermediate","tags":["perf","optimization"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-188","question":"How would you implement a performance budget system that automatically detects bundle regressions and enforces lazy-loading boundaries in a large-scale React application?","channel":"frontend","subChannel":"performance","difficulty":"advanced","tags":["lighthouse","bundle","lazy-loading"],"companies":["Airbnb","Atlassian","LinkedIn","Netflix","Uber"]},{"id":"q-301","question":"How would you optimize a React app's bundle size to achieve Lighthouse scores above 90, and what specific tools and metrics would you use to measure success?","channel":"frontend","subChannel":"performance","difficulty":"beginner","tags":["lighthouse","bundle","lazy-loading"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-329","question":"You're tasked with improving a React app's Lighthouse performance score from 65 to 90+. The bundle size is 2.1MB and Time to Interactive is 4.2s. What specific steps would you take to optimize the bundle and implement lazy loading?","channel":"frontend","subChannel":"performance","difficulty":"intermediate","tags":["lighthouse","bundle","lazy-loading"],"companies":["Amazon","Google","Mckinsey","Meta","Microsoft","Netflix","Scale Ai","Stripe"]},{"id":"q-378","question":"You're building a real-time trading dashboard at DE Shaw that needs to display 1000+ rapidly updating price cards. How would you optimize CSS layout and animations to maintain 60fps while cards are being added/removed/updated every 100ms?","channel":"frontend","subChannel":"performance","difficulty":"advanced","tags":["css","flexbox","grid","animations"],"companies":["DE Shaw","Discord","Instacart"]},{"id":"q-390","question":"You're working on a React app that loads slowly. Your Lighthouse performance score is 45. What specific steps would you take to improve it, and how would you implement lazy loading for a heavy component?","channel":"frontend","subChannel":"performance","difficulty":"beginner","tags":["lighthouse","bundle","lazy-loading"],"companies":["Crowdstrike","Deepmind","NVIDIA"]},{"id":"q-395","question":"You're building a complex dashboard with a responsive grid layout that must support dynamic column insertion, reordering, and animated transitions. How would you implement this using CSS Grid and JavaScript while maintaining 60fps performance during large dataset updates (10,000+ items)?","channel":"frontend","subChannel":"performance","difficulty":"advanced","tags":["css","flexbox","grid","animations"],"companies":["Google","Meta","Microsoft","Netflix","Salesforce","Stripe"]},{"id":"q-523","question":"Your React app has a 85 Lighthouse performance score. The bundle analyzer shows a 2.8MB main chunk with heavy libraries like moment.js and lodash. How would you optimize this to reach 95+?","channel":"frontend","subChannel":"performance","difficulty":"intermediate","tags":["lighthouse","bundle","lazy-loading"],"companies":["Cloudflare","Hugging Face","Stripe"]},{"id":"q-576","question":"How would you optimize a React app's Lighthouse score from 65 to 90+ using bundle analysis and lazy loading?","channel":"frontend","subChannel":"performance","difficulty":"beginner","tags":["lighthouse","bundle","lazy-loading"],"companies":["Coinbase","DoorDash"]},{"id":"fe-1","question":"How does React's Virtual DOM diffing algorithm work during reconciliation, and what role do keys play in optimizing list updates?","channel":"frontend","subChannel":"react","difficulty":"intermediate","tags":["react","perf","internals"],"companies":["Airbnb","Google","Meta","Microsoft","Netflix"]},{"id":"fr-161","question":"How would you implement a React hook that tracks component render count and warns when it exceeds a threshold, while avoiding infinite render loops?","channel":"frontend","subChannel":"react","difficulty":"advanced","tags":["react","perf"],"companies":["Airbnb","Microsoft","Netflix","Stripe","Uber"]},{"id":"q-215","question":"How would you implement a custom useDebounce hook that works with React's concurrent features and prevents stale closures?","channel":"frontend","subChannel":"react","difficulty":"intermediate","tags":["react","hooks","context","redux"],"companies":["Amazon","Google","Meta","Microsoft","Uber"]},{"id":"q-239","question":"How would you implement a React useMemo hook to optimize a recursive Fibonacci function with memoization, and what are the key trade-offs between top-down memoization vs bottom-up tabulation in this context?","channel":"frontend","subChannel":"react","difficulty":"intermediate","tags":["dp","memoization","tabulation"],"companies":["Airbnb","Amazon","Apple","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-315","question":"How would you optimize a React app's bundle size and loading performance using lazy loading, code splitting, and webpack optimization strategies?","channel":"frontend","subChannel":"react","difficulty":"intermediate","tags":["lighthouse","bundle","lazy-loading"],"companies":null},{"id":"q-434","question":"You're building a React app with multiple components needing access to user authentication state. When would you choose Context API over Redux, and what are the specific performance implications of each approach?","channel":"frontend","subChannel":"react","difficulty":"intermediate","tags":["react","hooks","context","redux"],"companies":["Anthropic","Microsoft","NVIDIA"]},{"id":"q-595","question":"Explain the difference between useState and useReducer hooks in React and when you would choose one over the other.","channel":"frontend","subChannel":"react-hooks","difficulty":"intermediate","tags":["react","hooks","state-management","useState","useReducer"],"companies":["Meta","Netflix","Airbnb","Uber","Spotify"]},{"id":"q-628","question":"Explain the differences between useState, useReducer, and Context API for state management in React. When would you choose each approach?","channel":"frontend","subChannel":"react-hooks","difficulty":"intermediate","tags":["react","state-management","hooks","context-api"],"companies":["Meta","Netflix","Airbnb","Uber","Spotify"]},{"id":"q-287","question":"How does a Service Worker intercept network requests and implement offline caching strategies?","channel":"frontend","subChannel":"web-apis","difficulty":"intermediate","tags":["dom","fetch","websocket","service-worker"],"companies":["Amazon","Google","Meta"]},{"id":"q-341","question":"You're building a real-time collaborative document editor. How would you implement a service worker to handle offline synchronization, WebSocket reconnection logic, and conflict resolution when multiple users edit simultaneously?","channel":"frontend","subChannel":"web-apis","difficulty":"advanced","tags":["dom","fetch","websocket","service-worker"],"companies":["Affirm","Broadcom","Roblox"]},{"id":"q-419","question":"You're building a real-time food delivery tracking app. How would you implement a WebSocket connection that handles network interruptions and maintains order status updates when the app goes offline?","channel":"frontend","subChannel":"web-apis","difficulty":"intermediate","tags":["dom","fetch","websocket","service-worker"],"companies":["Apple","DoorDash","MongoDB"]},{"id":"q-426","question":"You're building a real-time chat application that needs to work offline. How would you implement a service worker to cache messages and sync them when the user comes back online?","channel":"frontend","subChannel":"web-apis","difficulty":"beginner","tags":["dom","fetch","websocket","service-worker"],"companies":["Anthropic","Hugging Face","Snap"]},{"id":"q-493","question":"You're building a real-time grocery delivery tracking app for Instacart. How would you implement a service worker strategy to handle intermittent connectivity, ensure order updates are delivered via WebSockets, and maintain a consistent UI state across network drops?","channel":"frontend","subChannel":"web-apis","difficulty":"advanced","tags":["dom","fetch","websocket","service-worker"],"companies":["Instacart","MongoDB"]},{"id":"q-663","question":"You’re building a chat UI that loads initial messages with fetch('/api/messages'), caches assets via a service worker, and receives live updates over a WebSocket at '/ws'. Explain and implement: (1) pre-cache strategy and cache invalidation in the SW, (2) a fetch wrapper with timeout and offline fallback to cache, (3) DOM updates for incoming WebSocket messages, (4) reconciliation when offline vs online and message ordering?","channel":"frontend","subChannel":"web-apis","difficulty":"intermediate","tags":["dom","fetch","websocket","service-worker"],"companies":["Coinbase","Google","LinkedIn"]},{"id":"q-665","question":"Design and implement a minimal real-time dashboard that loads initial data via fetch('/api/data?limit=200'), then opens a WebSocket to wss://host/ws for live updates, and uses a Service Worker to cache the app shell and latest API responses for offline use. Describe how you batch DOM updates to minimize reflows, ensure reconnection logic, and implement a cache-first strategy with network fallback. Provide core code snippets and trade-offs?","channel":"frontend","subChannel":"web-apis","difficulty":"advanced","tags":["dom","fetch","websocket","service-worker"],"companies":["Apple","PayPal"]},{"id":"q-1003","question":"Design a multi-region DR plan for a Cloud Run API that reads from Cloud SQL and writes results to Cloud Storage and BigQuery. Define RPO/RTO targets, cross-region replication strategy for Cloud SQL, data residency constraints, traffic failover through a global load balancer, and automated DR tests. Include monitoring, IAM least privilege, and post-failover reconciliation steps?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Netflix","Tesla","Two Sigma"]},{"id":"q-1017","question":"Design a beginner-friendly ingestion workflow on GCP for daily CSV exports delivered via partner-signed URLs into a Cloud Storage bucket. Implement a Cloud Function (Python) triggered on finalization to validate, parse, and load aggregates into BigQuery, with structured Cloud Logging including a correlation_id. Outline per-project isolation (dev/stage/prod), idempotent replay, and a simple test plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Amazon","Snap","Snowflake"]},{"id":"q-1092","question":"Design a cross-tenant, multi-region data ingestion pipeline on Google Cloud to handle telemetry from partner apps. Data arrives as daily compressed NDJSON in per-tenant Cloud Storage buckets. Build end-to-end using Cloud Storage triggers or Pub/Sub, Dataflow (Beam) for parsing/transformations, and BigQuery with per-tenant datasets. Enforce least-privilege IAM, strict per-project isolation, Private Service Connect, data residency, auditable Cloud Logging, and idempotent replay with watermarking. Include architecture, data mapping, and a practical test plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Microsoft","Snowflake"]},{"id":"q-1195","question":"Design a beginner data-retention automation on GCP for a daily CSV export that lands in a shared Cloud Storage bucket and is ingested into a partitioned BigQuery table. Implement per-environment isolation (dev/stage/prod) by separate buckets and datasets. Create a Cloud Scheduler job that triggers a Cloud Function (Python) to apply 30-day retention on storage objects, prune BigQuery partitions, and log using Cloud Logging with a correlation_id. Outline validation tests and rollback plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Coinbase","NVIDIA"]},{"id":"q-1233","question":"Design an advanced, per-tenant data lake on Google Cloud for a SaaS platform serving 100 enterprise customers. Ingest on‑prem JSON logs via Pub/Sub to regional Cloud Storage, and use Dataflow to write per‑tenant BigQuery datasets with CMEK; ensure data locality, least‑privilege IAM, and exfiltration controls via VPC Service Controls and Private Service Connect. Outline observability, auditability, and a test plan for IAM changes and retention?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Anthropic","Lyft","Robinhood"]},{"id":"q-1308","question":"Design a beginner-friendly, cost-aware data ingestion pipeline on GCP for a fleet of devices sending JSON events to Pub/Sub. Create per-environment isolation with separate projects, topics, and BigQuery datasets (dev/stage/prod). Use Dataflow for streaming processing into BigQuery, optimize costs with regional resources, and implement Cloud Billing budgets with alerts plus a simple rollback and test plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Lyft","Netflix"]},{"id":"q-1430","question":"Design a multi-tenant ingestion pipeline on GCP where raw data lands in per-tenant GCS buckets across projects and streams into a single partitioned BigQuery dataset. Implement per-tenant isolation, CMEK, least-privilege IAM, cross-project sharing via authorized views, and end-to-end auditing. Include validation, schema evolution, and cost controls?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Citadel","Discord","MongoDB"]},{"id":"q-1483","question":"Design an advanced, multi-tenant streaming pipeline on GCP for a SaaS analytics product. Tenants across 20 projects publish events to Pub/Sub; a Dataflow streaming job validates per-tenant schemas and writes to per-tenant BigQuery datasets with daily partitions. Include least-privilege IAM, per-tenant service accounts, CMEK for BigQuery, cross-project Private Service Connect, idempotent writes, dead-letter handling, retention, and a complete test plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Airbnb","Cloudflare","Tesla"]},{"id":"q-1519","question":"Design a cross-region data ingestion and analytics pipeline on GCP for a global app. Ingest user events (JSON) from Pub/Sub in two regions, store raw data in regional Cloud Storage buckets with CMEK, process with region-specific Dataflow templates to write per-event aggregates to a partitioned BigQuery dataset, and emit redacted summaries to a separate table. Enforce per-environment isolation, VPC Service Controls, and IAM least privilege. Include end-to-end tests and a rollback plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Citadel","Instacart","OpenAI"]},{"id":"q-1636","question":"Design a region-aware streaming fraud pipeline in Google Cloud for a global payments platform. Ingest via Pub/Sub per region, deduplicate and enrich with Dataflow, score in a Cloud Run service, store raw events in Cloud Storage (CMEK) and scores in BigQuery (partitioned by region/tenant); enforce per-tenant isolation via separate projects and IAM; use VPC Service Controls; implement regional DR and monitoring/alerts. Describe architecture, IAM roles, and testing plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Slack","Square","Tesla"]},{"id":"q-1667","question":"Design a beginner-friendly GCP data pipeline: daily partner CSVs arrive in Cloud Storage via signed URLs; build a Dataflow (Python) batch pipeline to validate, deduplicate by id, and upsert into a date-partitioned BigQuery table. Implement per-env isolation with separate buckets/datasets, a dead-letter path for bad rows, and Cloud Logging correlation_id. Include a simple test plan and rollback steps?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Microsoft","Netflix"]},{"id":"q-1802","question":"Design a beginner-friendly, multi-environment GCP data-ingestion pipeline: partner daily CSV exports arrive via signed URLs into per-environment GCS buckets; build a Node.js Cloud Function that validates final URLs and triggers an Apache Beam Dataflow job to scrub PII and load into a partitioned BigQuery table. Outline IAM least-privilege, testing, and observability?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Cloudflare","Discord"]},{"id":"q-1825","question":"Design a cross-region streaming analytics pipeline on GCP where raw events from EU users must remain data-resident, while derived aggregates are queried globally. Describe your architecture using Pub/Sub, Dataflow (Streaming), BigQuery, and Cloud Storage; enforce least-privilege IAM and per-environment isolation; implement data residency checks and automated rollbacks. How would you validate and test this end-to-end?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Coinbase","Netflix","Tesla"]},{"id":"q-1937","question":"Design an end-to-end, multi-tenant event analytics pipeline on Google Cloud: ingest per-tenant events via Pub/Sub push subscriptions, deduplicate and normalize using Dataflow, store data in dedicated BigQuery datasets per tenant, implement strict per-tenant IAM and VPC boundaries, and ensure full auditability with Cloud Audit Logs/Cloud Logging. Include schema evolution handling with a registry and an accompanying test strategy?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Discord","Snowflake","Two Sigma"]},{"id":"q-2050","question":"Design a cost-conscious, multi-env event-driven pipeline for IoT telemetry on GCP: ingest from Pub/Sub, process with Dataflow (Beam) or Cloud Run, and store in partitioned BigQuery. Enforce per-env isolation with distinct topics/subscriptions and datasets; implement idempotent processing via insertId; provide end-to-end replay tests and a rollback plan. How would you implement monitoring and rollback?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Meta","PayPal","Scale Ai"]},{"id":"q-2077","question":"Advanced multi-tenant data lake on GCP: each tenant has isolated Cloud Storage prefixes and BigQuery datasets. Propose architecture enforcing per-tenant IAM conditions, Private Service Connect and VPC Service Controls for restricted egress, and central metadata with Data Catalog/Dataplex. Include ingestion, isolation validation, auditing, and rollback?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Coinbase","Snowflake"]},{"id":"q-2130","question":"Design a beginner-friendly, cost-conscious GCP ingestion and aggregation workflow for daily partner logs uploaded as signed URLs to Cloud Storage. Provide per-environment isolation (dev/stage/prod) via separate prefixes and BigQuery datasets, a Python Cloud Function triggered on finalization to validate, parse, and load aggregates into a partitioned BigQuery table, and simple monitoring/budget alerts. Address idempotence, a light test plan, and a lightweight reconciliation step?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Slack","Snap"]},{"id":"q-2160","question":"Design a secure, multi-tenant data-sharing and analytics pipeline on Google Cloud for a platform hosting ML models (similar to Hugging Face) where customers upload datasets via signed URLs to per-tenant Cloud Storage buckets, data is processed by Dataflow into per-tenant BigQuery datasets, and model training is kicked off in Vertex AI. Include per-tenant IAM least privilege, VPC Service Controls, data residency constraints, audit logging, and automated cost-guardrails?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Cloudflare","Google","Hugging Face"]},{"id":"q-878","question":"How would you implement a beginner-friendly, auditable deployment pipeline in Google Cloud for a Cloud Run app that reads from Cloud SQL and writes logs to Cloud Logging, ensuring least-privilege IAM, per-project isolation, and no public endpoints?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Cloudflare","Goldman Sachs","IBM"]},{"id":"q-907","question":"Design a private, regional data pipeline for a global fintech platform: events land in regional Pub/Sub topics, Dataflow performs streaming ETL, results stored in per-region BigQuery, and audit logs go to Cloud Logging. Enforce per-region IAM, least privilege, CMEK, Private Service Connect, and no public egress. Describe data flow, security controls, disaster recovery, and cost implications. How would you implement this pipeline?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Goldman Sachs","Hugging Face","Instacart"]},{"id":"q-977","question":"In a beginner setup, you deploy a Cloud Run API behind Private Service Connect, with logs going to Cloud Logging and traces to Cloud Trace. Outline a practical observability plan: which metrics, logs, and traces to collect; how to build a useful dashboard; how to configure a low-noise alert for 5xx latency; and a simple test to validate instrumentation and alerting?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Instacart","PayPal"]},{"id":"q-1016","question":"Design a regional streaming pipeline for a fintech app: on‑prem and GKE emit events to region Pub/Sub topics; a Dataflow streaming job enforces exactly-once, writes partitioned regional BigQuery tables, and triggers Vertex AI scoring in near real-time. How would you achieve low latency, data residency, schema evolution, and reliable failure recovery?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Adobe","PayPal","Stripe"]},{"id":"q-1055","question":"Design an audit-logging pipeline for a payments platform on GCP with sub-100ms end-to-end write latency at multi-region scale (millions of events/sec). Ingest via Pub/Sub, process with Dataflow (Beam), and sink to BigQuery. Explain how you ensure idempotent writes (insertId), handle schema evolution, and provide auditor access across projects without exposing sensitive data. Also outline retries, backoffs, and monitoring?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Amazon","Stripe"]},{"id":"q-1105","question":"You're building a beginner-friendly ingestion pipeline: external partners upload daily CSVs to a Cloud Storage bucket; design a minimal flow using a Cloud Function triggered on object finalize to parse the CSV and load a daily summary as a single row into BigQuery. Include IAM permissions, how to trigger on new files, and how to ensure idempotent writes?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Discord","Microsoft","Snap"]},{"id":"q-1269","question":"Design a real-time data pipeline on GCP for a multi-tenant SaaS where each tenant's data must reside in a specified region, is encrypted with CMEK, and access is strictly controlled per-tenant using IAM Conditions and VPC Service Controls; use Pub/Sub, Dataflow, and BigQuery, ensure idempotent writes and exactly-once semantics, and include monitoring and incident response steps?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Google","PayPal","Salesforce"]},{"id":"q-1331","question":"Design a beginner-friendly nightly backup workflow on GCP: a Cloud SQL MySQL export writes a dump to a Cloud Storage bucket each night; a Cloud Function triggers on the new object to gzip, append a date stamp, and move it to backups/archived. Include IAM bindings, trigger method on new files, and how to ensure exactly-one backup per day (idempotency)?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Instacart","NVIDIA","Tesla"]},{"id":"q-1370","question":"Design a private, streaming ingestion from on-prem to GCP for sensitive financial logs using Pub/Sub, Dataflow, and BigQuery. Include CMEK, IAM, VPC Service Controls, idempotent writes, exactly-once semantics, failure recovery, and cost considerations. Explain choices and trade-offs?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Databricks","IBM","MongoDB"]},{"id":"q-1413","question":"Design a real-time image ingestion pipeline on GCP for a multi-tenant SaaS app. Customers upload images to per-tenant Cloud Storage buckets with CMEK; a Pub/Sub topic notifies on new uploads; a Dataflow streaming job processes images to extract features and writes results to BigQuery, while archived copies are moved to per-tenant archive buckets. Outline architecture, IAM bindings, service account isolation, exactly-once guarantees (e.g., row_id dedup), and cost/latency trade-offs. Include monitoring and failure-response plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Amazon","Apple","IBM"]},{"id":"q-1479","question":"Design a beginner-friendly pipeline on GCP where a daily vendor JSON health report is uploaded to Cloud Storage; create a Cloud Function triggered by finalize to validate JSON against a schema, write valid rows to BigQuery, and move invalid files to a quarantine bucket with a Pub/Sub notice to the on-call channel. Include IAM roles and idempotent processing?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Google","Hashicorp","Hugging Face"]},{"id":"q-1661","question":"You're architecting a multi-tenant data lake on GCP. Ingest partner feeds via Pub/Sub, land raw into Cloud Storage, process with Dataflow streaming, and emit per-tenant results to dedicated BigQuery datasets with IAM controls. How would you enforce isolation, meet <60s latency, support schema evolution, and implement per-tenant cost governance and quotas? Include IAM bindings, Dataflow templates, and error handling?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Two Sigma","Uber"]},{"id":"q-1677","question":"In a high-value fintech setting, design a globally distributed, PCI-compliant fraud-detection pipeline on GCP that ingests events from on-prem via Private Service Connect, publishes to Pub/Sub, processes with Dataflow, stores raw and processed data in Cloud Storage and BigQuery, and uses Vertex AI for real-time scoring; describe IAM, CMEK, VPC, and failure handling, with idempotency?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Citadel","Robinhood"]},{"id":"q-1708","question":"Daily vendor CSVs arrive in Cloud Storage; design a beginner-friendly ingestion pipeline that first scans each file with Cloud DLP to detect PII, and if PII is found moves the file to a quarantine bucket and publishes an alert; if not, parse the CSV and load daily rows into a partitioned BigQuery table. Include IAM bindings, object-finalize trigger, and idempotent processing?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Databricks","Discord","Uber"]},{"id":"q-1729","question":"Design a beginner-friendly thumbnail pipeline for user-uploaded images in GCP. Images uploaded to Cloud Storage bucket incoming-images should trigger a Cloud Run container to generate two thumbnails (200px and 400px wide) and store them in image-thumbs. Use a Cloud Function to trigger on finalization and invoke the Cloud Run HTTP endpoint. Output names: <orig>_200_<hash>.jpg and <orig>_400_<hash>.jpg to ensure idempotency. IAM: restrict access so only the Cloud Run service account can write to image-thumbs. Include failure handling and a basic test plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Discord","NVIDIA","Snowflake"]},{"id":"q-1864","question":"Given a fintech app ingesting market ticks from multiple regions via Pub/Sub, design an intermediate streaming pipeline to load data into BigQuery with deduplication, schema evolution, and per-region partitioning. Use Dataflow for ETL, ensure exactly-once processing, handle late data up to 2 minutes, implement TTL retention on raw data, and outline monitoring and testing strategy?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Bloomberg","Google","Salesforce"]},{"id":"q-1912","question":"Design a beginner-friendly end-to-end GCP pipeline for user avatars stored in Cloud Storage. On object finalize, a Cloud Function should invoke a Cloud Run service to produce two thumbnails (100x100 and 256x256), store them in avatars-resized with deterministic naming, and publish a log entry. If processing fails, publish a Pub/Sub alert and quarantine the original file. Keep IAM minimal and describe a basic test plan and idempotency strategy?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["MongoDB","Twitter","Zoom"]},{"id":"q-2061","question":"Design an end-to-end cross-region streaming data pipeline on GCP to ingest high-volume telemetry from Pub/Sub into BigQuery with real-time dashboards. Use Dataflow (Beam) for streaming, enable exactly-once processing, implement automatic primary/DR region failover, use multi-region Pub/Sub topics and cross-region BigQuery datasets, handle schema evolution, and provide monitoring and rollback strategies?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Bloomberg","IBM"]},{"id":"q-2100","question":"Design a beginner-friendly, event-driven data validation pipeline on GCP for daily event JSONs uploaded to Cloud Storage: on file finalize, a Cloud Function validates each record against a simple JSON Schema, quarantines invalid files, and writes valid records to a BigQuery table with partitioning by date and a deterministic write-ID to ensure idempotency; explain IAM bindings and a basic test plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Airbnb","Databricks","Stripe"]},{"id":"q-904","question":"How would you configure a Cloud Run (fully managed) service to securely connect to a Cloud SQL PostgreSQL instance using a private connection, including IAM bindings and deployment steps to ensure the app talks via the Cloud SQL socket and never uses the instance's public IP?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["OpenAI","Slack","Snap"]},{"id":"q-1222","question":"Design a real-time ad-click analytics pipeline in GCP that ingests Pub/Sub events via Dataflow into BigQuery, while implementing 90-day TTL on PII data, exporting audits to Cloud Storage, and handling schema evolution, late data, dedup, and rollback. Provide concrete architecture, data models, and operational steps?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Cloudflare","NVIDIA","Snap"]},{"id":"q-1310","question":"You operate a streaming ingestion pipeline: Pub/Sub -> Dataflow -> BigQuery. Daily 50k events with fields user_id, event_type, amount (which can be string). Propose a simple ingestion-time data quality plan that validates JSON schema, coerces types, and routes invalid records to a dead-letter Pub/Sub topic without stopping the pipeline. Include testing with a small sample and how you monitor invalid counts?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Lyft","PayPal","Snap"]},{"id":"q-1319","question":"Design a compliant streaming data platform in GCP for a fintech app that ingests transaction events from Pub/Sub through Dataflow into BigQuery. New fields may appear over time; PII must be detected and masked before analytics, while raw data is retained with restricted access. Outline the end-to-end architecture, data models, schema evolution strategy, PII handling, lineage, retention, and operational monitoring with concrete steps and trade-offs?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Bloomberg","Plaid","Robinhood"]},{"id":"q-1389","question":"You receive streaming JSON events from Pub/Sub. Design a beginner-friendly GCP pipeline using Dataflow (Beam) to parse/flatten events, write to a partitioned BigQuery table (partition by event_date), and deduplicate by event_id. Late data allowed up to 6 hours. Outline architecture, data model, schema-change handling, and monitoring?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["MongoDB","Snap","Square"]},{"id":"q-1415","question":"You are building a multi-source data platform where raw data arrives as Parquet in Google Cloud Storage, Avro via Pub/Sub, and JSON through a partner API; design a two-stage pipeline that normalizes all formats into a canonical Parquet dataset, ingested into a partitioned BigQuery table, with schema drift handling, data validation, and end-to-end lineage. Include how you would implement idempotent ingests, rollback, and cross-team access controls?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Google","Snowflake","Uber"]},{"id":"q-1449","question":"Design a streaming enrichment pipeline: Pub/Sub → Dataflow → BigQuery, with an inline governance layer enforcing per-record rules (required fields, value ranges, cross-field consistency) at ingestion. Records failing rules go to a quarantine Cloud Storage bucket; good records load to a partitioned BigQuery table. Describe data model, rule engine approach, idempotence, backpressure, and testing/monitoring plan?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Apple","Instacart","Uber"]},{"id":"q-1473","question":"In a beginner friendly GCP data pipeline Pub/Sub -> Dataflow -> BigQuery design robust observability and recovery. Describe how to instrument Dataflow with metrics backlog throughput processed failed, implement a dead letter policy that dumps failed JSON messages to Cloud Storage, set retry backoff, and outline a replay path to re ing est from Cloud Storage. Include a concrete alert rule backlog throughput greater than 0.2 for 5 minutes and a brief example snippet?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Goldman Sachs","IBM"]},{"id":"q-1546","question":"In GCP, ingest daily Pub/Sub JSON events through Dataflow into BigQuery. Implement basic observability and reliability: 1) emit metrics for ingested, processed, and failed events; 2) export metrics to Cloud Monitoring and alert on failures for three consecutive checks; 3) deduplicate by event_id; 4) log audits to Cloud Storage. Outline architecture, data model, and concrete steps?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Coinbase","Microsoft","Salesforce"]},{"id":"q-1581","question":"In Pub/Sub, daily JSON events contain PHI fields. Design a beginner-friendly GCP pipeline using Dataflow (Beam) to redact PHI with Cloud DLP before loading into BigQuery, and emit a separate audit log to Cloud Storage. Outline architecture, data model, and validation steps, including how you’d monitor masking failures?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Amazon","Google","Uber"]},{"id":"q-1596","question":"Design a streaming pipeline: Pub/Sub → Dataflow (Beam) → BigQuery. Core: a dynamic data quality gate using a Firestore-stored policy engine that validates fields/types at ingest; invalid records serialized to Cloud Storage as JSONL with metadata; valid records load to BigQuery; publish quality metrics to Cloud Monitoring; handle drift/backpressure without downtime. Explain architecture, data model, and operational steps?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Cloudflare","Hashicorp","Scale Ai"]},{"id":"q-1722","question":"Design a cross-region, real-time analytics pipeline for a fintech on GCP. Ingest 100 GB/day of JSON events from Pub/Sub into BigQuery while ensuring per-tenant data isolation, a 90-day TTL on PII, and a rollback path. Show the architecture, data models, and operational steps for schema evolution, late data, dedup, and cost control?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Apple","Oracle","Tesla"]},{"id":"q-1754","question":"Design a beginner-friendly GCP pipeline to load daily 2-5 GB of newline-delimited JSON logs from Cloud Storage into BigQuery. Keep only the latest 45 days in a partitioned table (partition by load_date, clustered by record_id). Ensure dedup by record_id, handle optional new fields with a permissive schema, allow late data up to 24 hours, and provide basic monitoring. Choose between Dataflow (Beam) or Cloud Functions, justify, and outline data model, transformations, error handling, and testing strategies?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Hugging Face","Lyft","Oracle"]},{"id":"q-1787","question":"Design a data quality framework for a streaming pipeline ingesting 500 GB/day of JSON events from Pub/Sub into BigQuery. Specify in-flight schema validation, per-field checks (types, nullability, ranges), quarantining bad records, and automatic schema evolution with Data Catalog, plus end-to-end monitoring, alerting, and rollback procedures?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Apple","Goldman Sachs","PayPal"]},{"id":"q-1834","question":"Design a cross-tenant data governance pipeline on GCP for 1 TB/day of JSON user activity ingested from Pub/Sub into BigQuery in two regions. Enforce per-tenant privacy with field-level masking, auto-generate end-to-end data lineage with Data Catalog, support backward-compatible schema evolution, and export immutable audit logs to Cloud Storage. Include architecture, data models, rollback plan per-tenant (within 24h), and testing strategy?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Apple","Tesla"]},{"id":"q-1869","question":"Design a GCP streaming pipeline for 2 TB/day of multi-tenant JSON events from Pub/Sub into BigQuery. Create a schema registry in Cloud Storage, validate with a Beam DoFn, quarantine nonconforming records to an invalid dataset, and apply per-tenant DLP masking. Enforce row-level access with BigQuery policies, capture lineage in Data Catalog, and provide rollback and testing plan?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Goldman Sachs","Snap","Uber"]},{"id":"q-1906","question":"Ingest a daily batch of JSON records stored in Cloud Storage into BigQuery via a Dataflow (Beam) job. Build a beginner-friendly pipeline that validates a minimal schema (tenant_id, event_id, event_ts), filters out records missing required fields, and deduplicates by (tenant_id, event_id). Load valid data into a partitioned BigQuery table; write invalid records to a separate GCS errors bucket. Include basic health counters and a simple template?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Amazon","Twitter"]},{"id":"q-1953","question":"Design a GCP data pipeline for a multinational SaaS product that ingests 200 GB/day of JSON telemetry from Pub/Sub into BigQuery across two regions. Requirements: multi-tenant isolation, automatic schema evolution, per-tenant TTL, data quality checks (schema conformance, nulls, duplicates), and an automated rollback path for schema changes. Include data model sketches, partitioning strategy, testing, and rollback steps?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Apple","Hashicorp","NVIDIA"]},{"id":"q-1997","question":"Ingest daily 1 TB JSON exports dumped into Cloud Storage by partner apps. Design a beginner-friendly GCP batch pipeline to load into BigQuery with per-tenant isolation (datasets), ingestion-date partitioning, and a simple schema-evolution strategy (new fields added as nullable columns). Include data-quality checks (nulls, types) and a rollback path to revert a day’s ingest within 24h. Outline architecture and testing?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Databricks","Meta","Uber"]},{"id":"q-2073","question":"On GCP, design an observability-driven pipeline that detects schema drift and data-quality anomalies in near real-time for 1 TB/day of JSON events ingested from Pub/Sub into BigQuery across two regions. Include per-tenant lineage, automatic schema evolution, drift-triggered alerts, a self-healing rollback path, and export of audit trails to Cloud Storage. Provide architecture, data models, tests, and operational playbooks?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Anthropic","Databricks","Google"]},{"id":"q-2153","question":"Design a cost-aware multi-tenant GCP data pipeline for streaming telemetry that ingests 150 GB/day of JSON from Pub/Sub into BigQuery across two regions. Enforce per-tenant access with BigQuery row level security and per-tenant dataset versioning; support backward compatible schema evolution; and export versioned snapshots to Cloud Storage. Include data models, partitioning, testing, rollback, and synthetic-tenant generation?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["NVIDIA","Oracle","Snap"]},{"id":"q-931","question":"A GCP pipeline ingests 1 TB of JSON user activity daily from Pub/Sub into BigQuery via Dataflow. New fields appear over time; you must evolve the schema without downtime. What approach would you use for schema drift and nested fields, and outline concrete steps to implement it?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Citadel","Discord","Uber"]},{"id":"q-962","question":"In a GCP streaming pipeline, Pub/Sub feeds millions of events into BigQuery via Dataflow. Out-of-order arrivals and duplicates occur. Design an idempotent sink using a staging table and a final partitioned table, leveraging insertId for dedup and a MERGE strategy to upsert into the final table. Outline concrete steps and trade-offs to implement?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Databricks","Google","Instacart"]},{"id":"q-1013","question":"You're managing a multi-tenant SaaS on GCP across five projects connected via Shared VPC. You must enforce per-tenant network isolation, IAM conditions, and budget governance while keeping CI/CD simple. Propose an end-to-end setup using Shared VPC, IAM Conditions, VPC Service Controls, Billing Budgets, and Cloud Asset Inventory, and outline testing and rollback steps?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Discord","DoorDash","Snap"]},{"id":"q-1041","question":"Design a scalable, compliant log routing pipeline on GCP that collects logs from Kubernetes clusters, Cloud Run, and Cloud Functions, redacts PII, and stores in BigQuery with environment separation. Include data flow sinks, IAM and CMEK governance, failure modes and retries, and an end-to-end testing plan?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Databricks","Square"]},{"id":"q-1155","question":"You're deploying a globally distributed service on GKE across three regions, with Cloud Run and Cloud Functions used for specific workloads. Design a deployment pipeline that enforces policy-as-code, encryption at rest via CMEK, drift detection, automated rollback, and cross-region failover. Describe tooling, data planes, tests, and how you handle outages and compliance?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["Adobe","NVIDIA","Zoom"]},{"id":"q-1245","question":"You operate a global chat app with components on **GKE**, **Cloud Run**, and **Cloud Functions**. Latency spikes in one region go unnoticed in aggregated metrics. Design an end-to-end observability approach: (1) how to unify traces across runtimes, (2) how to instrument with **OpenTelemetry** and **OTLP** to a central collector, (3) how to build region-scoped dashboards and **SLO-based alerts**, and (4) how to validate during release with **canary** and **chaos testing**. Include tool choices, sample metrics, and a minimal config sketch?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Discord","Meta","Microsoft"]},{"id":"q-1281","question":"You're maintaining a Cloud Run Python service that reads an API key from Secret Manager. Implement a 30-day secret rotation using Cloud Scheduler to publish a rotation event to Pub/Sub, and enable the Cloud Run instance to fetch updated secret without a restart. Detail the IAM permissions, wiring, and a test plan to verify end-to-end rotation?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Hashicorp","Plaid","Robinhood"]},{"id":"q-1514","question":"Design a cost-aware DR plan for a multi-region GKE + Cloud Run service handling 2M events/min. implement active-active with regional load balancers, Istio-based traffic shifting, CMEK, and a VPC Service Controls perimeter. automate failover tests via Cloud Build + Chaos Mesh; define SLOs and error budgets, observability, and automatic rollback triggers on latency/error breaches?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["Airbnb","Hugging Face","Snap"]},{"id":"q-1664","question":"You’re tasked with enabling per-branch ephemeral environments in Google Cloud Platform for a microservice. Every feature branch should provision an isolated Cloud Run service, a dedicated Cloud SQL instance, and Secrets Manager entries, with least-privilege IAM, a per-branch namespace, and automatic teardown after 48 hours. Outline the exact steps using only GCP-native tools (no external CI), including budgets alerts, health checks, and teardown workflow?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Discord","MongoDB"]},{"id":"q-1685","question":"You’re tasked with cost-aware deployment for a Cloud Run service used by a mobile frontend. Create a policy that (1) halts new deployments when the monthly spend exceeds a threshold, (2) scales traffic to 20% during overages, and (3) posts a Slack alert via a Cloud Function if the bill crosses the threshold. Describe exact steps using only GCP-native tools (Billing Budgets, Cloud Monitoring, Cloud Functions) and outline a minimal end-to-end workflow?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["LinkedIn","Lyft"]},{"id":"q-1763","question":"You operate a large-scale IoT ingestion pipeline on GCP: millions of device events per minute flow from Pub/Sub into Dataflow (Apache Beam) and write to BigQuery. Design an architecture that guarantees near real-time processing with exactly-once semantics, handles late data, and supports schema evolution; include windowing, idempotent writes, backpressure, and a rollback/testing plan for Dataflow job updates. What would you implement and why?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Amazon","Google"]},{"id":"q-1819","question":"You manage a Cloud SQL PostgreSQL instance in GCP and need a reliable disaster-recovery workflow with automated daily backups to Cloud Storage. Explain how to implement this using Cloud Scheduler and a Cloud Function that calls the Cloud SQL Admin API to export to gs://my-backups/sql-dump-YYYYMMDD.sql. Include required IAM roles, bucket lifecycle policies, and how you would validate a restore?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Instacart","Snap","Snowflake"]},{"id":"q-1890","question":"In a multi-tenant SaaS on GCP requiring isolated per-tenant networks with a shared services hub, propose a scalable hub-and-spoke topology using Shared VPC and Private Service Connect. Include least-privilege IAM, per-tenant firewall rules, and Private Google Access. How would you implement policy-as-code, drift detection, automated rollback, and observability across tenants?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Discord","Microsoft"]},{"id":"q-1917","question":"Scenario: A Cloud Run service connects to a Cloud SQL instance. Secrets are stored in Cloud Secret Manager and injected into the container at runtime. Propose a beginner-friendly, low-risk, weekly password rotation workflow that updates the secret, triggers a rolling update with zero downtime, and provides an immutable audit trail. Include the exact GCP services you would use and a minimal 3-step sequence to implement?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Amazon","Databricks"]},{"id":"q-2006","question":"You're delivering a multi-tenant SaaS on GKE and Cloud Run with strict data residency and cost controls. Design an end-to-end pattern using only GCP-native tools to achieve per-tenant isolation, regional data stores, Canary/blue-green deployment, CMEK, VPC Service Controls, and automated rollback on degraded telemetry. Outline architecture, steps, and essential config snippets?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["Apple","PayPal","Tesla"]},{"id":"q-2054","question":"Design a beginner-friendly, GCP-native CI/CD for a Cloud Run service with a private artifact repository. Describe how you would: (a) configure Cloud Build to pull a private repo and push a container image to Artifact Registry, (b) inject a database password from Secret Manager into the Cloud Run container at runtime, and (c) rotate that secret monthly using Cloud Scheduler and a Cloud Function that updates Secret Manager. Include the specific services and minimal steps?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Amazon","Google"]},{"id":"q-877","question":"Design a cross-region disaster recovery plan for a streaming data pipeline on GCP (Pub/Sub, Dataflow, BigQuery) that must survive a regional outage with RTO < 15 minutes and RPO < 5 minutes. The primary region is us-central1; second region is us-east1. Include data paths, failover triggers, data integrity guarantees, and operational testing steps?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["Amazon","Apple","Scale Ai"]},{"id":"q-921","question":"In a multi-tenant GKE deployment across two GCP projects, you must enforce strict per-tenant network isolation and controlled egress to external services. Design a scalable architecture using Shared VPC, Private Service Connect, and per-tenant firewall policies to ensure tenants only reach whitelisted external endpoints, while preventing cross-tenant access. Include identity management, auditing, drift control, and operational notes for adding new tenants?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Amazon","Tesla"]},{"id":"gcp-ml-engineer-data-prep-1768249406549-2","question":"When tracking experiments and model lineage across teams, which combination provides end-to-end provenance and reproducibility?","channel":"gcp-ml-engineer","subChannel":"data-prep","difficulty":"intermediate","tags":["Vertex AI","Metadata","Data Catalog","Experiment Tracking","GKE","Terraform","certification-mcq","domain-weight-16"],"companies":null},{"id":"q-1008","question":"You're building a real-time customer-review sentiment classifier on GCP. Design a beginner-friendly end-to-end pipeline using Vertex AI for training and hosting, Vertex AI Feature Store for online features, Dataflow for ETL, and Pub/Sub for ingestion. Describe data flow, feature materialization cadence, a canary rollout strategy, and basic drift monitoring with rollback triggers. Include cost considerations?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Oracle","Snowflake","Two Sigma"]},{"id":"q-1199","question":"Design a multi-tenant, privacy-preserving online inference and feature materialization pipeline on GCP for a cross-region ride-hailing platform. Each tenant has its own feature schema and data residency needs. Outline how you would manage per-tenant Feature Store namespaces, Canary deployments across tenants, live vs. batch feature materialization, drift/bias monitoring, provenance, and automated rollback with Vertex AI Endpoints, Dataflow, and Pub/Sub. Include concrete rollback criteria and cost considerations?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Amazon","Lyft"]},{"id":"q-1225","question":"Design a beginner-friendly end-to-end GCP pipeline for a price-optimization model. Use Vertex AI for training and hosting, Vertex AI Feature Store for online/offline features, Dataflow for ETL into BigQuery, and Pub/Sub for ingestion. Describe data flow, feature derivation cadence, training trigger cadence, online/offline feature consistency, and a simple rollback strategy if offline metrics degrade. Include a basic cost plan?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Cloudflare","PayPal"]},{"id":"q-1438","question":"Design a production pipeline for a multi-tenant, real-time pricing model on GCP that isolates tenant data, supports per-tenant feature store versions, and enables tenant-scoped A/B testing. Use **Vertex AI**, **Feature Store**, **Pub/Sub**, and **Dataflow** to ingest events, materialize features, serve online predictions, and drive canary rollouts. Include tenancy isolation strategies, encryption at rest and in transit, drift monitoring, and cost-visibility dashboards across tenants?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Citadel","PayPal","Robinhood"]},{"id":"q-1462","question":"You're building a multi-tenant ML platform on GCP where each business unit requires isolated feature stores, per-tenant data locality, and separate budgets. Describe how you'd implement tenant isolation in Vertex AI Feature Store, manage per-tenant data lineage, and enable per-tenant canary model rollouts with drift checks and automated rollback. Include a concrete data path and cost controls?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Amazon","Databricks","Plaid"]},{"id":"q-1557","question":"Design a beginner-friendly end-to-end GCP pipeline for a real-time product-recommendation score using Vertex AI, Dataflow, Pub/Sub, and Vertex AI Feature Store. Include: 1) data validation and schema drift checks at ingestion, 2) per-customer feature isolation via IAM/VPC, 3) online feature materialization cadence and low-latency serving, 4) canary rollout strategy and rollback triggers, 5) practical cost-management tips?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Amazon","NVIDIA","Uber"]},{"id":"q-1694","question":"Design a beginner-friendly GCP ML pipeline for daily demand forecasting: Pub/Sub ingest, Dataflow ETL into BigQuery, Vertex AI training, and a Vertex AI online endpoint. Focus on observability: specify minimal metrics, dashboards, alerts for data drift and latency, and a safe rollback workflow that reverts to a previous model version when drift is detected. Include rough cost notes?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Apple","Square"]},{"id":"q-1799","question":"You run a real-time product risk scoring service on GCP with 50k QPS and 20 ms P95 latency, deployed in NA and EU. Design an end-to-end pipeline using Pub/Sub, Dataflow, Vertex AI, and BigQuery that enforces regional data residency, materializes features per region, serves online predictions with per-request explainability, and supports drift-driven rollback and cost controls. Outline architecture, data flow, and escalation criteria?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Adobe","DoorDash"]},{"id":"q-1837","question":"Design a beginner-friendly GCP ML pipeline to classify customer tickets with privacy in mind: Pub/Sub streams tickets, Dataflow applies DLP redaction and writes to BigQuery, Vertex AI trains a text classifier weekly, deploys a canary Vertex AI online endpoint, and uses drift metrics with alerts. If drift is detected, route traffic to the previous model version; include rough cost notes?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Bloomberg","Cloudflare"]},{"id":"q-1871","question":"Design an end-to-end, privacy-preserving multi-tenant ML pipeline on GCP that isolates customer data, uses Vertex AI for training and hosting, Dataflow for ETL, Pub/Sub for ingestion, and Data Catalog for lineage. Include differential privacy options, KMS-based key management, access controls, audit logging, and a rollback strategy for drift or privacy policy violations. Be concrete about components and data paths?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Anthropic","IBM","Stripe"]},{"id":"q-1940","question":"In a geo-distributed personalization pipeline on GCP, design a geo-canary rollout for a real-time ranking model across regions. Outline end-to-end usage of Vertex AI, Feature Store, Pub/Sub, and Dataflow with online/offline feature separation, drift monitoring, canary criteria, automatic rollback, and per-region cost controls?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Google","LinkedIn","Microsoft"]},{"id":"q-1966","question":"Design a region-aware, real-time update workflow for a multilingual product-support bot on GCP. Ingest user feedback via Pub/Sub; route to per-region Feature Store with Dataflow; train a multilingual NLU model in Vertex AI; deploy per-region canaries with automatic rollback; and implement drift alerts plus strict data residency controls and region-based cost caps?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Apple","Discord"]},{"id":"q-1973","question":"Design a beginner-friendly GCP ML pipeline to moderate user-uploaded product images in a marketplace. Ingest image events via Pub/Sub, Dataflow resizes and extracts safe metadata, stores references in BigQuery; Vertex AI trains a basic image classifier weekly using stored images, deploys an online endpoint with a canary rollout, and monitors drift per-tenant isolation. Include privacy safeguards and rough cost range?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Coinbase","Instacart","Twitter"]},{"id":"q-2020","question":"Design a multi-tenant, region-isolated content ranking system on GCP where each tenant enforces data residency in their region and supports per-tenant feature flags. Build with Vertex AI for model hosting, Vertex Feature Store for per-tenant features, Pub/Sub and Dataflow for streaming feature updates, and BigQuery for offline features. Describe tenant isolation, canary rollouts by tenant, drift detection thresholds, and rollback criteria with minimal impact?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Anthropic","LinkedIn","Uber"]},{"id":"q-882","question":"You run a real-time fraud-detection model on GCP at ~25k QPS with sub-20 ms P95 latency. Design a production pipeline using Vertex AI, Feature Store, Pub/Sub, and Dataflow that ingests events, materializes features, serves online predictions, and supports canary rollouts. Include data/model drift monitoring, automated rollback, and cost considerations?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Goldman Sachs","Snap"]},{"id":"q-905","question":"You operate a multi-tenant content recommender service on GCP used by advertisers. Each tenant has its own feature schema and data retention. Design a production ML pipeline (training and online serving) using Vertex AI, Feature Store, Pub/Sub, and Dataflow that supports **per-tenant namespaces**, **policy-based access**, **drift monitoring**, **automated rollback**, and **cost isolation**. Include data leakage prevention, schema evolution, and canary rollouts across regions?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Scale Ai","Tesla","Twitter"]},{"id":"q-922","question":"Design a real-time content moderation inference path on GCP that adds a Redis-based in‑memory cache in front of a Vertex AI online endpoint to reduce latency and cost. Outline what you cache (embeddings vs predictions), key schema (e.g., user_id, model_version, language), TTL and eviction policy, and how you invalidate cache on model deploy or feature updates. Include data privacy considerations and a plan for monitoring latency, cache hit rate, and drift. Provide a small Python sketch of the cache lookup?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Airbnb","Discord","Snap"]},{"id":"q-955","question":"Design a multi-tenant ML service on GCP that serves diverse customers with strict data isolation and retention policies. Propose a deployment and feature governance pattern using Vertex AI, Feature Store, Private Service Connect, Data Catalog, and Pub/Sub to isolate customer data, manage per-tenant feature lifecycles, perform drift monitoring, and enable tenant-specific canary rollouts with automated rollback and cost controls. Include concrete components, data flow, and rollback criteria?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Amazon","Meta"]},{"id":"q-1176","question":"In a multi-tenant GCP security environment, design an ephemeral admin-session workflow for Kubernetes and Cloud Run resources using IAP, Workload Identity Federation, and Binary Authorization. Include policy design, auditability, rollback, and how you'd verify there are no lingering grants after revocation?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Discord","LinkedIn","Microsoft"]},{"id":"q-1194","question":"In a GCP multi-tenant fintech data lake, design end-to-end safeguards to prevent tenant data leakage when multiple tenants share datasets in BigQuery and Cloud Storage. Propose a guardrail that blocks cross-tenant access at the data-product level using IAM Conditions, per-data-product VPC Service Controls, and Private Service Connect to a shared analytics endpoint. Include testing with synthetic misconfigurations and a rollback/audit plan?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Bloomberg","Plaid","Tesla"]},{"id":"q-1325","question":"In a multi-tenant GCP data platform used by Netflix‑like partners, outline a per-session data access model for a partner analytics job using IAM Conditions, Workload Identity Federation, and Access Context Manager to grant time-bounded, least-privilege access; include revocation, auditing, and validation with synthetic data?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Adobe","Microsoft","Netflix"]},{"id":"q-1400","question":"Design a multi-tenant GCP analytics stack (**Shared VPC**, **GKE**, Dataflow, BigQuery) with strict tenant isolation. Propose concrete network/IAM boundaries (per-tenant Namespaces, **NetworkPolicy**, **Private Service Connect**, **IAM Conditions**), a policy-as-code guard (**OPA**) in CI/CD to block cross-tenant paths, and a synthetic verification + rollback plan?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Airbnb","Apple","Slack"]},{"id":"q-1478","question":"In a GCP data-logging pipeline (Pub/Sub -> Dataflow -> BigQuery) for a fintech starter, outline a concrete beginner-friendly plan to ensure data never leaks: dedicated service accounts with least privilege, CMEK on the raw-logs bucket, IAM Conditions restricting access by time and principal, PII masking in outputs, and a CI test that injects synthetic logs to verify redaction and auditability. How would you validate in production?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Goldman Sachs","NVIDIA"]},{"id":"q-1505","question":"Design a secure, auditable cross‑organization data sharing pipeline in GCP: a data lake (Cloud Storage + BigQuery) holds PII-derived features; an external analytics partner connects via Private Service Connect to a synthetic dataset exposed for ad-hoc analysis while no raw PII leaves; specify ACM IAM Conditions per-tenant, PSC endpoints in a Shared VPC, CMEK for both stores, DLP masking prior to export, and automated revocation tests with synthetic data and rollback steps?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["NVIDIA","Scale Ai","Snowflake"]},{"id":"q-1587","question":"In a live GCP analytics stack (Pub/Sub → Dataflow → BigQuery) shared with external partners, you detect a suspected compromise of a Dataflow worker service account. Describe a concrete incident response playbook: containment (disable keys, rotate CMEKs), evidence preservation (export Cloud Audit Logs), access revocation with IAM Conditions, network containment (VPC Service Controls), and a post‑mortem with hardened controls. Include production validation steps?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Databricks","Tesla"]},{"id":"q-1719","question":"In a GCP multi-tenant data lake (BigQuery, Cloud Storage, Dataflow) used by three partners, design a crypto-agile CMEK rotation plan with zero downtime. Detail per-tenant key rings, IAM Conditions, and automatic key versioning; ensure data plane can switch keys without reprocessing. Include Access Context Manager, VPC Service Controls (PSC), DLP masking, and an automated attestations/rollback workflow with synthetic data?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Amazon","Google"]},{"id":"q-1736","question":"A GCP project hosts a Cloud Run API and a BigQuery warehouse. A contractor needs 2 weeks of read-only access to a small BI dataset, without touching production data. Draft a beginner-friendly plan: dedicated read-only service account, a dataset view for masking, IAM Bindings with an IAM Condition for a 14-day window, no public endpoints, and a test plan using a synthetic dataset and audit logs to validate revocation. How would you implement this?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Microsoft","PayPal","Robinhood"]},{"id":"q-1813","question":"In a GCP data science workspace using Vertex AI Workbench and BigQuery, draft a beginner-friendly plan to guarantee per-tenant data isolation across multiple tenants. Include: 1) per-tenant IAM roles/service accounts, 2) dataset-level access controls, 3) network borders via VPC Service Controls or Private Service Connect, 4) a CI test that injects synthetic data and validates isolation, and a safe rollback if misconfig is detected?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Hugging Face","Salesforce","Slack"]},{"id":"q-1830","question":"In a GCP-based security control plane for a fintech platform used by Robinhood and Zoom, external CI/CD pipelines must deploy and validate security baselines without long‑lived credentials. Design a Workload Identity Federation solution that maps external OIDC identities to short‑lived GCP service accounts, enforcing least privilege with IAM Conditions. Include per‑tenant isolation, auditability, token lifetimes, revocation, and automated drift/rollback tests in the CI pipeline?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Robinhood","Zoom"]},{"id":"q-1854","question":"In a GCP analytics platform with BigQuery, Dataflow, and Data Catalog used by multiple tenants, design a per-tenant row-level security model using BigQuery Row Access Policies tied to TenantID, and IAM Conditions for dataset access, complemented by Data Catalog tags and per-tenant CMEK. Include ingestion, query-time enforcement, testing with synthetic tenants, rollback, and auditability?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Google","Meta","Uber"]},{"id":"q-1986","question":"In a GCP-based data science platform shared by three tenants, design a practical beginner-friendly security baseline to prevent cross-tenant access during notebook runs, data prep, and model training. Include: 1) per-tenant Secrets Manager keys with rotation; 2) dataset/bucket access controls and per-tenant IAM/service accounts; 3) a VPC Service Controls perimeter around processing endpoints; 4) a CI check that injects synthetic data and validates isolation; 5) a rollback plan if misconfig detected?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Anthropic","Databricks","Two Sigma"]},{"id":"q-2002","question":"In a real-time GCP data lake (Pub/Sub -> Dataflow -> BigQuery -> GCS) shared across three brands, design an automated incident containment scenario: when a service account shows anomalous access patterns, how would you instantly restrict access, rotate keys, and quarantine the project while preserving pipelines? Include exact IAM bindings, PSC/VPC controls, CMEK strategies, and rollback tests?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Instacart","Tesla","Twitter"]},{"id":"q-2026","question":"In a shared GCP data lake (Pub/Sub → Dataflow → BigQuery) used by Snowflake, Netflix, and Apple, design a per-brand data isolation and crypto agility strategy. Specify per-brand CMEK key rings, IAM Conditions to enforce least privilege, per-brand VPC Service Controls perimeters, and an automated key rotation and rollback plan with end-to-end tests for authorization, auditing, and data access paths?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Apple","Netflix","Snowflake"]},{"id":"q-2055","question":"In a GCP multi-tenant streaming lake (Pub/Sub -> Dataflow -> BigQuery) shared by three brands, a service account is suspected of exfiltrating data. Design an automated, tenant-aware containment plan that isolates the compromised tenant without interrupting others: 1) tenant-scoped IAM bindings with conditional denies, 2) per-tenant VPC Service Controls and Private Service Connect paths, 3) CMEK-driven key rotation with automatic rotation triggers, 4) pipeline cutover and rollback procedures, 5) synthetic-data canaries and automated validation before full failover?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Apple","Instacart","Zoom"]},{"id":"q-2104","question":"Design a cryptographically verifiable data provenance system for a real-time GCP data lake (Pub/Sub -> Dataflow -> BigQuery -> GCS) shared by three brands. How would you generate per-record provenance tokens, sign them with Cloud KMS, attach them to the data, rotate keys, and verify integrity at read time while preserving pipeline throughput? Include how you'd store proofs, enforce tenant isolation, and rollback tests?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Anthropic","MongoDB","Plaid"]},{"id":"q-863","question":"In a GCP data pipeline that streams PII from Pub/Sub through Dataflow to BigQuery and Cloud Storage, outline a concrete hardening plan: exact IAM bindings with least privilege, VPC Service Controls, private access to API endpoints, encryption key management, access reviews, and monitoring/alerting. Include how you’d validate controls in production?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Adobe","Amazon","Meta"]},{"id":"q-899","question":"In a GCP data pipeline streaming PII from Pub/Sub through Dataflow to BigQuery and Cloud Storage, enable secure cross‑org sharing with an external analytics partner via Private Service Connect. Draft a concrete architecture: least‑privilege IAM bindings per data product, cross‑project scopes, PSC endpoints in a shared VPC, CMEK, DLP masking, and automated validation with synthetic data and revocation tests. End with auditable controls and rollback plan?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Slack","Zoom"]},{"id":"q-911","question":"In a GCP data pipeline that streams PII from Pub/Sub through Dataflow to BigQuery and Cloud Storage across two orgs, enable time-bounded access for an external analytics partner without static credentials. Draft a concrete design using Workload Identity Federation, IAM conditions, ephemeral credentials, Private Service Connect, and VPC Service Controls. Include trust, token lifetimes, auditing, automated revocation tests, and rollback?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Cloudflare","LinkedIn","PayPal"]},{"id":"q-945","question":"In a **Terraform-driven GCP** multi-tenant environment used by **Salesforce** and **Discord**, implement automated guardrails that block any public bucket or dataset and enforce least-privilege IAM at module boundaries. Describe how you’d implement **OPA constraints**, integrate with **CI/CD**, test with synthetic misconfigs, and provide rollback/auditability?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Discord","Salesforce"]},{"id":"q-1970","question":"Design a LangChain-based agent that uses Autogen to plan and execute a multi-step inquiry: check stock via /inventory API, fetch ETA via /shipping API, and present a precise delivery window with caveats; implement robust error handling with retries, fallbacks, and timeouts, and show how the agent would adjust its plan if the stock check returns uncertain results?","channel":"generative-ai","subChannel":"agents","difficulty":"intermediate","tags":["langchain","autogen","tool-use","planning"],"companies":["Instacart","Square"]},{"id":"q-430","question":"How would you implement a basic AI agent using LangChain that can use tools to answer user questions about weather data?","channel":"generative-ai","subChannel":"agents","difficulty":"beginner","tags":["langchain","autogen","tool-use","planning"],"companies":["Amazon","Anthropic","Apple","Google","Microsoft","OpenAI","Snowflake"]},{"id":"q-445","question":"You're building a multi-agent system using LangChain and AutoGen for autonomous code generation. How would you design a robust tool-use framework that prevents malicious code execution while maintaining agent autonomy?","channel":"generative-ai","subChannel":"agents","difficulty":"advanced","tags":["langchain","autogen","tool-use","planning"],"companies":["Databricks","Google","Tesla"]},{"id":"q-322","question":"How would you measure and reduce hallucination in a large language model deployed for customer service?","channel":"generative-ai","subChannel":"evaluation","difficulty":"beginner","tags":["hallucination","faithfulness","relevance"],"companies":["Amazon","Google","IBM","Mckinsey","Meta","Microsoft","Salesforce"]},{"id":"q-385","question":"You're building a hallucination detection system for a production LLM service. Design a multi-layered evaluation pipeline that balances false positives/negatives while maintaining sub-100ms latency. How would you implement confidence scoring and fallback mechanisms?","channel":"generative-ai","subChannel":"evaluation","difficulty":"advanced","tags":["hallucination","faithfulness","relevance"],"companies":["Anthropic","Google","Stripe"]},{"id":"q-402","question":"How would you design a hallucination detection system for a medical AI assistant that evaluates faithfulness against verified drug databases while maintaining 99.9% accuracy?","channel":"generative-ai","subChannel":"evaluation","difficulty":"advanced","tags":["hallucination","faithfulness","relevance"],"companies":["Microsoft","Netflix","Veeva"]},{"id":"q-463","question":"How would you evaluate if an LLM's response is faithful to the provided source documents?","channel":"generative-ai","subChannel":"evaluation","difficulty":"beginner","tags":["hallucination","faithfulness","relevance"],"companies":["Cloudflare","Microsoft","PayPal"]},{"id":"q-494","question":"How would you design a comprehensive evaluation framework to detect hallucinations in a large language model deployed for customer support, considering both factual accuracy and faithfulness to provided context?","channel":"generative-ai","subChannel":"evaluation","difficulty":"advanced","tags":["hallucination","faithfulness","relevance"],"companies":["Discord","Meta","Plaid"]},{"id":"q-524","question":"How would you evaluate a generative AI model's tendency to hallucinate when answering factual questions about company policies?","channel":"generative-ai","subChannel":"evaluation","difficulty":"beginner","tags":["hallucination","faithfulness","relevance"],"companies":["Microsoft","Uber"]},{"id":"q-1673","question":"Given a 7B-class LLM, design a practical PEFT plan to add domain-specific knowledge for a banking app using LoRA/QLoRA/adapter techniques. Specify target modules, r, alpha, dropout, quantization approach (e.g., 4-bit), dataset size, batch size, learning rate, and epochs. Explain how you'd validate improvements without harming generalization and how to deploy adapters for on-device inference?","channel":"generative-ai","subChannel":"fine-tuning","difficulty":"intermediate","tags":["lora","qlora","peft","adapter"],"companies":["Apple","Plaid"]},{"id":"q-1780","question":"How would you fine-tune a 7B base model for a live chat assistant using a LoRA adapter (via QLoRA/PEFT) on a 2k-example dataset? Include modules to target, rank, and precision, data prep, training setup, and evaluation strategy. Provide a minimal code snippet to attach a LoRA adapter to a transformer layer?","channel":"generative-ai","subChannel":"fine-tuning","difficulty":"beginner","tags":["lora","qlora","peft","adapter"],"companies":["Airbnb","MongoDB","Zoom"]},{"id":"q-225","question":"When implementing LoRA fine-tuning for a 7B parameter LLM, how do you determine the optimal rank (r) and alpha values to balance performance and memory efficiency while maintaining model quality?","channel":"generative-ai","subChannel":"fine-tuning","difficulty":"intermediate","tags":["lora","qlora","peft","adapter"],"companies":["Amazon","Databricks","Google","Meta","Microsoft","NVIDIA"]},{"id":"q-250","question":"What is LoRA and how does it reduce parameters when fine-tuning large language models?","channel":"generative-ai","subChannel":"fine-tuning","difficulty":"beginner","tags":["lora","qlora","peft","adapter"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"q-197","question":"How would you implement efficient KV caching in a transformer decoder to reduce redundant computation during autoregressive generation?","channel":"generative-ai","subChannel":"llm-fundamentals","difficulty":"intermediate","tags":["transformer","attention","tokenization"],"companies":["Amazon","Google","Meta","Microsoft","OpenAI"]},{"id":"q-308","question":"How does the self-attention mechanism in transformers compute token relationships?","channel":"generative-ai","subChannel":"llm-fundamentals","difficulty":"intermediate","tags":["transformer","attention","tokenization"],"companies":["Amazon","Google","Meta"]},{"id":"q-371","question":"You're designing a custom tokenizer for a multilingual LLM that needs to handle code-switching between English and Chinese. How would you optimize the vocabulary to minimize token count while preserving semantic meaning, and what attention mechanism modifications would you consider?","channel":"generative-ai","subChannel":"llm-fundamentals","difficulty":"advanced","tags":["transformer","attention","tokenization"],"companies":["Meta","Microsoft","NVIDIA"]},{"id":"q-414","question":"Explain how the self-attention mechanism in a transformer works and why it's more effective than RNNs for processing long sequences?","channel":"generative-ai","subChannel":"llm-fundamentals","difficulty":"beginner","tags":["transformer","attention","tokenization"],"companies":["Amazon","Anduril","Google","Meta","Microsoft","NVIDIA","OpenAI","Tesla"]},{"id":"q-577","question":"How would you debug a transformer model where attention weights are becoming uniform across all tokens, leading to poor performance?","channel":"generative-ai","subChannel":"llm-fundamentals","difficulty":"intermediate","tags":["transformer","attention","tokenization"],"companies":["IBM","PayPal"]},{"id":"q-293","question":"How do you optimize chunking strategies for different document types in RAG systems?","channel":"generative-ai","subChannel":"rag","difficulty":"advanced","tags":["retrieval","embeddings","vector-db","chunking"],"companies":["Amazon","Google","Meta"]},{"id":"q-335","question":"You're building a RAG system for SAP's customer support. How would you chunk a 10-page technical manual to ensure relevant sections are retrieved?","channel":"generative-ai","subChannel":"rag","difficulty":"beginner","tags":["retrieval","embeddings","vector-db","chunking"],"companies":["Amazon","Google","IBM","Microsoft","MongoDB","Planetscale","Sap"]},{"id":"q-438","question":"You're building a RAG system for DoorDash's restaurant search. How would you design a hybrid retrieval strategy combining semantic and keyword search to handle queries like 'cheap Italian delivery near me' while maintaining sub-100ms latency?","channel":"generative-ai","subChannel":"rag","difficulty":"advanced","tags":["retrieval","embeddings","vector-db","chunking"],"companies":["Amazon","Apple","DoorDash","Google","Lyft","Meta","Microsoft","Netflix"]},{"id":"q-551","question":"You're building a RAG system for Discord's message search. Messages have varying lengths, code blocks, and threaded conversations. How would you design your chunking strategy and what embedding model would you choose?","channel":"generative-ai","subChannel":"rag","difficulty":"advanced","tags":["retrieval","embeddings","vector-db","chunking"],"companies":["Discord","Snowflake"]},{"id":"q-1145","question":"Design an online nonlinear ICA pipeline for a 12-mic, 6-camera live broadcast where mixing is nonlinear and time-varying due to the environment. Propose an invertible neural network demixing model, online training with forgetting, a temporal prior to capture dynamics, and strategies for permutation/scale alignment. Include evaluation plan and DSP constraints?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Adobe","Citadel","Coinbase"]},{"id":"q-1208","question":"Design a privacy-preserving, edge-based ICA pipeline to separate overlapping speech captured by a distributed 32‑mic array where raw audio never leaves devices and only anonymized components are aggregated. Describe per‑device whitening with partial channels, online demixing updates, secure aggregation methods, permutation/scale alignment across devices, latency targets, drift handling, and an evaluation plan with synthetic ground truth and transcripts?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Discord","Goldman Sachs","Instacart"]},{"id":"q-1231","question":"In a smart conference room, 6 microphones capture audio while 2 cameras provide synchronized lip-movement visuals. Propose an ICA-based pipeline to jointly separate independent audio sources and align them to speaker identities using visual cues as auxiliary information. Include (i) whitening and joint diagonalization strategy, (ii) how to integrate visual cues into the contrast to improve permutation recovery, (iii) online adaptation for moving speakers, and (iv) a concrete evaluation plan with ground-truth sources and lip-sync metrics?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["LinkedIn","Meta"]},{"id":"q-1316","question":"3-mic wearable office demo: design a beginner ICA pipeline to separate two voices captured by near-field mics in a noisy room. Outline: (i) whiten the 3 channels, (ii) apply a simple real-valued FastICA on short FFT frames to recover two sources, (iii) align permutation across frames via spatial-map correlation, (iv) include a lightweight online update to the demixing matrix for motion, (v) evaluate with synthetic ground truth (SDR/SIR) and a listening check; target <100 ms latency on a low-power CPU?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["NVIDIA","Salesforce"]},{"id":"q-1458","question":"In a 4-channel EEG headset recording a brief resting-state session, design a beginner ICA pipeline to separate neural components from ocular and EMG artifacts. Outline: whitening, choice between real-valued FastICA or Infomax, frame-wise vs block-wise ICA with permutation alignment across windows, online adaptation for impedance drift, and a practical evaluation plan using simulated ground truth sources and known event-related potentials?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Google","Salesforce"]},{"id":"q-1500","question":"In a city-scale IoT deployment for smart buildings, 128 sensors stream heterogeneous, non-stationary signals (temperature, occupancy, vibration) that mix linearly in the cloud. Design a distributed online ICA to recover independent sources in real time, handling missing channels (packet loss), nonstationary mixing, and limited inter-node communication. Include whitening, update rules, drift handling, and an evaluation plan with ground truth?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Airbnb","Databricks","Uber"]},{"id":"q-1601","question":"Intermediate: In a 6-mic conference room capturing two moving speakers with strong reverberation and nonlinear mic distortions (AGC/compression), design a real-time post-nonlinear ICA (PNL-ICA) pipeline to separate sources. Include: a y = g(Ax) model, whitening strategy, online demixing updates, choice between per-bin ICA vs joint diagonalization, handling time-varying mixing, and an evaluation plan using SDR/SIR and transcript-aligned ground truth?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["Adobe","MongoDB","Zoom"]},{"id":"q-1646","question":"Beginner ICA task: A 4-mic array in a small classroom records two overlapping speakers with room reverberation. Design and implement a simple offline ICA pipeline (FastICA) to recover the two voices. Explain preprocessing (centering, whitening), nonlinearity, choice of whitening (PCA vs ZCA), how to align permutations across time windows, and provide a basic SDR/SIR evaluation using ground-truth signals?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Goldman Sachs","Google","OpenAI"]},{"id":"q-1742","question":"With an 8-mic array where each microphone samples at a slightly different rate causing synchronization drift, design a real-time ICA pipeline to separate sources under asynchronous observations. Specify (i) pre-alignment/handling of irregular samples, (ii) whitening strategy, (iii) online demixing updates tolerant to drift, (iv) latency targets and evaluation plan (SDR/SIR) with ground-truth alignment?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Databricks","LinkedIn","Twitter"]},{"id":"q-1768","question":"Beginner ICA task: In a live street interview, a 2-mic smartphone captures two overlapping speakers with wind noise. Design a lightweight online ICA pipeline that runs under 50 ms/frame on a mobile CPU. Include: (i) 20 ms frames with 50% overlap, (ii) whitening, (iii) a simple online ICA (2×2 demixing) with a tanh nonlinearity, (iv) frame-to-frame permutation alignment via cross-frame non-Gaussianity smoothing, (v) evaluation plan with synthetic ground-truth SDR/SIR and listening checks?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Adobe","Amazon","Coinbase"]},{"id":"q-1918","question":"In a 12-mic automotive cabin with three moving talkers and non-stationary noise, design an online convolutive ICA pipeline to separate sources in real time. Specify: whitening and natural-gradient demixing, per-bin vs joint diagonalization choice, online permutation/scale alignment with temporal continuity, drift handling, and an evaluation plan with SDR/SIR and ground-truth transcripts?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Amazon","Coinbase","Lyft"]},{"id":"q-1936","question":"Real-time 4‑mic ICA in a browser: use a 32 ms sliding window at 48 kHz with 50% overlap. Whitening via incremental PCA on 4 channels to produce uncorrelated components. Learn a 4×2 demixing matrix W with online gradient on y = W x using a tanh nonlinearity; adapt the learning rate to keep total latency under 60 ms. Resolve permutation by tracking envelope correlations frame-to-frame and reordering by energy; test with synthetic overlapping voices plus noise and report SDR/SIR?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Cloudflare","Databricks","Netflix"]},{"id":"q-1996","question":"You have 8 EEG channels capturing a simple task. Design an offline ICA pipeline (e.g., FastICA) to separate neural sources from eye-blink and muscle artifacts. Specify preprocessing (bandpass 1–40 Hz, centering), whitening, nonlinearity choice (tanh), number of components, criteria to identify artifact components (correlation with EOG/EMG, topographies), and a basic validation plan using simulated ground-truth sources and reference channels. End with a question mark?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Goldman Sachs","Stripe"]},{"id":"q-2117","question":"In a factory setting, an 8‑mic array is mounted on a mobile robot navigating among moving machinery noise. Design a real-time online convolutive ICA pipeline that (a) uses STFT framing, (b) models a time‑varying mixing matrix M(t), and (c) decides between per‑bin ICA vs joint diagonalization. Propose how to utilize IMU/pose priors to stabilize permutation and scale across time, handle drift, and meet real-time constraints. Include an evaluation plan with SDR/SIR and task‑specific metrics?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Citadel","Hashicorp","Square"]},{"id":"q-2129","question":"Design a real-time online ICA pipeline for a 12‑mic array on a factory floor capturing three moving sound sources (robot arm motors, alarms, and human chatter). The system must tolerate intermittent mic dropout, impulsive noise bursts, and a constrained CPU. Describe frame structure, whitening, demixing updates, per-bin vs joint diagonalization, dropout handling, latency targets, and evaluation plan (ground-truth SDR/SIR)?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["Plaid","Tesla","Twitter"]},{"id":"q-839","question":"**Advanced ICA Challenge**: Given a 3×N mixed signal matrix from sensors, outline a concrete plan to recover independent sources with FastICA. Include whitening steps, nonlinearity choice (e.g., g(u)=tanh(u)), convergence criteria, how you resolve sign/perm ambiguity, and how you compare to PCA on the same data. Include practical inputs and diagnostics?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["IBM","Two Sigma","Uber"]},{"id":"q-875","question":"Given a 6-channel wearable time-series with non-stationary motion artifacts and slowly varying latent sources, design an online ICA workflow to separate the sources in real time. Specify streaming whitening, adaptive contrast functions, choice of nonlinearity, handling sign and permutation drift, and a robust validation plan against synthetic ground truth and a PCA baseline?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["Citadel","Salesforce"]},{"id":"q-910","question":"An 8-microphone array records a live conference with moving speakers and reverberation. Design a practical convolutive ICA pipeline to separate sources. Include: STFT-based mixing, choice between per-bin ICA vs joint diagonalization, permutation alignment across frequency bins, tracking non-stationary mixing, real-time feasibility, and evaluation plan (SDR/SIR, ground truth)?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["LinkedIn","OpenAI","Robinhood"]},{"id":"q-953","question":"Design a real-time, online complex-valued ICA pipeline to separate RF sources from an 8-antenna receiver in a dynamic multipath environment with drifting mixing. Describe (a) complex whitening + fixed-point ICA update, (b) the complex contrast and convergence rule, (c) tracking sign and permutation drift over time, and (d) a practical evaluation plan with synthetic ground truth and over-the-air tests under DSP constraints?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["IBM","Microsoft","NVIDIA"]},{"id":"q-1064","question":"Design and implement a streaming app's episode grid using a UICollectionView with a compositional layout that supports dynamic item sizes, infinite scrolling, and efficient image caching; use a modern iOS approach (Diffable Data Source, NSCache, prefetching), ensure accessibility and testability?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["Apple","Netflix","Tesla"]},{"id":"q-1164","question":"You're building a lightweight notes app for iOS. Implement a NotesEditor view (SwiftUI) that autosaves to disk as the user types, using a 500ms debounce. Persist to a local JSON file in the app's documents directory. Ensure rapid typing doesn't cause multiple disk writes, and provide a minimal unit test that verifies the file content is updated after a debounce period?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Discord","Google","Tesla"]},{"id":"q-1185","question":"In an iOS app, you are displaying a feed of user avatars in a UICollectionView with infinite scrolling. Explain how you would implement incremental image loading that cancels obsolete requests, caches images both in memory and on disk, uses Swift concurrency for loading, and ensures smooth scrolling under memory pressure, including prefetching and memory-pressure handling strategies?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["Airbnb","Bloomberg","Goldman Sachs"]},{"id":"q-1439","question":"In a recipe-list iOS app, implement a multi-select ingredient filter as tappable chips using UIKit. Given a static dataset of recipes with ingredients, create a chips bar that allows selecting multiple ingredients, a Clear button, and a table view that shows only recipes containing all selected ingredients. Ensure accessibility labels and VoiceOver order, and provide a simple unit test validating the filter logic?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Apple","Databricks","DoorDash"]},{"id":"q-1573","question":"Design a beginner iOS feature: a SwiftUI-based **Favorites** screen for a recipe app. It should load a predefined array of Recipe objects from a local JSON file, display in a single-column list, allow tapping to mark/unmark as favorite, persist favorites in **UserDefaults**, and provide a search bar to filter by name **case-insensitively**. Explain how you'd structure the model, storage, and UI, and how you'd test search and persistence?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Airbnb","Google","Netflix"]},{"id":"q-1611","question":"Design and implement an offline-first notes feature for iOS: store locally in SQLite, use a simple CRDT-like merge or last-writer-wins with version vectors for conflict resolution, and sync changes via a WebSocket protocol, all in Swift using async/await. Include data model, conflict handling, and testing strategy?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["OpenAI","Oracle"]},{"id":"q-1681","question":"In an iOS SwiftUI app, build a minimal Notes editor screen: a multiline TextEditor bound to a string. Implement an autosave that triggers 1 second after the user stops typing, saving the draft to UserDefaults under the key 'noteDraft'. Load the draft on view appear. Include a tiny unit test that simulates typing and asserts the draft is saved after the debounce delay?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Airbnb","Google"]},{"id":"q-1718","question":"In a high-traffic iOS app for autonomous telemetry, describe a robust data pipeline that streams sensor data into local batches and uploads to cloud with offline queueing and crash recovery. Include how to handle backpressure, idempotency, and testing. Provide concrete Swift components and a minimal prototype?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["IBM","NVIDIA","Tesla"]},{"id":"q-1951","question":"You're building an **offline-first** image gallery in an iOS app. Describe in detail how you would implement a robust caching and download strategy that supports offline browsing, seamless updates when connectivity returns, and **conflict resolution**. Include data models, caching policy, background downloads, and testing approaches?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["Scale Ai","Tesla","Zoom"]},{"id":"q-2089","question":"Design an offline-first album sync for iOS that uses a CRDT-based OR-Set to resolve conflicts in PhotoMetadata across devices. Include data model, merge rules with per-album ACLs, and a test plan for intermittent connectivity?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["Discord","IBM","Scale Ai"]},{"id":"q-464","question":"How would you implement a custom UICollectionViewFlowLayout that supports dynamic cell heights and sticky headers while maintaining smooth scrolling performance?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["Amazon","Lyft","Meta"]},{"id":"q-495","question":"How would you implement a simple UITableView with custom cells in iOS using Swift?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Oracle","Snowflake","Uber"]},{"id":"q-525","question":"You're building a food delivery app like DoorDash. How would you implement background location updates to track delivery drivers while balancing battery life and accuracy?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["DoorDash","Microsoft","Zoom"]},{"id":"q-578","question":"What's the difference between weak and unowned references in Swift, and when would you use each?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Meta","NVIDIA"]},{"id":"q-181","question":"Explain the difference between weak and unowned references in Swift and provide practical use cases for each?","channel":"ios","subChannel":"swift","difficulty":"intermediate","tags":["swift","language"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"q-257","question":"What is optional chaining in Swift and how does it compare to force unwrapping, optional binding, and optional chaining with method calls when accessing nested optional properties?","channel":"ios","subChannel":"swift","difficulty":"beginner","tags":["optionals","protocols","generics"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Stripe"]},{"id":"q-204","question":"How would you optimize a UITableView with 10,000+ complex cells using Auto Layout while maintaining 60fps scrolling and memory efficiency?","channel":"ios","subChannel":"uikit","difficulty":"advanced","tags":["autolayout","tableview","collectionview"],"companies":["Airbnb","Apple","Capital One","Lyft","Uber"]},{"id":"q-232","question":"How does Auto Layout constraint resolution work when creating a UITableView with dynamic cell heights?","channel":"ios","subChannel":"uikit","difficulty":"beginner","tags":["autolayout","tableview","collectionview"],"companies":["Airbnb","Apple","Google","Meta","Uber"]},{"id":"q-1075","question":"You're building a beginner-friendly KCA flow for a data science platform where notebooks run in isolated containers across tenants. The platform uses a cloud CA to issue short-lived client certificates (2 hours) via token-based enrollment (JWT) rather than CSR, with a gateway that validates certs and maps tenants. Describe the enrollment flow, renewal, and how the gateway handles clock skew, expired certs, and failed enrollments. Include a minimal test plan and example API calls?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Databricks","Hugging Face"]},{"id":"q-1082","question":"Design a beginner-friendly KCA workflow focusing on policy-driven certificate issuance for a multi-tenant SaaS gateway: tenants publish per-tenant certificate policies (allowed CNs, key type, validity). A central CA issues short-lived client certs, and edge gateways refresh policy every 60 minutes while rotating certs to avoid handshake failures. Explain enrollment, policy propagation, revocation, and provide a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Adobe","Cloudflare","DoorDash"]},{"id":"q-1120","question":"Explain how you would implement a beginner-friendly KCA workflow for a multi-region SaaS where regional CAs issue per-service client certs with a 2-hour TTL. Include enrollment (CSR-based vs token-based), cross-region policy propagation, revocation, and how to validate certificates at the edge during region failover. Provide a minimal test plan and rollback strategy?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Apple","Databricks","Instacart"]},{"id":"q-1141","question":"For an industrial IoT fleet of 50k field devices with intermittent connectivity, design a KCA workflow where each device boots with a hardware-backed key and obtains a short-lived client certificate from a central CA via an attestation-enabled bootstrap. Include enrollment, attestation checks, certificate lifetimes (6 hours), automatic renewal after reconnection, revocation for decommissioned devices via OCSP/CRLs, offline root fallback, auditing, and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Meta","NVIDIA"]},{"id":"q-1189","question":"Design a beginner-friendly KCA flow for a multi-tenant SaaS gateway that issues per-tenant, short-lived client certificates via token-based enrollment, but enforces per-tenant issuance quotas (e.g., 100 certs/hour with burst to 10x). Explain enrollment, quota enforcement, renewal, revocation, and audit logging, and provide a minimal test plan. Include how you handle quota exhaustion during spikes?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Meta","Slack","Uber"]},{"id":"q-1353","question":"Design a beginner-friendly KCA workflow for a multi-region SaaS API gateway where a primary CA replicates to a secondary region for DR. Explain policy propagation, certificate rollover during failover, cache freshness, cross-region revocation handling, and how edge gateways and clients stay in sync with minimal downtime. Include a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Bloomberg","Databricks","Uber"]},{"id":"q-1527","question":"In a multi-region SaaS gateway architecture, design a KCA-based onboarding flow for both tenants and devices using mTLS. Devices boot with hardware attestation and CSR enrollment to a regional intermediate CA, which signs a 15-minute device cert bound to device identity and tenant policy. Describe enrollment, policy propagation, cross-region trust, cert rotation, revocation (OCSP/CRL), and audit logging. Include failure modes and a concrete 1-page test plan with synthetic onboarding scenarios?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Amazon","Microsoft"]},{"id":"q-1568","question":"Design a global **KCA onboarding** flow for an edge gateway fleet serving 100k tenants. Each gateway boots with a hardware-backed secret and attestation-enabled CSR enrollment, then obtains a short-lived client certificate whose policy extension encodes per-tenant access rules and rate limits. Explain policy refresh, renewal, revocation, and audit trails. Include a concrete example policy and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Adobe","Salesforce"]},{"id":"q-1645","question":"Design a KCA onboarding workflow for 20k edge gateways deployed across regions where each device boots with hardware attestation and a measured firmware hash. The regional CA issues a 12-hour TLS client certificate bound to the firmware hash and attestation identity. Describe enrollment, firmware-hash policy binding, renewal on upgrade, rollback revocation via OCSP/CRLs, offline root fallback, and comprehensive auditing. Include a practical test plan with upgrade/rollback scenarios?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["MongoDB","Scale Ai","Tesla"]},{"id":"q-1651","question":"Design a KCA onboarding and certificate lifecycle flow for 200 edge API gateways deployed across three data centers, serving both internal microservices and external partners. Each gateway boots with a hardware-backed root and attestation-enabled CSR enrollment, then obtains a 10-minute mTLS client certificate from a regional CA. Include enrollment, region-aware policy propagation via a streaming config service, cross-region trust, certificate renewal, revocation via OCSP/CRLs, offline root fallback, auditing, and a concrete 1-page test plan?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["IBM","Meta","Two Sigma"]},{"id":"q-1748","question":"Design a beginner-friendly KCA workflow for a data ingestion platform used by Snowflake, Airbnb, and OpenAI that authenticates per-tenant producers and external partners via mTLS? Implement per-session certificates (30 min) with hardware-backed attestation and policy refresh via streaming config every 15 min. Cover enrollment, renewal, revocation, audit logs, and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Airbnb","OpenAI","Snowflake"]},{"id":"q-1878","question":"In a high-throughput data desk environment, design a KCA-based onboarding for 1,000 streaming data connectors across two data centers. Each connector boots with hardware attestation and obtains a 60-second TLS client certificate from a regional CA, rotated automatically on reconnection. Include enrollment, attestation checks, policy binding to data-stream permissions, cross-region trust, revocation (OCSP/CRLs), offline root fallback, auditing, and a test plan with simulated reconnections and data bursts?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Bloomberg","Two Sigma"]},{"id":"q-1961","question":"Design a KCA onboarding for a global fleet of 5,000 IoT/edge devices migrating from RSA TLS to post-quantum TLS during active operation. Devices boot with hardware-backed keys and attestation-based CSR; regional CAs issue both RSA and PQC certificates with 6-hour lifetimes, automatic renewal on reconnect, and cross-region trust. Include dual-PKI policy binding, revocation (OCSP/CRLs), offline root fallback, auditing, and a 1-page test plan with PQC handshake fallbacks and rollback?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Citadel","LinkedIn","Uber"]},{"id":"q-1978","question":"Design a beginner-friendly KCA workflow for a serverless edge gateway fleet serving 50k tenants. Issue per-tenant client certs with 15-minute lifetimes and a 'scope' extension describing allowed APIs and rate limits. Enrollment via CSR or token; propagate policy to edge nodes via streaming config with staggered renewals. Support per-tenant revocation via OCSP stapling or small CRLs; include audit logs and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Amazon","Google","Lyft"]},{"id":"q-2004","question":"Design a beginner-friendly KCA workflow issuing per-store client certificates with 1h TTL for a distributed edge gateway fleet in 200 retail locations. Enrollment via CSR templates or token-based enrollment; embed per-store API access and rate limits in a certificate policy extension. Include policy propagation via streaming config with staggered renewals, offline revocation via local CRLs, and audit logs. Provide a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Instacart","Snowflake","Two Sigma"]},{"id":"q-2069","question":"Design a KCA onboarding and certificate lifecycle for an autonomous drone fleet deployed across three countries, operating with intermittent connectivity and strict flight-permission policies. Drones boot with hardware-backed keys and attestation; obtain a short-lived TLS client certificate from a regional CA, with policy-bound flight rights, cross-region trust, and automatic certificate rotation on rejoin. Include revocation via OCSP/CRLs, offline root fallback, auditing, and a 1-page test plan with realistic sorties?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Cloudflare","NVIDIA","Oracle"]},{"id":"q-2166","question":"Design a KCA flow for a global microservice mesh where workloads across 4 regions obtain ephemeral TLS client certificates for mTLS. Each workload presents hardware-backed attestation, is issued a short-lived certificate with a SPIFFE ID, and must maintain cross-region trust, rapid revocation, and auditable logs. Include enrollment, attestation, rotation cadence, and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Apple","Lyft","NVIDIA"]},{"id":"q-844","question":"You have a CSV log with columns: user_id, event_timestamp (ISO 8601), and event_type. Write a Python function using pandas to compute the number of unique active users per day for a given timezone, returning a dict mapping 'YYYY-MM-DD' to count. Explain how you handle timezone normalization and missing data. Provide sample usage?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Coinbase","Databricks"]},{"id":"q-884","question":"You're operating a Kafka-to-Spark streaming job in production and observe sporadic latency spikes; detail a concrete diagnostic plan to identify bottlenecks and a remediation strategy, including metrics, tooling (OpenTelemetry, Prometheus, Grafana), and validation steps?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Anthropic","DoorDash","Google"]},{"id":"q-889","question":"**How would you design a scalable KCA integration** for a multi-tenant SaaS using short-lived client certificates? Central CA in an HSM issues per-tenant CSRs, rotates certificates every 24h, and supports revocation via OCSP stapling and short CRLs with edge caching. Include audit trails, MFA for CA access, and clear renewal, compromise, and revocation workflows?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Cloudflare","Microsoft","Plaid"]},{"id":"q-952","question":"How would you design an on-demand KCA for service-to-service mTLS in a multi-tenant data platform (Stripe/Databricks) where a Kubernetes-hosted CA issues per-service CSRs, each cert valid for 5–15 minutes, supports cross-region trust, anomaly-driven revocation via telemetry, and full audit trails; include MFA protection and an offline root fallback?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Databricks","Stripe"]},{"id":"q-997","question":"Design a beginner-friendly KCA flow for a SaaS API gateway where a cloud CA issues per-tenant, short-lived client certificates (4 hours) for app authentication. Outline provisioning (CSR-based enrollment or token-based enrollment), automatic renewal, revocation strategy, and how the gateway validates certs and records audit logs. Include failure modes and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Bloomberg","Netflix","OpenAI"]},{"id":"q-1138","question":"You are building a real-time KCNA feed service used by Snap, Meta, and Discord-scale clients to publish and deliver announcements across regions with sub-100ms tail latency. Describe the end-to-end architecture, data model for Announcement, ingestion and delivery pipeline, guarantees (at-least-once vs exactly-once), ordering, deduplication, failover, tests, and observability. How would you scale to 10k updates/sec with 99.999% uptime?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Discord","Meta","Snap"]},{"id":"q-1295","question":"**KCNA Consumer Backpressure & Gap Handling**: In a beginner-friendly KCNA consumer, design a pull-based ingestion path that preserves per-topic offsets, guarantees at-least-once processing, and recovers from transient network slowdowns without duplicating messages. Describe the API shape, offset persistence, retry/backoff strategy, and a minimal test plan including a canary scenario?","channel":"kcna","subChannel":"general","difficulty":"beginner","tags":["kcna"],"companies":["Discord","Plaid","Two Sigma"]},{"id":"q-1431","question":"KCNA cross-region, multi-tenant QoS: Propose an architecture and API for declaring per-tenant Topic SLAs (retention, max throughput) and a cross-region offset store with region-local commit logs. How would you implement per-tenant backpressure, quota enforcement, and exactly-once vs at-least-once semantics under regional outages? Include a concrete canary and testing plan?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Oracle","Snowflake"]},{"id":"q-1471","question":"KCNA multi-tenant schema-evolution: design a zero-downtime migration for a KCNA-based event bus used by many tenants where the Event schema evolves from v1 to v2 (add region field, deprecate payload wrapper). How do you enforce backward/forward compatibility, isolate tenants during migration, and validate with canaries? Include tooling, rollback plans, and observability?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["DoorDash","Stripe","Twitter"]},{"id":"q-1503","question":"KCNA cross-region tenancy isolation: design a replication topology where tenants' streams stay regional unless opted into global analytics; implement per-tenant topic partitioning, region-aware routing, and idempotent retries with de-dup. How do you enforce locality, protect privacy in cross-region analytics, and handle failover/lag? Include concrete configs, testing strategy, and rollback plan?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Amazon","NVIDIA"]},{"id":"q-1520","question":"KCNA TTL Retention: design a per-topic TTL policy for KCNA events. How would you store TTL metadata, drop expired events without breaking consumer offsets, handle late-arriving events post-expiry, and observe/verify with canary tests? Provide a minimal API shape for setting TTL per topic, a compact storage layout, and a lightweight cleanup workflow?","channel":"kcna","subChannel":"general","difficulty":"beginner","tags":["kcna"],"companies":["Anthropic","Citadel","Tesla"]},{"id":"q-1602","question":"KCNA key management & tenant isolation: In a **KCNA**-based multi-tenant event bus, each tenant uses a per-tenant envelope encryption key managed by a centralized **KMS**. Design a **zero-downtime** key rotation workflow that rotates tenant keys without breaking consumption, re-encrypts in-flight payloads, and prevents cross-tenant leakage. Include data model (**tenant_id**, key_version, wrapped_key), rollout strategy, rollback, and observability?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Microsoft","Tesla"]},{"id":"q-1698","question":"Design a KCNA privacy-first feed where tenants specify a region (US/EU/APAC) and all data remains local. Use per-tenant envelope encryption with KMS, region-scoped brokers, field-level redaction before delivery, and tenant-aware access controls. Add audit trails and canary tests to prove zero cross-tenant leakage, correct redaction, and retention under burst load?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Anthropic","Tesla","Zoom"]},{"id":"q-1723","question":"KCNA dynamic tenancy fairness under bursty workloads: design a per-tenant ingestion path with token-bucket quotas and fair queuing; detail API surface, quota persistence, backpressure signaling, and dynamic rebalancing from telemetry. How do you validate isolation under burst traffic, and what would your canary rollout look like?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Databricks","NVIDIA"]},{"id":"q-1806","question":"KCNA privacy-by-design: design a tenant-isolated KCNA ingestion and delivery path that enforces per-tenant encryption keys for in-flight and at-rest data, supports on-the-fly key rotation with zero-downtime, and provides auditable access controls for analytic consumers. Describe the KMS integration, key-wrapping strategy, performance impact, and a minimal canary test for rotation?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Adobe","Robinhood","Slack"]},{"id":"q-1901","question":"KCNA Time-Travel Replay for Tenants: Suppose a tenant needs to audit a precise 2-hour window of events without halting live ingestion. Design a tenant-scoped time-travel replay feature in KCNA that allows replaying events from a given timestamp while live streams continue, guarantees exactly-once delivery to downstream analytics, and preserves per-tenant offsets. Describe API shapes, storage layout, consistency guarantees, security controls, and a minimal test plan (canaries)?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Anthropic","Meta","Snowflake"]},{"id":"q-1926","question":"Design a beginner-friendly KCNA feature: tenant-scoped data TTL. Each tenant can configure a TTL (e.g., 24h) for their streams. Describe the API (setTTL on a topic with ttlMs), how per-message metadata is stored, how a background purge runs safely without breaking consumers, and a minimal test plan with canaries?","channel":"kcna","subChannel":"general","difficulty":"beginner","tags":["kcna"],"companies":["LinkedIn","NVIDIA"]},{"id":"q-2025","question":"KCNA ingest fairness at scale: design a per-tenant fair-queuing strategy for bursty producers in a multi-tenant KCNA channel. Implement a two-layer approach with a per-tenant token-bucket at the gateway and a global weighted-round-robin scheduler across tenants to prevent starvation. Define quotas, bounded bursts, and backpressure signaling; outline API contracts, config knobs, and a test plan with synthetic tenants and canaries?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Databricks","Hugging Face","IBM"]},{"id":"q-2101","question":"KCNA policy-driven tenant isolation: design a per-tenant access-control mechanism at the gateway and stream processors that enforces tenant-scoped authorization for ingest and delivery, supports dynamic policy updates with zero-downtime rollout, and provides an auditable per-event trail. Outline the policy model (tenants, roles, actions), integration with a policy engine (e.g., OPA/ABAC), JWT-based identity, and testing strategy including canaries and rollback?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Instacart","NVIDIA","Stripe"]},{"id":"q-2135","question":"KCNA Dead-Letter & Poison Message Handling: For a multi-tenant KCNA deployment, design a per-tenant dead-letter queue strategy that routes malformed events out of the normal path, enforces per-tenant retry budgets, and preserves idempotent replays. Describe DLQ schema, routing rules, retention, operator visibility, and a safe rollout with canaries?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Citadel","Instacart","Meta"]},{"id":"q-2165","question":"KCNA observability & tenant-scoped tracing: design end-to-end observability for a multi-tenant KCNA event bus under burst traffic, preserving tenant data isolation while enabling operations debugging. Specify: how to propagate tenant_id and correlation_id across producer, gateway, and consumer; per-tenant metrics and alerting; privacy-preserving trace UI that exposes only metadata; retention, RBAC, and failure-mode testing with canaries?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Bloomberg","Goldman Sachs","Netflix"]},{"id":"q-941","question":"Scenario: A global chat platform with 2B MAUs must detect policy-violating content (spam, hate speech) in near real-time while preserving user privacy and multilingual support. Propose an end-to-end pipeline: ingestion, moderation models (rules + ML), latency SLOs (1-2s), privacy safeguards, backpressure handling, retries, and dead-letter queues. Compare on-device vs cloud inference and monitoring?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Slack","Twitter"]},{"id":"q-1021","question":"Design a globally distributed feature-flag engine for per-user rollout. Provide data model, consistency guarantees, and deployment strategy for scale across multiple regions. Include choice of storage (DynamoDB vs Redis), how regionOverrides and segments are merged, and hot-flip safety. Provide evaluation protocol and a concise pseudocode snippet for evaluateFeature(user, flag, context)?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Coinbase","Meta","Tesla"]},{"id":"q-1042","question":"Design a high-throughput, fault-tolerant event ingestion pipeline for a fraud-detection service. It must guarantee exactly-once processing, deduplicate events across shards, support backpressure, and provide end-to-end observability. Explain the data model for dedup IDs, queue choice (Kafka vs Kinesis), idempotent sinks, and how you'd test failure scenarios?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Meta","Plaid","Twitter"]},{"id":"q-1067","question":"In a Zoom-like multi-tenant Kubernetes cluster, you discover a pod running as root with a hostPath mounted. What concrete remediation plan would you implement using OPA Gatekeeper, Pod Security Standards, RBAC, NetworkPolicies, and image scanning? Include how you would verify tenant isolation with a minimal test that should fail if misconfigured?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Scale Ai","Zoom"]},{"id":"q-1096","question":"In a simple analytics microservice, implement a Python function that reads a CSV file with headers user_id, action, timestamp (timestamp optional) and returns a dict mapping each user_id to the list of unique actions performed in chronological order; if timestamp is missing, preserve input order. Explain your approach and trade-offs?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["OpenAI","Oracle","Twitter"]},{"id":"q-1129","question":"Given a Kubernetes-deployed event-processing service that suddenly drops throughput from 10k to 6k after a feature release, outline a concrete, testable plan to diagnose and restore throughput. Include: **metrics** to monitor, **bottlenecks** to consider, and concrete, incremental changes like **rate limiting**, **backpressure**, and **idempotency**. Provide a safe rollback strategy and canary plan?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Google","Scale Ai","Twitter"]},{"id":"q-1184","question":"You maintain a public API for ride estimates and expect bursts. Implement a per-API-key rate limiter that allows 60 requests per minute using an in-memory structure. Provide the core logic in JavaScript (Node.js) and briefly justify your data structure, test approach, and how you’d handle restart scenarios?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Discord","Lyft","Slack"]},{"id":"q-1227","question":"In a tiny model-usage service, write a Python function that validates a JSON payload with user_id, model_id, action, and timestamp, then inserts a document into MongoDB with a created_at field. Ensure an index exists on (model_id, timestamp) and handle duplicate submissions gracefully (idempotent using a unique composite index on (user_id, model_id, timestamp)). Include basic error handling and a minimal test snippet?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Apple","Hugging Face","MongoDB"]},{"id":"q-1246","question":"Design a policy for a shared Kubernetes cluster serving multiple teams, ensuring namespace isolation, least privilege RBAC, image provenance, Pod Security Standards, and auditable policies. Propose concrete constraints using RBAC, PSP/PSA, NetworkPolicy, and OPA Gatekeeper; include a sample ConstraintTemplate and a test that rejects nonconforming pods. Explain trade-offs and monitoring strategy?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Cloudflare","Google"]},{"id":"q-1298","question":"Design a globally-distributed rate limiter for a high-traffic service with per-tenant limits and cross-region fairness. Compare token-bucket and sliding-window approaches, specify data structures and caching, describe hot-path optimization, failure modes, and testing strategy. Assume latency budgets and real-time enforcement?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Amazon","LinkedIn","Meta"]},{"id":"q-1351","question":"In a Node.js Express API backed by PostgreSQL, POST /search accepts { term } and currently builds a SQL with string concatenation. Describe a secure rewrite using parameterized queries and input validation, including error handling and ensuring the DB user has minimal privileges. How would you structure this to prevent SQL injection?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Adobe","Salesforce","Tesla"]},{"id":"q-1680","question":"In a Kubernetes-deployed microservice that ingests up to 200 events/sec from a queue, each event has event_id and payload. The handler must be idempotent so a duplicate delivery does not write to Postgres. Propose a concrete Redis-based dedup strategy (SETNX with EXPIRE TTL) and outline how you'd implement the dedup path in code, including how you'd handle retries, restarts, and cleanup?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Instacart","NVIDIA","Netflix"]},{"id":"q-1788","question":"You're designing a payment authorization path for a fintech platform (think Plaid) where a mobile tap triggers multiple services: Auth, Fraud/Risk, Ledger, and Notification. How would you ensure idempotent processing, at-least-once retries, and eventual consistency across services? Describe data models, the flow (outbox or saga), and fault tolerance (backoffs, DLQ)?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Lyft","Plaid"]},{"id":"q-1828","question":"You operate a polyglot data stack with MongoDB and Oracle. A global e-commerce app experiences 300–500ms latency on order placement during peak hours, despite modest CPU usage. Propose an end-to-end plan to diagnose and fix, covering data model, indexing, shard/cluster topology, connection pooling, caching, and cross-database consistency. Include concrete knobs you would adjust and how you'd validate impact?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["MongoDB","Oracle"]},{"id":"q-1877","question":"In a multi-tenant data pipeline on Kubernetes serving a PayPal-like payments gateway, events flow from a single Kafka topic to a Spark streaming job that writes to a data lake. How would you enforce per-tenant data isolation, encryption, auditing, and masking while maintaining a 95th percentile latency under peak load?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Databricks","Google","PayPal"]},{"id":"q-1950","question":"You’re asked to implement a simple in-memory rate limiter for a Node.js/Express API: cap each API key at 100 requests per 10 minutes. Provide a minimal in-process solution using a per-key timestamp array, how you prune old entries, and how you handle bursts. Explain trade-offs and a plan to scale to multiple processes?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["DoorDash","Instacart","Meta"]},{"id":"q-2008","question":"In a cloud-native data platform delivering streaming telemetry to a data lake, how would you implement end-to-end data quality and schema drift control for Kafka → Flink → Parquet, when tenants frequently emit extra fields? Describe schema versioning, compatibility, drift detection, and automated remediation to downstream dashboards, with concrete knobs and testing steps?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Databricks","Google"]},{"id":"q-2085","question":"How would you implement compliant per-tenant data erasure in a streaming analytics pipeline that ingests tenant events from Kafka, writes to a data lake as Parquet, and serves BI queries, ensuring immutable data, auditability, and zero-downtime erasure while preserving peak-load latency? Include data-modeling, catalog updates, and operational steps?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Anthropic","Robinhood","Square"]},{"id":"q-2119","question":"In a frontend data pipeline, you receive two sorted arrays of strings representing tags. Write a function mergeUnique(a,b) that returns a new array with all unique elements in sorted order, without mutating inputs. Assume inputs are sorted. Provide a minimal, robust JS implementation and explain its time/space complexity?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Discord","Instacart"]},{"id":"q-836","question":"You are analyzing a web app's login events, captured as an array of objects: { userId: string, ts: string (ISO 8601) }. Write a JavaScript function that returns the top 3 users by login count in the past 24 hours. Tie-break with alphabetical userId. Provide a concise implementation and explain its time complexity?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Meta","NVIDIA"]},{"id":"q-869","question":"In a high-traffic search suggestions feature, implement a Node.js module that debounces input by 300ms and caches per-query results in memory. Show cache invalidation and handle concurrent requests for the same key without duplicating work. Provide a compact, testable example with usage?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Apple","Lyft","Zoom"]},{"id":"q-912","question":"**KCSA Beginner Question: Inventory Pagination** Scenario: In a warehouse app, design a minimal endpoint GET /items?limit=&offset= that returns items ordered by id, supports pagination, and scales with 10k+ rows. Explain data model, indexing, and error handling. How would you implement a function to validate and apply LIMIT/OFFSET in code, ensuring correctness?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Amazon","Apple"]},{"id":"q-947","question":"You're building a fintech API used by wallets and partner services to transfer funds; describe a secure, scalable auth and data-protection design for REST endpoints, including per-partner API keys, short-lived access tokens with rotating signing keys, mTLS between services, replay protection, and observability to detect abuse, plus failure modes and trade-offs?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Coinbase","PayPal","Plaid"]},{"id":"gh-56","question":"What are the key deployment strategies in Kubernetes, and how do you configure them considering resource limits, health checks, and rollback scenarios?","channel":"kubernetes","subChannel":"deployments","difficulty":"intermediate","tags":["automation","tools"],"companies":["Amazon","Google","Hashicorp","Microsoft","Netflix","Salesforce"]},{"id":"gh-7","question":"What is Kubernetes and how does it orchestrate containerized applications at scale?","channel":"kubernetes","subChannel":"deployments","difficulty":"beginner","tags":["k8s","orchestration"],"companies":["Airbnb","Amazon","Google","Microsoft","Uber"]},{"id":"gh-8","question":"Design a highly available Kubernetes cluster architecture. What are the main components, their interactions, and how do you ensure 99.95% uptime across multiple availability zones?","channel":"kubernetes","subChannel":"deployments","difficulty":"intermediate","tags":["k8s","orchestration"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-306","question":"How would you implement a canary deployment strategy in Kubernetes to minimize risk during application updates?","channel":"kubernetes","subChannel":"deployments","difficulty":"advanced","tags":["rolling-update","canary","blue-green"],"companies":["Amazon","Google","Meta"]},{"id":"q-334","question":"You're deploying a new version of a microservice in Kubernetes. Describe how you would perform a rolling update and what would you do if the new version starts failing health checks?","channel":"kubernetes","subChannel":"deployments","difficulty":"beginner","tags":["rolling-update","canary","blue-green"],"companies":["Amazon","Cisco","Google","Microsoft","Netflix","New Relic","Stripe"]},{"id":"q-369","question":"You're deploying a critical video processing service at Zoom. During a rolling update, 30% of users experience degraded performance while the new version is being deployed. How would you diagnose and resolve this issue, and what deployment strategy would you recommend instead?","channel":"kubernetes","subChannel":"deployments","difficulty":"intermediate","tags":["rolling-update","canary","blue-green"],"companies":["MongoDB","New Relic","Zoom"]},{"id":"q-465","question":"You're running a production Kubernetes cluster and notice pods are frequently getting OOMKilled despite having sufficient memory limits. How would you diagnose and resolve this issue?","channel":"kubernetes","subChannel":"general","difficulty":"intermediate","tags":["kubernetes"],"companies":["Adobe","Meta","Square"]},{"id":"q-526","question":"You're running a production Kubernetes cluster with 1000+ pods. Your monitoring shows that certain nodes are experiencing high memory pressure, causing pod evictions. How would you diagnose and resolve this issue systematically?","channel":"kubernetes","subChannel":"general","difficulty":"advanced","tags":["kubernetes"],"companies":["Bloomberg","Snap","Two Sigma"]},{"id":"q-552","question":"You're running a production Kubernetes cluster at scale and notice that some pods are experiencing intermittent network timeouts. How would you diagnose and resolve this issue, considering both application-level and cluster-level networking components?","channel":"kubernetes","subChannel":"general","difficulty":"advanced","tags":["kubernetes"],"companies":["Citadel","NVIDIA","Tesla"]},{"id":"q-579","question":"How would you debug a pod that's stuck in CrashLoopBackOff state in a production Kubernetes cluster?","channel":"kubernetes","subChannel":"general","difficulty":"intermediate","tags":["kubernetes"],"companies":["Cloudflare","Tesla"]},{"id":"de-135","question":"You have a Helm chart that needs to deploy different configurations for staging and production environments. The staging environment should use 2 replicas with 512Mi memory limit, while production should use 5 replicas with 2Gi memory limit. How would you structure your values files and templates to handle this requirement?","channel":"kubernetes","subChannel":"helm","difficulty":"intermediate","tags":["helm","k8s"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-49","question":"How does Helm manage Kubernetes application lifecycle through charts, releases, and templates?","channel":"kubernetes","subChannel":"helm","difficulty":"advanced","tags":["k8s","advanced"],"companies":["Amazon","Apple","Google","Hashicorp","Microsoft","Netflix"]},{"id":"q-400","question":"You're deploying a microservice using Helm and notice that the pod keeps crashing with 'ImagePullBackOff' error. The values.yaml specifies 'image.repository: my-service' and 'image.tag: latest'. How would you debug this issue and what's the proper way to configure image pull policies in production?","channel":"kubernetes","subChannel":"helm","difficulty":"intermediate","tags":["charts","values","templating"],"companies":["Adobe","Amazon","Google","Hashicorp","Jane Street","Microsoft","Netflix","Snowflake"]},{"id":"gh-55","question":"How does Tekton provide a cloud-native framework for building CI/CD pipelines on Kubernetes?","channel":"kubernetes","subChannel":"operators","difficulty":"beginner","tags":["automation","tools"],"companies":["Digitalocean","Google","IBM","Microsoft","Red Hat"]},{"id":"q-193","question":"What is the role of a Custom Resource Definition (CRD) in a Kubernetes Operator and how does it enable custom functionality?","channel":"kubernetes","subChannel":"operators","difficulty":"beginner","tags":["crds","controllers","reconciliation"],"companies":["Amazon","Datadog","Google","Microsoft","Prove"]},{"id":"q-291","question":"What is the role of a reconciliation loop in a Kubernetes operator controller?","channel":"kubernetes","subChannel":"operators","difficulty":"beginner","tags":["crds","controllers","reconciliation"],"companies":["Amazon","Google","Meta"]},{"id":"q-346","question":"You're building a Kubernetes operator for a custom resource that manages a fleet of microservices. Your controller is experiencing high memory usage and slow reconciliation loops. How would you design a solution to handle 10,000+ custom resources efficiently while ensuring proper event handling and preventing resource leaks?","channel":"kubernetes","subChannel":"operators","difficulty":"advanced","tags":["crds","controllers","reconciliation"],"companies":["Amazon","Cloudflare","Gitlab","Google","Hashicorp","Microsoft","MongoDB","Workday"]},{"id":"q-383","question":"You're building a Kubernetes operator for a custom database CRD. During reconciliation, you notice the controller is constantly updating the status even when no actual changes occur. How would you implement proper change detection and prevent unnecessary updates?","channel":"kubernetes","subChannel":"operators","difficulty":"intermediate","tags":["crds","controllers","reconciliation"],"companies":["AMD","Google","OpenAI"]},{"id":"gh-100","question":"What is a Sidecar Pattern in Kubernetes?","channel":"kubernetes","subChannel":"pods","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-48","question":"Explain how DaemonSets ensure pod distribution across Kubernetes nodes and describe the controller reconciliation loop that maintains this guarantee?","channel":"kubernetes","subChannel":"pods","difficulty":"advanced","tags":["k8s","advanced"],"companies":["Amazon","Google","Hashicorp","Microsoft","Netflix","Snowflake"]},{"id":"gh-51","question":"What is Container Runtime Interface (CRI) and why is it important in Kubernetes?","channel":"kubernetes","subChannel":"pods","difficulty":"advanced","tags":["k8s","advanced"],"companies":["Amazon","Google","Microsoft"]},{"id":"gh-9","question":"What is a Pod in Kubernetes and why is it considered the smallest deployable unit?","channel":"kubernetes","subChannel":"pods","difficulty":"beginner","tags":["k8s","orchestration"],"companies":["Amazon","Google","LinkedIn","Microsoft","Uber"]},{"id":"q-173","question":"What is a Kubernetes Pod and what is its primary purpose?","channel":"kubernetes","subChannel":"pods","difficulty":"beginner","tags":["pods","containers"],"companies":null},{"id":"q-245","question":"How do init containers differ from sidecar containers in Kubernetes pod lifecycle and resource sharing patterns?","channel":"kubernetes","subChannel":"pods","difficulty":"beginner","tags":["containers","init-containers","sidecars"],"companies":["Amazon","Databricks","Google","Microsoft","Netflix","Stripe"]},{"id":"q-271","question":"Design a zero-downtime database migration system using Kubernetes multi-container pods with init containers, sidecars, and shared volumes. How would you handle schema validation, migration execution, rollback, and coordination while maintaining service availability?","channel":"kubernetes","subChannel":"pods","difficulty":"intermediate","tags":["containers","init-containers","sidecars"],"companies":["Amazon","Databricks","Google","Microsoft","Netflix","Stripe"]},{"id":"q-356","question":"You're deploying a security scanning sidecar with a main application pod. The sidecar must complete its vulnerability scan before the main container starts, then continue monitoring runtime threats. Design this pod configuration with shared volumes, health checks, and graceful shutdown. What key components ensure the security scanning completes before application startup?","channel":"kubernetes","subChannel":"pods","difficulty":"intermediate","tags":["containers","init-containers","sidecars"],"companies":null},{"id":"q-412","question":"You're deploying a web application that needs to run database migrations before the main container starts. How would you configure a Pod with an init container to handle this, and what happens if the init container fails?","channel":"kubernetes","subChannel":"pods","difficulty":"beginner","tags":["containers","init-containers","sidecars"],"companies":["Figma","NVIDIA","Okta"]},{"id":"q-511","question":"How does Kubernetes handle pod scheduling and what factors influence scheduling decisions?","channel":"kubernetes","subChannel":"pods","difficulty":"intermediate","tags":["kubernetes","scheduling","pods","resource-management","container-orchestration"],"companies":["Amazon","Google","Microsoft","Red Hat"]},{"id":"q-636","question":"What are init containers in Kubernetes and how do they differ from regular containers?","channel":"kubernetes","subChannel":"pods","difficulty":"intermediate","tags":["kubernetes","containers","pod-lifecycle","initialization"],"companies":["Amazon","Google","Microsoft"]},{"id":"q-637","question":"What are the key differences between init containers, sidecar containers, and static pods in Kubernetes?","channel":"kubernetes","subChannel":"pods","difficulty":"intermediate","tags":["kubernetes","containers","pod-lifecycle","deployment-patterns"],"companies":["Amazon","Google","Microsoft","Red Hat"]},{"id":"gh-101","question":"What is a Service Mesh Control Plane and how does it manage microservices communication?","channel":"kubernetes","subChannel":"services","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Salesforce","Uber"]},{"id":"gh-50","question":"How does Istio implement service mesh architecture using sidecar proxies, and what are the key components for traffic management, security, and observability?","channel":"kubernetes","subChannel":"services","difficulty":"advanced","tags":["k8s","advanced"],"companies":null},{"id":"q-219","question":"How would you design a zero-downtime service migration strategy using Kubernetes Service selectors and Endpoints controller to avoid connection drops during rolling updates?","channel":"kubernetes","subChannel":"services","difficulty":"advanced","tags":["clusterip","nodeport","loadbalancer","ingress"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-320","question":"You have a microservice deployed in Kubernetes that needs to be accessible both internally within the cluster and externally via a custom domain. How would you configure the service and ingress to achieve this, and what are the trade-offs between using ClusterIP, NodePort, and LoadBalancer service types?","channel":"kubernetes","subChannel":"services","difficulty":"advanced","tags":["clusterip","nodeport","loadbalancer","ingress"],"companies":["Elastic","Snowflake","Zoom"]},{"id":"gh-26","question":"What are the essential Linux commands every DevOps engineer should master for system administration, troubleshooting, and automation?","channel":"linux","subChannel":"commands","difficulty":"beginner","tags":["linux","shell"],"companies":["Amazon","Cloudflare","Google","Hashicorp","Microsoft","Netflix"]},{"id":"q-1000","question":"A Linux host runs several Docker containers; during peak load, API responses slow and some requests time out. Describe a beginner-friendly, concrete diagnostic workflow to (1) confirm whether host saturation is CPU, memory, or I/O, (2) identify the container most responsible, and (3) apply a safe mitigation (e.g., graceful restart or CPU/memory throttling) while monitoring impact. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Instacart","LinkedIn"]},{"id":"q-1142","question":"Scenario: A Linux server hosting a small web service used by customers suddenly shows disk usage climbing on the root filesystem. Provide a beginner-friendly, concrete diagnostic workflow to (1) locate the directories/files consuming the most space, (2) identify top offenders, and (3) apply a safe mitigation (e.g., rotate/compress/archive logs or purge old data) while keeping the service up. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["DoorDash","Google","Salesforce"]},{"id":"q-1205","question":"You're operating a fleet of Linux hosts running a low-latency web API. Intermittent 100–200 ms latency spikes appear under load. Design a practical, end-to-end diagnostic plan using eBPF/BPFtrace or perf, iostat/vmstat, and container metrics to collect data, identify root cause (CPU scheduling, IO wait, or network), and propose minimal-disruption mitigations?","channel":"linux","subChannel":"general","difficulty":"advanced","tags":["linux"],"companies":["Salesforce","Snap"]},{"id":"q-1244","question":"Scenario: A Linux host runs a MongoDB primary in a high-traffic cluster. During peak hours, latency spikes and tail latency increases while CPU and memory appear stable. Using only default tooling, no downtime, describe a concrete, actionable diagnostic workflow to (1) determine if I/O wait, CPU, or memory is the bottleneck, (2) pinpoint the offending disk/device or process, and (3) apply a safe mitigation (e.g., adjust I/O scheduler, tune dirty writeback parameters, or throttle MongoDB) while monitoring impact. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Adobe","Databricks","MongoDB"]},{"id":"q-1266","question":"Context: Linux node in a Kubernetes cluster hosting a high-throughput data ingestion service. Intermittent tail latency spikes (>1s) appear during peak traffic, affecting processing. Without downtime, design a concrete troubleshooting workflow to (1) confirm whether CPU, I/O, or network is the bottleneck, (2) identify the exact subsystem or process responsible, and (3) implement a safe mitigation with minimal impact while maintaining observability. Include exact commands and realistic outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Airbnb","Anthropic","Snowflake"]},{"id":"q-1364","question":"A Linux host running multiple microservices behind NGINX exhibits intermittent latency spikes; new connections occasionally fail and FD usage appears high. Using only default tools, describe a beginner-friendly workflow to (1) confirm FD exhaustion is the bottleneck, (2) identify the offending process by per‑process FD usage, and (3) apply a safe mitigation (increase NOFILE limits, adjust per-service limits, or set a systemd limit) while monitoring impact. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Hugging Face","PayPal","Tesla"]},{"id":"q-1410","question":"On a Linux host, a data-processing job occasionally stalls during bursts. Using only default tooling and no downtime, outline a concrete, beginner-friendly diagnostic flow to (1) confirm CPU, memory, or I/O bound, (2) identify the offending process and its operation, and (3) apply a safe mitigation (e.g., adjust priority, pause/resume, or throttle) with monitoring. Which commands would you run and what outputs would you expect?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Instacart","OpenAI","Square"]},{"id":"q-1424","question":"Scenario: a high-traffic Linux service behind a reverse proxy experiences sporadic tail latency spikes during peak hours. CPU and IO look normal in aggregate. Design a practical end-to-end diagnostic using eBPF to identify root causes quickly: (1) instrument per-request latency across kernel and user-space, (2) attribute latency to network, disk, or app code, (3) propose safe mitigations and validate impact. Include specific probes, sample commands, and expected outputs?","channel":"linux","subChannel":"general","difficulty":"advanced","tags":["linux"],"companies":["Citadel","Cloudflare","Twitter"]},{"id":"q-1445","question":"A Linux host running GPU-accelerated video processing containers shows intermittent frame latency spikes during peak load, with no obvious CPU/memory/I/O bottlenecks. Without downtime, describe a concrete, real-world diagnostic workflow using only default tools to (1) verify if latency is caused by page cache pressure or Transparent Huge Pages, (2) identify the subsystem or container involved, and (3) apply a safe mitigation (e.g., disable THP on affected NUMA nodes, tune swappiness, or pin tasks) with minimal impact and observable results. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Amazon","DoorDash","NVIDIA"]},{"id":"q-1570","question":"A Linux CI node running multiple git builds experiences occasional stalls during parallel jobs. Using only default tooling and no downtime, describe a concrete diagnostic workflow to (1) determine if CPU, memory, or I/O is the bottleneck, (2) identify the specific component (e.g., git, filesystem, network) causing the stall, and (3) apply a safe mitigation (e.g., throttle parallel jobs, adjust I/O scheduler, or raise file descriptor limits) while monitoring impact. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["IBM","Tesla","Uber"]},{"id":"q-1705","question":"You have a Linux host running a Rust-based data-processing daemon that stalls for 30–60 seconds under peak load. Using only default tooling, design a concrete diagnostic workflow to (1) determine if the stall is CPU-, I/O-, or memory-bound, (2) identify the exact subsystem or process causing the stall, and (3) apply a safe mitigation (e.g., cgroup throttling or IO-scheduler tweaks) while preserving observability. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Google","Hugging Face","Lyft"]},{"id":"q-1873","question":"Context: A Linux host runs a Kubernetes-deployed real-time inference service behind Nginx; latency tail spikes occur during bursts. Without downtime, describe a concrete, repeatable workflow to determine whether latency is caused by CPU throttling, memory pressure, or network queueing, identify the offending container/pod, and apply a safe mitigation (e.g., adjust cgroup limits, scale the deployment, or tune kernel parameters) while preserving observability. Include exact commands, expected outputs, and a simple rollback plan?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Airbnb","Hugging Face","Snap"]},{"id":"q-2038","question":"Scenario: A Linux host running a batch queue occasionally fails to start new jobs with 'Too many open files' under moderate load. Without downtime, describe a concrete, beginner-friendly diagnostic workflow using default tools to (1) confirm FD limits are the bottleneck, (2) identify the process or user hitting the limit, and (3) apply a safe mitigation (e.g., raise per-user limits, adjust LimitNOFILE for the service) while keeping service available. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Amazon","Goldman Sachs","Salesforce"]},{"id":"q-466","question":"You're debugging a production Linux server where processes are randomly dying with 'Out of memory' errors, but `free -m` shows 8GB available RAM. How would you diagnose and fix this issue?","channel":"linux","subChannel":"general","difficulty":"advanced","tags":["linux"],"companies":["Amazon","Google","Tesla"]},{"id":"q-496","question":"How would you find all processes running on port 8080 and terminate them safely?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Apple","Google"]},{"id":"q-527","question":"How would you find and kill a process that's using port 8080 on a Linux system?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Amazon","Oracle","Two Sigma"]},{"id":"q-553","question":"You're troubleshooting a production server where a critical process keeps getting killed. How would you diagnose if it's an OOM kill versus other issues, and what specific commands would you use to investigate?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Databricks","Scale Ai","Snowflake"]},{"id":"q-580","question":"How would you find all processes using a specific port and terminate one safely?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Hashicorp","MongoDB","Twitter"]},{"id":"q-917","question":"Scenario: a Linux server hosting a web app experiences sporadic high response times during peak hours. Using only default tools and no downtime, describe a concrete, beginner-friendly diagnostic workflow to (1) determine whether CPU, memory, or I/O is the bottleneck, (2) identify the offending process, and (3) apply a safe mitigation (e.g., graceful restart) while monitoring impact. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Microsoft","PayPal","Tesla"]},{"id":"q-1044","question":"On a Linux host running multiple tenant services, peak I/O from a data ingestion daemon saturates the disk, causing latency spikes for others. Propose a production plan to isolate and throttle disk I/O across tenants using systemd slices and cgroup v2 io.max. Include concrete unit and slice snippets, per-device limits for a SATA HDD and NVMe, and a test plan using fio and iostat to verify tail latency improvements under concurrent workloads?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Databricks","Microsoft","Twitter"]},{"id":"q-1056","question":"On a Linux host, a TLS proxy reads its certificate from /etc/ssl/certs/app.crt and currently reloads by restarting, causing brief downtime during renewal. Propose a zero-downtime rotation using a systemd.path trigger that fires on a new cert symlink, with a separate service unit for ExecReload and an atomic update scheme (swap in a new cert, then switch a symlink). Include concrete unit snippets and a test plan?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Apple","Two Sigma"]},{"id":"q-1122","question":"On a Linux host running multiple ML model servers in separate systemd slices using cgroup v2, a spike in one tenant consumes memory and triggers host memory pressure despite per-tenant limits. Propose a production plan to enforce strict isolation and backpressure using memory.max and memory.high, enable group OOM behavior, and implement eviction/playback strategies. Include concrete unit and cgroup settings and a test plan with a reproducible spike and validation steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Hugging Face","MongoDB","Tesla"]},{"id":"q-1230","question":"On a Linux host with a multi-tenant workload sharing a single 10GbE NIC configured with multiple receive queues, one tenant bursts UDP traffic and starves others, causing increased latency and packet loss. Propose a production plan to enforce tenant isolation and fairness using NIC multi-queue, RSS/XPS mapping, IRQ affinity, and per-tenant cgroups (v2) with io.max and tc shaping. Include concrete steps and a test plan with realistic traffic?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["DoorDash","MongoDB","Twitter"]},{"id":"q-1327","question":"On a Linux host running multiple tenants via containers, a CPU-intensive workload from one tenant starves others. Propose a production plan to enforce CPU isolation using cgroup v2 slices, systemd integration, and per-tenant quotas; include concrete settings (cpu.max, cpu.weight), scheduling options (cpuset and isolcpus), and a test plan with realistic workloads and verification steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Bloomberg","Google","IBM"]},{"id":"q-1346","question":"On a production Linux host serving multiple ephemeral monitoring daemons, a heartbeat worker must emit a 5-second ping to a central collector. During peak load, the heartbeat misses several intervals, delaying alerts. Propose a concrete production plan to guarantee cadence and resilience using: (a) per-service CPU isolation and pinning, (b) high-priority timer/RT scheduling or systemd timers with precise tuning, (c) a fallback mechanism for missed heartbeats, and (d) a validation strategy with a controlled load. Include concrete config snippets for systemd, cpuset, and timer settings?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Citadel","LinkedIn"]},{"id":"q-1385","question":"On a Linux host running ephemeral build jobs managed by systemd, a rogue job floods /tmp with tiny files, exhausting inodes and delaying builds. Propose a production plan to isolate and bound /tmp usage and auto-clean stale files. Include: (a) per-service PrivateTmp and tmpfs /tmp with a size cap; (b) systemd-tmpfiles cleanup rules; (c) inode usage monitoring; (d) a reproducible test that proves isolation and cleanup. How would you implement this?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Anthropic","Coinbase","MongoDB"]},{"id":"q-1490","question":"On a Linux server, a small web app runs as a systemd service and reads its port and log level from /etc/myapp/config.yaml. Changes to this file must apply without dropping connections. Design a production-safe hot-reload mechanism using systemd ExecReload, a SIGHUP-based reload, and a small validation script. Include unit file snippets and a test plan with a reproducible config change and verification steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["DoorDash","Google"]},{"id":"q-1497","question":"On a Linux host running multiple tenants in systemd services sharing a single NVMe SSD and a 25Gbps NIC, a new requirement enforces strict I/O isolation to prevent a noisy tenant from stalling others. Propose a production plan to enforce per-tenant I/O isolation and backpressure using: (a) cgroup v2 io.max per service; (b) per-tenant filesystem isolation via PrivateMounts and PrivateDevices with a dedicated data path; (c) I/O scheduler tuning (none/deadline) and per-tenant block IO queuing discipline with tc; (d) a deterministic test plan with a reproducible burst scenario and rollback steps. Include concrete config snippets for systemd units and cgroups?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Discord","MongoDB"]},{"id":"q-1588","question":"On a dual-socket Linux host running PostgreSQL and a data-processor ETL job, both services contend for memory and cache across NUMA nodes; design an advanced plan to enforce NUMA-aware isolation with cpuset/memcg, disable NUMA balancing, set per-node IRQ affinity, and validate via targeted benchmarks. Include concrete commands and a test plan?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Coinbase","Twitter"]},{"id":"q-1665","question":"On a Linux server hosting a latency-sensitive database and a heavy batch analytics job that share a single 2-socket NUMA node and HDD I/O, propose a production plan to guarantee tail latency (P95/P99) for the database while allowing analytics to finish within a bounded window. Include NUMA memory binding, CPU pinning via cpuset/systemd slices, I/O throttling via cgroup v2 io.max, IRQ affinity, and a validation test plan with realistic workloads?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Apple","Snap"]},{"id":"q-1700","question":"On a Linux server running several io_uring-based data ingestors, one worker's backlog bursts, triggering tail latency spikes for others. Propose a production plan to guarantee per-worker latency and fairness using: (a) per-worker io_uring ring sizing and SQ depth; (b) per-worker cgroups v2 with io.max and task limits; (c) backpressure and pacing (budget tokens, per-worker deadlines); (d) monitoring and a reproducible test plan to validate latency under burst?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Apple","Snowflake"]},{"id":"q-1774","question":"On a Linux host, a file-watcher daemon uses inotify to monitor a large directory tree. As load grows, events start being missed and the daemon lags. Propose a production plan to keep event delivery reliable: tune inotify limits, raise file descriptor limits for the service, add a polling fallback or batching, and validate with a reproducible test that simulates churn?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Instacart","Oracle","Snowflake"]},{"id":"q-1807","question":"On a Linux host running a hot-reload deployment workflow for a critical service, mid-deploy tampering could swap the binary between copy and start, breaking integrity. Design a production-grade binary integrity strategy using Linux IMA with runtime attestation. Include how to sign binaries in CI, a signed binary repository, systemd integration (ExecStartPre), policy placement, rollback tests, and how you'd validate against tampering during deployment?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Coinbase","MongoDB"]},{"id":"q-1939","question":"On a Linux host running a web stack (Nginx proxy to a Node/Go app), the app writes to /var/log/webapp/app.log and /var/log/webapp/access.log. Bursts fill the disk and logrotation sometimes fails because the app keeps logs open. Propose a production-grade plan to ensure reliable log rotation with no data loss and no disk-full events. Include concrete logrotate config (copytruncate vs postrotate), systemd unit tweaks (Restart, ExecReload), filesystem layout advice, and a test plan to reproduce a burst and verify rotation completes without downtime?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Cloudflare","Stripe","Uber"]},{"id":"q-1954","question":"On a Linux host running three CI runners as systemd services sharing a single NVMe and a 10 Gb NIC, a long-running build on one runner starves CPU and I/O, delaying others. Propose a production plan to guarantee fair CPU and I/O while preserving peak throughput: (a) per-service CPU limits via cgroup v2 and CPU affinity; (b) per-service I/O throttling with io.max and a suitable I/O scheduler (BFQ/mq-deadline); (c) CPU pinning and IRQ isolation; (d) validation with bursts and latency metrics?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Anthropic","Apple","Databricks"]},{"id":"q-1987","question":"On a Linux host running CI jobs in rootless containers (e.g., Podman) shared across teams, a misconfigured container could attempt host escape. Propose a production hardening plan using user namespaces, rootless mode, Seccomp and AppArmor, and a tight per-job capset + cgroupv2 quotas. Include concrete commands and a rollback plan?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["IBM","Square","Twitter"]},{"id":"q-2014","question":"On a Linux host running multiple GPU-accelerated services in separate systemd slices under containerized workloads, occasional CPU contention causes tail latency spikes and timeouts for one service. Design a production plan to bound latency and preserve throughput without service interruption. Include per-slice isolation (isolcpus, cpuset), systemd slice configuration, cpu.max, and a kernel I/O scheduler tuning (mq-deadline), plus a reproducible test plan with mixed workloads and latency verification steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Adobe","Instacart","OpenAI"]},{"id":"q-2167","question":"On a KVM host running dozens of VMs with memory ballooning enabled, a burst in guest memory growth triggers host memory pressure and occasional OOM events. Propose a production-safe plan to cap balloon aggressiveness, enforce host backpressure, and prevent thrash, using libvirt XML (memory.size/currentMemory, memoryBacking, balloon device) and cgroup v2 memory.max/memory.high, plus a test plan with a reproducible spike and verification steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Google","Instacart","Snap"]},{"id":"q-881","question":"On a Linux host, a long-running daemon writes to `/var/log/myapp.log` and is managed by systemd, but log rotation occasionally causes logging to stop after rotation. Propose a practical fix to ensure logging continues after rotation without restarting the daemon. Include the exact approach and a sample logrotate config snippet and testing steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Apple","Meta","Plaid"]},{"id":"q-888","question":"A production Linux host runs a data-backup agent that uses /var/lib/backup/backup.lock to enforce a single instance. Sometimes a stale lock remains after a crash, blocking new runs; the agent also leaves non-terminating children on stop, risking partial backups. Propose a systemd‑based lifecycle fix: ensure one instance, auto-clean stale lock, and graceful stop with timeout and fallback to kill. Include concrete unit snippets and verification steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Goldman Sachs","Snowflake"]},{"id":"q-961","question":"On a Linux host, a memory-hungry log-harvester daemon managed by systemd sporadically triggers the kernel OOM killer during peak load, crashing the service and delaying alerts. Propose a production-safe plan to prevent OOM termination while preserving throughput. Include concrete systemd settings (MemoryLimit, MemorySwapMax, OOMScoreAdjust, Restart), kernel tuning (swap, swappiness), and a test plan with a reproducible high-memory scenario and verification steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Databricks","Two Sigma"]},{"id":"linux-foundation-sysadmin-networking-1768260262823-4","question":"You suspect ARP storms on a host with interface eth1. To quickly verify ARP traffic and identify abnormal ARP activity on that interface, which command should you run?","channel":"linux-foundation-sysadmin","subChannel":"networking","difficulty":"intermediate","tags":["Networking","ARP","Linux","Kubernetes","certification-mcq","domain-weight-12"],"companies":null},{"id":"q-199","question":"When deploying LLM inference with vLLM and Triton Inference Server, how do you handle request batching across multiple GPU nodes while maintaining sub-100ms latency for individual requests?","channel":"llm-ops","subChannel":"deployment","difficulty":"advanced","tags":["vllm","tgi","triton","onnx"],"companies":["Amazon","Google","Meta","Microsoft","NVIDIA"]},{"id":"q-1045","question":"You're running a beginner LLM inference API used by multiple clients. Design a low-cost, safe plan to implement per-tenant model versioning and zero-downtime hot-swapping. Include routing to the correct version, how you deploy new versions, rollback strategy, and observability to verify no regressions before full rollout?","channel":"llm-ops","subChannel":"general","difficulty":"beginner","tags":["llm-ops"],"companies":["Discord","NVIDIA"]},{"id":"q-1060","question":"You're delivering a privacy-preserving, regionally scoped, multi-tenant LLM assistant used by teams across geographies. Describe an end-to-end design that ensures tenant isolation, data residency, and per-tenant policy enforcement while meeting sub-200ms latency for common prompts. Include routing, key management, redaction, auditability, and deletion workflows, plus how you'd validate compliance across regions?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["LinkedIn","NVIDIA","Zoom"]},{"id":"q-1147","question":"You're operating a multi-tenant, multi-region LLM inference service for regulated financial firms. Design an architecture that enforces per-tenant data residency and per-tenant model pools, plus dynamic model-version rollout with feature flags and safe rollback. Describe policy-driven routing, on-the-fly prompt sanitization, observability, and incident response. Include concrete data-plane components and a rollout sequence for a new model version?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Apple","Goldman Sachs","Plaid"]},{"id":"q-1263","question":"You operate a global LLM inference service across three regions and must upgrade models with zero downtime. Describe a concrete plan for rolling upgrades with canaries, traffic-splitting, warm-up, health checks, and fast rollback, ensuring SLA adherence and minimal cold-start impact during the switch?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["Databricks","Slack","Snap"]},{"id":"q-1640","question":"You're launching a beginner-friendly LLM chat in a mobile-first app. To balance latency and privacy, design an edge-first routing: small prompts stay on-device; larger or sensitive prompts go to the cloud. Describe concrete components, a simple policy rule, data flow, and how you'd test it for privacy and latency before release?","channel":"llm-ops","subChannel":"general","difficulty":"beginner","tags":["llm-ops"],"companies":["Amazon","Apple","Snap"]},{"id":"q-1695","question":"Design a compliant, auditable LLM inference pipeline for a regulated financial platform that must deliver per-transaction data lineage, prompt provenance, and immutable end-to-end logging with 24-month retention while keeping latency under 150 ms end-to-end. Describe data flows, encryption, storage, and how you test for data leakage and drift?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Airbnb","Oracle","PayPal"]},{"id":"q-1858","question":"Design a beginner-friendly chat moderation assistant for a live-stream app with edge-first routing: short prompts stay on-device; longer or risk-prone prompts go to cloud. Specify the minimal architecture, a concrete routing rule (token threshold and risk score), data flow with redaction, and a practical test plan to validate latency (<200ms on-device, <600ms cloud) and privacy?","channel":"llm-ops","subChannel":"general","difficulty":"beginner","tags":["llm-ops"],"companies":["Cloudflare","Databricks"]},{"id":"q-2003","question":"Design a tenancy-aware LLM inference layer where 60 regional tenants share a single GPU pool but must enforce per-tenant data isolation, prompt sanitization, and per-tenant model versioning. When burst traffic occurs, guarantee 95th percentile latency under 300 ms while preventing cross-tenant data leakage. Describe data flows, policy evaluation, routing, and rollback?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["Amazon","Google","IBM"]},{"id":"q-2146","question":"Design a hybrid edge-cloud LLM inference system where tenants move between corporate networks and remote locations. How would you enforce per-tenant data locality, latency budgets, model versioning, and graceful failover while keeping observability clear and simple?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["Discord","Google","Snap"]},{"id":"q-467","question":"You're deploying a LLM inference service that must handle 10,000 RPS with <100ms latency. How would you design the architecture to balance cost, performance, and reliability?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["Discord","Microsoft","Netflix"]},{"id":"q-497","question":"How would you design a distributed inference serving system for LLMs that handles 100K RPS with sub-100ms latency while managing GPU memory fragmentation and ensuring high availability?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Hugging Face","Tesla"]},{"id":"q-528","question":"You're deploying a LLM inference service that must handle 1000 concurrent requests with <500ms latency. Your current setup uses a single GPU with vLLM. How would you architect the system to meet these requirements?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["Airbnb","Hugging Face","Robinhood"]},{"id":"q-554","question":"You're deploying a multi-tenant LLM inference service that must handle 10,000 concurrent requests with sub-100ms latency. How would you design the request routing, model loading strategy, and autoscaling to meet these SLAs while optimizing GPU utilization?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["DoorDash","IBM"]},{"id":"q-581","question":"How would you design a production LLM inference pipeline that handles 10K RPS with sub-200ms latency while managing GPU memory fragmentation and cold start issues?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Airbnb","Google"]},{"id":"q-849","question":"You operate a dual-tenant LLM inference service for sensitive internal docs and external users. Design a policy-driven routing and isolation architecture that guarantees tenant data separation, per-tenant model pools, on-the-fly prompt sanitization, and strict latency budgets under burst traffic. Include observability, data handling, and fail-open/closed strategies?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Cloudflare","Google","Instacart"]},{"id":"q-252","question":"What are the key techniques and trade-offs for optimizing large language models in production, including quantization strategies and their impact on performance?","channel":"llm-ops","subChannel":"optimization","difficulty":"beginner","tags":["quantization","pruning","distillation"],"companies":["Amazon","Apple","Google","Meta","Microsoft","NVIDIA"]},{"id":"q-422","question":"You're building a production LLM service handling 10K requests/second with transformer models experiencing memory spikes during multi-head attention. How would you optimize memory usage while maintaining throughput and latency requirements?","channel":"llm-ops","subChannel":"optimization","difficulty":"advanced","tags":["transformer","attention","tokenization"],"companies":null},{"id":"q-1177","question":"Scenario: a high-throughput edge service must enforce precise timeouts for thousands of connections. Design a lock-free, per-core timer wheel that manages up to 1,000,000 timers with microsecond granularity on a 4-socket server. API: add_timer(id, due_us, cb, ctx), cancel_timer(id), tick(). Requirements: no global locks, handle cancellation safely, cache-friendly layout, and crash-safe recovery. Include pseudo-code for add_timer, cancel_timer, and tick, plus a microbenchmark plan?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["DoorDash","Robinhood","Tesla"]},{"id":"q-1261","question":"Implement a cache-friendly transpose using tile-based approach for an N×N matrix with N a power of two. Provide a function to transpose A into B using 32×32 blocks, and a minimal test harness validating correctness. Explain how tiling reduces cache misses and how to pick block size relative to L1/L2 caches. Include a microbenchmark plan comparing with a naive transpose?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Microsoft","Plaid","Uber"]},{"id":"q-2105","question":"Design and implement a NUMA-aware per-thread arena allocator in C for a 2-socket server. Features: per-core cache-line-aligned pools, 128-byte padding to prevent false sharing, fast paths for 8/16/32-byte blocks, per-size freelists, and a cross-thread deallocation quarantine. Provide API (init, alloc, free, destroy) plus a minimal test harness and a microbenchmark plan comparing intra- vs inter-NUMA performance against malloc?","channel":"low-level","subChannel":"general","difficulty":"intermediate","tags":["low-level"],"companies":["Databricks","Two Sigma","Uber"]},{"id":"q-684","question":"Design a fixed-size ring buffer in C that stores bytes. Capacity N is a power of two (e.g., 1024). Show how to compute the next index using a mask (idx & (N-1)), and explain full vs empty detection using only head and tail counters. Provide enqueue and dequeue logic for a single-producer/single-consumer scenario?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Discord","Hugging Face","Instacart"]},{"id":"q-692","question":"Design a lock-free ring buffer that supports multiple producers and multiple consumers with bounded capacity. Provide enqueue/dequeue pseudo-code, explain how you avoid ABA, how memory reclamation is handled (hazard pointers or epochs), and why it scales under high contention. Include caveats on cache lines and false sharing. How would you validate under stress?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Anthropic","Databricks","Stripe"]},{"id":"q-694","question":"Design a cache-friendly, per-thread deque work-stealer for a multi-core task executor. Each worker maintains a fixed-size ring buffer for bottom push/pop; thieves steal from the top of other workers via CAS on a top index with a version counter. Explain ABA avoidance, memory ordering, and padding to avoid false sharing. Provide precise pseudo-code and a realistic burst workload scenario?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Coinbase","NVIDIA"]},{"id":"q-707","question":"Design a crash‑consistent, multi‑producer/multi‑consumer ring buffer backed by persistent memory PMEM. How would you ensure last enqueued item durability across power loss, implement recovery, and validate correctness? Provide concise enqueue/dequeue pseudocode with proper flush/fence ordering and discuss failure scenarios?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Airbnb","Coinbase","Two Sigma"]},{"id":"q-712","question":"Design a NUMA-aware in-memory index with per-node shards and a lock-free cross-node coordinator. Provide insertion and lookup with minimal locking, specify data layout (shards, padding, key/value encodings), and memory-order guarantees (fences, atomic ops). Describe a deadlock-free shard rebalancing protocol under high contention and outline tests with realistic Snowflake/Twitter-scale workloads?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Snowflake","Twitter"]},{"id":"q-723","question":"Design a software-based **TLB** for a 4-core 64-bit system with 4KiB pages. Each core has a private 128-entry **TLB** and a global page-table invalidation path. Provide lookup/refill pseudo-code, discuss eviction strategy (LRU vs. random), synchronization via **memory fences**, and cross-core shootdowns. Include a test under memory pressure and explain validation?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Instacart","Oracle","Tesla"]},{"id":"q-725","question":"Design a crash-consistent, in-memory index for a 4-byte key, 8-byte value store on an 8-core Linux machine. Use per-core log-structured segments and a Bloom filter; describe durable append ordering, startup recovery by replaying per-core logs, and validation under power-loss scenarios?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Goldman Sachs","LinkedIn","Plaid"]},{"id":"q-734","question":"Design an epoch-based reclamation scheme for a lock-free stack in a 64-bit, multi-threaded user-space library. Each thread publishes its local epoch; a global clock advances periodically. On pop, move the node to a retire-list with its epoch and reclaim only after all threads have observed an epoch older than that node. Include a stress test with high contention?","channel":"low-level","subChannel":"general","difficulty":"intermediate","tags":["low-level"],"companies":["Google","Lyft","Slack"]},{"id":"q-744","question":"In a 2-socket x86-64 server with MESI coherence, design a lock-free, multi-producer single-consumer ring buffer in shared memory for a 10 Gbps network path. Use per-slot sequence numbers to avoid ABA, with a fixed size N=1<<16 and 64-byte payload slots. Provide slot layout, push/pop pseudo-code with memory fences, discuss wrap-around and backpressure, and outline a minimal microbenchmark to validate throughput and data integrity under contention?","channel":"low-level","subChannel":"general","difficulty":"intermediate","tags":["low-level"],"companies":["Instacart","Tesla","Two Sigma"]},{"id":"q-752","question":"Design a crash-safe, persistent ring buffer in NVRAM for a 2-socket NUMA system with a PCIe NIC. Capacity N=1<<18, 128-byte payloads, per-slot 64-bit sequence to prevent ABA. Slot: [payload|seq|meta]. Enqueue: publish payload, fence, then update seq. Dequeue: verify seq before consume. Durability via per-slot commit log: flush payload and seq, then epoch commit. On crash, recover by replaying committed epochs and validating seq monotonicity. Provide a minimal microbenchmark to validate throughput and correctness under power loss?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Adobe","IBM","Tesla"]},{"id":"q-760","question":"On a dual-socket NUMA server, design a cross-NUMA, zero-copy ring buffer for a 40 Gbps path between two processes where producers live on socket A and a consumer on socket B. Use per-slot 64-bit sequence numbers to prevent ABA, 128-byte payload slots, and capacity 1<<18 with 64-byte alignment. Provide slot layout, enqueue/dequeue pseudo-code with memory fences and cache-line padding, wrap-around and backpressure handling, and outline a minimal microbenchmark to validate throughput and data integrity under cross-socket contention?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Amazon","Google"]},{"id":"q-772","question":"Design a cache-friendly fixed-size object pool for 128-byte blocks used by a multi-threaded producer-consumer path. Implement init, alloc, and free in C for a pool of 1<<20 blocks, each 128 bytes and 64-byte aligned. Use per-thread caches to reduce contention and a global lock-free free-list for overflow. Explain how you avoid false sharing, show the slot layout with a freelist pointer, and outline a microbenchmark to measure throughput and tail latency under contention?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Adobe","Netflix"]},{"id":"q-778","question":"Write a C function sum_prefetch that sums N 64-bit integers from an aligned array of length n using software prefetching to hide memory latency. Use 4-way unrolling and __builtin_prefetch to bring data 256 elements ahead. Ensure correctness for any length. Propose a microbenchmark plan to compare with a naive loop and discuss cache-line utilization and false sharing concerns?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Google","Tesla","Twitter"]},{"id":"q-784","question":"Design a deterministic, time-sliced barrier for a three-stage streaming pipeline on a 2-socket x86-64 system. Each stage runs on a fixed subset of cores; implement a barrier that advances phases only after every core finishes its assigned slice within a bounded time. Explain how to ensure bounded latency under cache-line contention, preserve MESI coherence, and prevent starvation. Provide pseudo-code for enter_barrier for a core and discuss validation?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Bloomberg","OpenAI"]},{"id":"q-794","question":"Design and implement a cache-friendly tiled matrix multiply for two large double-precision matrices stored in row-major order on a two-socket NUMA system. Propose a tile size of 32x32, provide C code for the tiled kernel with boundary handling, explain how to maximize L1/L2 reuse, NUMA locality (first-touch), avoid false sharing, and an inline 4-wide inner-loop unrolling. Include a microbenchmark plan comparing to a naive triple-nested loop and how you would measure throughput and cache behavior?","channel":"low-level","subChannel":"general","difficulty":"intermediate","tags":["low-level"],"companies":["Google","LinkedIn"]},{"id":"q-800","question":"Design a per-core, lock-free memory allocator for a shared-memory, NUMA-aware object store. Each core maintains a 2 MB local heap; allocations first attempt local allocation, with a fast cross-core path using atomic hand-offs; reclaimed memory is managed via hazard pointers and epoch-based reclamation. Provide allocate/free APIs, a sketch of the free-list structure, and a microbenchmark plan that shows fragmentation under steady-state load?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Discord","MongoDB","NVIDIA"]},{"id":"q-810","question":"Design a crash-consistent, bounded, multi-producer/multi-consumer queue backed by non-volatile memory. It must survive power loss, use per-slot sequence numbers to avoid ABA, provide push/pop pseudo-code with proper memory fences, and support epoch-based reclamation to avoid hazard pointers. Outline recovery on boot and a microbenchmark plan?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Airbnb","Plaid","Uber"]},{"id":"q-816","question":"Design a crash-consistent, persistent ring buffer for a multi-producer/multi-consumer event stream backed by non-volatile memory, with 1<<18 slots of 128-byte payloads; explain ABA avoidance via per-slot sequence numbers, two-phase publish with durable commit, and required flush/barrier order; describe recovery and a microbenchmark plan under simulated power loss?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Robinhood","Tesla"]},{"id":"q-826","question":"Design and implement a tiny spinlock in C11 for a shared memory region. Provide lock() and unlock() using stdatomic.h primitives, ensuring memory_order_acquire on successful lock and memory_order_release on unlock. Include a minimal two-thread test contending for the lock and explain your backoff/yield strategy and fairness limitations?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Goldman Sachs","Google","Snap"]},{"id":"q-834","question":"Design and implement a fixed-size object pool: a pre-allocated buffer partitioned into 1024 blocks of 64 bytes. Provide thread-safe allocate() and free() using a free-list stored in the blocks and a lightweight spinlock guarding the list. Include initialization and a small test snippet with 2 threads. Discuss fragmentation, cache locality, and how you'd validate concurrency?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Meta","Netflix","Uber"]},{"id":"q-358","question":"You're building a customer churn prediction model. Given a dataset with customer features (age, usage frequency, subscription type), how would you determine whether to use linear regression or logistic regression?","channel":"machine-learning","subChannel":"algorithms","difficulty":"beginner","tags":["regression","classification","clustering"],"companies":["Amazon","Datadog","Robinhood"]},{"id":"q-386","question":"You're building a fraud detection system for a large e-commerce platform. Your initial model using logistic regression has 85% accuracy but high false positives. How would you improve this system using ensemble methods, and what specific trade-offs would you consider between precision and recall?","channel":"machine-learning","subChannel":"algorithms","difficulty":"intermediate","tags":["regression","classification","clustering"],"companies":["Anthropic","Google","OpenAI"]},{"id":"q-201","question":"How does an LSTM cell's forget gate regulate information flow compared to a simple RNN?","channel":"machine-learning","subChannel":"deep-learning","difficulty":"beginner","tags":["lstm","gru","seq2seq"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"q-254","question":"When implementing a bidirectional GRU vs LSTM for sequence labeling, how do gradient clipping thresholds and batch size affect convergence and what are the memory trade-offs?","channel":"machine-learning","subChannel":"deep-learning","difficulty":"intermediate","tags":["lstm","gru","seq2seq"],"companies":["Airbnb","Amazon","Apple","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-415","question":"You're training a CNN for Tesla's Autopilot lane detection. The model works well on clear roads but fails in rainy conditions. How would you modify the architecture and training process to handle weather variations while maintaining real-time performance?","channel":"machine-learning","subChannel":"deep-learning","difficulty":"intermediate","tags":["cnn","rnn","transformer","attention"],"companies":["Coinbase","Epic Games","Tesla"]},{"id":"q-195","question":"How would you implement a canary deployment strategy for a TensorFlow model using MLflow and Kubernetes, ensuring zero downtime and automatic rollback on performance degradation?","channel":"machine-learning","subChannel":"deployment","difficulty":"intermediate","tags":["mlflow","kubeflow","sagemaker"],"companies":["Amazon","Databricks","Google","Microsoft","Uber"]},{"id":"q-227","question":"How would you implement dynamic quantization-aware training with mixed-precision to optimize inference latency while maintaining model accuracy across varying hardware constraints?","channel":"machine-learning","subChannel":"deployment","difficulty":"advanced","tags":["quantization","pruning","distillation"],"companies":["Google","Meta","Microsoft","NVIDIA","Tesla"]},{"id":"q-309","question":"Design a complete MLOps pipeline using MLflow and Kubeflow for a production ML system handling 1M daily predictions with 99.9% availability. What components would you include and how would you ensure model governance and automated retraining?","channel":"machine-learning","subChannel":"deployment","difficulty":"advanced","tags":["mlflow","kubeflow","sagemaker"],"companies":null},{"id":"q-323","question":"How would you design an ML pipeline using Kubeflow that handles model versioning, A/B testing, and automated rollback for a fraud detection system at Coinbase?","channel":"machine-learning","subChannel":"deployment","difficulty":"advanced","tags":["mlflow","kubeflow","sagemaker"],"companies":["Coinbase","Cruise","Tesla"]},{"id":"q-347","question":"You're deploying a machine learning model using MLflow. How would you track experiments and ensure reproducibility when moving from development to production?","channel":"machine-learning","subChannel":"deployment","difficulty":"beginner","tags":["mlflow","kubeflow","sagemaker"],"companies":["MongoDB","Okta","Warner Bros"]},{"id":"q-223","question":"How would you design a production-scale evaluation pipeline that dynamically adjusts metrics based on class imbalance and business priorities while maintaining sub-second latency?","channel":"machine-learning","subChannel":"evaluation","difficulty":"advanced","tags":["precision","recall","auc-roc","f1"],"companies":["Amazon","Google","Meta","Netflix","Stripe"]},{"id":"q-336","question":"You're evaluating a movie recommendation system at Warner Bros. The system has 95% accuracy but poor precision for popular movies. How would you diagnose and fix this issue using precision-recall analysis?","channel":"machine-learning","subChannel":"evaluation","difficulty":"intermediate","tags":["precision","recall","auc-roc","f1"],"companies":["Expedia","Microsoft","Warner Bros"]},{"id":"q-1126","question":"You're deploying a real-time anomaly detector for edge CDN traffic at a cloud provider. Spikes during events cause distribution drift. Propose an online learning approach that adapts without catastrophic forgetting, maintains latency under 30 ms, and keeps calibration. Include data retention policy, drift detection, update rules, and monitoring dashboards?","channel":"machine-learning","subChannel":"general","difficulty":"intermediate","tags":["machine-learning"],"companies":["Cloudflare","Google"]},{"id":"q-1170","question":"You're training a binary classifier on a dataset with 1% positives. After a baseline model, overall accuracy is high but positive precision is very low. Describe a practical plan to diagnose whether the issue is threshold choice or true data imbalance, and implement a minimal pipeline with stratified splits, class weights or resampling, and threshold tuning; outline metrics and validation?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Adobe","Tesla","Two Sigma"]},{"id":"q-1220","question":"You're deploying a single on-device model for real-time video analytics on an edge device with 12 ms per frame latency and 200 MB RAM. The model must perform both object detection and semantic segmentation. Describe a concrete plan to meet latency while preserving accuracy: architecture choices (shared backbone, task heads, feature pyramids), training strategies (loss weighting, distillation, data augmentation), deployment optimizations (quantization, operator fusion, memory layout, early exits), and validation strategy (latency budgets, mAP, mIoU, robustness across weather)?","channel":"machine-learning","subChannel":"general","difficulty":"advanced","tags":["machine-learning"],"companies":["NVIDIA","Tesla","Twitter"]},{"id":"q-1290","question":"You’re training a binary classifier for signup conversion on a dataset with numeric features (age, session_time) and categorical features (device, country). A logistic regression baseline yields high AUROC but poor calibration on holdout. Outline a practical plan to diagnose and fix calibration, comparing Platt scaling and isotonic regression, data preprocessing tweaks, and how you’d validate the fix with a minimal code sketch?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Snowflake","Uber"]},{"id":"q-468","question":"You're training a neural network and notice the validation loss starts increasing while training loss continues decreasing. What's happening and how would you diagnose and fix it?","channel":"machine-learning","subChannel":"general","difficulty":"intermediate","tags":["machine-learning"],"companies":["Amazon","Google","OpenAI"]},{"id":"q-498","question":"You're building a simple spam classifier for emails. What's the difference between precision and recall, and which metric would you prioritize if false positives are more costly than false negatives?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Airbnb","IBM","Tesla"]},{"id":"q-529","question":"You're building a customer churn prediction model for a SaaS platform. What are the key steps you'd take from data preprocessing to model evaluation, and which metrics would you prioritize?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Databricks","Salesforce"]},{"id":"q-555","question":"Explain how gradient descent works and why it's fundamental to training neural networks?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Citadel","Microsoft","Tesla"]},{"id":"q-582","question":"Explain the difference between classification and regression in machine learning, and provide a simple example of when to use each?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Meta","Oracle"]},{"id":"q-273","question":"What's the difference between hyperparameters and parameters in machine learning, and why is cross-validation important for selecting optimal hyperparameters?","channel":"machine-learning","subChannel":"model-training","difficulty":"beginner","tags":["hyperparameter","cross-validation","regularization"],"companies":["Amazon","Google","Meta","Microsoft"]},{"id":"q-372","question":"You're training a CNN for Snapchat lens effects and notice your validation loss increases after epoch 3 while training loss decreases. What's happening and how would you implement a comprehensive solution including data augmentation, learning rate scheduling, and monitoring strategies?","channel":"machine-learning","subChannel":"model-training","difficulty":"beginner","tags":["hyperparameter","cross-validation","regularization"],"companies":null},{"id":"q-403","question":"You're training a large language model for Notion's AI features. Your model is overfitting on the training data but underperforming on validation. Design a comprehensive regularization strategy that addresses both L1/L2 regularization and more advanced techniques like dropout and early stopping. How would you implement cross-validation to ensure your hyperparameters generalize across different user data distributions?","channel":"machine-learning","subChannel":"model-training","difficulty":"advanced","tags":["hyperparameter","cross-validation","regularization"],"companies":["Chime","Google","Notion"]},{"id":"q-1133","question":"Scenario: a constraint-satisfaction problem with four tasks A,B,C,D each can be in domains {Pending, Running, Done}. Constraints: (A = Done) → (B = Running); (B = Running) → (C = Done); (C = Pending) → (D = Running); and (A = Pending) ∨ (D = Done). Is there a feasible assignment with at least one Running? Explain reasoning and outline a backtracking algorithm with forward-checking and 3-valued propagation to decide arbitrary such constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Bloomberg","Lyft"]},{"id":"q-1253","question":"Scenario: A deployment feature-flag policy defines three booleans F1, F2, F3 with these constraints: F1 -> F2, F2 -> F3, and exactly one of F1 and F3 is true, plus at least one flag is true. Is there a satisfying assignment? If yes, give one; if not, explain why. Then outline a tiny solver that encodes such constraints into a 2-SAT instance using an implication graph and SCC, with pseudocode?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Amazon","Meta","PayPal"]},{"id":"q-1915","question":"You design a tiny policy engine for network ports where each port's state is Allow, Deny, or Unknown (A, D, U). Ports: 80, 443, 22, 23, 8080. Constraints: (80 → 443), (22 ∨ 23), (80 ∨ 22), (443 → ¬23). Is there an assignment with U that does not force any constraint to be definitively false? Explain how you would implement a 3-valued backtracking propagation to verify arbitrary such rule sets?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Cloudflare","LinkedIn","Scale Ai"]},{"id":"q-1930","question":"In a policy graph with rules of the form (P ∧ Q) → R, (P ∧ R) → S, and (Q ∧ S) → T, plus a set of base facts that you may set to true as needed, is there an assignment making T true without violating any rule? Provide an algorithm to compute a minimal witness (smallest base set) and apply it to this instance to show T is derivable with base facts {P, Q}?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Apple","Databricks","Twitter"]},{"id":"q-2088","question":"You manage access flags as a directed graph with A ≤ B meaning 'A true implies B true'. Given a required set T of flags that must be true, decide if a consistent assignment exists and produce the least-true model containing T. Describe a linear-time forward-closure algorithm that propagates truth along edges until a fixpoint, and how it handles cycles. Include a concrete 6-flag example with 7 relations?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Bloomberg","Goldman Sachs","Google"]},{"id":"q-683","question":"You're managing a streaming DAG with tasks A,B,C,D; edges enforce: A before B; B before C; at most one of C or D can occur in a window of size H. Given a log of events with timestamps per task, implement an O(n log n) verifier to determine if the log is valid under these constraints and describe how you'd extend to multiple windows in a distributed system?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Databricks","Google"]},{"id":"q-685","question":"Context: In a data-cleaning pipeline, records have boolean attributes a, b, c, d. Rules are Horn clauses of the form X ∧ Y -> Z. Given a partial assignment, decide if a full assignment exists that satisfies all clauses. Design a linear-time forward-chaining solver, justify its correctness, and discuss incremental updates and cycles with a concrete four-variable example (two rules)?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Adobe","Databricks","Scale Ai"]},{"id":"q-699","question":"You have a Horn-clauses policy language for access control. Given the following rules and facts, determine if allow(alice,read,records) is entailed using forward-chaining to a least fixpoint. Rules: 1) grant(U,act,res) :- haveRole(U,R), privilege(R,act,res). 2) allow(U,act,res) :- grant(U,act,res). Facts: haveRole(alice,dataEngineer). privilege(dataEngineer,read,records). Show your derivation steps?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Adobe","Databricks","MongoDB"]},{"id":"q-702","question":"In a home security system, three sensors—door, window, and motion—report activity as booleans a, b, c for the last minute. The alert should fire only when exactly one sensor is active. How would you implement a function that takes a, b, c and returns true iff exactly one is true, and what are its time and space complexities?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Amazon","Google"]},{"id":"q-715","question":"In a distributed data-processing pipeline across three services Ingest (I), Compute (C), Persist (P), each event carries a 3-element vector clock. Given E1 at I with [2,0,1] and E2 at C with [1,3,0], decide whether E1 happened-before E2, E2 happened-before E1, or they are concurrent. Explain the rule and show the comparison. How would you scale this to N services and detect concurrency in large logs?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Amazon","Bloomberg","Slack"]},{"id":"q-719","question":"Design a tiny solver for three boolean inputs A, B, C given constraints: (A -> B) and (B -> C) and (A or C). Is there an assignment that satisfies all three? Explain your reasoning and outline a simple backtracking approach you would implement to check arbitrary small sets of such implications?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Adobe","Google","Scale Ai"]},{"id":"q-728","question":"Scenario: You’re building a tiny policy engine for feature flags with three booleans A, B, C. Constraints: A implies B, B implies C, and at least two of the three must be true. Is there an assignment that satisfies all constraints? If so, give one example and briefly justify. Then outline a straightforward backtracking approach to enumerate all satisfying assignments for any n flags and any such constraints?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Amazon","Instacart","Plaid"]},{"id":"q-733","question":"You’re auditing a rule-set for access policy. There are four booleans **X1**, **X2**, **X3**, **X4** with constraints: (X1 -> X2), (X1 ∨ X3), (X2 -> X4), and (X3 -> ¬X4). Is there a satisfying assignment? If yes, provide one; if not, explain why. Then outline a minimal backtracking strategy with unit propagation to verify arbitrary similar constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Microsoft","Netflix","Salesforce"]},{"id":"q-745","question":"You are building a tiny rule engine for feature toggles with three booleans A, B, C. Constraints: A → B, B → ¬C, and at least one of A, B, C must be true. Is there a satisfying assignment? Explain how you would verify it via brute-force backtracking across the eight possibilities and return one concrete example if it exists?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["NVIDIA","Twitter"]},{"id":"q-753","question":"In a feature-flag synthesis task, you have booleans A1..A8. Constraints include: (Ai -> Aj) implications, (Ai XOR Aj) mutual exclusions, and (Ai OR Aj OR Ak) group obligations. The implication graph is acyclic and each node has at most two outgoing edges. Design a backtracking solver that exploits the DAG to decide satisfiability for up to 12 vars. Provide a concrete 8-variable instance and show the solution or UNSAT?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Apple","LinkedIn","Netflix"]},{"id":"q-758","question":"You manage feature flags for a distributed service. Let F1..F5 be booleans with constraints: (F1 -> F2), (F1 ∨ F3), (F2 ⊕ F4), (F3 -> F5), and (F4 -> ¬F5). Is there a satisfying assignment? If yes, provide one; if not, explain. Then outline a minimal backtracking strategy with forward checking to verify arbitrary similar constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Cloudflare","Discord","Snowflake"]},{"id":"q-771","question":"You are validating a tiny access-control policy with two booleans: A = 'account is active', B = 'email verified'. The policy must satisfy: (¬A -> B), (B -> A), and (A ∨ B). Is there a satisfying assignment for A and B? Explain your reasoning and outline a simple backtracking check to verify arbitrary small sets of such clauses?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["NVIDIA","PayPal","Snowflake"]},{"id":"q-775","question":"You're building a constraint-solver for a feature-flag system across a microservices deployment. Given four flags S1..S4 with constraints: (S1 -> S2), (S2 -> S3), (S4 -> ¬S1), and (S1 ∨ S4) and (S2 ∨ S4). Is there a satisfying assignment? Provide one concrete assignment and outline a backtracking algorithm with unit propagation to verify arbitrary similar constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Slack","Two Sigma"]},{"id":"q-789","question":"Is there a truth assignment with exactly three flags true that satisfies all constraints A -> B; B -> C; D -> E; F -> G; not C -> H; A -> D? If yes, provide one and justify why it satisfies every implication?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Apple","Square","Twitter"]},{"id":"q-799","question":"Scenario: four booleans P, Q, R, S where P = user has role X, Q = grants read, R = has role Y, S = grants write. Constraints: (P -> Q), (R -> S), (P ∨ R), and at least two of {Q,S} must be true. Is there a satisfying assignment? If yes, give one; if not, explain why. Then outline a minimal backtracking strategy with unit propagation for arbitrary similar constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Cloudflare","DoorDash","Google"]},{"id":"q-812","question":"Instance-based SAT with mixed Horn and parity constraints: Variables A,B,C,D,E,F. Constraints: (A ∧ B) → C; (C ∧ D) → E; A ⊕ D ⊕ F = 1; (B ∧ E) → F. Is there an assignment to A..F that satisfies all constraints? If yes, provide one; then describe how you would build a solver that combines forward-chaining on Horn clauses with Gaussian elimination over GF(2) for parity constraints, including data structures and complexity considerations?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Citadel","Google","Lyft"]},{"id":"q-822","question":"Scenario: a tiny feature-toggle system for a data-annotation pipeline uses five booleans A–E: A = 'data augmentation enabled', B = 'sampling enabled', C = 'privacy mode on', D = 'live monitoring on', E = 'throttle rate limited'. Constraints: (A → B), (B → D), (C → ¬D), (A ∨ C), (D → E), (E → ¬A). Is there a satisfying assignment? If yes, give one; if not, explain why. Then outline a minimal backtracking strategy with unit propagation to verify arbitrary similar constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Databricks","Hugging Face","Square"]},{"id":"q-828","question":"Scenario: a feature-flag set uses four booleans A,B,C,D with rollout statuses True, False, Unknown (U). Constraints: (A → B), (C ∨ D), (A ∨ C), (B → ¬D). Is there a satisfying assignment allowing Unknowns? Explain reasoning and outline a backtracking approach using 3-valued logic to propagate U and verify arbitrary such constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Hashicorp","MongoDB","Robinhood"]},{"id":"q-180","question":"What is the primary purpose of DNS in computer networking and how does it enable internet communication?","channel":"networking","subChannel":"dns","difficulty":"beginner","tags":["dns","resolution"],"companies":["Amazon","Cisco","Google","Meta","Microsoft"]},{"id":"q-1031","question":"In a beginner-friendly scenario, a REST API behind a global CDN shows sporadic 1–2s latency for some users while synthetic tests pass. Outline a practical, end-to-end diagnostic plan to isolate DNS, TLS, caching, and client-network factors, including concrete commands and data you would collect and an initial fix you would try?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Discord","DoorDash","Snowflake"]},{"id":"q-1078","question":"In a small office network, a workstation intermittently cannot reach a public API behind Cloudflare during peak hours while other destinations are responsive; outline a practical, hands-on plan to diagnose using DNS (A/AAAA, TTLs), path tracing, TLS handshakes (ALPN/SNI), and edge routing behavior, with concrete mitigations to test?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Cloudflare","Tesla"]},{"id":"q-1089","question":"In a multi-region Kubernetes deployment using VXLAN overlays for pod networking, you notice intermittent packet loss and high tail latency when pods in region A talk to pods in region B. The overlay adds headers that push MTU beyond 1500 on inter-region links, causing fragmentation in some paths. You can adjust MTU, MSS clamping, and overlay parameters but cannot modify application code. How would you diagnose end-to-end and implement a robust fix that preserves ECMP load balancing and minimizes fragmentation? Provide concrete steps?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["DoorDash","Google"]},{"id":"q-1163","question":"In a two-region service, IPv6 clients report higher latency and intermittent timeouts to a TLS-enabled API when accessed from IPv6 only networks. You cannot modify app code. Outline a practical diagnostic plan focusing on IPv6 path MTU discovery, ICMPv6/firewall behavior, and how to verify with traceroute6, tcpdump, and TLS handshake timings. Propose concrete mitigations like adjusting VPN MTU and enabling IPv4 fallback?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Discord","Robinhood","Snowflake"]},{"id":"q-1232","question":"In a two-region deployment (us-east-1, eu-west-1) with a TLS-enabled API behind a global load balancer, peak hours yield p95 latency spikes to 350 ms while basic tests pass. No app changes allowed. Provide a concrete diagnostic plan to distinguish TLS handshake delays, ALPN/SNI issues, path MTU fragmentation, and load balancer behavior, with exact commands and data you’d collect?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Hashicorp","MongoDB"]},{"id":"q-1461","question":"In a globally distributed API behind a CDN and global load balancer, Asia users report login latency significantly higher than North America, while overall API latency remains acceptable. The client app is mobile and uses TLS with SNI; no code changes are allowed. Outline a concrete diagnostic plan to determine whether ECS (EDNS Client Subnet), DNS TTL, CDN edge variance, or inter-region routing is responsible, including exact commands, data to collect, and initial mitigations to test?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Google","MongoDB","Tesla"]},{"id":"q-1507","question":"In a globally distributed TLS-enabled API for real-time inference, traffic flows from Asia, EU, US behind a global load balancer and regional edge caches. During peak, Asia users see p95 latency spikes while NA remains stable. No code changes allowed. Provide a concrete diagnostic plan to distinguish between (1) TLS handshake/ALPN at edge vs origin, (2) inter-region routing and MTU fragmentation, (3) load balancer ECMP behavior, and (4) CDN edge miss penalties. Include exact commands, data to collect, and initial mitigations to test?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["NVIDIA","OpenAI","PayPal"]},{"id":"q-1671","question":"In a multi-cloud microservice mesh spanning AWS and Azure, inter-region service-to-service calls intermittently fail under load with rising p95 latencies. Tracing shows ECMP paths changing mid-request and MTU-related fragmentation on some hops. Without modifying applications, outline a concrete diagnostic plan and a stabilization strategy addressing PMTUD behavior, IPv4/IPv6 MTU alignment, ICMP blocking, and MTU discovery pitfalls across clouds, while preserving ECMP load balancing?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["Microsoft","Netflix","Oracle"]},{"id":"q-1766","question":"In a global data service using DNS-based global load balancing across us-east-1 and eu-west-1, cross-region reads spike latency during peak hours while intra-region calls remain fast. No app changes allowed. Outline a concrete diagnostic plan to distinguish stale DNS routing, BGP route flaps, and edge TLS termination delays, with exact commands and data you’d collect, interpretation rules, and recommended mitigations?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["Instacart","NVIDIA","Snowflake"]},{"id":"q-469","question":"Explain what happens when you type google.com into your browser and press Enter, focusing on the networking layers involved?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Discord","Goldman Sachs","IBM"]},{"id":"q-499","question":"How would you design a TCP load balancer that handles 1M concurrent connections with consistent hashing while preventing connection thrashing during backend failures?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["Snowflake","Twitter"]},{"id":"q-583","question":"How would you design a load balancer to handle 1M concurrent connections with sub-10ms latency, considering TCP connection pooling, health checks, and graceful degradation?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["Adobe","Goldman Sachs"]},{"id":"q-926","question":"In a multi-region deployment (US-East, EU-West) for a high-throughput service, end-to-end latency spikes to 150–300 ms during peak hours while synthetic tests pass. You observe edge drops and retransmissions. Provide a practical plan to diagnose and mitigate network-related factors, including path MTU discovery, ECN, TCP congestion control, TLS handshakes, SNI/ALPN behavior, and load-balancing strategy across regions, with data you’d collect and initial fixes?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["NVIDIA","Snowflake","Uber"]},{"id":"q-946","question":"In a globally distributed service behind a multi-region, ECMP-enabled load balancer, you observe sporadic high-tail latency while averages look fine. Explain the network mechanisms that could cause tail latency in this setup (per-path RTT variance, path MTU, reordering, retransmissions). Propose a concrete diagnostic workflow using production telemetry (eBPF per-flow histograms, NetFlow/SFlow, MTU checks) and practical remediation steps (MTU tuning, pacing, queue management)?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["Coinbase","Salesforce","Two Sigma"]},{"id":"gh-72","question":"How would you design and implement network segmentation for a microservices architecture, including Zero Trust principles, east-west traffic monitoring, and compliance requirements?","channel":"networking","subChannel":"load-balancing","difficulty":"advanced","tags":["security","network"],"companies":["Amazon","Cloudflare","Google","Hashicorp","Microsoft","Stripe"]},{"id":"q-186","question":"How would you implement session affinity (sticky sessions) in HAProxy while maintaining high availability, and what are the trade-offs compared to stateless load balancing?","channel":"networking","subChannel":"load-balancing","difficulty":"intermediate","tags":["lb","traffic","nginx","haproxy"],"companies":["Amazon","Bloomberg","Google","Microsoft","Netflix"]},{"id":"sd-1","question":"Explain load balancing strategies and when to use Layer 4 vs Layer 7. How do round-robin, least connections, and IP hash algorithms compare?","channel":"networking","subChannel":"load-balancing","difficulty":"advanced","tags":["infra","scale","networking"],"companies":["Amazon","Cloudflare","Google","Meta","Microsoft","Netflix"]},{"id":"q-203","question":"How does TCP's congestion control algorithm interact with HTTP/2's multiplexing when multiple streams compete for bandwidth?","channel":"networking","subChannel":"tcp-ip","difficulty":"intermediate","tags":["tcp","udp","http2","quic"],"companies":["Amazon Aws","Cloudflare","Google","Microsoft","Netflix"]},{"id":"q-256","question":"How does QUIC solve TCP's head-of-line blocking problem in HTTP/2 multiplexing, and what are the implementation trade-offs?","channel":"networking","subChannel":"tcp-ip","difficulty":"intermediate","tags":["tcp","udp","http2","quic"],"companies":["Amazon","Cloudflare","Google","Meta","Microsoft","Netflix"]},{"id":"q-275","question":"How does QUIC solve HTTP/2's head-of-line blocking issue over TCP, and what are the implementation trade-offs?","channel":"networking","subChannel":"tcp-ip","difficulty":"intermediate","tags":["tcp","udp","http2","quic"],"companies":["Amazon","Google","Meta","Microsoft"]},{"id":"q-1035","question":"In an advanced NLP interview, design an end-to-end multilingual QA system over English, Spanish, and Mandarin medical documents. The user asks in English. Outline architecture, data flow, privacy controls, latency targets, domain adaptation, and an evaluation plan. Include concrete components, trade-offs, and a short example of validation for a high-risk medical claim?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Adobe","IBM","MongoDB"]},{"id":"q-1101","question":"You're building a real-time brand-monitoring NLP service that ingests up to 100k tweets per minute in multiple languages. Design a scalable pipeline to classify sentiment and issue categories (e.g., billing, outages) with <300 ms latency per tweet, handle code-switching and slang, detect and adapt to drift, and provide a rollout plan including testing, monitoring, and rollback?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Stripe","Twitter"]},{"id":"q-1215","question":"Design a beginner-friendly pipeline for a Slack-based support bot that lives in a single workspace. It should: (1) classify Slack messages into intents: 'password_reset', 'access_request', 'billing_issue', 'incident'. (2) retrieve and present the most relevant FAQ article from a 100-article KB in English or Spanish. (3) operate with minimal latency on a shared CPU, and include a simple drift-detection plan and a rollout strategy with a safe fallback. Provide concrete components, data flow, and a short code snippet showing the classifier and retriever?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Slack","Snap"]},{"id":"q-1332","question":"Design an offline-first, on-device NLP pipeline for field technicians in remote areas. The device must classify support requests into hardware, network, or software issues and extract actionable items from multilingual speech transcripts (English, Spanish, Portuguese). Constraints: 256MB RAM, ≤200ms latency per utterance, no network access except periodic OTA updates, privacy-preserving embeddings, and robust drift detection with authenticated weight patches. Provide architecture, models, data handling, evaluation, and rollout plan?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Databricks","Hashicorp","Snap"]},{"id":"q-1394","question":"You are given a multilingual customer support dataset containing code-switching between English and Spanish and occasional emojis. Design an end-to-end NLP solution for intent classification and slot filling, with limited labeled data in the target language. Describe data collection, preprocessing, model choice, and evaluation strategy, including how you'd handle code-switching and emoji semantics?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Google","IBM","Two Sigma"]},{"id":"q-1456","question":"You're deploying a multilingual on-device sentiment and intent classifier for a mobile app, handling English/Spanish with code-switching and emojis. Latency budget: <200 ms on a 2-core device, offline-first. Design an end-to-end pipeline: data flow, model architecture (tiny quantized model plus emoji/slang rules), feature extraction, on-device drift detection, and a practical evaluation plan using ~50 labeled examples for quick adaptation?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Salesforce","Scale Ai","Twitter"]},{"id":"q-1657","question":"Design a beginner-friendly NLP pipeline to extract Date, Money, and Person entities from bilingual English/Spanish Slack-like messages with slang and emojis, using minimal labeled data. Outline preprocessing, tool choices (regex, spaCy), and a concrete evaluation plan with a simple baseline?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Slack","Snap","Two Sigma"]},{"id":"q-1702","question":"Design an end-to-end NLP system to detect safety-critical incidents from real-time chat and voice transcripts in a multi-tenant ride-hailing platform. Include data ingestion, ASR/translation, latency targets, privacy controls, and model versioning/deployment. Compare rule-based vs learned approaches and detail production evaluation (offline metrics plus live A/B and rollback plans)?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Hashicorp","Lyft","Robinhood"]},{"id":"q-2021","question":"Design a multilingual, low-latency NLP pipeline to summarize and extract action items from enterprise meeting transcripts in English, Spanish, and Mandarin. Include language detection, on-device vs cloud trade-offs, privacy controls, and a lifecycle for models and data. How would you evaluate accuracy and latency, and handle update rollouts with minimal downtime?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Google","LinkedIn","Snap"]},{"id":"q-2171","question":"Design a beginner-friendly on-device NLP classifier for a multilingual chat dataset in English and Spanish labeled as spam in a Discord-like app. Outline data prep, feature choices such as character n-grams, a lightweight model (logistic regression), multilingual handling, and a minimal evaluation plan with a held-out test and drift checks. Include a tiny Python snippet to train on a toy dataset?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Amazon","Discord"]},{"id":"q-470","question":"How would you implement a sentiment analysis pipeline for customer reviews that handles negation and domain-specific slang? What preprocessing steps would you prioritize?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["DoorDash","IBM","Square"]},{"id":"q-500","question":"How would you implement basic text preprocessing for sentiment analysis, including tokenization, stop word removal, and stemming?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Airbnb","Apple","Google"]},{"id":"q-584","question":"How would you implement a transformer-based model for real-time text generation with attention mechanisms that handle variable-length sequences efficiently?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Databricks","Lyft","Tesla"]},{"id":"q-229","question":"What is the difference between tokenization and stemming in NLP text preprocessing, and when would you choose lemmatization over stemming?","channel":"nlp","subChannel":"text-processing","difficulty":"beginner","tags":["tokenization","stemming","ner"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-294","question":"How does the attention mechanism in transformers allow the model to handle variable-length sequences without recurrent connections?","channel":"nlp","subChannel":"transformers","difficulty":"intermediate","tags":["cnn","rnn","transformer","attention"],"companies":["Amazon","Google","Meta"]},{"id":"q-1124","question":"On a NUMA‑aware Linux host with a fixed‑size worker pool handling high‑frequency RPCs, cross‑socket memory traffic is a bottleneck. Propose a concrete plan to minimize inter‑socket traffic: pin threads to sockets, allocate per‑socket data, and choose memory policies (numa_bind/numa_alloc_onnode). Include measurement steps and success criteria?","channel":"operating-systems","subChannel":"general","difficulty":"advanced","tags":["operating-systems"],"companies":["Discord","Netflix","Salesforce"]},{"id":"q-1299","question":"Scenario: two threads share a global 32-bit counter. Thread A increments it in a tight loop; Thread B logs the value once per second. Without synchronization, describe a concrete interleaving that yields a stale read or lost update, and explain the cache/coherence mechanics behind it. Then outline the minimal fix and how it ensures atomicity and visibility (e.g., atomic fetch_add or a mutex)?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Instacart","Square"]},{"id":"q-1354","question":"You implement a lock-free ring buffer with two atomics (head, tail) and a data array for inter-thread communication. Describe a concrete interleaving on a weak memory model (e.g., ARM64) where the consumer observes a stale value or an invalid read due to missing ordering. Propose a minimal fix using memory_order_release on the data write/tail update and memory_order_acquire on the consumer read, and sketch a safe patch?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Apple","Hugging Face","Meta"]},{"id":"q-1578","question":"On a Linux server, four worker processes share a 100GB read-only memory-mapped dataset loaded from disk on demand via mmap. Describe the sequence of page fault handling, TLB behavior, and how the kernel page cache and optional swap interact with this pattern. Propose two concrete knobs to maximize throughput without starving others (e.g., MADV_WILLNEED with madvise, NUMA binding with mbind) and how you would measure success?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Coinbase","DoorDash","Two Sigma"]},{"id":"q-1733","question":"Scenario: After a fork, a child writes to a Copy-On-Write (COW) page. Describe the end-to-end kernel steps from the write fault to the point where the parent and child have separate views, including page table updates, TLB changes, and how the private copy is created and isolated from the parent's mapping?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Databricks","Discord","Salesforce"]},{"id":"q-1759","question":"In a multithreaded server, each worker maintains a per-thread stats counter in an array of N 64-byte structs (one per thread). A separate thread periodically sums these counters every second. Without padding, explain a concrete interleaving that leads to cache line false sharing and degraded throughput, and propose a fix (padding, alignas cache-line, or per-thread local counters plus a reduction) that preserves correctness and improves performance?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["DoorDash","Netflix"]},{"id":"q-2084","question":"In a kernel memory allocator with per-core freelists and a global free pool protected by a spinlock, describe a concrete interleaving that yields a use-after-free for a block still in use by a reader, and explain how either hazard pointers or epoch-based reclamation prevents it, including the required memory-order guarantees on x86-64 and how grace periods are enforced?","channel":"operating-systems","subChannel":"general","difficulty":"advanced","tags":["operating-systems"],"companies":["Citadel","Netflix","Oracle"]},{"id":"q-448","question":"Explain how process scheduling works in an operating system. Which scheduling algorithm would you choose for a real-time system and why?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["DoorDash","OpenAI","Snap"]},{"id":"q-471","question":"You're designing a GPU memory manager for CUDA applications. How would you implement a memory allocator that handles both unified memory and explicit device memory, considering fragmentation, coalescing, and the 48-bit address space limitations?","channel":"operating-systems","subChannel":"general","difficulty":"advanced","tags":["operating-systems"],"companies":["Amazon","NVIDIA"]},{"id":"q-530","question":"A process is stuck in 'D' state (uninterruptible sleep) during I/O operations. How would you debug this, what causes it, and how does it differ from 'Z' zombie state?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Adobe","LinkedIn","Netflix"]},{"id":"q-556","question":"How would you debug a process that's consuming 100% CPU but not responding to signals? What tools and steps would you use?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Snowflake","Two Sigma"]},{"id":"q-585","question":"How would you implement a lock-free concurrent queue using atomic operations and memory barriers? What are the trade-offs between ABA problem solutions?","channel":"operating-systems","subChannel":"general","difficulty":"advanced","tags":["operating-systems"],"companies":["Citadel","OpenAI","Square"]},{"id":"q-927","question":"In a system with a fixed-size circular buffer of size N shared by a producer and a consumer thread, implement a thread-safe producer-consumer solution using semaphores and a mutex in C. Include initialization, edge cases (buffer full/empty), and show how you would test for deadlocks and correctness under concurrent producers/consumers?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Adobe","OpenAI","Scale Ai"]},{"id":"q-995","question":"In a system using paging with a TLB, describe the end-to-end sequence when a 4 KB page accessed by a process is not mapped in RAM, from fault to resume, including the fault handler, page-table walk, TLB update, disk I/O to swap, and how the eviction policy decides which page to replace?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Lyft","NVIDIA","Stripe"]},{"id":"q-263","question":"How does demand paging optimize memory utilization in virtual memory systems, what triggers page faults, and which algorithms handle page replacement when physical memory is full?","channel":"operating-systems","subChannel":"memory","difficulty":"intermediate","tags":["virtual-memory","paging","segmentation","cache"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-1070","question":"You are building a latency-sensitive telemetry ingestion pipeline for autonomous fleets. Design an end-to-end data path from edge sensors to a real-time analytics sink. Include data schemas, serialization (protobuf vs Avro), transport (Kafka vs Kinesis), fault tolerance (idempotency, replay), schema evolution, and testing strategy. Explain trade-offs for throughput, latency, and reliability?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Lyft","NVIDIA","Tesla"]},{"id":"q-1090","question":"Design a real-time notification system for 1M+ concurrent users with end-to-end latency under 200ms. Describe architecture, data model, delivery guarantees, back-pressure, disaster recovery, and how you measure and enforce SLOs?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Apple","Cloudflare","Discord"]},{"id":"q-1115","question":"In a global OTCA stack for a large platform, design a fault-tolerant telemetry pipeline that ingests 10k events/sec, preserves dashboards with sub-30s freshness, and scales regionally. Describe data model, streaming, storage, and tracing choices (e.g., Kafka, OpenTelemetry, ClickHouse/BigQuery), backpressure handling, and testing strategy?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["DoorDash","Meta","Twitter"]},{"id":"q-1171","question":"Design a multi-tenant OTCA telemetry pipeline with per-tenant quotas and privacy masking. Ingest 20k events/sec regionally via Kafka, route by tenant_id, and compute deduplicated aggregates in Flink. Hot metrics in Redis, long-term data in ClickHouse/BigQuery, traces via OpenTelemetry. Include idempotent writes, backpressure handling, and robust testing?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Adobe","Coinbase","Google"]},{"id":"q-1219","question":"In a multi-tenant OTCA pipeline for a global fintech platform, how would you implement tenant-scoped telemetry collection that isolates data, preserves sub-second dashboards, and enforces per-tenant quotas? Describe data model, per-tenant exporters, sampling, storage with row-level security, and testing strategy?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Goldman Sachs","PayPal"]},{"id":"q-1237","question":"In a multi-tenant OTCA telemetry stack for a global SaaS, migrate from a fixed event schema to an evolving, backward/forward-compatible schema while preserving per-tenant data residency. The system must sustain up to 60k events/sec with regional bursts. Describe data model, schema versioning, streaming backbone, storage strategy (raw + materialized), tracing, backpressure handling, tenant-level sampling, and testing plan?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Databricks","MongoDB","Zoom"]},{"id":"q-1286","question":"In a global OTCA stack, design a fault-tolerant telemetry pipeline that ingests 20k events/sec per region from web/mobile clients, enforces per-tenant data residency, and uses adaptive sampling for long-tail tenants. Specify data model, streaming, storage, tracing, backpressure, and a test plan that validates burst behavior, schema evolution, and failover?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Meta","Snowflake"]},{"id":"q-1307","question":"In a global OTCA telemetry stack with three regions, enforce tenant residency by region while enabling real-time global dashboards with sub-500ms latency. Provide the end-to-end ingestion, storage, and aggregation plan, including data model, streaming/backplane choices, per-tenant partitioning, cross-region replication policy, and a validation strategy for residency, schema evolution, and burst traffic?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Cloudflare","Oracle","Twitter"]},{"id":"q-1495","question":"In a two-region OTCA telemetry pipeline for a microservices platform, design a privacy-preserving, adaptive sampling plan for distributed traces that enforces per-tenant data residency, minimizes data egress, and sustains sub-400ms end-to-end latency for dashboards. Detail the trace data model, propagation scheme, sampling algorithm, backpressure handling, and a validation plan?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["LinkedIn","NVIDIA","Snap"]},{"id":"q-1598","question":"Design a lightweight client-side OTCA telemetry exporter for a mobile app used across regions. The exporter must batch events (5 seconds or 1000 events), attach fields: tenant_id, device_id, app_version, event_type, and timestamp; implement offline queuing with local storage, retry with exponential backoff and jitter, and ensure eventual delivery when connectivity returns. Describe data model, batching, retry, and testing plan, plus how you measure correctness and dashboards?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["Anthropic","Tesla","Zoom"]},{"id":"q-1731","question":"In a global OTCA telemetry stack spanning five regions, tenants' raw events must remain within their origin region; only anonymized aggregates cross regions for global dashboards with sub-200ms latency. Design the end-to-end ingestion, streaming, storage, and access controls. Specify data models, de-identification/privacy controls, cross-region aggregation, backpressure, and a testing plan to validate residency, privacy, and latency under burst traffic?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Bloomberg","Microsoft","Oracle"]},{"id":"q-1847","question":"For a global OTCA telemetry stack supporting a multi-tenant ML inference platform, design a region-aware telemetry pipeline that records: tenant_id, model_id, input_hash, latency_ms, outcome, and drift_score. Propose a streaming backbone, OLAP store, and a per-tenant residency policy with SLA-based QoS, plus backpressure, schema evolution, and testing plan?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Hugging Face","Robinhood","Two Sigma"]},{"id":"q-1900","question":"You're building a beginner OTCA telemetry pipeline for a mobile app used in two regions. Design a minimal streaming path that logs only essential fields (tenant_id, event_type, timestamp, latency_ms) and enforces per-tenant data access and privacy (redaction/anonymization). Describe the data model, streaming backbone, storage, and a practical test plan to validate privacy, QoS, and dashboard freshness (<=60s)?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["Apple","Microsoft"]},{"id":"q-1924","question":"Design a beginner OTCA telemetry path for a mobile app where events include tenant_id, event_type, timestamp, latency_ms. Implement a simple on-device dedup using event_id, normalize event_type into a canonical event family, and publish to a single Kafka topic with per-tenant routing to regional storage (Parquet on S3) to meet residency. Describe data model, streaming path, storage layout, and a minimal test plan validating dedup, schema evolution, and cross-region consistency?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["Amazon","Plaid"]},{"id":"q-1999","question":"Design a global OTCA telemetry pipeline for real-time feature experimentation across MongoDB, Netflix, and Nvidia workloads. Each event includes device_id, experiment_id, feature_id, timestamp, latency_ms, and consent_flag. Requirements: per-tenant QoS with adaptive sampling, privacy masking for device_id, versioned schema with backward compatibility, cross-region attribution, and sub-200ms dashboard freshness. Outline data model, streaming backbone, storage layout, backpressure handling, and testing strategy?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["MongoDB","NVIDIA","Netflix"]},{"id":"q-2044","question":"Design a beginner OTCA telemetry path for a mobile app that must enforce per-tenant residency, basic privacy redaction, and monthly cost quotas while handling up to 5k events/sec. Provide a concrete data model (tenant_id, event_type, timestamp, latency_ms, event_id), a streaming topology (on-device dedup, region-specific Kafka, region-local Parquet storage with cross-region replication), and a test plan to verify residency, privacy, dedup, quota enforcement, and dashboard freshness?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["NVIDIA","Oracle","Scale Ai"]},{"id":"q-2157","question":"Design a federated OTCA telemetry pipeline for a multi-tenant platform deployed on AWS, GCP, and Azure, where each event includes tenant_id, service_id, event_type, timestamp, latency_ms, and model_version. Propose a central schema registry with per-tenant Protobuf contracts and strict forward/backward compatibility, a streaming backbone with tenant-scoped topics (e.g., Pulsar), cross-cloud replication, and a compliant data deletion/retention policy. Outline data contracts, backfill strategy, testing, and observability?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Anthropic","Plaid","Salesforce"]},{"id":"q-838","question":"You’re building a minimal product API in Express. Implement routes GET /products and GET /products/:id that returns 404 when not found. Include input validation, safe DB access via parameterized queries, and robust error handling. Explain your approach and provide a concise code sample?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["Airbnb","Discord","Meta"]},{"id":"q-908","question":"You have a global edge API gateway handling peak bursts. How would you implement a distributed token-bucket rate limiter per API key across regions using Redis? Outline the Lua script for atomic refill and consume (capacity 1000, refill 50 tokens/sec), how you expose headers, handle clock skew, and fallback when Redis is unavailable?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Bloomberg","Cloudflare","Square"]},{"id":"q-1012","question":"In a churn prediction problem, you have 20k customers and 500 features (mix of binary indicators and continuous metrics). PCA will be used before a logistic regression model to predict churn. Describe an end-to-end plan to (1) handle missing values and mixed data types, (2) scale features appropriately, (3) choose the number of components with cross-validated downstream performance, (4) interpret the top loadings for business insight, and (5) guard against leakage and overfitting in a production pipeline?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["DoorDash","NVIDIA","Plaid"]},{"id":"q-1068","question":"Design an online robust PCA for a streaming fraud-detection pipeline: 50k events/sec, 1000 features with missing values. Describe incremental component updates, robust outlier handling (robust PCA or GoDec variants), missing-data strategy, drift monitoring (eigenvalue gaps, loadings stability, score distribution), and how you’d validate downstream classifier performance under tight memory/time constraints?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Meta","Oracle","Tesla"]},{"id":"q-1104","question":"In a production streaming recommender system, you maintain an incremental PCA basis on 1M users and 3k features, updated hourly. Design an online robust PCA pipeline that adapts to non-stationary covariances, handles missing data, and detects concept drift. Describe algorithm choices (incremental/robust variants, forgetting factors), when to re-train vs update, how to align with past loadings, validation of downstream models after projection, and monitoring for numerical stability?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Airbnb","Goldman Sachs","Google"]},{"id":"q-1128","question":"You have 50k samples, 1k gene-expression features with many missing values. Design an end-to-end Sparse PCA pipeline to produce 40 interpretable components. How would you handle missing data, choose sparsity vs components, validate downstream models, and assess stability and biological coherence of loadings across folds?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Adobe","Twitter","Two Sigma"]},{"id":"q-1153","question":"Design a PPCA-based dimensionality reduction pipeline for real-time telemetry data: 1B feature vectors daily, 500 real-valued features with missing values and skew, to feed a downstream anomaly detector. Explain fitting PPCA with EM, selecting k via BIC on a rolling window, comparing to standard PCA, handling streaming updates with forgetting factors, and validating in production?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Salesforce","Zoom"]},{"id":"q-1175","question":"In a factory IoT setting, 20 devices stream 40 features each (numeric, with occasional missing values). You want a beginner-friendly PCA-based anomaly detector on the edge. Describe how you would handle missing values, decide the number of components, and translate top loadings into actionable maintenance signals for operators, while keeping the model lightweight on-device?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Apple","Twitter"]},{"id":"q-1218","question":"You have a 40-feature numeric customer-survey dataset with some missing values and skewed distributions. You want a beginner-friendly PCA-based feature set for a churn-classification model. Describe preprocessing steps (imputation, transformations, outlier handling), how to choose the number of components, and how to translate top loadings into concrete business signals for a dashboard while keeping the pipeline lightweight?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Databricks","Google"]},{"id":"q-1249","question":"You're building a real-time risk-scoring system for cross-border payments. Data arrives as numeric features with occasional missing values and a few graph-derived signals, streaming at high velocity. You need an incremental PCA that adapts to non-stationary distributions and yields 40 components. Describe how you would: (a) choose/update the number of components under drift, (b) perform online imputation without data leakage, (c) keep loadings interpretable for dashboards, (d) coordinate PCA updates with downstream models to control drift, and (e) design a robust rollback strategy with governance in production?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Cloudflare","Coinbase","Uber"]},{"id":"q-1288","question":"Design a daily-updated PCA-based representation for streaming telemetry with 300 features per vector, many missing values and sparse signals. Outline preprocessing, choice of incremental PCA approach (IPCA vs randomized SVD), when to refresh the basis, how to align new loadings with the existing basis, and how to validate the downstream anomaly detector after dimensionality reduction. Include concrete knobs (batch size, forgetting factor, drift thresholds)?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Google","Snowflake"]},{"id":"q-1306","question":"Imagine a factory IoT setup where edge devices stream 60 numeric features (with occasional missing values). You need a beginner-friendly, PCA-based anomaly detector that runs on-device. Outline a concrete pipeline: (a) how to handle missing values, (b) how to standardize and fit PCA (including incremental options), (c) how to decide the number of components for robust anomaly signals, and (d) how to translate top loadings into actionable operator cues on a dashboard or device indicator?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["MongoDB","Tesla","Uber"]},{"id":"q-1350","question":"Design a privacy-preserving, incremental PCA system for a fintech platform with 2M daily sessions and 150 numeric features, where missing values occur and regulatory privacy requires differential privacy. You must deliver a 50-component representation, support federated updates, monitor drift, and keep loadings interpretable for dashboards. Describe architecture, privacy budget, and an evaluation plan; include a concrete update protocol?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["PayPal","Salesforce"]},{"id":"q-1387","question":"Design an online, privacy-preserving PCA for streaming multi-tenant transaction data to support real-time fraud detection. Specify how to perform incremental PCA with a sliding window, apply differential privacy to loadings, detect drift and decide when to refresh the basis, handle missing values online, allocate the privacy budget, and validate downstream models under DP constraints. Include concrete metrics and thresholds?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Cloudflare","MongoDB","PayPal"]},{"id":"q-1467","question":"In a real-time analytics pipeline that ingests 50k vectors/sec, each with 128 features (some missing), you previously computed offline **PCA** on historical data. Design a streaming, incremental **PCA** approach to (a) decide when to refresh the basis, (b) handle missing values on arrival, (c) monitor component drift, and (d) validate downstream models after dimensionality reduction?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Google","Stripe","Twitter"]},{"id":"q-1512","question":"Design a real-time PCA-based anomaly detector for a streaming platform like Airbnb where a listing's feature vector mixes 150 numeric sensor values and 30 one-hot categorical indicators; describe an incremental PCA workflow that handles mixed data, missing values, and concept drift while maintaining interpretability and low latency; specify preprocessing, component refresh triggers, evaluation, and how to map loadings back to actionable signals?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Airbnb","Salesforce","Scale Ai"]},{"id":"q-1534","question":"Edge PCA on 30 numeric sensors: design a beginner-friendly pipeline that runs on a Raspberry Pi for real-time data compression and anomaly flagging. Describe normalization and simple imputation, how to pick components (explained variance with knee), how to interpret top loadings for operators, and a lightweight drift check over a day with few dependencies?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["IBM","NVIDIA","Uber"]},{"id":"q-1703","question":"Scenario: A healthcare analytics team has a dataset with 100 predictors, mix of continuous and binary indicators, plus missing values. They want to apply PCA to reduce to 5 components for a dashboard and as features for a simple logistic classifier. Describe the exact preprocessing steps, including handling missing data, dealing with binary features during PCA, scaling, and how you would decide the number of components. How would you translate the top loadings into interpretable dashboard signals for clinicians, and how would you validate this approach on a small holdout set?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Google","IBM","Two Sigma"]},{"id":"q-1863","question":"You're building a real-time fraud-detection pipeline that ingests 200 features per transaction, with many outliers and missing values. You choose RPCA to reduce to 6 components for an online anomaly detector and a logistic classifier. Outline concrete steps: how to impute/mangle missing data for RPCA, how to handle heavy tails, how to select k using cross-validated reconstruction error with a complexity penalty, how RPCA would differ from standard PCA in this scenario, how to implement incremental RPCA updates (forgetting factor, windowed EM) in streaming, and how to map top loadings to business signals in a dashboard and detect component drift across batches?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Coinbase","Discord","Snap"]},{"id":"q-2034","question":"Design an online, memory-bounded PCA system for a fleet of 1,000 warehouse-edge sensors producing 200 features per vector. Data include missing values and heavy-tailed noise. Describe end-to-end: (a) how many components to retain under a fixed RAM, (b) how to incrementally update the PCA basis and trigger refresh on drift, (c) online imputation for missing values, (d) how to translate top loadings into actionable alerts for maintenance or throughput, and (e) how to validate offline and online performance. Include concrete metrics and thresholds?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Anthropic","Databricks","Instacart"]},{"id":"q-2106","question":"You have a production-monitoring dataset with 60 numerical telemetry features and 30 binary indicators across 20 services. For a PCA-based compression to 5 components powering a dashboard health score, outline practical preprocessing (imputation, scaling, handling binary features), how to pick the 5 components (explained variance threshold and cross-validated downstream anomaly detection), and how to translate the top loadings into actionable operator signals, keeping the pipeline lightweight?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Hashicorp","PayPal"]},{"id":"q-2164","question":"In a live ad-placement system, a streaming feature set of 320 numeric features arrives at 5 Hz with intermittent missing values and occasional outliers. Design an online, robust PCA (sliding-window, robust covariance, and outlier-resilient loadings) that maintains a 40-component basis. Explain how you would (1) update the basis with drift detection, (2) handle missing data online without corrupting the basis, (3) translate top loadings into low-latency serving signals, and (4) validate downstream models under non-stationarity?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Discord","DoorDash","Twitter"]},{"id":"q-837","question":"In a dataset with 120k samples and 200 features, after standardizing, you fit PCA and observe 95% variance explained by the first 8 components. How would you validate using PCA for downstream linear regression, decide the number of components, and interpret the top loadings? Consider missing values and large-scale data in your answer?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Amazon","Google","Plaid"]},{"id":"q-934","question":"Suppose a streaming analytics pipeline ingests 10k new vectors daily, each with 180 features, and PCA was computed on historical data. Describe an end-to-end approach to decide when to refresh the PCA vs keep the existing basis, how to measure component drift, how to align new loadings with the old basis, how to handle missing values in streaming data, and how to validate downstream models after dimensionality reduction?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Citadel","Discord","Hashicorp"]},{"id":"q-1117","question":"You're adding a real-time AI-powered product search for an Instacart-like app. With 8,000 concurrent users during lunch peak, design a beginner-friendly performance test that isolates the ML inference path from normal search, including cache warming and a clear success criteria?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Instacart","Snap","Zoom"]},{"id":"q-1210","question":"You're introducing a search API for a streaming e-commerce platform during a flash sale; expect 8k concurrent users, target median latency under 120ms and p99 under 250ms. The stack uses Redis caching with a relational DB. Design a beginner-friendly performance test plan to validate this, including tools, test data strategy, ramp pattern, metrics (p50, p90, p95, p99, error rate), and how you identify bottlenecks without affecting production?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["DoorDash","Netflix","Snowflake"]},{"id":"q-1533","question":"You're benchmarking a real-time bidding platform that processes 200k events per second with a 150ms SLA, deployed on Kubernetes with horizontal autoscaling and external services (ad server, fraud check, payment). Design a practical performance testing plan to identify bottlenecks and validate autoscaling, including workload mix, metrics, tools, and failure scenarios?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["IBM","LinkedIn"]},{"id":"q-1697","question":"You're benchmarking a real-time IoT telemetry pipeline: 2M events/sec ingested into Kafka, processed by Spark Structured Streaming, stored to a warehouse, and routed to a ML inference service with a 100ms SLA. Design a performance test plan to validate end-to-end latency, backpressure handling, and autoscaling when downstream latency spikes cause backlog. Include workload mix, metrics, tooling, and failure scenarios?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["Google","MongoDB","PayPal"]},{"id":"q-1712","question":"In a real-time payments microservice stack used by Amazon/PayPal, the /payments/process path calls fraud checks, credit checks, and a settlement queue. Bursts occur daily, with architectures needing to verify latency SLAs, error budgets, and autoscaling. Design a practical performance-test plan: workload model, test harness, metrics, failure scenarios, and how you would handle external-service variability and idempotency?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["Amazon","PayPal"]},{"id":"q-1803","question":"Design a performance-test for a real-time trade-confirmation service handling 150k events/s with an 8 ms SLA (99th percentile) during market open. System: Kubernetes, Kafka, Postgres, Redis; multi-region. Validate autoscaling, backpressure, circuit breakers, and tail latency under bursty traffic and regional failover. Outline workload, metrics, tools, and failure scenarios?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["Amazon","IBM","Robinhood"]},{"id":"q-2107","question":"Design a performance-test plan for a real-time personalized recommender serving 3 regional markets and relying on a centralized feature-flag service to drive A/B experiments. The flag service can throttle or fail under load. Outline traffic modeling, latency/outage injection, metrics (end-to-end P95/P99, throughput, error rate), backpressure handling, circuit-breakers, and graceful degradation criteria to validate SLOs?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Databricks","Tesla","Twitter"]},{"id":"q-2147","question":"You're performance-testing a ride-booking app's driver-matching API used by 3,000 concurrent riders during a rainstorm. The API uses an in-memory queue before dispatching to drivers. Design a beginner-friendly test plan to quantify end-to-end latency, identify bottlenecks in queue processing, and validate the impact of increasing the queue size?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Hugging Face","IBM","Meta"]},{"id":"q-472","question":"You're load testing a high-frequency trading platform that processes 100K requests/second. Your load generator becomes the bottleneck. How would you design a distributed load testing architecture to accurately simulate production traffic patterns?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Goldman Sachs","Two Sigma"]},{"id":"q-501","question":"You're testing a grocery delivery app like Instacart that handles 10,000 concurrent users during peak hours. How would you design a performance testing strategy to identify bottlenecks in the order processing pipeline?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Instacart","Tesla"]},{"id":"q-531","question":"You're load testing a food delivery platform's order processing system. How would you design a performance testing strategy to identify bottlenecks during peak lunch hours (12-2 PM) when order volume increases 10x?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Citadel","DoorDash"]},{"id":"q-557","question":"You're load testing a trading platform that processes 10,000 orders/second. Your load generator shows 95th percentile latency at 200ms, but actual users report 2-3 second delays. What's happening and how would you diagnose it?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["NVIDIA","Robinhood"]},{"id":"q-586","question":"How would you measure and optimize the performance of a REST API endpoint that's responding slowly?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Plaid","Uber"]},{"id":"q-996","question":"Design a performance testing plan for a MongoDB-backed ride-hailing backend where a single /alloc-trip endpoint coordinates availability updates and cross-service trip allocations under production-like bursts up to 50k RPS; outline how you would identify bottlenecks across DB, services, and network, and concrete steps to reduce tail latency?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["Google","MongoDB","Uber"]},{"id":"gh-40","question":"What is Performance Testing and how does it differ from Load and Stress Testing?","channel":"performance-testing","subChannel":"load-testing","difficulty":"beginner","tags":["perf","testing"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-41","question":"What are the different types of performance testing and when would you apply each type in a real-world scenario?","channel":"performance-testing","subChannel":"load-testing","difficulty":"intermediate","tags":["perf","testing"],"companies":["Amazon","Google","Meta"]},{"id":"q-237","question":"How would you design a distributed load testing setup using k6 with multiple cloud regions to simulate 100k concurrent users while avoiding rate limiting and ensuring accurate metrics collection?","channel":"performance-testing","subChannel":"load-testing","difficulty":"intermediate","tags":["jmeter","k6","gatling","locust"],"companies":["Amazon","Google","Netflix","Stripe","Uber"]},{"id":"q-210","question":"How would you implement comprehensive CPU profiling with flame graphs using clinic.js and async hooks to identify performance bottlenecks in a Node.js microservice handling concurrent requests, including production considerations and memory leak detection?","channel":"performance-testing","subChannel":"profiling","difficulty":"intermediate","tags":["cpu-profiling","memory-profiling","flame-graphs"],"companies":["Amazon","Cloudflare","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-280","question":"What is the difference between CPU profiling and memory profiling, and when would you use a flame graph?","channel":"performance-testing","subChannel":"profiling","difficulty":"beginner","tags":["cpu-profiling","memory-profiling","flame-graphs"],"companies":["Amazon","Google","Microsoft","Netflix"]},{"id":"q-1024","question":"You're building a prompt-routing system for a consumer-support assistant serving Apple, Airbnb, and Snap customers. It must decide auto-response, request clarification, or escalate to a human agent, based on intent, risk, PII presence, and policy compliance. Describe end-to-end design, including a 3-template prompt bank (concise, friendly, authoritative), a safety/brand-voice rubric, and a minimal Python prototype that routes auto vs escalate under edge cases. Include a testing plan?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Airbnb","Apple","Snap"]},{"id":"q-1037","question":"You're building a dynamic prompt orchestration system for Scale AI and MongoDB enterprise-support chatbots. It must select from a bank of five templates (concise, empathetic, technical, authoritative, business-friendly) based on user intent, data sensitivity, and risk signals, while applying strict safety guardrails to prevent prompt injection and data leakage. Describe the architecture, routing rules, and a minimal Python prototype that demonstrates template selection and a veto guardrail for edge cases?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["MongoDB","Scale Ai"]},{"id":"q-1119","question":"You're building a beginner-friendly prompt routing module for a multilingual customer-support chatbot serving Snowflake and Airbnb users. Design a two-step prompt selection: first classify intent (order_status, account_help, security_alert), then select a single template from (concise, friendly, authoritative) that preserves safety and privacy. Provide the concrete routing rules and a tiny Python prototype that demonstrates intent classification and template selection with sample inputs?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Airbnb","Snowflake"]},{"id":"q-1137","question":"Design a multilingual prompt evaluation pipeline for a Tesla/Square customer-support bot. It must detect language, route prompts to language-specific template banks, apply safety gates for PII and injection, and track drift metrics to trigger template updates. Provide architecture and a minimal Python prototype that returns a selected template and a veto flag?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Square","Tesla"]},{"id":"q-1206","question":"You're building a prompt lifecycle service for a multi-tenant chat assistant used by Tesla support and MongoDB customers. It must manage versioned templates, canary rollouts, per-tenant experiments, and safe rollback if a new version underperforms or violates safety guards. Describe the architecture, data model, and provide a minimal Python prototype that resolves the tenant's latest approved version and supports rollback via a veto gate?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["MongoDB","Tesla"]},{"id":"q-1217","question":"You're building a budgeted prompt engine for a multilingual support bot. With a 60-token cap for prompts in English and Spanish, design a rule-based condenser that preserves intent, routes to one of three templates (concise, empathetic, clarifying), and rejects unsafe prompts. What would the architecture look like, and provide a minimal Python prototype that compresses input to fit the budget and demonstrates template routing?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Google","Lyft","Microsoft"]},{"id":"q-1317","question":"You’re building a privacy-preserving prompt pipeline for a customer-support chatbot that must operate under GDPR. Outline a design to redact PII (emails, phone numbers) from prompts before feeding them to an LLM, while preserving intent to route to three templates (concise, empathetic, escalate). Include a minimal Python prototype that demonstrates redaction and routing, and discuss edge-case testing and auditability?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Anthropic","Google"]},{"id":"q-1455","question":"You're building a beginner-friendly, client-side prompt calibrator for a real-time support chat used by Tesla, Uber, and Scale AI customers. Design a scoring rubric that evaluates prompts on clarity, safety, and bias risk. Implement a tiny TypeScript prototype that: 1) scores prompts with the rubric, 2) routes to one of three templates (concise, empathetic, authoritative), and 3) flags prompts needing human review. Include basic tests and a sample input?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Scale Ai","Tesla","Uber"]},{"id":"q-1842","question":"Scenario: a large enterprise runs a prompt orchestration layer that routes user prompts to tenant-specific policies and models. You must design a dynamic gating layer that preserves privacy across tenants, adheres to latency budgets, and enforces safety guardrails against prompt injection. Describe the architecture, the routing rules, and provide a minimal Python prototype that demonstrates the gating decision (tenant, model, template) and a pluggable sanitizer?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["Citadel","IBM","Meta"]},{"id":"q-1852","question":"You're building a real-time, multi-tenant prompt router for an all-in-one chat assistant used by Instacart, Tesla, and Netflix employees. The system must route each user prompt to one of three personas: support-centric, revenue-aware, and compliance-oriented, based on user role, prior interactions, context window, and explicit data-sensitivity cues. It should apply a safety veto for prompts that could leak policy or PII, and adjust routing to meet SLA targets. Describe the architecture, routing rules, and provide a minimal Python prototype that demonstrates persona selection and a veto gate for edge cases. How would you measure latency, correctness, and safety?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Instacart","Netflix","Tesla"]},{"id":"q-2032","question":"You're building a multilingual prompt-routing system for a live Discord-like chat platform. Design an approach to detect language, assess safety risk, and route prompts to one of four templates (concise, friendly, formal, safety-first). Include data schemas, routing rules, and a minimal Python prototype that demonstrates language detection, risk scoring, and template selection?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Databricks","Discord"]},{"id":"q-447","question":"You're building a prompt for a customer service chatbot that needs to extract order details from unstructured user messages. How would you design the prompt to handle variations like 'I need to cancel order #12345' vs 'Can't find my recent purchase 12345' while maintaining high accuracy?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["DoorDash","Google","NVIDIA"]},{"id":"q-450","question":"You're building a prompt optimization system for a large language model API. The system needs to automatically improve prompt performance while maintaining safety constraints. How would you design an architecture that balances prompt effectiveness with content safety, and what metrics would you track?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["Apple","Robinhood","Tesla"]},{"id":"q-473","question":"You're building a chatbot for Instacart's customer service. How would you design a prompt template that handles both order status inquiries and refund requests while maintaining consistent tone and preventing prompt injection?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Goldman Sachs","Instacart","Netflix"]},{"id":"q-502","question":"How would you design a prompt engineering system to handle multi-turn conversations with context windows, ensuring consistent persona adherence while managing token limits and preventing prompt injection attacks?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["IBM","Meta","Zoom"]},{"id":"q-532","question":"You're building a prompt engineering system for a cloud infrastructure tool. How would you design prompts to handle ambiguous user input like 'setup database' while maintaining context and preventing hallucination?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Google","Hashicorp","Stripe"]},{"id":"q-558","question":"You're building a prompt optimization system for a large language model serving 10M+ daily requests. How would you design a system to automatically detect and mitigate prompt injection attacks while maintaining 99.9% uptime?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["Coinbase","NVIDIA","PayPal"]},{"id":"q-587","question":"How would you design a prompt to extract structured data from unstructured text while handling edge cases and ensuring consistent output format?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Meta","Snowflake","Zoom"]},{"id":"q-892","question":"You're building a beginner-friendly prompt evaluation harness for a customer-support chatbot. Given user prompts about orders or refunds, design a lightweight, rule-based template selector that picks among three templates (concise, friendly, authoritative). How would you implement and test this with a tiny Python prototype that scores templates on safety, tone, and length?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Coinbase","Google","Lyft"]},{"id":"q-251","question":"How would you implement a DSPy optimizer to automatically improve few-shot prompts for a classification task using BootstrapFewShot with evaluation metrics?","channel":"prompt-engineering","subChannel":"optimization","difficulty":"intermediate","tags":["prompt-tuning","dspy","automatic-prompting"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"q-198","question":"How would you design a multi-layered guardrail system to prevent prompt injection and jailbreak attacks while maintaining legitimate user functionality, and what are the key trade-offs between security and user experience?","channel":"prompt-engineering","subChannel":"safety","difficulty":"beginner","tags":["jailbreak","guardrails","content-filtering"],"companies":["Amazon","Google","Meta"]},{"id":"q-226","question":"How would you design a prompt-engineering system that dynamically selects between chain-of-thought, few-shot, and zero-shot prompting based on real-time performance metrics and task complexity?","channel":"prompt-engineering","subChannel":"techniques","difficulty":"advanced","tags":["chain-of-thought","few-shot","zero-shot"],"companies":["Amazon","Google","Meta","Microsoft","OpenAI"]},{"id":"q-196","question":"How would you implement a rate-limited async HTTP client using aiohttp and asyncio.Semaphore to handle 1000 requests while respecting API limits?","channel":"python","subChannel":"async","difficulty":"intermediate","tags":["asyncio","aiohttp","concurrency"],"companies":["Amazon","Google","Meta"]},{"id":"q-224","question":"How would you implement a thread-safe singleton in Python with lazy initialization, proper type hints, and discuss the trade-offs between metaclass, decorator, and module-level approaches for a production system?","channel":"python","subChannel":"best-practices","difficulty":"advanced","tags":["pep8","typing","testing"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-178","question":"In Python, when do `is` and `==` return different results, and why does this happen with object identity vs equality?","channel":"python","subChannel":"fundamentals","difficulty":"intermediate","tags":["python","basics"],"companies":["Amazon","Google","Meta","Microsoft","Uber"]},{"id":"q-1085","question":"You’re building a real-time log processor in Python for a messaging app. You receive a stream of JSON lines like {\"user_id\": 123, \"event\": \"message_sent\", \"ts\": 1610000000}. Emit only the first event per (user_id, event) within a 60-second sliding window. Design a memory-bounded, generator-based pipeline that reads from an iterable of strings and yields deduplicated dicts in order. Include edge cases and memory considerations?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Hugging Face","Slack"]},{"id":"q-1149","question":"In a streaming data etl, you're given a JSONL stream where each line is a JSON object with 'host' and 'value'. Implement a memory-efficient Python generator top_n_hosts(n, lines) that yields the top N hosts by total value after consuming the stream, without keeping the entire per-host totals in memory. Use a min-heap to maintain current top-N and skip invalid lines?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Google","Meta"]},{"id":"q-1250","question":"Design an asynchronous NDJSON ingestion pipeline in Python that reads an async iterable of lines, validates each with a Pydantic model, and routes records to per-tenant partitions based on the 'tenant_id'. Implement bounded queues per partition with a total memory cap, guarantee per-tenant in-order processing, and a slower downstream sink that creates backpressure. Provide a test plan with skewed partitions and slow sinks?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Snowflake","Uber"]},{"id":"q-1275","question":"Design an advanced async Python pipeline: NDJSON lines arrive via a TCP socket, each is validated with a Pydantic model, then a deduplication stage uses a sliding-window Bloom filter to emit each event at most once in a 24-hour window. The pipeline must stay memory-bounded, support backpressure to the producer, and allow the downstream sink to pace itself. Provide a concrete test plan with clock skew and bursty traffic?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["DoorDash","Hugging Face","Two Sigma"]},{"id":"q-1285","question":"Given a list of dictionaries representing users, implement normalize_users(users) that returns a list of User dataclass instances with fields id:int, name:str, signup_ts:datetime, is_active:bool. Coerce id from str to int, parse signup_ts ISO8601 strings, interpret is_active from common truthy values, and fill defaults for missing fields. If any record is invalid, raise ValueError with the indices of invalid records?","channel":"python","subChannel":"general","difficulty":"beginner","tags":["python"],"companies":["Apple","MongoDB","Uber"]},{"id":"q-1476","question":"Design an asynchronous Python engine to join two JSON event streams by id within a 5-second window. Streams A and B are async iterables yielding {'id': str, 'ts': int, 'payload': Any}. Emit matched pairs to a sink when both sides have an event with same id within the 5s window. Route late events past the lateness bound to a 'late' sink. Enforce a global memory bound for buffered events and propose a test plan with out-of-order arrivals?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Airbnb","Lyft","Stripe"]},{"id":"q-1639","question":"Write a Python function process_events(file_path) that reads a newline-delimited JSON (JSONL) log file of Stripe-like events. Each line is a JSON object with 'type' (str) and 'data' (dict with 'id' key). The function should return a dict mapping event 'type' to count, skipping lines with missing keys or invalid JSON, and writing errors to a separate errors.log. Make it memory-efficient using a streaming approach?","channel":"python","subChannel":"general","difficulty":"beginner","tags":["python"],"companies":["Hugging Face","Stripe"]},{"id":"q-1675","question":"Design a memory-bounded streaming processor in Python for a JSONL event stream with fields: timestamp, service, event_type, payload. Group by (service, event_type), maintain an in-order per-key queue with bounded capacity, and emit a 60-second rolling histogram of payload sizes per group to a downstream sink. Ensure backpressure via per-key queues and a global memory cap, and provide a test plan with skewed keys and slow sinks?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Airbnb","Amazon","Scale Ai"]},{"id":"q-1783","question":"Implement a memory-efficient Python function top_n_words(filepath, n) that streams a text file line by line to find the n most frequent words. Normalize case, strip punctuation, ignore empty tokens, and return a list of the top n words sorted by frequency. Ensure it never loads the whole file into memory?","channel":"python","subChannel":"general","difficulty":"beginner","tags":["python"],"companies":["Amazon","Google","PayPal"]},{"id":"q-1867","question":"Design a Python async NDJSON ingestion pipeline that reads lines from an async source, validates each line against a versioned Pydantic model, supports hot-reloadable schema versions from a shared config, and guarantees exactly-once delivery with an idempotent sink and per-record deduplication, all while enforcing bounded memory and backpressure. How would you implement it?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Amazon","Netflix"]},{"id":"q-1884","question":"Implement an asynchronous Python data transformer for a JSONL event stream where each line includes a 'version' field. Build transform_stream(input: AsyncIterable[str], schemas: Dict[int, Type[BaseModel]]) that validates each line against its versioned Pydantic model, applies a version-aware field mapping, and outputs transformed JSONL lines to a downstream sink while guaranteeing per-version in-order processing, memory-bounded streaming, and backpressure. Include a small test scaffold showing a v1→v2 migration?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Google","NVIDIA","Tesla"]},{"id":"q-2090","question":"Design a memory-bounded streaming top-K aggregator in Python. Data arrives as an async iterable of numeric events with timestamps. Implement a class that maintains an approximate top-10 using a Count-Min Sketch plus a min-heap, supports a sliding time window, and exposes add(value, ts) and get_top_k() reflecting the current window. Describe API, memory guarantees, and a test plan for bursty traffic?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Anthropic","Databricks","Google"]},{"id":"q-474","question":"Write a Python function that takes a list of integers and returns the sum of all even numbers. How would you handle edge cases?","channel":"python","subChannel":"general","difficulty":"beginner","tags":["python"],"companies":["Bloomberg","Microsoft","Robinhood"]},{"id":"q-503","question":"How would you implement a distributed rate limiter using Redis with sliding window algorithm to handle 10,000 requests per second across multiple API servers?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Google","LinkedIn","Zoom"]},{"id":"q-559","question":"You're building a data processing pipeline that needs to handle large CSV files efficiently. How would you implement a memory-efficient solution using Python generators to process files that don't fit in RAM?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Apple","Snap","Snowflake"]},{"id":"q-588","question":"How would you implement a rate limiter using Python's asyncio to prevent API abuse while maintaining high throughput?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Apple","DoorDash","MongoDB"]},{"id":"q-852","question":"Design a memory-efficient streaming dedup pipeline in Python that reads a multi-GB log file line-by-line and prints only the first occurrence of each unique line, skipping duplicates, while persisting dedup state across restarts. Use a probabilistic data structure with a configurable false-positive rate and provide a minimal runnable snippet?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["IBM","MongoDB","Robinhood"]},{"id":"q-975","question":"Design an asynchronous NDJSON pipeline in Python that ingests lines of JSON from an async source, validates each line against a Pydantic model, applies a lightweight transform, and yields results to a downstream consumer. Ensure memory stays bounded when the consumer slows by using a bounded asyncio.Queue?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Amazon","Coinbase","Stripe"]},{"id":"q-1048","question":"You're building a React Native field-ops app with intermittent connectivity that must display offline-first tasks with images and support incremental sync across devices. Describe architecture and provide a small implementation sketch using a local DB (WatermelonDB or Realm), a sync service, and conflict resolution. What edge cases and tests would you include?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["Google","Hugging Face","Tesla"]},{"id":"q-1065","question":"You're building a React Native app for live collaboration that streams up to 4 simultaneous camera feeds using WebRTC. Describe end-to-end architecture for capture, encoding, and transport, how you'd implement backpressure and frame pacing to sustain ~30fps per feed, and provide a small implementation sketch (camera hook + simple backpressure queue) in code?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["Discord","Google","NVIDIA"]},{"id":"q-1098","question":"You're building a React Native app for field technicians that must collect GPS and sensor data in the background every 15 minutes, even when the app is suspended. Describe a cross‑platform architecture using Android WorkManager and iOS BGTaskScheduler, a minimal RN bridge, and a small code sketch of a BackgroundTaskManager that schedules tasks, persists deadlines, and handles results. Include edge cases like battery saver, app termination, and user-initiated cancel?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Google","Microsoft"]},{"id":"q-1265","question":"You're building a real-time collaborative whiteboard in React Native that must support up to 1,000 participants with low latency and offline fallback. Describe an end-to-end architecture using WebRTC data channels for deltas, WebSocket signaling, and a CRDT for merging concurrent strokes. Include data model (stroke encoding, timestamps), backpressure handling, and a small code sketch implementing a delta encoder and an in-app delta queue that feeds an RN Canvas/Skia surface, with clear acceptance criteria?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Cloudflare","Two Sigma"]},{"id":"q-1537","question":"You're building a React Native app for field engineers in regulated environments. It collects GPS and accelerometer telemetry, signs each record with a device key, and batches uploads to a backend with offline-first guarantees and tamper-evident auditing. Describe the architecture, data flow, and provide a small code sketch: a LocalAuditLog module backed by SQLite and TweetNaCl for Ed25519 signing, plus a transport adapter with exponential backoff and key-rotation handling. Include edge cases around tenant isolation, data retention, and replay protection?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["Cloudflare","Goldman Sachs","Zoom"]},{"id":"q-1790","question":"You're building a cross-platform React Native app for field technicians that must display a map with thousands of POIs, support offline caching of tiles and markers, and perform incremental sync of POI updates when online. Design a data layer and UI flow to support offline-first maps, marker clustering, and conflict resolution. Provide a small code sketch for a WatermelonDB/Realm model and a MarkerLayer component, plus test ideas?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["DoorDash","Meta","Oracle"]},{"id":"q-2072","question":"You’re building a beginner-friendly React Native gallery that shows a 2x2 grid of images loaded from a remote JSON endpoint. Describe how you would implement offline-first caching with AsyncStorage, automatic data refresh on reconnect, and a small hook sketch to fetch and cache the image list (including error handling and a simple retry strategy)?","channel":"react-native","subChannel":"general","difficulty":"beginner","tags":["react-native"],"companies":["LinkedIn","Oracle"]},{"id":"q-475","question":"You're building a React Native app with complex animations that need to run at 60fps. The app has multiple animated components including a custom carousel, gesture-driven interactions, and background video processing. How would you optimize performance to maintain smooth animations?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Amazon","Google","NVIDIA"]},{"id":"q-504","question":"How would you implement a custom button component in React Native that handles both iOS and Android platform-specific styling while maintaining consistent behavior?","channel":"react-native","subChannel":"general","difficulty":"beginner","tags":["react-native"],"companies":["Meta","Robinhood","Stripe"]},{"id":"q-533","question":"How would you optimize a React Native app with 50+ screens that's experiencing slow navigation and memory leaks, particularly on lower-end devices?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Citadel","PayPal"]},{"id":"q-560","question":"You're building a React Native app that needs to display a list of user profiles with images. The list should be performant with 1000+ items and support pull-to-refresh. How would you implement this using FlatList and what optimizations would you apply?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["Databricks","Microsoft"]},{"id":"q-589","question":"How do you handle different screen sizes and orientations in React Native?","channel":"react-native","subChannel":"general","difficulty":"beginner","tags":["react-native"],"companies":["Hashicorp","Instacart","LinkedIn"]},{"id":"q-183","question":"What are Native Modules in React Native, when should you use them, and what are the key performance and threading considerations?","channel":"react-native","subChannel":"native-modules","difficulty":"beginner","tags":["native","bridge"],"companies":null},{"id":"q-206","question":"How would you optimize React Native list performance with Hermes and Reanimated when dealing with 10k+ items containing complex animations?","channel":"react-native","subChannel":"performance","difficulty":"advanced","tags":["hermes","reanimated","profiling"],"companies":["Airbnb","Coinbase","Meta","Microsoft","Uber"]},{"id":"q-233","question":"How does the Hermes engine improve React Native app startup performance compared to JavaScriptCore, and what are the specific trade-offs?","channel":"react-native","subChannel":"performance","difficulty":"beginner","tags":["hermes","reanimated","profiling"],"companies":["Airbnb","Meta","Microsoft","Netflix","Salesforce","Shopify"]},{"id":"q-1023","question":"Design a centralized, tamper-evident logging pipeline for 6 RHEL hosts. Include: enable persistent journald, forward logs over TLS to a central collector, configure rotation/retention, protect in transit with certificate-based auth, and a rollback/validation plan that proves delivery during outages. Outline testing steps and failure scenarios?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Apple","Hashicorp"]},{"id":"q-1118","question":"On a RHEL 9 host, a service named app writes to /srv/app/data. After deployment, SELinux denials prevent writes. Without disabling SELinux, outline exact, implementable steps to restore functionality, including identifying the AVC, creating a targeted policy module with audit2allow, loading it, labeling the data directory, and validating the fix with a controlled write and audit checks?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Apple","Hugging Face","Twitter"]},{"id":"q-1148","question":"On a RHEL 8 server, implement a daily backup of /home/userdata to /backup/userdata-YYYYMMDD.tgz, exclude caches and temp dirs, preserve permissions, and generate a sha256 checksum. Schedule at 02:30 via cron and rotate backups to keep last 7 days. Provide commands and a script outline?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Goldman Sachs","Instacart","Lyft"]},{"id":"q-1169","question":"Design and implement an encrypted root on LVM for a production RH host. Boot partition remains unencrypted; the root filesystem sits on a LUKS2 container inside an LVM PV, with a keyfile for unattended boot. Provide a concrete, command-level plan including crypttab, fstab, initramfs (dracut) generation, and grub configuration to ensure the system boots automatically after rotation?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Bloomberg","Plaid","Snap"]},{"id":"q-1192","question":"On a RHEL8 host, set up a minimal Python HTTP server listening on port 8080, accessible only from 192.168.100.0/24. Use a non-root user, a systemd service, SELinux port labeling, and firewalld rules. Provide exact commands to create the service, configure semanage for port 8080, apply firewall rules, and test from a client. Address potential SELinux and port conflict caveats?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Instacart","LinkedIn"]},{"id":"q-1247","question":"On a RHEL 8 server, a web service listens on 127.0.0.1:9090. Configure firewalld to expose port 9090 only to the 10.1.0.0/24 management network, log drops, and persist across reboots. Provide exact commands, and describe test steps using curl from an allowed host and from a non-allowed host?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Amazon","Snap"]},{"id":"q-1399","question":"You manage a RHEL 8 server running SSH on port 22. To improve security, change SSH to listen on port 2222, disable root SSH login, and require key-based authentication, ensuring no downtime for existing sessions. List the exact commands and steps to implement this, including firewall and SELinux considerations, and how you verify connectivity afterwards?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Hugging Face","LinkedIn","MongoDB"]},{"id":"q-1484","question":"On a Red Hat-based host, you must deploy a statically compiled Go web app that runs on port 8080 behind firewalld with SELinux enforcing. The app should auto-start on boot, restart on failure, and log to rsyslog. Propose the concrete steps, files, and exact commands to configure systemd, firewalld, and SELinux contexts, ensuring minimal downtime during deploy?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["Meta","Salesforce"]},{"id":"q-1517","question":"On a RHEL 9 host, a custom service started via systemd fails to write logs to /var/www/app/logs after a patch, with SELinux enforcing. Describe a precise, minimal-risk remediation plan to identify and fix the root cause without broad permission grants, including AVC collection, targeted policy generation, and validation under load. What exact commands and steps would you perform?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Meta","Snap"]},{"id":"q-1577","question":"You must serve a static site from content stored on an NFS mount at /srv/www with SELinux enforcing on a RHEL-based system. The SELinux policy blocks httpd from reading the files. Describe the exact sequence of commands and configurations to allow Apache to serve the site without disabling SELinux or putting the system in permissive mode, including boolean toggles, labeling, and firewall rules?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["Lyft","Meta","Twitter"]},{"id":"q-1610","question":"Configure log rotation for a custom web app log path /var/log/myapp/*.log on a live Linux host. The log is written by a root-owned process and must rotate weekly, keep 4 copies, compress old logs, and after rotation reload nginx to reopen its handles. Provide a minimal, working logrotate.conf snippet and explain how this prevents log loss?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Coinbase","IBM","Plaid"]},{"id":"q-1649","question":"On a RHEL system with root on an LV named /dev/vg_rhel/root, describe a precise method to test a disruptive system update using an LVM snapshot: create a 20G snapshot, apply the update inside the snapshot, validate service health, and rollback by booting the live system from the original LV if needed. Include exact commands?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["Citadel","Google"]},{"id":"q-1679","question":"On a RHEL/CentOS host with two NICs (enp0s3 and enp0s8) connected to two switches, configure a 802.3ad (LACP) bond0 using NetworkManager. Bond should have a static IP 192.0.2.100/24, and both NICs must be slaves. Provide exact nmcli commands to create bond0, add slaves, bring it up, and verify; note required switch settings (LACP active on both ports)?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["Databricks","Scale Ai","Stripe"]},{"id":"q-1738","question":"On a RHEL-based host, configure firewalld to allow SSH access only from 192.0.2.0/24 and deny all other inbound SSH; ensure the changes persist after reboot and can be tested quickly; outline exact commands and verification steps, including revert plan?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Microsoft","Tesla","Zoom"]},{"id":"q-1756","question":"On a freshly provisioned Linux host (RHEL8), configure a new user 'audit' to log in exclusively via SSH key authentication, with a restricted shell rbash so only basic commands are allowed, and grant passwordless sudo to restart the 'auditd' service. Provide exact commands and edits to users, sshd_config, and sudoers, and how you would verify the setup?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Google","IBM"]},{"id":"q-1771","question":"On a RHEL 8 server, you must host a small static site with Nginx, ensure it starts on boot, expose only port 80, and serve content from /var/www/html with correct SELinux context. Provide exact commands to configure Nginx, firewall, and SELinux so SELinux stays enforcing?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Meta","Snap","Snowflake"]},{"id":"q-1804","question":"On a Red Hat-based host, a Python web app writes to /srv/app/data and /var/log/app. After a system update, SELinux denies these writes. Outline an operational plan to diagnose and restore write access without disabling SELinux, including commands for audit review, context restoration, and persistent policy adjustments?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Amazon","Meta","Square"]},{"id":"q-1982","question":"On a Linux host, create a project share at /srv/project where Alice can read/write and Bob can read only, with all others denied. Use POSIX permissions plus ACLs so new files created by Alice inherit Bob's read access. Provide exact commands to: 1) create the group, 2) add users, 3) set up the directory with setgid, 4) apply ACLs (explicit for Bob and default for new files), 5) verify with tests?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["DoorDash","Oracle","Zoom"]},{"id":"q-933","question":"On a fresh RHEL 9 installation with a 120 GB disk, implement an LVM layout: ROOT 40G, HOME 40G, VAR 40G, all using XFS. Create PV, VG, and LVs, format, and mount at /, /home, /var with fstab. Enable and configure firewalld to allow http and https. Ensure SELinux is enforcing. Create a non-root user 'dev' and add to the wheel group with sudo privileges. Show commands and rationale?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["Adobe","IBM","Microsoft"]},{"id":"gh-24","question":"What is DevSecOps and how does it differ from traditional DevOps security approaches?","channel":"security","subChannel":"application-security","difficulty":"advanced","tags":["security","devsecops"],"companies":["Amazon","Coinbase","Google","Microsoft","Uber"]},{"id":"gh-44","question":"How do you implement a comprehensive API security strategy that protects against common vulnerabilities while maintaining developer productivity?","channel":"security","subChannel":"application-security","difficulty":"beginner","tags":["api","service-mesh"],"companies":["Amazon","Microsoft","Morgan Stanley","PayPal","Stripe"]},{"id":"gh-69","question":"How does Zero Trust Security implement identity-based access control with micro-segmentation using modern cloud infrastructure and identity providers?","channel":"security","subChannel":"application-security","difficulty":"advanced","tags":["security","network"],"companies":null},{"id":"q-230","question":"How would you implement a Content Security Policy (CSP) with nonce-based inline script protection to prevent XSS while maintaining compatibility with third-party analytics?","channel":"security","subChannel":"application-security","difficulty":"intermediate","tags":["xss","csrf","sqli","ssrf"],"companies":["Airbnb","Google","Microsoft","Stripe","Uber"]},{"id":"q-276","question":"How would you design a secure job scheduling system for a microservices environment that prevents privilege escalation while ensuring reliable execution across distributed services?","channel":"security","subChannel":"application-security","difficulty":"advanced","tags":["systemd","cron","users","permissions"],"companies":null},{"id":"q-283","question":"What is the difference between XSS and CSRF attacks?","channel":"security","subChannel":"application-security","difficulty":"beginner","tags":["xss","csrf","sqli","ssrf"],"companies":["Amazon","Google","Meta"]},{"id":"q-359","question":"You discover a reflected XSS vulnerability in a search feature. The search term is displayed back to the user without sanitization. How would you fix this, and what's the difference between reflected XSS and stored XSS in terms of impact and remediation?","channel":"security","subChannel":"application-security","difficulty":"intermediate","tags":["xss","csrf","sqli","ssrf"],"companies":["Fortinet","Google","Tesla"]},{"id":"q-404","question":"You're building a financial trading platform at Jane Street. How would you design a secure authentication and authorization system that prevents XSS, CSRF, SQLi, and SSRF attacks while maintaining high performance for real-time trading data?","channel":"security","subChannel":"application-security","difficulty":"advanced","tags":["xss","csrf","sqli","ssrf"],"companies":["Canva","Jane Street","Miro"]},{"id":"q-423","question":"You discovered a critical security vulnerability in your team's production system that could expose customer data, but fixing it requires delaying a major product launch. How would you handle this situation, considering CVSS scoring, stakeholder communication, and risk-benefit analysis?","channel":"security","subChannel":"application-security","difficulty":"advanced","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-202","question":"How do passkeys implement passwordless authentication using public-key cryptography?","channel":"security","subChannel":"authentication","difficulty":"beginner","tags":["mfa","passkeys","zero-trust"],"companies":["Apple","Google","Meta","Microsoft","Okta"]},{"id":"q-241","question":"How would you implement JWT authentication with RS256 signing and refresh token rotation to prevent token replay attacks?","channel":"security","subChannel":"authentication","difficulty":"intermediate","tags":["jwt","oauth2","oidc","saml"],"companies":["Airbnb","Amazon","Apple","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-302","question":"Explain the technical differences between OAuth 2.0 authorization flows and OpenID Connect authentication, including token structures, validation patterns, and security considerations?","channel":"security","subChannel":"authentication","difficulty":"beginner","tags":["jwt","oauth2","oidc","saml"],"companies":["Amazon","Google","Meta","Microsoft","Salesforce","Stripe"]},{"id":"q-324","question":"How would you design a zero-trust video conferencing platform using WebAuthn passkeys with continuous authentication for enterprise users?","channel":"security","subChannel":"authentication","difficulty":"advanced","tags":["mfa","passkeys","zero-trust"],"companies":["Snowflake","Spotify","Zoom"]},{"id":"q-387","question":"Design a zero-trust authentication system for Snowflake's data warehouse that supports MFA, passkeys, and handles 100M+ daily auth requests. How would you prevent replay attacks while ensuring sub-100ms latency?","channel":"security","subChannel":"authentication","difficulty":"advanced","tags":["mfa","passkeys","zero-trust"],"companies":["Infosys","New Relic","Snowflake"]},{"id":"gh-70","question":"How does the TLS 1.3 handshake establish secure communication and what cryptographic mechanisms ensure perfect forward secrecy?","channel":"security","subChannel":"encryption","difficulty":"advanced","tags":["security","network"],"companies":["Amazon","Cloudflare","Google","Hashicorp","Netflix","Square"]},{"id":"q-179","question":"Explain how Perfect Forward Secrecy (PFS) works in TLS, describe the ECDHE key exchange mechanism, and analyze the security trade-offs compared to RSA key exchange?","channel":"security","subChannel":"encryption","difficulty":"intermediate","tags":["encryption","crypto"],"companies":["Amazon","Apple","Cloudflare","Google","Microsoft","Stripe"]},{"id":"q-295","question":"How does AES-256-GCM provide both confidentiality and integrity in a single cryptographic operation?","channel":"security","subChannel":"encryption","difficulty":"advanced","tags":["aes","rsa","tls","hashing"],"companies":["Amazon","Google","Meta"]},{"id":"q-310","question":"How does TLS 1.3 improve security compared to TLS 1.2?","channel":"security","subChannel":"encryption","difficulty":"intermediate","tags":["aes","rsa","tls","hashing"],"companies":["Amazon","Google","Meta"]},{"id":"q-337","question":"Design a secure key exchange system for autonomous vehicle communication between cars and infrastructure. How would you handle forward secrecy and key rotation in a high-mobility environment?","channel":"security","subChannel":"encryption","difficulty":"advanced","tags":["aes","rsa","tls","hashing"],"companies":["Apple","Cruise","Uber"]},{"id":"q-348","question":"You're designing a secure booking system for Expedia that handles payment data. How would you implement a hybrid encryption scheme using RSA for key exchange and AES-256-GCM for data encryption, and what specific security considerations would you address for PCI DSS compliance?","channel":"security","subChannel":"encryption","difficulty":"advanced","tags":["aes","rsa","tls","hashing"],"companies":["Amazon","Apple","Expedia","Microsoft","OpenAI","PayPal","Square","Stripe"]},{"id":"q-1234","question":"You operate a CDN edge platform that lets customers deploy WebAssembly modules for request processing. Outline a practical, auditable approach to securely load, validate, and sandbox these modules, covering authentication (signatures/attestation), host-function access, resource quotas, revocation, and incident response?","channel":"security","subChannel":"general","difficulty":"advanced","tags":["security"],"companies":["Apple","Cloudflare","Microsoft"]},{"id":"q-476","question":"How would you prevent SQL injection in a web application and what are the common attack vectors?","channel":"security","subChannel":"general","difficulty":"beginner","tags":["security"],"companies":["Lyft","Netflix"]},{"id":"q-505","question":"You're building a payment processing API that must handle PCI compliance. How would you design the architecture to ensure sensitive card data never touches your servers while maintaining low latency for payment validation?","channel":"security","subChannel":"general","difficulty":"advanced","tags":["security"],"companies":["Hugging Face","Microsoft","Square"]},{"id":"q-534","question":"How would you implement secure session management in a distributed web application to prevent session hijacking and fixation attacks?","channel":"security","subChannel":"general","difficulty":"intermediate","tags":["security"],"companies":["Airbnb","Google","Two Sigma"]},{"id":"q-561","question":"How would you implement secure session management for a web application using JWT tokens, and what are the key security considerations?","channel":"security","subChannel":"general","difficulty":"intermediate","tags":["security"],"companies":["Discord","MongoDB"]},{"id":"gh-71","question":"How does a Web Application Firewall (WAF) protect against OWASP Top 10 attacks at the application layer?","channel":"security","subChannel":"owasp","difficulty":"advanced","tags":["security","network"],"companies":["Akamai","Amazon","Cloudflare","Google","Microsoft"]},{"id":"q-255","question":"How would you implement OWASP ASVS L3 input validation for a REST API endpoint that accepts JSON payloads with nested objects?","channel":"security","subChannel":"owasp","difficulty":"intermediate","tags":["top10","asvs","samm"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-373","question":"How would you implement a comprehensive defense-in-depth strategy to prevent SQL injection attacks in a modern web application following OWASP Top 10 guidelines?","channel":"security","subChannel":"owasp","difficulty":"beginner","tags":["top10","asvs","samm"],"companies":["Amazon","Cloudflare","Google","Meta","Microsoft","Netflix"]},{"id":"q-1086","question":"Design a Snowflake CDC pattern for an SCD2 customer_dim using a stream on the source table. Explain how to implement an idempotent upsert via MERGE, how deletes are represented, and how to handle late-arriving changes to preserve history?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Google","NVIDIA"]},{"id":"q-1152","question":"Design a least-privilege access layer in Snowflake for a multi-tenant data lake spanning five domains (marketing, sales, finance, analytics, operations). Describe how you would implement ROW ACCESS POLICIES and MASKING POLICIES on a payments table (columns: id, customer_id, amount, card_number, region) to restrict data by region and user role, and how you would audit access?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Adobe","Databricks","Lyft"]},{"id":"q-1239","question":"You’re building a beginner Snowflake task: a large SALES table is routinely queried with date-range filters. Propose a minimal clustering solution to improve pruning. Include exact commands to add a single clustering key on SALE_DATE, how to monitor its effectiveness, and how to decide if reclustering is needed. Keep changes focused and explain validation steps with a simple test?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Hugging Face","PayPal"]},{"id":"q-1337","question":"You're ingesting data daily into table sensor_readings(device_id STRING, ts TIMESTAMP_NTZ, reading FLOAT). You frequently query last 24 hours per device. Propose a beginner-friendly optimization path, choosing clustering key, automatic clustering, or a materialized view, and include an exact query to compute per-device count and average reading for the last 24 hours?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["PayPal","Plaid","Tesla"]},{"id":"q-1475","question":"You have a Snowflake table raw_events with columns file_name STRING, payload VARIANT containing an array at payload:'events'. Each event is an object { event, timestamp, user: { id }, region, amount }. Design a beginner-friendly approach to compute daily total_amount and per-user purchases for the last 7 days using a temporary VIEW and LATERAL FLATTEN. Provide the exact query you would run?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Citadel","Cloudflare","Oracle"]},{"id":"q-1499","question":"You're building a Snowflake-based telemetry store: table `raw.events` (device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) ingests 50M+ rows daily. BI dashboards query per-device event_count and median reading for last 7 days and last 24 hours. Propose a pragmatic optimization, selecting between clustering, automatic clustering, and a rolling aggregate MV/table. Include exact commands to implement your approach and how you would validate improvements?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Databricks","Tesla","Zoom"]},{"id":"q-1567","question":"You're operating a Snowflake telemetry store with table raw.events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) ingesting 100M+ rows daily. BI requires per-device metrics: (a) last hour event_count, (b) last 24h median of payload.metrics.reading. Propose a production-ready strategy: data model, clustering choices vs automatic clustering vs materialized views, and how to validate performance. Include exact commands to implement and how to verify improvements?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Lyft","MongoDB"]},{"id":"q-1605","question":"Design a Snowflake-based near real-time anomaly-detection pipeline for telemetry events. Ingested table: raw_events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). Ingests 200M+ rows/day. You need to detect spikes in a metric inside payload (e.g., latency) on a per-device basis with a 5-minute window and surface alerts to an alerts table when a spike exceeds a dynamic threshold. Describe the architecture using Streams, Tasks, and possibly a Snowflake procedure; include DDLs to create the stream, a task schedule, sample SQL for the rolling compute, and how you would validate the pipeline end-to-end?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Airbnb","Scale Ai","Stripe"]},{"id":"q-1678","question":"You're provisioning a Snowflake warehouse for a multi-tenant SaaS app. All tenant data sits in one table: raw_events (tenant_id STRING, event_ts TIMESTAMP_NTZ, event_type STRING, payload VARIANT) ingesting 40M+ rows/day. Propose a hybrid approach: (1) add a composite clustering key (tenant_id, event_ts) for pruning, and (2) a rolling 7-day materialized view with per-tenant metrics. Include exact SQL commands to implement clustering, MV, and a validation query showing the improvement?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Cloudflare","Goldman Sachs","Snowflake"]},{"id":"q-1711","question":"In a Snowflake telemetry store with table raw.events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) ingesting 50M+ rows daily, implement a production-grade data retention pipeline that archives data older than 365 days to an external stage and keeps only the last 365 days in the main table, while preserving query performance on recent data. Describe the architecture and provide exact SQL commands to create the stage, stream, task, and archive procedure, plus a validation plan?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Microsoft","Oracle","Snap"]},{"id":"q-2024","question":"Implement row-level security for regional data access in Snowflake. Table `customer.sales` has `customer_id`, `region`, `amount`, `last_purchase_ts`. Roles `SA_US` and `SA_EU` should only see rows for their region. Describe and implement a Snowflake ROW ACCESS POLICY using a role-context and attach it to the table, then provide a sample query that would be allowed for role `SA_US` and a test plan to verify enforcement?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Apple","Hugging Face","Microsoft"]},{"id":"q-2048","question":"You're building a multi-tenant analytics warehouse in Snowflake. Ingested events go into raw.events(tenant_id STRING, event_ts TIMESTAMP_NTZ, payload VARIANT). BI needs per-tenant metrics: (a) count of events in the last 15 minutes, (b) 95th percentile of latency from payload.metrics.latency in the last 24 hours. Propose a production-ready architecture: data model (partitioning, clustering), streaming vs batch paths (streams/tasks vs MV), decision between automatic clustering and materialized views, and a concrete validation plan with exact SQL commands?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Amazon","Scale Ai","Square"]},{"id":"q-868","question":"Design a cross-account data-sharing solution in Snowflake for a multinational fintech requiring regional affiliates to access a shared dataset containing PII. How would you implement Secure Data Sharing, dynamic data masking, region-specific RBAC, and auditable access, while enabling updates to masking policies without breaking consumer queries?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Coinbase","Microsoft"]},{"id":"q-981","question":"How would you configure Snowflake so regional analysts can run ad-hoc queries on a shared dataset while ensuring isolation, predictable performance, and cost control? Include concrete settings for: (a) warehouse topology (min/max clusters, auto-suspend/resume), (b) RBAC (roles, grant scopes on databases/schemas/tables), (c) cost governance (resource monitors and credit caps), (d) a sample GRANT script giving REGIONAL_ANALYST access to only SALES and EVENTS schemas, and (e) auditing and reproducibility considerations?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Anthropic","Cloudflare","Robinhood"]},{"id":"q-305","question":"How would you determine the required capacity for a service expecting 10x traffic growth during a product launch?","channel":"sre","subChannel":"capacity-planning","difficulty":"beginner","tags":["forecasting","autoscaling","load-testing"],"companies":["Amazon","Google","Meta"]},{"id":"sr-131","question":"You're managing a microservices platform with 50 services. Service A has a 95th percentile latency of 200ms and handles 10,000 RPS. It calls Service B (50ms, 5,000 RPS) and Service C (100ms, 3,000 RPS). During Black Friday, you expect 5x traffic. Service A's CPU utilization is currently 60%, memory at 70%. How do you plan capacity to maintain <500ms 95th percentile end-to-end latency?","channel":"sre","subChannel":"capacity-planning","difficulty":"advanced","tags":["capacity","scaling"],"companies":["Amazon","Google","Meta","Microsoft","Uber"]},{"id":"sr-143","question":"Your web application currently handles 1000 requests per minute during peak hours. Each request takes an average of 200ms to process. If you expect traffic to double in the next 6 months, how many additional server instances do you need if each server can handle 50 concurrent requests?","channel":"sre","subChannel":"capacity-planning","difficulty":"beginner","tags":["capacity","scaling"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"sr-149","question":"You're designing capacity planning for a microservices platform handling 10M daily active users. Each user generates 50 API calls/day with 80% during peak hours. How many instances do you need for each service?","channel":"sre","subChannel":"capacity-planning","difficulty":"advanced","tags":["capacity","scaling"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-218","question":"How would you design a chaos engineering experiment to test database failover while maintaining transaction consistency across a microservices architecture?","channel":"sre","subChannel":"chaos-engineering","difficulty":"advanced","tags":["chaos-monkey","litmus","gremlin"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-399","question":"You're using Litmus Chaos to test a microservices application. One of your chaos experiments is causing unexpected cascading failures across multiple services. How would you debug this issue and what specific steps would you take to limit the blast radius?","channel":"sre","subChannel":"chaos-engineering","difficulty":"intermediate","tags":["chaos-monkey","litmus","gremlin"],"companies":["Airtable","Fortinet","Tempus"]},{"id":"sr-146","question":"Design a chaos engineering experiment to test the resilience of a microservices-based e-commerce platform during a database partition event. How would you ensure the experiment doesn't cause customer data loss while still providing meaningful insights?","channel":"sre","subChannel":"chaos-engineering","difficulty":"advanced","tags":["chaos","resilience"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"sr-150","question":"You're implementing chaos engineering for a distributed payment system processing $10M daily transactions. Design a chaos experiment to test resilience against Byzantine failures where 30% of payment validation nodes provide conflicting consensus results. How would you ensure financial accuracy while testing system behavior under adversarial conditions?","channel":"sre","subChannel":"chaos-engineering","difficulty":"advanced","tags":["chaos","resilience"],"companies":["Amazon","Coinbase","Microsoft","Netflix","Stripe"]},{"id":"sr-153","question":"You're implementing chaos engineering for a microservices architecture. Your payment service has a 99.9% SLA. During a chaos experiment, you inject 500ms latency into 20% of requests to the database. The service starts timing out after 1 second. What's the most critical metric to monitor first, and what would indicate the experiment should be stopped immediately?","channel":"sre","subChannel":"chaos-engineering","difficulty":"intermediate","tags":["chaos","resilience"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-477","question":"You're running a chaos experiment in production. How do you determine the blast radius and ensure you don't impact customer experience while still getting meaningful failure data?","channel":"sre","subChannel":"general","difficulty":"intermediate","tags":["sre"],"companies":["Goldman Sachs","Google","LinkedIn"]},{"id":"q-506","question":"You're on-call and receive an alert that a critical service is experiencing 99% error rate. What are your immediate first steps and how do you approach incident response?","channel":"sre","subChannel":"general","difficulty":"beginner","tags":["sre"],"companies":["IBM","Scale Ai"]},{"id":"q-535","question":"You're an SRE at Netflix and notice your CDN cache hit ratio dropped from 95% to 70% during peak hours. How would you diagnose and resolve this issue?","channel":"sre","subChannel":"general","difficulty":"advanced","tags":["sre"],"companies":["Discord","Netflix","Snowflake"]},{"id":"q-590","question":"How would you design a canary deployment strategy for a microservice handling 10K RPS with 99.99% SLA requirements?","channel":"sre","subChannel":"general","difficulty":"advanced","tags":["sre"],"companies":["Bloomberg","Google","Twitter"]},{"id":"gh-65","question":"What is Mean Time to Recovery (MTTR), how do you calculate it, and what specific strategies would you implement to optimize it for SRE teams?","channel":"sre","subChannel":"incident-management","difficulty":"beginner","tags":["metrics","kpi"],"companies":null},{"id":"gh-97","question":"How do you design incident response playbooks that balance automation with human oversight for SRE teams?","channel":"sre","subChannel":"incident-management","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-262","question":"Describe a critical production outage you managed during peak traffic. How did you coordinate the response, communicate with stakeholders, and implement both immediate fixes and long-term preventive measures?","channel":"sre","subChannel":"incident-management","difficulty":"advanced","tags":["situation","task","action","result"],"companies":["Amazon","Cloudflare","Google","Microsoft","Netflix","Oracle"]},{"id":"q-319","question":"You are on-call and receive a high-severity PagerDuty alert for a production service degradation. What are your immediate steps and how do you coordinate with the team?","channel":"sre","subChannel":"incident-management","difficulty":"advanced","tags":["pagerduty","runbooks","postmortem"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Servicenow","Stripe","Wipro"]},{"id":"q-367","question":"You're managing a multi-cluster GitOps setup at Warner Bros with 50+ microservices. ArgoCD suddenly starts showing 'Unknown' sync status for critical services during peak traffic. How would you diagnose and resolve this production incident while ensuring zero downtime?","channel":"sre","subChannel":"incident-management","difficulty":"advanced","tags":["argocd","flux","declarative"],"companies":["Amazon","Google","Hashicorp","LinkedIn","Microsoft","Netflix","Salesforce","Warner Bros"]},{"id":"q-368","question":"You're on-call at Tesla when the vehicle telemetry pipeline shows 95% packet loss. Your PagerDuty alert shows the Kafka cluster is healthy, but the downstream processing service is crashing. What's your immediate triage process and how do you determine if this is a network, application, or data format issue?","channel":"sre","subChannel":"incident-management","difficulty":"intermediate","tags":["pagerduty","runbooks","postmortem"],"companies":["Discord","Tesla","Zscaler"]},{"id":"sr-126","question":"How would you design and implement a comprehensive blameless postmortem process that includes incident response coordination, root cause analysis using 5 Whys and fishbone diagrams, and actionable improvement tracking?","channel":"sre","subChannel":"incident-management","difficulty":"advanced","tags":["incident","postmortem"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"sr-142","question":"You receive a PagerDuty alert at 3 AM: 'Production API is returning 500 errors'. What are your first three steps in handling this incident, and what specific tools and metrics would you use to assess impact and coordinate response?","channel":"sre","subChannel":"incident-management","difficulty":"beginner","tags":["incident","postmortem"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"gh-19","question":"What is monitoring in DevOps and how does it differ from observability?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["observability","monitoring","logging"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-20","question":"Design a comprehensive logging architecture using the ELK Stack with File Beats for a high-traffic e-commerce platform processing 50,000 requests per minute. How would you ensure data integrity and real-time monitoring?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["observability","monitoring","logging"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"gh-21","question":"How does Prometheus implement a pull-based monitoring system, and what are the key components in its architecture?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["observability","monitoring","logging"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-22","question":"What is Grafana and how does it integrate with different data sources for monitoring and visualization?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["observability","monitoring","logging"],"companies":["Airbnb","LinkedIn","Microsoft","Stripe","Uber"]},{"id":"gh-23","question":"Explain the key differences between monitoring and logging in DevOps, and when would you use each?","channel":"sre","subChannel":"observability","difficulty":"intermediate","tags":["observability","monitoring","logging"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-61","question":"What are Service Level Indicators (SLIs) and how do they differ from SLOs?","channel":"sre","subChannel":"observability","difficulty":"intermediate","tags":["sre","reliability"],"companies":["Amazon","Google","Meta"]},{"id":"gh-77","question":"How would you design a comprehensive monitoring strategy for a distributed system, including tool selection, SLI/SLO definition, and alerting implementation?","channel":"sre","subChannel":"observability","difficulty":"intermediate","tags":["monitoring","infra"],"companies":["Amazon","Cloudflare","Google","Meta","Microsoft","Netflix"]},{"id":"gh-78","question":"How would you design a comprehensive monitoring strategy for a production microservices system, including SLI/SLO definitions and alerting thresholds?","channel":"sre","subChannel":"observability","difficulty":"intermediate","tags":["monitoring","infra"],"companies":["Amazon","Cloudflare","Google","Microsoft","Netflix","Stripe"]},{"id":"gh-79","question":"What is Application Performance Monitoring?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["monitoring","infra"],"companies":["Amazon","Datadog","Google","Microsoft","Splunk"]},{"id":"gh-95","question":"What is a Service Level Indicator (SLI)?","channel":"sre","subChannel":"observability","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Citadel","Goldman Sachs","Google","Microsoft"]},{"id":"gh-99","question":"What is Tracing in Observability?","channel":"sre","subChannel":"observability","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Goldman Sachs","Netflix","Stripe","Uber"]},{"id":"q-192","question":"How would you implement OpenTelemetry instrumentation to capture RED metrics (Rate, Errors, Duration) for a microservice using Prometheus as the backend?","channel":"sre","subChannel":"observability","difficulty":"intermediate","tags":["prometheus","grafana","opentelemetry"],"companies":["Chronosphere","Datadog","Grafana Labs","Microsoft","New Relic"]},{"id":"q-244","question":"What is the difference between metrics, logs, and traces in observability, and how do OpenTelemetry collectors correlate them?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["prometheus","grafana","opentelemetry"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-345","question":"You're monitoring a streaming service that suddenly experiences 500 errors. How would you use Prometheus and Grafana to quickly identify the root cause?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["prometheus","grafana","opentelemetry"],"companies":["Amazon","Cloudflare","Google","Infosys","Microsoft","Netflix","Warner Bros"]},{"id":"q-382","question":"You're the SRE lead for a rocket launch telemetry system. Prometheus is showing high memory usage on your OpenTelemetry collector during peak launch events, causing metric loss. How would you architect a solution to handle 100K+ metrics/second while ensuring zero data loss during critical launch windows?","channel":"sre","subChannel":"observability","difficulty":"advanced","tags":["prometheus","grafana","opentelemetry"],"companies":["Notion","OpenAI","SpaceX"]},{"id":"q-391","question":"You're an SRE at HashiCorp and your Prometheus alerts are firing every 5 minutes due to a memory leak in a Go service using OpenTelemetry. How would you debug this using the observability stack?","channel":"sre","subChannel":"observability","difficulty":"intermediate","tags":["prometheus","grafana","opentelemetry"],"companies":["Hashicorp","Instacart","Western Digital"]},{"id":"q-411","question":"You're on-call and receive an alert: 'API response time increased from 200ms to 2s over the last 5 minutes'. Using Prometheus, Grafana, and OpenTelemetry, how would you diagnose this issue?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["prometheus","grafana","opentelemetry"],"companies":["Amazon","Cloudflare","Google","Intel","Microsoft","Netflix","Palo Alto Networks","Stripe"]},{"id":"sr-124","question":"How would you implement the four golden signals of monitoring in a production microservices architecture, and what trade-offs would you consider when designing your observability strategy?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["metrics","monitoring"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"sr-133","question":"How do you implement the three pillars of observability (logs, metrics, traces) in a microservices architecture, and what are the key trade-offs between them?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["metrics","monitoring"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Snowflake"]},{"id":"sr-155","question":"What is the difference between metrics, logs, and traces in observability, and when would you use each?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["metrics","monitoring"],"companies":["Amazon","Bloomberg","Datadog","Goldman Sachs","Google","Microsoft","Netflix","New Relic","Splunk","Uber"]},{"id":"sre-1","question":"How would you design and implement SRE monitoring with SLIs, SLOs, and SLAs for a high-traffic e-commerce platform? What specific metrics would you track and how would they drive engineering decisions?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["metrics","policy","definitions","observability"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"gh-103","question":"What is a Self-Healing System and how does it work in distributed architectures?","channel":"sre","subChannel":"reliability","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-35","question":"Design a backup and disaster recovery strategy for a high-availability e-commerce platform processing 10,000 transactions/minute with 99.99% uptime SLA. What are your RTO/RPO targets and how would you implement multi-region failover?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["backup","dr"],"companies":["Amazon","Google","Meta"]},{"id":"gh-59","question":"What is Site Reliability Engineering and how does it differ from traditional operations?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["sre","reliability"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-60","question":"How do you design and implement Service Level Objectives (SLOs) with proper SLI definitions, error budgets, and monitoring strategies?","channel":"sre","subChannel":"reliability","difficulty":"intermediate","tags":["sre","reliability"],"companies":null},{"id":"gh-62","question":"What is an Error Budget and how does it impact SRE decision-making?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["sre","reliability"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-63","question":"What is Toil in Site Reliability Engineering and how should SREs approach managing it?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["sre","reliability"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-93","question":"How do you implement and monitor Service Level Agreements (SLAs) in a distributed system, including specific metrics, tools, and alerting strategies?","channel":"sre","subChannel":"reliability","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Cloudflare","Datadog","Google","Microsoft","Netflix"]},{"id":"gh-94","question":"What is a Service Level Objective (SLO) and how does it differ from an SLA?","channel":"sre","subChannel":"reliability","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-270","question":"Your microservice has a 99.9% availability SLO over 30 days with a 1-hour burn rate alert threshold. If you experience a 10-minute outage at 10% traffic, how much error budget remains and what's the burn rate? Should you alert?","channel":"sre","subChannel":"reliability","difficulty":"intermediate","tags":["slo","sli","error-budget"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"q-290","question":"Explain the relationship between SLIs, SLOs, and SLAs in reliability engineering, including how you would implement error budgets and monitor burn rate?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["slo","sli","error-budget"],"companies":["Amazon","Apple","Cloudflare","Google","Microsoft","Netflix"]},{"id":"q-333","question":"Your SLO for API response time is 99.9% with a 500ms threshold. You're at 99.7% and the error budget is exhausted. The product team wants to ship a new feature that will increase traffic by 20%. How do you handle this situation?","channel":"sre","subChannel":"reliability","difficulty":"intermediate","tags":["slo","sli","error-budget"],"companies":["Atlassian","Databricks","Unity"]},{"id":"q-355","question":"Your SLO is 99.9% for API latency (p95 < 200ms). You're at 99.85% and have 15% error budget remaining. A critical security patch requires 30% traffic shift to new version with unknown latency characteristics. How do you proceed while maintaining service reliability?","channel":"sre","subChannel":"reliability","difficulty":"advanced","tags":["slo","sli","error-budget"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"sr-130","question":"Your web service has an SLO of 99.9% availability over 30 days. You've had 3 outages: 45 minutes, 20 minutes, and 15 minutes. What's your current availability, error budget status, and what immediate actions would you take to prevent SLO breach?","channel":"sre","subChannel":"reliability","difficulty":"intermediate","tags":["slo","sli","error-budget"],"companies":["Amazon","Cloudflare","Google","Meta","Microsoft","Netflix"]},{"id":"sr-147","question":"Your distributed system has 5 microservices with the following failure rates: Service A (0.1%), Service B (0.2%), Service C (0.05%), Service D (0.15%), Service E (0.25%). Design a fault-tolerant architecture to achieve 99.5% SLO with specific implementation details?","channel":"sre","subChannel":"reliability","difficulty":"advanced","tags":["reliability","incident"],"companies":["Amazon","Databricks","Google","Meta","Microsoft","Netflix"]},{"id":"sr-154","question":"Your API serves 10M requests/day with a 99.9% availability SLO and 30-day error budget. After a 4-hour outage affecting 100% of traffic, calculate the remaining error budget and explain how you'd handle post-incident SLO adjustments, error budget recovery strategies, and burn rate monitoring?","channel":"sre","subChannel":"reliability","difficulty":"intermediate","tags":["slo","sli","error-budget"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"sr-169","question":"Your API service has an SLO of 99.9% availability. If you have 5 incidents this month with downtimes of 10min, 5min, 15min, 8min, and 12min, did you meet your SLO and what's the remaining error budget for the rest of the month?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["slo","sli","error-budget"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"sre-2","question":"How do you calculate and manage Error Budgets for a microservices architecture with multiple SLOs, and what strategies do you use for burn rate monitoring and recovery?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["management","concept","risk"],"companies":["Adobe","Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-45","question":"How do rate limiting algorithms like Token Bucket and Leaky Bucket control API request flow and what are their trade-offs?","channel":"system-design","subChannel":"api-design","difficulty":"beginner","tags":["api","service-mesh"],"companies":["Amazon","Google","Microsoft","Stripe","Uber"]},{"id":"q-406","question":"Design a REST API for a cryptocurrency exchange that handles 100,000 trades per day with real-time price updates. How would you ensure data consistency and handle high-frequency trading requests?","channel":"system-design","subChannel":"api-design","difficulty":"beginner","tags":["api","rest","grpc","graphql"],"companies":["Coinbase","Oracle","Twilio"]},{"id":"q-424","question":"Design a RESTful API for a hotel booking system. What endpoints would you create and how would you handle concurrent bookings for the same room?","channel":"system-design","subChannel":"api-design","difficulty":"beginner","tags":["api","rest","grpc","graphql"],"companies":["Airbnb","Amazon","Booking.com","Google","Microsoft"]},{"id":"q-507","question":"Design a unified API gateway for Databricks that supports REST, gRPC, and GraphQL with protocol translation, rate limiting, and unified authentication?","channel":"system-design","subChannel":"api-design","difficulty":"advanced","tags":["api","rest","grpc","graphql"],"companies":["Databricks","IBM"]},{"id":"sy-151","question":"Design a rate limiting API for a multi-tenant SaaS platform where different customers have different rate limits (free: 100 req/hour, premium: 1000 req/hour, enterprise: custom). How would you design the API endpoints and data structures to efficiently track and enforce these limits?","channel":"system-design","subChannel":"api-design","difficulty":"intermediate","tags":["api","rest"],"companies":["Amazon","Google","Microsoft","Stripe","Uber"]},{"id":"q-604","question":"Design a rate limiting system for a public API that can handle 10,000 requests per second with different rate limits for free and paid tiers (100 requests/minute for free, 1000 requests/minute for paid). How would you implement this to ensure fairness and prevent abuse?","channel":"system-design","subChannel":"api-rate-limiting","difficulty":"intermediate","tags":["rate-limiting","api-design","distributed-systems","redis","token-bucket"],"companies":["Twitter","Stripe","GitHub","Google","Amazon"]},{"id":"q-1622","question":"Design a distributed caching system for a high-traffic e-commerce platform that handles 10,000 requests per second with 99.9% availability. How would you handle cache invalidation, consistency, and failover?","channel":"system-design","subChannel":"cache-architecture","difficulty":"intermediate","tags":["distributed-caching","redis","consistency","scalability","high-availability"],"companies":["Amazon","Google","Meta","Netflix","Uber","Airbnb"]},{"id":"q-597","question":"Design a distributed caching system for a global e-commerce platform that handles 100,000 requests per second with 99.9% availability. How would you handle cache consistency, invalidation strategies, and failover across multiple geographic regions?","channel":"system-design","subChannel":"cache-architecture","difficulty":"advanced","tags":["distributed-systems","caching","redis","high-availability","consistency","scalability"],"companies":["Amazon","Google","Meta","Netflix","Uber","Airbnb","Spotify","Twitter"]},{"id":"q-603","question":"Design a distributed caching system for a global e-commerce platform that serves 10 million daily active users. The system must handle product catalog caching with 99.99% availability, sub-millisecond latency for hot items, and cache consistency across multiple data centers.","channel":"system-design","subChannel":"cache-architecture","difficulty":"advanced","tags":["distributed-systems","caching","redis","high-availability","consistency"],"companies":["Amazon","Netflix","Google","Meta","Microsoft","Uber","Airbnb"]},{"id":"q-621","question":"Design a distributed caching system for a social media platform that needs to handle 10 million active users with 99.9% availability. How would you ensure cache consistency across multiple data centers while minimizing latency?","channel":"system-design","subChannel":"cache-architecture","difficulty":"intermediate","tags":["distributed-systems","caching","consistency","scalability","high-availability"],"companies":["Facebook","Twitter","LinkedIn","Reddit","Instagram"]},{"id":"q-634","question":"Design a distributed caching system for a global e-commerce platform that serves 10 million requests per second with 99.99% availability. The system must handle cache consistency across multiple data centers, support cache warming for popular products, and provide graceful degradation when cache nodes fail.","channel":"system-design","subChannel":"cache-architecture","difficulty":"advanced","tags":["distributed-systems","caching","consistency","high-availability","scalability"],"companies":["Amazon","Netflix","Google","Meta","Microsoft","Uber"]},{"id":"q-169","question":"Design a caching strategy for a high-traffic e-commerce platform handling 10,000 RPS. Compare cache-aside vs read-through patterns, including write-through considerations, consistency guarantees, and performance implications. When would you choose each pattern and what are the trade-offs?","channel":"system-design","subChannel":"caching","difficulty":"beginner","tags":["cache","redis"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-213","question":"Design a multi-tier caching strategy for a 99.9% availability e-commerce platform handling 10M requests/day with 100ms P99 latency. How would you implement cache warming, invalidation, and fallback mechanisms?","channel":"system-design","subChannel":"caching","difficulty":"advanced","tags":["cache","redis","memcached","cdn"],"companies":null},{"id":"q-231","question":"How would you design a multi-region CDN cache purging system that guarantees content propagation within 5 seconds while handling 10,000 concurrent invalidations per second?","channel":"system-design","subChannel":"caching","difficulty":"intermediate","tags":["edge","caching","purging"],"companies":["Amazon","Cloudflare","Google","Meta","Microsoft","Netflix"]},{"id":"q-299","question":"How would you design a caching layer for a high-traffic e-commerce website?","channel":"system-design","subChannel":"caching","difficulty":"beginner","tags":["cache","redis","memcached","cdn"],"companies":["Amazon","Google","Meta"]},{"id":"q-392","question":"Design a distributed caching layer for Fortinet's threat intelligence system that serves 50M security devices with real-time malware signatures and threat data. How would you ensure cache consistency across global edge locations while maintaining sub-50ms response times?","channel":"system-design","subChannel":"caching","difficulty":"advanced","tags":["cache","redis","memcached","cdn"],"companies":["Fortinet","Microsoft","Tesla"]},{"id":"q-417","question":"Design a distributed caching strategy for a global e-commerce platform handling 10M daily users with frequent price/inventory updates. How would you ensure cache consistency, handle invalidation, and optimize performance across regions?","channel":"system-design","subChannel":"caching","difficulty":"intermediate","tags":["cache","redis","memcached","cdn"],"companies":null},{"id":"q-441","question":"Design a distributed caching layer for a social media feed serving 10M DAU with 99.9% availability. How would you handle cache invalidation across multiple data centers?","channel":"system-design","subChannel":"caching","difficulty":"advanced","tags":["cache","redis","memcached","cdn"],"companies":["Apple","LinkedIn","Two Sigma"]},{"id":"q-478","question":"Design a caching layer for a product catalog API serving 10K requests/second with 100K products. How would you handle cache invalidation and ensure data consistency?","channel":"system-design","subChannel":"caching","difficulty":"beginner","tags":["cache","redis","memcached","cdn"],"companies":["Instacart","Plaid"]},{"id":"q-601","question":"Design a communication strategy for a microservices-based e-commerce platform where the Order Service needs to notify the Inventory Service, Payment Service, and Notification Service when a new order is placed. How would you handle communication failures and ensure data consistency?","channel":"system-design","subChannel":"distributed-communication","difficulty":"intermediate","tags":["microservices","event-driven","distributed-systems","message-brokers","saga-pattern"],"companies":["Amazon","Netflix","Uber","Spotify","Airbnb"]},{"id":"q-625","question":"Design a distributed rate limiter for a high-traffic API that can handle 10,000 requests per second with per-user, per-IP, and per-endpoint limits. How would you ensure accuracy and prevent race conditions across multiple servers?","channel":"system-design","subChannel":"distributed-rate-limiting","difficulty":"intermediate","tags":["rate-limiting","distributed-systems","api-gateway","redis","token-bucket"],"companies":["Google","Amazon","Meta","Microsoft","Netflix"]},{"id":"gh-43","question":"Design an API Gateway for a high-traffic e-commerce platform handling 10M daily requests. How would you implement rate limiting, circuit breakers, and service discovery while ensuring 99.9% availability and sub-100ms latency?","channel":"system-design","subChannel":"distributed-systems","difficulty":"intermediate","tags":["api","service-mesh"],"companies":null},{"id":"q-189","question":"How would you design a distributed transaction system using the Saga pattern for an e-commerce platform handling inventory, payment, and shipping services, ensuring exactly-once processing and eventual consistency?","channel":"system-design","subChannel":"distributed-systems","difficulty":"beginner","tags":["saga","cqrs","event-sourcing"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-238","question":"How does Raft consensus algorithm ensure leader election and log replication in distributed systems?","channel":"system-design","subChannel":"distributed-systems","difficulty":"beginner","tags":["dist-sys","cap-theorem","consensus"],"companies":["Airbnb","Amazon","Apple","Cockroach Labs","Etcd","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-260","question":"Design a scalable Selenium Grid architecture to handle 10,000 concurrent test sessions with 99.9% uptime, ensuring zero memory leaks through automatic session lifecycle management, real-time monitoring, and graceful node failure recovery across multiple data centers?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["selenium","webdriver","grid"],"companies":["Amazon","Google","Microsoft","Netflix","Salesforce","Snowflake"]},{"id":"q-282","question":"Design an event sourcing system for a high-throughput e-commerce platform handling 10,000 orders/second with 99.99% availability. How would you implement the event store, handle versioning, and ensure event ordering while supporting replay and recovery?","channel":"system-design","subChannel":"distributed-systems","difficulty":"intermediate","tags":["event-sourcing","distributed-systems","architecture","cqrs","immutability"],"companies":["Amazon","Databricks","Microsoft","Netflix","Salesforce","Stripe"]},{"id":"q-313","question":"How would you design a distributed chat system like Slack that handles real-time messaging with strong consistency guarantees across global deployments?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["dist-sys","cap-theorem","consensus"],"companies":null},{"id":"q-352","question":"Design a distributed order processing system using Saga pattern for a high-frequency trading platform. How would you handle compensation transactions when a market data feed fails mid-transaction?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["saga","cqrs","event-sourcing"],"companies":["Citadel","Tcs","Western Digital"]},{"id":"q-435","question":"You're building a ride-sharing service similar to Lyft. How would you design the database architecture to handle 10,000 concurrent rides with real-time location updates? What sharding strategy would you use?","channel":"system-design","subChannel":"distributed-systems","difficulty":"beginner","tags":["scaling","sharding","replication"],"companies":["Amazon","Google","Lyft","Meta","Netflix","Salesforce","Uber"]},{"id":"q-536","question":"Design a distributed consensus service for a ride-sharing platform handling 10M concurrent rides with real-time location updates. How do you ensure consistency across geo-distributed data centers while maintaining <100ms latency?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["dist-sys","cap-theorem","consensus"],"companies":["LinkedIn","Lyft","Meta"]},{"id":"q-612","question":"How would you design a communication pattern for a microservices architecture where services need to maintain real-time data consistency while handling high-throughput requests?","channel":"system-design","subChannel":"distributed-systems","difficulty":"intermediate","tags":["microservices","communication-patterns","event-driven","consistency","message-queues"],"companies":["Netflix","Uber","Amazon","Twitter"]},{"id":"sd-2","question":"Design a distributed caching system using Consistent Hashing. How would you handle node failures, load balancing, and ensure minimal data movement when scaling from 10 to 100 nodes?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["hashing","dist-sys","caching"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"sd-3","question":"Explain the CAP Theorem. Can you really 'choose two' and what are the practical tradeoffs?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["theory","dist-sys","database"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"sd-4","question":"Design a database sharding strategy for a social media platform with 100M+ users. How would you handle data distribution, cross-shard queries, and rebalancing?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["db","scale","architecture"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"sd-5","question":"Design a distributed rate limiter for a microservices API handling 10,000 RPS with 99.9% availability using Redis Cluster. How would you handle cache invalidation, circuit breakers, and multi-region consistency?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["security","api","algorithms"],"companies":["Amazon","Cloudflare","Google","Meta","Netflix","Stripe"]},{"id":"sy-132","question":"Design a distributed rate limiting system that can handle 1M+ requests per second across multiple data centers while maintaining consistency and low latency. How would you handle burst traffic, different rate limiting algorithms (token bucket, sliding window), and ensure fair distribution across users?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["api","rest"],"companies":["Amazon","Google","Meta","Microsoft","Uber"]},{"id":"sy-137","question":"Design a distributed system that provides exactly-once processing guarantees for event streams with out-of-order delivery and network partitions. How would you handle idempotency, deduplication, and causal consistency across multiple processing nodes?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["dist-sys","architecture"],"companies":["Goldman Sachs","LinkedIn","Netflix","Stripe","Uber"]},{"id":"sy-138","question":"Design a distributed rate limiting system that can handle 10M requests per minute across 100+ microservices with different rate limit policies per service. How would you ensure high availability, consistency, and sub-millisecond latency while handling failures and scaling?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["api","rest"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"sy-139","question":"Design a rate limiting system for a multi-tenant API serving 100M+ daily calls across 5 regions, supporting tiered rate limits (1000-100K RPS), burst capacity (3x sustained rate), sub-50ms latency, and 99.99% availability using distributed token bucket algorithm?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["api","rest"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"sy-140","question":"Design a rate limiting service that can handle 10 million requests per second with distributed consistency across multiple data centers. The service should support multiple rate limiting strategies (token bucket, sliding window, fixed window) and provide sub-millisecond latency. How would you architect this to handle bursts, prevent thundering herd problems, and ensure accurate global rate limits?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["api","rest"],"companies":["Amazon","Google","Meta","Microsoft","Stripe"]},{"id":"sy-141","question":"Design a globally distributed serverless platform for real-time collaborative document editing with offline support and conflict resolution. How would you handle data consistency, versioning, and low-latency synchronization across AWS regions while maintaining sub-50ms response times?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["infra","scale"],"companies":["Amazon","Dropbox","Google","Meta","Microsoft"]},{"id":"sy-158","question":"Design a distributed rate limiter that can handle 1M requests/second across 100 data centers with <10ms latency. How do you ensure accurate rate limiting while avoiding coordination overhead?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["dist-sys","architecture"],"companies":["Amazon","Google","Meta","Microsoft","Uber"]},{"id":"q-622","question":"How would you design communication between microservices when you need to ensure data consistency across multiple services, and what patterns would you consider?","channel":"system-design","subChannel":"distributed-transactions","difficulty":"intermediate","tags":["microservices","distributed-systems","data-consistency","saga-pattern","event-driven"],"companies":["Netflix","Amazon","Uber","LinkedIn","Spotify"]},{"id":"gh-14","question":"How would you design a scalable cloud platform architecture that integrates compute, storage, networking, and database services?","channel":"system-design","subChannel":"infrastructure","difficulty":"beginner","tags":["cloud","aws","azure","gcp"],"companies":["Amazon","Google","Meta"]},{"id":"gh-84","question":"Design a cloud-native modernization strategy for a 10M-user monolithic e-commerce platform requiring 99.9% uptime. How would you migrate to microservices while maintaining business continuity and optimizing costs?","channel":"system-design","subChannel":"infrastructure","difficulty":"beginner","tags":["migration","cloud"],"companies":null},{"id":"gh-91","question":"How would you design a comprehensive feature flagging system that supports both server-side and client-side flags with proper performance considerations?","channel":"system-design","subChannel":"infrastructure","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-265","question":"How would you design a unified system monitoring dashboard that aggregates real-time process metrics, system call tracing, and network connection data from htop, strace, and lsof?","channel":"system-design","subChannel":"infrastructure","difficulty":"intermediate","tags":["top","htop","strace","lsof"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-316","question":"How would you design a database architecture to handle 10 million users with 99.99% uptime? What sharding and replication strategies would you use?","channel":"system-design","subChannel":"infrastructure","difficulty":"beginner","tags":["scaling","sharding","replication"],"companies":["Chime","Salesforce","Snowflake"]},{"id":"q-327","question":"Design a simple API rate limiter that can handle 10,000 requests per second. How would you prevent abuse while ensuring legitimate users aren't blocked?","channel":"system-design","subChannel":"infrastructure","difficulty":"beginner","tags":["infra","scale","distributed"],"companies":["Salesforce","Square","Supabase"]},{"id":"q-562","question":"Design a real-time vehicle telemetry system for Tesla's fleet of 10M cars collecting sensor data at 100Hz?","channel":"system-design","subChannel":"infrastructure","difficulty":"advanced","tags":["infra","scale","distributed"],"companies":["Apple","Tesla"]},{"id":"q-619","question":"What is an ambient mesh and how does it differ from traditional service mesh architectures?","channel":"system-design","subChannel":"infrastructure","difficulty":"intermediate","tags":["service-mesh","kubernetes","infrastructure","networking"],"companies":["Google","IBM","Microsoft"]},{"id":"sy-169","question":"Design a URL shortening service that handles 1 billion URLs with 10M daily requests, achieving 99.99% uptime. How would you architect the system for high availability and scalability?","channel":"system-design","subChannel":"infrastructure","difficulty":"beginner","tags":["infra","scale"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-607","question":"Design a communication strategy for a microservices architecture where you have 10+ services that need to exchange data. What patterns would you use and why?","channel":"system-design","subChannel":"inter-service-communication","difficulty":"intermediate","tags":["microservices","communication-patterns","system-design","event-driven","api-gateway"],"companies":["Netflix","Amazon","Uber","Spotify","Airbnb"]},{"id":"q-617","question":"Design a communication pattern for a microservices architecture where an order service needs to notify inventory, payment, and shipping services when a new order is placed. Discuss the trade-offs between synchronous REST calls, message queues, and event streaming.","channel":"system-design","subChannel":"inter-service-communication","difficulty":"intermediate","tags":["microservices","communication-patterns","distributed-systems","message-queues","event-streaming"],"companies":["Netflix","Amazon","Uber","LinkedIn","Spotify"]},{"id":"gh-33","question":"How do different load balancing algorithms distribute traffic across servers, and what are the trade-offs between performance and resource utilization?","channel":"system-design","subChannel":"load-balancing","difficulty":"advanced","tags":["scale","ha"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Netflix"]},{"id":"q-285","question":"How would you design a load balancer that handles 1M concurrent connections using NGINX vs HAProxy?","channel":"system-design","subChannel":"load-balancing","difficulty":"intermediate","tags":["lb","traffic","nginx","haproxy"],"companies":["Amazon","Google","Meta"]},{"id":"q-376","question":"Design a load balancing system for a global e-commerce platform handling 50M concurrent users during Black Friday sales. How would you ensure zero downtime while handling 10x traffic spikes?","channel":"system-design","subChannel":"load-balancing","difficulty":"advanced","tags":["lb","traffic","nginx","haproxy"],"companies":["Hashicorp","Thoughtworks","Workday"]},{"id":"q-393","question":"Design a global load balancer for Google Cloud that handles 10M concurrent connections with sub-10ms failover across 5 regions. How would you ensure zero-downtime deployments while maintaining 99.999% availability?","channel":"system-design","subChannel":"load-balancing","difficulty":"advanced","tags":["lb","traffic","nginx","haproxy"],"companies":null},{"id":"q-591","question":"How would you design a load balancer for a microservices architecture handling 10,000 requests per second with 99.99% uptime?","channel":"system-design","subChannel":"load-balancing","difficulty":"intermediate","tags":["lb","traffic","nginx","haproxy"],"companies":["Anthropic","Oracle","Tesla"]},{"id":"q-598","question":"Design a load balancer for a high-traffic e-commerce platform that must handle 100,000 requests per second with 99.99% uptime. Explain how you would choose between round-robin, least connections, and weighted round-robin algorithms, and describe your failover strategy.","channel":"system-design","subChannel":"load-balancing-algorithms","difficulty":"intermediate","tags":["load-balancing","system-design","scalability","high-availability","algorithms"],"companies":["Amazon","Google","Meta","Netflix","Microsoft","Uber","Airbnb"]},{"id":"q-605","question":"Design a load balancer for a high-traffic e-commerce website that must handle 100,000 requests per second with 99.99% uptime. Explain which load balancing algorithm you would choose and why, considering factors like session persistence, health checks, and failover mechanisms.","channel":"system-design","subChannel":"load-balancing-algorithms","difficulty":"intermediate","tags":["load-balancing","system-design","scalability","high-availability","algorithms"],"companies":["Amazon","Google","Netflix","Meta","Microsoft","Apple"]},{"id":"q-615","question":"Design a load balancer for a high-traffic e-commerce website. Which load balancing algorithm would you choose and why? How would you handle session persistence and failover?","channel":"system-design","subChannel":"load-balancing-algorithms","difficulty":"intermediate","tags":["load-balancing","system-design","high-availability","scalability","algorithms"],"companies":["Amazon","Google","Meta","Netflix","Uber","Airbnb"]},{"id":"q-609","question":"Design a load balancer for a high-traffic e-commerce platform. Which load balancing algorithm would you choose and why? Consider factors like session persistence, server health, and traffic distribution.","channel":"system-design","subChannel":"load-balancing-strategies","difficulty":"intermediate","tags":["load-balancing","algorithms","system-design","high-availability","scalability"],"companies":["Amazon","Google","Netflix","Meta","Microsoft","Apple"]},{"id":"q-610","question":"Design a load balancer for a high-traffic e-commerce platform. Which load balancing algorithm would you choose and why? Consider the trade-offs between different algorithms.","channel":"system-design","subChannel":"load-balancing-strategies","difficulty":"intermediate","tags":["load-balancing","algorithms","system-design","scalability","high-availability"],"companies":["Amazon","Google","Netflix","Meta","Microsoft"]},{"id":"q-266","question":"Design a distributed message queue system that handles 1M events/sec with exactly-once delivery, sub-second latency, and 99.99% availability. How would you ensure data consistency across partitions while handling consumer failures and network partitions?","channel":"system-design","subChannel":"message-queues","difficulty":"intermediate","tags":["kafka","rabbitmq","sqs","pubsub"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-361","question":"Design a distributed message queue system for processing 10M financial transactions per hour with exactly-once delivery guarantees across multiple data centers. How would you handle message ordering, deduplication, and cross-region consistency?","channel":"system-design","subChannel":"message-queues","difficulty":"advanced","tags":["kafka","rabbitmq","sqs","pubsub"],"companies":["Amazon","Broadcom","Google","Netflix","PayPal","Robinhood","Stripe","Western Digital"]},{"id":"q-432","question":"Design a food delivery app's order processing system using message queues to handle 10,000 orders per minute with exactly-once processing?","channel":"system-design","subChannel":"message-queues","difficulty":"beginner","tags":["kafka","rabbitmq","sqs","pubsub"],"companies":["Lyft","Microsoft","Oracle"]},{"id":"q-623","question":"Design a communication strategy for a microservices architecture where some services need real-time updates while others can tolerate eventual consistency. What patterns would you use and how would you handle service discovery and load balancing?","channel":"system-design","subChannel":"microservices-architecture","difficulty":"intermediate","tags":["microservices","communication-patterns","service-discovery","load-balancing","consistency"],"companies":["Netflix","Amazon","Uber","Spotify","Airbnb"]},{"id":"q-616","question":"Design a CI/CD pipeline for a microservices application with 10 services, each with its own repository. The pipeline must support parallel deployments, canary releases, automated testing, and rollback capabilities. How would you ensure zero-downtime deployments and maintain consistency across services?","channel":"system-design","subChannel":"pipeline-architecture","difficulty":"advanced","tags":["cicd","microservices","kubernetes","devops","gitops"],"companies":["Google","Netflix","Amazon","Microsoft","Uber","Airbnb"]},{"id":"q-631","question":"Design a CI/CD pipeline for a microservices application with 10 services. Each service has its own repository and needs automated testing, security scanning, and deployment to both staging and production environments. How would you ensure fast feedback loops while maintaining quality and security?","channel":"system-design","subChannel":"pipeline-architecture","difficulty":"intermediate","tags":["cicd","microservices","devops","security","automation"],"companies":["Google","Amazon","Microsoft","Netflix","Spotify","Uber"]},{"id":"q-626","question":"Compare and contrast synchronous vs asynchronous communication patterns in microservices, and when would you choose one over the other?","channel":"system-design","subChannel":"service-communication","difficulty":"intermediate","tags":["microservices","communication-patterns","system-design","architecture"],"companies":["Netflix","Amazon","Uber","Spotify","Airbnb"]},{"id":"q-632","question":"Design a communication strategy for a microservices architecture where you have an Order Service, Payment Service, and Inventory Service. The Order Service needs to coordinate with both Payment and Inventory services to process an order. What communication patterns would you use and why?","channel":"system-design","subChannel":"service-communication","difficulty":"intermediate","tags":["microservices","communication-patterns","system-design","async-messaging","api-design"],"companies":["Netflix","Amazon","Uber","Spotify","Airbnb"]},{"id":"q-1343","question":"How would you design a distributed caching system for a real-time analytics platform that processes streaming data from millions of IoT devices, requiring sub-second query responses and handling high write throughput with eventual consistency?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["distributed-caching","real-time-analytics","iot-streaming","eventual-consistency","write-throughput"],"companies":[]},{"id":"q-1621","question":"Design a rate limiting system for a video streaming platform that needs to enforce different limits based on content type and user subscription tier: free users (5 videos/hour, 100MB bandwidth), premium users (50 videos/hour, 1GB bandwidth), and enterprise users (unlimited videos, 10GB bandwidth). How would you implement a multi-dimensional rate limiter that tracks both request count and data transfer while preventing abuse and ensuring fair resource allocation?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","multi-dimensional","video-streaming","redis","sliding-window"],"companies":[]},{"id":"q-1623","question":"Design a rate limiting system for a video streaming platform that needs to handle different rate limits for various user tiers and content types: free users (5 video plays/hour), premium users (50 video plays/hour), and content creators (1000 API calls/hour for upload management). How would you implement a hybrid rate limiter that combines user-based and content-based limits while preventing abuse during peak viewing hours?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","redis","video-streaming","user-tiers","hybrid-rate-limiting"],"companies":[]},{"id":"q-1624","question":"Design a rate limiting system for a video streaming platform that needs to handle different rate limits for various user tiers and content types: free users (5 video plays/hour), premium users (unlimited plays), and API access (1000 req/hour). How would you implement a hybrid rate limiting approach that combines user-based and content-based limits while preventing abuse during peak events like live streams?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","redis","video-streaming","hybrid-limits","adaptive-throttling"],"companies":[]},{"id":"q-1625","question":"How would you design a communication pattern for a microservices architecture where services need to handle both high-volume batch processing and low-latency interactive requests, while ensuring message ordering and exactly-once processing guarantees?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["microservices","message-queues","exactly-once","batch-processing","low-latency"],"companies":[]},{"id":"q-2122","question":"Design a rate limiting system for a mobile banking API that must handle different limits based on transaction type and user risk level: low-risk users (1000 req/day), high-risk users (100 req/day), with stricter limits for sensitive operations like money transfers (10/hour) vs. balance checks (100/hour). How would you implement a dynamic rate limiter that adapts to real-time fraud detection signals?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","redis","fraud-detection","mobile-banking","sliding-window"],"companies":[]},{"id":"q-2123","question":"How would you design a distributed caching system for a real-time analytics platform that processes streaming data from millions of IoT devices, requiring sub-100ms query response times for time-series aggregations while handling high write throughput and data freshness requirements?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["distributed-caching","time-series","iot-analytics","real-time","cache-consistency"],"companies":[]},{"id":"q-2124","question":"Design a rate limiting system for a financial trading platform that needs to enforce different limits based on market volatility: normal markets (1000 req/sec), high volatility (100 req/sec), and flash crash scenarios (10 req/sec). How would you implement a dynamic rate limiter that can automatically adjust limits based on real-time market conditions while preventing manipulation and ensuring fair access for all clients?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","financial-systems","dynamic-algorithms","market-data","circuit-breaker"],"companies":[]},{"id":"q-639","question":"How would you design a distributed caching system for a real-time analytics platform that processes streaming event data from millions of IoT devices, requiring sub-10ms query latency for time-series aggregations while handling high write throughput and ensuring data freshness?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["distributed-caching","time-series","real-time-analytics","iot","write-through"],"companies":[]},{"id":"q-645","question":"Design a distributed caching system for a real-time collaborative editing platform (like Google Docs) that must handle concurrent edits from thousands of users on the same document while maintaining strong consistency and sub-10ms latency. How would you handle conflict resolution, version control, and cache invalidation when multiple users edit the same document simultaneously?","channel":"system-design","subChannel":"system-design","difficulty":"advanced","tags":["distributed-caching","real-time-collaboration","conflict-resolution","operational-transformation","strong-consistency"],"companies":[]},{"id":"q-646","question":"Design a rate limiting system for a real-time chat application that needs to handle different types of messages with varying limits: text messages (100/min), file uploads (10/min), and API calls (1000/min). How would you implement a multi-tier rate limiter that prevents spam while ensuring critical messages (like emergency alerts) are always delivered?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","real-time-systems","token-bucket","redis","priority-queues"],"companies":[]},{"id":"q-647","question":"Design a rate limiting system for a video streaming platform that needs to limit API calls based on both user subscription tier (free: 1000 calls/day, premium: 10000 calls/day) AND content type (video uploads: 10/hour, metadata queries: 100/minute). How would you implement multi-dimensional rate limiting while ensuring fair usage and preventing abuse?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","redis","multi-dimensional","api-gateway","token-bucket"],"companies":[]},{"id":"q-649","question":"How would you design a communication pattern for a microservices architecture where services need to handle both request-response interactions and event-driven updates, while ensuring backward compatibility during API version transitions?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["microservices","api-versioning","hybrid-communication","backward-compatibility","system-design"],"companies":[]},{"id":"q-650","question":"Design a rate limiting system for a video streaming platform that needs to limit API calls based on both user tier (free/premium) and content type (metadata vs video transcoding requests). How would you implement a multi-dimensional rate limiter that can handle 100K concurrent users with different limits per content type while preventing abuse and ensuring fair resource allocation?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","redis","multi-dimensional","user-tiers","content-aware"],"companies":[]},{"id":"q-658","question":"How would you design a multi-tier distributed caching strategy for a microservices architecture where different services have varying access patterns and consistency requirements?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["distributed-caching","microservices","cache-tiers","consistency-patterns"],"companies":[]},{"id":"q-764","question":"Design a rate limiting system for a content delivery network (CDN) that needs to enforce different limits based on content type and geographic region. Static assets (images, CSS) can serve 10,000 req/min globally, while dynamic API endpoints have stricter limits: 1,000 req/min per region for North America, 500 req/min for Europe, and 200 req/min for Asia-Pacific. How would you implement a hierarchical rate limiter that balances global fairness with regional capacity constraints?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","cdn","geo-distribution","redis","hierarchical-limits"],"companies":[]},{"id":"q-1015","question":"Design a TensorFlow 2.x data pipeline for a document-classification model trained on 8 GPUs with MirroredStrategy. Data comes from two sources: TFRecords with image_raw and a CSV with per-record numeric metadata. Build a single tf.data pipeline that yields a dict {'image': image_tensor, 'meta': meta_tensor}, with image decoded and resized to 224x224 and scaled to [0,1], metadata normalized, deterministic per-epoch shuffling with a fixed seed, interleaving sources with parallelism, caching, and prefetching. Then implement gradient accumulation to reach a global batch size of 1024 while per-replica batch size is 128, and outline reproducibility checks and simple throughput measurements. Provide key code blocks?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Bloomberg","Meta","Microsoft"]},{"id":"q-1072","question":"You're deploying a TensorFlow 2.x recommender with dense features and a massive sparse feature 'item_id'. The item vocabulary comes from Redis and updates in real time without downtime. Describe a practical serving approach that keeps latency under 20 ms, handles unseen IDs gracefully, and updates embeddings without restarting the service. Include a minimal code sketch showing how to load a Redis-backed vocabulary into a tf.lookup MutableHashTable and map incoming IDs to embeddings inside a tf.function?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Adobe","Tesla"]},{"id":"q-1103","question":"Scenario: You have a dataset of short audio clips stored as WAV files in data/train/{class}/*.wav. As a beginner TensorFlow developer, implement a minimal end-to-end solution: (1) a tf.data pipeline that reads file paths and infers the label from the parent directory, (2) loads WAVs as mono, (3) pads/trims to 16000 samples, (4) normalizes to [-1,1], (5) shuffles with a fixed seed, (6) caches and prefetches, (7) batches 32. Then define a tiny Conv1D classifier for 2 classes and show how to train with model.fit using the pipeline. Include only the essential code blocks?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Meta","MongoDB"]},{"id":"q-1168","question":"You’re building a TensorFlow model that jointly processes images and captions stored in a CSV with columns: image_path, caption, label. Implement an efficient tf.data pipeline that (1) reads the CSV, (2) loads and decodes images from disk with aspect-ratio-preserving resize to 224x224, (3) tokenizes captions using a saved BPE tokenizer loaded from a file, (4) pads captions to the max length within each batch, (5) caches, (6) shuffles with a fixed seed, (7) batches 64, (8) runs under a multi-GPU distribution strategy. Provide the core code blocks and discuss performance trade-offs?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Amazon","Google"]},{"id":"q-1252","question":"You are training a large Transformer-based recommender model on multiple GPUs with a custom training loop in TensorFlow 2.x. The per-device batch is 256, but you want an effective global batch of 4096. Describe and implement how to use gradient accumulation to achieve this, including how to adjust the learning rate, BN handling, and mixed-precision considerations?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["NVIDIA","Snap"]},{"id":"q-1384","question":"Describe how to deploy a TensorFlow model that accepts variable-length input sequences in production without a fixed max_length. Include input signatures, RaggedTensor usage, encoder compatibility, dynamic batching (bucketed), memory/latency considerations, and validation strategy with latency and throughput benchmarks?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Microsoft","Snowflake"]},{"id":"q-1422","question":"You have a local dataset of 50k JPEG images organized as train/{class} and val/{class}. Build a beginner-friendly TensorFlow 2.x data pipeline that trains a simple CNN on CPU. Describe and implement reading files with tf.data, decoding JPEG, resizing to 128x128, applying basic augmentations, and batching with prefetch. Include a method to verify input throughput keeps the trainer busy?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Amazon","Google"]},{"id":"q-1454","question":"Given a directory of JPEG images and a CSV file with two columns (path, label) for a 2-class image classification task, design a beginner-friendly tf.data pipeline that keeps the GPU busy on a single GPU. What steps would you include and provide a minimal code snippet using cache, shuffle, batch, and prefetch?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Amazon","Apple","Zoom"]},{"id":"q-1510","question":"You're deploying a browser-based image classifier in a product used by large-scale apps (e.g., Discord, Instacart, Lyft). Describe end-to-end how to convert a trained Keras MobileNetV2 model to TensorFlow.js, including quantization choices and asset packaging, and how to serve it from a static site. Then provide a minimal JavaScript snippet to load the model, preprocess a 224x224 HTMLImageElement, and output the top-3 class labels?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Discord","Instacart","Lyft"]},{"id":"q-1518","question":"On a workstation with 4 GPUs, train a multi-modal model (image + text) using a shared backbone and two heads. How would you implement a single training loop that (1) accumulates gradients to emulate a larger global batch, (2) applies per-branch loss weights to form a stable total loss under mixed-precision, and (3) keeps data sharding synchronized across GPUs? Provide a minimal code sketch?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["OpenAI","Snowflake","Tesla"]},{"id":"q-1629","question":"You are building a real-time fraud-detection model in TensorFlow 2.x. Data arrives as streaming JSON with dense features and a high-cardinality categorical field. Design a streaming tf.data pipeline that hashes the categorical feature, caches preprocessed tensors, and trains with focal loss on a single-GPU setup. Provide a minimal, runnable dataset builder and focal loss snippet that demonstrates the end-to-end flow?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["IBM","Salesforce","Tesla"]},{"id":"q-1739","question":"You’re deploying a multi-tenant vision model behind TensorFlow Serving on Kubernetes. Tenants share a single model graph but require different input normalization pipelines (e.g., color space, resizing strategy, and augmentation). How would you design a solution that isolates per-tenant preprocessing without duplicating the model, keeps latency under 50 ms per inference, and allows updating tenant parameters without redeploying the model?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Adobe","Google","Oracle"]},{"id":"q-1752","question":"In training a Transformer language model on 4 GPUs with mixed precision using tf.distribute.MirroredStrategy, how would you implement gradient accumulation to simulate a larger global batch while keeping updates stable? Provide a minimal code snippet showing an accumulation loop and the cadence for applying the optimizer?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["DoorDash","Microsoft","MongoDB"]},{"id":"q-1840","question":"How would you implement a multi‑objective training loop in TensorFlow 2.x that trains a model with a primary cross-entropy loss and an auxiliary contrastive loss under tf.distribute.MultiWorkerMirroredStrategy, ensuring stable convergence via dynamic loss weighting (GradNorm), per-batch gradient normalization, and proper sequence masking for variable-length inputs?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["IBM","NVIDIA","Twitter"]},{"id":"q-1859","question":"You are training a graph neural network for molecular property prediction with graphs of varying sizes. The dataset is stored as sharded TFRecord files on GCS and you want multi-GPU training (2–4 GPUs). Propose a tf.data pipeline that buckets graphs by node count, pads to the bucket max, preserves per-epoch shuffling, and integrates with tf.distribute.Strategy. Describe the approach and provide a minimal batching sketch?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Cloudflare","Google","Lyft"]},{"id":"q-2081","question":"In a distributed training setup with tf.distribute.MultiWorkerMirroredStrategy across 8 workers, how would you guarantee deterministic sharding and data order for a long-text Transformer training run? Include how to set global_batch_size, per_replica_batch_size, dataset sharding via input_context/experimental_distribute_dataset, and seeds/flags to ensure reproducibility; avoid hidden data leakage across workers?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Citadel","Cloudflare"]},{"id":"q-2098","question":"Using TensorFlow 2.x, implement a gradient accumulation training loop inside tf.distribute.MirroredStrategy to achieve an effective batch size of 1024 with per-step batch 64 on multi-GPU. Include how you would apply loss scaling for mixed precision, and how to accumulate and apply gradients every 'accum_steps' steps. Provide skeleton code for the train_step and train_loop, and explain memory and determinism considerations?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Discord","Google"]},{"id":"q-2161","question":"You're deploying a Transformer-based sequence model for a real-time recommender in TensorFlow Serving behind a microservice API. Clients send requests with variable-length sequences up to 1024 tokens. You need dynamic batching, mixed precision, and memory-efficient attention to meet P95 latency under 30 ms at 1k RPS, plus canary rollouts. Outline the end-to-end approach: preprocessing, model packaging, dynamic batching strategy, memory management, and benchmarking. Which TF APIs and Serving config would you use, and what are the trade-offs?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Databricks","DoorDash"]},{"id":"q-857","question":"You’re deploying a multimodal TensorFlow 2.x Keras model that consumes an image [N,224,224,3] and a text embedding [N,128] to TensorFlow Serving on Kubernetes. Explain how to export a SavedModel with a serving_default signature that accepts a dict input {'image': ..., 'text': ...} and a separate 'predict_dense' signature for A/B testing. Include concrete input signatures, how to create concrete_functions, and how to manage versioning for backward compatibility?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["DoorDash","Meta","Slack"]},{"id":"q-943","question":"You're running distributed TensorFlow training with tf.distribute.MultiWorkerMirroredStrategy across 8 workers. Intermittent batch loss suggests non-deterministic per-worker data sharding and uneven batch boundaries. Describe a concrete fix: deterministic sharding, fixed seeds, per-replica batch sizing, and validation steps; specify exact API calls, TF_CONFIG handling, and how you'll verify convergence is repeatable?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Airbnb","Citadel"]},{"id":"q-966","question":"How would you deploy a text classifier in TF2 that must support vocab expansion without retraining? Provide a single SavedModel with two signatures: 'predict' for input {'texts': tf.Tensor<String>} and 'extend_vocab' for {'new_tokens': tf.Tensor<String>, 'vectors': tf.Tensor<float>}. Explain embedding resizing, token→id mapping, stateful management, and versioning; include a minimal code outline?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Citadel","Hugging Face","Oracle"]},{"id":"q-992","question":"You’re building a beginner TensorFlow image classifier. The dataset sits under data/train with subfolders per class (e.g., cat/dog). Write a minimal tf.data pipeline that (1) reads image files with automatic label inference, (2) decodes and resizes to 224x224, (3) scales pixels to [0,1], (4) shuffles with a fixed seed for reproducibility, (5) batches 32, and (6) caches to speed training. Include the key code blocks?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Anthropic","Snowflake","Tesla"]},{"id":"do-3","question":"What is Infrastructure as Code (IaC) and why is Terraform preferred over manual infrastructure management?","channel":"terraform","subChannel":"basics","difficulty":"beginner","tags":["infra","automation","terraform"],"companies":["Airbnb","Amazon","Google","Meta","Microsoft","Netflix","Stripe","Uber"]},{"id":"gh-17","question":"What is Terraform and how does it implement Infrastructure as Code (IaC) workflows?","channel":"terraform","subChannel":"basics","difficulty":"beginner","tags":["iac","terraform","ansible"],"companies":["Airbnb","Databricks","Goldman Sachs","Microsoft","Snowflake"]},{"id":"de-137","question":"You have a Terraform configuration that creates an AWS S3 bucket. After running 'terraform apply', you realize you need to add versioning to the bucket. What's the safest way to modify your existing infrastructure?","channel":"terraform","subChannel":"best-practices","difficulty":"beginner","tags":["terraform","iac"],"companies":["Amazon","Google","Hashicorp","Microsoft","Netflix"]},{"id":"q-272","question":"How would you implement a DRY Terraform configuration using Terragrunt and Atlantis for multi-environment deployments?","channel":"terraform","subChannel":"best-practices","difficulty":"intermediate","tags":["dry","terragrunt","atlantis"],"companies":["Amazon","Google","Netflix","Stripe"]},{"id":"q-284","question":"Design a production-grade Terraform architecture for a multi-environment AWS infrastructure with 100+ resources, including state management, CI/CD integration, and security controls. How would you handle state locking, workspace strategy, and deployment validation?","channel":"terraform","subChannel":"best-practices","difficulty":"advanced","tags":["infrastructure-as-code","automation","best-practices"],"companies":["Amazon","Google","Hashicorp","Microsoft","Netflix","Snowflake"]},{"id":"q-1049","question":"In a multi-account AWS setup, a core Terraform module is versioned in a private registry and consumed by 12 workspaces. A regional failover requires a safe rollback to the previous core module version without drift. Describe the end-to-end strategy, including version pinning, CI validation, and state/rollback mechanisms?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["Citadel","Goldman Sachs","Lyft"]},{"id":"q-1197","question":"In a multi-account AWS setup, a single Terraform repo provisions VPCs and IAM roles per environment using provider aliases. A governance rule requires per-environment tagging and automatic drift detection that blocks non-Terraform changes. Describe a concrete pattern to enforce per-account isolation, tagging, and drift guardrails, including provider aliasing, remote state per environment, and a PR-based drift test workflow?","channel":"terraform","subChannel":"general","difficulty":"intermediate","tags":["terraform"],"companies":["Citadel","Twitter"]},{"id":"q-1271","question":"You have a Terraform project that provisions an AWS VPC and a small app stack. You want developers to run the same config against their own environments using per-environment secrets (DB_PASSWORD, APP_SSH_KEY) that are never stored in git. Outline a minimal structure (files, vars, and commands) to supply these secrets safely, and explain how you prevent secrets from triggering plan changes or drift?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["LinkedIn","Microsoft","Twitter"]},{"id":"q-1615","question":"You have a VPC with public and private subnets and an Internet Gateway. You want to optionally provision a NAT Gateway in the public subnet based on a boolean var create_nat_gateway (default true). How would you implement conditional creation of the Elastic IP, NAT Gateway, and the private route to 0.0.0.0/0 using Terraform 0.12+ syntax? Explain how you handle plan stability for existing deployments when the flag toggles?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Meta","Plaid","Snap"]},{"id":"q-1666","question":"You manage a Terraform project that already provisions a VPC with a public subnet, a private subnet, and an EC2 instance in the private subnet via a NAT Gateway. Add a feature flag to optionally create an RDS instance in the private subnet, but only when var.create_rds is true. Ensure running 'terraform apply' in non-prod environments does not touch the RDS resource. Describe the exact Terraform changes you would make, including the variable declaration, the RDS resource, and any dependencies, with a minimal disruption?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["NVIDIA","Two Sigma"]},{"id":"q-2099","question":"Create a minimal Terraform setup using provider aliases for Cloudflare and IBM Cloud to provision a single IBM Cloud VM and a Cloudflare A record for example.com, such that the A record always points to the VM's public IP. Outline folder structure, how to reference outputs, and how updates occur with no downtime?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Cloudflare","IBM","Stripe"]},{"id":"q-479","question":"You're managing a multi-region infrastructure with 50+ Terraform modules. How would you design a strategy to handle state locking, drift detection, and safe deployments across regions while minimizing downtime?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["Microsoft","Uber"]},{"id":"q-508","question":"You have a Terraform configuration that creates multiple EC2 instances across different availability zones. How would you implement a blue-green deployment strategy using Terraform workspaces and what are the key considerations?","channel":"terraform","subChannel":"general","difficulty":"intermediate","tags":["terraform"],"companies":["Citadel","PayPal","Scale Ai"]},{"id":"q-563","question":"You're deploying a simple web application using Terraform. How would you create an AWS EC2 instance with a security group that allows HTTP traffic on port 80?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Coinbase","Discord","Slack"]},{"id":"q-592","question":"How would you use Terraform variables to manage different environments (dev/staging/prod) while keeping your configuration DRY?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Oracle","Snowflake","Stripe"]},{"id":"q-854","question":"In a Terraform Cloud setup spanning AWS and GCP, you must enforce a cross-cloud policy: every resource must carry a non-empty 'cost-center' tag and new regions must not auto-create default VPCs. How would you implement drift detection, policy gating, and automatic remediation across workspaces without downtime?","channel":"terraform","subChannel":"general","difficulty":"intermediate","tags":["terraform"],"companies":["Amazon","Discord","Google"]},{"id":"q-983","question":"In a Terraform project that provisions an AWS S3 bucket, add a new boolean variable enable_sse to toggle server-side encryption; when enable_sse is true, the bucket should have server-side encryption AES256 enabled. How would you implement this in the bucket resource using Terraform 0.12+ syntax, ensuring existing deployments remain stable and the plan doesn't force unnecessary changes?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Microsoft","Tesla"]},{"id":"gh-105","question":"What is Infrastructure Drift and how do you detect and prevent it?","channel":"terraform","subChannel":"state-management","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Stripe"]},{"id":"q-175","question":"You have a Terraform configuration with multiple developers working on the same infrastructure. How would you implement remote state locking to prevent state corruption and enable team collaboration?","channel":"terraform","subChannel":"state-management","difficulty":"intermediate","tags":["state","backend"],"companies":["Amazon","Google","Microsoft","Stripe","Uber"]},{"id":"q-221","question":"How would you implement a zero-downtime blue-green deployment strategy using Terraform workspaces, remote state locking, and Atlantis for production-scale microservices?","channel":"terraform","subChannel":"state-management","difficulty":"advanced","tags":["dry","terragrunt","atlantis"],"companies":["Amazon","Google Cloud","Microsoft","Stripe","Uber"]},{"id":"q-247","question":"How does Terraform remote state prevent conflicts when multiple team members work on the same infrastructure, and what are the key mechanisms involved?","channel":"terraform","subChannel":"state-management","difficulty":"beginner","tags":["remote-state","locking","workspaces"],"companies":["Amazon","Hashicorp","Microsoft","Netflix","Stripe"]},{"id":"q-1009","question":"Scenario: a single repo provisions per-tenant AWS resources for many tenants. To isolate state without workspaces, use a shared S3 backend with a DynamoDB lock table and a tenant-scoped key. Describe the backend config (bucket, region, dynamodb_table, key pattern per tenant), how you detect drift across tenants, and CI gating for dev-to-prod promotions (plan -out, tests, approve, apply)?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Amazon","Coinbase","Databricks"]},{"id":"q-1154","question":"In a Terraform module that provisions an AWS RDS instance, operators sometimes alter maintenance_window directly in AWS, creating drift. You want Terraform to ignore external changes to maintenance_window but still apply code-driven updates (e.g., allocated_storage). How would you implement this using a lifecycle block? Provide a minimal aws_db_instance snippet showing ignore_changes for maintenance_window and outline tradeoffs?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Salesforce","Scale Ai","Tesla"]},{"id":"q-1172","question":"In AWS us-east-1, you must provision 3 private subnets in a single VPC across 3 AZs using one module. Define a map variable with AZs and CIDRs, create the subnets with for_each, attach appropriate tags, and output the subnet IDs. Then show how to reference these IDs in a resource that requires subnet_id (eg NAT Gateway) and justify for_each vs count?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Netflix","Oracle"]},{"id":"q-1202","question":"Scenario: you must bring under Terraform management a set of existing AWS S3 buckets across teams. Some buckets already exist and must be imported; you will manage encryption and versioning with a single module using for_each over a bucket map. How would you import, structure, and promote changes safely via CI?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Amazon","NVIDIA","Robinhood"]},{"id":"q-1338","question":"Scenario: You maintain a Terraform repo that provisions an AWS RDS primary in us-east-1 and an optional cross-region read replica in us-west-2 controlled by a feature flag var.enable_replica. If the flag is false, Terraform should destroy the replica on apply. How would you implement the cross-region provider, conditional resource creation, and safe destruction without leaving dangling resources? Provide code patterns for the provider alias and the replica resource?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Hugging Face","NVIDIA","Netflix"]},{"id":"q-1367","question":"You\\'re managing 100 AWS subnets defined in a single Terraform module using for_each. A drift occurred in one subnet\\'s route_table_id because it was changed outside Terraform. Describe exactly how you\\'d detect the drift and fix only that resource with zero-drift impact, using Terraform CLI commands. Include the exact commands to init plan, taint the resource, apply with -target, and re-run a full plan to confirm drift-free state?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Databricks","IBM","Meta"]},{"id":"q-1584","question":"You are refactoring a Terraform repo and moving an existing AWS S3 bucket resource from the root module into a submodule named storage. Describe exact steps and commands to relocate the resource in state without recreation, including the state mv addresses and a follow-up plan?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Coinbase","Instacart","Zoom"]},{"id":"q-1655","question":"You discover an AWS VPC and related resources created outside Terraform and want to bring them under a new network module without recreating. Describe exact steps to import the VPC, its subnets (for_each), and a peering connection into the module, including the resource addresses you’ll use, how to handle multiple subnets, and how to verify with a plan that nothing changes?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Lyft","PayPal"]},{"id":"q-1740","question":"Advanced cross-repo Terraform: network state is stored in a dedicated repo/backends across envs. The application repo imports VPC IDs and subnets via data terraform_remote_state. Explain how you would structure backends and modules to avoid drift, ensure isolation between dev/staging/prod, and orchestrate safe promotions from dev to prod with exact commands for init/plan/apply per environment. Include how you would test drift and rollback?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Adobe","Databricks"]},{"id":"q-1749","question":"You manage a single Terraform repo deploying a shared 'network' module into two AWS accounts (prod and dev) using provider aliases. Describe how you structure the provider blocks, module calls, and var-files to deploy both environments without duplicating code. Include exact commands to init, plan, and apply for each environment, ensuring isolation and no cross-env state changes?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Robinhood","Slack","Snap"]},{"id":"q-1805","question":"How would you implement input validation in a Terraform module to enforce that a variable instance_count is between 1 and 5 and that a string region is one of an allowed set? Provide the approach and expected Terraform behavior during plan and apply?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Netflix","Square","Tesla"]},{"id":"q-1836","question":"Scenario: A Terraform repo uses a module named network that creates subnets via for_each over a map of subnet_id to name. Three subnets were created manually (subnet-aaa, subnet-bbb, subnet-ccc) and must be brought under Terraform control without changing the rest. Describe exact steps and commands to import only these existing subnets into the module state, update the for_each map to include their IDs, and verify drift with a full plan. Include init/plan/apply commands and how to handle dependent resources (route tables, tags)?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["NVIDIA","Plaid"]},{"id":"q-1888","question":"Scenario: You manage 120 Cloudflare DNS records in a single Terraform module with for_each. A drift occurred in one record's IP address due to a manual edit in Cloudflare. Describe a surgical plan to detect drift and fix only that resource with zero-drift impact, using Terraform CLI commands. Include the exact commands to init, plan, taint the resource, apply with -target, and re-run a full plan to confirm drift-free state?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Citadel","Microsoft","Plaid"]},{"id":"q-1948","question":"You’re extending a VPC Terraform module to support both a managed NAT Gateway and a NAT instance via a single input variable named nat_type with default 'gateway'. How would you implement this with conditional resources, including the variable, two resource blocks, and outputs? Provide the minimal code and explain how you’d validate with terraform plan to ensure no changes when nat_type remains 'gateway'?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Google","PayPal","Twitter"]},{"id":"q-1976","question":"In a monorepo, a Terraform module provisions a multi-tenant AWS VPC setup with per-tenant isolation via tenant_id. A new tenant must be added without touching existing ones. Describe exact steps and commands to add the tenant using a fresh state segment, ensure no drift to others, and verify with targeted and full plans. Include backend-key organization and apply sequencing?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Goldman Sachs","LinkedIn"]},{"id":"q-2037","question":"You're provisioning a private API gateway via a custom Terraform provider. Implement a blue/green deployment pattern with two gateway instances and an atomic traffic switch using a single Terraform plan. Describe the exact structure (for_each, modules, and a central traffic_split map), how to prevent unintended recreation, and the precise steps and commands to drift-check and promote from blue to green in prod with minimal blast radius?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Netflix","PayPal"]},{"id":"q-2049","question":"You maintain a per-environment security module that provisions an AWS Security Group with environment-scoped rules (dev, stg, prod). An external admin altered prod inbound 22 to 0.0.0.0/0, causing drift. Describe exact steps to detect the drift and fix prod only, using Terraform CLI with init/plan/apply and a -target run, then re-run a full plan to confirm no drift. Provide the exact resource address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Meta","Tesla"]},{"id":"q-2079","question":"In a Terraform repo using a single VPC module across AWS prod and canary via provider aliases, a CIDR change would recreate many subnets. Describe a blue/green deployment strategy that updates production with zero downtime. Include exact commands to initialize, create/apply canary resources with targeted -target, run drift checks, then migrate canary state to prod with terraform state mv and finalize with a full plan?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Bloomberg","Netflix"]},{"id":"q-2127","question":"You manage a Terraform repo with a Google Cloud IAM bindings module using for_each over projects. A drift occurred when a member was removed in prod outside Terraform. Describe exact steps to detect the drift and fix only prod binding using Terraform CLI with init/plan/apply and a -target run, then re-run a full plan to confirm no drift. Include the exact resource address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Adobe","Google","Slack"]},{"id":"q-2168","question":"You find an AWS IAM role that was created outside Terraform and you want to bring it under Terraform control without changing its attached policies—what exact steps and commands would you use to import it, reconcile the state, and apply only the intended configuration?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Adobe","Cloudflare","Coinbase"]},{"id":"q-864","question":"In Terraform, you need to manage two AWS accounts in a single repo: prod (provider aws.prod) and audit (provider aws.audit). Create an S3 bucket in prod and a cross-account IAM policy in audit that grants read access to that bucket. How do you configure aliased providers, reference the prod bucket ARN from the audit module, and enforce deterministic apply order (e.g., data fetch before policy) with minimal risk? Include a minimal config outline?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Snap","Snowflake","Square"]},{"id":"q-960","question":"In a single Terraform repo that provisions prod, staging, and dev AWS environments, how would you configure a single S3 backend to isolate state for each environment without using separate Terraform workspaces? Provide the exact backend config (bucket, region, dynamodb_lock_table, key per env) and explain how you would promote changes from dev to prod, including drift handling and CI plan/apply steps?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Bloomberg","Google","Hashicorp"]},{"id":"q-1033","question":"You build a small service that fetches user profiles via GET /api/users/{id} and caches results in memory for 5 minutes. Write concrete tests that verify: (1) a cache miss calls the API, stores the result with TTL; (2) a cache hit returns cached value without API call; (3) TTL expiry triggers a fresh API call to refresh cache. Provide example test code?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["Google","Lyft","Netflix"]},{"id":"q-1076","question":"You implement a debounce utility in frontend code: debounce(fn, wait) returns a wrapper that ensures fn is called at most once per wait ms, using the last invocation's arguments. Write a focused test that demonstrates rapid successive calls do not trigger fn more than once, and that a final call after waiting triggers with the latest args?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["DoorDash","Instacart","Oracle"]},{"id":"q-1287","question":"Design a testing strategy for a real-time data pipeline built with Apache Flink processing millions of events per second, ensuring exactly-once semantics across sources and sinks, handling out-of-order and late data, with stateful operators and checkpointing. Outline how you'd structure unit, integration, and end-to-end tests, simulate late data and failures, verify sink idempotence and recovery guarantees, and specify concrete tools and success criteria?","channel":"testing","subChannel":"general","difficulty":"advanced","tags":["testing"],"companies":["Amazon","Google","Scale Ai"]},{"id":"q-2150","question":"You're deploying a Delta Lake ingestion pipeline on Databricks: a Spark Structured Streaming job reads from a Kafka topic (Avro, Schema Registry), upserts into a partitioned Delta table, and downstream queries rely on the latest state. Design a test plan that (1) proves exactly-once semantics and idempotent upserts under retry/replay, (2) validates safe schema evolution without breaking downstream code, and (3) detects data quality regressions (missing keys, late data) within a 24h window. Include artifacts, environments, and metrics?","channel":"testing","subChannel":"general","difficulty":"advanced","tags":["testing"],"companies":["Databricks","IBM","Scale Ai"]},{"id":"q-480","question":"How would you design a comprehensive testing strategy for a distributed microservices architecture handling 10M requests/day, ensuring 99.9% uptime while maintaining fast CI/CD pipelines?","channel":"testing","subChannel":"general","difficulty":"advanced","tags":["testing"],"companies":["Bloomberg","LinkedIn","NVIDIA"]},{"id":"q-509","question":"How would you test a REST API endpoint that creates a user account, including validation, error handling, and database integration?","channel":"testing","subChannel":"general","difficulty":"intermediate","tags":["testing"],"companies":["Amazon","Coinbase","Tesla"]},{"id":"q-537","question":"You're testing a real-time chat application that uses WebSockets. How would you design a test strategy to verify message ordering, connection resilience, and concurrent user scenarios?","channel":"testing","subChannel":"general","difficulty":"intermediate","tags":["testing"],"companies":["NVIDIA","Twitter","Two Sigma"]},{"id":"q-593","question":"How would you test a function that makes HTTP requests to an external API? What testing strategies would you use?","channel":"testing","subChannel":"general","difficulty":"intermediate","tags":["testing"],"companies":["Apple","Microsoft"]},{"id":"q-990","question":"You maintain a Node.js API function getUser(userId) that reads from MongoDB via Mongoose and caches the result in an in-memory TTL cache (60s). Write a practical test plan and code to verify: (1) first call hits DB and caches, (2) second call returns cached value, (3) after TTL expires a new DB hit occurs and cache updates, (4) DB error propagates to caller. Use Jest with minimal mocks and demonstrate time-control?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["Airbnb","MongoDB"]},{"id":"q-259","question":"How would you design integration tests for a Saga pattern implementation across 5 microservices to ensure exactly-once transaction processing and proper compensation handling during partial failures?","channel":"testing","subChannel":"integration-testing","difficulty":"advanced","tags":["api-testing","database-testing","mocking"],"companies":["Airbnb","Amazon","LinkedIn","Netflix","Spotify","Twitter","Uber"]},{"id":"q-207","question":"How would you implement a test-driven development workflow for a REST API endpoint using Jest and Supertest, following the red-green-refactor cycle with proper test organization and mocking strategies?","channel":"testing","subChannel":"tdd","difficulty":"intermediate","tags":["test-driven","red-green-refactor","test-first"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-360","question":"You're building a simple calculator class. Write a failing test first, then implement the add method using TDD. What's the red-green-refactor cycle?","channel":"testing","subChannel":"tdd","difficulty":"beginner","tags":["test-driven","red-green-refactor","test-first"],"companies":["Amazon","Anthropic","Google","Meta","Microsoft","Salesforce","Stripe"]},{"id":"q-405","question":"You're building a real-time collaborative drawing feature where multiple users can simultaneously edit a canvas. How would you apply TDD to test the conflict resolution mechanism when two users edit the same element at the same time?","channel":"testing","subChannel":"tdd","difficulty":"intermediate","tags":["test-driven","red-green-refactor","test-first"],"companies":["Canva","Unity","Zoom"]},{"id":"q-234","question":"How would you design a scalable test architecture for a microservices application handling 10,000+ concurrent tests across multiple environments while ensuring test isolation, performance, and CI/CD integration?","channel":"testing","subChannel":"test-strategies","difficulty":"advanced","tags":["jest","mocha","pytest","junit"],"companies":["Amazon","Google","Microsoft","Netflix","Salesforce","Stripe"]},{"id":"q-278","question":"How would you design a comprehensive testing strategy for a microservices architecture that scales to handle millions of requests per second while ensuring 99.99% availability?","channel":"testing","subChannel":"test-strategies","difficulty":"advanced","tags":["test-pyramid","coverage","mutation-testing"],"companies":["Amazon","Google","Meta","Netflix"]},{"id":"q-325","question":"How would you implement mutation testing to validate your test suite's quality and what's the relationship between mutation testing and code coverage?","channel":"testing","subChannel":"test-strategies","difficulty":"advanced","tags":["test-pyramid","coverage","mutation-testing"],"companies":["Epic Systems","Jane Street","Western Digital"]},{"id":"q-349","question":"You're building a distributed event streaming platform similar to Kafka. How would you design a comprehensive testing strategy that ensures message ordering guarantees, exactly-once semantics, and fault tolerance across a cluster of brokers?","channel":"testing","subChannel":"test-strategies","difficulty":"advanced","tags":["test-pyramid","coverage","mutation-testing"],"companies":["Confluent","Epic Games","Meta"]},{"id":"q-374","question":"You're testing a ServiceNow form validation module. How would you structure your test pyramid and what coverage metrics would you track?","channel":"testing","subChannel":"test-strategies","difficulty":"beginner","tags":["test-pyramid","coverage","mutation-testing"],"companies":["Amazon","Google","Microsoft","Netflix","Okta","Salesforce","Servicenow"]},{"id":"q-416","question":"You're building a React component library. How would you structure your test pyramid and what specific coverage metrics would you target for each layer?","channel":"testing","subChannel":"test-strategies","difficulty":"beginner","tags":["test-pyramid","coverage","mutation-testing"],"companies":["Broadcom","Hugging Face","Meta"]},{"id":"q-296","question":"In Jest, how would you implement advanced mocking patterns including sequential return values, async behavior, and proper mock lifecycle management for comprehensive test coverage?","channel":"testing","subChannel":"unit-testing","difficulty":"intermediate","tags":["jest","mocha","pytest","junit"],"companies":["Google","Meta","Netflix","Salesforce","Stripe"]},{"id":"q-311","question":"How do you mock a function in Jest that's called within another function under test?","channel":"testing","subChannel":"unit-testing","difficulty":"advanced","tags":["jest","mocha","pytest","junit"],"companies":["Amazon","Google","Meta"]},{"id":"q-338","question":"You're testing a React component that fetches user data from an API. How would you write a unit test using Jest to mock the API call and verify the component renders the user's name correctly?","channel":"testing","subChannel":"unit-testing","difficulty":"beginner","tags":["jest","mocha","pytest","junit"],"companies":["Cisco","Hulu","Postman"]},{"id":"q-388","question":"You're testing a REST API endpoint that returns user data. Write a basic unit test using Jest that verifies the endpoint returns a 200 status code and the response contains a 'name' field. What's the most important assertion to include?","channel":"testing","subChannel":"unit-testing","difficulty":"beginner","tags":["jest","mocha","pytest","junit"],"companies":["Postman","Retool","Supabase"]},{"id":"q-1002","question":"In a Unix environment logs are stored in /var/log/app/*.log with lines formatted as timestamp|user|action|resource. Write a practical one-liner using standard UNIX tools to output the top 5 users by total actions in the last 24 hours. Explain how you would handle log rotation and malformed lines?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Amazon","Robinhood"]},{"id":"q-1032","question":"How would you capture stdout and stderr of a simple shell command into separate log files while still displaying live output in the terminal? Provide a concrete Bash command and brief justification?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Databricks","LinkedIn","Uber"]},{"id":"q-1069","question":"In a Unix environment, logs live under /var/log/metrics/*.log and are hourly rotated to .log and .log.gz. Each line is like [YYYY-MM-DD HH:MM:SS] LEVEL: message. Propose a robust, portable approach (one-liner preferred) to output the number of ERROR events per hour for the last 6 hours, handling missing files and rotations without external dependencies?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Adobe","Meta","Plaid"]},{"id":"q-1123","question":"In a Unix environment, multiple services write JSON logs under /var/log/diag/*.log and rotated hourly to *.log.gz. Each line is a JSON object like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"ERROR\",\"msg\":\"...\"}; Propose a robust one-liner (no external dependencies beyond standard UNIX tools) to output the number of ERROR events per hour for the last 4 hours, aggregated across all services, and tolerant of missing files and rotated archives?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Apple","OpenAI","Snap"]},{"id":"q-1139","question":"Scenario: JSON logs at /var/log/diag/*.log, rotated hourly to *.log.gz. Each line: {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"ERROR\",\"msg\":\"...\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) that outputs the per-hour 99th percentile of message length for the last 4 hours, across all services, deduplicating identical messages per hour, and tolerant of missing files and gz archives?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["OpenAI","Plaid","Uber"]},{"id":"q-1224","question":"Scenario: In a Unix cluster, logs are emitted as JSON lines under /var/log/cluster/*/*.log and rotated hourly to *.log.gz. Each line contains {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"tenant\":\"tenant-id\",\"svc\":\"service\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}; Propose a robust one-liner (no external deps beyond standard UNIX tools) to identify the top 3 tenants by error rate (ERROR / total events) for the last 6 hours, aggregated across all services, tolerant of missing files and gz archives?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Apple","Coinbase","PayPal"]},{"id":"q-1241","question":"Scenario: In a multi-user Unix workspace, /home contains subdirectories per user with various files. Some are large, some are temporary. Provide a robust one-liner (no external dependencies) that lists the five largest regular files under /home (recursively), excluding hidden files and symlinks, printing each as 'size<TAB>path' with sizes in human-readable form. Explain how you ensure safety against spaces and newlines in filenames?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Databricks","Lyft","Tesla"]},{"id":"q-1492","question":"Scenario: multiple services log JSON lines to /var/log/app/*.log, rotated hourly to *.log.gz. Each line looks like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lat_ms\":123}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to compute the per-service average lat_ms in the last 2 hours, across all logs, tolerant of missing and gzipped files, ignoring malformed lines. Output: 'service avg_ms'?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["PayPal","Salesforce"]},{"id":"q-1809","question":"Scenario: A Linux host logs auth events into /var/log/auth/{service}/*.log and rotates hourly to *.log.gz. Each line is JSON like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"user\":\"alice\",\"action\":\"LOGIN\",\"result\":\"FAIL\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to print the top 5 users by failed login rate in the last 2 hours, aggregated across all services, tolerant of missing files and gz archives, and resilient to malformed lines?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Databricks","Robinhood","Tesla"]},{"id":"q-1903","question":"Propose a robust one-liner to compute the number of ERROR lines per hour for the previous 12 hours, aggregating across all logs in /var/log/app and across both uncompressed (*.log) and rotated/compressed files (*.log.*, *.log.gz). The one-liner must tolerate missing files and gz archives and rely only on standard UNIX tools?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Databricks","Slack","Stripe"]},{"id":"q-1975","question":"In a Unix host, logs live in /logs/web/*.log and are rotated hourly to *.log.gz. Each line is a JSON object like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"ip\":\"1.2.3.4\",\"method\":\"GET\",\"path\":\"/api\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to output the top 5 IPs by request count in the last 4 hours, aggregating across all files and rotations, tolerant of missing files and compressed archives?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Google","Snap","Twitter"]},{"id":"q-2094","question":"Scenario: In a Unix CI environment, logs are written by build agents under /build/logs/*/*.log and rotated hourly to *.log.gz. Each line is BUILDID|TIMESTAMP|STEP|STATUS|MESSAGE. Propose a robust one-liner (no external deps beyond standard UNIX tools) to report, for the last 24 hours, per BUILDID and per hour, the count of failed steps (STATUS != 'SUCCESS'), aggregating across all agents and rotations and tolerating missing files and compressed archives?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Apple","IBM"]},{"id":"q-481","question":"You're debugging a production system where processes are hanging. Using only Unix tools, how would you identify which processes are blocked on I/O, what they're waiting for, and safely terminate them without causing data corruption?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Snap","Snowflake"]},{"id":"q-510","question":"You're debugging a production issue where a process is stuck in uninterruptible sleep (D state). How would you identify and handle this situation?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["OpenAI","Tesla"]},{"id":"q-538","question":"You notice a process is consuming excessive CPU on a production server. How would you diagnose and troubleshoot this issue using Unix commands?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Citadel","Goldman Sachs","Microsoft"]},{"id":"q-564","question":"You're debugging a production system where processes are hanging. Using only Unix tools, how would you identify which processes are stuck in uninterruptible sleep (D state) and what could be causing this?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Adobe","OpenAI","Square"]},{"id":"q-894","question":"On a Linux host, /var/log/myapp.log is written by multiple processes. Implement a robust log rotation that triggers when the file reaches 100MB, keeps 7 rotated files, compresses older logs, and ensures no log loss while writers continue. Describe the approach, commands, and failure modes for concurrent writers?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Coinbase","Discord","OpenAI"]},{"id":"q-264","question":"How do Unix pipes enable inter-process communication and what are their performance implications?","channel":"unix","subChannel":"system-programming","difficulty":"beginner","tags":["posix","signals","pipes","sockets"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"q-1014","question":"Scenario: A multinational SaaS runs Vault with cross-region replication. Tenants use Vault's DB Secrets Engine for Postgres with per-tenant roles and short TTLs. Explain how you would enforce per-tenant rotation and immediate revocation on disable, while preventing cross-tenant leakage during DR failover?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Adobe","Goldman Sachs","MongoDB"]},{"id":"q-1054","question":"Scenario: A multi-tenant SaaS stores per-tenant API keys in Vault KV v2 with versioning. A rotation accidentally overwrote the previous key. Describe exactly how you would recover the previous version, which Vault paths and commands you'd use, and what policy controls you would enforce to prevent accidental deletions and ensure auditability?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Coinbase","Hugging Face","Tesla"]},{"id":"q-1094","question":"Design a per-tenant envelope encryption workflow using Vault Transit: each tenant has a dedicated Transit key; generate a per-tenant DEK, encrypt data with the DEK, and store the ciphertext. How would you rotate the Transit key without downtime, rewrap existing ciphertext to the new version, handle tenant disablement (revocation), and maintain end-to-end auditability? Include specific Vault paths and commands?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Hashicorp","Instacart","Twitter"]},{"id":"q-1174","question":"In a multi-tenant SaaS, you store per-tenant data encryption keys in Vault Transit with a dedicated key per tenant and daily rotation. Describe how you would implement per-tenant key lifecycle (creation, rotation without downtime, data rewrap for existing data, and immediate revocation when a tenant is disabled) and how you would monitor/audit across services?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Hugging Face","MongoDB","Stripe"]},{"id":"q-1229","question":"You’re building a multi-tenant SaaS and plan to encrypt data at rest using Vault's Transit engine. Describe how you would enable Transit, create per-tenant keys, implement encryption/decryption via the API, and perform zero-downtime key rotation while preserving ciphertext integrity. Include policy considerations to prevent data leakage and how you validate rotation?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Hugging Face","Meta","Tesla"]},{"id":"q-1357","question":"In a microservices setup, Vault's PKI engine issues short-lived TLS certs for mTLS. Describe end-to-end: enable PKI at path pki, create a role microservice-tls with allowed_domains='service.local', allow_subdomains=true, max_ttl='24h', issue a cert for 'auth-service.service.local', and revoke certificates immediately when a service is decommissioned and audit issuance?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Bloomberg","Lyft","MongoDB"]},{"id":"q-1405","question":"In a multi-DC Vault deployment used by a large enterprise, explain how to provision ephemeral SSH access to hosts using Vault's SSH secrets engine. Include how to establish a per-user role, certificate TTLs, revocation on logout or compromise, host-key rotation, and how to audit all SSH sessions end-to-end?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["LinkedIn","Uber"]},{"id":"q-1446","question":"You operate a multi-tenant app using Vault's Transit engine to encrypt per-tenant data with keys at transit/tenant-{tenant_id}. When retiring a tenant, you must rotate its key, re-encrypt historical data, and keep live traffic unaffected. Describe a concrete plan for key rotation, re-encryption strategy, audit logging, and rollback safeguards to preserve decryptability?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Snowflake","Tesla","Twitter"]},{"id":"q-1504","question":"Scenario: A project alpha needs read-only access to its KV v2 secrets at secret/data/projects/alpha and secret/metadata/projects/alpha. Define a policy 'alpha-read' granting read to those endpoints with renewable=false and a max TTL of 1h, and show the token issuance command to grant this policy. How would you verify expiry and revoke immediately if the project ends?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["NVIDIA","Snowflake"]},{"id":"q-1569","question":"Describe how you would configure Vault audit logging and per-tenant access controls for a SaaS that issues per-tenant Snowflake credentials via Vault's database secrets engine. Include audit setup, tenant-scoped policies, how to attach tenant metadata to credentials, and a test plan to verify instant revocation and tenant-specific logs?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Meta","NVIDIA","Snowflake"]},{"id":"q-1676","question":"You manage a SaaS that grants per-tenant SSH access to a fleet using Vault's SSH Secret Engine. Each tenant receives ephemeral SSH credentials valid for 24 hours, scoped to a per-tenant username and a dedicated user CA. Outline the operational steps to configure the engine, define a per-tenant role, enforce 24h TTLs, and ensure immediate revocation on de-provisioning, including how you would issue credentials and revoke them?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Airbnb","Anthropic","Two Sigma"]},{"id":"q-1699","question":"Design a practical key-rotation workflow using Vault Transit to rotate a dataset encryption key (DEK) for a Databricks workflow that writes Parquet files to S3, ensuring zero-downtime encryption for new data, safe rewrap of existing ciphertext, and immediate revocation if a cluster is decommissioned. Include exact Vault commands and rollout plan?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Databricks","Zoom"]},{"id":"q-1855","question":"You run a multi-tenant Databricks data platform using Vault's database secret engine to issue per-tenant Snowflake credentials with short TTLs. Design a rotation strategy that rotates credentials across 1000+ concurrent jobs with zero downtime, and enables immediate revocation when a tenant is disabled. Include Vault role config, rotation/renewal workflow, and how to propagate changes to active notebooks/clusters?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Databricks","Two Sigma"]},{"id":"q-1894","question":"You're operating a Slack/Anthropic-scale SaaS using Vault for **Kubernetes** workloads. Each namespace gets a dedicated Vault role via the Kubernetes auth method, issuing per-pod credentials with a TTL of 15 minutes. Describe how you would implement scalable rotation and safe renewal to avoid expired secrets, including: (a) agentless vs agent-based rotation strategies, (b) pre-rotation and lease renewal coordination across many pods, (c) handling long-running jobs that outlive their pods, and (d) auditing, revocation on namespace deletion, and potential upgrade paths. Include concrete Vault commands and config snippets?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Anthropic","Slack"]},{"id":"q-1896","question":"Design an end-to-end workflow using Vault's SSH secrets engine to issue ephemeral SSH credentials for a multi-tenant Linux fleet. Include per-tenant roles under ssh/roles/tenant-*, TTL 10m, a renewal approach that preserves live sessions, and an immediate revocation path on tenant disable. Include audit scopes and concrete Vault commands?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Bloomberg","LinkedIn","NVIDIA"]},{"id":"q-2007","question":"In a multi-tenant SaaS running on Kubernetes, each tenant's app pods authenticate to Vault via Kubernetes auth. Design an end-to-end plan to ensure per-tenant secret isolation, automatic rotation with no downtime, and immediate revocation when a tenant is disabled. Include: (1) Identity/Group mapping per tenant, (2) per-tenant DB role in the Vault DB secret engine with TTL, (3) rotation flow and how apps will access new creds without downtime, and (4) tenant disable revoke process and audit considerations. Provide sample policies and a rollout plan?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Oracle","Plaid","Square"]},{"id":"q-2023","question":"You’re deploying a small SaaS that requires per-service mTLS authentication. Using Vault's PKI engine, design an issuance flow to deliver short-lived client certificates (TTL 15m) to services, with immediate revocation on termination. Specify role config, issuance path, revocation process, and how you'd validate revocation in a running cluster?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Instacart","OpenAI","Snap"]},{"id":"q-2108","question":"Design a PCI-DSS compliant multi-tenant Vault deployment across three regions. Each tenant retrieves per-tenant DB credentials via Vault with strict isolation, rotation, and revocation. Provide an isolation strategy (namespaces vs path-based), a cross-region DR plan, a zero-downtime rotation workflow, and concrete Vault commands and policies for rollout and rollback?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Goldman Sachs","LinkedIn","Tesla"]},{"id":"q-2156","question":"You're running a multi-region Vault deployment for a SaaS with tenants isolated in namespaces. Plan a cross-region DR replication that preserves tenant isolation and minimizes downtime. Include: 1) namespace/policy design across regions, 2) replication topology and sync behavior, 3) tenant token/lease revocation paths during failover, 4) a failover testing plan with realistic success metrics?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Apple","Coinbase","Discord"]},{"id":"q-971","question":"In a PCI-compliant SaaS, each tenant uses Vault's database secret engine with a dedicated role to issue per-tenant PostgreSQL credentials. TTLs are short (1h). Describe how you would configure per-tenant roles, handle rotation without downtime, and ensure immediate revocation when a tenant is disabled?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Anthropic","Google","Stripe"]}]