[{"id":"q-1158","question":"How would you implement a health check mechanism for a load balancer that uses exponential backoff for failed servers, and how does this approach prevent cascading failures during partial outages?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["load-balancing","health-checks","exponential-backoff","fault-tolerance","cascading-failures"],"companies":[]},{"id":"q-2121","question":"How would you implement a load balancer that uses predictive scaling based on request patterns and historical data, and what machine learning techniques would you use to forecast traffic spikes?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["predictive-scaling","machine-learning","time-series-forecasting","load-balancing"],"companies":[]},{"id":"q-2617","question":"How would you implement a consistent hashing load balancer that minimizes server remapping when adding or removing nodes, and what data structures would you use to achieve O(log n) lookup time?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["consistent-hashing","load-balancing","distributed-systems","data-structures"],"companies":[]},{"id":"q-2620","question":"How would you implement a consistent hashing load balancer that minimizes data remapping when servers are added or removed, and what trade-offs exist between ring size and lookup performance?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["consistent-hashing","load-balancing","distributed-systems","hash-rings","scalability"],"companies":[]},{"id":"q-653","question":"How would you implement a consistent hashing load balancer that handles server additions and removals with minimal key remapping? What data structures would you use and how would you handle virtual nodes?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["consistent-hashing","load-balancing","distributed-systems","hash-ring","virtual-nodes"],"companies":[]},{"id":"q-659","question":"How would you implement a consistent hashing load balancer and what advantages does it provide over traditional hash-based load balancing when servers are added or removed?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["load-balancing","consistent-hashing","distributed-systems","scalability"],"companies":[]},{"id":"q-767","question":"Design a load balancer that implements adaptive load balancing using real-time server metrics. How would you collect and weight server performance data, and what algorithm would you use to dynamically adjust traffic distribution?","channel":"algorithms","subChannel":"algorithms","difficulty":"intermediate","tags":["adaptive-load-balancing","performance-monitoring","dynamic-weighting","real-time-metrics"],"companies":[]},{"id":"al-1","question":"When would you choose a Linked List over an Array and what are the key trade-offs for each data structure?","channel":"algorithms","subChannel":"data-structures","difficulty":"beginner","tags":["struct","comparison","basics"],"companies":["Adobe","Amazon","Apple","Google","Meta","Microsoft"]},{"id":"al-165","question":"Implement a Trie data structure for efficient prefix search with insert, search, and startsWith operations. What are its advantages over hash maps for autocomplete systems, and what are the trade-offs?","channel":"algorithms","subChannel":"data-structures","difficulty":"intermediate","tags":["struct","basics"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"q-187","question":"How would you implement a thread-safe LRU cache using a HashMap and DoublyLinkedList, considering eviction policy and O(1) operations?","channel":"algorithms","subChannel":"data-structures","difficulty":"intermediate","tags":["arrays","linkedlist","hashtable","heap"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Uber"]},{"id":"q-277","question":"How would you efficiently process a 50GB log file to extract the top 10 most frequent IP addresses from millions of entries while handling memory constraints and optimizing for performance?","channel":"algorithms","subChannel":"data-structures","difficulty":"advanced","tags":["find","xargs","cut","sort"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Snowflake"]},{"id":"q-377","question":"Implement a min-heap using an array that supports insert, extractMin, and peek operations in O(log n) time. Include time/space complexity analysis and edge cases?","channel":"algorithms","subChannel":"data-structures","difficulty":"beginner","tags":["arrays","linkedlist","hashtable","heap"],"companies":null},{"id":"q-407","question":"Given a stream of log events with timestamps, design an algorithm to find the top K most frequent error messages in the last N minutes using O(K) space, where each event contains timestamp, error type, and message?","channel":"algorithms","subChannel":"data-structures","difficulty":"intermediate","tags":["arrays","linkedlist","hashtable","heap"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-418","question":"Design a data structure that supports range sum queries and point updates on a dynamic array with O(log n) operations. How would you implement this using a segment tree, and what are the trade-offs compared to a Binary Indexed Tree?","channel":"algorithms","subChannel":"data-structures","difficulty":"advanced","tags":["bst","avl","trie","segment-tree"],"companies":null},{"id":"q-425","question":"Given an array of integers and a target sum, find two numbers that add up to the target. How would you implement this efficiently and what's the time complexity?","channel":"algorithms","subChannel":"data-structures","difficulty":"beginner","tags":["arrays","linkedlist","hashtable","heap"],"companies":["Adobe","Amazon","Two Sigma"]},{"id":"q-442","question":"Given a stream of user actions with timestamps, design a system to find the top K most frequent actions in the last N minutes using O(1) time per query?","channel":"algorithms","subChannel":"data-structures","difficulty":"intermediate","tags":["arrays","linkedlist","hashtable","heap"],"companies":["LinkedIn","OpenAI"]},{"id":"q-565","question":"Given a stream of timestamped events, find the maximum number of concurrent events at any time?","channel":"algorithms","subChannel":"data-structures","difficulty":"advanced","tags":["arrays","linkedlist","hashtable","heap"],"companies":["Salesforce","Slack","Square"]},{"id":"al-152","question":"You have a staircase with n steps. You can climb 1, 2, or 3 steps at a time. How many distinct ways can you reach the top? Implement a solution with O(n) time and O(1) space?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"intermediate","tags":["dp","optimization"],"companies":null},{"id":"al-166","question":"Given a string, find the minimum cost to transform it into a palindrome where insertions cost 2 and deletions cost 1. What is the optimal dynamic programming approach?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"intermediate","tags":["dp","optimization"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"al-167","question":"Given a target sum n, count the number of ways to reach it using dice rolls where each roll can be 1-6. Return the result modulo 10^9+7. Optimize for O(n) time and O(1) space?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"intermediate","tags":["dp","optimization"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"al-170","question":"Given an array of integers where each element represents the maximum number of steps you can jump forward from that position, find the minimum number of jumps required to reach the last index. If it's not possible to reach the end, return -1. How would you implement this efficiently?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"advanced","tags":["dp","optimization"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"al-3","question":"What is Dynamic Programming and how does it differ from plain recursion? When would you choose one over the other?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"advanced","tags":["dp","optimization","theory"],"companies":["Amazon","Google","Meta"]},{"id":"q-328","question":"Given a grid of size m x n where each cell contains a non-negative integer representing the cost to enter that cell, find the minimum cost path from the top-left corner (0,0) to the bottom-right corner (m-1,n-1) moving only right or down. Return both the minimum cost and the path itself?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"intermediate","tags":["dp","memoization","tabulation"],"companies":["Amazon","Apple","Google","Meta","Microsoft","NVIDIA"]},{"id":"q-440","question":"Given a string s and dictionary wordDict, return all possible sentences where s can be segmented into space-separated words from wordDict. Handle overlapping subproblems efficiently?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"advanced","tags":["dp","memoization","tabulation"],"companies":["Adobe","Google","NVIDIA"]},{"id":"q-540","question":"Given a string s and a dictionary wordDict, return all possible sentences formed by inserting spaces in s such that each word exists in wordDict. Use DP with memoization to avoid exponential recomputation?","channel":"algorithms","subChannel":"dynamic-programming","difficulty":"advanced","tags":["dp","memoization","tabulation"],"companies":["Goldman Sachs","LinkedIn","Slack"]},{"id":"q-214","question":"Given a directed weighted graph with up to 10^6 edges and frequent edge weight updates, design a data structure that supports dynamic shortest path queries with sub-millisecond response time?","channel":"algorithms","subChannel":"graphs","difficulty":"advanced","tags":["bfs","dfs","dijkstra","topological"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Uber"]},{"id":"q-286","question":"Explain the difference between BFS and DFS and when would you use each?","channel":"algorithms","subChannel":"graphs","difficulty":"intermediate","tags":["bfs","dfs","dijkstra","topological"],"companies":["Amazon","Google","Meta"]},{"id":"q-350","question":"Given a directed graph representing city intersections and one-way streets, implement a function to find if there's a valid route from point A to point B using BFS. Return the shortest path distance or -1 if no route exists?","channel":"algorithms","subChannel":"graphs","difficulty":"beginner","tags":["bfs","dfs","dijkstra","topological"],"companies":["Amazon","Apple","Cruise","Google","Meta","Microsoft","Netflix","Vercel"]},{"id":"q-394","question":"Given a directed acyclic graph representing task dependencies where each task takes 1 unit of time and you have unlimited workers, what is the minimum time to complete all tasks?","channel":"algorithms","subChannel":"graphs","difficulty":"advanced","tags":["bfs","dfs","dijkstra","topological"],"companies":null},{"id":"q-662","question":"Given a directed graph with N nodes and M weighted edges (positive weights) and a source s, describe an implementation to (1) identify nodes reachable from s via BFS, (2) compute shortest distances dist[] to all nodes with Dijkstra, and (3) count the number of distinct shortest s→v paths for every v (mod 1e9+7). How would you build a DAG of edges on shortest paths and perform a topological DP over dist-ordered nodes to obtain path counts?","channel":"algorithms","subChannel":"graphs","difficulty":"advanced","tags":["bfs","dfs","dijkstra","topological"],"companies":["Anthropic","NVIDIA","Robinhood"]},{"id":"q-596","question":"Explain the differences between round-robin, least connections, and IP hash load balancing algorithms. When would you choose each one?","channel":"algorithms","subChannel":"load-balancing-algorithms","difficulty":"intermediate","tags":["load-balancing","algorithms","networking","scalability","system-design"],"companies":["Google","Amazon","Meta","Netflix","Microsoft","Twitter","Uber","Airbnb"]},{"id":"q-627","question":"Explain the difference between round-robin and weighted round-robin load balancing algorithms. When would you choose one over the other?","channel":"algorithms","subChannel":"load-balancing-algorithms","difficulty":"intermediate","tags":["load-balancing","algorithms","distributed-systems","networking"],"companies":["Amazon","Google","Microsoft","Netflix","Facebook","Twitter"]},{"id":"al-163","question":"You have an array where each element appears twice except one element that appears once. Sort the array in O(n) time without using extra space for sorting. How would you approach this?","channel":"algorithms","subChannel":"sorting","difficulty":"intermediate","tags":["sort","complexity"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"al-2","question":"Compare QuickSort, MergeSort, and Timsort. When would you choose each algorithm and what are their key trade-offs in production systems?","channel":"algorithms","subChannel":"sorting","difficulty":"intermediate","tags":["sort","recursion","complexity"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-300","question":"Explain the difference between quicksort and mergesort, including their time and space complexities?","channel":"algorithms","subChannel":"sorting","difficulty":"beginner","tags":["quicksort","mergesort","complexity"],"companies":["Amazon","Google","Meta"]},{"id":"q-362","question":"Given an array of integers, implement quicksort with proper partitioning, explain its O(n log n) average vs O(n²) worst-case complexity, and compare with mergesort in terms of stability, space usage, and practical performance?","channel":"algorithms","subChannel":"sorting","difficulty":"beginner","tags":["quicksort","mergesort","complexity"],"companies":null},{"id":"q-433","question":"Implement quicksort and explain when you'd choose it over mergesort. What's the worst-case scenario and how do you avoid it?","channel":"algorithms","subChannel":"sorting","difficulty":"beginner","tags":["quicksort","mergesort","complexity"],"companies":["Hashicorp","Oracle","Snowflake"]},{"id":"q-167","question":"Write a function to find the maximum depth of a binary tree using both recursive DFS and iterative BFS approaches. Discuss time/space complexity and handle edge cases?","channel":"algorithms","subChannel":"trees","difficulty":"beginner","tags":["tree","binary"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-314","question":"Given a binary search tree with n nodes, find the kth smallest element where 1 ≤ k ≤ n. Discuss both recursive and iterative approaches with their time and space complexities?","channel":"algorithms","subChannel":"trees","difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"companies":null},{"id":"q-340","question":"Given a BST that may have duplicate values, implement a function to find the kth smallest element considering duplicates. What's the time complexity and how would you handle edge cases?","channel":"algorithms","subChannel":"trees","difficulty":"intermediate","tags":["bst","avl","trie","segment-tree"],"companies":["Hrt","New Relic","Sap"]},{"id":"q-451","question":"Given a BST, write a function to find the kth smallest element using O(h) space and O(n) time, where h is height and n is nodes?","channel":"algorithms","subChannel":"trees","difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"companies":["Anthropic","Cloudflare"]},{"id":"q-660","question":"You’re building a chat app that autocompletes words as users type. Implement a Trie with insert(word), search(word), and startsWith(prefix). Provide a compact JavaScript class with these methods, assuming lowercase a-z. After inserting 'apple','app','application', does search('app') return true and does startsWith('appl') return true?","channel":"algorithms","subChannel":"trees","difficulty":"beginner","tags":["bst","avl","trie","segment-tree"],"companies":["Cloudflare","IBM","Slack"]},{"id":"q-258","question":"How would you design a reactive Android ViewModel using StateFlow with sealed classes to handle network API responses, ensuring proper error handling and loading states?","channel":"android","subChannel":"architecture","difficulty":"intermediate","tags":["coroutines","flow","sealed-classes"],"companies":["Airbnb","Google","Meta","Microsoft","Uber"]},{"id":"q-1022","question":"You're building an Android field-inspection app where users collect forms and photos offline. When connectivity returns, design a robust sync engine that uploads only new or updated items, resolves conflicts via last_modified, and handles intermittent networks. Use WorkManager with network constraints and a foreground service for long syncs; include backoff and tests?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Airbnb","Goldman Sachs","Scale Ai"]},{"id":"q-1127","question":"Design a beginner Android feature: a simple offline notes app. Notes stored in Room with fields id, text, tag, last_modified. Provide a tag-based search, and a background export to cloud via WorkManager that runs only on WiFi and while charging. Ensure deduplication by last_modified, handle restarts, and outline a minimal test plan?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Anthropic","Robinhood"]},{"id":"q-1143","question":"You’re building a chat-like Android app. Messages are stored in Room with fields: id (UUID), text (String), timestamp (Long), status (pending, sending, sent, failed). When sending, insert a pending message and enqueue a WorkManager task to upload unsent messages when online, using exponential backoff. On success, save serverId and set status to sent; on failure, keep failed with a retriable option. Outline the data flow, DAO/Worker skeleton, and offline→online test plan?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Snowflake","Twitter"]},{"id":"q-1267","question":"You're building an Android offline-first app that records field observations (id, timestamp, value) in Room. Changes are queued when offline and synced to a REST backend when online. Implement a robust sync with versioning, conflict resolution (server-wins, client-wins, or merge), and tombstones. Describe data flow, DAO/Repository, a WorkManager worker, and a testing strategy for edge cases like concurrent edits and delayed pushes?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Cloudflare","Hugging Face","Tesla"]},{"id":"q-1388","question":"Implement a field survey photo capture in Android: tap Capture to take a photo, request CAMERA permission via the Activity Result API, use TakePicturePreview to obtain a Bitmap, save it to internal storage with a timestamped filename, and display a preview in an ImageView. Ensure rotation safety and a graceful denial/retry flow. What code would you write?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Adobe","Bloomberg","Google"]},{"id":"q-1715","question":"You're building an Android module to support background, continuous BLE logging from an OBD-II dongle for a fleet-tracking app. Requirements: maintain a stable BLE connection with automatic reconnect and backoff; buffer data locally in Room during disconnect; periodically upload batched logs via WorkManager with exponential backoff; use a ForegroundService while in the background; respect Doze/App Standby; outline data flow, architecture, and a minimal skeleton of the BLE manager, DAO, and Worker; include a test plan for connectivity changes and battery constraints?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Databricks","Lyft"]},{"id":"q-1880","question":"You’re building an Android app that streams live telemetry video from a drone's camera over an unstable network. Using CameraX, encoding with H.264, and a hybrid UDP/TCP transport, design a solution that includes buffering and backpressure, a foreground service with Doze-friendly behavior, adaptive bitrate, and robust error handling. Provide data flow, key classes (StreamingManager, PacketSender), and a skeleton implementation plus an end-to-end test plan for packet loss and latency?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Cloudflare","Oracle"]},{"id":"q-2076","question":"You're building a beginner Android app that logs field visits. The main screen shows a list of visits (id, siteName, timestamp). Implement adding a visit via a dialog, persist with Room, and a DataStore-backed sort toggle (timestamp ascending/descending) that preserves the choice across rotations. Describe data flow, DAO/Repository, ViewModel, and a minimal UI test plan?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Amazon","Salesforce","Uber"]},{"id":"q-2143","question":"You're building a beginner Android checklist app: store items in Room (id, title, completed, dueDate), display them in a RecyclerView, and support add/edit/delete. Implement a one-time reminder using WorkManager that triggers a Notification at dueDate, with a backoff if scheduling fails or the device is asleep. Describe data flow (DAO/Repository), Worker implementation, and a test plan for offline item creation and overdue reminders?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Apple","Meta"]},{"id":"q-2195","question":"You're building an Android field tool app used by technicians to annotate images and run on-device ML for scene classification with offline-first data collection. The app must train a lightweight model locally from user-labeled samples, support incremental model updates delivered from the backend, and handle intermittent connectivity. Design end-to-end architecture and data flow using Room, DataStore, WorkManager, and TensorFlow Lite. Address offline training, model versioning, delta updates, rollback, and testing under Doze and flaky networks?","channel":"android","subChannel":"general","difficulty":"advanced","tags":["android"],"companies":["Hugging Face","Lyft","Oracle"]},{"id":"q-2216","question":"You're building an Android offline-first note app that provides fast full-text search across local notes and syncs with a central server. Implement using Room + FTS for search on title/content, a tombstone strategy, and a versioned sync via WorkManager with exponential backoff. Describe the data model (Note: id, title, content, updatedAt, version, tombstone), DAO/FTS setup, sync flow, and a test plan for offline edits, conflicts, and index consistency?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Airbnb","Hashicorp","MongoDB"]},{"id":"q-2447","question":"Design an Android enterprise app feature delivery strategy using dynamic feature modules and server-driven flags. Describe how you would (1) ship features on demand with Play Feature Delivery, (2) implement server-driven flags with offline caching, versioning, and safe rollbacks, (3) support canary and percentage rollout per device, (4) ensure startup safety and crash resilience, and (5) validate with test plans and telemetry?","channel":"android","subChannel":"general","difficulty":"advanced","tags":["android"],"companies":["Hashicorp","Microsoft"]},{"id":"q-2547","question":"Design an Android module that performs on-device NLP inference using a Hugging Face model for chat intent detection. Build a ModelManager with components: ModelRepo (versioned storage), InferenceEngine (off-UI thread), MemoryMonitor, and Updater (WorkManager with backoff). It should load from internal storage, download updates when online, switch between a large and compact model based on memory, support cancellation with timeouts, and rollback on failure. Describe architecture, data flow, and a concrete test plan for offline updates, memory pressure, and model-load failure?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Hashicorp","Hugging Face"]},{"id":"q-2587","question":"You're building a beginner Android feature that records in-app interactions as Event records (id, screen, action, ts) in Room. When online, batch-upload unsynced events to a REST API via WorkManager with a network constraint and exponential backoff. Show your data model, DAO/repository, how you mark events synced, batch sizing, and a basic test plan for offline queueing and retry behavior?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["IBM","Instacart","Tesla"]},{"id":"q-2664","question":"You're building an Android module for real-time, offline-first collaborative document editing. Each document uses a CRDT to merge concurrent edits locally and remotely. The app syncs with a central server over WebSocket with a REST fallback; local changes are persisted in Room; security via per-document encryption keys. Describe architecture, data model, CRDT design, sync protocol, and testing plan?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["IBM","Robinhood"]},{"id":"q-2715","question":"Design an Android chat module that delivers end-to-end encrypted real-time messaging for a multi-device collaboration app. Implement a Signal-like ratchet, store device keys in Android Keystore, perform nightly ephemeral key rotation, and queue outgoing messages in Room for offline delivery. Describe architecture, data flow, storage schema, and a test plan for offline queuing, key rotation, and out-of-order delivery under churn?","channel":"android","subChannel":"general","difficulty":"advanced","tags":["android"],"companies":["Hashicorp","MongoDB","Zoom"]},{"id":"q-452","question":"How would you implement a RecyclerView with multiple view types while maintaining smooth scrolling performance on large datasets?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Adobe","Citadel","Google"]},{"id":"q-482","question":"How would you handle Activity lifecycle when screen rotates and you need to preserve user input data?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Amazon","Google","Oracle"]},{"id":"q-512","question":"How would you implement a simple RecyclerView in Android to display a list of user profiles with name and email?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["MongoDB","Tesla","Two Sigma"]},{"id":"q-541","question":"How would you implement a RecyclerView with ViewHolder pattern to display a list of user profiles efficiently?","channel":"android","subChannel":"general","difficulty":"beginner","tags":["android"],"companies":["Databricks","Goldman Sachs","Tesla"]},{"id":"q-976","question":"You are building an Android app that tracks a delivery ride; location updates every 5 seconds; battery life; Doze; Provide plan using FusedLocationProvider, ForegroundService, and WorkManager; include backoff; testing; intermittent connectivity?","channel":"android","subChannel":"general","difficulty":"intermediate","tags":["android"],"companies":["Goldman Sachs","Oracle","Uber"]},{"id":"q-205","question":"How would you implement Compose Navigation with nested graphs, shared ViewModels, configuration change handling, and deep linking in a production Android app?","channel":"android","subChannel":"jetpack-compose","difficulty":"intermediate","tags":["composables","state","navigation"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-182","question":"What is the first lifecycle method called when an Android Activity is created, and what critical initialization tasks must be performed within it?","channel":"android","subChannel":"lifecycle","difficulty":"beginner","tags":["lifecycle","components"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-236","question":"How would you implement a comprehensive contract testing strategy using MSW (Mock Service Worker) with OpenAPI to ensure frontend API mocks stay synchronized with backend specifications, including CI/CD integration and drift detection?","channel":"api-testing","subChannel":"contract-testing","difficulty":"intermediate","tags":["wiremock","mockserver","msw"],"companies":["Amazon","Microsoft","Netflix","Salesforce","Square","Stripe"]},{"id":"q-1030","question":"Design a test strategy for an API gateway that enforces per-tenant sliding-window rate limits with dynamic quotas updated via admin API. Outline how you'd simulate high-concurrency traffic, verify quota propagation across nodes, validate headers and 429 responses, and test failure modes (Redis outage or misconfig). Include concrete test cases and tooling suggestions?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Citadel","Google","Meta"]},{"id":"q-1050","question":"Design an automated test plan for a REST + streaming API: /inventory/{sku}/status returns current stock via a streaming endpoint /inventory/stream (Server-Sent Events). The plan should cover stream resilience, event deduplication, per-warehouse aggregation under bursts, and failure modes when downstream storage becomes partially unavailable. Provide concrete test cases, tooling suggestions, and expected outcomes, with emphasis on realism for high-scale retail backends?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Amazon","Apple","DoorDash"]},{"id":"q-1061","question":"Design a practical test plan for a GraphQL API that aggregates data from products, pricing, and reviews. Include how you validate query depth limits, detect N+1 issues, test caching and cache invalidation under high concurrency, and ensure partial responses when some downstream services fail. Provide concrete test cases and tooling?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Instacart","NVIDIA","PayPal"]},{"id":"q-1166","question":"You manage a public REST API with versioning: /v1/... and /v2/... Design a practical, automated test plan to validate backward compatibility as v2 introduces a new field (tags) and changes a field type (price from int to decimal). Include concrete test cases, OpenAPI contract checks, cross-version schema validation, and how to verify deprecation behavior via a warnings header and 429s for old clients. Outline tooling and steps?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["Google","Lyft","NVIDIA"]},{"id":"q-1251","question":"You're testing an inventory REST API for a global retail platform. `/inventory/{sku}` returns current stock and scheduled restock ETA from a separate availability service that uses eventual consistency and a message bus (Kafka). Design a practical test plan to verify consistency windows, eventual accuracy under bursts, and cross-region cache coherency, including late-arriving events, out-of-order messages, and failure modes of Kafka and Redis caching. Include concrete test cases, tooling, and expected outcomes?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Discord","Square"]},{"id":"q-1284","question":"You maintain a simple REST API with POST /checkout that creates a payment intent for a cart. As a beginner, outline an end-to-end test plan to verify input validation, idempotent retries, and error handling under transient failures. Include concrete test cases, tooling suggestions, and expected responses?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["DoorDash","MongoDB"]},{"id":"q-1720","question":"Design a test strategy for a multi-tenant GraphQL API exposing persisted queries. Include N+1 detection, depth limits, per-tenant access controls, cross-tenant isolation, mutation cache invalidation, and canary schema rollout?","channel":"api-testing","subChannel":"general","difficulty":"advanced","tags":["api-testing"],"companies":["Google","Hashicorp","Microsoft"]},{"id":"q-1744","question":"Design a test for a bulk user import API: POST /imports/users accepts up to 100k records, uses an idempotency-key, and publishes per-record results to a downstream analytics queue. Specify test data, idempotency scenarios, partial downstream failures, retry/backoff, and observability. Include concrete cases, tooling, and expected outcomes?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Airbnb","Hashicorp","Meta"]},{"id":"q-1957","question":"Design a concrete, end-to-end automated test plan for an asynchronous data-processing API: POST /data/process returns 202 with an operation-id; messages flow via Kafka to downstream workers; results exposed at GET /data/process/{operationId}/result. Include idempotency tests (Idempotency-Key), eventual consistency with versioning, and fault-injection scenarios (Kafka outage, consumer restart, DB outage) with concrete test cases and tooling recommendations?","channel":"api-testing","subChannel":"general","difficulty":"advanced","tags":["api-testing"],"companies":["Google","Plaid","Two Sigma"]},{"id":"q-2017","question":"Design a beginner-friendly test plan for an API endpoint GET /products/{id} that uses a real-time feature flag to toggle response fields. Validate that additional fields (e.g., discount, promotionTag) appear when the flag is ON and disappear when OFF; cover admin-API downtime fallback; and test mid-request toggles with concurrent requests to ensure each response matches its observed flag. Include tooling suggestions?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["Adobe","Meta","Plaid"]},{"id":"q-2187","question":"You add a new endpoint POST /imports/candidates to ingest candidate records from a CSV file for a recruitment platform. The endpoint returns 202 with a jobId and status 'accepted'; processing is asynchronous via a worker queue. Design a beginner-friendly test plan covering input validation (file type/size), idempotency with Idempotency-Key, correct 202 response and jobId, status polling with GET /imports/{jobId}, and failure modes (malformed CSV, partial failure, queue downtime). Include concrete test cases and tooling?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["Amazon","LinkedIn"]},{"id":"q-2275","question":"Design an advanced API testing plan for a high-scale analytics API with asynchronous batch report generation. The endpoint GET /tenant/{tenantId}/reports/{reportId} returns a report produced by a background worker that reads from a write-ahead log and stores results in a read-optimized store. Outline concrete tests for data correctness across tenants and roles, eventual consistency across regions, idempotent retries, failure modes (worker crash, MQ outage), and performance under burst traffic. Include tooling and observability requirements?","channel":"api-testing","subChannel":"general","difficulty":"advanced","tags":["api-testing"],"companies":["Coinbase","Discord","IBM"]},{"id":"q-2343","question":"Design a beginner-friendly test plan for POST /webhook that validates an HMAC-SHA256 signature in X-Signature over the JSON payload with a shared secret. Include: happy-path with valid signature; missing/invalid header; invalid JSON; replay protection via timestamp/nonce; and basic rate-limiting checks. Recommend tooling and expected responses?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["OpenAI","Oracle","Uber"]},{"id":"q-2369","question":"Design an automated test plan for a webhook listener at /webhooks/ci ingesting push events from a CI service. Each payload is signed with HMAC-SHA256 and delivered at-least-once to a Redis-backed event store. Outline concrete test cases for signature validation, replay/duplicate handling, event ordering or out-of-order resilience, retries/backoff, and failure modes (secret rotation, Redis outage). Include tooling suggestions and expected outcomes?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["MongoDB","Slack"]},{"id":"q-2585","question":"Design a practical test plan for a WebSocket-based real-time update API at /ws/updates, where clients authenticate with JWT and receive per-tenant event streams. Include test cases for authentication failure, tenant isolation, message ordering and deduplication across reconnects, backpressure, and failure modes (broker outage, client disconnects). Recommend tooling and expected outcomes?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Cloudflare","Hashicorp","Uber"]},{"id":"q-2623","question":"Design a practical API-test plan for a multi-tenant image-processing service. The REST endpoint POST /v1/process/image returns a signed URL to the processed result. Create concrete test cases for: idempotent retries with an Idempotency-Key; streaming upload of large payloads; per-tenant routing and SLA verification across backends; failure modes (storage outage, worker pool exhaustion, network partition) with graceful fallbacks; and security checks for tenant scoping and signed URL expiry. Include tooling and metrics?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Anthropic","Discord","Hugging Face"]},{"id":"q-453","question":"You're testing a REST API that returns paginated results. The endpoint has a rate limit of 100 requests per minute and sometimes returns 500 errors under load. How would you design a comprehensive test strategy?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Airbnb","Google","Lyft"]},{"id":"q-483","question":"You're testing a REST API that returns paginated results. How would you design a comprehensive test strategy to verify pagination works correctly across different page sizes and edge cases?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Discord","Netflix","Salesforce"]},{"id":"q-513","question":"How would you test a REST API endpoint that returns user data, including both success and error scenarios?","channel":"api-testing","subChannel":"general","difficulty":"beginner","tags":["api-testing"],"companies":["Bloomberg","OpenAI"]},{"id":"q-542","question":"You're testing a payment API that processes transactions. How would you design test cases to verify idempotency, and what specific HTTP status codes would you expect for duplicate requests?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Cloudflare","NVIDIA","Stripe"]},{"id":"q-566","question":"How would you design a comprehensive API testing strategy for a machine learning model deployment pipeline that handles real-time inference requests?","channel":"api-testing","subChannel":"general","difficulty":"advanced","tags":["api-testing"],"companies":["Goldman Sachs","Hugging Face","Snap"]},{"id":"q-909","question":"Design a practical test plan for an asynchronous data ingestion API: POST / ing est accepts CSV payload and returns 202 with a job_id. It enqueues to a queue, then a worker writes to storage and updates a /status/{job_id} endpoint. Some tenants require PII redaction controlled by a tenant flag; a 'force' param bypasses CSV schema validation. Outline concrete test cases to verify correctness, privacy, idempotency, race conditions, and failure modes when queue or storage fail. Include sample CSV payloads and expected outcomes?","channel":"api-testing","subChannel":"general","difficulty":"intermediate","tags":["api-testing"],"companies":["Robinhood","Snowflake","Two Sigma"]},{"id":"q-209","question":"How would you design a REST API testing framework that handles rate limiting, circuit breaking, and distributed tracing for microservices with 10,000+ concurrent requests?","channel":"api-testing","subChannel":"rest-testing","difficulty":"advanced","tags":["postman","rest-assured","supertest"],"companies":["Amazon","Goldman Sachs","Microsoft","Netflix","Stripe"]},{"id":"gh-12","question":"What are the three main service models of cloud computing and how do they differ?","channel":"aws","subChannel":"compute","difficulty":"beginner","tags":["cloud","aws","azure","gcp"],"companies":["Amazon","Google","Meta"]},{"id":"gh-13","question":"What is AWS (Amazon Web Services)?","channel":"aws","subChannel":"compute","difficulty":"beginner","tags":["cloud","aws","azure","gcp"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Netflix"]},{"id":"gh-15","question":"Compare AWS IaaS, PaaS, and SaaS service models with specific examples and use cases?","channel":"aws","subChannel":"compute","difficulty":"intermediate","tags":["cloud","aws","azure","gcp"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-34","question":"How would you design an Auto Scaling configuration for a high-traffic e-commerce application that handles 10,000 RPS with 99.99% availability, including scaling policies, health checks, and cost optimization?","channel":"aws","subChannel":"compute","difficulty":"advanced","tags":["scale","ha"],"companies":null},{"id":"gh-57","question":"What is Cloud Cost Optimization and what are the key strategies to reduce cloud spending in production environments?","channel":"aws","subChannel":"compute","difficulty":"beginner","tags":["finops","cost"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-58","question":"What are AWS Reserved Instances and how do they compare to On-Demand pricing?","channel":"aws","subChannel":"compute","difficulty":"intermediate","tags":["finops","cost"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Uber"]},{"id":"gh-83","question":"How do you evaluate cloud services for business needs using TCO analysis, SLA metrics, and migration strategies?","channel":"aws","subChannel":"compute","difficulty":"advanced","tags":["migration","cloud"],"companies":["Amazon","Google","IBM","Microsoft","Oracle","Salesforce"]},{"id":"gh-85","question":"How do cloud migration tools automate application and data transfer between on-premise and cloud environments, and what are the key technical challenges in ensuring data consistency and minimal downtime?","channel":"aws","subChannel":"compute","difficulty":"intermediate","tags":["migration","cloud"],"companies":["Amazon","Citadel","Goldman Sachs","Google","Microsoft"]},{"id":"gh-87","question":"How would you implement a multi-cloud cost allocation system using tagging strategies and automation APIs?","channel":"aws","subChannel":"compute","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Stripe","Uber"]},{"id":"q-174","question":"You have an EC2 instance that suddenly becomes unresponsive. What step-by-step troubleshooting methodology would you follow, which specific AWS tools and commands would you use at each stage, and how would you handle different instance states and recovery scenarios?","channel":"aws","subChannel":"compute","difficulty":"intermediate","tags":["ec2","compute"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-321","question":"You have a containerized web application that needs to handle variable traffic loads. When would you choose ECS Fargate over EKS and what are the key trade-offs?","channel":"aws","subChannel":"compute","difficulty":"beginner","tags":["ec2","ecs","eks","fargate"],"companies":["Cloudflare","Figma","MongoDB"]},{"id":"q-216","question":"How would you design an eventual consistency strategy for a multi-region DynamoDB application using Global Tables to handle write conflicts, ensure data convergence, and minimize latency?","channel":"aws","subChannel":"database","difficulty":"intermediate","tags":["mongodb","dynamodb","cassandra","redis"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-357","question":"You're designing a security monitoring system that needs to store 10M+ events per day with millisecond read latency. How would you choose between DynamoDB, Aurora, and ElastiCache, and what's your data partitioning strategy?","channel":"aws","subChannel":"database","difficulty":"intermediate","tags":["rds","aurora","dynamodb","elasticache"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix","Palo Alto Networks"]},{"id":"q-401","question":"You're designing a real-time analytics dashboard for Scale AI that needs to handle 10,000 events/second. Your team is debating between using DynamoDB with DAX vs. Aurora with ElastiCache. What are the key trade-offs you'd consider, and which would you choose for this use case?","channel":"aws","subChannel":"database","difficulty":"intermediate","tags":["rds","aurora","dynamodb","elasticache"],"companies":["Cohere","Oscar Health","Scale Ai"]},{"id":"q-413","question":"You're designing a real-time analytics dashboard for an IoT application that receives 10,000 events per second. The dashboard needs to show current metrics and historical trends. How would you design the database architecture using AWS services, and what caching strategy would you implement?","channel":"aws","subChannel":"database","difficulty":"intermediate","tags":["rds","aurora","dynamodb","elasticache"],"companies":["Amazon","Apple","Databricks","Google","Micron","Microsoft","Netflix","Snowflake"]},{"id":"q-1256","question":"You're operating a global real-time analytics pipeline on AWS: data streams from mobile apps ingest via Kinesis Data Streams, processed by Lambda, stored in DynamoDB and S3 Parquet. A new release causes timeouts and duplicate processing under peak load. Propose a concrete plan to fix cold starts and throttling, ensure exactly-once semantics, and safely deploy with minimal data loss. Include services, config values, and rollout steps?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Amazon","Robinhood","Two Sigma"]},{"id":"q-2571","question":"How would you implement a cross-region, multi-account data ingestion pipeline for real-time analytics on AWS, ensuring tenant isolation, least-privilege IAM roles, cross-account access, and automatic CMK rotation, using Kinesis Streams, S3, Lake Formation, and Glue?","channel":"aws","subChannel":"general","difficulty":"intermediate","tags":["aws"],"companies":["Adobe","Google","Meta"]},{"id":"q-2694","question":"In a real-time recommendation platform on AWS, eu-west-1 outage impacts feature storage and SageMaker endpoints. Propose a tested disaster-recovery plan that preserves data availability and latency: include cross-region S3 replication, multi-region SageMaker endpoints with traffic routing, a cross-account feature store, and an automated failover workflow using Step Functions. Outline validation steps for RTO and RPO?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Hugging Face","Snap"]},{"id":"q-454","question":"You need to host a static website with high availability and low latency globally. How would you configure AWS S3 and CloudFront to achieve this?","channel":"aws","subChannel":"general","difficulty":"beginner","tags":["aws"],"companies":["Cloudflare","Google","Netflix"]},{"id":"q-484","question":"You're designing a real-time ML inference pipeline on AWS that must process 10,000 requests/second with sub-100ms latency. How would you architect this using serverless components, and what trade-offs would you consider?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Databricks","Hugging Face","Snowflake"]},{"id":"q-514","question":"You're building a serverless application that needs to process user uploads. How would you design an architecture using S3, Lambda, and API Gateway to handle file uploads securely and efficiently?","channel":"aws","subChannel":"general","difficulty":"beginner","tags":["aws"],"companies":["OpenAI","Stripe"]},{"id":"q-543","question":"You're deploying a microservices application on AWS ECS. One service is experiencing intermittent 503 errors during peak traffic. How would you diagnose and resolve this issue?","channel":"aws","subChannel":"general","difficulty":"intermediate","tags":["aws"],"companies":["Apple","Bloomberg","Meta"]},{"id":"q-567","question":"How would you design a multi-region serverless architecture for a real-time chat application using AWS services, ensuring low latency and high availability?","channel":"aws","subChannel":"general","difficulty":"advanced","tags":["aws"],"companies":["Slack","Tesla"]},{"id":"q-220","question":"How would you design a multi-AZ VPC architecture with Route53 latency-based routing to CloudFront, ALB, and private EC2 instances while ensuring failover within 30 seconds?","channel":"aws","subChannel":"networking","difficulty":"intermediate","tags":["vpc","route53","cloudfront","alb"],"companies":["Amazon","Databricks","Goldman Sachs","Microsoft","Netflix"]},{"id":"q-384","question":"You're designing a multi-region SaaS application with users in North America and Europe. How would you configure Route53, CloudFront, ALB, and VPC to ensure low latency and high availability? What are the key trade-offs?","channel":"aws","subChannel":"networking","difficulty":"intermediate","tags":["vpc","route53","cloudfront","alb"],"companies":["Airtable","Cisco","Epic Games"]},{"id":"gh-66","question":"How does serverless computing abstract infrastructure management and what are its key execution characteristics?","channel":"aws","subChannel":"serverless","difficulty":"beginner","tags":["serverless","lambda"],"companies":["Airbnb","Amazon","Google","Microsoft","Uber"]},{"id":"q-246","question":"How would you design a serverless order processing workflow using AWS Step Functions with Lambda functions, implementing specific retry patterns, error handling, and state management?","channel":"aws","subChannel":"serverless","difficulty":"intermediate","tags":["lambda","api-gateway","step-functions"],"companies":["Amazon","Google","Microsoft","Netflix","Salesforce","Stripe"]},{"id":"q-292","question":"How would you design a data lifecycle strategy for a media company storing petabytes of video content requiring immediate access, archiving, and cost optimization across AWS storage services?","channel":"aws","subChannel":"storage","difficulty":"advanced","tags":["s3","ebs","efs","glacier"],"companies":["Meta","Netflix","Youtube"]},{"id":"q-307","question":"What are the key differences between S3, EBS, and EFS in terms of performance, scalability, and use cases?","channel":"aws","subChannel":"storage","difficulty":"intermediate","tags":["s3","ebs","efs","glacier"],"companies":["Amazon","Google","Meta"]},{"id":"q-370","question":"You're designing a file storage system for Canva's design assets. Users upload large PSD files (up to 10GB) that need versioning and quick access. How would you architect this using AWS storage services, considering cost, performance, and durability?","channel":"aws","subChannel":"storage","difficulty":"intermediate","tags":["s3","ebs","efs","glacier"],"companies":["Affirm","Booking.com","Canva"]},{"id":"q-1001","question":"You're building a beginner-friendly AWS-only pipeline to ingest customer chat transcripts (text files up to 50 KB) uploaded to S3. Design how to automatically redact PII using Amazon Comprehend PII detection, store the redacted transcript back to S3, and index metadata in DynamoDB. Include data flow, IAM permissions, error handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Snowflake","Zoom"]},{"id":"q-1058","question":"Design a cross region, multi account AI inference platform for real time pricing and risk scoring in a fintech setting. Ingest streaming data, enforce per tenant data residency, and meet sub 100 ms latency. Describe data flow, services, IAM boundaries, model registry, feature store, drift monitoring, error handling, and cost controls?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Coinbase","NVIDIA","Stripe"]},{"id":"q-1259","question":"Design an end-to-end AWS-native real-time fraud detection pipeline for a global e-commerce platform. Ingest event streams (Kinesis Data Streams), redact PII, create real-time features stored in SageMaker Feature Store (online) and offline store, with governance, lineage, access control, and cost constraints. Include data flow, IAM, retry logic, backpressure, testing, and incident response?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Airbnb","Cloudflare","Salesforce"]},{"id":"q-1292","question":"Design a real-time fraud-detection pipeline on AWS for a FinTech use-case. Ingest streaming transactions via Kinesis Data Streams, preprocess with Lambda, and invoke a SageMaker endpoint for real-time scores. Persist results to DynamoDB with audit logs in S3. Address latency (<200 ms), data privacy (KMS, VPC endpoints), IAM, drift monitoring, error handling, and cost control. Provide concrete components and trade-offs?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Citadel","Coinbase"]},{"id":"q-1313","question":"You're designing a beginner-friendly AWS-only pipeline for user-submitted PDFs (max 5 MB) uploaded to S3. Build an end-to-end workflow that uses Textract to extract text, runs a lightweight topic model via Comprehend or a tiny SageMaker endpoint to derive a topic label, stores a compact summary and a searchable index in DynamoDB, and exposes a read path via API Gateway + Lambda. Include data flow, IAM roles, error handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Google","Snap"]},{"id":"q-1366","question":"You're operating a real-time text classifier API on a SageMaker hosting endpoint behind API Gateway. You need a canary deployment strategy using CodeDeploy for SageMaker endpoints, with 2% of traffic to the new version, ramping to 50% over 2 hours, then full if no regressions. Describe data flow, required services, IAM, drift detection via Model Monitor, and rollback/failover logic, plus privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["NVIDIA","Netflix","Salesforce"]},{"id":"q-1437","question":"You're building a beginner-friendly, AWS‑only pipeline to process user-submitted emails stored as JSON in S3 (max 20 KB per file). Design an end-to-end workflow that detects language, translates to English, analyzes sentiment, and stores results in DynamoDB, with a simple read path via API Gateway + Lambda. Include data flow, services, error handling, IAM roles, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["LinkedIn","Robinhood","Tesla"]},{"id":"q-1460","question":"Design an advanced, low-latency fraud-detection pipeline for real-time payments in AWS. Ingest events via Kinesis Data Streams, derive features into SageMaker Feature Store, train/deploy with SageMaker Pipelines, score via a real-time SageMaker Endpoint called from Lambda, and log audits in CloudTrail. Enforce KMS encryption, data minimization/retention policies, IAM least privilege, and a rollback/failover plan. Include data flow, IAM, privacy, and failure handling?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Bloomberg","PayPal"]},{"id":"q-1482","question":"Design a production-ready, AWS-native content moderation pipeline for a multi-tenant SaaS platform handling images and captions uploaded to S3. Must route per-tenant policies to a custom SageMaker multi-model endpoint, apply per-tenant threshold overrides, enforce data isolation, log audit trails, and keep costs predictable with auto-scaling and caching. Describe data flow, IAM, encryption, error handling, and privacy implications?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Adobe","Cloudflare","MongoDB"]},{"id":"q-1552","question":"You're deploying a privacy-preserving AI rating model for financial support tickets on AWS. Incoming tickets arrive as JSON (up to 10 KB) in S3; design an end-to-end pipeline that uses SageMaker for inference, writes per-ticket audit logs to DynamoDB, encrypts data at rest with SSE-KMS, and supports reliable, rate-limited processing with error handling and drift monitoring. How would you implement this?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["PayPal","Scale Ai"]},{"id":"q-1583","question":"You're running a real-time fraud-detection model for payments. Data arrives in a Kinesis stream; design an end-to-end AWS-native pipeline that performs per-record inference, monitors drift and bias with SageMaker Clarify/Model Monitor, archives data in S3 with metadata, and auto-triggers a rollback to a previous model version via Step Functions if drift thresholds are exceeded. Include data flow, IAM, encryption, backpressure handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Amazon","Anthropic","Snowflake"]},{"id":"q-1627","question":"You're building an AWS-native ML platform that serves a global analytics product with data residing in Snowflake and Databricks across regions. Design a pipeline that ingests from both sources, uses SageMaker Feature Store, trains/registers with a human approval, serves real-time inference, and uses HashiCorp Vault for secrets. Include data lineage, IAM, encryption, drift monitoring, rollback policy, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Databricks","Hashicorp","Snowflake"]},{"id":"q-1750","question":"You're building a beginner-friendly AWS-only pipeline for multilingual customer recordings. Audio files (up to 90 seconds) are uploaded to S3. Design an event-driven flow that uses Amazon Transcribe to produce a transcript in the source language, Amazon Translate to produce a Spanish version, stores both transcripts in S3, and writes a compact index in DynamoDB with fields like customerId, fileKey, sourceLang, translatedKey. Include data flow, IAM, error handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Anthropic","PayPal"]},{"id":"q-1789","question":"Design an AWS-native, real-time fraud-detection pipeline: ingest streaming transactions from Kinesis Data Streams, score risk with a SageMaker Inference endpoint (Transformer-based), store scores in DynamoDB, and trigger a Step Functions workflow for high-risk events. Include data flow, IAM, privacy controls, drift monitoring, and cost governance?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["DoorDash","Goldman Sachs"]},{"id":"q-1800","question":"You're building a compliant multi-tenant, multilingual content moderation service on AWS. Users upload video to S3; extract audio, transcribe with Amazon Transcribe, run a SageMaker classifier to flag policy-violating content, redact PII, and store transcripts and decisions, indexing per-tenant metadata in DynamoDB. Design an end-to-end, event-driven pipeline with data isolation, data sovereignty, error handling, and privacy controls?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Google","Square"]},{"id":"q-1831","question":"You're building a beginner AWS-only pipeline to process user-uploaded audio stories (up to 120 seconds). When an audio file lands in S3, design an event-driven flow that uses Amazon Transcribe to produce a transcript, Amazon Comprehend to extract keywords, and a SageMaker endpoint to classify genre. Save transcript and metadata to S3 and index in DynamoDB. Include data flow, IAM permissions, error handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Adobe","Anthropic","Plaid"]},{"id":"q-1902","question":"You're building a beginner AWS-only pipeline to moderate user-uploaded profile pictures (JPEG/PNG, up to 2MB) in S3; on upload, design an event-driven flow using Rekognition to detect unsafe content and top labels, write a JSON report to S3, and upsert a summary in DynamoDB with fields like userId, objectKey, safeFlag, topLabel. Include data flow, IAM, error handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Databricks","MongoDB"]},{"id":"q-1935","question":"Design a real-time, multi-tenant fraud-detection pipeline for a SaaS platform. Ingestion uses Kinesis Data Streams per tenant; a central SageMaker detector scores each event; PII is redacted before storage in per-tenant S3 buckets; an index is kept in DynamoDB with fields tenantId, sessionId, eventTime, fraudScore. Explain data isolation, cross-account IAM, error handling, privacy controls, and idempotency/replay safety?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Adobe","PayPal"]},{"id":"q-2018","question":"Design an end-to-end, event-driven AWS pipeline for a regulated financial analytics service. Tenants upload encrypted JSON trade logs (up to 5 MB) to S3. Build per-tenant, region-isolated processing: decrypt with KMS, run a SageMaker multi-tenant analytics model, redact PII with Comprehend and regex, and store outputs in region-scoped S3 prefixes. Maintain an immutable audit trail in DynamoDB with versioning, implement retry and DLQ, and enforce strict IAM boundaries?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Adobe","Discord","Uber"]},{"id":"q-2045","question":"Design a real-time, AWS-only PII redaction pipeline for streaming chat messages. Ingest messages enter regional Kinesis Data Streams with tenant isolation. Use SageMaker endpoint (or Comprehend) to detect PII, redact spans, store redacted messages in S3, and index per-tenant audit logs in DynamoDB (tenantId, messageId, detectedPII, redacted, timestamp). Include data flow, IAM, error handling, privacy controls, and rate-limiting?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["DoorDash","Lyft","Netflix"]},{"id":"q-2075","question":"Design a real-time fraud detection pipeline for a high-volume platform used by Lyft, PayPal, and Snap. Ingest payment events into Kinesis, enrich with per-tenant risk data from DynamoDB via Lambda, compute a fraud score with a SageMaker endpoint, and enforce decisions in DynamoDB while routing high-risk events to a Step Functions human-in-the-loop workflow and notifying security via SNS. Include data isolation, sub-200 ms latency budget, and privacy controls with PII masking and audit trails?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Lyft","PayPal","Snap"]},{"id":"q-2175","question":"You're building a real-time, AWS-native, multi-tenant text inference service for content safety at scale. Incoming messages arrive with tenant context and must be isolated either via per-tenant endpoints or tenant-scoped routing. Describe architecture for throughput, deployment strategy (canary/rolling), privacy controls (encryption, PII redaction), drift monitoring, data retention, and cost governance. Include data flow, IAM, and observability?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Hugging Face","Meta","NVIDIA"]},{"id":"q-2208","question":"You're building a privacy-preserving, multi-tenant receipt processor on AWS. Clients upload receipts (PDF/JPG up to 5MB) to S3. Design an event-driven flow: Textract extracts fields, a SageMaker endpoint classifies line items, redact PII, and write structured data to DynamoDB with tenant isolation. Include data flow, IAM, SSE-KMS, retries, drift monitoring, audit logs, privacy controls, and testing strategy?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"intermediate","tags":["aws-ai-practitioner"],"companies":["Airbnb","DoorDash","OpenAI"]},{"id":"q-2387","question":"You're processing PDFs uploaded to S3 (up to 5 MB). Design an event-driven AWS pipeline that uses Textract to extract text and tables, Comprehend to identify keywords, stores outputs in S3, and writes a compact index to DynamoDB with fields like documentId, s3Key, textSnippet, phrases. Include IAM, error handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["Instacart","MongoDB","Oracle"]},{"id":"q-2445","question":"You're designing a real-time, multi-tenant chat moderation pipeline on AWS. Ingest messages from multiple tenants via Kinesis Data Streams, score content with a versioned SageMaker endpoint, redact PII, and persist per-tenant results to DynamoDB with TTL, while archiving raw and redacted messages to S3. Design end-to-end architecture with latency target ~150 ms, strong data isolation, model versioning, encryption, and cost controls; include IAM roles, VPC endpoints, and monitoring?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"advanced","tags":["aws-ai-practitioner"],"companies":["Amazon","Twitter"]},{"id":"q-951","question":"You're building a low‑ops internal voice assistant for support scripts. Audio files up to 60 seconds are uploaded to S3. Design an AWS‑only pipeline that transcribes, analyzes sentiment, and stores a brief summary plus an index in DynamoDB, with minimal cost and ops. Include data flow, services, error handling, and privacy considerations?","channel":"aws-ai-practitioner","subChannel":"general","difficulty":"beginner","tags":["aws-ai-practitioner"],"companies":["MongoDB","Snap"]},{"id":"q-1336","question":"In a multi-account AWS data lake used by a global product analytics team, ingest semi-structured data from several SaaS APIs into S3, enforce schema evolution and data quality checks, and expose curated tables with time-travel queries. Describe the end-to-end design including data formats, upsert strategy, and governance controls you would apply using AWS services?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Amazon","Apple"]},{"id":"q-1419","question":"Design an advanced, multi-region data ingestion and governance pipeline for a global fintech platform. Data from partners arrives as JSON and Parquet; must enforce per-tenant isolation, support near real-time analytics, and GDPR erasure. Outline architecture using AWS Kinesis (streams), DMS for CDC, Glue (ETL), Iceberg on S3 for schema-evolving tables, Lake Formation/IAM for access, and Athena/Redshift Spectrum for queries. Include format choices, CDC vs batch mix, lineage, and failure modes?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Meta","Stripe"]},{"id":"q-1487","question":"You operate an AWS data lake where streaming user activity flows from Kinesis Data Streams into S3 via Firehose, with Glue Data Catalog, Athena queries, and dashboards. Schemas evolve and data quality is mission-critical. Design a real-time data quality and drift detection framework: schema drift, per-record quality checks, quarantine failing files, alert owners, and preserve cross-account auditability?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Adobe","Coinbase","Oracle"]},{"id":"q-1506","question":"Scenario: A partner API provides daily batches of image assets with evolving metadata. Build an end-to-end ingest to a multi-region data lake: landing in S3, metadata in a versioned, upsertable table, idempotent processing, schema evolution, and governance across accounts. Describe data formats, partitioning, dedupe strategy, and cross-account orchestration?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Instacart","Slack","Stripe"]},{"id":"q-1521","question":"You manage a global, multi-tenant data platform where streaming events bucket into S3 and downstream engines like Redshift and Athena rely on per-tenant schemas. Design a resilient end-to-end architecture that versions datasets, enforces per-tenant schema contracts, detects drift, quarantines bad records, and supports rollback without downtime. Specify services, data formats, governance, and observability?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Adobe","Google","NVIDIA"]},{"id":"q-1543","question":"Design an event-driven data platform to ingest order and driver events from regional services into a centralized data lake for near-real-time analytics. Data arrives as evolving JSON schemas; must handle schema evolution, efficient partitioning, cross-account access, and cost. Which AWS services and data design patterns would you implement, and how would you validate data quality and enable fast ad-hoc queries?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["DoorDash","Square"]},{"id":"q-1608","question":"Scenario: A new source streams JSON events from a mobile app via Kinesis Data Streams. Design a beginner-level ingestion pipeline to land data in S3 as Parquet with daily partitions, using AWS Glue for ETL and cataloging. Include services, data formats, schema evolution approach, basic data quality checks, and how you would test end-to-end before analytics?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["NVIDIA","Netflix","Zoom"]},{"id":"q-1631","question":"You receive a daily nested JSON file and a separate CSV in an S3 bucket. Design a beginner-level ingestion using AWS Glue Studio to flatten the JSON, normalize types, parse the CSV, and write Parquet under a date-partitioned path s3://data-lake/events/date=YYYY-MM-DD/. Register a Glue catalog table, outline simple schema-drift handling, and include basic data-quality checks plus a quick end-to-end test plan with sample files?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["Instacart","Oracle","Twitter"]},{"id":"q-1654","question":"New data source writes daily CSVs to S3. Design a beginner pipeline to validate a defined schema, fail-fast on missing required fields, and route bad records to a quarantine bucket with a reason. Store valid data as Parquet in S3 with daily partitions, and catalog via Glue so Athena can query. Include a basic end-to-end test plan?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["Plaid","Two Sigma","Zoom"]},{"id":"q-1785","question":"Two-account AWS data lake (us-east-1 and eu-west-1) ingests partner JSON events via API into S3. Design an end-to-end pipeline using Glue for ETL and cataloging, store Parquet with daily partitions by country/date, enable Athena queries with Lake Formation access controls, and handle schema evolution, data quality, late-arrivals, and cross-region replication trade-offs?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Lyft","Snap"]},{"id":"q-1816","question":"You collect protobuf-encoded telemetry from thousands of IoT devices via AWS IoT Core; design an end-to-end ingestion that lands as daily-partitioned Parquet in S3, uses AWS Glue Schema Registry for evolving schemas, and implements idempotent deduplication plus a 3-sigma anomaly check. Include data formats, partition keys, testing strategy?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Amazon","Meta","Square"]},{"id":"q-1883","question":"Design a streaming data contract and quality workflow for events arriving from multiple teams via MSK and Kinesis. Use AWS Glue Schema Registry to enforce evolving Avro/JSON schemas with versioning and compatibility (backward/forward). Describe publishing schemas, validating payloads at ingest, storing Parquet in S3 with daily partitions, and monitoring for drift and quality across regions?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Cloudflare","Google"]},{"id":"q-1968","question":"Design an end-to-end streaming-to-lake pipeline: ingest real-time events from AWS MSK, consolidate into Apache Iceberg tables on S3 with daily partitions, support CDC-style upserts/deletes, and implement schema evolution. Include governance with Lake Formation/IAM, testing strategy (canaries, synthetic data), and data quality/lineage monitoring?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Anthropic","Google"]},{"id":"q-1981","question":"Design a data lake ingestion pipeline on S3 for semi-structured SaaS data that must support upserts and deletes with time-travel queries; choose between Apache Iceberg, Hudi, or Delta Lake on AWS (Glue, EMR, Athena) and justify: schema evolution, compaction, CDC handling, partitioning, and data quality checks; include testing plan?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Hashicorp","Instacart","Two Sigma"]},{"id":"q-2019","question":"Scenario: IoT sensors on factory floors emit JSON telemetry with evolving schemas (new metrics appear over time). Design a streaming ingestion pipeline that reads from Kinesis Data Streams, writes to S3 as Parquet with daily partitions, and uses an Iceberg catalog (Glue-backed) to handle schema evolution. Compare upsert strategies for late data (Iceberg MERGE INTO vs Hudi), outline data quality checks, and a testing plan including sandbox data and end-to-end validation. Include governance via Lake Formation?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Citadel","IBM","Tesla"]},{"id":"q-2060","question":"Design a multi-region, streaming-plus-batch ingestion pipeline for a mobile app that emits JSON events at high volume. Ingest via Kinesis Data Streams to S3 Parquet using Apache Iceberg tables with a Glue catalog, supporting upserts, deletes, and schema evolution. Include multi-source CDC, data quality checks, governance via Lake Formation, and end-to-end testing?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["LinkedIn","Meta","PayPal"]},{"id":"q-2095","question":"You receive a financial dataset hourly from a partner API in JSON with nested fields. Design a beginner-level ingestion pipeline to land data in S3 as Parquet with hourly partitions, using AWS Glue for ETL and cataloging. Include how you flatten nested JSON, define a stable schema, enable basic data quality checks, ensure encryption with KMS, and implement least-privilege IAM roles and Lake Formation permissions. Outline how you'd test end-to-end before analytics?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["Robinhood","Snap"]},{"id":"q-2144","question":"Design a real-time fraud signals pipeline: ingest 50 banks via Kinesis Data Streams, use Glue Schema Registry for evolving Avro schemas, run Spark Streaming to land Parquet in S3 partitioned by date and merchantId, with a raw zone and a masked curated zone. Apply IP masking and immutability retention; catalog with Glue Data Catalog; enforce access via Lake Formation; emit lineage/audit logs. Include failover tests and latency considerations?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Plaid","Square"]},{"id":"q-2204","question":"Design a real-world, multi-tenant, multi-account ingestion pipeline: Ingest JSON events from microservices via Kinesis Data Streams, store as Parquet in S3 with daily partitioning by tenant_id, and catalog with AWS Glue. Enforce tenant isolation with Lake Formation across accounts, handle schema evolution, embed data quality checks, and outline end-to-end testing strategies?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Airbnb","Bloomberg","Google"]},{"id":"q-2244","question":"You receive daily JSON event files from a mobile app stored in S3 with nested arrays. Design a beginner-friendly ingestion pipeline to flatten into Parquet with daily partitions, catalog in AWS Glue, and implement a basic data quality gate (required fields, non-null user_id, ISO8601 event_time). Include testing approach and handling of schema drift?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["Amazon","LinkedIn","Lyft"]},{"id":"q-2264","question":"In a cross-account AWS data lake spanning us-east-1 and eu-west-1, ingest streaming events from Kinesis into an Apache Hudi dataset on S3 to support upserts and time-travel. Design the end-to-end architecture including how to implement upserts, schema evolution, compaction, cross-account Lake Formation policies, and an end-to-end test and rollback plan, with concrete services and data formats?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Databricks","Google"]},{"id":"q-2312","question":"Scenario: you manage a multi-tenant SaaS data lake across three AWS accounts. Ingest telemetry events via Kinesis Data Streams, land as Parquet in S3 with daily partitions, and register metadata in the Glue Data Catalog. Enforce schema evolution and tenant-level masking using Lake Formation; enable cross-account analytics with Athena. Describe architecture, governance, and testing strategy?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Lyft","Microsoft","MongoDB"]},{"id":"q-2350","question":"Design an end-to-end ingestion and upsert pipeline for customer records with SCD Type 2 using Apache Iceberg on S3. Ingest JSON events from Kinesis and batch CDC from RDS; ensure schema evolution, partitioning, and time travel; discuss how to reconcile late-arriving changes and record-level lineage. Include services, table format, upsert strategy, and testing?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Adobe","Apple","Meta"]},{"id":"q-2440","question":"Scenario: Daily JSON app-logs arrive in S3 at s3://bucket/raw/app-logs with fields userId, timestamp, action, and optional device {model,os}. For a beginner, design an ingestion pipeline that flattens to Parquet with daily partitions into s3://bucket/curated/app-logs/yyyy/mm/dd/, catalogs in Glue, and uses a defined schema (userId string, timestamp timestamp, action string, device_model string, device_os string) with schema evolution enabled. Include data quality gates (required userId, non-null timestamp, restricted action set), drift handling, and end-to-end testing plan?","channel":"aws-data-engineer","subChannel":"general","difficulty":"beginner","tags":["aws-data-engineer"],"companies":["Coinbase","Plaid","Tesla"]},{"id":"q-2494","question":"Ingest a hybrid workload into an Iceberg table on S3: streaming JSON events and nightly relational extracts, partitioned by date. Outline a concrete pipeline using AWS Glue, S3, and query engines (Athena/Databricks), with data formats, schema evolution, upserts, late-arriving data, and end-to-end validation. How would you monitor partitions and performance and enforce access controls?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Anthropic","Databricks","Plaid"]},{"id":"q-2594","question":"Design an end-to-end AWS data lake ingestion and governance solution that provides complete data lineage across multiple sources (RDS, SaaS API, IoT), transformations in AWS Glue, and consumption in Athena. Include how you model lineage metadata (source, transform, target), handle schema evolution, and how you validate lineage during failing ETL runs and across re-processing, with Lake Formation fencing?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["PayPal","Snap"]},{"id":"q-2653","question":"Scenario: A global product analytics data lake ingests streaming clickstream events from mobile apps with evolving JSON schemas containing PII. You must mask PII in the shared dataset before analytics, support cross-account access, and keep raw data auditable for compliance. Design end-to-end ingestion using AWS (Kinesis or MSK, Glue, S3, Lake Formation/Macie, KMS). Address schema evolution, late data, data quality, testing, and rollout?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Meta","MongoDB","Scale Ai"]},{"id":"q-2686","question":"Design an ingestion pipeline for a new SaaS data stream of JSON events that lands in S3 with daily partitions. Use AWS Glue (Spark) and Apache Iceberg on S3 to materialize updatable tables with idempotent merges, handle schema evolution, and cope with late-arriving data. Describe partitioning, upsert strategy, schema evolution, data quality checks, testing, and monitoring?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Instacart","Snap","Tesla"]},{"id":"q-886","question":"Scenario: you manage a financial data lake with multiple transactional sources feeding S3 via DMS. Stakeholders require upserts, full history for compliance, fast dashboards using a latest-state table, and seamless schema evolution. Compare Iceberg, Hudi, and Delta Lake for this scenario and outline a concrete pipeline (CDC, ETL, compaction, governance)?","channel":"aws-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["aws-data-engineer"],"companies":["Goldman Sachs","Tesla"]},{"id":"q-948","question":"In a multi-tenant data lake on S3 across two AWS accounts, each tenant's data must be isolated, with auditable access, cost accounting, and fast analytics for dashboards. Design a solution using AWS Lake Formation for governance, S3 prefixes per tenant, and Athena/Glue for analytics. Explain cross-account sharing, tag-based access, and data lineage, plus failure modes?","channel":"aws-data-engineer","subChannel":"general","difficulty":"advanced","tags":["aws-data-engineer"],"companies":["Apple","Discord","Netflix"]},{"id":"q-1074","question":"Scenario: A global time-series platform ingests 1M events/hour in us-west-2; dashboards in eu-central-1 and ap-southeast-2 need sub-200ms reads on the latest window. Data must be immutable for 90 days for compliance. Compare DynamoDB Global Tables with DAX vs Aurora PostgreSQL Global Database with cross-region backups. Provide topology, replication, PITR/backup plans, and RPO/RTO targets?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Apple","Microsoft"]},{"id":"q-1131","question":"**Hybrid Analytics Path for Multiregion Aurora**\n\nYou're running an Aurora PostgreSQL OLTP cluster with tenant isolation via RLS in us-east-1. A regulatory BI team in eu-west-1 requires near real-time analytics with masked PII. Design a hybrid analytics path using Aurora Global Database for OLTP replicas and a CDC-based analytic store (Redshift or DynamoDB+Lambda) in eu-west-1. Describe data flow, masking strategy, encryption, failover, and how to meet RPO 5s and RTO 60s, including cost considerations?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Coinbase","Goldman Sachs","Two Sigma"]},{"id":"q-1279","question":"In a multi-tenant SaaS on AWS, run a single Aurora PostgreSQL cluster with per-tenant schemas and RLS to isolate data. An analytics team in eu-west-1 requires cross-tenant BI with masked PII in near real-time dashboards. Design a cost-aware architecture that delivers masking, auditing, and SLA, comparing per-tenant schemas in a single cluster vs separate clusters per tenant. Include data flow, backup, and failover?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Amazon","Hugging Face","Robinhood"]},{"id":"q-1303","question":"In a multi-tenant SaaS using Aurora PostgreSQL with Global Database spanning us-west-2 and us-east-1, tenants must have isolated data access and BI dashboards must mask PII in real time. Propose an end-to-end design using per-tenant RLS, dynamic masking for BI, and a separate analytics store fed by CDC (DMS/Debezium). Include cross-region DR with RPO <5s and RTO <60s, data flow, encryption, backups, and a concrete sizing plan (replicas, window, network)?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Databricks","Lyft","Tesla"]},{"id":"q-1314","question":"Design a GDPR-compliant data deletion strategy for a multi-region Aurora PostgreSQL Global Database that uses us-east-1 as the writer and replicas in multiple regions. How would you implement Right-to-Erasure for tenant data, propagate deletions with minimal latency, handle referential integrity, and maintain an auditable trail while meeting RPO/RTO targets? Include practical steps and trade-offs?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Google","Hashicorp","Lyft"]},{"id":"q-1382","question":"In an Aurora PostgreSQL Global Database with a writer in us-east-1 and replicas in eu-west-1, a financial balance update must be atomic across regions. Explain why cross-region distributed transactions are not supported and propose a practical pattern to achieve atomic-ish behavior with low latency, including data flow, failover handling, and cost/latency trade-offs?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Airbnb","Stripe"]},{"id":"q-1441","question":"In Aurora PostgreSQL (us-east-1) with tenant isolation via RLS, design a near real-time analytics path for a eu-west-1 consumer needing masked data and <5s lag. Use Aurora Global Database for OLTP and a CDC store in eu-west-1 (Redshift or DynamoDB+Lambda). Explain data flow, masking/encryption, consistency, failover, and cost with concrete config sketches?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Amazon","Microsoft"]},{"id":"q-1447","question":"You run a high-volume ecommerce on Aurora PostgreSQL Global Database with a single writer in us-east-1 and read replicas in eu-west-1. An outage in us-east-1 requires routing writes to eu-west-1 within 60s while ensuring RPO<5s, idempotent writes, and no double billing. Design the architecture and concrete configuration (replica counts, failover procedures, analytics CDC path, masking, PITR) to meet these goals?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Airbnb","Hugging Face"]},{"id":"q-1550","question":"In a two-region deployment with a single writer in us-east-1 and analytic reads in eu-west-1, design a CDC pipeline to keep a near real-time analytic store updated within 5 seconds of commits, while masking per-tenant data and enforcing encryption at rest and in transit. Compare AWS DMS, Debezium/Kafka, and native Aurora logical replication, and provide concrete configuration (engine, instance types, replica counts, PITR, KMS keys, VPC endpoints, and network topology) to meet RPO 5s and RTO 60s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Discord","DoorDash","Snowflake"]},{"id":"q-1747","question":"You run a multi-region SaaS with Aurora PostgreSQL as the OLTP in us-east-1 and read replicas in eu-west-1. A new requirement enforces strict per-tenant data isolation via Row-Level Security and data residency controls for backups. Design a concrete approach: RLS policy skeletons for all tables, session-based tenant_id from authentication, per-tenant restore strategy, cross-region backup copy schedule, and a testing/validation plan that proves no cross-tenant leakage under burden. Include concrete config knobs and a sample policy?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["MongoDB","Slack"]},{"id":"q-1775","question":"In a multi-region Aurora PostgreSQL Global Database setup (writer in us-east-1; readers in eu-west-1 and ap-south-1) with strict tenant-level data residency, design a scalable architecture that provides sub-50ms reads for hot paths in each region while ensuring RPO <= 5s and RTO <= 60s, using row-level security and a CDC-based analytic store; explain data partitioning, access controls, and failover strategy, plus cost trade-offs?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Adobe","Zoom"]},{"id":"q-1882","question":"Design a cross-region analytic path for a SaaS app with an Aurora PostgreSQL OLTP cluster in us-east-1 as the single writer and regional read replicas in us-west-2. The goal: near real-time analytics with masked PII in the analytics store. Propose a CDC-based pipeline (Aurora CDC, DMS, Debezium, or Kinesis) to load into Redshift or DynamoDB in us-west-2, choose masking strategy, encryption, data freshness target (RPO), failover plan, and cost considerations. Include concrete config choices (instance types, retention, network, and security)?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Snowflake","Tesla"]},{"id":"q-2139","question":"You run OLTP in Aurora PostgreSQL us-east-1 and need near real-time BI in us-west-2. Design a CDC pipeline: enable a dedicated logical replication slot in Aurora; use DMS in CDC mode to stream changes to Redshift in us-west-2 via a staging S3 bucket; apply PII masking at BI layer; enable PITR and cross-region backups; target end-to-end latency ~2s and RTO <60s. Include data flow, failover, and cost trade-offs?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Google","Netflix"]},{"id":"q-2163","question":"A multi-region SaaS app needs sub-20ms reads for hot tenants across three continents, writes allowed in any region, and PCI-DSS data residency constraints. Design a data-layer using AWS: compare Aurora PostgreSQL Global Database with DynamoDB Global Tables plus CDC to an analytics store, including consistency, DR, backups, and concrete configurations to meet RPO 5s and RTO 60s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Databricks","Google"]},{"id":"q-2211","question":"Two-region, multi-tenant SaaS with strict data residency: EU tenants' data must stay in EU, US tenants' data in US. Needs sub-15ms reads for hot tenants, writes in any region, and cross-region analytics. Compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables with analytics options; provide a concrete topology, replication, backups, and DR plan to meet RPO 5s and RTO 60s, including per-tenant routing and residency enforcement?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Hugging Face","Salesforce","Twitter"]},{"id":"q-2248","question":"A SaaS news site logs user events at ~1000 writes/s, with dashboards needing sub-200 ms reads. Data is append-only; hot data kept 30 days, archived after. Compare DynamoDB (on-demand, TTL, GSI) vs Aurora PostgreSQL (partitioned tables, read replicas) for this workload. Provide concrete configs (primary key design, indexes, TTL window, backup/retention, RPO/RTO) and justify choice?","channel":"aws-database-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-database-specialty"],"companies":["Netflix","Square","Two Sigma"]},{"id":"q-2268","question":"Your app uses AWS Lambda functions that connect to an RDS PostgreSQL instance; during bursts, you see many connections causing failures. How would you leverage Amazon RDS Proxy to manage connections, configure auth, and ensure stable performance? Include what to monitor, any pricing considerations, and a basic setup outline?","channel":"aws-database-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-database-specialty"],"companies":["Amazon","Goldman Sachs","MongoDB"]},{"id":"q-2320","question":"Scenario: A SaaS app stores event data in DynamoDB and must retain 90 days in DynamoDB and archive older events to S3 for analytics. Design a pragmatic lifecycle: data model, TTL, export to S3, per-tenant/year/month partitioning, storage class selection, and validation plan to ensure data integrity and queryability via Athena. Include concrete steps and considerations for cost?","channel":"aws-database-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-database-specialty"],"companies":["Databricks","Scale Ai"]},{"id":"q-2331","question":"For a multi-tenant SaaS app storing per-tenant PII with EU/US data residency, compare Aurora PostgreSQL with Row-Level Security (RLS) vs DynamoDB with per-tenant access patterns. Explain schema design, replication strategy, consistency, DR, and cost. Provide a concrete configuration to meet RPO < 5s and RTO < 60s, including region placement, replica counts, backup windows, and key management?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["MongoDB","Plaid","Uber"]},{"id":"q-2411","question":"In an IoT platform, 100k writes/sec of time-series data arrive from devices worldwide. You must ingest region-locally with sub-20ms latency, perform near real-time analytics in a separate region, retain 30 days of data, and ensure PCI-DSS residency. Compare AWS Timestream, DynamoDB Global Tables with a CDC pipeline, and an Aurora-based time-series schema. Propose architecture, data model, retention, DR, and concrete config to meet RPO 5s and RTO 60s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["IBM","Twitter","Two Sigma"]},{"id":"q-2465","question":"An Aurora PostgreSQL cluster in us-east-1 serves 10k+ tenants via IAM database authentication. Each tenant must only access its own rows using Row-Level Security. A separate analytics workload must run against a eu-west-1 replica with data within 5 seconds of writes. Design the architecture: RLS policy and session management for per-tenant isolation, how analytics access is granted without leaking data, replication strategy (global DB vs separate KV store), backup/PITR, and a practical test plan to verify isolation and latency?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Citadel","Oracle","Two Sigma"]},{"id":"q-2511","question":"In a globally distributed SaaS, Aurora PostgreSQL Global Database writer in us-east-1 and read replicas in eu-west-1 and ap-southeast-2. To deliver sub-50ms reads for hot tenants during peak while writes can occur anywhere, design a hybrid path using ElastiCache Redis in each region plus a cache-aside strategy. Include concrete config: DB instance classes, replica counts, Redis node types, TTL, invalidation mechanism, and a failover plan that meets RPO 5s and RTO 60s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["IBM","MongoDB","Scale Ai"]},{"id":"q-2533","question":"In a multi-region SaaS app using Aurora PostgreSQL with a single writer in us-east-1 and reads in eu-west-1 and ap-south-1, implement row-level security to restrict each tenant's data. Explain how you would design the RLS policy, index usage, and the impact on cross-region CDC via DMS or logical replication, and outline monitoring for unauthorized access. Include testing steps?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["LinkedIn","Netflix","Uber"]},{"id":"q-2581","question":"Global SaaS with EU data residency: Writes in us-east-1; reads in EU must be sub-20ms; PCI-DSS data residency constraints; design a cross-region OLTP data layer and compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables with CDC. Provide concrete configurations (engine/edition, instance types, replica counts, PITR window, backup cadence, KMS keys, VPC design, inter-region networking), DR plan, and how you meet RPO <5s and RTO <60s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-database-specialty"],"companies":["Coinbase","NVIDIA","Stripe"]},{"id":"q-2701","question":"In Aurora PostgreSQL design per-tenant data isolation using Row-Level Security and partitioned tables for a global setup: writer in us-east-1; regional reads in eu-west-1 and ap-south-1. Propose a concrete data path and DR strategy to meet RPO 5s and RTO 60s, including instance types, backup windows, PITR, KMS, cross-region replication, and a real failover plan?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Discord","Microsoft"]},{"id":"q-851","question":"Two-region OLTP SaaS with a single writer in us-east-1 and read replicas in eu-west-1. Compare Aurora Global Database (PostgreSQL) vs DynamoDB Global Tables for this workload: latency targets, consistency model, failover behavior, and cost. Which approach would you pick and why, and what concrete configuration (replica count, failover window, write routing) would you implement to meet RTO < 60s and RPO < 5s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Robinhood","Snowflake"]},{"id":"q-871","question":"Migration plan: An OLTP app runs on Aurora PostgreSQL provisioned; traffic is bursty; you want to evaluate Aurora Serverless v2. Provide a concrete plan to migrate, including: (1) start/stop criteria and scaling configuration; (2) handling of long-running transactions and prepared statements; (3) how to keep reads consistent during scaling; (4) testing approach for failover/RTO targets; (5) cost considerations and potential pitfalls with Serverless v2?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Hugging Face","IBM","Uber"]},{"id":"q-895","question":"Your multi-region SaaS needs an audit-friendly cross-tenant analytics store with writes transactional in us-east-1 and analytics queries in eu-west-1 under GDPR. Compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables for this workload, focusing on transactional integrity, analytics capability, PITR/retention, cross-region latency, and cost. Recommend a concrete configuration (writer region, replica counts, PITR window, tenant isolation, ETL approach) to meet RPO 15 minutes and RTO 1 hour?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Google","Slack","Square"]},{"id":"q-956","question":"For a real-time fraud graph application needing sub-100ms neighbor lookups across two AWS regions, compare Amazon Neptune Global Database with DynamoDB (using graph patterns and DAX) for this workload. Writer region us-east-1; readers in eu-west-1; assess graph traversal latency, consistency guarantees, failover behavior, and total cost. Provide a concrete setup (cluster engine and size, replica counts, PITR window, backup schedule, and network/config) to meet an RPO of 5s and an RTO of 60s?","channel":"aws-database-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-database-specialty"],"companies":["Meta","NVIDIA","Snap"]},{"id":"q-964","question":"You run an Amazon RDS PostgreSQL in **us-east-1** with automated backups. A regional outage blocks access from that region. How would you achieve **RPO ≤ 60s** and **RTO ≤ 15 minutes** by restoring to **eu-west-1**? Compare cross-region read replicas, backup copy, and Aurora Global Database, and outline concrete steps, knobs, and caveats?","channel":"aws-database-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-database-specialty"],"companies":["LinkedIn","Slack","Tesla"]},{"id":"q-1183","question":"Design a cross-account AWS CI/CD flow for a real-time analytics platform with data plane in Account A, model training in Account B, and API endpoints in Account C. Implement Terraform-driven IaC, policy-driven approvals, drift detection, canary promotion with synthetic monitoring, and automated rollback. How do you enforce least privilege, secret rotation, and auditable artifact pipelines across accounts?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Coinbase","Microsoft","MongoDB"]},{"id":"q-1416","question":"Design a beginner-friendly AWS CI/CD pipeline for a Dockerized REST API deployed to ECS Fargate behind an ALB. Source from GitHub; CodeBuild runs pytest and a Trivy container image scan; pushes image to ECR; deploys with a blue/green traffic shift across ECS target groups. Explain IAM least-privilege and Secrets Manager usage; include manual Prod approval and basic observability?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Airbnb","Microsoft","Stripe"]},{"id":"q-1553","question":"Design a cross-region, risk-aware deployment for a real-time streaming data platform (Kinesis Data Streams → Lambda → DynamoDB) using CodePipeline/CodeBuild and CDK. Implement blue/green rollout across us-east-1 and us-west-2, with synthetic canary tests, latency/SLA-based automatic rollback, and drift detection. Explain IAM roles, Secrets rotation, and audit logging?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Robinhood","Square","Zoom"]},{"id":"q-1613","question":"Design a Git-driven CI/CD workflow to deploy a two-region microservice API on EKS (us-east-1, us-west-2) with Argo Rollouts canary, Terraform IaC, and DynamoDB Global Tables. Include region-specific config via AppConfig, IAM least privilege, drift detection, and automated rollback on latency or error-rate spikes?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Airbnb","Netflix","Zoom"]},{"id":"q-1728","question":"Design a beginner-friendly AWS CI/CD workflow that builds a Docker image from a GitHub repo, runs unit tests, runs a static security scan, pushes to ECR, and deploys to an ECS Fargate service using CodeDeploy blue/green with a canary (15%). Include rollback on failure, health checks, and IAM least-privilege. Explain staging vs prod isolation and how you’d structure permissions and secrets?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["NVIDIA","Snowflake","Two Sigma"]},{"id":"q-1811","question":"Design a GitHub-driven CI/CD pipeline that builds microservices to ECR, tests in CodeBuild, and deploys via ArgoCD to per-service namespaces on EKS. Store per-service configs in Secrets Manager/SSM; enforce IAM least privilege with scoped roles; implement canary rollouts with synthetic checks and automatic rollback on failure. Include artifact/config flow and audit strategy?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Google","Oracle","Square"]},{"id":"q-1822","question":"Design a cross-account, multi-region deployment for a real-time fraud-detection platform: data plane in Account A (Kinesis/S3), model training in Account B (SageMaker), live REST endpoints in Account C (API Gateway/Lambda/DynamoDB). Provide a concrete plan using Terraform, CodePipeline, cross-account roles, drift checks, end-to-end tracing, and canary promotions with automated rollback. Include IAM, secret rotation, and success criteria?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Coinbase","Databricks","Scale Ai"]},{"id":"q-1865","question":"You have a small containerized web app in GitHub, deployed to AWS ECS Fargate behind an Application Load Balancer. Design a beginner CI/CD pipeline using GitHub Actions to build and push a Docker image to ECR, register a new ECS task definition, update the staging service, run a smoke test against the staging ALB, and require a manual approval before production. How would you implement IAM least-privilege and secrets management (Secrets Manager or SSM) for this flow?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Amazon","OpenAI","Robinhood"]},{"id":"q-1895","question":"In a three-region deployment for a real-time personalization service using Lambda@Edge and CloudFront, design a CI/CD workflow that sources code from Git, provisions infra with Terraform, and coordinates region-local deployments via CodePipeline. Implement per-region canaries with traffic weights, automated rollback on synthetic checks, Secrets Manager rotation, KMS key rotation, and auditable artifacts. Separate staging and prod accounts and validate edge caching with synthetic tests?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["Salesforce","Scale Ai","Snap"]},{"id":"q-1907","question":"Design a beginner-friendly AWS DevOps pipeline for a small Node.js API containerized app deployed to ECS Fargate behind an Application Load Balancer. Source from GitHub; CodeBuild runs npm test and a lightweight security scan (Trivy); build Docker image and push to ECR; deploy via CodePipeline using ECS blue/green (CodeDeploy) with canary shifts across dev, stage, prod. Explain IAM least privilege and Secrets Manager use for env vars, plus rollback triggers?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Meta","Microsoft","Slack"]},{"id":"q-1923","question":"Design a two-account AWS CI/CD pipeline for a multi-tenant SaaS app deployed to ECS Fargate, sourced from GitHub. Use Terraform for IaC, enforce per-tenant isolation via tag-based deployment, and AppConfig feature flags to enable tenant-specific features. Implement cross-account artifact sharing, canary promotions with synthetic monitoring, and automatic rollback on latency or error-rate thresholds. Explain IAM guardrails and auditability across accounts?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Anthropic","Databricks","Zoom"]},{"id":"q-1960","question":"You’re deploying a simple REST API on AWS Lambda behind API Gateway with a GitHub source. Implement a runtime feature flag using AWS AppConfig that can be toggled without redeploying. Describe the minimal IAM permissions, how Lambda fetches the flag on cold starts and keeps it refreshed, and a safe canary rollout workflow with rollback. Include a brief code snippet in your preferred language?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Apple","Lyft"]},{"id":"q-1984","question":"Design a two-region CodePipeline per region (us-east-1, eu-west-1) for a three-service ECS/Fargate app behind an ALB. Source from GitHub; CodeBuild runs unit/integration tests plus Trivy/CodeQL scans; artifacts encrypted in S3; deploy via CDK to StackSets; blue/green per service; Route 53 latency routing; automatic rollback on health checks; drift detection with AWS Config; Secrets in Secrets Manager; enforce least-privilege IAM?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Airbnb","OpenAI","Stripe"]},{"id":"q-1998","question":"You're deploying a small REST API to AWS ECS Fargate in us-east-1 with GitHub as the source and a basic CodePipeline. Implement a beginner-friendly disaster-recovery workflow: cross-region deployment to us-west-2, Route 53 failover, and S3 artifact replication. Show minimal IaC (CloudFormation), a simple health-check-based failover policy, and how you test failover?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Citadel","OpenAI"]},{"id":"q-2029","question":"Design a GitOps-based CI/CD workflow for a microservices app running on two AWS accounts with an EKS cluster in each (staging and prod), using FluxCD to deploy Helm charts, IRSA for service accounts, and OPA Gatekeeper for policy enforcement, plus Istio for canary promotions and automatic rollback on 5xx errors or latency spikes. Include repo structure, IAM roles, and example configurations?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Goldman Sachs","Google","PayPal"]},{"id":"q-2058","question":"Design a multi-region, zero-downtime deployment for a serverless REST API using AWS Lambda, API Gateway, and DynamoDB, with cross-region replication, automated canary shifts, and regional failover testing. Provide Terraform that wires cross-account roles, Secrets Manager, and CloudWatch logs, plus a rollback plan?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Google","OpenAI","PayPal"]},{"id":"q-2115","question":"You're deploying a small REST API container to AWS ECS Fargate in us-east-1 behind an Application Load Balancer, with GitHub as the source and a basic CodePipeline. Implement a beginner-friendly blue/green deployment using CodeDeploy for ECS: provide a minimal CloudFormation snippet to enable CODE_DEPLOY deployments, configure a 10% canary start, include a simple health check path, and outline rollback criteria if health checks fail or error rate exceeds threshold. How would you test failover and enforce least-privilege IAM roles?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Coinbase","Square","Twitter"]},{"id":"q-2142","question":"Design a beginner-friendly AWS CI/CD pipeline for a small ECS Fargate app where the Infrastructure as Code is written in Terraform and source is GitHub. Implement a remote Terraform backend using S3 with DynamoDB locking, and a CodePipeline that runs `terraform init` and `terraform plan` on pushes, requiring a manual approval before `terraform apply`. Add drift-detection that halts deployments when `terraform plan` shows changes, and outline a safe rollback mechanism to the previous Terraform state. Include minimal Terraform snippets for the S3 backend and a pipeline step?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Adobe","Robinhood"]},{"id":"q-2191","question":"Design a cross-account CI/CD workflow for a data platform where Snowflake is the analytics warehouse and an AWS-based ETL stack runs in two accounts (prod and analytics). Implement GitOps-style deployment for Snowflake objects and Snowpipe pipelines, while deploying application updates to an EKS cluster. Include drift detection with automatic rollback, canary data tests using synthetic events, least-privilege IAM, and example IaC snippets in Terraform and SnowSQL. How would you structure this end-to-end pipeline and what are the key failure modes and mitigations?","channel":"aws-devops-pro","subChannel":"general","difficulty":"advanced","tags":["aws-devops-pro"],"companies":["DoorDash","Lyft","Snowflake"]},{"id":"q-2317","question":"You’re deploying a small REST API container to AWS ECS Fargate in us-east-1 via CodePipeline with GitHub as the source. Design a beginner-friendly observability-based rollback: add a CloudWatch Synthetics canary that exercises the endpoint every minute, emit a CanarySuccess metric, and a rollback alarm that triggers CodePipeline to rollback if canary success rate falls below 98% for 3 consecutive runs. Include minimal CloudFormation snippets and testing steps?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["NVIDIA","PayPal","Two Sigma"]},{"id":"q-2348","question":"Design a two-account AWS CI/CD pipeline for a microservices app running on ECS Fargate behind an ALB, source from GitHub, with CodePipeline and CodeBuild. Deploy to staging in Account A and production in Account B. Use CodeDeploy for blue/green deployments, store artifacts in a central S3 bucket, and implement promotion to prod only after 24h of synthetic canaries (CloudWatch Synthetics) and real-user latency/error-rate below thresholds. Include IAM roles, Secrets Manager rotation, and minimal IaC sketches?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Salesforce","Snap"]},{"id":"q-2535","question":"You're deploying a small stateless Node.js REST API to AWS ECS Fargate in us-east-1, sourced from GitHub, via a beginner-friendly CodePipeline. Provide a minimal CloudFormation snippet for the ECS cluster, task definition, service, an Application Load Balancer and target group, and an ECR repo. Enable a 10% canary deployment via CodeDeploy with a 90/10 traffic split, a /health endpoint health check, and a plan to test failover and rollback in CI?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Snap","Square"]},{"id":"q-2590","question":"Design a controlled chaos plan for a multi-region AWS setup hosting microservices on EKS and Lambda, with data stores in DynamoDB and RDS. Use AWS Fault Injection Simulator (FIS) to inject faults (latency, throttling, container crash) in one region at a time. Define blast radius, guardrails, and automatic rollback. How would you orchestrate tests with Step Functions, capture metrics (SLA latency, error rate), and ensure safe rollback with automated rollback and audit trails? Include minimal IaC references?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Databricks","Google","NVIDIA"]},{"id":"q-676","question":"You have a Node.js app deployed on EC2 instances behind an Application Load Balancer in a private VPC. You want a beginner-friendly, repeatable CI/CD pipeline that triggers on git pushes, runs tests, and safely deploys with rollback. Describe a practical setup using AWS CodePipeline, CodeBuild, and CodeDeploy (blue/green) to auto-build, test, and deploy with artifact flow in S3, and IAM roles with least privilege, including how to isolate staging vs production using separate target groups?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Cloudflare","Discord","Google"]},{"id":"q-890","question":"Design an AWS-based CI/CD pipeline for a data platform (S3 data lake, Lambda ETL, ECS) that sources from Git, runs unit tests and data quality checks (Great Expectations) on staging data, then plans/applies Terraform in staging and promotes to production with canary deployment and traffic shift. Explain IAM least-privilege, S3 artifact flow, and staging vs prod isolation?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Anthropic","Citadel","Databricks"]},{"id":"q-924","question":"Design a beginner-friendly AWS CI/CD pipeline for a serverless REST API deployed to AWS Lambda behind API Gateway. Source from GitHub; CodeBuild runs unit tests with pytest and a basic security scan with bandit; artifacts stored in S3; deployment uses SAM/CFN to Lambda with separate stages dev/stage/prod and a canary traffic shift via Lambda aliases. Explain IAM least-privilege and secure env vars (Secrets Manager or SSM)?","channel":"aws-devops-pro","subChannel":"general","difficulty":"beginner","tags":["aws-devops-pro"],"companies":["Microsoft","Square","Uber"]},{"id":"q-998","question":"In a 3-account AWS setup (Dev, SecProd, Prod), you need a Git-driven CI/CD that builds a container image in Dev, promotes IaC and app config via Terraform, and signals canary tests in Prod before full rollout. Explain cross-account CodePipeline stages, IAM roles with least privilege, Secrets Manager rotation, and canary deployment with rollback triggers. Include artifact flow and auditing considerations?","channel":"aws-devops-pro","subChannel":"general","difficulty":"intermediate","tags":["aws-devops-pro"],"companies":["Apple","Databricks"]},{"id":"q-1025","question":"In a multi-account AWS DVA data platform, streaming IoT telemetry into Kinesis Data Firehose feeding an S3 data lake with Glue catalog. Data schemas evolve and backfills are needed without full reprocessing. Design an end-to-end approach for schema evolution, idempotent writes, and backfill using Glue Schema Registry, Iceberg on S3, and partition pruning. Include data validation, DLQ, and observability?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Goldman Sachs","Instacart","Snap"]},{"id":"q-1268","question":"Design an AWS data lake pattern for multi-tenant analytics where each tenant's data sits under /tenants/{tenantId} in S3 and is exposed to Athena and QuickSight. How would you implement strict tenant isolation, least-privilege access, and automated policy-driven discovery and auditing using Lake Formation, IAM, CMKs, and SCPs? Include governance, testing, and performance considerations?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Discord","Oracle","Salesforce"]},{"id":"q-1381","question":"A serverless data ingestion pipeline: an S3 PUT triggers a Lambda that transforms JSON logs into CSV and writes to a separate bucket; failed records go to a DLQ. Explain how you implement idempotent writes, choose between DLQ mechanisms (Lambda DLQ vs SQS), and set up minimal monitoring/alerts to catch processing failures?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Discord","OpenAI"]},{"id":"q-1436","question":"In a beginner AWS DVA workflow, JSON logs arrive to S3 at s3://data-logs/raw/. Propose a minimal pipeline where a Lambda validates each record against a JSON schema, writes valid records as Parquet to s3://data-logs/processed/YYYY/MM/DD/, and routes invalid ones to a DLQ. Explain idempotent writes, choose between Lambda DLQ vs SQS, and basic monitoring setup?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["LinkedIn","Meta","Salesforce"]},{"id":"q-1549","question":"Design a secure, scalable cross-account analytics pattern for a multi-tenant data lake. Data for tenants live at /tenants/{tenantId}/ in S3 and must be queryable via Athena/QuickSight with strict isolation. Explain how Lake Formation, per-tenant LF permissions, and cross-account IAM roles control access from a central analytics account. Include encryption (CMKs), cross-account RAM/trust, schema evolution handling, and a testing plan for isolation and governance?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["LinkedIn","Meta","Slack"]},{"id":"q-1575","question":"In a multi-tenant data platform on AWS, tenants stream JSON events into a single Kinesis Data Stream; you aggregate into per-tenant Parquet files in S3 using Firehose. Design an end-to-end pipeline with idempotence, schema validation, and auditing. Include DLQ handling and isolation via Lake Formation. What are your concrete steps?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Hugging Face","Lyft","Salesforce"]},{"id":"q-1617","question":"Design a per-tenant data lake access system on AWS where billions of Parquet files in S3 are consumed by Athena/Glue; describe how you would implement tenant isolation, masking, and row-level security using Lake Formation, and how you would validate auditing and performance under burst workloads?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Discord","IBM","Robinhood"]},{"id":"q-1637","question":"In a beginner AWS DVA ingestion pipeline, a CSV file uploads to s3://data/tenant-{tenantId}/uploads/YYYY/MM/DD/file.csv triggers a Lambda that validates the header has exactly [timestamp, tenant_id, metric], converts to Parquet, and writes to s3://data/tenant-{tenantId}/processed/YYYY/MM/DD/file.parquet; design for idempotent writes (no duplicates), choose a DLQ strategy, and add minimal CloudWatch alarms for failures. How would you implement this end-to-end, and why?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Adobe","LinkedIn","Salesforce"]},{"id":"q-1706","question":"Design a beginner-friendly, end-to-end data quality check for a daily Parquet dataset stored in S3: s3://telemetry/processed/YYYY/MM/DD/. The data is produced by a Glue job from JSON input. Propose a minimal workflow (using Lambda, Glue, or Athena) that validates schema conformance, computes a day-over-day row-count delta, and emits a quality score JSON to s3://telemetry/quality-reports/YYYY/MM/DD/. Include how you would trigger, idempotency, and basic monitoring?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Amazon","Discord","DoorDash"]},{"id":"q-1772","question":"In a multi-account AWS data lake, ingested data lands in s3://lake/raw from several tenants via Kinesis Firehose into a shared account. Propose an end-to-end DVA pipeline that enforces per-tenant isolation, uses Lake Formation for access control, handles schema evolution with Parquet, and provides tenant-aware lineage and auditing. Include how you test isolation and how you monitor for cross-tenant data leakage?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Discord","Google","Snowflake"]},{"id":"q-1801","question":"Ingest JSON events from multiple payment rails into a single streaming layer, partitioned by exchange, and store per-exchange Parquet data in an Iceberg-backed table on S3. Describe the end-to-end design focusing on idempotent writes, late-arriving data, schema evolution, and auditability with time travel. Include concrete steps and trade-offs between Iceberg vs Glue Catalog?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Coinbase","Square","Stripe"]},{"id":"q-1821","question":"In a real-world data platform on AWS, streaming JSON events from many producers land in a single Kinesis Data Stream and are ingested into per-tenant Parquet files in S3. Over time the schema evolves and late data arrives. Describe an end-to-end approach that ensures idempotent writes, supports schema evolution, isolates tenants with Lake Formation, and provides reliable auditing and monitoring. Include specific services, data formats, and trade-offs?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Apple","Databricks","Goldman Sachs"]},{"id":"q-1881","question":"Design an advanced cross-region data ingestion and governance pattern on AWS for a high-volume streaming platform. In us-east-1 raw JSON events arrive via Kinesis Firehose into S3, then replicate to us-west-2 with minimal latency. Propose an architecture that guarantees exactly-once ingestion, supports schema evolution via Iceberg, and enforces per-tenant isolation with Lake Formation. Explain idempotent writes, cross-region replication, date-based partitioning, and robust monitoring (lag, drift, backfills) with minimal tenant impact?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["MongoDB","NVIDIA"]},{"id":"q-1925","question":"You're building a beginner AWS DVA pipeline: 2,000 devices emit JSON telemetry to S3 as daily JSONL; a Lambda validates lines and writes valid records as Parquet to a partitioned dataset, while invalid lines go to a DLQ. Propose a minimal approach to ensure idempotent processing, schema evolution, and late-data handling, with concrete services, data formats, and a simple monitoring plan?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Apple","Meta"]},{"id":"q-2001","question":"Design a cross-account, multi-region ingestion pipeline: devices in Account A stream telemetry via Kinesis to a central data lake in Account B. Use Firehose to write Parquet to S3, Glue Catalog for schema, and Lake Formation for access control. Ensure idempotent writes with a dedup key and conditional Put, support schema evolution via Glue Schema Registry, handle late data with watermarks, and implement observability with CloudWatch metrics and a DLQ?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["LinkedIn","Lyft","Stripe"]},{"id":"q-2064","question":"In a cross-region telemetry pipeline for millions of devices, ensure exactly-once processing, schema evolution with optional fields, and late-arriving data backfill within 24 hours. Propose a concrete architecture using AWS DVA primitives (Kinesis Data Streams, Lambda/Fargate, S3, Glue, Athena, DynamoDB), describe idempotent writes, partitioning, late data handling, monitoring, and failure plans. Include trade-offs?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Meta","Netflix","Zoom"]},{"id":"q-2113","question":"Design a minimal AWS DVA ingestion for 2,000 devices publishing JSON to a Kinesis Data Stream. Implement a Lambda dedupe layer using DynamoDB (keyed by deviceId and eventId), then write validated records as Parquet to S3 partitioned by date/deviceId. Use Glue Schema Registry for optional fields and versioning. Backfill late data within 24 hours; include a basic monitoring plan and DLQ for invalid records?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Google","Robinhood","Slack"]},{"id":"q-2202","question":"Design a real-time payments fraud detector handling 100k events/sec peak with Kinesis Data Streams. End-to-end latency <200 ms, exactly-once processing, and cross-region DR. Data schemas evolve with optional fields; late data allowed within 10 minutes backfill. Propose concrete AWS DVA architecture using KDS, Lambda/Fargate, S3, Glue, Athena, DynamoDB; explain idempotent writes, partitioning, backpressure, monitoring, and failure plans?","channel":"aws-dva","subChannel":"general","difficulty":"advanced","tags":["aws-dva"],"companies":["Stripe","Two Sigma"]},{"id":"q-2316","question":"Beginner AWS DVA task: 2,000 devices send telemetry as JSON lines to a Kinesis Data Stream, consumed by a Lambda that writes Parquet to S3 and uses a DynamoDB table for dedupe. A burst from one device creates hot shards and latency. Propose concrete steps to identify/mitigate shard hotspots, adjust partition keys, tune Lambda concurrency, implement idempotent writes, preserve schema evolution, and plan late-data backfill within 24 hours. Include basic monitoring?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Databricks","Google","Uber"]},{"id":"q-2336","question":"Beginner AWS DVA task: 2,000 devices emit JSON telemetry to a Kinesis Data Stream. Build a minimal pipeline that (a) validates and redacts PII fields (e.g., userId, deviceId) in real time, (b) writes sanitized records to S3 as Parquet partitioned by date and region, (c) preserves optional fields for schema evolution, and (d) routes and logs any failed records. Propose concrete services, data formats, and a simple monitoring plan?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Coinbase","Hugging Face","Netflix"]},{"id":"q-2412","question":"**Beginner** AWS DVA task: 10k devices emit telemetry as JSON to Kinesis in Account A. Propose a minimal cross-account pipeline (A->B) using KDS, Lambda/Fargate, S3, Glue, Athena, DynamoDB that validates schema, deduplicates, enriches with device metadata, and writes partitioned Parquet to S3. Include data lineage, cross-account access controls, late-arrival handling within 48 hours, and a simple monitoring plan. Outline trade-offs?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Amazon","Google","PayPal"]},{"id":"q-2716","question":"Beginner AWS DVA task: 2,000 devices emit telemetry as JSON lines to a Kinesis Data Stream. Build a minimal pipeline with Lambda to validate and redact PII, and to write Parquet to S3 partitioned by region and date. Use DynamoDB for dedupe (deviceId+seq). Plan schema evolution for optional firmware.version and geo, and a 7-day backfill. Include monitoring and cost considerations?","channel":"aws-dva","subChannel":"general","difficulty":"beginner","tags":["aws-dva"],"companies":["Snap","Twitter","Two Sigma"]},{"id":"q-674","question":"You're building an AWS DVA data-analytics pipeline ingesting telemetry from 2000 devices/sec via Kinesis Data Streams. A Spark/Glue path processes windows and writes Parquet to S3; aggregation state in DynamoDB. Data loss or duplicates occur during retries and outages; costs spike at peak. Design a resilient, cost-efficient approach: shard sizing, processing path, idempotent writes, error handling with DLQ, and observability. What would you implement and why?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["Microsoft","OpenAI"]},{"id":"q-916","question":"In an AWS-based DVA pipeline ingesting telemetry from 2000 devices/sec via Kinesis Data Streams, with a Spark/Glue path writing Parquet to S3 and cataloged in Glue, design an end-to-end strategy for robust schema evolution, data validation, and partitioning that minimizes reprocessing and supports backfills. Include schema registry, idempotence, DLQ, and observability?","channel":"aws-dva","subChannel":"general","difficulty":"intermediate","tags":["aws-dva"],"companies":["MongoDB","Snowflake","Tesla"]},{"id":"q-1228","question":"Design a drift-aware continuous training and multi-region deployment workflow for a fraud-detection model, using SageMaker Model Monitor, Pipelines, and Model Registry. Explain how you detect data and feature drift (PSI/KS against baselines), retrain triggers, versioning, canary validation, rollback, and how cross-region consistency is maintained?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Amazon","Bloomberg","Square"]},{"id":"q-1297","question":"You're deploying a multilingual sentiment-analysis model for a global customer-support chatbot. To minimize downtime when updating language adapters, design a SageMaker-based deployment with per-language variants, Model Registry, and canary rollouts that preserve latency SLAs and isolate traffic. Describe autoscaling, traffic routing, validation, and rollback criteria with concrete values?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Amazon","Google"]},{"id":"q-1324","question":"In a two-region SageMaker real-time inference setup for fraud detection, data drift is likely between regions and latency targets are strict. Outline a concrete canary deployment with per-region endpoint configs, Drift Detection thresholds, and Feature Store versioning/replication. Include traffic split, rollback criteria, validation plan, and monitoring strategy?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Cloudflare","LinkedIn","Twitter"]},{"id":"q-1378","question":"You’re building a SageMaker ML workflow that validates incoming data in a processing step before training. Design a minimal Processing Job using Python to check (i) all required features exist, (ii) numeric columns have ≤5% missing values, (iii) categoricals are within allowed sets. How would you trigger it from a SageMaker Pipeline, store results, and surface metrics? Include concrete resource choices?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Adobe","Instacart"]},{"id":"q-1457","question":"You're building a real-time risk-scoring model for a multi-region e-commerce platform. The model consumes streaming events, uses SageMaker Feature Store for features, and is deployed as a real-time endpoint with cross-region routing. Describe a concrete end-to-end deployment and monitoring design that ensures deterministic latency, supports feature versioning, detects data drift, and handles canary rollouts with rollback triggers; include governance, cost controls, and testing strategy?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Amazon","MongoDB"]},{"id":"q-1564","question":"You're deploying a global churn-prediction model for a SaaS app that requires real-time scoring in-app and nightly analytics reports, while complying with data residency rules. Propose an end-to-end AWS pattern using SageMaker real-time endpoints for live inference, Batch Transform for nightly analytics, per-region Feature Store isolation, and a governance framework with Model Registry versioning, drift detection, automated rollback, and cost controls. Include testing and validation steps?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Amazon","Google"]},{"id":"q-1572","question":"Design a multi-tenant, per-tenant inference service on SageMaker for a financial risk model where each client has isolated data, separate feature store namespace, and per-tenant model version, yet share a common endpoint. Describe the architecture, how you isolate data and billing, how you route requests by tenant_id, how you handle feature/version drift, and how you implement canary rollouts and rollback?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Coinbase","Hugging Face","Tesla"]},{"id":"q-1618","question":"You're deploying a predictive maintenance model to 10,000 industrial edge devices using SageMaker Edge Manager. Outline a phased OTA rollout with canary groups, offline devices, and automated rollback triggers based on telemetry. Specify packaging and signing process, versioned Edge Manifest in S3, device-grouping strategy, and how you monitor model accuracy, latency, and update success; discuss cost controls and governance?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Adobe","IBM"]},{"id":"q-1690","question":"You're deploying a SageMaker real-time endpoint for a financial risk model that ingests customer PII from EU and US users. Propose a compliant deployment pattern that enforces data locality (EU data stays EU), supports active-active regional endpoints, and provides GA-ready drift and privacy controls. Include resource layout, data flow, encryption, IAM/KMS, auditing, canaries, and cost controls?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Robinhood","Slack"]},{"id":"q-1725","question":"Design an end-to-end, multi-account, multi-region real-time fraud-detection pipeline on AWS. The model is deployed as SageMaker endpoints in two regions with cross-region routing. Provide concrete choices for endpoint configuration, SageMaker Feature Store versioning, canary rollout, drift detection, monitoring, autoscaling, governance, and cost controls; include validation steps and rollback criteria?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Amazon","Salesforce","Square"]},{"id":"q-1761","question":"You're building a privacy-preserving, multi-tenant real-time inference platform on AWS where each tenant's data must stay isolated (data locality + encryption) and costs are allocated per tenant. Propose an architecture using SageMaker Endpoints behind PrivateLink, per-tenant Feature Store versions, and a tenant-scoped Model Registry with canary rollouts. Explain how you validate latency, monitor drift, trigger retraining via SageMaker Pipelines, and enforce governance and per-tenant cost controls?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Google","Meta","Stripe"]},{"id":"q-1868","question":"Design a hybrid on-prem plus AWS inference workflow for a regulated financial service where customer data must never leave the on-prem site, but model updates are deployed from SageMaker. Propose an architecture using SageMaker Edge Manager for edge endpoints, PrivateLink to AWS backends, and a regulated Model Registry with per-tenant access controls. Include latency targets, canary rollouts to edge devices, drift detection, retraining triggers, and auditability?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Microsoft","Square","Uber"]},{"id":"q-1887","question":"Design a beginner-friendly SageMaker multi-model endpoint setup that serves two small text classifiers from a single endpoint. Route requests by a tenant_id included in the JSON input, ensuring models load on demand, monitor latency with CloudWatch, and implement a simple canary switch to compare model A vs B for a subset of tenants before full rollout. Include basic file structure, IAM roles, and a minimal test plan?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Bloomberg","Discord","IBM"]},{"id":"q-1913","question":"You’re deploying a beginner-friendly real-time sentiment moderation model for a global social app on SageMaker. End-user data must stay in one region and be routed through PrivateLink. Propose a concrete deployment: a single SageMaker endpoint behind PrivateLink, basic drift and bias checks with SageMaker Clarify, and a versioned Feature Store for user interactions, plus a simple canary and rollback plan. Include latency targets, retraining triggers, and governance basics?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Bloomberg","Snap","Uber"]},{"id":"q-2039","question":"You're deploying a beginner-friendly text classification service with SageMaker Serverless Inference for a low-traffic social app. Data must remain in-region, and endpoint credentials must rotate every 90 days. Propose a concrete setup: packaging and artifact storage, a single variant serverless endpoint, monthly drift-driven retraining triggers, a rollback plan, and basic monitoring/alerts for latency and errors?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Cloudflare","Twitter"]},{"id":"q-2052","question":"Design an automated rollout/rollback strategy for a multi-tenant real-time SageMaker inference platform with per-tenant Feature Store variants and cross-region canaries. How would you implement retraining triggers, drift validation, and per-tenant cost controls? Include concrete thresholds, duration, and rollback criteria?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Goldman Sachs","Meta","Snowflake"]},{"id":"q-2078","question":"Design a multi-tenant, privacy-preserving inference path on AWS that returns per-prediction explanations without leaking tenant data. Propose using SageMaker Endpoints behind PrivateLink, SageMaker Clarify explainability, per-tenant Feature Store versions, and a tenant-scoped Model Registry with canaries. Include drift detection, retraining triggers, and auditability; specify latency targets and rollback criteria?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["IBM","Plaid","Two Sigma"]},{"id":"q-2096","question":"Design an end-to-end deployment for a real-time anomaly detection pipeline across three data-residency regions with strict data sovereignty. Raw data must stay on-prem; only aggregated signals may traverse to AWS. Propose an architecture using SageMaker Endpoints in each region, PrivateLink to on-prem data sources, and a Global data-plane that routes latency-critical inferences. Include canary rollouts, drift detection, automated retraining, and rollback criteria?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Google","Meta","Twitter"]},{"id":"q-2151","question":"Design a data-quality and feature-drift monitoring plan for a real-time fraud-detection inference service deployed as SageMaker Endpoints across four Regions using PrivateLink. Outline detection of shifts in feature distributions before inference, retraining triggers via SageMaker Pipelines, and canary rollouts with rollback criteria. Include data provenance, access control, and governance?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Instacart","MongoDB","Zoom"]},{"id":"q-2243","question":"You're deploying a beginner-friendly real-time image classifier on a SageMaker Endpoint in a VPC with PrivateLink to a private data lake. Requirement: average latency <= 200 ms at 100 req/s, data never leaves the VPC, cost under $30/day. Propose endpoint sizing, autoscaling policy (min, max, metric, target, cooldown), a canary rollout, and a validation plan (latency samples, error rate, PrivateLink verification)?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Airbnb","Anthropic","MongoDB"]},{"id":"q-2257","question":"You're building a beginner-friendly NLP inference path that runs on edge devices with intermittent connectivity. Data never leaves the device in raw form; when online, anonymized summaries are sent to a cloud endpoint for improvement. Propose architecture using **SageMaker Edge Manager**, a **PrivateLink**-backed cloud channel, and a two-version model registry with a canary rollout. Include latency targets and a basic retraining trigger?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Google","Meta","Oracle"]},{"id":"q-2296","question":"You're running a real-time credit-risk inference across two regulated regions with strict data residency. Propose an architecture using SageMaker Endpoints behind PrivateLink, per-tenant data isolation, SageMaker Clarify for per-prediction explanations, and Model Monitor drift alerts. Outline a canary rollout for a feature, automated rollback on drift, and a practical validation plan including latency targets, explainability latency, and audit logging?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Microsoft","PayPal","Two Sigma"]},{"id":"q-2392","question":"You're evaluating two beginner-friendly, built-in classifiers on a small tabular dataset stored in S3. Propose an end-to-end workflow: data split, training with cost-conscious instance types, evaluation plan, model registry, and a private real-time endpoint behind a VPC/PrivateLink. Include a simple A/B testing plan and basic monitoring?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Discord","Lyft"]},{"id":"q-2431","question":"Design a real-time sentiment and intent inference service for a multi-tenant SaaS chat platform with strict data residency: raw data must stay in each region; aggregated signals may be shared for model improvement. Propose SageMaker Endpoints behind PrivateLink, per-tenant isolation via Feature Store, drift detection with automated retraining, and a canary rollout. Include latency targets, explainability, audit logging, and cost?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Adobe","Square"]},{"id":"q-2531","question":"You're deploying an edge-first anomaly-detection model on factory telemetry with intermittent connectivity. Design a SageMaker Edge Manager OTA rollout: per-device versioning, canary progression 5% → 20% → full, and rollback criteria based on latency and drift; support offline fallback to last-good model; ensure full audit logging. What would your concrete plan look like?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Amazon","Discord","Google"]},{"id":"q-2583","question":"You're deploying a real-time fraud-detection SageMaker Endpoint for a multi-tenant fintech app. Each tenant's data must be de-identified before inference and never logged with tenant identifiers. Propose an end-to-end architecture using PrivateLink to isolate endpoints, per-tenant KMS keys for envelope encryption, a tenant-scoped Feature Store, and a guarded model registry. Include canary rollouts, drift detection, rollback criteria, and latency targets (<=150 ms at 500 rps)?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Netflix","Plaid","Twitter"]},{"id":"q-2702","question":"You're deploying a multi-tenant real-time moderation inference service for user-generated content across regions, with per-tenant quotas, data residency, and cost controls. Propose a architecture using SageMaker Endpoints behind PrivateLink, per-tenant model versions, and a feature store for inputs; ensure region routing and tenancy isolation. Include canary rollout strategy, rollback criteria, and tenant-specific auditing?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Cloudflare","Discord"]},{"id":"q-876","question":"You're deploying a SageMaker real-time endpoint for a model expected to see bursty, unpredictable traffic. Propose a concrete autoscaling setup using AWS Application Auto Scaling that keeps latency under a target while never scaling to zero. Specify min and max instances, the metric and target value (latency or invocations), the policy type, and cooldowns; discuss validation steps?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-ml-specialty"],"companies":["Bloomberg","Lyft"]},{"id":"q-896","question":"You run a SageMaker real-time endpoint serving a risk-scoring model for payments. After a drift alert, outline a canary deployment plan using endpoint variants and the Model Registry to shift 20% of traffic to a new version while preserving latency and safety. Describe how you automate metric validation (latency, error rate, and drift), rollback triggers, and guardrails, and how you promote a stable canary to baseline?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-ml-specialty"],"companies":["Bloomberg","PayPal","Robinhood"]},{"id":"q-969","question":"In a production AWS ML pipeline, you must serve multiple fraud-detection models across two regions using a SageMaker Multi-Model Endpoint (MME). Propose a concrete deployment and autoscaling strategy that keeps p95 latency under 200 ms during peak, prevents cold starts, and optimizes memory by loading only active models. Describe per-model versioning with SageMaker Model Registry, traffic routing, canary validation, rollback triggers, cost implications, and cross-region consistency?","channel":"aws-ml-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-ml-specialty"],"companies":["Bloomberg","Netflix","Tesla"]},{"id":"q-1321","question":"In a multi-account, multi-VPC setup in us-west-2, VPCs A and B must reach external SaaS apps via private endpoints only, while all outbound internet egress is inspected by a centralized firewall in VPC C. Propose a beginner-friendly design using Transit Gateway, PrivateLink, and Route 53 Resolver DNS. Describe routing, DNS resolution, and observability?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["LinkedIn","Meta","Square"]},{"id":"q-1349","question":"Two AWS accounts across us-east-1 and us-west-2 host a data science platform with multiple VPCs. Design a scalable, secure network that minimizes cross-region data transfer, keeps east-west private, and supports easy onboarding of new accounts. Requirements: hub Transit Gateway across regions, centralized outbound egress via a shared firewall stack, private access to S3/data lake using PrivateLink with Private DNS, IPv6 dual-stack readiness, and automatic account onboarding. Explain architecture, routing, firewall policy model, cost controls, and observability?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["NVIDIA","Snowflake"]},{"id":"q-1433","question":"In a single AWS account, two VPCs exist: a private app VPC (no Internet gateway) and a separate VPC hosting the patch bucket. You want the private VPC to pull software updates from S3 in the same region without any public Internet access, and you must restrict access to only the vendor-patches bucket. Design and describe the steps to implement an S3 VPC Endpoint with an endpoint policy, route table changes, and any security considerations?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Coinbase","IBM","Robinhood"]},{"id":"q-1466","question":"Design a two-region hub-and-spoke network for a SaaS platform with 3 AWS accounts per region and 4 VPCs per region. They require private east-west traffic between services, deterministic private access to S3/DynamoDB via PrivateLink, centralized firewall egress, and easy onboarding of new tenants via an account factory. Propose a design using Transit Gateway, PrivateLink, and centralized firewalling; detail routing, tenancy isolation, scaling, and observability?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Instacart","Meta"]},{"id":"q-1469","question":"Design a two-region, multi-tenant SaaS network: per-tenant VPCs in Region A and Region B, all funneling to a centralized Transit Gateway and a regional Service VPC hosting API gateways and Network Firewall. Use PrivateLink for S3/DynamoDB, Route 53 private DNS for discovery, and an automated account factory for onboarding. Explain tenancy isolation, onboarding, cross-region failover, and observability?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["NVIDIA","Salesforce","Slack"]},{"id":"q-1545","question":"In two AWS accounts with two VPCs in a single region, internal services require private DNS resolution for api.internal and auth.internal without public endpoints. Design a simple cross‑account Route 53 Resolver setup using a central DNS hub VPC with inbound/outbound endpoints and forwarding rules, and outline onboarding for new VPCs?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Meta","Robinhood","Tesla"]},{"id":"q-1576","question":"In a two-region deployment (Region A and Region B) across two AWS accounts per region, tenants are isolated in VPCs attached to regional Transit Gateways. Design a DR-ready network that preserves private east–west traffic, ensures tenant isolation during regional outage, and enables automatic failover of control-plane services to the DR region. Propose TGW topology with a DR region, synchronized PrivateLink services, replicated firewall rules, and a testing/validation plan with telemetry?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Google","Instacart","LinkedIn"]},{"id":"q-1600","question":"Design a DR-ready, multi-region network for a SaaS platform with 3 tenants per region, each tenant in its own VPC attached to a regional Transit Gateway. Propose topology to keep private east-west traffic between tenants and control plane across regions, enable automatic failover of the control-plane to DR region using PrivateLink, support tenant onboarding, and specify telemetry and testing plans?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Google","Hashicorp"]},{"id":"q-1820","question":"Beginner-level: You operate a SaaS API in Account A, Region us-east-1. You must share the API with a partner in Account B using PrivateLink, with automatic DR failover to DR region us-west-2 and DNS-based failover via Route 53 private hosted zones. Design the minimal topology, specify resources (PrivateLink service, VPC endpoints, private DNS records) and a basic testing plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Anthropic","Lyft","Twitter"]},{"id":"q-1835","question":"Design a DR-ready topology for a SaaS platform with per-tenant VPCs attached to regional Transit Gateways in two AWS regions. Ensure private east-west traffic, tenant isolation during regional outage, and automatic control-plane failover to the DR region. Explain TGW topology, PrivateLink replication, DNS failover, cross-region storage replication, and telemetry-based validation. **Edge cases** considered?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Microsoft","NVIDIA"]},{"id":"q-1862","question":"Design a cross-region, multi-account SaaS network using AWS VPC Lattice for per-tenant service discovery and authorization across Region A and Region B. Tenants reside in isolated VPCs yet must privately reach a central API service in a hub region. Outline: tenant-to-service mapping with Lattice; inter-region PrivateLink; centralized firewall strategy; DR with DNS failover and telemetry; onboarding and revocation processes?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["MongoDB","Slack"]},{"id":"q-1908","question":"Scenario: A SaaS company hosts 5 tenants in 5 VPCs in Region us-east-1; all attach to a central Transit Gateway in a shared-services account. Each tenant must access a per-tenant PrivateLink API service while keeping strict east-west isolation. Design the topology, attach VPCs, implement per-tenant firewall rules, and provide telemetry to verify isolation and scalability. Include onboarding a new tenant?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Google","Scale Ai","Uber"]},{"id":"q-2158","question":"In a two-region deployment for a multi-tenant SaaS, each region houses 4 tenant VPCs attached to a regional Transit Gateway, plus a central Service VPC hosting API gateways and a Firewall fleet. Design a DR-ready topology that keeps tenant traffic private to the local region, ensures isolation during regional outages, and enables automatic control-plane failover to the DR region via PrivateLink. Include routing, PrivateLink replication, firewall policy synchronization, tenant onboarding, telemetry, and a DR validation plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Hugging Face","OpenAI","Oracle"]},{"id":"q-2240","question":"Beginner-level: You run a multi-tenant SaaS in AWS with 3 tenants, each in its own VPC within account A, region us-east-1. You want to onboard new tenants quickly using AWS RAM to share a central Security/Service VPC that hosts a Firewall, PrivateLink services, and a NAT. Design a minimal topology: how to share the central VPC, connect tenants via Transit Gateway, expose private access to shared services, enforce tenant isolation, and prepare telemetry and DR testing?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Lyft","Two Sigma"]},{"id":"q-2260","question":"Two-region SaaS with 4 tenant VPCs per region attached to regional TGWs. Design a DR-ready topology that keeps private east–west traffic during normal operations, ensures tenant isolation during regional outage, and enables automatic control-plane failover to the DR region. Specify: (a) TGW/VPC layout and route propagation, (b) PrivateLink service replication for the control plane API across regions, (c) per-tenant firewall/rules synchronization, (d) cross-region service discovery with Cloud Map and private DNS, (e) telemetry, validation, and failover testing plan, including rollback?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Microsoft","Salesforce","Tesla"]},{"id":"q-2385","question":"Design a DR-ready, two-region, multi-account AWS networking solution where each tenant sits in its own VPC connected to a regional Transit Gateway in Region A and Region B. Include an on-premises Direct Connect link via a DX Gateway, a central Analytics VPC reachable only through PrivateLink endpoints, automatic control-plane failover to DR region, and tenant onboarding. Outline TGW topology, PrivateLink replication, DX Gateway setup, observability, and DR validation?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Anthropic","Goldman Sachs","Google"]},{"id":"q-2463","question":"Design a DR-ready, two-region, multi-account AWS networking fabric where each tenant lives in its own VPC connected to regional Transit Gateway hubs. Include cross-region PrivateLink to analytics in a central VPC, IPv6 dual-stack readiness with NAT64/GNAT for IPv6-only workloads, and automated cross-region control-plane failover via Route 53 health checks and Lambda. Include DX gateway integration, observability, and a DR validation plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Google","Instacart","Lyft"]},{"id":"q-2481","question":"Beginner-level: In Account A, Region us-east-1, you host a private REST API exposed to tenants via PrivateLink. Tenants own their domain and want it mapped to the PrivateLink endpoint using Route 53 private hosted zones, with automatic DR failover to us-west-2. Design the minimal topology, cross-account sharing, and a testing plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["Google","Hugging Face","Tesla"]},{"id":"q-2524","question":"You operate a two-region SaaS with 20 tenants; each tenant has a dedicated VPC in Region A and Region B attached to per-region Transit Gateways. Design a DR-ready onboarding flow where a central Onboarding Service in a Service VPC uses PrivateLink to onboard/offboard tenants in both regions, with automatic DR failover of the control plane and tenant data paths staying local. Outline TGW topology, PrivateLink replication, DX connectivity, DNS failover, telemetry, and a DR validation plan?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-networking-specialty"],"companies":["Microsoft","PayPal"]},{"id":"q-2562","question":"In a two-region SaaS deployment, each tenant owns a VPC in Region A and a mirrored VPC in Region B, both attached to their region’s Transit Gateway. Tenant onboarding is automated via an account factory. Design an end-to-end onboarding workflow that creates/attaches VPCs, configures TGW attachments and route tables, and exposes per-tenant PrivateLink endpoints to a central Analytics VPC. Include DR failover of the control plane and telemetry?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Databricks","Snowflake"]},{"id":"q-2609","question":"Two-region, multi-account SaaS: every tenant has a dedicated VPC attached to a regional Transit Gateway in Region A or Region B. Design a centralized telemetry fabric that aggregates VPC Flow Logs and firewall logs to a shared data lake via Kinesis Data Firehose, with DR failover of the telemetry stack and guaranteed data integrity?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Citadel","Netflix"]},{"id":"q-2622","question":"In a two-region, multi-account SaaS deployment, each tenant resides in its own VPC connected to a shared Transit Gateway in each region. Central Analytics is exposed as a PrivateLink endpoint in a Shared Services VPC and shared to tenants via RAM. The DR region must auto-takeover control-plane using Route 53 failover and PrivateLink. Outline topology, onboarding, DNS strategy, telemetry, and a DR verification plan. How would you implement this end-to-end?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["IBM","LinkedIn","Netflix"]},{"id":"q-902","question":"A multinational bank operates 6 VPCs across 3 AWS accounts in two regions. They want to centralize north-south internet egress for security inspection, keep east-west traffic private, minimize inter-region data transfer costs, and streamline onboarding of new accounts. Propose a hub-and-spoke design using AWS Transit Gateway, Network Firewall, and VPC Endpoints. Describe routing, policy model, and observability considerations?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["Goldman Sachs","Google","Tesla"]},{"id":"q-923","question":"In a multi-account AWS environment with four accounts (prod, dev, staging, shared-services) and two regions, internal services (e.g., app.internal, db.internal) must resolve to private IPs across accounts, with public DNS unchanged for customers. Propose a scalable private DNS design using a private hosted zone and Route 53 Resolver (inbound/outbound as needed). Explain setup steps, minimal cross-account boundaries, and basic validation methods?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-networking-specialty"],"companies":["DoorDash","Hashicorp","Meta"]},{"id":"q-939","question":"An enterprise runs 4 AWS accounts across 2 regions with 10 VPCs. They require centralized north-south internet egress through a firewall inspection stack, private east-west traffic, cross-region replication, and scalable SaaS access via PrivateLink. Design an end-to-end network using Transit Gateway, VPC Endpoints, Private DNS, and optional Direct Connect/Interconnect. Describe routing, policy model, failover, and observability considerations?","channel":"aws-networking-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-networking-specialty"],"companies":["DoorDash","Microsoft","Square"]},{"id":"q-1047","question":"Design a multi-tenant analytics data lake on AWS for 3,000 tenants with strict data isolation. Propose an architecture using a shared S3 data lake with per-tenant prefixes, Lake Formation permissions, and ABAC via tenant tags. Outline governance (IAM, KMS, RAM), onboarding/offboarding automation, cost accounting, and cross-region DR. Include testing strategies to validate isolation and detect privilege escalation?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Cloudflare","Robinhood","Snowflake"]},{"id":"q-1203","question":"A web app runs in a private subnet in a VPC with no Internet access. It must fetch a config.json from a single S3 bucket owned by the same account. Design the minimal IAM role for the EC2 instance, a bucket policy, and a VPC endpoint setup to allow access via the VPC endpoint while denying access to other buckets. What steps and policies would you implement?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Cloudflare","DoorDash","Snowflake"]},{"id":"q-1318","question":"A Secrets Manager secret in account A stores DB credentials used by an app in account B. You need a Lambda in account B to rotate the secret daily. Describe the cross-account access setup, including the secret's resource policy, the Lambda's execution role, and the minimal privileges on the DB user, plus a simple test plan?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Cloudflare","Coinbase"]},{"id":"q-1390","question":"In a managed SaaS environment, each customer gets its own AWS account under a shared Organization. You require SSO-based access to their account via IAM Identity Center, provisioning per-customer roles via SCIM, with temporary credentials; ensure strict tenant isolation and prevent cross-tenant access. Describe the IAM role structure, trust/policy design, SSO mappings, and use of SCPs, tagging, and audit strategy. Include how you'd validate isolation and detect misconfigurations?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["DoorDash","Lyft","Tesla"]},{"id":"q-1402","question":"Scenario: You operate a multi-tenant SaaS on AWS. Each tenant stores its data in separate accounts with per-tenant prefixes in S3 and DynamoDB. Partners require selective data exports to their accounts for analytics. Design a scalable, secure data export and access model that guarantees tenant isolation, supports per-tenant export scopes, uses least-privilege cross-account roles, per-tenant KMS keys, and catalog governance (Lake Formation/Data Catalog). Include onboarding/offboarding, cost controls, monitoring, and testability?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Airbnb","Databricks","Netflix"]},{"id":"q-1425","question":"In a multi-tenant SaaS on AWS with per-tenant accounts and a central analytics account, design secure cross-account access so tenants can query their data without seeing others. Describe the cross-account role structure, trust policies, session controls, least-privilege, and validation/audit approach for onboarding/offboarding?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Microsoft","Salesforce","Scale Ai"]},{"id":"q-1515","question":"Scenario: A Lambda function in a VPC must securely retrieve a DB password from Secrets Manager to connect to an RDS instance. The secret is encrypted with a customer-managed CMK in the same account. Describe the exact IAM policy statements for the Lambda execution role to permit GetSecretValue and DescribeSecret on that secret, and kms:Decrypt on the CMK; include any needed resource-based or key policies and how you’d validate least privilege and rotation compatibility?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Databricks","Meta","Snowflake"]},{"id":"q-1532","question":"Design a per-tenant, region-aware analytics pipeline for a multi-account AWS SaaS where each tenant's data residency must stay in its region. Explain how you’d enforce data locality, isolate tenant data, share insights back to a central analytics account, and test DR?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Bloomberg","Google"]},{"id":"q-1566","question":"In a multi-tenant SaaS on AWS, each tenant's data sits in a separate prefix under a shared data lake. Propose a beginner-friendly, cost-conscious approach to let tenants access their own analytics dashboards through a central BI tool without cross-tenant data exposure. Specify IAM patterns, data access controls, and how you'd validate isolation and cost?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Microsoft","Robinhood"]},{"id":"q-1592","question":"An analytics SaaS runs in a single AWS region with private data stores. You must enable cross‑region DR with automated failover to a secondary region within 30 minutes, preserving tenant isolation and continuous client access. Design the architecture using Aurora Global Database, S3 cross‑region replication, Route 53 failover, per‑tenant data partitioning, and regionally isolated KMS keys. Include data‑sync, cutover automation, and a testing plan?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Databricks","Microsoft"]},{"id":"q-1778","question":"You run a multi-tenant SaaS on AWS with a central control plane and per-tenant accounts. On signup, you must provision a dedicated VPC Endpoint service and per-tenant IAM roles with least privilege for cross-account access, revocable within 60 seconds. Describe the architecture, trust policies, RAM/PrivateLink usage, SCIM provisioning, and how you would automate validation and rollback with EventBridge and Step Functions?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Cloudflare","Instacart","NVIDIA"]},{"id":"q-1796","question":"Scenario: A SaaS runs per-tenant accounts across AWS Organizations. A tenant's IAM role is suspected to be compromised, enabling sideways access to the central control plane. Propose a zero-downtime incident response workflow that (a) detects via CloudTrail/GuardDuty/EventBridge, (b) revokes the compromised role’s trust, (c) isolates the tenant from cross-account access, and (d) notifies security and customer teams within 60 seconds. Include workflow steps and rollback?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Adobe","Goldman Sachs","Google"]},{"id":"q-1826","question":"Design a self-serve cross-account restore workflow: a tenant requests data restore from a central backup vault into their own AWS account. Outline the per-tenant cross-account role with TTL, trust policy, least-privilege permissions, and how you orchestrate AWS Backup restore, validation, and rollback within SLA?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Airbnb","Apple","Oracle"]},{"id":"q-1866","question":"You operate a multi-tenant SaaS on AWS with per-tenant data stored in separate accounts. Upon signup, you need to provision an isolated workspace with a per-tenant VPC, a dedicated KMS CMK for data at rest, and strictly limited cross-account access for support tooling that can revoke within 60 seconds. Describe the end-to-end architecture, including IAM/RAM roles, SCPs, KMS key management, S3 and DynamoDB data isolation, and how you test onboarding/rollback and audits at scale?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Salesforce","Twitter"]},{"id":"q-1920","question":"You operate a multi-tenant SaaS on AWS with per-tenant accounts under an Organization. A tenant triggers suspicious activity suggesting cross-account data exfiltration. Design a zero-downtime incident response that detects via CloudTrail/EventBridge/GuardDuty, revokes trust and rotates CMKs, quarantines the tenant with SCPs, and notifies teams within 60 seconds. Include rollback steps and testing strategy?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Oracle","Tesla","Two Sigma"]},{"id":"q-1974","question":"You run a multi-tenant SaaS on AWS that provisions per-tenant data pipelines (Glue/EMR) on signup. Tenants upload ETL scripts; you must sandbox execution with strict runtime and cost caps, guarantee per-tenant data isolation, and enable revocation within 60 seconds. Describe the control plane, roles, policies, data isolation, and rollback/audit strategy?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Salesforce","Scale Ai"]},{"id":"q-2056","question":"Scenario: A multi-tenant SaaS stores each customer's data in a regional S3 bucket owned by that customer, with your service ingesting data via cross-account roles. On signup you must provision a per-tenant IAM role, per-tenant KMS CMK, and per-tenant Lake Formation permissions, with revocation within 60 seconds if policy is violated. Describe the architecture, trust policies, and automation (EventBridge, Step Functions, RAM) to onboard, rotate keys, and revoke access?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Coinbase","MongoDB","Square"]},{"id":"q-2149","question":"In a two-account setup (dev and prod), how would you grant a developer permission to deploy to prod S3 and prod ECR via a cross-account role with MFA, using minimal permissions and fast revocation? Describe the trust policy, role inline permissions, and a basic CI check to verify access?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Hugging Face","LinkedIn","NVIDIA"]},{"id":"q-2177","question":"Design a scalable, private cross-tenant analytics model for a SaaS where each tenant has an isolated AWS account. Propose how to share aggregated data to a central analytics account using cross-account IAM roles, Lake Formation permissions, and KMS; outline onboarding/offboarding with EventBridge and Step Functions, data access revocation within 60 seconds, auditing, and failure recovery?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Coinbase","Instacart","Zoom"]},{"id":"q-2269","question":"You operate a SaaS on AWS with 1,000 tenant accounts under an Organization. Tenants demand data residency in their own region and private cross-region analytics access to a centralized data lake. Design an end-to-end solution that (1) isolates per-tenant data in region-scoped S3 buckets with per-tenant KMS keys, (2) enforces Lake Formation permissions per tenant, (3) connects privately to regional hubs via PrivateLink, (4) supports cross-region replication with controlled consent, and (5) provides a 60-second rollback workflow using EventBridge and Step Functions, plus automated tests and drift checks?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Airbnb","Databricks","Tesla"]},{"id":"q-2284","question":"Design an automated containment workflow for a 50-account AWS environment. When GuardDuty flags a compromised EC2 in any development account, automatically isolate the host within 60 seconds by detaching the ENI, revoking outbound traffic via Security Groups, and exporting forensics data to the central security account. Minimize production impact and ensure a rollback path with false-positive handling. Describe architecture, cross-account IAM roles, EventBridge/Step Functions flow, and testing strategy?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Hashicorp","Meta"]},{"id":"q-2381","question":"Design a scalable data-access pattern for a multi-tenant SaaS on AWS where each tenant’s data must be privately accessible by a third-party ML partner (Databricks, Snowflake, or OpenAI) via private connectivity, without data leakage or public Internet. Describe per-tenant PrivateLink endpoints, cross-account RAM shares, strict IAM trusts, governance via Lake Formation or bucket policies, 60-second revocation, and auditing?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Databricks","OpenAI","Snowflake"]},{"id":"q-2567","question":"You run a small web app in a single AWS region with minimal downtime constraints. Propose a beginner-friendly disaster recovery plan that uses S3 for backups, RDS automated backups or DynamoDB backups, Route 53 failover routing to a secondary region, and a one-click failover test. Describe how you'd validate RTO/RPO and the cost implications, with minimal operational effort?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["NVIDIA","Tesla"]},{"id":"q-2629","question":"Scenario: A SaaS running across two AWS regions must ensure tenant data residency by storing each tenant's data only in their assigned region and enabling revocation of access within 60 seconds; design an end-to-end architecture using AWS Organizations, PrivateLink Endpoint Services, per-tenant IAM roles with SCPs, DynamoDB Local/Global Tables and S3 region replication, and an automated revocation workflow via Step Functions and EventBridge, including failure modes and DR/cost tradeoffs?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Cloudflare","Microsoft"]},{"id":"q-672","question":"Design a scalable, cost-aware data ingestion and processing pipeline on AWS for 1 TB/day of log data arriving from multiple on-prem and cloud sources. The pipeline must deliver raw data immutably for 90 days, provide near-real-time enrichment within 5 seconds, and support cross-region failover. Specify services, data flows, constraints, and trade-offs?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Hashicorp","IBM"]},{"id":"q-930","question":"An existing web service runs on EC2 in a single VPC with an ALB. Traffic surges cause latency spikes and occasional outages during AZ failures. Propose a beginner-friendly, cost-conscious HA setup using an Application Load Balancer, Auto Scaling across at least two AZs, and a relational database option. Include networking, health checks, a scaling policy, and a basic DB deployment choice with trade-offs. What would you implement first and why?","channel":"aws-saa","subChannel":"general","difficulty":"beginner","tags":["aws-saa"],"companies":["Amazon","MongoDB"]},{"id":"q-944","question":"Design a multi-tenant SaaS on AWS that ingests telemetry from thousands of tenants daily, enforces strict data isolation, and provides cross-region DR. Outline data partitioning, access control, encryption strategy, immutable logging, and DR failover. Include services, trade-offs, and how you test isolation?","channel":"aws-saa","subChannel":"general","difficulty":"advanced","tags":["aws-saa"],"companies":["Anthropic","Meta","Snap"]},{"id":"q-989","question":"Design a centralized security telemetry pipeline for 1,000 AWS accounts to detect anomalies in near real-time. Ingest VPC Flow Logs, CloudTrail, and GuardDuty findings into a central security account using AWS Organizations, implement least-privilege cross-account roles, normalize data into a common schema in S3, partition by source account and region, apply Lake Formation permissions, and set up alerting with EventBridge and security findings. Include scaling, cost, DR, and testing?","channel":"aws-saa","subChannel":"general","difficulty":"intermediate","tags":["aws-saa"],"companies":["Google","Plaid"]},{"id":"q-1005","question":"You operate SAP S/4HANA on AWS with analytics in Snowflake and must implement a data masking/tokenization pipeline so analytics do not expose PII. Design end-to-end data flow, masking rules by field, latency (<5 minutes), and governance using KMS/IAM. Include auditing, rollback, and failover considerations?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Oracle","Snap","Snowflake"]},{"id":"q-1011","question":"How would you implement an automated cross-region DR for SAP HANA on AWS? Use SAP HANA System Replication with Region A as primary and Region B as hot standby, orchestrated by AWS Step Functions; back up to EBS/S3 with Data Lifecycle Manager and cross-region KMS keys; ensure automated DR tests, rollback playbooks, and meet RPO <5 minutes, RTO <15 minutes?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["PayPal","Uber"]},{"id":"q-1059","question":"Design an advanced SAP S/4HANA on AWS architecture using SAP HANA MDC on EC2 across 3 AZs, with analytics separated into a data lake. Propose a rolling OS/kernel/SAP patching and upgrade strategy that preserves near-zero downtime, enables automated cross-region DR testing, and guarantees RPO <2 minutes and RTO <5 minutes; specify automation, services, failure modes, and rollback?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Airbnb","Hashicorp","Stripe"]},{"id":"q-1179","question":"Design an automated, auditable patching workflow for SAP S/4HANA on EC2 across multiple AWS accounts and regions. Use AWS Systems Manager Patch Manager to patch OS and SAP kernel updates with rolling upgrades, pre/post checks, and automated rollback if SLA drift occurs. Include governance, approvals, testing, and validation of success before go-live?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Apple","Citadel","Plaid"]},{"id":"q-1186","question":"Design a multi-account SAP S/4HANA on AWS with SAP HANA MDC across 3 AZs and a cross-region DR setup. Route SAP system and security logs to a centralized, cross-account S3 data lake with Object Lock (WORM) and cross-region replication. Use AWS Glue/Data Catalog and Lake Formation for lineage and access control; enforce least privilege with SCPs. Automate DR tests and integrity checks?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Instacart","Lyft","Plaid"]},{"id":"q-1328","question":"Design a cross-account SAP S/4HANA MDC deployment where production data sits in Account A and an analytics MDC mirrors in Account B. Use near real-time CDC data replication (e.g., DMS) into a centralized S3 data lake and Glue catalog, with strict IAM governance, private networking, and automated drift checks. Include rollback and DR testing plan; target RPO <60s, RTO <5m. How would you implement it?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Microsoft","Netflix"]},{"id":"q-1404","question":"Design an advanced SAP S/4HANA on AWS architecture for a global user base focused on latency and data residency. Use SAP HANA MDC on EC2 across 2 AZs in Region A with a cross-region MDC replica in Region B for DR; enforce EU data residency with local S3 buckets and KMS keys; implement IAM/SCP least privilege; outline automated DR tests, failover/failback, and rollback?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Bloomberg","Discord","Slack"]},{"id":"q-1434","question":"Scenario: You operate SAP S/4HANA on SAP HANA MDC on EC2 across two AWS regions (Region A as primary, Region B as hot standby). Design an automated DR test plan that uses SAP HANA System Replication, AWS Step Functions, and AWS Fault Injection Simulator to perform regular, reportable failover tests without impacting production. Define test cadence, success criteria (RPO <2m, RTO <10m), rollback procedures, and how to prove continuity to stakeholders with logs, metrics, and cross-region backups?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Google","MongoDB"]},{"id":"q-1494","question":"You manage SAP HANA MDC on EC2 in a single AWS region. Design a beginner-friendly, automated backup strategy using only AWS native services (EBS snapshots, S3, AWS Backup, KMS) with cross-region replication to a warm standby. Include backup frequency, retention, encryption, and a reproducible restore test plan with objective to meet RPO 15 minutes and RTO 1 hour?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Adobe","Hashicorp"]},{"id":"q-1559","question":"In a single-region AWS SAP deployment (SAP NetWeaver + SAP HANA on EC2), propose a beginner-friendly automated health-check using only AWS native services. The check runs daily, verifies SAP instance status, HANA availability, and key OS metrics, writes a JSON report to S3, and triggers an SNS alert on any failure. Include data flow, AWS services used, and a minimal script snippet the Lambda would run via SSM Run Command?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Adobe","Airbnb","Hugging Face"]},{"id":"q-1606","question":"In a single-region SAP HANA on EC2 deployment, design a beginner-friendly incident-response automation using only AWS-native services to detect an outage within 5 minutes, isolate the SAP subnet, perform a warm-standby failover, and notify stakeholders via SNS. Include data flow, services used, and a minimal Lambda snippet that checks SAP status via SSM Run Command?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Hashicorp","Two Sigma"]},{"id":"q-1709","question":"You operate SAP HANA on EC2 with a parallel SAP ABAP stack in a multi-AZ VPC. Propose a concrete AWS-native auto-scaling and DR plan to handle quarterly peak workloads with zero SAP downtime. Include (1) app-tier scaling strategy (ASG, launch templates), (2) HANA scale-out readiness and data protection, (3) storage IOPS and sizing choices, (4) cross-region DR with replication and failover testing cadence, (5) rollback criteria and metrics?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Discord","Google","PayPal"]},{"id":"q-1779","question":"In an SAP HANA on EC2 landscape spanning three AWS environments (dev, stage, prod), design a policy-driven security hardening and continuous compliance workflow using only AWS-native services. Include OS baselines, SAP user and kernel parameters, and file permissions; enable drift detection, automated remediation via SSM Run Command, and audit reports to S3 + Glue catalog. Provide architecture and a runnable Lambda snippet to enforce the baseline?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["IBM","Meta","Snowflake"]},{"id":"q-1814","question":"In a two-region AWS SAP HANA deployment (NetWeaver + S/4HANA on EC2), design an observability-driven auto-remediation plan to detect SAP performance anomalies (e.g., high ABAP work process waits, HANA stalls, long SAP responses) using only AWS-native services. Define metrics, thresholds, data flow, remediation actions (auto-scale, restart SAP components via SSM Run Command), and how you validate success?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Adobe","Snowflake"]},{"id":"q-1838","question":"Design a beginner-friendly cost governance workflow for SAP HANA on EC2 in a single AWS region that uses only native AWS services: enforce SAP-resource tagging with Environment and Owner, set a monthly spend budget, and trigger an SNS alert if forecasted spend exceeds 80% of the budget. Describe data flow, services used, and provide a minimal CloudFormation snippet to create the budget and a Lambda skeleton to respond to forecast notifications?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Bloomberg","MongoDB"]},{"id":"q-1905","question":"In a multi-region SAP HANA on EC2 deployment, design an automated upgrade path for SAP NetWeaver stack from 7.50 to 7.52 with zero downtime using only AWS-native services. Include blue/green deployment via CodePipeline/CodeDeploy and Route 53, SSM Automation for upgrade steps, rollback triggers, and post-cutover SAP health verification?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Airbnb","Anthropic","OpenAI"]},{"id":"q-1942","question":"You manage an SAP HANA MDC deployment on EC2 across two AWS regions and two accounts. Design a policy-driven, automated patch rollout for SAP kernel and HANA patches using only AWS-native tools, with zero-downtime, predefined maintenance windows, and automated rollback. Include staging, validation, auditing, and a runnable Step Functions workflow that triggers SSM Run Command patches and reports success?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Hashicorp","Scale Ai","Snowflake"]},{"id":"q-2067","question":"Design a daily cost-performance monitoring solution for SAP HANA on EC2 using AWS-native services that alerts when savings potential exceeds 20% or utilization thresholds are breached for consecutive days?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Anthropic","Apple","Oracle"]},{"id":"q-2071","question":"Scenario: You operate a single-region SAP NetWeaver + HANA on EC2. A quarterly kernel upgrade and patch cycle must be performed with minimal downtime. Design a blue/green rollout using only AWS-native services: two identical stacks (Blue/Green) across the same VPC/AZs, Route 53 weighted routing, SSM automation for kernel patching, CloudFormation templates for reproducibility, and automated health checks (SAP status, HANA availability, sample ABAP batch). Include rollback steps and metrics?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Meta","Microsoft"]},{"id":"q-2148","question":"Design a beginner-friendly, AWS-native license-compliance workflow for SAP HANA on EC2 in a single region. Use AWS License Manager to model SAP licenses, collect SAP and OS license data via a daily SSM Run Command, compare against entitlements, and publish a JSON report to S3 with an SNS alert on non-compliance. Include data flow, services, and a minimal Lambda snippet to fetch License Manager data and SAP status?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Adobe","Microsoft","Snowflake"]},{"id":"q-2230","question":"Design a beginner-friendly, AWS-native transport validation workflow for SAP NetWeaver on EC2 in a single region. Outline a minimal CI/CD pipeline (CodeCommit/CodePipeline) that builds transports, stores artifacts in S3, uses a Lambda to validate transport naming, sequence, and signature, requires a manual approval before import to QA, and triggers a CloudWatch alert on failure?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["LinkedIn","Meta","NVIDIA"]},{"id":"q-2439","question":"Design a multi-tenant backup strategy for SAP HANA on EC2 across AWS accounts and regions, ensuring tenant data isolation. Outline AWS Backup usage with per-tenant vaults, cross-account roles, and cross-region replication to S3 encrypted with KMS CMKs. Define per-tenant retention, automated restore tests via Lambda, and a checksum verification step to confirm data integrity. Address RPO/RTO?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Apple","Coinbase"]},{"id":"q-2446","question":"Design an AWS-native cross-region SAP HANA MDC deployment with strict network segmentation. Explain how to implement per-SAP-module security groups, Transit Gateway topology, and DR failover readiness using only AWS-native services. Include trade-offs and a minimal IaC snippet to enforce module boundaries?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["IBM","Netflix","Tesla"]},{"id":"q-2492","question":"In an SAP HANA MDC cluster on AWS EC2 spanning three AZs, design a zero-downtime patching workflow for SAP kernel, HANA, and OS, using rolling upgrades with one node online as primary while others are secondary. Include prechecks, quiesce/resume steps, data-consistency validation, automated rollback, and monitoring with CloudWatch/SNS. Provide an IaC outline and a minimal Lambda snippet to trigger prechecks and start patching?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Citadel","Netflix","Tesla"]},{"id":"q-2527","question":"In a two-region AWS SAP HANA MDC deployment (prod in us-east-1, DR in eu-west-1), design a zero-downtime module upgrade strategy using only AWS-native services. Include per-module traffic isolation, rolling upgrades, cross-region replication, automated DR tests, and a minimal IaC snippet to enforce the module boundaries?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Lyft","Snowflake","Tesla"]},{"id":"q-2554","question":"In a single-region SAP HANA on EC2 deployment, design a beginner-friendly backup/restore workflow using only AWS-native services. Use AWS Backup to schedule daily SAP HANA backups, store backups in S3 with versioning, verify integrity via a Lambda triggered over SSM that runs an SAP backup verification command, and publish a JSON report to S3 with an SNS alert on failures. Include data flow and a minimal Lambda snippet?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Apple","Lyft","Oracle"]},{"id":"q-2578","question":"Design a blue/green upgrade workflow for SAP NetWeaver + SAP HANA on EC2 across prod and a DR AWS account using only AWS-native services. Include traffic cutover with an ALB, ABAP transport isolation, RFC connectivity, <2 minutes downtime, and rollback. Provide data flow and a minimal IaC snippet to implement the switchover?","channel":"aws-sap","subChannel":"general","difficulty":"intermediate","tags":["aws-sap"],"companies":["Citadel","Salesforce","Uber"]},{"id":"q-2689","question":"In an AWS SAP deployment (SAP HANA on EC2 + SAP NetWeaver), design an advanced observability and cost-optimization strategy that ties SAP-layer metrics to AWS resource usage. Include data flow, services used, alerting, and a minimal snippet showing how to emit a custom CloudWatch metric from a Lambda that pings SAP and reports latency?","channel":"aws-sap","subChannel":"general","difficulty":"advanced","tags":["aws-sap"],"companies":["Bloomberg","Databricks","Snowflake"]},{"id":"q-673","question":"You manage a small SAP NetWeaver footprint on AWS using EC2 for the app tier and a separate DB tier on HANA. Describe a practical single-region HA and backup plan to meet an RPO of 15 minutes and an RTO of 60 minutes. Include chosen services, EC2 sizing approach, storage strategy (EBS/S3), backup schedule, and a cost-conscious trade-off you’d consider?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Microsoft","Robinhood"]},{"id":"q-900","question":"Scenario: You manage a single-region SAP NetWeaver deployment on AWS with a separate SAP HANA DB on EC2. You need a beginner-friendly, cost-conscious maintenance workflow that automates OS patching and SAP kernel upgrades using only AWS native services, with minimal downtime. Outline the steps, services, and a sample two-hour weekly maintenance window, including how you validate success and perform rollback?","channel":"aws-sap","subChannel":"general","difficulty":"beginner","tags":["aws-sap"],"companies":["Cloudflare","Hashicorp","Zoom"]},{"id":"q-1302","question":"A data pipeline in Account A must read daily compressed data from a bucket in Account B, with no Internet access and least privilege. Propose a practical cross-account access pattern using a role in Account B that can be assumed by a service role in Account A, a bucket policy, and a CMK policy. Include how you would validate that only authorized principals can access and that encryption keys are used and rotated?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Hashicorp","NVIDIA","Uber"]},{"id":"q-1345","question":"In a multi-account setup, a Lambda in Account B must read a secure string from Parameter Store in Account A, encrypted with a CMK in Account A. The environment has no Internet access. Outline a concrete cross-account pattern: (1) IAM role trust, (2) Parameter Store policy, (3) CMK policy, and (4) plan to verify access and CMK rotation?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Citadel","Cloudflare"]},{"id":"q-1428","question":"Scenario: A SaaS app serves 100 tenants. Each tenant stores data in its own S3 bucket encrypted with a per-tenant CMK (SSE-KMS). The backend uses a per-tenant role to access both the bucket and key. Design the IAM roles, bucket policies, and KMS key policies to enforce strict per-tenant isolation, automatic CMK rotation, and auditable access. Include how you’d validate no cross-tenant access and rotate status?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Google","Hugging Face","Snowflake"]},{"id":"q-1574","question":"In a multi-account, multi-region data lake, design a zero-trust cross-account analytics boundary that ensures only approved principals can read S3 data via private endpoints, while data at rest is encrypted with a CMK in a centralized security account that rotates automatically. Include CMK and bucket policies, Lake Formation permissions, VPC endpoints/PrivateLink use, cross-account roles, and how you would validate no privilege creep and immutable logs?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Lyft","Two Sigma"]},{"id":"q-1593","question":"In a cross-account ETL job: a service in Account A needs to copy only files tagged ETL=Allowed from a bucket in Account B, over a private VPC endpoint, with data at rest encrypted by a central CMK. Design the least-privilege pattern: trust policy for a cross-account role, bucket and KMS policies, and how you would enforce per-object tag-based access. Include how you’d validate no privilege creep and that logs are immutable?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Google","NVIDIA","Uber"]},{"id":"q-1612","question":"In a multi-account setup, a worker in Account A must read API credentials stored in Secrets Manager in Account B, with automatic rotation and centralized auditing. Propose a concrete pattern using a cross-account Secrets Manager resource policy, an IAM role in Account A, a CMK in a Security account with rotation, and a least-privilege policy for the worker. Describe validation steps to detect privilege creep and ensure immutable logs?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Bloomberg","Microsoft","Uber"]},{"id":"q-1792","question":"Scenario: A data processor in Account A must read encrypted inputs from bucket 'data-inbound' in Account B, decrypt with a CMK in the Security account, and write results to 'data-out' in Account B. No Internet access. Propose exact cross-account roles, trust policies, bucket policies, and KMS key policy to grant least privilege; show how you would test that only the intended principals can access, and how to ensure immutable auditing logs?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Bloomberg","Discord","Microsoft"]},{"id":"q-1870","question":"Design a federation pattern for querying a shared S3 data lake from multiple AWS accounts using Athena workgroups, cross-account IAM roles, and Lake Formation with ABAC-based dynamic data masking. Ensure least privilege, private connectivity (VPC endpoints), central CMK encryption with rotation in the Security account, immutable audit logs, and automated drift/privilege creep detection. Include IAM roles, bucket policies, Lake Formation permissions, KMS key policy, and a test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Databricks","Lyft","Meta"]},{"id":"q-1897","question":"Scenario: An ETL job in Account A reads from a shared data lake in Account B and writes results to a bucket in Account C. Design an ephemeral, least-privilege cross-account access pattern using STS AssumeRole, ABAC tags, Lake Formation permissions, VPC endpoints, and a centralized CMK in the Security account with rotation. Include trust, session policies, encryption, audit, and a test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Databricks","PayPal"]},{"id":"q-1929","question":"You need to share a subset of an S3 data lake between two AWS accounts. Design a least-privilege cross-account pattern that grants read access to specific prefixes via a dedicated role, enforces access only over S3 VPC Endpoints, and logs access with immutable logs; include bucket policy, IAM role trust, and a test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Apple","Cloudflare","Hugging Face"]},{"id":"q-2012","question":"Design a secure cross-account data-processing pattern for a multi-account AWS setup where data flows from a private S3 data lake in Account A to Account B for processing, with a central Security account handling encryption keys. Include ABAC-based access via STS AssumeRole, Lake Formation permissions, VPC endpoints, and immutable logs. Consider an EventBridge trigger from a partner account and private connectivity, with least privilege and automatic key rotation?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Coinbase","Microsoft","MongoDB"]},{"id":"q-2043","question":"A Lambda-backed API in Account A reads DB credentials from AWS Secrets Manager. Design a minimal, auditable approach to rotate that secret automatically with zero downtime. Include cross-account access if the DB is in another account, the required IAM roles/policies, and a concrete test plan to validate rotation without API downtime?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Hashicorp","Oracle","Stripe"]},{"id":"q-2068","question":"Design a beginner-friendly secure CI/CD pipeline for deploying Lambda functions across three AWS accounts (dev, stage, prod) using GitHub Actions. Enforce least privilege with per-environment execution roles, separate cross-account deployment roles, and approval gates. Include IAM trust policies, role permissions boundaries, artifact handling, Secrets rotation, and a basic test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Adobe","Microsoft","Tesla"]},{"id":"q-2112","question":"Design an on-demand data science workspace using AWS SageMaker notebooks for researchers in a multi-account data lake. Enforce ABAC with data-tag constraints, Lake Formation permissions, per-notebook IAM roles, and cross-account trusts. Require private connectivity via VPC endpoints, centralized CMK rotation, immutable audit logs, and automated drift/privilege creep detection. Provide IAM roles, KMS, Lake Formation, and a test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Adobe","Instacart","LinkedIn"]},{"id":"q-2233","question":"A new project runs a REST API in an AWS account using API Gateway + Lambda, which must be consumed by a partner in a different AWS account. The partner should be able to read a specific S3 data bucket and invoke one Lambda function, with no other access. Design a minimal, auditable cross-account access pattern using a dedicated role in your account that the partner can assume. Include the trust policy, the permissions policy, least-privilege scoping, logging via CloudTrail, and a simple test plan to validate it?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Google","IBM","Robinhood"]},{"id":"q-2270","question":"Design a beginner-friendly cross-account access pattern: analysts in accounts B and C need time-limited access to s3://data-lake/teams/alpha/ for a day. Use a central role in account A named DataLakeAnalyst, MFA, a 1‑day STS session, and a session policy granting s3:GetObject and s3:ListBucket on that prefix plus kms:Decrypt for the data key. What are the roles, policies, and checks?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Databricks","Lyft","Meta"]},{"id":"q-2380","question":"Design a beginner-friendly security baseline for a web app using API Gateway, Lambda, S3, and DynamoDB in a single AWS account. Ensure private connectivity via VPC endpoints, enforce least-privilege IAM roles for compute, rotate DB credentials with Secrets Manager, enable at-rest encryption, and provide an auditable test plan with immutable logs?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Cloudflare","MongoDB","Snowflake"]},{"id":"q-2417","question":"Scenario: A production S3 bucket 'company-reports-prod' stores customer reports. A trusted vendor in a separate AWS account needs temporary, read-only access to objects under the 2025/Q4 prefix for 48 hours. Design a minimal cross-account access pattern using STS AssumeRole, a restricted trust policy, a least-privilege role policy, and a bucket policy. Include revocation, logging, and a basic test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Apple","Discord","PayPal"]},{"id":"q-2461","question":"You manage a web app with API Gateway + Lambda uploading files to a private S3 bucket. Design a secure, beginner-friendly flow using pre-signed URLs (short TTL) to allow client uploads without public S3 access. Include IAM roles, bucket policy restricting the signer, SSE-KMS, VPC endpoints, and an automated test plan that validates access control and immutable logs. What steps would you implement?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Apple","Databricks","Uber"]},{"id":"q-2474","question":"Scenario: You manage a shared data lake across accounts A, B, and C. A partner account D needs to run quarterly Athena queries on a restricted S3 subset, with **private connectivity**, CMK encryption in a central Security account with **monthly rotation**, and immutable audit logs. Design cross-account access (roles, policies, Lake Formation permissions, KMS key policy), ABAC tags, VPC endpoints, and an automated test plan to detect drift and privilege creep; include revocation and incident response steps?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Bloomberg","Robinhood","Twitter"]},{"id":"q-2498","question":"Design a beginner-friendly secure image upload workflow in AWS: a Lambda generates a presigned PUT URL for an S3 bucket. Enforce Content-Type image/* and max 5 MB on upload, bucket policy restricts PutObject to that Lambda with SSE-KMS encryption, and rotate CMK automatically. Send CloudTrail logs to a separate audit bucket with immutable Object Lock. Include a simple test plan and IAM policy sketches?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["Discord","Goldman Sachs","Snowflake"]},{"id":"q-2633","question":"In a 3-account AWS Organization SaaS setup, require every new S3 bucket to: (1) default-encrypt with a CMK in the Security account, (2) Block Public Access, (3) bucket policy enforcing x-amz-server-side-encryption and TLS 1.2, and (4) allow access only from VPC endpoints. Propose an automated enforcement pattern using SCPs, IAM boundaries, KMS key policy, and a CloudFormation/CDK template plus a comprehensive test plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Cloudflare","DoorDash","Lyft"]},{"id":"q-2654","question":"You run a cross-account data-processor: a Lambda in Account A reads a DB credential stored in Secrets Manager in Account S and writes results to a central S3 bucket in Account A. Design a beginner-friendly pattern using a cross-account IAM role, a resource-based policy on the secret, a CMK in the security account, and private connectivity via VPC endpoints. Include a concrete test plan to verify least privilege and immutable logs. What steps would you implement?","channel":"aws-security-specialty","subChannel":"general","difficulty":"beginner","tags":["aws-security-specialty"],"companies":["DoorDash","Google","Oracle"]},{"id":"q-853","question":"In a multi-account AWS setup, centralize a logs bucket in Account A that stores CloudTrail and VPC Flow Logs from Accounts B and C. All objects must be encrypted at rest with a CMK in Account A that rotates automatically. Design the KMS key policy, bucket policy, and cross-account IAM roles to allow Account B/C services to encrypt, while preventing decryption except via a centralized IAM role in Account A. Include how you would validate encryption, rotation status, and auditability?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["Citadel","IBM"]},{"id":"q-867","question":"Design cross-account data sharing using Lake Formation and S3 that lets Account B write to a shared data lake in Account A and Account C read it via a service role, with a CMK in A that rotates automatically and multi-region, tamper-evident audit logs. Provide IAM/Lake Formation/KMS policies, cross-account trust, and a validation plan?","channel":"aws-security-specialty","subChannel":"general","difficulty":"advanced","tags":["aws-security-specialty"],"companies":["Bloomberg","MongoDB","Snap"]},{"id":"q-938","question":"In a two-account data lake, enforce immutable data retention with S3 Object Lock across regions. Buckets in Account A replicate to Account B via CRR. Design the retention policy (COMPLIANCE vs GOVERNANCE), enable automatic CMK rotation, and set cross-account IAM trust for replication. Explain how you'd validate retention, replication integrity, and rotation status?","channel":"aws-security-specialty","subChannel":"general","difficulty":"intermediate","tags":["aws-security-specialty"],"companies":["DoorDash","MongoDB","Scale Ai"]},{"id":"q-1004","question":"Scenario: A serverless API stack (API Gateway, Lambda, DynamoDB, S3, CloudFront) runs in us-east-1 with a DR region eu-west-1. Propose an automated DR plan using AWS Global Accelerator and Route 53 health checks to failover within 15 minutes. Include data replication choices, Lambda versioning strategy, S3 replication mode, and a safe rollback/verification approach that avoids production impact during tests?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Databricks","Google"]},{"id":"q-1291","question":"In a 12-account AWS Organization, you must implement near real-time central audit logging for VPC Flow Logs, Lambda logs, and RDS logs in a dedicated Logging account. Design the end-to-end mechanism to ship logs from all member accounts to the central account, ensuring secure cross-account access, encryption, and resilience across Regions. What is your approach?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["MongoDB","Salesforce","Scale Ai"]},{"id":"q-1481","question":"In a multi-account AWS Organization for a global SaaS app, mandate data residency: S3 buckets and DynamoDB tables used by customer data must not store or replicate data outside the designated region per account. Design an automated governance solution using SCPs, AWS Config, IAM Roles, EventBridge, and Lambda to enforce this, with testing and alerting?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Google","IBM","Snowflake"]},{"id":"q-1536","question":"Design a DR orchestration for a global app with primary S3 (with Object Lock) and DynamoDB in us-east-1 and a DR site in us-west-2. Require 15 min RTO and 5 min RPO, data residency, automated backups via AWS Backup, cross-region replication that respects residency, and Route 53 failover. Describe a Step Functions workflow to automate failover, validation, and rollback without production impact, with testing cadence and success criteria?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Cloudflare","Hashicorp","Tesla"]},{"id":"q-1542","question":"In a three-account AWS Organization (prod, staging, dev) for a global SaaS app, enforce cost governance by requiring that all new resources carry the tags Owner and Environment in prod, with automated remediation for noncompliant resources and separate budgets/alerts per account. Design an end-to-end process using SCPs, AWS Config rules, EventBridge, Lambda, and AWS Budgets to detect, remediate, and alert. Include testing steps?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Citadel","DoorDash","Stripe"]},{"id":"q-1591","question":"In a multi-account AWS Organization hosting a global service, a misconfigured VPC peering or transit gateway leaks a route to the internet, risking data exposure and egress costs. Design an automated, end-to-end remediation and validation workflow (detection, isolation, rollback) using AWS Config and Config Rules, EventBridge, Lambda, IAM roles, and CloudWatch alarms, plus a controlled traffic test before rollback. What steps and artifacts would you implement?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Amazon","Instacart","Netflix"]},{"id":"q-1634","question":"How would you implement a guardrail so every new EC2 instance in a single AWS account for a mobile backend launches only in Prod-Subnet, carries Owner and Environment tags, and uses IAM role MobileBackendRole, with automated remediation and alerts via AWS Config, Lambda, EventBridge, and SNS while providing a testing plan?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Cloudflare","DoorDash"]},{"id":"q-1672","question":"Across a three-account AWS Org (prod, staging, dev) hosting a Databricks-powered data lake on S3, design an automated end-to-end remediation to ensure all compute resources access S3 only via VPC Endpoints, enforce private DNS, and block public S3 access. Include SCPs, AWS Config rules, EventBridge, Lambda, and GuardDuty findings with testing steps and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Databricks","Hashicorp"]},{"id":"q-1765","question":"Within a multi-account AWS environment across six regions hosting a critical financial service, implement automated detection and remediation of IAM role trust policy misconfigurations that could expose cross-account access. Use AWS Config, Lambda, EventBridge, IAM Access Analyzer, and SCPs in a central Governance account. Outline controls, testing, and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Apple","Goldman Sachs","Lyft"]},{"id":"q-1886","question":"In a 3-account AWS Organization (prod, staging, dev) for a global SaaS app, enforce a global IAM password policy across all accounts: min length 14, require uppercase, lowercase, number, symbol, password age 90 days, and forbid reuse of last 5. Design an end-to-end automation using SCPs, an AWS Config custom rule, EventBridge, Lambda, and AWS Budgets to detect drift, remediate, and alert. Include testing steps?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Hugging Face","Oracle"]},{"id":"q-1934","question":"In a 5-account, multi-region AWS setup (prod, prod-sec, staging, dev, shared) spanning 3 regions, design an automated disaster recovery plan for a critical app running on **EKS** and **RDS**. The DR must auto‑failover **RDS** to cross‑region replicas, switch **Route 53** DNS, re‑sync **EKS** state with **ArgoCD**, rotate encryption keys, and enforce **SCPs** for DR accounts. Include testing, rollback, and post‑drill validation?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["OpenAI","Snowflake"]},{"id":"q-1949","question":"In a 3-account AWS Organization (prod, staging, dev) for a global SaaS app, configure automatic idle-instance remediation. If an EC2 instance in prod has the tag AutoStop=true and CPUUtilization < 5% for 24h, stop it at 02:00 local time. Use AWS Config to detect noncompliance, EventBridge/Lambda to stop, and a Config rule to enforce tag presence. Include testing steps?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Cloudflare","Two Sigma"]},{"id":"q-2009","question":"In a single-region AWS setup hosting a small SaaS app, design a weekly automated DR test that validates restoring an RDS snapshot and at least one EBS volume, then runs a basic end-to-end check against the app. Outline practical steps using AWS Config, EventBridge, Lambda, and a separate DR bucket/account for test artifacts, including rollback steps and how you verify success?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Google","Hashicorp","Square"]},{"id":"q-2046","question":"In a multi-region AWS deployment (prod across 4 regions) with accounts prod, security, infra, and audit, design an automated containment workflow triggered by GuardDuty findings of UnauthorizedAccess or PrivilegeEscalation. Automatically quarantine affected EC2s by swapping in a deny-all security group and routing traffic to a quarantine subnet, while persisting original SGs for rollback. Restoration requires security approval. Include cross-region EventBridge routing, Lambda orchestration, IAM permissions, rollback, and testing?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Coinbase","Meta","Two Sigma"]},{"id":"q-2136","question":"In a 3-account AWS Organization (prod, staging, dev) for a global SaaS app, enforce a secure S3 baseline in prod: no public access, SSE-KMS with a central CMK, and versioning enabled on all buckets. Use AWS Config rules (e.g., s3-bucket-public-access-prohibited, s3-bucket-server-side-encryption-enabled, s3-bucket-versioning-enabled), EventBridge, Lambda, and SCPs to detect, remediate, and alert. Include testing steps and rollback plan?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Instacart","Oracle","Uber"]},{"id":"q-2196","question":"In a 4-account Organization across 3 regions hosting a real-time analytics stack (Kinesis Data Streams, S3 data lake, DynamoDB Global Tables), design an automated DR test and remediation plan that validates cross-region replication, IAM role trust policies, and automated failover with minimal downtime. Include tooling (AWS Config rules, EventBridge, Lambda, SSM Automation), rollback strategy, and testing steps?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Google","Netflix","Oracle"]},{"id":"q-2241","question":"In a multi-account, multi-region production setup hosting a real-time analytics microservice, design an automated DR failover/fallback that shifts traffic to a hot standby region within 5 minutes of regional outage while preserving data consistency. Specify the wiring of Route 53 failover, AWS Global Accelerator, DynamoDB global tables, S3 cross-region replication, and IAM boundaries, plus orchestration via Step Functions/Lambda, post-failover checks, testing, and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Bloomberg","Hashicorp","Stripe"]},{"id":"q-2323","question":"Beginner scenario: A SaaS app stores per-tenant data in a single S3 bucket and serves tenants from a shared app tier. Design a cost- and security-friendly, auditable isolation model using per-tenant S3 Access Points and IAM roles, plus an Org SCP to prevent cross-account bucket listing. Outline required policies, Access Point config, testing, and rollback steps?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Hugging Face","IBM","Tesla"]},{"id":"q-2359","question":"Single-region web app on EC2 behind an Application Load Balancer experiences daily traffic spikes. Outline a beginner-friendly Auto Scaling setup using a Launch Template, an ASG with Target Tracking on CPU, a CloudWatch alarm, and ALB health checks. Include how you would test scaling in and out and rollback if metrics misbehave?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Anthropic","Hugging Face","IBM"]},{"id":"q-2576","question":"In a two-region real-time trading data platform, design an automated security incident response workflow triggered by a GuardDuty finding that an EC2 is compromised. Outline the end-to-end flow: finding → EventBridge → Lambda to (a) attach a restrictive security group, (b) snapshot/quarantine EBS volumes, (c) pause real-time Kinesis streams, (d) copy forensic data to a dedicated S3 bucket, (e) alert on-call via SNS. Include rollback, cross-region considerations, and a test plan?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Google","IBM","Robinhood"]},{"id":"q-2597","question":"In a three-account, multi-region AWS setup (prod, security, shared) spanning two regions, design an automated S3 governance workflow: require production buckets to enable default encryption with a CMK, block public access, and enforce versioning, with centralized cross-account access controls. Outline an end-to-end solution using AWS Config custom rules, Lambda remediation, EventBridge alerts, and Organizations SCPs; include testing steps and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Databricks","Google","NVIDIA"]},{"id":"q-2645","question":"Across a 6-account AWS Organization (prod, staging, dev) spanning three regions, design an automated cross-account S3 security posture that blocks public access and enforces encryption at rest for all buckets. Include automated remediation, drift detection, and rollback using AWS Config, Lambda, EventBridge, IAM Access Analyzer, and SCPs. How would you test and verify?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Google","LinkedIn","Robinhood"]},{"id":"q-2652","question":"Across six AWS accounts in three regions, design a centralized, immutable audit-log pipeline: have all accounts deliver CloudTrail, VPC Flow Logs, and Config logs to a single S3 bucket in an Audit account with Object Lock (Compliance) for 7 years; enforce encryption via a shared KMS CMK and SCPs; use EventBridge and Lambda for delivery health checks and auto-remediation; outline validation, testing, and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Airbnb","Uber"]},{"id":"q-2676","question":"In a single AWS account hosting a regional web app, require that every new resource carries Owner and Environment tags; propose an end-to-end remediation flow using AWS Config Custom Rules, Lambda remediation, EventBridge, and SNS alerts. How would you test it, and what rollback steps would you include? Provide a minimal CloudFormation snippet for the Config rule and remediation Lambda, please?","channel":"aws-sysops","subChannel":"general","difficulty":"beginner","tags":["aws-sysops"],"companies":["Google","Hugging Face","Netflix"]},{"id":"q-2718","question":"In a 3-region, multi-account SaaS deployment, enforce customer data locality so S3 data stays in the region defined by each customer and is not replicated cross-region unless explicitly approved. Design an automated policy engine using AWS Config custom rules, EventBridge, Lambda, and SCPs to prevent unauthorized replication, enforce region-bound KMS keys, and route alerts to a centralized security sink. Include testing and rollback steps with a simulated cross-region copy attempt?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Amazon","Cloudflare","Google"]},{"id":"q-675","question":"You manage a two-region AWS deployment (us-east-1, us-west-2) behind an ALB with private subnets, NAT gateway, and RDS in us-east-1. During business hours, us-west-2 exhibits spike in 5xx errors and higher latency. Outline immediate incident triage steps, AWS CLI commands to run, how you’d identify root causes (NAT saturation, DNS routing, cross-region replication lag), and both short- and long-term mitigations with verification steps?","channel":"aws-sysops","subChannel":"general","difficulty":"intermediate","tags":["aws-sysops"],"companies":["Apple","Snowflake"]},{"id":"q-982","question":"Design an automated cross-region disaster recovery plan for a globally distributed web app currently active in us-east-1 with a DR site in us-west-2, covering RDS, DynamoDB, S3, and ALB-backed frontend. Specify data synchronization, failover steps, testing, and rollback?","channel":"aws-sysops","subChannel":"general","difficulty":"advanced","tags":["aws-sysops"],"companies":["Anthropic","Google"]},{"id":"q-1036","question":"Scenario: A new external vendor needs read-only access to a single Azure AD-secured web portal (SaaS app) via B2B. Create a guest user, assign them to a dedicated App Role of the portal, enforce MFA and device-compliant access with a Conditional Access policy restricted to the office IP range, and outline post-grant validation and rollback?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Citadel","NVIDIA","PayPal"]},{"id":"q-1109","question":"How would you enable Self-Service Password Reset (SSPR) for a 20-user Azure AD group, enforce MFA during resets, and ensure auditable reset events with a simple rollback plan? Include licensing notes, enrollment flow, verification steps, and failure handling?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["MongoDB","Stripe"]},{"id":"q-1191","question":"Scenario: A data platform CI/CD pipeline deploys to Data Lake Gen2, Synapse, and a storage account across three subscriptions using a non-interactive service principal. Design a secure, rotating, auditable auth model: App Registration with certificate-based OAuth, Key Vault-stored certs rotated every 30 days, per-resource RBAC with least privilege, automatic revocation on build failure, and validation steps. Include how you would test access during runs and how to roll back changes if needed?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Databricks","Google","PayPal"]},{"id":"q-1304","question":"Scenario: a startup with 5–15 users is moving to Azure AD for the first time. You need to enable basic identity services: (a) create users and groups, (b) provide SSO for a SaaS app using SAML, (c) enforce MFA, (d) automatically assign Office 365 licenses via group membership, and (e) enable self-service password reset for all users. Outline an end-to-end setup plan and a minimal validation checklist prior to go-live?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Discord","IBM","Lyft"]},{"id":"q-1363","question":"Design a cross-tenant temporary access workflow for a contractor to a SaaS portal secured by Azure AD, leveraging Azure Lighthouse, B2B guest accounts, PIM with Just-In-Time activation, Conditional Access (MFA, device compliance, IP restrictions), and an access-review auto-revoke policy. Detail validation during activation and rollback?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Anthropic","Apple"]},{"id":"q-1380","question":"Scenario: Implement Just-In-Time elevation for Azure AD roles using Privileged Identity Management (PIM) to grant temporary Global Administrator and Privileged Role Administrator access. Include activation workflows, MFA requirements, approval routing, auto-expiry, and post-activation auditing. How would you validate elevated access usage and ensure revocation, including staging validation prior to production?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Instacart","LinkedIn","Two Sigma"]},{"id":"q-1427","question":"Implement Just-In-Time privileged access for production admin roles using Azure AD PIM. Requirements: MFA, approval workflow, time-bound activation, auto-expiry, and audit trails. Describe end-to-end steps: role setup, activation process, testing, monitoring, and how to safely handle emergency bypasses?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Adobe","Apple","Cloudflare"]},{"id":"q-1448","question":"How would you architect an end-to-end cross-tenant access governance flow that grants a partner contractor temporary access to Snowflake via Azure AD B2B and Entitlement Management, with MFA, sign-in risk conditions, an approvals workflow, auto-revocation after 10 days, and automated access reviews before renewal?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Coinbase","Netflix","Snowflake"]},{"id":"q-1561","question":"Design a beginner-friendly plan to enforce conditional access for a single Azure AD‑secured internal portal used by contractors. Include which policies you would create (MFA, device-compliance, IP ranges), how you would test rollout with a small pilot, and how you would validate access during the pilot and after, with minimal automation?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Airbnb","Discord","DoorDash"]},{"id":"q-1589","question":"Scenario: You run Azure AD for a multinational org with internal staff and external partners. Secure access to a sensitive SharePoint Online library and a CI/CD portal. Design a concrete end-to-end plan using: (a) Conditional Access with device compliance and sign-in risk, (b) Entitlement Management for guest access with time‑boxed access and automated reviews, (c) Privileged Identity Management for service accounts with Just‑In‑Time, approvals, and MFA, and (d) comprehensive auditing and alerting. Include validation steps and rollback?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Google","Meta","Stripe"]},{"id":"q-1753","question":"Scenario: A regulated fintech uses Azure AD Entitlement Management to grant external QA vendors episodic access to five internal SaaS apps. Design an Access Package strategy: define apps/roles, set time-limited assignments, enforce MFA, approval workflows, and auto-expiry with revocation. Outline validation across apps during the window and post-expiry?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Bloomberg","Robinhood","Snap"]},{"id":"q-1784","question":"Design a risk‑aware Azure AD access plan for two high‑risk apps (CI/CD portal and HR app) in a multi‑tenant setup, using Conditional Access with sign‑in risk, device compliance, and trusted IPs; implement auto‑remediation via Access Reviews and PIM for admins; outline testing, rollback, and auditing while minimizing automation?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["DoorDash","Salesforce"]},{"id":"q-1824","question":"Pilot passwordless sign-in for 10 users accessing a single internal Azure AD‑secured portal. Describe exact steps to enable passwordless (FIDO2 or Windows Hello), enroll devices, register credentials, enforce a Conditional Access policy for that app, run a 2-week pilot, and validate success and rollback procedures if issues arise?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Cloudflare","Google"]},{"id":"q-1876","question":"In a Cloudflare/NVIDIA-style Azure AD tenant, a partner needs access to a single Azure AD-secured portal for 30 days. Outline a concrete plan to grant time-bound access using Entitlement Management (Access Packages): define the package scope and roles, configure approvals, enforce auto-expiry, and audit/validate during the window and after expiry with minimal scripting. Include pilot testing steps?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Cloudflare","NVIDIA"]},{"id":"q-1985","question":"Scenario: You need to onboard external vendors to access a single Azure AD‑secured SaaS app via Azure AD B2B. Draft a beginner‑friendly, end‑to‑end plan to (1) grant guest access through a dedicated security group, (2) auto‑assign the required SaaS license via group membership, (3) enforce MFA for guests with Conditional Access, and (4) set a 14‑day expiry with Access Reviews and a rollback path. Include validation steps during the window and after expiry?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Coinbase","Google","Lyft"]},{"id":"q-2042","question":"Scenario: A regulated fintech team requires time-bound admin access to production Azure AD and resources via Privileged Identity Management (PIM) for a 72-hour window each quarter. Outline an end-to-end plan to (1) assign and activate a PIM-eligible role, (2) require MFA and Just-In-Time activation, (3) implement approvals and alerts, (4) auto-expire and enable post-activation reviews, and (5) validate access during and after the window with minimal scripting?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Anthropic","Coinbase","Snowflake"]},{"id":"q-2110","question":"Architect an identity governance pattern to manage admin access to multiple critical apps (both SaaS and internal portals) in Azure AD across a multi-tenant org. Design an integrated solution using Identity Governance features: Entitlement Management with Access Packages, Access Reviews with auto-remediation, Privileged Identity Management for JIT elevation, and Conditional Access signals to enforce device compliance and MFA. Include lifecycle flows, automation touchpoints, testing, and rollback plan?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Databricks","LinkedIn","NVIDIA"]},{"id":"q-2290","question":"Design a robust 60-day access plan for external contractors to a single Azure AD‑secured internal portal. Include (1) an Entitlement Management access package with a dedicated security group, (2) B2B guest provisioning and license assignment, (3) Conditional Access policies enforcing MFA, device compliance, and mapped IP ranges, (4) a validation plan during the window and a rollback path on expiry, with minimal automation?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Amazon","DoorDash","Zoom"]},{"id":"q-2328","question":"Scenario: A legacy on-prem HR web app is published through Azure AD Application Proxy. A contractor needs 7 days of access. Outline a beginner-friendly, end-to-end plan to (1) create a dedicated security group, (2) assign the app’s App Role to that group for the proxy, (3) enforce MFA and device compliance via Conditional Access for the group, (4) set a 7‑day Access Review with auto-revoke, and (5) validate access during the window and after expiry with minimal automation?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Goldman Sachs","Plaid","Uber"]},{"id":"q-2490","question":"Scenario: A consulting contractor requires temporary access to a mix of 5 internal apps and 3 SaaS services for six weeks. Outline an end-to-end plan using Azure AD Entitlement Management to create a catalog, build an access package, define approval workflows, enforce time-bound access, auto-assign licenses and group memberships, and validate access during and after the window with minimal scripting. Include monitoring and post-expiry audit steps?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Databricks","Goldman Sachs","Tesla"]},{"id":"q-2509","question":"Scenario: External auditors require temporary administrative access to a cross‑tenant data portal. Provide an end‑to‑end plan using Azure AD Privileged Identity Management (PIM) for Just‑In‑Time elevation to Privileged Role Administrator scoped to the portal, with manager approval, MFA on activation, and a CA policy enforcing device compliance. Include expiry, auto‑revoke, and a rollback/audit path with minimal automation?","channel":"azure-administrator","subChannel":"general","difficulty":"intermediate","tags":["azure-administrator"],"companies":["Cloudflare","Coinbase","Meta"]},{"id":"q-2572","question":"Propose an end-to-end plan to grant time-limited admin access to a production Azure subscription using **PIM**. Include (1) just-in-time elevation with formal approval, (2) mandatory **MFA** and device compliance during activation, (3) narrow RBAC scope (least privilege) for the elevated role, (4) auto-expiry plus periodic **Access Reviews**, (5) robust auditing, validation steps, and a rollback path. Minimize automation; no custom scripts?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["DoorDash","Stripe","Zoom"]},{"id":"q-855","question":"Your organization needs a temporary access workflow: grant a contractor read/write access to a single Blob Storage container for 30 days using Azure AD groups and RBAC, and automatically revoke access at day 30. How would you implement this end-to-end?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["DoorDash","IBM","Stripe"]},{"id":"q-897","question":"Scenario: A contractor needs access to a single Azure AD‑secured app (CI/CD portal) for 14 days. Outline an end‑to‑end approach using a dedicated Azure AD security group, app RBAC, and an Access Review to auto‑revoke access at day 14. Include how you would validate access during the window and after expiry, with minimal automation and no custom scripts?","channel":"azure-administrator","subChannel":"general","difficulty":"beginner","tags":["azure-administrator"],"companies":["Airbnb","Twitter","Two Sigma"]},{"id":"q-929","question":"Scenario: A multinational bank needs external data scientists to access multiple Azure resources (Data Lake Gen2, Synapse, and a storage account) for a 6-week analytics project. Propose an end-to-end access model using Azure AD Entitlement Management, Access Reviews, B2B collaboration, and Privileged Identity Management (PIM) for Just-In-Time role activations, with cross-subscription considerations, least privilege, and auditability. Include how you enforce MFA, token lifetimes, revocation at end, and validation steps?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Goldman Sachs","Netflix","PayPal"]},{"id":"q-942","question":"Scenario: A vendor must run nightly ingestion pipelines against Data Lake Gen2, a storage account, and Synapse in a shared Azure AD tenant for 10 days using a single service principal. Propose an end-to-end access model using an App Registration with scoped RBAC, Just-In-Time activation (PIM) for the service principal, Entitlement Management or Access Reviews, and automatic revocation at expiry. Include how you would validate access during the window and after expiry with minimal automation and no custom scripts?","channel":"azure-administrator","subChannel":"general","difficulty":"advanced","tags":["azure-administrator"],"companies":["Instacart","NVIDIA"]},{"id":"q-1053","question":"You’re deploying a multi-tenant Azure OpenAI-powered customer-support assistant for a global marketplace. Describe an end-to-end plan for runtime isolation and governance: prevent prompt injection, redact PII before OpenAI calls, enforce per-tenant quotas, maintain data lineage in Purview, ensure regional residency, and implement drift alerts via Azure Monitor plus a lightweight detector in Azure ML?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Cloudflare","DoorDash","Snowflake"]},{"id":"q-1134","question":"You're building a beginner-level Azure OpenAI-powered chat assistant for a rideshare service that serves clients in two regions. Outline a concrete data path and a minimal routing implementation that ensures user messages and model outputs stay in-region. Include a TypeScript function that selects the regional OpenAI endpoint based on client region, a latency fallback policy, and basic in-region logging to Azure Monitor. Provide testing approaches?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Citadel","Google","Uber"]},{"id":"q-1330","question":"Design a region-agnostic, tenant-aware GenAI service on Azure OpenAI that updates safety and governance policies at runtime via a central config store (Azure App Configuration) without redeploying prompts. Include how you route queries, enforce per-tenant data residency, and measure latency under 200ms for common tasks?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Hashicorp","Snowflake","Square"]},{"id":"q-1360","question":"You're building a real-time, multi-tenant Azure OpenAI-powered support chatbot for PayPal, MongoDB, and Two Sigma. You want canary deployments of two model variants (v1, v2) with automatic rollback on degradation, regional routing, and per-tenant quotas. Describe a concrete end-to-end approach: how you implement versioned deployments, traffic routing, governance, latency/quality monitoring, and rollback triggers using Azure OpenAI, API Management, Functions, Front Door, and Monitor. Include data flow and concrete metrics?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["MongoDB","PayPal","Two Sigma"]},{"id":"q-1414","question":"Scenario: You're deploying a multilingual, multi-tenant fintech chat assistant on Azure OpenAI Service. Each tenant's data must remain in their region, with per-tenant quotas, prompt-injection defenses, and automatic redaction before OpenAI calls. Outline a concrete deployment and testing plan using Azure API Management, Azure Functions, Text Analytics, Purview, regional OpenAI endpoints, and Azure Monitor. What edge cases exist and how would you verify data residency and drift monitoring across languages?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Meta","Scale Ai","Two Sigma"]},{"id":"q-1468","question":"You’re building an Azure OpenAI-powered support assistant for a payments platform that must isolate tenant data regionally, enforce per-tenant quotas, and attach citations to every answer. Design the end-to-end architecture, data paths, and testing plan using Azure API Management, regional OpenAI deployments, Redis quotas, Azure Cognitive Search with vector store, Purview, and residency checks. What are the key trade-offs?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["PayPal","Twitter"]},{"id":"q-1509","question":"Design a regional, policy-driven retrieval-augmented generation (RAG) pipeline on Azure for a global platform. Enforce per-tenant data residency, block secrets in prompts, redact PII automatically. Implement per-tenant quotas and budget alarms; emit audit trails to Purview. Route via APIM to regional OpenAI endpoints and a regional vector store (Cognitive Search or Cosmos DB) with drift monitoring and rollback?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Airbnb","MongoDB","Twitter"]},{"id":"q-1597","question":"You're building a beginner-friendly SaaS chat API powered by Azure OpenAI. It should be fronted by Azure API Management and implemented with an Azure Function backend. Implement a per-tenant rate limit of 60 requests per minute using an API Management policy and log usage to Azure Monitor. Describe the end-to-end setup, plus a minimal policy snippet and a tiny function wrapper showing the data flow. How would you validate it?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Instacart","NVIDIA"]},{"id":"q-1641","question":"Scenario: A Meta/Anthropic-scale social platform needs an Azure-hosted, multi-tenant content moderation bot. It must support multilingual queries, preserve tenant data residency in their regions, harden against prompt injection, redact PII before any OpenAI calls, enforce per-tenant quotas, and provide drift alerts. Outline an end-to-end pipeline using Azure API Management, Azure Functions, Azure OpenAI Service (regional endpoints), Text Analytics for PII, Purview for data lineage, and Azure Monitor plus a lightweight detector in Azure ML. Include a minimal policy snippet and a tiny function wrapper to illustrate data flow?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Anthropic","Meta"]},{"id":"q-1650","question":"Scenario: You are building a beginner-level multi-tenant Azure OpenAI chat assistant that uses a versioned prompt catalog stored in Azure Blob. API Management selects the tenant’s prompt version, enforces a per-tenant quota, redacts PII before calling OpenAI, and falls back to a default prompt if the catalog fetch fails. Outline the end-to-end flow, a minimal policy snippet, a simple function wrapper, and a test/failover plan?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Coinbase","Databricks"]},{"id":"q-1762","question":"Design a private, per-tenant Azure OpenAI-powered assistant exposed via API Management with strict data residency and governance. Describe end-to-end architecture using Private Link to OpenAI, tenant-scoped API Management gateways, per-tenant Managed Identities to call the private endpoint, regional OpenAI endpoints, Key Vault for secret rotation, Purview auditing, per-tenant quotas, and a drift detector in Azure ML. Include deployment, data flow, failure modes, and testing?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Amazon","Hashicorp"]},{"id":"q-1777","question":"Design an Azure OpenAI code-assistant workflow for a SaaS platform where developers paste private repository snippets to generate patches. Implement hard secret redaction before OpenAI calls, integrate Azure Key Vault for secret lookups with RBAC, enable per-tenant data isolation, and log all prompts/responses to Purview. How would you route, guard, and verify outputs end-to-end?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Cloudflare","Microsoft"]},{"id":"q-1833","question":"Design a real-time, search-augmented chat assistant on Azure OpenAI Service for a video conferencing platform. It must query a region-bound Azure Cognitive Search index, redact PII before OpenAI calls, enforce per-tenant quotas, and provide per-tenant explainability of retrieved docs. Outline the end-to-end data path (ingestion, vector search, redaction, prompt assembly, streaming response), a minimal API Management policy snippet, and a tiny backend snippet illustrating the flow and versioning strategy?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Cloudflare","Hugging Face","Zoom"]},{"id":"q-1891","question":"You're deploying an Azure OpenAI-powered enterprise assistant for a multi-tenant SaaS app. Each tenant's data must never mix, residency must be region-bound, and prompts must be safeguarded against leakage via memory or external tools. Outline a concrete data path and governance using Azure OpenAI Service (private endpoint), Azure Functions, API Management, per-tenant Cosmos DB for embeddings, and per-tenant quotas, with a lightweight detector for leakage. Include an example policy snippet and a test plan?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Apple","Goldman Sachs","Meta"]},{"id":"q-1898","question":"Design a privacy-preserving, multi-tenant AI data enrichment pipeline on Azure that uses Azure OpenAI Service for generation and embeddings to augment customer data with external sources, while enforcing per-tenant data residency, PII redaction, drift detection, and auditable governance via Purview; outline architecture, data flows, and testing edge cases for latency and cost under burst traffic?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Citadel","Plaid","Uber"]},{"id":"q-1933","question":"You're deploying a privacy-preserving Azure OpenAI-powered analytics assistant for a fleet of autonomous vehicles with edge-to-cloud hybrids. Tenants must keep data in their own stores, even at the edge, and APIs are exposed via APIM. Propose a concrete pipeline using Azure Arc-enabled OpenAI, Private Endpoints, Functions, Purview, and Monitor, detailing data flow, tenancy isolation, data residency, offline mode and drift monitoring. Include testing and edge-case notes?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Slack","Snowflake","Tesla"]},{"id":"q-1958","question":"Design a multilingual customer feedback analytics pipeline that runs on Nvidia edge GPUs at retail kiosks and pairs with Azure OpenAI for summarization. Ingests local text, translates to English, analyzes sentiment, flags PII locally, then pushes aggregated metrics to Azure for long-term storage. Describe the data path, services involved, governance, and testing approach?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Goldman Sachs","NVIDIA","Snowflake"]},{"id":"q-1994","question":"Beginner-level: You have a multi-service Azure OpenAI chat pipeline: APIM -> Function wrapper -> OpenAI. Latency is variable. Describe a minimal end-to-end tracing plan using Application Insights, OpenTelemetry, and correlation IDs. Show how you would propagate a trace across APIM policy, Function, and OpenAI call, and outline a simple test to reproduce and measure end-to-end latency?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Coinbase","MongoDB","Salesforce"]},{"id":"q-2066","question":"Scenario: You run a multi-tenant Azure OpenAI-powered assistant where each tenant supplies per-tenant safety policies and tone guides that must be enforced before any OpenAI call. Design an end-to-end path using Azure API Management, Azure Functions, a lightweight policy engine, regional OpenAI endpoints, and Purview for data lineage. Explain policy evaluation, tenant isolation, drift handling, and testing with realistic workloads?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Coinbase","IBM","Lyft"]},{"id":"q-2116","question":"Design a multi-tenant Azure OpenAI-powered support bot for fintech apps with per-tenant regional residency, live moderation gating, and PII redaction before OpenAI calls. Outline the end-to-end data path, components, and a testing plan; include edge cases and how residency and drift are validated?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["PayPal","Stripe","Uber"]},{"id":"q-2162","question":"You're building a multi-tenant Azure OpenAI-powered analytics assistant that ingests user-uploaded CSVs and returns AI-generated insights via chat. Each tenant requires data isolation, per-tenant model versions, and privacy controls enforcing in-region processing and PII redaction in outputs. Design an end-to-end pipeline: user → API gateway → function orchestrator → per-tenant data store in Data Lake (Purview-tagged) → OpenAI inference with tenant-specific prompts and versioned deployments. Include canary rollouts, tests for data leakage, and drift monitoring?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Amazon","Google","MongoDB"]},{"id":"q-2179","question":"Design a multi-tenant, region-resident compliance assistant API that uses Azure OpenAI for responses, a retrieval store (Azure Cognitive Search) for context, and per-tenant guardrails: data residency, content filtering, rate limits, and a canary model rollout. Show end-to-end data path and governance, and outline testing strategies for prompt injection, drift, and data leakage?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Meta","Oracle","Robinhood"]},{"id":"q-2325","question":"You're building a beginner-level Azure OpenAI-powered FAQ bot for a multinational retailer with regional tenants. Describe a practical flow that enforces per-tenant data isolation and brand voice using a versioned prompt catalog in Azure Blob, APIM, and Functions. Include fallbacks, PII redaction, and a simple test harness that validates outputs against tenant-specific rules before returning to users?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["IBM","Snowflake"]},{"id":"q-2375","question":"Scenario: You’re building a beginner-level Azure OpenAI-powered internal helpdesk bot for a multinational. It must auto-detect user language (English, Spanish, Chinese) via Azure Text Analytics, route prompts to a regional OpenAI deployment (US/EU/APAC), and keep data residency by region. Outline the end-to-end data path, components, and a simple test plan; include fallback translation path if language isn’t supported?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Apple","Microsoft"]},{"id":"q-2424","question":"Scenario: You’re building a real-time fraud-analytics assistant using Azure OpenAI Service to process streaming transactions from multiple financial partners. You must achieve sub-100 ms per-query latency, enforce strict per-tenant data isolation, redact PII before hitting OpenAI, and produce auditable decisions. Design the data path and architecture using Azure API Management, Event Hubs, Functions, a fast cache, Key Vault for keys, regional OpenAI endpoints, Purview for lineage, and ADR/audit logging. Include a concrete data flow and at least three concrete controls?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Citadel","Hashicorp","Snowflake"]},{"id":"q-2559","question":"Scenario: Build a real-time analytics assistant that ingests streaming telemetry from thousands of tenant apps and uses Azure OpenAI Service to summarize incidents. Data includes secrets and PII; ensure per-tenant residency, data redaction before OpenAI calls, and strict isolation in a pay-as-you-go cost model. Outline the end-to-end data path, gating, and testing plan using Event Hubs, Functions, Text Analytics, regional OpenAI endpoints, and Purview?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-ai-engineer"],"companies":["Instacart","Robinhood","Two Sigma"]},{"id":"q-2589","question":"You’re deploying a real-time Azure OpenAI-powered field-support assistant for multiple automaker tenants. Outline a concrete, region-aware prompt-routing gateway that guarantees per-tenant isolation and data residency using API Management, Azure Front Door, and versioned OpenAI deployments. Include the data path, model routing rules, rollback plan, and a testing strategy for latency and drift?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-ai-engineer"],"companies":["Databricks","Google","Tesla"]},{"id":"q-2703","question":"Design a beginner-level Azure OpenAI-powered FAQ bot for a software product. The bot must (1) run behind Azure OpenAI Service and API Management, (2) use a simple prompt template with a versioned FAQ catalog stored in Azure Blob, (3) implement a minimal guardrail: redact PII in user questions and apply a content filter to block disallowed topics in the response before it reaches the user. Outline the end-to-end data path, components, and a basic test plan. Include a small code snippet for the redaction function and for the post-filter?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Tesla","Zoom"]},{"id":"q-866","question":"You're deploying a multi-tenant chat assistant on Azure OpenAI Service for a rideshare company. PII must never be sent to OpenAI and responses must redact sensitive data before delivery. Outline a practical, beginner-friendly data path using Azure API Management, Azure Functions, Text Analytics for PII detection, and a regional OpenAI deployment. Include a simple data flow?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["MongoDB","Tesla","Uber"]},{"id":"q-963","question":"You're building a beginner-friendly customer support bot on Azure OpenAI Service. How would you design a lightweight API boundary policy (Azure API Management) to rate-limit per user, cap monthly spend, and gracefully fall back to a rule-based reply if OpenAI is unavailable? Describe the data flow from API call through OpenAI or fallback, and include a minimal policy snippet?","channel":"azure-ai-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-ai-engineer"],"companies":["Apple","Bloomberg","Square"]},{"id":"q-1029","question":"In a global IoT telemetry pipeline ingesting 100k events/s per region into ADLS Gen2 and Databricks Delta Lake, implement schema drift tolerant ingestion, end-to-end data lineage via Purview, RBAC, and cross-region DR replication. How would you architect the data contracts, partitioning, watermarking, retention, and governance to meet data residency and fault-tolerance requirements?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Cloudflare","Twitter","Uber"]},{"id":"q-1043","question":"Design an end-to-end Azure data-ecosystem pipeline that ingests incremental changes from a MongoDB Atlas collection into Delta Lake on ADLS Gen2, preserving SCD Type 2 history for a customers dimension, and handling schema drift. Include data lineage to Azure Purview, late-arriving updates, and on-demand reprocessing without data loss. Which components and config would you choose, and why?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["MongoDB","NVIDIA","Robinhood"]},{"id":"q-1311","question":"Design an end-to-end incremental ELT pipeline on Azure that ingests 2 TB/day of nested JSON telemetry from Event Hubs into a Delta Lake on ADLS Gen2, then loads a star schema in Azure Synapse. Requirements: (1) schema drift tolerant writes and schema evolution, (2) upserts by a composite key (tenant_id, event_id), (3) end-to-end data lineage to Purview, (4) late-arriving data support via watermarking, (5) partitioning by region and day with file-size/compaction strategies. Which components and patterns would you use, and why? Compare Delta Lake MERGE vs Delete+Insert for upserts?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["Bloomberg","Google","Snowflake"]},{"id":"q-1326","question":"Design an end-to-end pipeline that streams user activity from Azure Event Hubs into Delta Lake on ADLS Gen2, supports schema evolution, and updates an SCD Type 2 customer dimension in Synapse. Include exactly-once guarantees, late-arriving data handling, idempotent sinks, and end-to-end data lineage via Purview in a hybrid estate. List components, data flows, and governance approach?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["IBM","Snap"]},{"id":"q-1453","question":"You are starting a beginner-friendly Azure data engineer task: daily telemetry logs arrive in ADLS Gen2 under /raw/telemetry/ in mixed formats (CSV and nested JSON). Design an event-driven pipeline using Azure Data Factory that fires on blob creation, ingests files, flattens nested structures to a canonical schema, handles optional fields and few schema drift cases, and loads into a simple star schema in Azure Synapse Analytics. Include incremental loading with a watermark and basic data quality checks. What components and steps would you implement?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Airbnb","Google"]},{"id":"q-1474","question":"Design a nightly Azure data pipeline that ingests delta updates from a SaaS source into ADLS Gen2, handles schema drift with automatic inference, and materializes a star schema in Azure Synapse. Ensure end-to-end data lineage, idempotent upserts, and reliable rollback after failures. Which services and patterns would you deploy, and how would you verify correctness and lineage end-to-end?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["Oracle","Plaid","Two Sigma"]},{"id":"q-1524","question":"You operate a global event lake: multiple producers push JSON events into Event Hubs. Ingest to ADLS Gen2, apply drift-tolerant transformations, and publish a canonical star schema in Azure Synapse with incremental loads. Propose a production-ready architecture leveraging Data Factory, Databricks Delta Lake, and Purview; detail schema evolution, late-arriving data handling, data quality gates, and privacy masking?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["LinkedIn","Scale Ai","Square"]},{"id":"q-1563","question":"Daily JSON feed arrives in ADLS Gen2. Design a beginner Data Factory pipeline to stage, flatten nested fields, validate data quality, and write a partitioned Parquet table in Azure Synapse. Handle minor schema drift by defaults and ignoring new fields. Which components and steps would you use?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Bloomberg","DoorDash","Twitter"]},{"id":"q-1638","question":"Design a streaming ingestion from an on-prem ERP into ADLS Gen2 for real-time analytics. Use Delta Lake with Auto Loader-like ingestion, bronze-silver-gold data path, exact-once delivery, and schema-drift tolerant processing. The silver layer feeds a star schema in Azure Synapse. Ensure end-to-end lineage in Purview, data quality gates, and robust late-arriving data handling. Describe architecture, contracts, and rollback strategy?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["IBM","Snowflake","Two Sigma"]},{"id":"q-1682","question":"Design an end-to-end near-real-time data pipeline for a global adtech dataset with mixed sources (on-prem SQL Server, Azure SQL, and SaaS APIs). Implement CDC, incremental loads, and schema drift tolerance, load into Delta Lake on Synapse, and enforce end-to-end lineage in Purview. Include data masking for PII, data quality gates, and a GitOps workflow for pipelines?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Amazon","Scale Ai"]},{"id":"q-1782","question":"Design a beginner-level Azure data ingestion task: In a fintech scenario, ingest daily transaction rows from an on-premises SQL Server into Azure Data Lake Storage Gen2 as Parquet files using Azure Data Factory. Include steps to handle date partitioning, security, and reliable runs. What would your pipeline look like, and what minimal code or configuration would you provide?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Citadel","Coinbase"]},{"id":"q-1916","question":"You manage a multi-region analytics platform with data from Salesforce, Workday, and IoT devices. Data lands in ADLS Gen2; you need to apply policy-driven redaction for PII, support incremental loads, enforce schema drift tolerance, and publish end-to-end lineage to Azure Purview and downstream consumers (Power BI, Synapse). Propose an Azure-native pipeline design using Data Factory, Databricks Delta Live Tables, Purview, and Synapse; highlight design decisions, governance, and potential pitfalls?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["LinkedIn","MongoDB","Snap"]},{"id":"q-1979","question":"In Azure, design a production-grade data pipeline for streaming ecommerce telemetry from Azure Event Hubs into ADLS Gen2, with downstream star schema in Synapse, enabling incremental loads, automatic schema drift handling, and end-to-end data lineage via Purview. Include details on Delta Lake usage, MERGE-based upserts, late-arriving data handling, and a minimal Spark code snippet for a MERGE with schema drift tolerance?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Databricks","OpenAI"]},{"id":"q-2102","question":"A batch of daily JSON click events arrives in ADLS Gen2 under /raw/events/ from an on‑prem source via Self-Hosted IR. Build a beginner Data Factory pipeline that (1) copies raw JSON to /staging/events/, (2) uses a schema-drift-tolerant mapping to normalize fields and cast ts to timestamp, (3) writes to a daily-partitioned Parquet table in Azure Synapse, and (4) loads incrementally by tracking max ts in a control table and filtering ts > last_ts. Include a basic data quality check for userId?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Airbnb","NVIDIA","Uber"]},{"id":"q-2378","question":"Design a scalable pipeline to ingest daily customer event logs from an on-prem Kafka cluster into Azure Data Lake Gen2, apply schema drift tolerant transformations, and maintain an SCD Type 2 star schema in Azure Synapse, while providing end-to-end data lineage via Purview. Which Azure services and patterns would you use, and how would you implement incremental loads?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["DoorDash","Lyft","Snowflake"]},{"id":"q-2495","question":"Ingest JSON telemetry from multiple partners into ADLS Gen2 and upsert into a Delta Lake on Synapse while preserving end-to-end lineage and data quality. Outline an Azure-based architecture using IoT Hub/Event Hubs, Data Factory or Synapse pipelines, Databricks, Purview, and Delta Lake features to handle schema drift, late data, and incremental upserts. Include security, cost considerations, and observability?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["OpenAI","Plaid","Snap"]},{"id":"q-2529","question":"Design an Azure data engineering pipeline for streaming IoT telemetry from store sensors into ADLS Gen2 and Synapse, with schema drift tolerant transformations, incremental loads, and end-to-end data lineage. Use Event Hubs to ADLS Gen2, Delta Lake in Synapse, and Purview for lineage. Address late data, schema evolution, security, and cross-region replication. What services and data contracts would you implement, and how would you validate correctness?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["Airbnb","Hashicorp","Meta"]},{"id":"q-2605","question":"Design an end-to-end real-time ingestion and analytics pipeline for a live market tick feed (timestamp, symbol, price, volume) arriving from multiple brokers at high throughput. Ingest to ADLS Gen2 as Delta Lake, with schema drift tolerance and automatic schema evolution, and deliver upserts to a central ticks table. Ensure exactly-once streaming, late-arriving data handling, end-to-end data lineage via Purview, and scalable partitioning by date and symbol. Outline components (Event Hubs, Databricks, Delta Lake, Purview) and provide a workable configuration sketch?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Apple","Slack","Two Sigma"]},{"id":"q-2625","question":"Daily on-prem JSON event files arrive; design a beginner ELT pipeline using Data Factory that ingests JSON, enforces a basic schema, converts to Parquet, partitions by event_date, and loads a simple star schema into Azure Synapse. Explain how you’d implement basic data lineage and a 7-day retention policy, without advanced schema drift handling?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Amazon","Goldman Sachs","Slack"]},{"id":"q-2669","question":"Design an Azure streaming data pipeline for a global fleet of IoT devices sending JSON telemetry to Event Hubs, with data landing in ADLS Gen2 and a Delta Lake lakehouse. Requirements: sub-5 second ingestion latency, end-to-end latency <15 seconds for dashboards, automatic schema evolution, idempotent upserts, and end-to-end lineage with Purview. Outline components, dataflow, and a minimal Delta table evolution example?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["Citadel","Discord","Tesla"]},{"id":"q-2680","question":"Design an end-to-end Azure data platform to ingest per-tenant e-commerce clickstream data from an on-prem SFTP into ADLS Gen2, handle schema drift, and deliver per-tenant analytics in Synapse while preserving end-to-end lineage in Purview and enforcing RBAC/column masking. Implement incremental loads, late-data handling, and cost/perf tradeoffs between serverless vs dedicated pools?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Amazon","Coinbase","Snowflake"]},{"id":"q-2712","question":"Design a global Azure data lakehouse for real-time fraud analytics. Ingest streaming transaction events from on-prem ERP into ADLS Gen2, apply schema-drift tolerant transformations, and store as Delta Lake tables with governance via Unity Catalog. Implement incremental loads with CDC and watermark, ensure end-to-end lineage via Purview, and support multi-region reads with geo-replication. Which components and patterns would you choose, and how would you configure them to balance latency, cost, and compliance?","channel":"azure-data-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-data-engineer"],"companies":["Bloomberg","Google","Microsoft"]},{"id":"q-940","question":"Daily CSV exports arrive in an ADLS Gen2 container at /incoming/sales/. Files may evolve over time as columns drift. Build a beginner pipeline using Data Factory to stage raw data, infer/handle schema changes, and load a simple star schema in Azure Synapse Analytics. Include a watermark-based incremental load and basic scheduling. Which services and steps would you implement?","channel":"azure-data-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-data-engineer"],"companies":["Adobe","Amazon","Zoom"]},{"id":"q-980","question":"Design a global streaming-to-batch data pipeline for a PayPal/Adobe/Netflix-scale analytics platform: ingest real-time clickstream from Event Hubs into ADLS Gen2, maintain a Delta Lake with drift-tolerant schema, mask PII at ingestion, expose masked aggregates via serverless SQL pool, and enforce end-to-end lineage with Purview while supporting cross-region DR and data residency. Which Azure components and patterns would you use, and how would you handle schema evolution and failure modes?","channel":"azure-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-data-engineer"],"companies":["Adobe","Netflix","PayPal"]},{"id":"q-1006","question":"Design a real-time telemetry ingestion pipeline for a fleet of autonomous vehicles on Azure. Events arrive at high volume per region; you must store compact per-vehicle summaries in Cosmos DB and archive raw events to Data Lake Gen2. How would you achieve exactly-once processing for aggregates, sub-200 ms latency, and zero data loss on transient failures? Propose architecture using Event Hubs, Functions, Databricks, and cross-region replication; justify idempotency and retry strategies?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Lyft","Slack","Tesla"]},{"id":"q-1136","question":"Design an end-to-end telemetry ingestion pipeline for 1M devices/min delivering messages {vehicleId, ts, lat, lon, speed}. Ingest via HTTPS into Event Hubs with vehicleId as partition key, process with a Function app (Event Hubs trigger) using batchSize=100; deduplicate per vehicle with Durable Entity and upsert to Cosmos DB multi-region. Explain data model, idempotency, Change Feed, backpressure, and monitoring?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Airbnb","Amazon","Tesla"]},{"id":"q-1248","question":"Design an end-to-end Azure ingestion pipeline for multi-tenant IoT events: thousands of devices per region send JSON to a gateway, per-tenant aggregates stored in Cosmos DB, raw data archived to Data Lake Gen2. Explain chosen services (Event Hub, Function/Durable Function, Cosmos DB with TTL, Data Lake), how you enforce per-tenant isolation and auditability, and how you achieve exactly-once processing and retry semantics?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Amazon","Citadel","Google"]},{"id":"q-1277","question":"Design a real-time multi-tenant feature-store pipeline on Azure for a high-velocity AI platform. Ingest telemetry events via Event Hubs (tenantId, featureName, value, ts). Build end-to-end streaming with exactly-once semantics, isolation by tenant, and low-latency online reads. Specify concrete components (Event Hubs, Spark Structured Streaming, Cosmos DB with tenantId partition, Redis online store), auditability, TTL, and testing strategy?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Google","Hugging Face","OpenAI"]},{"id":"q-1359","question":"A small API running on Azure Functions must securely retrieve a database connection string from Azure Key Vault at startup and refresh it periodically without restarting the function. Propose a beginner-friendly, low-latency approach using Managed Identity and Key Vault, including caching strategy, rotation handling, and error fallback?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["OpenAI","Salesforce","Snowflake"]},{"id":"q-1397","question":"Design a multi-region, per-tenant Azure API with data residency, deterministic retries, and exactly-once semantics at scale. Propose services (APIM, Functions + Durable Functions, Cosmos DB per-tenant, Front Door, Private Endpoints) and explain how per-tenant isolation, data residency, and retry determinism are achieved?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Goldman Sachs","Oracle","Plaid"]},{"id":"q-1539","question":"Design a real-time, multi-tenant analytics pipeline for a chat platform: events arrive at 5-10k msgs/sec total via Azure Event Hubs; process with Azure Databricks on Delta Lake stored in ADLS Gen2; governance via Unity Catalog; ensure strict per-tenant isolation, auditability, and exactly-once processing; TTL/retention; cross-region read; cost constraints. Describe architecture decisions, data layouts, and failure scenarios?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Databricks","Discord","Microsoft"]},{"id":"q-1565","question":"Design a zero-trust, per-tenant access model to a private Azure SQL Database from a microservices mesh. Use Azure AD, Managed Identities, Row-Level Security, and dynamic data masking. Explain how you enforce least privilege, tenant isolation, auditing, and how you validate access policies in CI/CD?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Discord","Google","Plaid"]},{"id":"q-1594","question":"You're building a real-time, multi-tenant feature-flag platform on Azure to serve traffic across three regions with sub-50ms evaluation latency. Each tenant has per-flag rules, canary/A/B experiments, and strict audit requirements. Outline end-to-end design: data model for flags and experiments, evaluation path, storage choices (Cosmos DB vs SQL), caching, event sourcing, cross-region synchronization, security (Managed Identities, Key Vault, RBAC), rollback strategy, and failure modes. Include how you'd test canary safety and ensure tenant isolation?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Citadel","DoorDash"]},{"id":"q-1620","question":"You're architecting a **multi-region telemetry ingestion** pipeline for a real-time fraud-detection service. Edge devices per region push JSON events to **Azure IoT Hub**; you must ingest, partition by tenant, store raw and processed results with exact-once semantics, and enforce strict per-tenant isolation and auditability within tight cost constraints. Propose architecture choices (IoT Hub, **Event Hubs**, **Delta Lake**, **Unity Catalog**, **Cosmos DB**), data layout, and failure modes; how would you test end-to-end dedup and cross-region replayability?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Google","Hugging Face"]},{"id":"q-1668","question":"You’re building a beginner Azure Function (HTTP trigger) that calls a 3rd‑party API. Do not store API keys in code or app settings. How would you securely fetch the API key at runtime from Azure Key Vault using a managed identity? Outline the steps to enable the identity, grant access, and provide a minimal code snippet to read the secret?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["LinkedIn","Slack"]},{"id":"q-1727","question":"You're building a polyglot Azure-based service with HTTP APIs, background workers, and a data lake. You need end-to-end tracing across Functions, AKS, and Databricks jobs using OpenTelemetry and a single trace across services. Describe how you'd implement tracing, propagate context, and collect/export to Azure Monitor, including sampling strategy and validation steps?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Databricks","Meta","Zoom"]},{"id":"q-1791","question":"You're building an Azure-native, multi-tenant data platform for real-time payments used by PayPal and Tesla. Ingest via Event Hubs, process with AKS and Functions, store in Delta Lake on ADLS Gen2, expose a data API. How would you enforce strict per-tenant isolation, achieve end-to-end exactly-once semantics across services, and implement a shadow-traffic ML model deployment with safe rollback and audit trails? Include architecture choices, data layouts, and failure modes?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["PayPal","Tesla"]},{"id":"q-1839","question":"You're building an Azure IoT telemetry platform for a global fleet. Devices send 20–30k events/sec to IoT Hub. You need per-tenant isolation, real-time enrichment (geolocation, device type), and fan-out to three sinks: Delta Lake on ADLS Gen2, Azure Data Explorer dashboards, and an AI inference service on AKS. Must guarantee at-least-once semantics, handle out-of-order data, and support cross-region DR with measurable RPO. Compare two architectures: (A) serverless micro-batch: IoT Hub → Event Hubs → Functions for lightweight enrichment; Spark Structured Streaming writes to Delta Lake on ADLS Gen2; sinks feed Data Explorer and AKS inference. (B) pure streaming: Event Hubs → Spark Structured Streaming with larger cluster, stricter SLAs, and end-to-end exactly-once semantics. Explain data models, failure modes, and governance?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Amazon","Google","Netflix"]},{"id":"q-1962","question":"You’re building a beginner Azure Function (HTTP trigger) that receives event payloads and stores them in a data container. Design a lightweight, auditable approach to log every write without slowing latency. Include how you would generate an immutable audit trail and what storage pattern you’d use. Provide a minimal code snippet to append an audit line with timestamp, eventId, and userId derived from Authorization header?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["Google","Tesla"]},{"id":"q-1995","question":"Describe how you would implement a timer-triggered Azure Function that runs every 15 minutes to poll an on‑prem REST endpoint and write a daily aggregation blob to Azure Blob Storage. How would you guarantee idempotent writes to avoid duplicates across retries, including blob naming strategy and a minimal check-then-write code pattern?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["Cloudflare","Plaid","Uber"]},{"id":"q-2059","question":"Design an Azure-native data export service for a fintech that must export per-customer data on demand and on a schedule, with strict data residency, consent checks, and an audit trail. Use ADLS Gen2, Cosmos DB, Event Grid, Durable Functions or Functions, and Key Vault. Explain data partitioning, encryption, access control, idempotency, and failure modes including retries, outages, and rollback. Provide a concrete data model and flow?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Goldman Sachs","Robinhood","Stripe"]},{"id":"q-2109","question":"You're building a real-time moderation pipeline for a global social app on Azure. Ingest flows via Azure Event Hubs at 20-40k msgs/sec, then you apply NLP classification in a chain of Functions (or Durable Functions) to flag policy-violating messages, store results in Cosmos DB with per-tenant isolation, and index metadata in Azure Cognitive Search for fast queries. How would you design for latency under 150ms per event, strict per-tenant data isolation, idempotent retries, and auditable trails across services? Include error handling and rollback strategies?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Airbnb","Amazon","Plaid"]},{"id":"q-2155","question":"You're building a tenant-aware API gateway on Azure that serves dozens of microservices for hundreds of tenants. Implement per-tenant quotas, latency budgets, and canary rollouts. Describe a concrete architecture using Azure API Management, Azure Front Door, Redis for rate-limiting, and per-tenant state in Cosmos DB or Redis; explain failure modes, rollback, and testing strategies under traffic spikes?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["IBM","Uber"]},{"id":"q-2176","question":"You're building a beginner Azure-based image processing workflow: a user uploads a photo to Azure Blob Storage. A Function App should trigger, resize the image into three thumbnails, store metadata in Cosmos DB with per-user partition keys, and publish a status message to Azure Service Bus. How would you implement idempotent processing, retries, and per-user isolation, with a minimal code sketch showing how to deduplicate on blob name and initiate the three resizes?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["Adobe","Cloudflare"]},{"id":"q-2203","question":"Design a privacy-preserving, auditable streaming pipeline for real-time telemetry in a multi-tenant SaaS on Azure. Ingest via Azure Event Hubs at ~60k msgs/sec; run per-tenant aggregation and anomaly detection in Durable Functions; store per-tenant data in Cosmos DB with strict isolation; enforce data residency per tenant region and provide rollback for feature-flag changes. Describe architecture, data model, exactly-once guarantees, audit trails, and failure modes?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Anthropic","Plaid","Slack"]},{"id":"q-2235","question":"Design a real-time, multi-tenant feature-flag platform on Azure for a geo-distributed microservices stack. Tenant isolation must be enforced; flag evaluation latency < 200 ms at peak. Use API Management, Functions, Cosmos DB (partitioned by tenant), Redis (near the API layer), and Event Grid. Describe data model, cache strategy, canary rollout, rollback plan, auditing, and failure handling?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Stripe","Uber","Zoom"]},{"id":"q-2307","question":"You're building a privacy-preserving analytics marketplace on Azure: ingest telemetry via Event Hubs; anonymize with differential privacy during ingestion or enrichment; store tenant-scoped data in Delta Lake on ADLS Gen2 with strict per-tenant partitioning; catalog lineage in Azure Purview; and share results through per-tenant REST APIs with RBAC and data-sharing controls. Outline the architecture, data flow, and trade-offs to meet privacy, latency, and cost targets?","channel":"azure-developer","subChannel":"general","difficulty":"advanced","tags":["azure-developer"],"companies":["Bloomberg","Oracle","Scale Ai"]},{"id":"q-2379","question":"Build a beginner Azure REST API using Azure Functions (HTTP trigger) to manage product data for multiple tenants. Each tenant must be isolated, configs sourced from Azure App Configuration, and telemetry sent to Application Insights. Describe the authentication model, per-tenant data isolation strategy, and a minimal test plan, plus a small code sketch showing API key validation and tenant extraction?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["Airbnb","Hashicorp","PayPal"]},{"id":"q-2484","question":"Design a multi-tenant telemetry pipeline on Azure: events arrive through Event Hubs per-tenant, processed by Durable Functions, stored in per-tenant Cosmos DB, and surfaced via Azure Data Explorer dashboards. How would you implement end-to-end tracing with OpenTelemetry across all components, ensure per-tenant correlation, minimize overhead, handle retries idempotently, and preserve privacy controls (pseudonymization, access controls)?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Adobe","Apple","Meta"]},{"id":"q-2679","question":"Build a real-time fraud scoring pipeline on Azure for a multi-tenant fintech. Ingest 60k msgs/sec via Event Hubs, run scoring in Azure ML, store per-tenant results in Cosmos DB with region-bound writes, and publish to downstream systems. How would you enforce per-tenant data residency, hit sub-200ms latency, support hot model updates, and ensure end-to-end auditability and replay safety? Include regional failover and RBAC?","channel":"azure-developer","subChannel":"general","difficulty":"intermediate","tags":["azure-developer"],"companies":["Coinbase","OpenAI","PayPal"]},{"id":"q-891","question":"Design a beginner-friendly serverless image-upload API on Azure: clients POST JPEGs to an HTTP-triggered Function, which saves the file to Blob Storage, enqueues a processing job, and stores a metadata record in Cosmos DB. How would you ensure idempotency, handle retries with back-off, and keep costs predictable on a Consumption plan?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["Robinhood","Salesforce","Tesla"]},{"id":"q-973","question":"Case: You’re building a beginner-friendly Azure API that accepts events from mobile apps. Each event includes userId, eventType, and timestamp. The API should write a compact summary to Cosmos DB and stream raw events to Event Hubs for analytics. On a Consumption plan, outline the minimal architecture, bindings, and error handling to ensure low latency, safe retries, and no data loss during transient outages?","channel":"azure-developer","subChannel":"general","difficulty":"beginner","tags":["azure-developer"],"companies":["PayPal","Scale Ai"]},{"id":"q-1063","question":"With a 60-service monorepo deployed to Azure subscriptions across three regions, design an end-to-end release strategy that builds per-service artifacts, promotes to dev/stage/prod environments, gates each promotion with environment approvals tied to region-owner groups, and enforces that PRs link to an Azure Boards item. Include a minimal YAML template and gating approach?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Airbnb","Cloudflare"]},{"id":"q-1135","question":"Design an Azure DevOps multi-tenant canary deployment pipeline for a SaaS service that promotes per-tenant changes to prod only after a staged rollout window, uses tenant-scoped feature flags, enforces per-tenant approvals before prod, and rolls back automatically if telemetry thresholds are exceeded; outline the pipeline structure, environment gates, and auditing approach?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-devops-engineer"],"companies":["LinkedIn","Stripe"]},{"id":"q-1255","question":"Scenario: You’re configuring a new Azure DevOps project with Repos and Boards for a small service. How would you implement beginner-friendly PR governance to ensure PRs into main are linked to a Boards work item, the PR title includes the work item ID, a PR validation build runs and passes, and an automatic staging deployment with a manual prod gate? Outline exact steps and considerations?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Microsoft","Robinhood"]},{"id":"q-1329","question":"In Azure DevOps, design an end-to-end pattern to provide per-PR ephemeral environments in AKS for a microservices app. Use a single AKS cluster with namespace-per-PR, Helm for deployments, Azure Key Vault for per-PR secrets, and a TTL-based teardown. Describe the pipeline steps, gating, security, costs, and how to isolate test data and telemetry. End with ?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["DoorDash","LinkedIn"]},{"id":"q-1355","question":"Configure a PR-only security gate for a Node.js service: trigger on PRs to main (trigger: none; pr: branches include: main) and skip pushes. Add a security job that runs npm ci, npm audit --json, and fail if any advisories of severity high or critical exist, printing a concise summary. This blocks the PR until issues are addressed?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Google","Robinhood"]},{"id":"q-1423","question":"In Azure DevOps for a monorepo with multiple npm packages, design a PR validation that runs tests only for changed packages, builds per-package artifacts, and gates PR merge until all relevant tests pass; include a minimal YAML snippet showing path-based triggers, npm workspaces, and artifact publishing?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Databricks","MongoDB","Twitter"]},{"id":"q-1463","question":"How would you implement an Azure Pipelines YAML that tests a Node.js app against Node.js 14 and 16 using a matrix, caches npm dependencies with Cache@2 to speed up PR builds, and invalidates the cache when package-lock.json changes? Provide the YAML snippet and explain the key derivation?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["IBM","PayPal"]},{"id":"q-1486","question":"Design a blue/green canary rollout in AKS for a critical microservice using Azure Pipelines. Use Istio for traffic splitting, separate namespaces for canary and stable, and Azure Key Vault for per-environment secrets. Describe pipeline structure, gating with approvals, health checks, telemetry correlation across versions, auto-rollback, and cost controls?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Databricks","Microsoft","Scale Ai"]},{"id":"q-1513","question":"Design a scalable Azure Pipelines pattern for a multi-tenant SaaS app where each tenant has its own Azure Key Vault and database shard. Outline a per-tenant environment bootstrap, secret injection into deployments, governance checks (Azure Policy, approvals), telemetry tagging to prevent cross-tenant leakage, TTL-based teardown, and a robust rollback path with canary moves. How would you implement this end-to-end?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Apple","Bloomberg"]},{"id":"q-1535","question":"You have a small Python service in Azure Repos. Create a beginner CI pipeline in Azure Pipelines that: (1) runs on push/PR, (2) installs Python 3.11, (3) runs pytest with coverage, (4) builds a wheel and publishes it as a build artifact, and (5) gates release to Prod with a manual approval and a built-in security scan before promotion. How would you implement this?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Discord","MongoDB","Snap"]},{"id":"q-1644","question":"In Azure DevOps, you manage a beginner Node.js microservice stored in Azure Repos. Create a practical CI/CD flow (YAML) that: 1) triggers on push and PR, 2) uses Node.js 18, 3) runs npm ci and npm test -- --coverage, 4) builds a Docker image and pushes to a private Azure Container Registry, 5) updates a Helm chart in the repo and promotes to a staging environment with a manual approval gate and a basic readiness check. What files and steps would you implement?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Google","LinkedIn","Oracle"]},{"id":"q-1670","question":"Design a robust multi-region release strategy in Azure DevOps for a real-time collaboration service deployed to AKS. The pipeline must support per-region canary rollouts using Istio, region-scoped feature flags in Azure App Configuration, per-region secrets in Azure Key Vault, auto-rollback on SLA deviations, and cost controls via HPA and budget alerts. Describe the YAML structure, gating approvals, health checks, telemetry correlation across regions, and rollback triggers. What files and steps would you implement?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-devops-engineer"],"companies":["Microsoft","Netflix","Zoom"]},{"id":"q-1770","question":"Design a beginner CI/CD workflow in Azure DevOps for a Go microservice stored in Azure Repos, containerized and deployed to AKS. Requirements: 1) Triggers on push and PR; 2) go test ./... -cover and build the binary; 3) multi-stage Dockerfile to build and publish image to ACR; 4) Helm upgrade dev with values.dev.yaml; 5) manual approval gate before prod upgrade with values.prod.yaml; 6) readinessProbe and livenessProbe in deployment; 7) health endpoint for readiness and rollout status check in pipeline. What files and steps would you implement?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Hashicorp","Lyft","Oracle"]},{"id":"q-1850","question":"Design an auditable, region-aware release workflow in Azure DevOps for a global payment service deployed to AKS. The pipeline must promote a single immutable container image across regions (EU, US, APAC) in sequence, generate SBOMs, run security scans, enforce region-specific approvals, implement per-region health checks and rollback triggers, and record full provenance in Azure Artifacts. Describe the YAML structure, gating, and rollback strategy?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["IBM","Meta","Stripe"]},{"id":"q-1911","question":"Design a cross-region release pipeline in Azure DevOps for a high-availability service deployed to AKS across two regions. Use Istio for traffic splitting, per-region Key Vault secrets, and region-scoped feature flags via Azure App Configuration. Implement a single reusable YAML template shared by regions, with region-specific approvals, health checks, telemetry correlation, auto-rollback on cross-region latency SLA breach, and cost controls via HPA and budget alerts. What files and steps would you implement?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Lyft","Meta"]},{"id":"q-1927","question":"Design a tenant-aware release pipeline for a multi-tenant SaaS service deployed to AKS in two regions. Include per-tenant RBAC with Azure AD, per-tenant secrets in Key Vault, per-tenant feature flags in App Configuration, and per-tenant canary traffic routed via Front Door. Architect a policy-driven gate that rejects a release if any tenant SLA is missed, and implement auto-rollback on regional drift. Provide files, stages, and gates you would implement?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Airbnb","Cloudflare"]},{"id":"q-1955","question":"Design an Azure Pipelines release for an AKS-hosted data ingestion service that reads from MongoDB Atlas and writes to Snowflake. Roll out a new transformer to three regions (US, EU, APAC) with per-region approvals, blue/green deployment, and a runtime feature flag to switch transformers. Include data-quality gates, idempotent Snowflake upserts, telemetry lineage, and auto-rollback on ingestion errors within 24 hours. Provide YAML structure and gating strategy?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-devops-engineer"],"companies":["MongoDB","Snowflake"]},{"id":"q-1977","question":"Design an incident-driven rollback for a real-time analytics service deployed on AKS via Azure DevOps. When production alerts breach SLA (latency > 500ms or error rate > 2%), automatically rollback to the previous stable revision, shift traffic back using Istio, pause the Canary, and run post-rollback health checks and synthetic tests before resuming promotion. Describe pipeline changes, alert integrations, gating, rollback criteria, and telemetry correlation?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Citadel","Cloudflare"]},{"id":"q-2103","question":"Design a PCI-DSS compliant CI/CD workflow in Azure DevOps for a payments microservice deployed to AKS, with per-tenant data isolation, image signing, SBOM generation, policy gates, and attestation-based deployments. Describe YAML structure, Key Vault secrets management, runtime scanning, rollback criteria, and how you’d prove compliance. What files and steps would you implement?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Lyft","Tesla"]},{"id":"q-2138","question":"Design a zero-downtime fleet upgrade for 5,000 Azure IoT Edge devices across regions using **Azure Pipelines** to build and sign Edge modules, publish to a registry, and use **Device Update for IoT Hub** to orchestrate per-group rollouts. Include per-group approvals, region-specific configuration, telemetry correlation, and automatic rollback if update failures exceed threshold. How would you structure the pipeline, gating, and rollback criteria?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Apple","Plaid"]},{"id":"q-2186","question":"Design a beginner Azure DevOps YAML pipeline for a Python FastAPI app stored in Azure Repos and containerized for Azure App Service. The pipeline must trigger on pushes and PRs to main, run unit tests with coverage, build and push a Docker image to ACR, deploy to the App Service staging slot, run a smoke test on /health, and swap to production if the smoke test passes. Use Key Vault/App Configuration for per-environment secrets and propagate a correlation-id header for telemetry?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Coinbase","Lyft"]},{"id":"q-2239","question":"Design a beginner-friendly YAML pipeline in Azure Pipelines for a Python Flask app in Azure Repos deployed to Azure App Service (Linux). Include triggers on push and PR, test with pytest/coverage, lint, publish artifacts, and multi-stage deployments to dev, stage, prod with per-environment secrets from Azure Key Vault, approvals on Stage/Prod, and a post-deploy health check plus rollback path?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Bloomberg","Microsoft","MongoDB"]},{"id":"q-2266","question":"Design a real-world release approach in Azure DevOps for a globally distributed identity service on AKS. Implement progressive rollout by percentage with Azure Front Door routing; per-region namespaces; per-tenant isolation via RBAC; automated rollback on latency >300ms or error rate >1%; enforce per-env budgets via Azure Cost Management; telemetry correlation with Application Insights. Describe YAML structure, gating, health checks, and rollback criteria?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Apple","LinkedIn","Microsoft"]},{"id":"q-2327","question":"Create a beginner CI/CD YAML in Azure DevOps for a Python Flask app stored in Azure Repos, deploying to Azure App Service with two deployment slots: staging and production. Requirements: triggers on push and PR; install dependencies with pip, run pytest with coverage; deploy to staging slot; perform smoke tests against /health and /ready; if tests pass, require manual approval before swapping to production; on failure, rollback or revert swap; configure per-environment settings via deployment slots and Key Vault integration?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Databricks","PayPal"]},{"id":"q-2374","question":"Design an Azure DevOps pipeline strategy for a multi-tenant SaaS on AKS with per-tenant namespaces. Create a reusable YAML template that provisions per-tenant environments, pulls secrets from Azure Key Vault, assigns per-tenant RBAC, and enforces per-tenant budgets. Include dynamic environment creation, approvals, canary deployment with traffic routing, health checks, and automatic rollback on SLA breaches. Provide gating and rollback criteria?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Amazon","Apple","Meta"]},{"id":"q-2456","question":"In a monorepo with 50 microservices (AKS deployments and Azure Functions) and Azure Pipelines, design a YAML-driven CI for PRs that builds and tests only the services touched by the PR, produces per-service Docker images pushed to Azure Container Registry, uses per-service templates, runs unit/integration tests, caches dependencies, and gates deployment with per-service approvals. Describe structure, triggers, and rollback strategy?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-devops-engineer"],"companies":["Bloomberg","Goldman Sachs","Slack"]},{"id":"q-2482","question":"You are scaling a globally distributed real-time messaging microservice on AKS across 6 regions. You must implement a per-region progressive rollout using Istio traffic shifting, region-scoped feature flags from Azure App Configuration, per-tenant RBAC in Kubernetes, and cost controls via Azure Cost Management. Outline a concrete Azure DevOps YAML pipeline that does region-wise canary, approvals, readiness and synthetic health checks, telemetry correlation in Application Insights, and automatic rollback if p95 latency or error rate exceed thresholds?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-devops-engineer"],"companies":["Google","MongoDB","Twitter"]},{"id":"q-2556","question":"Design a beginner YAML pipeline in Azure DevOps for a Python FastAPI app stored in Azure Repos. Trigger on push/PR, install dependencies, run pytest with coverage, build a Docker image, push to ACR, deploy with Helm to AKS in dev/stage/prod namespaces using per-env values. Add a manual prod approval gate and a cost-forecast check (az costmanagement forecast); fail when forecast exceeds a threshold; include health checks and a rollback path on failure?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Google","Microsoft","Oracle"]},{"id":"q-2607","question":"Design a beginner Azure DevOps YAML pipeline for a Java Spring Boot REST API stored in Azure Repos, containerized and deployed to Azure App Service (Linux) with two deployment slots: staging and production. Include: triggers on push and PR; mvn -B -DskipTests=false test; build Docker image and push to ACR; deploy to staging; run a lightweight load test with k6 against /health and /metrics; if thresholds pass, require manual approval before swapping to production; on failure, rollback?","channel":"azure-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-devops-engineer"],"companies":["Cloudflare","Hugging Face","Snap"]},{"id":"q-1007","question":"A web app hosted in Azure App Service must securely call a private API hosted in an Azure Function behind a VNet. How would you implement private connectivity so traffic never leaves the Azure backbone? Include which resources you’d use, how to create a Private Endpoint for the API, DNS configuration, and how to restrict public access?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["DoorDash","Lyft","Salesforce"]},{"id":"q-1083","question":"An enterprise web app must enforce data residency per region, minimize egress costs, and meet real-time latency targets. Design an Azure-native, region-aware architecture that enforces residency, uses regional storage and a global entry point, and supports auditing and DR. Outline services, data replication, access control, and failover strategy?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Coinbase","Oracle","Scale Ai"]},{"id":"q-1110","question":"Scenario: A web API running in Azure App Service must persist user uploads to an Azure Storage account. Public access to the storage is disabled. How would you implement a Private Endpoint so the App Service talks to Storage over a private network, including enabling VNet integration, creating the Private Endpoint in the storage subnet, DNS configuration for privatelink, and any trade-offs?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Citadel","Coinbase","Goldman Sachs"]},{"id":"q-1212","question":"Scenario: a new web app stores and serves user-uploaded images globally. To keep costs predictable and delivery fast, which Azure services would you pair to store, serve, and secure access to images, and what trade-offs would you consider for storage tiering, CDN option, and access control?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Adobe","Hugging Face"]},{"id":"q-1242","question":"A startup runs a web app that stores user-uploaded images in Azure Blob Storage and serves them globally via CDN. Describe a practical storage design to minimize costs and latency: container layout, default and lifecycle tiers, lifecycle rules to auto-tier/delete, access control (RBAC vs SAS), and how you’d wire in CDN caching. Include concrete steps and example commands?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Cloudflare","IBM","Two Sigma"]},{"id":"q-1276","question":"Design an end-to-end Azure architecture for a globally distributed analytics platform servicing EU customers with strict data residency. Your plan should cover: data at rest with customer-managed keys, cross-region disaster recovery, private networking (Private Link/Endpoints), least-privilege access, encryption key rotation, and continuous policy compliance checks. Explain key services and trade-offs?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Hugging Face","Oracle"]},{"id":"q-1386","question":"You're building a real-time telemetry pipeline in Azure. Ingest devices via IoT Hub, route events to a Function App for normalization, and persist to an ADLS Gen2 data lake. Compare Event Grid vs Event Hubs for this pub-sub pattern, and justify your choice, including at-least-once delivery, backoff strategy, and observability requirements?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Microsoft","Square"]},{"id":"q-1491","question":"In a global fleet telemetry use-case, events from devices arrive across regions at high volume. Propose an end-to-end Azure pipeline that achieves sub-200 ms latency, ingests 200k events/sec per region, and writes to Delta Lake on ADLS Gen2 with exactly-once semantics and cross-region DR. Which components would you choose (Event Hubs, Functions/Durable Functions, Databricks or Synapse, Delta Lake, identity/security), and how would you implement idempotent sinks, retries, and cross-region failover?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Airbnb","Databricks","Discord"]},{"id":"q-1516","question":"A startup hosts a static frontend in Azure Blob Storage and a small API in Azure Functions. They want low costs, automatic scaling, and simple CI/CD using GitHub Actions. Design a beginner-level end-to-end setup: storage for static site, enable static website hosting, configure Functions with a Consumption plan, set up GitHub Actions for deployment, and outline basic security considerations?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["DoorDash","NVIDIA","OpenAI"]},{"id":"q-1580","question":"You have a REST API backend hosted in a private subnet (AKS/VM) that must be exposed to external partners. Requirements: IP allowlisting, DDoS protection, WAF, token-based authentication via Azure AD, and per-client rate limiting. Which Azure services would you assemble, and what specific configurations would you apply to meet these requirements while keeping backend private?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Coinbase","PayPal"]},{"id":"q-1710","question":"For a new Azure project with a static site on Blob Storage and a REST API on Azure Functions (Consumption), outline a beginner-friendly, least-privilege deployment setup: create an Azure AD service principal for GitHub Actions, assign minimal roles to deploy both resources, and use Azure Key Vault to store API keys. Include concrete roles and example CLI commands?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Citadel","Meta","Netflix"]},{"id":"q-1743","question":"Design a beginner-friendly, low-cost Azure webhook receiver that ingests JSON POSTs, runs in a Consumption-plan Function, validates an HMAC signature with a shared secret, stores each event in Azure Table Storage with PartitionKey as source and RowKey as GUID, and emits a basic success/failure metric in Application Insights. Outline steps and provide a minimal code snippet for signature verification in JavaScript?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Google","Microsoft","Snap"]},{"id":"q-1818","question":"A multi-tenant SaaS app must enforce strict per-tenant data isolation while serving a global user base with sub-100ms latencies. Propose an Azure-based storage and caching design (e.g., Cosmos DB with per-tenant partitioning vs relational sharding, caching strategy) and justify data residency, consistency, and failover choices. Include how you'd scale and monitor?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["DoorDash","Google","Hugging Face"]},{"id":"q-1829","question":"Advanced Azure Fundamentals: Design a cross-region real-time telemetry pipeline for a global IoT fleet (300k events/sec, 5 regions). Ingest with Event Hubs, process with Databricks Structured Streaming, and write to Delta Lake on ADLS Gen2 with exactly-once semantics. Implement geo-replication to a DR region, enforce data residency via Azure Policy and Purview, and ensure idempotent sinks with a tested failover process. What components and trade-offs would you choose, and how would you validate DR?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["IBM","MongoDB","NVIDIA"]},{"id":"q-1921","question":"In a multinational SaaS app where data residency is user-controlled, design an end-to-end Azure architecture that auto-scales with demand, minimizes latency across regions, and enforces per-tenant data isolation. Include data stores, messaging, compute, routing, security, DR, and a plan for idempotent processing with exact-once semantics?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Google","Netflix","Slack"]},{"id":"q-1983","question":"Design a beginner-level, end-to-end global web API deployment using two Azure regions. The API serves mobile clients worldwide, requires low latency, automatic regional failover, and basic security. Which Azure services would you use (Front Door, Traffic Manager, App Service, WAF), and outline the minimal wiring: two regional API endpoints, Front Door with a backend pool and health probes, WAF policy, and DNS configuration?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Citadel","MongoDB","Tesla"]},{"id":"q-2192","question":"Design a scalable, multi-tenant analytics pipeline on Azure for a SaaS app with globally distributed customers. Each tenant's data must be isolated, while the system ingests hundreds of thousands of events per second in real time. Propose an end-to-end architecture using Event Hubs (region-per-tenant or per-tenant partitions), Functions/Durable Functions, and a lakehouse on Delta Lake in ADLS Gen2 or Synapse. Explain per-tenant isolation, exactly-once semantics, CMEK with Key Vault, RBAC, cross-region DR, and cost governance; include a concrete rationale and trade-offs?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Bloomberg","Citadel","Discord"]},{"id":"q-2207","question":"In a globally distributed, multi-tenant BI platform, tenant data sovereignty and masked PII are required. Outline an Azure-native end-to-end architecture using Azure Purview for data catalog and classification, region-scoped ADLS Gen2 for raw data, Synapse Analytics with dynamic data masking and Row-Level Security, and Azure Key Vault for per-tenant keys. Explain how you enforce per-tenant data residency, masking, auditing, and cross-region governance?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Instacart","NVIDIA"]},{"id":"q-2232","question":"A beginner-level interview question: You have a web app storing user images in Azure Blob Storage. How would you implement a cost-conscious lifecycle strategy to automatically move infrequently accessed items to a cheaper tier and delete after retention, while preserving recoverability and basic governance?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Apple","Snowflake"]},{"id":"q-2274","question":"You're building a multi-tenant SaaS on Azure with tenants worldwide and strict data residency. You need low-latency reads globally, per-tenant data isolation, and predictable costs. Design a practical data layer using Cosmos DB with multi-region writes, throttling, and governance. How would you handle partitioning, consistency, backups, and DR while meeting residency constraints?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Bloomberg","Google","Microsoft"]},{"id":"q-2278","question":"Design a compliant, scalable governance baseline for a multi-tenant analytics data lake on Azure (ADLS Gen2, Databricks, Synapse). Customers require BYOK with rotation, per-tenant isolation, private endpoints, and auditable RBAC. Outline the architecture, enforcement using Azure Policy, key management with Key Vault CMK, data cataloging with Purview, and a reproducible deployment blueprint?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Databricks","LinkedIn","MongoDB"]},{"id":"q-2399","question":"In a multi-tenant Azure SaaS platform, describe how you would implement ephemeral administrator access to a production resource group using Privileged Identity Management (PIM). Include configuring eligible roles, approval workflows, maximum activation duration, MFA requirements, access reviews, and end-to-end auditing. How would you validate that least privilege is enforced and that there are no standing admin privileges outside the workflow?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Airbnb","LinkedIn"]},{"id":"q-2449","question":"Global e-commerce platform with two primary regions and a DR region. You must ensure low latency, strict data residency, and automated failover with per-tenant data isolation. Design the data layer and DR plan using Azure services. Compare Cosmos DB with multi-region writes vs SQL DB for per-tenant isolation, routing with Front Door, and storage with ADLS Gen2. Include governance controls (Policy/RBAC) and a practical DR test plan with RPO/RTO targets?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Lyft","NVIDIA","PayPal"]},{"id":"q-2551","question":"Design an Azure-native, cost-conscious, multi-region, multi-tenant log-aggregation pipeline for a regulated finance app. Ingest public API logs into Event Hubs, route to per-tenant landing zones in Delta Lake on ADLS Gen2, apply per-tenant schema enforcement, and implement exactly-once semantics with retries, plus cross-region DR. Choose components (Event Hubs, Functions/ Durable Functions, Synapse or Databricks, Delta Lake, Key Vault, Private Link) and outline governance, security, and cost-control measures (RBAC, PIM, CMK, budgets)?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Citadel","Databricks","Twitter"]},{"id":"q-2678","question":"Design a compliant data lake ingestion pipeline for a multi-tenant fintech using Azure Data Lake Gen2. Each tenant must have strict data isolation and immutability, fixed retention with legal holds, and auditable access trails. Ingest from diverse sources with Data Factory, catalog with Purview, process with Synapse or Databricks, expose a curated layer for BI, and implement cross-region DR. Include per-tenant RBAC and row-level security, plus cost considerations?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Bloomberg","IBM","Robinhood"]},{"id":"q-2706","question":"Scenario: You manage a three-subscription Azure environment (prod, staging, dev) for a global SaaS. You must enforce that no resource in prod or staging has a public IP, and automatically remediate exposures within 15 minutes. Describe the governance approach, policy initiative, remediation workflow, exemptions, and testing plan. Include integration with Private Endpoints and VNet peering to preserve connectivity while enforcing privacy?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Amazon","Bloomberg","Google"]},{"id":"q-859","question":"You have a REST API hosted on Azure App Service that processes user uploads and stores them in a separate Azure Blob Storage account. To avoid embedding secrets, how would you securely grant the API read/write access to the blob container using a managed identity? Outline the exact steps and roles you would apply, and mention any network considerations?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["Amazon","MongoDB","Two Sigma"]},{"id":"q-865","question":"An IoT platform deployed in Azure collects about 5 million device events per minute from global regions. You must build an ingestion and processing pipeline that guarantees at-least-once delivery with idempotent writes to Cosmos DB, tolerates regional outages, and minimizes cost. Describe the end-to-end architecture, data flow, dedup strategy, and monitoring?","channel":"azure-fundamentals","subChannel":"general","difficulty":"intermediate","tags":["azure-fundamentals"],"companies":["Discord","Scale Ai"]},{"id":"q-932","question":"Scenario: a multi-tenant SaaS app runs on Azure App Service and stores per-tenant files in separate containers in Azure Blob Storage. To avoid hard-coded keys, describe a secure pattern using managed identities and RBAC to grant the app the correct container access while ensuring tenant isolation and minimal permission surface. Outline steps, roles, and any network considerations (e.g., Private Endpoint)?","channel":"azure-fundamentals","subChannel":"general","difficulty":"beginner","tags":["azure-fundamentals"],"companies":["DoorDash","Oracle"]},{"id":"q-984","question":"In a globally distributed API using Azure Functions and Cosmos DB, implement per-tenant data isolation with minimal cross-region data transfer. How would you design the architecture, enforce least-privilege access via managed identities and RBAC, and ensure data residency with Private Endpoints and cross-region replication policies?","channel":"azure-fundamentals","subChannel":"general","difficulty":"advanced","tags":["azure-fundamentals"],"companies":["Bloomberg","MongoDB","Square"]},{"id":"q-1322","question":"Scenario: You operate a multi-tenant Azure landing zone with AKS clusters and storage across three tenants. Implement scalable tenant isolation and policy-driven security without per-VM NSG churn, while enabling selective cross-tenant analytics via Private Link. Propose a concrete design using Azure Firewall Manager, Private Endpoints, managed identities, and Azure Policy, plus a scale-test plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Google","Snap"]},{"id":"q-1358","question":"In a three-tenant Azure deployment, implement Just-In-Time privileged access for security admins across tenants using Lighthouse. Outline how you would provision roles, enforce time-bound activations, integrate approval workflows, and ensure continuous auditing and cross-tenant access reviews. Include a concrete rollout plan and rollback safety checks?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Apple","DoorDash","Oracle"]},{"id":"q-1411","question":"In a three-tenant Azure deployment hosting critical workloads (AKS and serverless), design a cross-tenant threat-hunting workflow that federates Defender for Cloud and Azure Sentinel telemetry into a shared analyst workspace using Lighthouse. Specify data schema, access controls, data residency, rollback steps if ingestion fails, rollout milestones, and success criteria?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Cloudflare","Lyft","Uber"]},{"id":"q-1421","question":"Design an end-to-end security model for a serverless data ingestion pipeline in Azure: per-tenant telemetry from IoT devices arrives at IoT Hub, then lands in a Gen2 Data Lake with per-tenant partitions. Outline tenant isolation, identity (RBAC, PIM), data encryption at rest (CMK), network controls (Private Link), and auditing. Include rollout steps and rollback checks?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Coinbase","LinkedIn","Meta"]},{"id":"q-1496","question":"In a three-tenant Azure deployment hosting microservices in AKS and tenant storage, design a scalable zero-trust inter-service access model that avoids NSG churn on every VM. Use Azure AD workload identity, Istio mTLS, Private Endpoints, and ABAC with policy-driven access. Explain how you would enforce least privilege, audit access, and plan rollout with rollback steps?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Amazon","Snap"]},{"id":"q-1538","question":"Scenario: in a three-tenant Azure deployment with shared automation pipelines and cross-tenant data access, design a scalable, auditable model to grant and revoke time-bound automation access without leaking credentials. Leverage Lighthouse as a central RBAC anchor, Azure AD app roles, PIM for Just-In-Time elevation, ABAC with claims, and CI/CD policy gates. Include rollout, rollback, and telemetry strategy?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Microsoft","Netflix"]},{"id":"q-1582","question":"Scenario: A single Azure subscription hosts an App Service and an Azure SQL Database. Harden security with Defender for Cloud, Private Endpoints for Storage, RBAC via two Azure AD groups (Developers and Admins), and basic access reviews and alerts. Provide concrete steps, a minimal RBAC mapping, and a rollout plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["IBM","Instacart"]},{"id":"q-1616","question":"In a tri-tenant Azure deployment (tenants A, B, C) hosting microservices in AKS and serverless functions, design a cross-tenant secret management strategy using Azure Key Vault. Include per-tenant vaults plus a central SharedKV, implement 90‑day secret rotations with automatic versioning, enforce cross-tenant access via Azure AD B2B and access reviews, and specify auditing, rollout, and rollback steps?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Google","Microsoft","Oracle"]},{"id":"q-1647","question":"In a two-tenant Azure deployment, Tenant Alpha hosts a data lake (ADLS Gen2) and Synapse pipelines; Tenant Beta hosts identity and apps. Design a concrete, auditable cross-tenant threat containment strategy to prevent data exfiltration and lateral movement during peak load. Include per-tenant Private Endpoints, Conditional Access, cross-tenant RBAC, Defender for Cloud alerts, Key Vault key rotation (90 days), and an automated rollback plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["IBM","Oracle","Twitter"]},{"id":"q-1656","question":"In a single Azure subscription with dev/test/prod resource groups, a CI/CD pipeline currently uses secrets in code. Propose a beginner-friendly, concrete strategy to manage secrets and identities using Azure Key Vault, managed identities, and least-privilege RBAC. Include steps for retrieval at deploy time, rotation readiness, and rollback checks?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Adobe","LinkedIn","Meta"]},{"id":"q-1689","question":"In a beginner Azure Security Engineer scenario, you manage a single subscription hosting an App Service and a SQL Database. Outline a concrete, auditable process to grant a developer time-limited access to both resources using Azure RBAC and Privileged Identity Management (PIM). Include onboarding, role provisioning, activation policy, approval workflow, logging, and a rollback plan if access is no longer needed?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Apple","Cloudflare","DoorDash"]},{"id":"q-1726","question":"In a three-tenant Azure deployment hosting microservices in AKS across tenants A, B, and C, design a scalable, zero-trust API security model where a central API gateway (in Tenant A) issues short-lived access tokens for per-tenant services, using OAuth2/OIDC with Azure AD, cross-tenant service principals, and mTLS between gateway and services. Include token lifecycle, per-tenant scopes, automatic revocation, cross-tenant auditing with Azure Monitor/Sentinel, and a rollback plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Databricks","Salesforce","Uber"]},{"id":"q-1773","question":"In a two-tenant Azure deployment hosting analytics workloads across TenantA and TenantB (ADLS Gen2 and Synapse), design a zero-trust data access model with per-tenant data boundaries and a central broker in TenantA. Explain how you would implement RBAC and CMK, cross-tenant Data Share approvals, managed identities, Private Endpoints, and centralized monitoring with Sentinel/ Defender, plus automated access reviews and anomaly alerts. Include rollout and rollback steps?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Airbnb","Citadel"]},{"id":"q-1857","question":"In a three-tenant Azure landing zone, implement centralized drift detection for security baselines across tenants using Azure Policy, Azure Monitor, and Azure Arc-enabled resources. Propose per-tenant policy scopes, remediation tasks, cross-tenant alerting, and a rollback plan including safe rollback for policy changes and resource remediation. Include a concrete rollout plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Microsoft","Tesla"]},{"id":"q-1931","question":"In a tri-tenant Azure landing zone hosting workloads on AKS and App Services, design a cross-tenant incident response workflow leveraging Azure Sentinel, Azure Lighthouse, and Defender for Cloud. Include roles, cross-tenant playbooks, containment steps, and rollback tests; specify automated evidence collection, cross-tenant alert correlation, and post-incident reporting. Outline rollout plan and rollback safety checks?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Netflix","Snap","Twitter"]},{"id":"q-2074","question":"In a two-subscription Azure environment (AppInfra and DataInfra) with CI/CD pipelines deploying to both, how would you implement service principal credential hygiene to prevent credential leakage? Outline steps to (1) create a dedicated SPN with least privilege, (2) rotate credentials automatically, (3) enforce ephemeral credentials for pipelines via Azure DevOps service connections with Managed Identity, (4) implement auditing via Defender for Cloud and Azure AD sign-in logs, and (5) a rollback plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Databricks","NVIDIA","Tesla"]},{"id":"q-2111","question":"In a tri-tenant Azure deployment (Tenants A, B, C) hosting AKS and serverless services, design a cross-tenant software supply chain security workflow that enforces artifact signing, image provenance, and runtime integrity. Specify the integration points across Azure DevOps, Azure Container Registry with content trust, Azure Policy, and Lighthouse for cross-tenant approvals; include per-tenant gates, rollback safety checks, and a concrete rollout plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Instacart","Lyft","Tesla"]},{"id":"q-2298","question":"In a tri-tenant Azure deployment hosting CI/CD pipelines and production workloads, design a cross-tenant incident isolation and rollback plan that automatically quarantines affected resources across tenants using Lighthouse, Defender for Cloud, and Azure Arc-enabled resources. Include per-tenant triggers, cross-tenant approval gates, rollback safety checks, and a concrete rollout plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Amazon","Bloomberg","Meta"]},{"id":"q-2300","question":"In a four-tenant Azure deployment (Tenants A–D) hosting AKS microservices and serverless backends, design a cross-tenant breach containment workflow triggered by Defender for Cloud alerts. Include per-tenant isolation steps (networks, identities), revocation of tokens, rotation of keys in Key Vault, traffic redirection, Lighthouse approvals, and a concrete rollout plan with rollback safety checks?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["IBM","Plaid","Tesla"]},{"id":"q-2334","question":"In a single Azure tenant hosting a web app and its containerized worker across two regions, implement a lightweight secure CI/CD gate that enforces artifact signing and image provenance before deployments to AKS, using Azure DevOps, ACR content trust, and a policy gate. Provide concrete steps, rollback, and testing plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Google","Snowflake","Uber"]},{"id":"q-2586","question":"In a multi-region Azure deployment hosting a global Data Lake (ADLS Gen2) and Synapse workspaces across three regions, design an automated data access governance workflow that enforces per-user, per-dataset, and per-column access with dynamic data masking and query-time enforcement. Use Azure Purview for classification/lineage, Synapse RBAC, Azure AD Conditional Access, and Privileged Identity Management; include rollout, rollback, and testing plan?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["Apple","Oracle","Snowflake"]},{"id":"q-2648","question":"Across a single Azure tenant with three environments (dev, test, prod) spanning multiple subscriptions, design a CMK-backed storage encryption baseline. Use Azure Policy to require customer-managed keys in Azure Key Vault for all storage accounts, enforce key rotation, and block cleartext backups. Include per-subscription scopes, remediation tasks, alerting, and a rollback plan. Provide concrete rollout steps and testing criteria?","channel":"azure-security-engineer","subChannel":"general","difficulty":"beginner","tags":["azure-security-engineer"],"companies":["Apple","Oracle"]},{"id":"q-898","question":"In a multi-cluster AKS deployment with rapid scale-out, design a workload-based segmentation pattern that avoids per-VM NSGs. Use Kubernetes NetworkPolicy/Calico, central allowlists, and Azure Policy for drift remediation. Include how to enforce per-service identity, automate policy checks, and detect violations with Defender for Cloud?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Bloomberg","Google","NVIDIA"]},{"id":"q-925","question":"In a multi-subscription Azure deployment with ephemeral, autoscaled workloads across AKS and VMs, how would you achieve scalable, workload-oriented segmentation without updating NSG rules on every VM? Propose a concrete design using Azure Firewall Manager, managed identities, Private Link, and policy-driven groupings, plus a plan to test at scale?","channel":"azure-security-engineer","subChannel":"general","difficulty":"advanced","tags":["azure-security-engineer"],"companies":["Discord","Microsoft"]},{"id":"q-968","question":"In a global Azure deployment with microservices spread across AKS clusters and VM Scale Sets, design a workload-based segmentation that avoids touching NSG rules on every VM. Outline a concrete end-to-end approach using Azure Firewall Manager, policy-based groups, Private Endpoints, workload tags, and Managed Identities. Include deployment steps, scale testing, and violation monitoring?","channel":"azure-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["azure-security-engineer"],"companies":["MongoDB","Snap","Uber"]},{"id":"q-1046","question":"You're building a regulated, multi-tenant analytics platform on Azure that ingests IoT and application logs from customers across three continents. Customers demand regional data residency while analytics must be global for cross-tenant benchmarks. Propose a practical, cost-conscious architecture that enforces per-tenant data isolation (at rest and in transit), regional ingestion, geo-redundant storage, cross-region analytics, and auditable access control using Azure native services. Include data plane vs control plane separation, and show how you'd satisfy RPO/RTO targets and regulatory requirements?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Databricks","Google"]},{"id":"q-1181","question":"You operate a fintech SaaS platform serving tenants across US, EU, and APAC. Each tenant's data must reside regionally at rest, yet global analytics require anonymized cross-tenant insights. Describe an Azure-native architecture that (1) enforces per-tenant data isolation in storage and processing, (2) supports real-time ingestion of fraud/transaction events, (3) enables cross-region analytics without tenant leakage, (4) meets DR targets with RPO <15 minutes and RTO <5 minutes, and (5) provides end-to-end auditing and governance. Include components, data flows, trade-offs, and a concrete failover test plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Plaid","Snap"]},{"id":"q-1305","question":"You manage a global healthcare analytics platform on Azure. Regulations require that **PHI** stays in-country while **non-PHI** can aggregate regionally. Propose an end-to-end data pipeline using **Azure Data Lake Storage Gen2**, **Data Factory**/Synapse, and **Purview** to enforce residency, enable regional analytics, and provide auditable data lineage and masking. Include encryption, governance, and failover strategies across regions?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Google","Instacart","OpenAI"]},{"id":"q-1365","question":"Design an Azure-based, EU-resident real-time trading analytics pipeline for a regulated fintech platform that must achieve sub-100 ms end-to-end latency, robust multi-region DR, and strict auditability. Outline the services, data flows, data residency, encryption (BYOK), governance, and cost controls you would implement?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Coinbase","Netflix","Robinhood"]},{"id":"q-1391","question":"You are tasked with building a EU-resident, real-time analytics platform with strict data residency and disaster recovery. Ingest events via Azure Event Hubs in the EU, land in a Data Lake Gen2 in the EU, process with Azure Synapse and Azure Databricks, and expose global analytics through external tables or Synapse Link. Include governance with Purview, BYOK via Key Vault, and private connectivity via Private Link/ExpressRoute. Design DR: RPO <5m, RTO <15m, and cost controls; outline testing plan (blue/green, chaos) and tradeoffs?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Hugging Face","Slack","Square"]},{"id":"q-1432","question":"For a new EU-resident SaaS app serving multiple tenants, you choose data isolation in Azure SQL Database. Compare using a single database with Row-Level Security (RLS) vs separate contained databases per tenant, focusing on cost, governance, backup/restore, and scale. Propose a concrete decision and outline the basic migration steps, including how you’d implement BYOK with Key Vault for per-tenant encryption and Azure AD authentication?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Google","Instacart"]},{"id":"q-1477","question":"For a EU-resident, multi-tenant SaaS app deployed in Azure, data residency requires tenant data to remain in EU while global analytics runs from a separate region. Design an end-to-end architecture that enforces per-tenant residency, enables cross-geo analytics, uses Arc-enabled data services, Cosmos DB, Synapse, and Purview, implements BYOK and private connectivity, and achieves DR with RPO<5m and RTO<15m. Include data flows, governance model, and testing plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Salesforce","Tesla"]},{"id":"q-1541","question":"Design a two-region, Azure-native DR for a beginner-friendly SaaS API with EU residency; propose a minimal architecture using Azure Front Door, App Service, and Azure SQL with geo-replication to achieve sub-minute RTO and RPO under 15 minutes; outline the failover process and a practical testing plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Hugging Face","Uber"]},{"id":"q-1642","question":"Design a cost-conscious EU-resident SaaS API hosted in Azure: propose a minimal architecture using Azure App Service, Azure SQL Database, and a public endpoint with autoscale and a regional failover to a secondary Azure region; detail traffic routing, RTO/RPO targets, and a practical testing plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Apple","Google","Netflix"]},{"id":"q-1652","question":"Design a beginner-friendly, Azure-native, event-driven ingestion path for a multi-tenant SaaS that streams user actions to analytics. Use a single Azure Event Hub, a Function with Event Hub trigger, and ADLS Gen2 as the sink. Include idempotent processing, dedup, backoff retries, and a simple tenant-scoped data schema. Outline validation steps to prove end-to-end latency under 2 minutes and zero data loss?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["LinkedIn","Oracle","Salesforce"]},{"id":"q-1696","question":"You’re designing a EU-resident, multi-tenant analytics platform that ingests in near real-time and serves tenant-isolated dashboards. Propose a cost-conscious Azure-based lakehouse architecture using ADLS Gen2, Event Hubs, Data Factory, and Synapse, detailing data isolation, per-tenant encryption with BYOK in Key Vault, and Azure AD-based access control; include a pragmatic migration path from a single-tenant baseline?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Adobe","Robinhood","Salesforce"]},{"id":"q-1730","question":"As the Azure Solutions Architect for a Zoom-scale platform, design an Azure-native, EU-resident data pipeline that ingests telemetry from MongoDB Atlas (Change Streams), streams it with minimal data loss, processes it in near real-time, and serves dashboards without EU data leaving the region. Outline data flow, services, DR, encryption (BYOK), and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["MongoDB","Zoom"]},{"id":"q-1781","question":"Design a beginner-friendly EU-resident telemetry pipeline for a gaming platform: ingest per-session data via Azure Event Hubs, process with Functions, store per-tenant data in Cosmos DB with Row-Level Security, and surface per-tenant dashboards in Power BI. Ensure data stays in the EU, implement BYOK, and outline DR and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Google","NVIDIA"]},{"id":"q-1797","question":"Design an Azure-native, EU-resident, cross-tenant data platform that ingests telemetry from an on-prem gateway fleet into Azure, processes it in near real-time while guaranteeing data sovereignty (EU only), uses BYOK with Key Vault, and supports tenant-level dashboards with RBAC. Outline data model, services, DR, and cost controls?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Adobe","Apple"]},{"id":"q-1841","question":"Design a beginner-friendly Azure-native telemetry pipeline for a fleet of IoT devices used by a mobile app, with EU residency constraints. Ingest device telemetry via Azure IoT Hub, preprocess at the edge with IoT Edge (sampling and filtering), route to Azure Functions for enrichment, and store per-tenant aggregates in Azure SQL with Row-Level Security. Expose dashboards in Power BI; include BYOK with Key Vault, DR planning, and cost levers; keep data in the EU region?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Airbnb","MongoDB","Robinhood"]},{"id":"q-1860","question":"Design an EU-resident Azure-native architecture for a real-time streaming recommendations engine serving EU users; telemetry is generated globally and must be processed entirely within the EU with no data leaving the region. Propose ingestion, real-time processing (sub-second latency), state storage, and serving path, tenant isolation, BYOK with Key Vault, private endpoints, and a regional DR plan with automated failover. Include concrete services and trade-offs?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["MongoDB","Netflix","PayPal"]},{"id":"q-1919","question":"EU-resident, regulated fintech SaaS: design a fully Azure-native, event-driven analytics platform that ingests on-prem transaction streams from a gateway into EU-region data plane; ensure tenants are isolated, data never leaves the EU, with BYOK in Key Vault, and automatic regional failover to a secondary EU region. Choose services (Event Hubs, Functions/Stream Analytics, Cosmos DB multi-tenant with per-tenant containers or databases, ADLS Gen2, Synapse), network controls (Private Link, VNets), security, cost levers, and migration steps. Provide data flow, DR plan, testing plan, and governance considerations?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Goldman Sachs","Hugging Face","Meta"]},{"id":"q-1943","question":"You're building an EU-resident telemetry observability layer for a real-time game platform. Telemetry ingested through EU-based Event Hubs is processed by Functions and stored per-tenant in Cosmos DB; dashboards display in Power BI. Propose an end-to-end observability design that enables per-tenant debugging without exporting data outside the EU. Include logs, tracing, retention, and testing?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["NVIDIA","Plaid"]},{"id":"q-1947","question":"Scenario: design a beginner-friendly **EU-resident** analytics pipeline for an Instacart-like app focusing on **support tickets** and **in-app events**. Ingest via **Azure Event Hubs** in the EU, enrich with **Functions**, store per-tenant metrics in **Cosmos DB** with **RLS**, and visualize in **Power BI**. Keep data EU-only, enforce **BYOK** via **Key Vault**, propose a 30-day retention and a purge workflow, plus a minimal **DR** plan and cost levers. Provide end-to-end data path and governance touches?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Instacart","Lyft"]},{"id":"q-2035","question":"You're building a EU-resident, multi-tenant fintech SaaS with strict data isolation. Propose a 2-region EU Azure-native data fabric that keeps each tenant's data isolated, supports real-time analytics and ML scoring, and enforces BYOK. Include data flow, services, governance, DR, and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Google","Hashicorp","Robinhood"]},{"id":"q-2180","question":"Design an Azure-native, EU-resident, edge-enabled telemetry platform for a real-time messaging app used by enterprise customers (Snap, Discord, Goldman Sachs). The system must ingest client telemetry at the edge (IoT Edge or SDKs), perform regional near-real-time aggregation, keep raw data within EU borders, implement BYOK with Key Vault, and serve dashboards with sub-second latency. Compare IoT Hub vs Event Hubs, edge compute placement, DR across two EU regions, and cost levers. Include a concise migration/testing plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Discord","Goldman Sachs","Snap"]},{"id":"q-2326","question":"You’re the Azure Solutions Architect for a beginner-friendly EU-resident ride-hailing platform onboarding cities as tenants. Propose a minimal, Azure-native onboarding pipeline that creates isolated per-city data stores, uses per-tenant encryption keys in Key Vault, and provides per-city dashboards via Power BI with Row-Level Security. Include data flow, services, cost levers, and a lightweight disaster-recovery plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Amazon","Lyft"]},{"id":"q-2344","question":"As the Azure Solutions Architect for a multi-tenant fintech platform with EU residency requirements, design a cross-tenant data-sharing and analytics pattern that preserves data sovereignty, minimizes data movement, and supports real‑time dashboards. Include governance with Azure Purview, data sharing mechanisms, encryption (BYOK via Key Vault), per-tenant isolation (RBAC/AD), streaming ingestion (Event Hubs), and analytics (Synapse or Databricks). Outline data flow, DR, and testing plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Netflix","PayPal","Snowflake"]},{"id":"q-2376","question":"Design an Azure-native, multi-region architecture for a real-time ad-bidding platform with per-tenant isolation for advertisers, ensuring data residency in chosen regions. Use Event Hubs, ADLS Gen2, Synapse or Databricks, and Confidential Computing. Include data flow, cross-region replication, BYOK, access controls, cost levers, and testing strategy?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Meta","Twitter"]},{"id":"q-2469","question":"As Azure Solutions Architect for a multi-tenant SaaS platform serving EU residents, design a secure, real-time analytics stack that ingests events from diverse SaaS apps via Event Hubs, stores per-tenant data in Delta Lake on Synapse, enforces per-tenant access with Azure AD RBAC, uses BYOK with Key Vault for at-rest keys, and applies cost controls and EU-region DR. What is the end-to-end data flow and governance plan?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Cloudflare","LinkedIn","Salesforce"]},{"id":"q-2550","question":"As Azure Solutions Architect for a high-velocity ride-hailing platform with EU residency, design a real-time fraud detection data fabric that ingests telemetry from mobile apps via Event Hubs, enriches with user risk signals from Cosmos DB, processes in near-real-time with Synapse Spark, stores a lakehouse in ADLS Gen2, and serves risk signals to dashboards and a monitoring API with per-tenant RBAC, BYOK via Key Vault, and active-active DR across regions. Compare architecture options: Synapse Spark vs Databricks. Outline data flow, governance, testing plan, and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Snowflake","Uber"]},{"id":"q-2568","question":"As Azure Solutions Architect for a EU-resident, multi-tenant SaaS, design a beginner-friendly data governance and catalog workflow that automatically discovers, classifies, and catalogs per-tenant data assets using Azure Purview, with tenant-scoped RBAC and BYOK, keeping data in EU, and delivering compliant lineage from Event Hubs through Delta Lake in Synapse to dashboards. Outline end-to-end data flows, services, and cost levers?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"beginner","tags":["azure-solutions-architect"],"companies":["Databricks","Meta"]},{"id":"q-2593","question":"As Azure Solutions Architect for a global streaming platform with stringent data sovereignty, design an end-to-end cross-cloud analytics stack that ingests telemetry from on-prem and multi-cloud edge devices, streams into Azure Event Hubs, writes per-tenant data into Delta Lake on Synapse, and serves dashboards with real-time insights via Databricks model scoring. Include BYOK, per-tenant RBAC, data contracts, governance with Purview, cross-region DR, and cost controls. Outline data flows, services, and failure modes?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Databricks","Netflix"]},{"id":"q-2719","question":"Design an Azure-native, per-tenant ML governance and real-time inference stack for a global, multi-tenant platform that keeps data regionally resident, isolates tenants, and supports model versioning, drift detection, and cost controls. Include data ingress, feature store, model registry, per-tenant endpoints, monitoring, DR, BYOK, RBAC, and governance tooling; justify services and trade-offs?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"advanced","tags":["azure-solutions-architect"],"companies":["Databricks","Hugging Face"]},{"id":"q-887","question":"You’re building a multi-tenant analytics platform on Azure for a consumer-brand SaaS product. Each tenant must have isolated data processing, with per-tenant data lake isolation, on-demand Spark/notebook compute that auto-suspends, and cost governance at the tenant level. Propose an architecture using Azure Data Lake Storage Gen2, Unity Catalog or RBAC, Synapse or Databricks, private endpoints, and auditing. How do you ensure data isolation, prevent cross-tenant leakage, and meet compliance while keeping ops simple?","channel":"azure-solutions-architect","subChannel":"general","difficulty":"intermediate","tags":["azure-solutions-architect"],"companies":["Instacart","Microsoft","Snap"]},{"id":"q-606","question":"How would you implement a rate limiter for a REST API to prevent abuse while ensuring legitimate users aren't blocked? Describe the algorithm and data structures you would use.","channel":"backend","subChannel":"api-design","difficulty":"intermediate","tags":["rate-limiting","api-design","redis","distributed-systems","backend"],"companies":["Google","Amazon","Twitter","Stripe","GitHub"]},{"id":"q-614","question":"How would you implement API rate limiting for a high-traffic service that needs to handle millions of requests per minute? Discuss the trade-offs between different algorithms and your approach for distributed systems.","channel":"backend","subChannel":"api-gateway","difficulty":"intermediate","tags":["rate-limiting","api-design","distributed-systems","redis","token-bucket","scalability"],"companies":["Google","Meta","Twitter","Stripe","Amazon","Netflix"]},{"id":"q-624","question":"How would you implement API rate limiting in a distributed system to prevent abuse while ensuring fair usage across multiple servers?","channel":"backend","subChannel":"api-infrastructure","difficulty":"intermediate","tags":["rate-limiting","redis","distributed-systems","api-design","scalability"],"companies":["Stripe","Twitter","GitHub","Google","Amazon"]},{"id":"q-611","question":"How would you implement API rate limiting to prevent abuse while ensuring fair usage for legitimate clients?","channel":"backend","subChannel":"api-middleware","difficulty":"intermediate","tags":["rate-limiting","api-design","middleware","redis","security"],"companies":["Twitter","GitHub","Stripe","Google","Amazon"]},{"id":"gh-46","question":"How would you design comprehensive API documentation that ensures smooth developer integration and reduces support overhead?","channel":"backend","subChannel":"apis","difficulty":"beginner","tags":["api","service-mesh"],"companies":["GitHub","LinkedIn","Microsoft","Postman","Stripe"]},{"id":"q-267","question":"Compare REST, GraphQL, and gRPC performance characteristics and identify optimal use cases for each protocol in modern microservices architecture?","channel":"backend","subChannel":"apis","difficulty":"beginner","tags":["rest","graphql","grpc","openapi"],"companies":["Amazon","Google","Microsoft","Netflix","Square","Stripe"]},{"id":"q-396","question":"You're building a microservice that needs to expose both REST and GraphQL endpoints for the same data model. How would you design the architecture to avoid code duplication while maintaining optimal performance for each query type?","channel":"backend","subChannel":"apis","difficulty":"intermediate","tags":["rest","graphql","grpc","openapi"],"companies":["Amazon","Booking.com","Citadel"]},{"id":"q-515","question":"You're building a REST API for a payment service. How would you design the endpoint for processing a payment, and what HTTP status codes would you return for different scenarios?","channel":"backend","subChannel":"apis","difficulty":"beginner","tags":["rest","graphql","grpc","openapi"],"companies":["PayPal","Twitter"]},{"id":"q-539","question":"What is dependency injection in Spring and how does it improve application design?","channel":"backend","subChannel":"apis","difficulty":"intermediate","tags":["spring","dependency-injection","ioc","design-patterns","java"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-1196","question":"In a production backend with multiple IdPs (OIDC providers and a SAML bridge), design a token validation strategy to prevent replay and bind tokens to a device/session. Outline how you would implement: (a) JWKS caching and per-provider key rotation, (b) replay protection using jti stored in a distributed cache with TTL, (c) token binding via mTLS or client certificate binding, and (d) cross-provider revocation propagation and token lifecycle (short-lived access tokens with refresh tokens)?","channel":"backend","subChannel":"authentication","difficulty":"intermediate","tags":["jwt","oauth2","oidc","saml"],"companies":["Goldman Sachs","Hashicorp","Twitter"]},{"id":"q-1240","question":"Within an enterprise multi-IdP setup (OIDC and SAML), you run an API gateway that issues short-lived JWTs for a service mesh. Propose a concrete bridge design that: (a) supports converting SAML assertions and OIDC tokens into token-bound JWTs with audience and scope constraints, (b) binds tokens to a device fingerprint and a one-time nonce, (c) supports PKCE-backed mobile/native flows and refresh token rotation, (d) provides revocation and token introspection across regions, and (e) prevents token replay in a globally distributed environment. Explain data flows, token formats, and security checks?","channel":"backend","subChannel":"authentication","difficulty":"advanced","tags":["jwt","oauth2","oidc","saml"],"companies":["Anthropic","MongoDB"]},{"id":"q-342","question":"You're implementing OAuth2 for a SaaS product. A user reports their access token works but refresh token fails. What are the top 3 causes and how would you debug each?","channel":"backend","subChannel":"authentication","difficulty":"intermediate","tags":["jwt","oauth2","oidc","saml"],"companies":["Cohere","Hulu","Spotify"]},{"id":"q-455","question":"Design a secure authentication system for a microservices architecture that supports JWT, OAuth2, and SAML. How would you handle token rotation, session management, and prevent token replay attacks across multiple services?","channel":"backend","subChannel":"authentication","difficulty":"advanced","tags":["jwt","oauth2","oidc","saml"],"companies":["OpenAI","Twitter"]},{"id":"q-544","question":"You're implementing SSO for an enterprise application using SAML 2.0. The IdP sends signed assertions but you're seeing intermittent 'Invalid Signature' errors. What are the most common causes and how would you debug them?","channel":"backend","subChannel":"authentication","difficulty":"intermediate","tags":["jwt","oauth2","oidc","saml"],"companies":["Adobe","Hashicorp"]},{"id":"q-1340","question":"How would you implement a tiered rate limiting system that provides different limits for free, premium, and enterprise customers while preventing users from bypassing limits by creating multiple accounts?","channel":"backend","subChannel":"backend","difficulty":"intermediate","tags":["rate-limiting","authentication","abuse-prevention","distributed-systems"],"companies":[]},{"id":"q-1116","question":"You're building a highly cached backend for a social app. **Redis** stores user profiles and feeds; **Memcached** caches post details. On a user profile edit, describe a precise, scalable strategy for **cache invalidation** that prevents stampedes, maintains consistency, and minimizes stale reads. Include data structures, TTLs, invalidation triggers, and atomic operations across **Redis** and **Memcached**, with concrete commands or pseudo-code?","channel":"backend","subChannel":"caching","difficulty":"advanced","tags":["redis","memcached","cache-invalidation"],"companies":["Apple","Google","Snap"]},{"id":"q-2663","question":"You're building a read-heavy analytics API that caches per-user daily summaries in Redis and Memcached. When a user performs an action that changes their summary, the cached entry must be invalidated and rebuilt on the next read. Design a practical cache invalidation approach that minimizes drift and avoids cache stampede. Include (a) the read path and data structures you would use, (b) how you trigger invalidation on writes, (c) handling bulk invalidation for many users in a single event, and (d) a basic testing approach to verify correctness?","channel":"backend","subChannel":"caching","difficulty":"beginner","tags":["redis","memcached","cache-invalidation"],"companies":["Netflix","Snap"]},{"id":"q-427","question":"You're building a user profile service that caches frequently accessed profiles. How would you implement cache invalidation when a user updates their profile, and what trade-offs would you consider between Redis and Memcached?","channel":"backend","subChannel":"caching","difficulty":"beginner","tags":["redis","memcached","cache-invalidation"],"companies":["Airbnb","Amazon","Google","Microsoft","Netflix","Snowflake","Stripe","Zoom"]},{"id":"q-443","question":"You're building a user profile API that caches user data in Redis. How would you implement cache invalidation when a user updates their profile, and what's the difference between using TTL vs explicit invalidation?","channel":"backend","subChannel":"caching","difficulty":"beginner","tags":["redis","memcached","cache-invalidation"],"companies":["Meta","MongoDB","NVIDIA"]},{"id":"q-330","question":"You're building a collaborative whiteboard app like Miro. When a user drags a shape, you need to update the UI immediately and persist the change. How would you implement this using CQRS?","channel":"backend","subChannel":"microservices","difficulty":"beginner","tags":["saga","cqrs","event-sourcing"],"companies":["Miro","Slack","Snowflake"]},{"id":"q-364","question":"You're building an order management system using CQRS with microservices architecture. How would you ensure data consistency between the write and read models when a command to create an order is processed, considering network partitions and potential service failures?","channel":"backend","subChannel":"microservices","difficulty":"beginner","tags":["saga","cqrs","event-sourcing"],"companies":null},{"id":"q-379","question":"You're building a distributed order processing system using the Saga pattern. How would you handle compensation when a payment service fails after inventory has been reserved?","channel":"backend","subChannel":"microservices","difficulty":"beginner","tags":["saga","cqrs","event-sourcing"],"companies":["Elastic","Epic Systems","Oscar Health"]},{"id":"q-666","question":"How would you implement a **saga**-driven checkout across services using **CQRS** and **event-sourcing**? Provide a concrete flow for an order touching Inventory, Payment, and Shipping: what commands and events you define, orchestration vs choreography, idempotency, compensating actions, and how read models are projected and kept consistent. Include reliability patterns like outbox and retries to ensure at-least-once delivery?","channel":"backend","subChannel":"microservices","difficulty":"intermediate","tags":["saga","cqrs","event-sourcing"],"companies":["DoorDash","OpenAI","Oracle"]},{"id":"q-667","question":"In a microservices backend for a retail platform, design a saga-driven workflow using CQRS and event sourcing across Order, Inventory, Payment, and Shipping. When an order is created, reserve inventory and authorize payment; on success, create shipping and complete the order. If inventory or payment fails, apply compensations (InventoryRelease, RefundPayment). Detail the event/command sequence, data in the event store, idempotency strategy, and orchestration vs choreography trade-offs?","channel":"backend","subChannel":"microservices","difficulty":"advanced","tags":["saga","cqrs","event-sourcing"],"companies":["Meta","Snowflake"]},{"id":"q-1095","question":"Design a globally distributed event store for a chat app where user_id determines the shard via consistent hashing. Each shard has 3 replicas in distinct regions; ingestion writes go to a leader replica and durably commit to all replicas using a 2-of-3 quorum. Reads are served from any replica with read-your-writes guarantees. Explain shard rebalancing without downtime, hot shard mitigation, cross-region replication lag, and failure recovery strategies?","channel":"backend","subChannel":"server-architecture","difficulty":"advanced","tags":["scaling","sharding","replication"],"companies":["Discord","MongoDB","Tesla"]},{"id":"q-249","question":"How would you implement a connection pool manager for aiohttp that handles graceful degradation under high load and connection timeouts?","channel":"backend","subChannel":"server-architecture","difficulty":"advanced","tags":["asyncio","aiohttp","concurrency"],"companies":["Airbnb","Amazon","Google","Meta","Microsoft","Netflix","Stripe","Uber"]},{"id":"q-2555","question":"Design a globally distributed event store for real-time analytics that shards by tenant_id and supports multi-region writes, cross-region replication, and per-tenant SLA guarantees. How would you choose shard keys, handle hot shards, implement idempotent writes, resolve conflicts, and perform resharding with minimal downtime?","channel":"backend","subChannel":"server-architecture","difficulty":"intermediate","tags":["scaling","sharding","replication"],"companies":["Databricks","PayPal","Zoom"]},{"id":"q-485","question":"You're designing a distributed database for a fintech platform handling 10M transactions/day. How would you implement sharding and replication to ensure strong consistency while maintaining 99.99% availability?","channel":"backend","subChannel":"server-architecture","difficulty":"advanced","tags":["scaling","sharding","replication"],"companies":["Amazon","Coinbase","Plaid"]},{"id":"q-568","question":"How would you design a database schema for a user authentication system that needs to handle 1 million users with proper indexing and sharding considerations?","channel":"backend","subChannel":"server-architecture","difficulty":"beginner","tags":["scaling","sharding","replication"],"companies":["Citadel","LinkedIn","Tesla"]},{"id":"q-1648","question":"In a scenario with a two-week deadline for a critical feature, you mediate between a senior engineer pushing for scope expansion and a PM pressing to cut scope to meet the deadline. Describe your mediation steps, the decision framework you use, and how you communicate the final plan and follow-ups?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"advanced","tags":["negotiation","mediation","feedback"],"companies":["Bloomberg","Meta","Twitter"]},{"id":"q-185","question":"Describe a specific situation where you had to resolve a technical disagreement with a difficult team member. What conflict resolution techniques did you use, and what was the measurable outcome?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"intermediate","tags":["communication","collaboration"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-2636","question":"How would you mediate a cross-functional negotiation where shipping a feature on a tight deadline clashes with potential technical debt? Outline your data collection, trade-off framing (impact vs risk), stakeholder involvement, and how you document decisions and gather feedback to close the loop?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"intermediate","tags":["negotiation","mediation","feedback"],"companies":["LinkedIn","Meta","PayPal"]},{"id":"q-312","question":"Tell me about a time you had to negotiate a solution between two team members with conflicting approaches?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"beginner","tags":["negotiation","mediation","feedback"],"companies":["Amazon","Google","Meta"]},{"id":"q-326","question":"Tell me about a time you had a conflict with a team member. How did you handle it and what was the outcome?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"beginner","tags":["negotiation","mediation","feedback"],"companies":["Crowdstrike","Salesforce","Tesla"]},{"id":"q-439","question":"Tell me about a time you had to mediate a conflict between two senior engineers who disagreed on a critical technical approach for a high-stakes project with a tight deadline?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"advanced","tags":["negotiation","mediation","feedback"],"companies":["Amazon","Apple","Google","Instacart","Meta","Microsoft","Netflix"]},{"id":"q-446","question":"Tell me about a time you had a disagreement with a teammate about how to approach a project. How did you handle it?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"beginner","tags":["negotiation","mediation","feedback"],"companies":["IBM","OpenAI"]},{"id":"q-486","question":"Tell me about a time you had to mediate a conflict between two senior engineers with opposing technical approaches. How did you handle it?","channel":"behavioral","subChannel":"conflict-resolution","difficulty":"advanced","tags":["negotiation","mediation","feedback"],"companies":["Microsoft","OpenAI","Square"]},{"id":"q-1039","question":"Describe a real incident in a fintech app where a release caused a customer-visible outage during market hours. Outline the explicit ownership and the immediate bias-for-action decision (hotfix vs rollback) with severity criteria, and how to ensure customer-obsessed restitution (transparent status updates, potential compensation, postmortem, and preventive steps)?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"beginner","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Google","Robinhood"]},{"id":"q-1243","question":"You're inheriting a mission-critical data pipeline that powers dashboards for a global customer; a nightly ETL job is failing in one region. Describe exactly how you would take ownership, act with urgency, and prioritize fixes while keeping customers informed. What trade-offs would you make and how would you measure success?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"advanced","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Airbnb","NVIDIA","Snowflake"]},{"id":"q-1969","question":"How would you own a cross‑functional initiative to ship a high‑impact feature under a tight deadline when early signals favor a different direction than customer feedback suggests? Describe your approach to ownership, bias-for-action, and customer obsession, including decision criteria, quick experiments, and stakeholder communication?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"intermediate","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Coinbase","Databricks"]},{"id":"q-2063","question":"In a Netflix-like app, a low-severity but high-visibility startup latency bug is reported by many users. Describe a concrete, end-to-end plan showing ownership, bias-for-action, and customer obsession: triage steps, quick hotfix, monitored rollout, rollback criteria, and a plan for a permanent fix with clear metrics and communication?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"beginner","tags":["ownership","bias-for-action","customer-obsession"],"companies":["NVIDIA","Netflix"]},{"id":"q-431","question":"Tell me about a time when you noticed a small issue that others overlooked. How did you take ownership and what was the impact?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"beginner","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Hugging Face","IBM","Tesla"]},{"id":"q-516","question":"Tell me about a time you had to make a critical decision with incomplete data. How did you balance speed vs accuracy?","channel":"behavioral","subChannel":"leadership-principles","difficulty":"intermediate","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Cloudflare","Coinbase","PayPal"]},{"id":"q-1273","question":"Describe a time you faced a technical disagreement in a cross-functional project and successfully influenced a decision without direct authority. What was the conflict, what steps did you take to communicate your view, what data or patterns supported you, and what was the outcome?","channel":"behavioral","subChannel":"soft-skills","difficulty":"beginner","tags":["communication","collaboration","influence"],"companies":["NVIDIA","Tesla"]},{"id":"q-1282","question":"Describe a time when influence changed a project's approach after data showed the initial plan underperformed. What data did you present, what communication channels did you use, how did you secure collaboration, and what was the outcome and learning?","channel":"behavioral","subChannel":"soft-skills","difficulty":"beginner","tags":["communication","collaboration","influence"],"companies":["Google","Tesla","Twitter"]},{"id":"q-2082","question":"Describe a time when a project failed due to miscommunication across cross-functional teams (engineering, product, and QA). What happened, how did you identify the root cause, what concrete steps did you take to restore alignment, and what would you do differently to prevent recurrence?","channel":"behavioral","subChannel":"soft-skills","difficulty":"beginner","tags":["communication","collaboration","influence"],"companies":["Citadel","Google","Oracle"]},{"id":"q-2154","question":"Describe a time when a project faced scope drift due to a miscommunication across teams. What steps were taken to clarify priorities, align stakeholders, and avoid future breakdowns? Include the channels you used, artifacts (like a decision log or a RACI matrix), and the outcome?","channel":"behavioral","subChannel":"soft-skills","difficulty":"beginner","tags":["communication","collaboration","influence"],"companies":["Google","IBM","Snap"]},{"id":"q-2421","question":"Describe a time you faced a tight deadline and needed to align multiple functions on a trade‑off decision. How did you map stakeholders, justify scope changes, communicate the rationale, and secure buy‑in to move forward?","channel":"behavioral","subChannel":"soft-skills","difficulty":"intermediate","tags":["communication","collaboration","influence"],"companies":["Apple","LinkedIn","NVIDIA"]},{"id":"q-2569","question":"Explain a situation where you had to persuade a cross-functional team to adopt a technical approach you believed was best. How did you diagnose concerns, tailor your communication, obtain buy-in, and ensure follow-through across teams?","channel":"behavioral","subChannel":"soft-skills","difficulty":"advanced","tags":["communication","collaboration","influence"],"companies":["IBM","Meta","Tesla"]},{"id":"q-297","question":"Tell me about a time you had to influence a senior stakeholder who disagreed with your technical approach. How did you handle it?","channel":"behavioral","subChannel":"soft-skills","difficulty":"advanced","tags":["communication","collaboration","influence"],"companies":["Amazon","Google","Meta"]},{"id":"q-913","question":"Tell me about a time you persuaded a cross-functional team to adopt a non-obvious technical approach. What was the situation, how did you influence without direct authority, what data or experiments did you rely on, what trade-offs did you communicate, and what was the result?","channel":"behavioral","subChannel":"soft-skills","difficulty":"intermediate","tags":["communication","collaboration","influence"],"companies":["Bloomberg","Google","Salesforce"]},{"id":"q-212","question":"How would you structure a STAR method response when describing a time you resolved a technical conflict between frontend and backend teams over API design, and what specific communication strategies did you employ?","channel":"behavioral","subChannel":"star-method","difficulty":"beginner","tags":["situation","task","action","result"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Postman"]},{"id":"q-2493","question":"Describe a time you shipped a small feature under a tight deadline. What was the situation and your task, what actions did you take to deliver on time, and what was the result? Include how you defined the MVP scope, handled trade-offs, and validated the outcome?","channel":"behavioral","subChannel":"star-method","difficulty":"beginner","tags":["situation","task","action","result"],"companies":["Robinhood","Square"]},{"id":"q-375","question":"Tell me about a time when you had to make a critical technical decision with incomplete data that impacted production systems. What was the situation, what data did you have, what decision did you make, and what was the result?","channel":"behavioral","subChannel":"star-method","difficulty":"advanced","tags":["situation","task","action","result"],"companies":["Amazon","Apple","Coinbase","Google","Meta","Microsoft","Netflix"]},{"id":"q-389","question":"Tell me about a time when you had to convince your team to adopt a new technology or approach that they were initially resistant to. What was the situation, what did you do, and what was the outcome?","channel":"behavioral","subChannel":"star-method","difficulty":"intermediate","tags":["situation","task","action","result"],"companies":["Expedia","IBM","Shopify"]},{"id":"q-569","question":"Tell me about a time you had to make a difficult technical decision with incomplete information under extreme pressure?","channel":"behavioral","subChannel":"star-method","difficulty":"advanced","tags":["situation","task","action","result"],"companies":["Citadel","Netflix","Tesla"]},{"id":"q-1073","question":"Implement transpose64(uint64_t x) by treating x as an 8x8 bit matrix in row-major order (bits 0-7 are row 0, 8-15 row 1, etc.). Return the transposed matrix (bit (r,c) moves to (c,r)). Use only bitwise ops and shifts; no loops or conditionals. Provide signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Cloudflare","Coinbase","Hashicorp"]},{"id":"q-1216","question":"Implement reverse128 for a 128-bit value stored as two 64-bit halves hi and lo. Provide a function:\n\nvoid reverse128(uint64_t hi, uint64_t lo, uint64_t *out_hi, uint64_t *out_lo);\n\nThe reversal should map bit i to bit 127 - i, using only bitwise operations and shifts (no loops or conditionals). Include a brief justification and 1-2 quick test examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Google","Snowflake","Stripe"]},{"id":"q-2040","question":"Implement countTrailingZeros64(uint64_t x) using only bitwise operations and shifts (no loops or conditionals). Return 64 if x == 0. Use a De Bruijn sequence with a 64-entry lookup table. Provide the function signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Oracle","Twitter","Uber"]},{"id":"q-2221","question":"In a 32-bit word handling module, implement rotateRight32(uint32_t n, uint32_t k) that returns n rotated right by k positions. Use only bitwise operations and shifts; no loops or conditionals. Assume 0 <= k < 32. Provide the function signature and a brief justification, plus 1–2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Google","MongoDB","Twitter"]},{"id":"q-2271","question":"Given a 64-bit bitboard representing an 8x8 grid (bit 0 = A1, bit 63 = H8), implement neighborMask64(uint64_t board) that returns a mask with a 1 where a bit has any of its eight neighboring cells set in the original board. Use only bitwise operations and shifts; no loops or conditionals. Provide the function signature and a brief justification, plus 1-2 quick tests?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Snap","Snowflake"]},{"id":"q-2354","question":"In bit-manipulation interviews, implement rotateRight64(uint64_t n, unsigned k) that rotates n right by k bits (0 <= k < 64). Use only bitwise operations and shifts; no loops or conditionals. Provide the function signature and a brief justification, plus 1-2 quick examples demonstrating the behavior?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Amazon","Google","Instacart"]},{"id":"q-2644","question":"Given a 64-bit unsigned integer n with k set bits, implement nextHigherWithSameBitCount(n) that returns the smallest y > n with popcount(y) == k; if no such y exists, return 0. Provide a function signature and 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Airbnb","Netflix"]},{"id":"q-2665","question":"Treat n as an 8x8 bit matrix in row-major order. Implement rotate90Clockwise64(uint64_t n) that returns the matrix rotated 90 degrees clockwise, using only bitwise operations and shifts (no loops or conditionals). Provide the signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["LinkedIn","Snap"]},{"id":"q-2723","question":"Implement a 32-bit Morton code generator morton2D(uint16_t x, uint16_t y) that interleaves bits of x and y (x bit 0 -> code bit 0, y bit 0 -> bit 1, etc.). Use only bitwise operations and shifts (no loops). Provide the signature and a brief justification, plus 1-2 examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Goldman Sachs","IBM"]},{"id":"q-680","question":"Given a 32-bit unsigned int n, implement a function hasAdjacentOnes(n) that returns true if n contains any two consecutive 1 bits (for example 0b1100100 has adjacent ones). Use only bitwise operations, no loops or lookups. Explain the core trick in a sentence?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Cloudflare","Oracle","Two Sigma"]},{"id":"q-689","question":"Given a 32-bit unsigned integer n, implement a function isSingleEvenBitSet(n) that returns true if exactly one bit is set and that bit lies at an even index (0, 2, 4, ...). Use only bitwise operations, no loops or built-in helpers. Provide the expression and a brief justification, plus a couple of quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["LinkedIn","MongoDB","Snowflake"]},{"id":"q-698","question":"Implement a 32-bit unsigned integer function popcount32(n) that returns the number of set bits in n using only bitwise operations and shifts, with no loops or built-ins. Use the SWAR (SIMD Within A Register) technique: apply a sequence of masks and shifts (0x55555555, 0x33333333, 0x0F0F0F0F) and a final multiply to consolidate counts. Provide the function and a brief justification, plus a couple of quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Google","Two Sigma"]},{"id":"q-705","question":"Implement reverseBits32(n) that reverses all 32 bits in a 32-bit unsigned integer using only a fixed sequence of bitwise operations (no loops or conditionals). Use a SWAR-style approach with masks 0x55555555, 0x33333333, 0x0F0F0F0F and 0x00FF00FF, finishing with a 16-bit half-swap. Provide the function signature and a brief justification, plus a couple of quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Coinbase","Snap"]},{"id":"q-711","question":"In a network packet parser, a 32-bit field uses trailing-zero count to encode the length of a value. Given a 32-bit unsigned n, implement countTrailingZeros32(n) that returns the number of trailing zero bits (0-32). If n==0, return 32. Use only bitwise operations, shifts, and basic arithmetic, no loops or built-ins. Provide signature, brief justification, and 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Amazon","Google","Uber"]},{"id":"q-724","question":"In memory allocation, implement nextPowerOfTwo32(uint32_t n) that returns the smallest 32-bit unsigned power-of-two >= n, given n > 0, using only bitwise operations with no loops or conditionals. If the result would overflow 32 bits, return 0. What is the correct implementation signature and approach?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Apple","IBM","Uber"]},{"id":"q-730","question":"Given a 32-bit unsigned integer n, implement swapAdjacentBits32(n) that returns a new 32-bit value with every adjacent bit pair swapped (bits 0-1, 2-3, ..., 30-31). Use only bitwise operations, no loops or conditionals. Provide the function signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Amazon","Plaid","Twitter"]},{"id":"q-738","question":"Implement a 64-bit bit reversal function reverse64(uint64_t n) that returns the bitwise reversal of n (bit 0 becomes bit 63, bit 63 becomes bit 0). Use only bitwise operations and shifts, no loops or conditionals. Provide the function signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Lyft","MongoDB","PayPal"]},{"id":"q-747","question":"Implement isBinaryPalindrome32(uint32_t n) that returns 1 if the 32-bit binary representation of n is a palindrome (bit0 equals bit31, bit1 equals bit30, etc.), and 0 otherwise. Use only bitwise operations and shifts; no loops or conditional branches. Provide the function signature and a brief justification, plus 1-2 quick examples.\n\nExamples:\n- 0x80000001 is a palindrome\n- 0xA5A5A5A5 is not a palindrome?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Coinbase","Google","Netflix"]},{"id":"q-751","question":"Implement popcount64(uint64_t x) that returns the number of set bits in x using only bitwise operations and shifts, with no loops and no built-in popcount. Use a fixed SWAR approach with masks 0x5555555555555555ULL, 0x3333333333333333ULL, 0x0F0F0F0F0F0F0F0FULL and a final multiply/shift step. Provide the function signature and a brief justification, plus 1-2 quick examples of inputs and outputs?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Lyft","MongoDB","Snap"]},{"id":"q-757","question":"Given a 32-bit unsigned integer n, implement maskBelowLSB32(n) that returns a 32-bit mask with all bits at positions <= the least significant set bit of n turned on; if n is 0 return 0. Use bitwise operations and shifts (and optional arithmetic). Provide the signature, justification, and 1-2 examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Meta","Microsoft","Zoom"]},{"id":"q-776","question":"Implement a 64-bit bitboard function knightAttacks64(n) that returns a 64-bit mask of all squares attacked by knights, given a 64-bit bitboard n where 1s indicate knight positions. Use only bitwise operations and shifts, no loops or conditionals. Provide the signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Bloomberg","Netflix","Plaid"]},{"id":"q-783","question":"Given a 32-bit unsigned n, implement hasPattern101(n) that returns true if there exists any i such that bits i, i+1, i+2 form 101 (n_i=1, n_{i+1}=0, n_{i+2}=1). Use only bitwise operations and shifts, no loops or conditionals. Provide the function signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Apple","Plaid","Slack"]},{"id":"q-791","question":"Implement parity32(n) that returns 1 if the number of set bits in a 32-bit unsigned n is odd, otherwise 0. Do not use loops or built-ins; only bitwise ops and shifts. Provide function signature parity32(uint32_t n) and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Databricks","DoorDash"]},{"id":"q-798","question":"Implement nextHigherWithSamePopcount64(uint64_t n) that returns the smallest integer greater than n with the same number of 1-bits. Use only bitwise operations and shifts; no loops or conditionals. If no such number exists, return 0. Provide the function signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Google","Two Sigma"]},{"id":"q-807","question":"Implement rotateLeft128 by k on a 128-bit value stored as two 64-bit words (hi, lo). The function rotates within the 128-bit boundary by k bits (0 <= k < 128) using only bitwise operations and shifts, no loops or conditionals. Provide the signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"advanced","tags":["bit-manipulation"],"companies":["Google","Meta","Twitter"]},{"id":"q-814","question":"Implement interleave16(uint16_t a, uint16_t b) that returns a 32-bit value with bits interleaved as a0 b0 a1 b1 ... a15 b15, where a0 is LSB of a and b0 is LSB of b. Use only bitwise operations and shifts (no loops or conditionals). Provide the function signature and a brief justification, plus 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"intermediate","tags":["bit-manipulation"],"companies":["Adobe","Snowflake","Zoom"]},{"id":"q-823","question":"Implement rotateLeft32(uint32_t n, unsigned int k) that returns the 32-bit value formed by rotating n left by k bits. Constraints: no loops or conditionals; handle k >= 32 by using k % 32. Provide the function signature and a brief justification, and give 1-2 quick examples?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Citadel","LinkedIn","Stripe"]},{"id":"q-830","question":"Implement xor32(uint32_t a, uint32_t b) that returns a ^ b without using the ^ operator. Use only &, |, ~ and shifts. Provide the function signature and a brief justification, plus 1-2 quick examples to verify correctness?","channel":"bit-manipulation","subChannel":"general","difficulty":"beginner","tags":["bit-manipulation"],"companies":["Instacart","Netflix"]},{"id":"q-1091","question":"Scenario: An OTA firmware update causes GPS altitude drift in a subset of IoT devices across regions. Design a CAPA program to detect, collect evidence, and prevent recurrence. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API for CAPAs with device logs, 4) region/device-type metrics (recurrence rate, MTTR, false positives), 5) an RCA template and a canary rollback plan for OTA updates?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Plaid","Snowflake","Tesla"]},{"id":"q-1111","question":"Scenario: Production ML feature store drift after a data refresh degrades latency and CTR across regions due to stale features and late streaming data. Propose a CAPA program: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPAs, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region-aware metrics to prove containment and recurrence, 5) an RCA template and a canary rollout plan to prevent recurrence during future refreshes?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["LinkedIn","Tesla"]},{"id":"q-1132","question":"Scenario: batch ingestion misses PII masking in two tenants across regions, raising privacy risk. Design a CAPA program: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) tenant/region-aware metrics to prove containment and recurrence (MTTD, MTTR, false positives), 5) an RCA template and a canary rollout plan to validate policy enforcement before global deployment?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Coinbase","MongoDB","Salesforce"]},{"id":"q-1156","question":"Scenario: A distributed streaming analytics pipeline processes click events for an online marketplace. A recently deployed shard rebalancing causes out-of-order events in two regions, leading to incorrect revenue attribution and fraud alerts. Design a CAPA program that covers: 1) a CAPA data model that captures evidence (events, traces, timestamps, orderings) and artifacts (config, manifest, canary results), 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces and event metadata, 4) region- and shard-aware metrics to prove containment (recurrence rate, MTTR, misattribution rate), 5) an RCA template and a canary-rollback plan for shard rebalancing?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Amazon","Apple"]},{"id":"q-1200","question":"Scenario: A global real-time telemetry platform for autonomous vehicles experiences intermittent PII exposure due to a log-redaction misconfiguration after a software update in two regions. Design a CAPA program to detect, document, and prevent recurrence. Include: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) tenant- and region-aware metrics (recurrence rate, MTTR, false positives), 5) an RCA template and a canary-based rollback/patch plan, 6) a policy gate preventing deployment until redact coverage is above threshold?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["MongoDB","Tesla"]},{"id":"q-1426","question":"Scenario: A multi-tenant SaaS billing system release unintentionally changes pricing rules for a subset of tenants, triggering incorrect invoices and revenue churn. Design a beginner CAPA program to address this. Your task: 1) propose a CAPA data model capturing evidence (invoices, logs, traces) and artifacts (config, feature flags, pricing rules); 2) define a lifecycle state machine for CAPA progression; 3) outline a minimal REST API to create/update CAPAs with linked billing events; 4) specify tenant- and plan-level metrics to prove containment (recurrence rate, MTTR, false positives); 5) provide an RCA template and a canary-based patch plan before full rollout; 6) draft a simple policy gate that prevents deployment until pricing rules pass a preflight check?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Hashicorp","Zoom"]},{"id":"q-1450","question":"Scenario: A global multi-tenant data platform implements a new consent-logging feature. After rollout, a bug in the consent service causes PII redaction failures in two regions, risking data exposure and compliance violations. Design a CAPA program to detect, collect evidence, and prevent recurrence. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region- and tenant-aware metrics (recurrence rate, MTTR, false positives), 5) an RCA template and a canary rollout plan to validate fixes before global deployment?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Citadel","LinkedIn","Stripe"]},{"id":"q-1485","question":"**Scenario**: A cross-tenant data export feature in a multi-tenant SaaS app accidentally exposes data from unrelated tenants during a canary rollout in two regions. Design a beginner **CAPA** program to detect, document, and prevent recurrence. Your task: 1) propose a CAPA data model framing evidence and artifacts, 2) define a lifecycle state machine, 3) outline a minimal REST API to create/update CAPAs with linked logs/traces, 4) specify tenant-scoped metrics for containment, 5) provide an RCA template and a canary-based fix plan to validate before broader rollout?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Salesforce","Twitter"]},{"id":"q-1502","question":"Scenario: A new analytics feature ingests raw customer IDs into a 3rd-party BI dataset due to a masking policy misconfiguration in the data pipeline. Design a CAPA program to detect, document, and prevent recurrence. Your tasks: 1) propose a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) tenant-scoped metrics for containment and recurrence, 5) an RCA template and a canary-based remediation plan to validate before rollout?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Oracle","PayPal","Tesla"]},{"id":"q-1528","question":"Scenario: A vendor data feed for pricing and product metadata experiences occasional delays and duplicates during a quarterly refresh, causing price mismatches in two regions. Design a beginner CAPA program to address this. Your tasks: 1) propose a CAPA data model capturing evidence and artifacts, 2) define a lifecycle state machine, 3) outline a minimal REST API to create/update CAPAs with linked feed records, 4) specify region- and tenant-aware metrics to prove containment, 5) provide an RCA template and a canary-based preventive action plan to validate before broader rollout?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Adobe","Coinbase","Microsoft"]},{"id":"q-1556","question":"Two regions saw bids inflated after a cache invalidation caused an expired pricing model to be applied in an ad-bidding pipeline. Design a CAPA program to detect, document, and prevent recurrence. Your task: 1) CAPA data model for evidence and artifacts, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with logs/traces, 4) region-asset metrics for containment (recurrence rate, MTTR, false positives), 5) RCA template and a canary cache-invalidation plan before rollout?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Discord","DoorDash","LinkedIn"]},{"id":"q-1635","question":"A distributed cache layer across two regions intermittently serves stale reads after a deployment; design a beginner CAPA program to detect, document, and prevent recurrence?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Microsoft","MongoDB"]},{"id":"q-1745","question":"Scenario: In a multi-tenant payments platform serving PayPal, Lyft, and Coinbase, a policy change to transaction masking and logging fails to propagate to streaming and analytics pipelines in two regions, raising privacy risk and audit gaps. Design a CAPA program to address this: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces and policy versions, 4) region-aware metrics to prove containment (recurrence rate, MTTR, false positives), 5) an RCA template and a canary rollback plan for policy changes?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Coinbase","Lyft","PayPal"]},{"id":"q-1848","question":"Design a CAPA for drift in an ML fraud detector across regions: 1) CAPA data model with driftIndex, affectedFeatures, modelVersion, evidence and artifact links; 2) lifecycle: detected → investigating → containment → RCA → closed; 3) minimal REST API to create/update CAPAs with logs and model snapshots; 4) region metrics: drift magnitude, MTTR, FPR/TPR changes; 5) RCA and canary rollback plan?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Cloudflare","Netflix","Uber"]},{"id":"q-1899","question":"Scenario: A real-time analytics service relies on a 3rd-party enrichment API. Under peak load, enrichment latency spikes cause backpressure and data loss in two regions. Design a beginner CAPA program to address this. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with evidence, 4) region-aware metrics to prove containment, 5) an RCA template and a canary-based fallback plan?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Meta","Netflix","Twitter"]},{"id":"q-1952","question":"Scenario: A multi-tenant data platform powering market data feeds experiences cross-tenant data leakage under peak load. Design a CAPA program to detect, contain, and prevent recurrence. Include: 1) a CAPA data model capturing tenantId, policyId, incidentId, evidence and artifact links; 2) a lifecycle state machine; 3) a minimal REST API to create/update CAPAs with tenant-scoped logs and traces; 4) tenant-aware metrics (recurrence, MTTR, spillover rate); 5) RCA template and a canary policy rollout plan?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Meta","MongoDB","Robinhood"]},{"id":"q-2132","question":"Scenario: A multi-region real-time bidding system processes ad events via Kafka across us-east-1 and eu-west-1. A recently deployed bid normalization microservice causes timeouts and mispricing, leading to revenue variance and increased false positives in two regions. Design a CAPA program that covers: 1) a CAPA data model capturing evidence, artifacts, and cross-region traces; 2) a lifecycle state machine for CAPA progression; 3) a minimal REST API to create/update CAPAs with linked logs/traces; 4) region-aware metrics to prove containment (recurrence rate, MTTR, revenue delta, false-positive rate); 5) an RCA template and a canary rollout plan for the normalization service?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Amazon","Salesforce","Slack"]},{"id":"q-2174","question":"Scenario: A data-access policy violation occurs when a misconfigured feature-flag rollout temporarily allows cross-tenant data access in two regions. Design a CAPA program to detect, document, and prevent recurrence. Include: 1) a CAPA data model capturing lineage, access changes, and remediation artifacts; 2) a lifecycle state machine; 3) a minimal REST API to create/update CAPAs with linked logs/traces and policy changes; 4) region- and tenant-scoped metrics; 5) an RCA template and a canary-based remediation plan?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Microsoft","Twitter"]},{"id":"q-2228","question":"Scenario: A webhook-driven notification service intermittently loses messages in two regions after deploying a new retry-backoff policy. Design a beginner CAPA program to address this. Your task: 1) propose a CAPA data model, 2) define a lifecycle state machine, 3) outline a minimal REST API to create/update CAPAs with evidence, 4) specify practical, region-aware metrics to prove containment, 5) provide an RCA template and a canary-based rollback/test plan before full rollout?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Bloomberg","Instacart","LinkedIn"]},{"id":"q-2363","question":"In a multi-region streaming analytics platform, a surge triggers backpressure and late events in two regions. Design a CAPA program that focuses on data-plane backlogs affecting downstream attribution. Include: 1) a CAPA data model capturing evidence and artifacts, 2) a region/shard-aware lifecycle, 3) a minimal REST API to create/update CAPAs with logs/traces, 4) metrics proving containment (latency drift, backlog depth, MTTR), 5) RCA template and a canary rollout for a dynamic circuit-breaker in the ingestion path?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["LinkedIn","Snap"]},{"id":"q-2393","question":"Scenario: A cost spike in cloud spend occurs due to a runaway batch job in two regions; design a beginner CAPA that addresses this. Include: 1) a CAPA data model, 2) a lifecycle state machine, 3) a minimal REST API to create/update CAPAs with evidence, 4) region-aware metrics to prove containment, 5) an RCA template and a canary-based remediation plan?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Instacart","Snowflake","Twitter"]},{"id":"q-2430","question":"Scenario: A multi-region analytics platform begins exporting customer PII to a BI partner after a schema change, risking regulatory non-compliance. Design a CAPA program to address this: 1) CAPA data model for evidence and artifacts, 2) lifecycle state machine, 3) minimal REST API to create/update CAPAs with linked logs and data-access policy checks, 4) region-aware metrics (policy-violation rate, MTTR, false positives), 5) RCA template and a canary rollout plan with automated data redaction?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Tesla","Two Sigma"]},{"id":"q-2611","question":"Scenario: A global data-analytics pipeline intermittently drops events in two regions during peak load due to a misconfigured ETL window. Design a beginner CAPA program: 1) CAPA data model, 2) lifecycle state machine, 3) a minimal REST API to create/update CAPAs with evidence, 4) region-aware metrics to prove containment and prevent recurrence, 5) RCA template and a canary-based remediation plan. Keep it implementable with simple tooling?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["LinkedIn","MongoDB","Square"]},{"id":"q-841","question":"Design a CAPA workflow for a high-volume platform (Airbnb/LinkedIn scale). The system must log incidents, perform RCA, implement corrective and preventive actions, and verify outcomes before closing. Provide: 1) a CAPA data model, 2) a lifecycle state machine, 3) an API surface to create/update CAPAs, 4) metrics to prove effectiveness (recurrence rate, time-to-close)?","channel":"capa","subChannel":"general","difficulty":"advanced","tags":["capa"],"companies":["Airbnb","LinkedIn"]},{"id":"q-937","question":"Scenario: A post-rollout incident caused latency spikes and higher error rates for a subset of regions when a new feature flag was enabled. Design a beginner-friendly CAPA to address this. Your task: 1) propose a CAPA data model, 2) define a lifecycle state machine, 3) outline a minimal REST API to create/update CAPAs with evidence, 4) specify practical metrics to prove effectiveness, 5) provide a simple RCA template and a canary-based preventive action you would test before full rollout?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["DoorDash","Twitter"]},{"id":"q-965","question":"Scenario: A multilingual moderation model update causes spikes in unsafe content in two locales. Design a beginner CAPA plan focusing on locale-scoped evidence, drift checks, and a safe rollback with feature flags. Include: 1) a CAPA data model, 2) a lifecycle machine, 3) a minimal REST API to capture CAPAs with evidence, 4) locale-specific success metrics, 5) an RCA template and a locale-specific canary plan for preview before global rollout?","channel":"capa","subChannel":"general","difficulty":"beginner","tags":["capa"],"companies":["Apple","Hugging Face"]},{"id":"q-986","question":"Scenario: After a schema evolution in the event ingestion pipeline, latency spikes and incorrect bids appear in two regions. Design a CAPA program with: 1) a CAPA data model capturing evidence and artifacts, 2) a lifecycle state machine for CAPA progression, 3) a minimal REST API to create/update CAPAs with linked logs/traces, 4) region-aware metrics to prove effectiveness (recurrence rate, mean time to containment, false-positive rate), 5) an RCA template and a canary rollout plan to validate before global deployment?","channel":"capa","subChannel":"general","difficulty":"intermediate","tags":["capa"],"companies":["Citadel","Oracle","Uber"]},{"id":"q-1020","question":"You’re evaluating a beginner feature: a daily market digest in a Robinhood-like app. Do a practical cost‑benefit analysis: state assumptions, estimate data/API costs, storage and engineering time, quantify benefits (retention lift, ARPU), and compute break-even time. Provide a rough calculation and rationale?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["MongoDB","NVIDIA","Robinhood"]},{"id":"q-1052","question":"Scenario: Real-time ingestion service receives JSON events from edge devices. Guarantee per-user throughput while allowing bursts in a distributed cluster. Design and implement a practical throttling mechanism, specify data structures, atomicity (e.g., Redis Lua script), failure modes, testing strategy, and observability?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["NVIDIA","OpenAI","Tesla"]},{"id":"q-1057","question":"In a real-time feed system using a contextual bandit with attention weighting (CBA), design a policy that balances short-term CTR and long-term engagement. Explain your reward decomposition, exploration strategy, and handling of non-stationarity. How would you validate offline with CPE and ramp online safely? Provide a concise update rule?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Hugging Face","Microsoft","Uber"]},{"id":"q-1084","question":"Given a large social network planning to adopt a real-time feature flag evaluation service that runs on a hybrid stream/batch pipeline. Current pipeline: 1.2M events/sec, median latency 50 ms, 5% tail. New service promises 20–30% latency reduction and 25% cost increase, plus migration risk. Perform a cost-benefit analysis: quantify costs, benefits, risks, horizon (12 months), and decision rule with sensitivity ranges?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["LinkedIn","Meta","Snowflake"]},{"id":"q-1114","question":"You’re migrating a real-time feature flag engine (multi-tenant SaaS) to reduce tail latency and cost. Propose a three-phase migration: centralized eval, regional edge gateway, then hybrid routing with per-tenant caches. Specify success metrics, rollback criteria, and a 12-month plan?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["DoorDash","Hashicorp","Two Sigma"]},{"id":"q-1151","question":"In a real-time analytics system for engagement on a large social app, design a privacy-preserving cohort analytics pipeline that ingests ~3M events/sec with sub-100 ms latency per region. Requirements: data residency, differential privacy for cohort counts, delta- or exact-once streaming state, drift detection, and cost-conscious multi-region deployment. Outline architecture, data contracts, and trade-offs?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Coinbase","Meta","Snap"]},{"id":"q-1209","question":"You’re building an offline-first version of a daily digest app for low-connectivity users. Outline a minimal data model, eviction policy, and a practical plan to quantify cost savings from reduced network usage versus increased storage and complexity; provide a concrete example with rough numbers?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Adobe","DoorDash","Goldman Sachs"]},{"id":"q-1238","question":"You're evaluating a beginner feature: a daily 'Portfolio Health Snap' panel that assigns a risk score to each user based on volatility of top holdings. Do a practical cost-benefit analysis: state assumptions, data/API costs, storage, and engineering time; quantify benefits (retention lift, ARPU) and compute break-even time. Provide rough calculations and rationale?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Adobe","Square","Two Sigma"]},{"id":"q-1274","question":"You're assessing migrating a high-volume telemetry pipeline from a centralized data warehouse to a lakehouse with streaming ingestion and on-demand materialized views. Current throughput 5M events/sec, latency 5–7 min; target latency 2–3 min, 25% cost increase. Build a 12-month cost-benefit model: incremental storage/compute, streaming infra, data egress, drift/rollback costs; specify decision rules and sensitivity ranges?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Bloomberg","Citadel","Slack"]},{"id":"q-1403","question":"Design a real-time, fault-tolerant order processing pipeline for a high-traffic ecommerce system. Partition Kafka by user_id, use transactional producers for exactly-once ingestion, and idempotent consumers keyed by event_id. Maintain a durable log, emit to a warehouse via an idempotent sink, and support replay via event sourcing and schema evolution. Include chaos and backpressure tests?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Goldman Sachs","Instacart","Square"]},{"id":"q-1472","question":"Design and implement a cost-benefit analysis module for a feature-flag rollout in a global streaming platform. Given 1000 edge regions, 1M users, latency budget 50ms, and known incremental costs, specify a data model, metrics to collect, uplift estimation, and a core function that returns whether to roll out. Include rollout policy and plan for validation via A/B tests in production?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Bloomberg","Microsoft","Netflix"]},{"id":"q-1508","question":"You're deploying a real-time personalization pipeline where a new ML model runs behind a feature flag with a 1% canary. If live drift is detected against offline benchmarks and latency deviates beyond a threshold, outline a concrete plan for rollout control, rollback, and data integrity checks that minimizes user impact while preserving auditability?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Airbnb","Apple","Hashicorp"]},{"id":"q-1531","question":"Context: A streaming feature store for real-time personalization must decide between two deployment models under GDPR/CCPA constraints: (A) cloud-region-centric with EU data replicated to a central US region for global analytics, delivering tail latency 30–70 ms; (B) EU-first on-prem data plane with a lightweight cloud cache for global lookup, targeting 50–120 ms tail latency. Provide a 12–18 month cost model including data residency compliance, data transfer, storage, compute, tooling, outage risk, and a decision rule with sensitivity ranges?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Databricks","Google","Two Sigma"]},{"id":"q-1724","question":"You operate a real-time feature store for a global e-commerce platform. Ingested events arrive from multiple regions with clock skew and occasional late arrivals. You must deliver online feature values with tail latency under 100 ms while ensuring correctness when late data retroactively updates aggregates (e.g., cart_value_last_24h). Propose the architecture, covering data versioning, watermarking strategy, exactly-once processing, per-tenant isolation, and testing approach. Include a concrete late-event example and the system’s expected behavior?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Coinbase","IBM","Meta"]},{"id":"q-1817","question":"Design and describe a scalable, fault-tolerant real-time collaboration pipeline: ingest per-document operations via a durable queue (Kafka), assign strict sequence numbers, and ensure exactly-once processing with idempotent workers. Explain data model, ordering guarantees, CRDTs vs OT, cross-region replication, failure modes, tests, and rollback strategy. How would you implement end-to-end?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Adobe","Google","Zoom"]},{"id":"q-1874","question":"You’re evaluating a beginner feature: an in-app guided onboarding widget for a CRM product. Do a practical cost-benefit analysis: state assumptions, estimate data/hosting costs, content creation time, estimate uplift in activation and paid conversion, and compute break-even time. Provide a rough calculation and rationale?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["PayPal","Salesforce"]},{"id":"q-1963","question":"How would you design a practical 1-day task to ingest daily payments from an Oracle source CSV into Snowflake, ensuring idempotent loads, proper schema mapping, and data quality checks, with a minimal Python MERGE-based load skeleton by transaction_id and a quick test plan?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Oracle","Snowflake","Square"]},{"id":"q-1989","question":"You're evaluating replacing a centralized analytics pipeline with a federated learning-based recommendation model deployed on-device across 3 regions for 5M MAU; build a 2-year cost-benefit model including capex, opex, data-transfer savings, regulatory risk penalties, and uplift in engagement/ARPU, and specify the break-even horizon and go/no-go criteria?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Google","Twitter"]},{"id":"q-2005","question":"In a 3-site edge anomaly-detection rollout for 10k sensors per site (roughly 100 GB/day), design a 2-year cost-benefit analysis for an on-device autoencoder with federated updates to a central model. Include capex for edge hardware, opex for cloud training and data transfer, and quantified benefits from reduced downtime, maintenance savings, and MTBF uplift. Provide break-even horizon and go/no-go criteria?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Amazon","PayPal","Tesla"]},{"id":"q-2057","question":"You’re evaluating a starter feature named 'Retention Policy Engine' for a Slack/HashiCorp-like chat app that auto-purges messages older than a per-channel window, with per-user holds and legal holds; design a 2-year cost-benefit model, including storage savings, indexing/compute overhead, engineering time, potential compliance penalties avoided, and a go/no-go break-even horizon?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Hashicorp","Slack"]},{"id":"q-2086","question":"You're evaluating a privacy-preserving on-device NLP summarizer that runs on user devices with federated updates to a central model via secure aggregation. Design a 2-year cost-benefit model for deploying across Android and iOS in 4 regions with ~25M MAU. Include capex for secure enclaves and on-device storage, opex for federated aggregation and governance, update bandwidth, regulatory penalties for leakage, and uplift in engagement. Provide break-even and go/no-go criteria?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Meta","MongoDB","OpenAI"]},{"id":"q-2218","question":"You're evaluating a beginner feature: an in-app, offline-first guided onboarding using a localized FAQ (5 topics) and 3 short tutorial videos to cut first-week support tickets. Build a 2-year cost-benefit model: content creation time, local storage impact per user, update/hosting costs, uplift in activation, and reduction in first-week tickets; compute break-even horizon?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["Airbnb","Amazon","Plaid"]},{"id":"q-2256","question":"You're evaluating a privacy-preserving on-device recommender for a mobile wallet app to surface merchant offers. Use federated learning with differential privacy across 3 regions and 5M MAU. Build a 2-year cost-benefit: edge hardware/storage, server aggregation, data transfer, content updates, regulatory penalties, activation uplift, ARPU uplift, and support costs. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Apple","OpenAI","Stripe"]},{"id":"q-2429","question":"You're deploying an on-device, privacy-preserving content moderation assistant for real-time group chats at Microsoft/Discord scale. It runs a distilled transformer on user devices to flag policy-violating messages, with periodic federated updates to keep the model aligned. Build a 2-year cost-benefit model: device storage and energy, on-device latency, update bandwidth, cloud aggregation costs, regulatory risk penalties, and expected safety/retention uplift. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Discord","Microsoft"]},{"id":"q-2690","question":"You’re evaluating a new Data Quality as a Service (DQaaS) layer for a lakehouse used by Snowflake, Uber, and Databricks. The plan is to run continuous profiling, schema drift detection, and automated remediation across 4 domains (orders, users, payments, events) with 3 prod regions. Build a 12–18 month cost-benefit model detailing profiling compute, metadata storage, alerting, remediation actions, and data egress; include break-even horizon, go/no-go criteria, and 3–5 concrete quality rules with thresholds?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Databricks","Snowflake","Uber"]},{"id":"q-2717","question":"You're evaluating moving from a centralized real-time alerting system to edge-first, on-device anomaly detection for fraud in a mobile shopping app. Three regions (US, EU, APAC) total 5M MAU. Build a 2-year cost-benefit model including device storage, on-device inference costs, data transfer, cloud orchestrator costs, regulatory penalties, and uplift in fraud detection and retention. Define break-even horizon and go/no-go criteria; include failure modes and mitigations?","channel":"cba","subChannel":"general","difficulty":"intermediate","tags":["cba"],"companies":["Google","Instacart"]},{"id":"q-843","question":"Given a CSV file with columns user_id, action, timestamp, write a Python function using only the standard library that returns the most recent action per user by deduplicating on user_id and keeping the latest timestamp; describe time complexity and edge cases?","channel":"cba","subChannel":"general","difficulty":"beginner","tags":["cba"],"companies":["OpenAI","PayPal","Twitter"]},{"id":"q-872","question":"You are evaluating two deployment options for a new AI inference service under budget and latency constraints. Outline a practical, end-to-end cost-benefit analysis framework to decide which to deploy in production. Include: data you would collect (traffic, latency, SLA penalties, accuracy), metrics (NPV, ROI, payback), horizons, discount rate, handling uncertainty (scenarios), and a concrete calculation workflow you would run?","channel":"cba","subChannel":"general","difficulty":"advanced","tags":["cba"],"companies":["Meta","Scale Ai"]},{"id":"q-1010","question":"You're building a GPU-accelerated graph analytics pipeline that streams 1e9 edges. Design a cache coherence protocol (CCA) to keep per-vertex state consistent across 8 GPUs via a central directory. Choose directory vs snooping, invalidation vs update, and granularity. Describe data layout, coherence transitions, and a minimal update protocol with atomic operations; include trade-offs and performance tips?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Amazon","LinkedIn","NVIDIA"]},{"id":"q-1102","question":"You’re building a real-time 'cca' analytics service ingesting 10k events/sec from multiple services; it must provide low latency, deduplicate, and support backfill. Describe the architecture, data model, and exactly-once strategy, including how you’d implement dedup, transactional writes, and testing under node failures. What trade-offs do you consider?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Lyft","Netflix"]},{"id":"q-1106","question":"Design an end-to-end CDC pipeline that ingests change events from Salesforce and MongoDB and publishes to downstream consumers with at-least-once delivery. Explain your transport choice, deduplication, ordering across partitions, schema evolution, and strategies for backfills, replay, and rollbacks. Include monitoring, testing, and failover plans?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["MongoDB","Salesforce"]},{"id":"q-1190","question":"You're building a real-time collaborative whiteboard for a chat/video platform at Discord/Airbnb/Netflix scale. Each of 5–10k rooms can have up to 200 concurrent editors and must stay highly available with <100 ms latency. Explain your stack decisions: transport (WebSocket vs gRPC streaming), per-room state partitioning, operation encoding, and conflict resolution (CRDT vs OT). How would you handle exactly-once delivery and failure recovery?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Airbnb","Discord","Netflix"]},{"id":"q-1211","question":"In a multi-region service, each region maintains a local L1 cache and a shared L2 cache. How would you implement a robust cache coherence protocol to prevent stale reads while keeping latency low during write-heavy workloads? Include data paths, an invalidation strategy (push vs TTL), race-condition handling, and testing approaches?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Bloomberg","Microsoft"]},{"id":"q-1260","question":"You're building a real-time cca analytics service that ingests 20k-50k events/sec from multiple microservices and external partners. It must deliver per-user engagement scores with sub-second latency, handle out-of-order and late data, deduplicate events, and support backfill. Describe the end-to-end architecture, data model, and exactly-once strategy, including how you'd implement dedup, transactional writes, watermarking, and backfill testing under network partitions and clock skew?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Google","Uber","Zoom"]},{"id":"q-1296","question":"Design a privacy-conscious extension of a real-time cca analytics pipeline that computes per-user engagement scores across multiple geo-regions for three partner firms (Lyft, NVIDIA, Instacart). The extension must minimize PII exposure, support synthetic data feeds for testing without leaking real PII, provide auditable data events for compliance, and preserve correctness under backpressure, partition rebalancing, and clock skew. Describe architecture, data model changes, masking strategies, and how you’d validate with synthetic data?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Instacart","Lyft","NVIDIA"]},{"id":"q-1333","question":"Design a beginner-friendly data quality and observability pattern for a cca event ingestion pipeline. Ingest 1000–2000 events/sec from mobile and web sources. Specify a lightweight schema: user_id, event_type, ts, event_id. Implement at ingest: schema validation, DLQ for invalid records, per-field quality metrics, and a 60s watermark for late data. Describe implementation details and a concrete test plan with synthetic late and malformed events?","channel":"cca","subChannel":"general","difficulty":"beginner","tags":["cca"],"companies":["Apple","Google"]},{"id":"q-1344","question":"You're building a privacy-preserving, cross-tenant event ingestion and analytics service for a media analytics platform. Ingest 40k-120k events/sec from partner APIs and mobile SDKs, including PII fields. Design the end-to-end pipeline to enforce per-tenant isolation, field-level consent-based access, and auditability while preserving low-latency analytics. Include data model, masking, consent revocation handling, schema evolution, and testing strategy?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Anthropic","Meta","Snowflake"]},{"id":"q-1372","question":"Design a beginner-friendly, end-to-end pipeline to generate a daily per-user cca_score from 20-50k events/day across services. Each event has event_id, user_id, type (view, click, purchase), ts. Weights: view 0.2, click 0.5, purchase 2.0. Handle out-of-order data with a 4-hour watermark, deduplicate by event_id, and support day-level corrections (if a repair event arrives, recompute that day and upsert). Describe data schema, ETL steps, dedup strategy, and a minimal test plan?","channel":"cca","subChannel":"general","difficulty":"beginner","tags":["cca"],"companies":["NVIDIA","Stripe","Two Sigma"]},{"id":"q-1464","question":"You’re building a real-time cca scoring pipeline ingesting 30k–80k events/sec from multiple vendors and regions. A feature update changes the engagement formula and must be reproducible for historical backfills without mutating past results. Design end-to-end data lineage, feature versioning, and deterministic backfill workflows: how to version features, catalog definitions, replay with identical inputs, handle non-determinism, and test under clock skew and partitions?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Citadel","Microsoft"]},{"id":"q-1489","question":"You're building an advanced real-time cca analytics platform for a multi-tenant product used by Zoom and Microsoft. The pipeline must respect per-tenant data residency, apply on-the-fly PII masking, and support per-user consent states while still delivering sub-second per-user scores. Explain the end-to-end architecture, privacy controls, and test strategy for backfill and audit trails?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Microsoft","Zoom"]},{"id":"q-1585","question":"You're running a real-time cca analytics pipeline ingesting 20k-50k events/sec from multiple partners. Beyond latency and dedup, design a GDPR/CCPA-compliant data erasure and retention mechanism: when a user requests deletion, purge all derived scores and raw events across online stores, backfills, and audit logs within sub-second latency. Describe architecture, data model, and guarantees, plus testing under partitions and clock skew?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Netflix","Oracle","PayPal"]},{"id":"q-1607","question":"You're operating an advanced streaming cca scoring pipeline where a new feature-weighting model must be rolled out with minimal disruption. Design a canary rollout strategy that guarantees deterministic routing, comparability of scores, and safe rollback under drift or latency spikes. What data-plane changes, testing plans, and rollback criteria would you implement?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Google","Two Sigma"]},{"id":"q-1662","question":"You're operating a privacy-preserving, real-time cca scoring service for a multi-region delivery platform with strict tenant isolation. Design an architecture that guarantees per-tenant data isolation, deterministic routing for canary vs production, and safe rollback under drift or latency spikes. Include data partitioning, encryption, feature gating, testing plan, and rollback criteria?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["DoorDash","Meta"]},{"id":"q-1684","question":"You're building a production-grade per-user cca_score service that spans multiple regions and partner integrations. Design a hybrid real-time/batch pipeline to compute and refresh cca_score with model versioning, ensuring region-local data processing, strict tenant isolation, dedup and exactly-once semantics, and safe backfill testing under clock skew and partitions. Include data model, streaming windowing, incremental scoring, canary rollout, rollback criteria, and test plan?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["DoorDash","Twitter","Uber"]},{"id":"q-1717","question":"You're building a production-grade real-time cca scoring service with tenant data residency rules. Some tenants require EU-only storage and compute, others are global. Design an architecture that (a) deterministically routes requests by tenant and user, (b) version-controls per-tenant models, and (c) supports zero-downtime hot-swapping with safe rollback under drift. Include data model, isolation, testing, and rollback criteria?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Instacart","Lyft","Twitter"]},{"id":"q-1769","question":"You're building a region-scoped, multi-tenant cca_score streaming system with regional data residency guarantees for each tenant. The pipeline ingests 15k–60k events/sec from partner feeds, computes per-tenant cca_scores with model_versioning, and must support cross-region dedup, exactly-once semantics, and safe backfill during partitions. Describe end-to-end architecture, data model, and deployment strategy to meet residency, isolation, and rollback guarantees?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Meta","Plaid"]},{"id":"q-1991","question":"Design a privacy- and compliance-focused real-time cca_score pipeline for a multi-tenant enterprise. The system must enforce per-tenant data isolation, immutable audit logs, and an on-demand privacy mode that halts ingestion and model updates for regulatory checks. Describe architecture, data lineage, deletion policies (Right-to-Deletion), drift detection, and rollback criteria under partitions and clock skew. How would you implement this?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Anthropic","Instacart","Microsoft"]},{"id":"q-2062","question":"You're building a beginner-friendly per-user cca_score API for a mobile app used worldwide. Design a minimal, region-aware scoring service with a 2-feature linear model, cache results in Redis (TTL 15m), and deduplicate requests via a request_id. Use Postgres for user features, gate features by region to meet privacy rules, and keep writes idempotent. Provide endpoints, data schemas, and a simple test plan with canary rollout, clock skew, and backfill considerations?","channel":"cca","subChannel":"general","difficulty":"beginner","tags":["cca"],"companies":["Apple","IBM","NVIDIA"]},{"id":"q-2267","question":"You're adding beginner-friendly observability to a cca_score microservice used by multiple regions. Design a minimal OpenTelemetry tracing plan that captures per-request latency, propagates trace context across API gateway, auth, feature-store fetch, scoring, and response. Show sample spans and how you'd link traces to centralized logs and metrics. Propose Grafana dashboards and alert thresholds for regional latency spikes. Include a reproducible test to verify trace propagation and a region-failover scenario?","channel":"cca","subChannel":"general","difficulty":"beginner","tags":["cca"],"companies":["Amazon","Google","Hashicorp"]},{"id":"q-2390","question":"You're designing a multi-tenant cca_score service with SLA-aware routing. Some tenants require sub-200 ms online scoring; others tolerate batch refresh. Design a dual-path architecture: hot path with per-tenant in-memory cache for latency-critical tenants, and a cold path for others. Include routing rules, data model, cache key schema, backpressure, degraded scoring modes, and a testing plan?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Apple","Hugging Face","Salesforce"]},{"id":"q-2413","question":"You're building a privacy-preserving, multi-tenant cca_score service with partners and regulatory constraints. Detail a design that enforces strict tenant isolation, per-tenant key management for features and models, encrypted data in transit, and explainability traces that redact PII. Include data flow, threat model, testing (simulated breaches, leakage tests), and rollback criteria?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Goldman Sachs","Meta"]},{"id":"q-2455","question":"You’re building a privacy-preserving, real-time cca scoring service that runs in TEEs across multiple regions. Tenant data is encrypted in transit and at rest, and tenants supply per-tenant feature pipelines and keys. Design deterministic tenant routing, per-tenant model/versioning, TEEs attestation and key management, and a testing strategy to prove no data leakage, handle drift, and rollback safely under partitions?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Cloudflare","Microsoft","Netflix"]},{"id":"q-2599","question":"You're rolling out a region-aware cca scoring model that weights features by user segment. Design a deterministic, hash-based routing canary with region-level rollout, ensuring score comparability and fast rollback on drift or latency spikes. Include versioning, testing plan, metrics, and rollback criteria?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Snap","Tesla"]},{"id":"q-2650","question":"Design a privacy-preserving, multi-tenant cca scoring pipeline that evaluates per-user scores inside confidential compute enclaves. Features and raw data must remain encrypted at rest and in transit. Describe data flows, tenant isolation, key management, audit trails, and methods for drift detection and rollback if enclave compromise is suspected. Include concrete components (KMS, envelope encryption, SGX/TEE, HSM), testing plan, and deployment strategy?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Airbnb","DoorDash"]},{"id":"q-2671","question":"You're operating a real-time cca scoring service with per-tenant model_versioning and drift-control. Design an automated drift-detection and canary rollout framework that (a) compares live feature distributions against a stable reference per tenant, (b) promotes per-tenant canaries when drift crosses a threshold, (c) enables zero-downtime rollback with defined criteria during latency spikes or model degradation. Include data path, metrics, and rollback plan?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["OpenAI","Oracle","Two Sigma"]},{"id":"q-840","question":"In a secure messaging service, ciphertexts are decrypted by a server with a decryption oracle exposed to clients, creating a potential CCA risk. Design a practical IND-CCA secure scheme for message confidentiality using existing primitives (e.g., OAEP, AES-GCM, MACs). Explain how you prevent chosen-ciphertext attacks, outline a concrete protocol, and discuss trade-offs?","channel":"cca","subChannel":"general","difficulty":"intermediate","tags":["cca"],"companies":["Microsoft","Oracle","Stripe"]},{"id":"q-879","question":"Describe a cross-region user preferences syncing protocol using MongoDB that tolerates regional partitions. Specify the data model (per-field version stamps), conflict resolution policy, and read/write configurations. Provide a concrete merge approach and an example conflict scenario?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Coinbase","MongoDB","Uber"]},{"id":"q-970","question":"You're shipping an E2E chat feature for an internal Meta–Microsoft product. An attacker may access a decryption oracle. Outline a concrete IND-CCA2 secure scheme for message exchange using public-key crypto, detailing padding (OAEP), a KEM/DEM split or AEAD wrapper, ephemeral keys, and how you bind metadata (timestamps, sender IDs) to prevent malleability. What are the failure modes and mitigations?","channel":"cca","subChannel":"general","difficulty":"advanced","tags":["cca"],"companies":["Meta","Microsoft"]},{"id":"q-1026","question":"Design a CGOA wrapper for a C library that provides two symbols: int* generate_seq(int n) which allocates an int array of length n via malloc filled with 0..n-1, and void free_seq(int* p) to free it. Provide the C header, the Go binding using CGO, and a small Go program that concurrently requests arrays of sizes 4 and 8, validates contents, and frees them. Include exact build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Bloomberg","Coinbase","Oracle"]},{"id":"q-1099","question":"Given a C library with an asynchronous API: \n\n- void run_async(const char* input, void (*cb)(int status, const char* data, void* user), void* user);\n\nwhere the callback runs on a worker thread and data is malloc-allocated or NULL on error. Design a CGO-based Go wrapper that exposes RunAsync(input string, cb func(status int, data string, err error)). The wrapper must: manage the Go callback safely across C boundaries, free C data, map non-zero status to errors, and handle thread attachment; provide header + binding + minimal test harness to demonstrate safety?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Google","LinkedIn","PayPal"]},{"id":"q-1207","question":"Design a CGO bridge for a C EventLib that exposes a function: void register_event_source(int stream_id, void (*cb)(int, const char*)); void start_event_loop(); Build a Go binding that lets two independent streams subscribe and receive events via a single exported Go callback, using a C shim to bridge into Go. Describe memory management and thread-safety; provide header, Go binding, and a small Go program demonstrating two streams; include build steps?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Hashicorp","Twitter","Two Sigma"]},{"id":"q-1235","question":"Implement a CGOA binding for an opaque C Counter handle. API: typedef struct Counter Counter; Counter* CounterNew(int); void CounterInc(Counter*, int); int CounterValue(Counter*); void CounterFree(Counter*); In Go wrap as type Counter with NewCounter, Inc, Value, Close. Show two goroutines each creating its own Counter, incrementing independently, and printing values to verify isolation and no data races. Include header, binding, and a minimal program with build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Adobe","Instacart","Tesla"]},{"id":"q-1294","question":"Implement a CGOA bridge that lets C trigger a Go callback asynchronously when an external sensor fires events. Provide a C header and stub, a CGO binding in Go that registers a Go callback and routes events through an exported Go function, and a safe cleanup mechanism to release resources when producers stop?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Anthropic","Lyft","NVIDIA"]},{"id":"q-1371","question":"Implement a CGO binding for a C function that computes sum and mean of an int array. Provide header and C source with int compute_stats(const int* values, size_t n, long long* sum, double* mean); 0 on success, -1 if n==0. In Go, implement ComputeStats(values []int) (int64, float64, error) calling the C function via CGO, converting types. Add a small main.go that runs two goroutines, each calling ComputeStats on its own slice. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Lyft","Two Sigma"]},{"id":"q-1396","question":"Implement a CGO bridge for a C event source that runs callbacks on its own threads and delivers events to Go via a callback of the form void cb(int code, const char* msg, void* ctx). Expose StartEventSource(cb, ctx) -> handle and StopEventSource(handle). Provide a C header, the Go binding using CGO (with a Go-exported callback and pointer-ownership strategy), and a small Go program that starts two sources and validates receipt of events through a single Go channel. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Instacart","PayPal","Slack"]},{"id":"q-1548","question":"Implement a CGO wrapper for a C library that registers a Go callback and invokes it from a C event loop. Provide a C header and implementation for: - void register_callback(void (*cb)(int)); - void trigger(int value); Write Go bindings using CGO to pass a Go function as the callback, ensuring safe cross-language invocation, correct Go runtime considerations, and demonstrate with two goroutines each registering and triggering events. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Databricks","Meta","NVIDIA"]},{"id":"q-1659","question":"Design a CGOA wrapper to let a Go function be registered as a callback for a C API that emits events asynchronously on external threads. The C API exposes: typedef void (*cb_t)(int eventCode, void* context); void register_cb(cb_t cb, void* ctx); void emit_event(int code); Implement header, CGO binding, and a minimal program that registers a Go callback, triggers an event, and exits safely. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Google","Scale Ai","Twitter"]},{"id":"q-1758","question":"You have a C library exposing void start_event_loop(void (*cb)(int event, void* ctx), void* ctx) and void stop_event_loop(). Implement a CGO bridge in Go that safely delivers events to per-stream Go handlers without passing Go pointers to C, supports multiple concurrent streams, and clean teardown. Provide C header, a thin C wrapper to register a per-stream callback, and a Go binding plus a small program that starts two streams and stops them?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Lyft","Netflix","Snap"]},{"id":"q-1815","question":"Design and implement a CGOA bridge for a C streaming library that emits integers via a callback. Expose a Go API StartStream() (<-chan int, func Stop()) using cgo.Handle to pass the Go-side context. Ensure thread-safe delivery of values to Go, proper cleanup with finalizers, and robust error reporting across the boundary. Provide C header, Go binding, and a minimal caller?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Cloudflare","Google","Plaid"]},{"id":"q-1845","question":"Implement a CGOA wrapper around a C function that returns a heap-allocated string: char* greet(const char* name). Provide a header with greet and void free_string(char*). Write a Go binding using CGO to call greet from multiple goroutines and free the result with free_string to avoid leaks. Include the header, a Go binding file, and a small Go program that calls greet(\"Alice\"), greet(\"Bob\"), greet(\"Carol\") concurrently and prints results. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["MongoDB","NVIDIA","Snowflake"]},{"id":"q-1910","question":"Design a CGOA exercise: Create a C header exposing void scale_and_offset(const float* input, int n, float* output, float scale, float offset); Implement a Go binding using CGO that wraps ScaleAndOffset and returns a slice of float results. Provide a minimal C implementation, a Go binding, and a short Go program that launches two goroutines, each feeding distinct input arrays to scale_and_offset concurrently? Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Bloomberg","Instacart","PayPal"]},{"id":"q-1993","question":"Design a CGO bridge for a C API that uses a progress callback and supports cancelable work. The C API exposes: typedef void (*progress_cb)(int percent, void* user); void register_progress(progress_cb cb, void* user); int do_work(int steps); The callback may be invoked on worker threads. Implement: (1) a C header that defines the callback type and registration; (2) a Go binding using CGO that exposes a Go channel-based Progress stream; (3) a small Go program that starts do_work in a goroutine and prints progress, with a context-based cancellation. Ensure memory and lifecycle safety across boundaries?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Cloudflare","Google","Stripe"]},{"id":"q-2011","question":"Implement a beginner CGO binding that exposes a C API void compute_stats(const double* data, size_t n, double* mean, double* stddev) to Go. Provide a C header, a Go binding using CGO, and a small Go program that launches 4 goroutines, each calling ComputeStats on its own data slice concurrently. Ensure memory safety, zero-copy access where possible, and that data races are avoided. Include explicit build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Airbnb","Meta","Square"]},{"id":"q-2036","question":"Your C library exposes a function int start_task(int task_id, void (*progress_cb)(int, const char*)); void stop_task(int task_id); The callback is invoked from worker threads and passes a progress value and a C string message. Design a CGO-based Go binding that allows registering a Go callback per task, safely translates C strings to Go strings, guarantees thread-safe delivery of progress events to Go, and supports cancellation via stop_task while ensuring resources are freed. Include a header, the Go binding, and a minimal Go program launching two concurrent tasks with per-task callbacks. Provide build steps?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Google","NVIDIA","Robinhood"]},{"id":"q-2185","question":"Design a CGOA binding for a C library exposing a thread-safe ring buffer with multiple producers and a single consumer. Provide a C header with:\n- typedef struct ring_buffer ring_buffer;\n- ring_buffer* rb_new(size_t capacity);\n- void rb_free(ring_buffer*);\n- int rb_push(ring_buffer*, int value);\n- int rb_pop(ring_buffer*, int* value); // 0 on success, -1 if empty\nImplement a Go binding using CGO that wraps rb_push/rb_pop, exposing a Go type RingBuffer with New, Push, Pop. Include a small demo with 4 producers and 2 consumers and build steps?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Anthropic","Apple","Discord"]},{"id":"q-2263","question":"Design a CGOA binding for a C library function that registers a callback and a context pointer. Create a Go binding using CGO that lets Go pass a Go function as the callback, supports multiple concurrent subscriptions, and is memory-safe with proper cleanup. Provide the C API, the Go binding, and a short Go snippet showing two subscriptions and callbacks concurrently?","channel":"cgoa","subChannel":"general","difficulty":"advanced","tags":["cgoa"],"companies":["Apple","Lyft","Netflix"]},{"id":"q-2305","question":"## Question\n\n**CGO Binding: In-Place String Reversal with Concurrency**\n\nYou have a C function:\n```c\nvoid reverse_inplace(char* s);\n```\nProvide:\n- A C header exposing the signature\n- A Go binding using CGO implementing ReverseGo(s string) string\n- A short Go program that launches two goroutines calling ReverseGo on different inputs\n- Build steps\n\nEnd with a question mark?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Netflix","Snowflake","Square"]},{"id":"q-2373","question":"Implement a CGO binding for a C API that processes an int array and reports results via a callback. The C API: typedef void (*cb_t)(int result, void* ctx); void sum_with_callback(const int* data, size_t n, cb_t cb, void* ctx); Provide a C header exposing the signature, a Go binding using CGO that offers func SumWithCallback(data []int, cb func(int)) error, and a short Go program that concurrently calls SumWithCallback from multiple goroutines. Include build steps and CGO thread-safety and memory-management considerations. End with a question mark?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["DoorDash","Hugging Face","NVIDIA"]},{"id":"q-2478","question":"Implement a CGOA binding for a C matrix-vector multiply. Provide matvec.h and a minimal matvec.c implementing void matvec(const double* A, const double* x, double* y, int rows, int cols) that computes y[i] = sum_j A[i*cols + j] * x[j]. Write a CGO Go binding exposing MatVec(A []float64, x []float64, rows, cols int) ([]float64, error). Include a small Go program that runs two goroutines, each calling MatVec with different inputs to prove concurrency safety, and build steps to compile and run?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Coinbase","NVIDIA","Scale Ai"]},{"id":"q-842","question":"You have a Go service using cgo to wrap a C API. A C function 'char* fetch_data(int id)' returns a malloc-allocated string or NULL on error. Design a safe Go wrapper that converts the result to a Go string, ensures the C allocation is freed, handles NULL with a meaningful error, and notes CGO thread-safety considerations. What implementation would you write?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Meta","PayPal","Snowflake"]},{"id":"q-870","question":"Design a Go wrapper for a C API with malloc’d results that must be freed, exploring a new angle: ensure thread-safe, single-point ownership transfer for each call and robust error handling when the C call returns a non-zero code or NULL. API: typedef struct { int code; const char* msg; } ScanResult; ScanResult* perform_scan(const char* query); void free_scan_result(ScanResult*); Implement function: func Scan(query string) (string, error)?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Instacart","Netflix"]},{"id":"q-914","question":"Using cgo, how would you wrap a C function that allocates a string (char*) and returns it, ensuring memory is freed by Go code without leaks, and provide a minimal working example?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Instacart","Snap","Twitter"]},{"id":"q-950","question":"Implement a minimal CGOA wrapper that exposes a C function int add(int a, int b) to Go. Provide the C header, the Go binding using CGO, and a small Go program that concurrently calls Add from two goroutines to demonstrate thread safety. Include build steps?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Google","Snap","Twitter"]},{"id":"q-978","question":"Design a CGO bridge for a C streaming API that delivers chunks via a callback: void stream_data(int id, void (*chunk_cb)(const char* data, size_t len, void*), void* ctx). Implement a Go wrapper StreamFromC(id int) (io.Reader, error) that buffers chunks into an io.Pipe and exposes a safe reader, supporting concurrent streams and proper backpressure. Include a C header, a Go CGO binding, and a simple consumer showing two goroutines reading from the reader?","channel":"cgoa","subChannel":"general","difficulty":"intermediate","tags":["cgoa"],"companies":["Goldman Sachs","Microsoft","NVIDIA"]},{"id":"q-987","question":"Implement a CGO binding for a C function char* greet(const char* name) that returns a newly allocated string. Provide the C header and implementation, a Go binding using CGO that wraps greet in a safe Go function returning (string, error), and a small Go program that concurrently calls the binding from multiple goroutines and frees the allocated memory. How would you handle NULL returns and memory deallocation robustly?","channel":"cgoa","subChannel":"general","difficulty":"beginner","tags":["cgoa"],"companies":["Google","MongoDB","Snap"]},{"id":"cissp-communication-network-1768227886993-3","question":"An organization wants to ensure the integrity and authenticity of DNS responses to prevent cache poisoning, while not necessarily encrypting DNS query payloads. Which mechanism provides this guarantee?","channel":"cissp","subChannel":"communication-network","difficulty":"intermediate","tags":["AWS","DNS","Terraform","certification-mcq","domain-weight-13"],"companies":null},{"id":"q-1097","question":"An enterprise with microservices deployed across AWS, Azure, and GCP stores API keys and credentials in Kubernetes secrets and environment variables. After an audit finds stale keys and overly broad service accounts, design a defense‑in‑depth credential management strategy: architecture changes, tooling choices (Vault, AWS Secrets Manager, KMS, Kubernetes CSI or equivalent), rotation cadence, access controls, and how you would validate effectiveness?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Apple","Discord","Meta"]},{"id":"q-1113","question":"In a multi-tenant ML feature store on AWS, a compromised notebook can access all tenants due to broad IAM policy. Propose a concrete mitigation plan to enforce tenant isolation and data protection: separate namespaces, least-privilege roles with explicit ARNs, ABAC tags (TenantID/DataClass), SCPs to block cross-tenant actions, VPC isolation with PrivateLink, per-tenant KMS keys, and centralized logging with CloudTrail, Config, and GuardDuty. Include testing steps?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Scale Ai","Twitter"]},{"id":"q-1373","question":"In a global organization with Kubernetes clusters on AWS and GCP, CI/CD pipelines currently pull secrets from plaintext files. Design a CISSP-aligned secrets management solution: vault choice (HashiCorp Vault or cloud-native), access controls, dynamic Secrets, short‑lived tokens, rotation cadence, audit logging, break-glass, and CI/CD integration with GitHub Actions/Jenkins. Include trade-offs?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Adobe","Bloomberg"]},{"id":"q-1407","question":"Scenario: A data platform uses MongoDB Atlas for operational data and Snowflake for analytics across three regions; admins use SSO, but access is excessive and logs are incomplete. Design a defense-in-depth plan that enforces least privilege, KMS-backed encryption, and robust auditing. Include concrete steps for MongoDB Atlas and Snowflake: provisioning, key management, logging, data masking for PII, and verification?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["MongoDB","Snowflake"]},{"id":"q-1480","question":"In a multinational fintech operating across Google Cloud, AWS, and on-prem environments, PCI data flows through a centralized data lake used by data scientists for analytics. Propose a data-centric security design that ensures PCI data never leaves regulated regions while enabling cross-cloud analytics. Include data tagging, tokenization or masking strategy, key management, and telemetry/audit hooks. What pattern do you choose and why, and outline concrete steps to implement in the next sprint?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Bloomberg","Google","PayPal"]},{"id":"q-1493","question":"An international company runs CI/CD pipelines in GitHub Actions that assume cross‑cloud roles across AWS and GCP. Secrets are stored in plaintext, and pipeline logs may expose credentials. Propose a CISSP-aligned remediation: pick an access model (e.g., short‑lived credentials + Just‑In‑Time access), specify token lifetimes, rotation, logging/auditing, and CI/CD integration checks. How would you validate fixes in production-like staging and what trade-offs exist?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Airbnb","Discord","Tesla"]},{"id":"q-1558","question":"Design a CISSP-aligned access-control plan for a multinational org running data science workloads across AWS and GCP. After a contractor's credentials were compromised, enforce Just-In-Time access, short‑lived tokens, and auditable trails. Specify tooling (AWS STS, Vault, GCP IAM, OPA, Cloud Audit Logs), break-glass process, and how you validate the policy before production rollout?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["LinkedIn","NVIDIA","OpenAI"]},{"id":"q-1619","question":"A multinational fintech runs microservices on AWS EKS and Azure AKS with a shared Snowflake data lake. Credential theft enables pivot via cross-cloud service accounts. Design a CISSP-aligned control set: (1) cross-cloud Just-In-Time access with tokens from AWS STS, Azure AD, and GCP IAM; (2) ABAC/RBAC policy-as-code with OPA; (3) service-mesh mTLS and micro-segmentation; (4) audit trails; (5) break-glass and staging purple-team validation. Specify acceptance criteria and evidence for production rollout?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["IBM","Meta"]},{"id":"q-1633","question":"A global retailer stores customer data in AWS S3, Google Cloud Storage, and an on-prem Hadoop cluster. Data is classified as public, internal, or restricted. Design a CISSP-aligned data-classification and access-control policy using ABAC with tags to enforce least privilege and enable auditable access across all platforms. Include example tags, roles, token lifetimes, and a minimal policy snippet?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Hashicorp","Two Sigma"]},{"id":"q-1660","question":"Scenario: A multinational runs workloads in AWS and IBM Cloud with Cloudflare Zero Trust. An insider exfiltrates data using DNS tunneling to a rogue domain. Propose a concrete, CISSP‑level detection and containment plan that preserves legitimate traffic. Include monitoring sources, detection signatures, tools, and IR steps across the three platforms?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Amazon","Cloudflare","IBM"]},{"id":"q-1704","question":"Design a CISSP-aligned zero-trust admin access model for remote production sessions across a hybrid cloud environment (AWS/Azure) with Windows/Linux endpoints. Include device posture checks, MFA, ephemeral credentials, just-in-time roles, jump-host/RDP/SSH brokering, session recording, break-glass controls, and auditable logging. Provide a concrete deployment plan and a safe red-team validation approach?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Meta","Tesla"]},{"id":"q-1798","question":"Design a CISSP-aligned admin-access governance model for a global platform with multi-cloud assets (AWS, GCP, Azure) and microservices. A contractor needs short-term admin rights across prod and staging; outline controls, tooling (IdP, Vault, OPA, Cloud Audit/Logging, Kubernetes RBAC), break-glass, and how you would validate the policy before production rollout?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Google","Slack","Tesla"]},{"id":"q-1892","question":"An organization runs production apps across AWS, on‑prem, and Azure. A compromised developer workstation exposed a long‑lived API key used by a microservice to access production data. Outline a CISSP‑aligned incident response plan: containment, eradication, recovery; specify evidence sources (CloudTrail, VPC flow logs, EDR), credential rotation (short‑lived tokens), revocation (IAM/Vault), network segmentation, and post‑incident hardening. Include a Vault runbook snippet to revoke the compromised token and issue a new one?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Microsoft","Tesla"]},{"id":"q-1904","question":"Describe a CISSP-aligned secure software supply chain plan for a new product shipped via CI/CD pipelines across AWS and GitHub Actions. Include SBOM generation/verification, code signing and binary attestation, secrets management in pipelines, and auditable logs plus incident response. Provide concrete steps and tooling?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["NVIDIA","PayPal","Scale Ai"]},{"id":"q-1980","question":"A multi-tenant delivery platform serving Microsoft and DoorDash customers processes sensitive order data across microservices in AWS. Propose a CISSP-aligned data protection plan covering data-in-transit and data-at-rest, key management (rotation, separation of duties, access controls), and auditing/incident response. Include concrete tools and steps?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["DoorDash","Microsoft"]},{"id":"q-2051","question":"Scenario: a junior developer accidentally commits a cloud service key to a public repo, exposing AWS and GCP access. Outline a CISSP-aligned, beginner-friendly containment and recovery plan: immediate revocation of exposed keys, rotate credentials, enforce MFA, apply least-privilege IAM changes, enable cross-cloud audit logs, and run a quick tabletop exercise to validate readiness. Which steps and tools would you use?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Amazon","Google","IBM"]},{"id":"q-2083","question":"In a global fintech with services on AWS and GCP plus on‑prem, a contractor's workstation is compromised and used to harvest ephemeral credentials authenticating to production apps. Propose a CISSP‑aligned containment, eradication, and recovery plan covering cross‑cloud token revocation, key rotation, Just‑In‑Time access, vault integration, break‑glass, and auditable logging. Include tool‑specific steps and a runbook snippet to revoke and reissue tokens across AWS IAM, GCP IAM, and Vault?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["LinkedIn","Robinhood","Snap"]},{"id":"q-2118","question":"In a multi-cloud dev sandbox spanning AWS, Azure, and GCP, service accounts for CI deployments have overly broad admin rights, risking secret exposure in production. Provide a CISSP-aligned, beginner-friendly containment and remediation plan: inventory affected identities, revoke excessive privileges, implement least-privilege roles per cloud, enable just-in-time access, rotate keys, enable cross-cloud audit logs, and validate with a tabletop exercise. Which steps and tools would you use?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Apple","Coinbase","Databricks"]},{"id":"q-2137","question":"Scenario: a multinational platform operates across AWS, Azure, and on‑prem storage. A CSV containing customer PII was exposed publicly due to a drift in IAM/RBAC policies and a misconfigured data catalog. Outline a CISSP‑aligned incident response playbook: detection signals, containment (isolate storage, revoke tokens), eradication (remediate drift, enforce ABAC), recovery, and post‑incident audit across clouds; include tooling, roles, and timing?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["LinkedIn","MongoDB","Tesla"]},{"id":"q-2172","question":"In a data lake spanning AWS S3, GCP Cloud Storage, and on-prem HDFS, design a CISSP-aligned, beginner-level plan to enforce least privilege for 120 analysts. Describe an RBAC/ABAC approach, just-in-time elevation, access revocation, and auditable logging, plus onboarding and withdrawal workflows and pre-deployment validation?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Apple","Meta","Two Sigma"]},{"id":"q-2209","question":"Scenario: a contractor with multi-cloud access (AWS, GCP, on‑prem Kubernetes) is compromised, and customer PII starts exfiltrating via a rogue data pipeline. Design a CISSP‑aligned containment, eradication, and recovery plan that emphasizes data-centric security, cross‑cloud audit consolidation, and Just‑In‑Time access. Include runbook steps to revoke service accounts, rotate keys, re-encrypt data, and validate containment before returning to production. What specific controls and tools would you invoke, and in what order?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Bloomberg","Lyft","MongoDB"]},{"id":"q-2476","question":"Design a CISSP-aligned, beginner-level plan to ensure data privacy and access auditing for a multi-tenant analytics dashboard used by Robinhood, Discord, and Meta. The plan must cover data minimization, ABAC vs RBAC decisions, just-in-time elevation, data tagging and lineage, auditable logging, and onboarding/offboarding workflows; include concrete steps and tooling?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Discord","Meta","Robinhood"]},{"id":"q-2532","question":"You manage a serverless data pipeline on AWS Lambda and GCP Cloud Functions that reads from S3 and GCS and writes to a data warehouse. For 120 analysts triggering jobs via an internal portal, design a CISSP-aligned, beginner-level plan to enforce least privilege using ABAC with just-in-time elevation, ephemeral credentials, and strict auditing. Include onboarding/withdrawal workflows, credential lifecycle, and pre-deployment validation?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Google","Meta","Oracle"]},{"id":"q-2561","question":"In a multi-cloud setup with CI/CD pipelines building images for AWS EKS and Google GKE, a leaked credential could enable a backdoored image. Design a CISSP-aligned, practical supply-chain plan covering: code signing and provenance, SBOM, image scanning, policy enforcement (OPA), runtime controls, ephemeral credentials for CI/CD, least-privilege IAM, cross-cloud auditing, and an incident runbook. Include concrete steps and tools?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Google","Microsoft","MongoDB"]},{"id":"q-2646","question":"In a global streaming platform distributed across AWS, GCP, and on-prem, design a CISSP-aligned CI/CD security plan to protect the software supply chain. Include SBOM enforcement, artifact signing with an HSM, provenance checks in GitOps, release access controls, and an incident playbook for a compromised signing key. Provide concrete tooling and steps?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["IBM","NVIDIA","Netflix"]},{"id":"q-2693","question":"Scenario: A multinational uses a third‑party market data feed delivered via gRPC over mutual TLS into on‑prem Kubernetes and AWS analytics services. Downstream policies are weak and vendor attestations are sparse. Design a CISSP‑aligned security plan to ensure integrity, confidentiality and availability. Specifically address: **identity and access model**, **secret management** and rotation, **logging/audit** of data events, and **vendor risk/contract** controls; include concrete steps and tooling?","channel":"cissp","subChannel":"general","difficulty":"intermediate","tags":["cissp"],"companies":["Bloomberg","Salesforce"]},{"id":"q-2711","question":"Multi-cloud CI/CD compromise: a fintech platform runs services on AWS EKS, GCP GKE, and on‑prem OpenShift. A GitHub Actions runner was compromised, pushing a malicious image that escalates permissions for a service account across clouds. Provide a CISSP-aligned containment and recovery plan: isolate artifacts, rotate credentials, revoke access, enable cross‑cloud audit logs, verify SBOM and image signatures, and run a tabletop to validate runbooks. Which steps and tools would you prioritize and why?","channel":"cissp","subChannel":"general","difficulty":"advanced","tags":["cissp"],"companies":["Citadel","Square","Twitter"]},{"id":"q-958","question":"Scenario: A fintech startup uses cloud IAM for multi-cloud apps. Admins use weak passwords and MFA is not enforced; API keys are shared in chat and stored insecurely. You’re asked to pick one first CISSP-aligned control to reduce risk while enabling operations. Which baseline control should be implemented first and why?","channel":"cissp","subChannel":"general","difficulty":"beginner","tags":["cissp"],"companies":["Coinbase","Google","Meta"]},{"id":"q-1087","question":"You're running a 5-node HA Kubernetes control plane (3 in AZ-a, 2 in AZ-b) with a 3-member etcd cluster. After a regional outage, etcd loses quorum. Describe exact, command-level steps to restore quorum, rejoin the third member, and validate API availability across both AZs, including risk notes and DR readiness checks?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Cloudflare","Scale Ai","Snap"]},{"id":"q-1368","question":"In a 3-node etcd-backed Kubernetes cluster, one node loses network connectivity and becomes partitioned while the other two remain healthy. How do you preserve availability and data integrity, avoid split-brain, and recover the partitioned member? Outline health checks, how you isolate the partition, recovery steps, and verification?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Netflix","Tesla"]},{"id":"q-1395","question":"In a 3-node Kubernetes cluster, deploy a stateless web app using a Deployment with 3 replicas and a ClusterIP Service. Include readiness and liveness probes, CPU/memory requests and limits, and populate APP_MODE via a ConfigMap and DB_PASSWORD from a Secret. Describe the steps to perform a rolling update to 4 replicas with zero downtime and how you would verify the rollout across nodes?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Adobe","Snap"]},{"id":"q-1418","question":"Enable at-rest encryption for Kubernetes Secrets using a KMS (envelope) provider on an existing 3-control-plane cluster. Describe exact steps to configure the EncryptionConfig, rotate KEKs without downtime, trigger re-encryption of existing Secrets, and verify that new and existing Secrets are stored encrypted at rest (without decrypting at-rest data). Include concrete commands and caveats?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Google","Robinhood","Scale Ai"]},{"id":"q-1452","question":"You operate a 3-node Kubernetes cluster with a StatefulSet of 3 replicas backed by PVs. A critical fix requires upgrading to image app:v2 with zero downtime. Outline a precise upgrade plan: set a PodDisruptionBudget to minAvailable 2, apply a RollingUpdate with partition=2, ensure readiness probes tolerate brief pod restarts, and verify data integrity during rollout with concrete kubectl commands?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Google","Oracle","Uber"]},{"id":"q-1562","question":"How would you design a Kubernetes Job named app-init-seed that seeds app data on first run? The Job should use an Alpine-based image, mount a PVC at /data, load a script from a ConfigMap at /scripts/seed.sh, skip if /data/.seeded exists, optionally fetch seed.json from https://example.com using API key from a Secret, and write a log and the marker file upon success; include full YAML and apply steps?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Adobe","Salesforce"]},{"id":"q-1737","question":"You're deploying a 3-replica Deployment in Kubernetes for a payment app. Suddenly 2 pods crash with CrashLoopBackOff. Describe a practical debugging workflow to identify if the issue is container startup, config, resources, or image, and outline the exact kubectl commands and file checks you would perform. Include how you would propose a minimal fix and a rollback plan?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Apple","Robinhood"]},{"id":"q-1764","question":"Advanced: In a 5-node Kubernetes cluster where etcd memory usage has spiked after a noisy deployment, outline a production-safe remediation plan: verify health with etcdctl, diagnose via metrics, perform defrag/compact, take a snapshot, and adjust API watch load while preserving API server availability. What steps would you take?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Google","Microsoft","Snowflake"]},{"id":"q-1810","question":"You're operating a 5-node Kubernetes cluster with a validating webhook named cfg.example.com enforcing a label requirement on Deployments in all namespaces. A critical patch must be applied in prod-adv while the webhook service is temporarily unavailable. Outline a safe, auditable plan to bypass the webhook with exact kubectl commands to identify, disable, apply, and revert?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["DoorDash","Google","Meta"]},{"id":"q-1885","question":"In a high-traffic delivery platform, you deploy a Kubernetes-based worker pool for async order processing. Explain how you would implement a robust, rate-limited queue with backpressure, idempotent workers, and at-least-once delivery, using Kubernetes primitives and common tools. Include how you handle spikes, retries, and data-store consistency?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["DoorDash","Two Sigma"]},{"id":"q-1932","question":"Describe a **concrete, production-ready plan** for zero-downtime deployments in a Lyft-scale Kubernetes cluster with ~40 microservices, multiple data stores, and strict uptime. How would you implement **blue/green or canary releases**, traffic shaping, and automated rollbacks? Include rollout strategy, readiness checks, RBAC, DR, and a compact manifest example comparing Istio and Linkerd approaches?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Lyft","Tesla","Twitter"]},{"id":"q-1946","question":"On a 3-node Kubernetes cluster, a Deployment’s Pods stay Pending while 2 nodes are Ready. The scheduler shows a NoSchedule taint on node-1. Explain exact commands to identify the taint, check the pod’s tolerations, and either remove the taint or add a matching toleration so the workload schedules. How do you validate after changes?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Amazon","Apple","Uber"]},{"id":"q-1972","question":"You maintain a Node.js app deployed as a 3-replica Kubernetes Deployment behind a service. A security patch requires updating the container image from v1.0.1 to v1.0.2 with zero downtime. List exact kubectl steps to perform a safe rolling update, how you confirm readiness, and how you rollback if the rollout fails?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Microsoft","Netflix"]},{"id":"q-2027","question":"You're managing a shared Kubernetes cluster with namespaces prod, analytics, and dev. A nightly analytics batch job sometimes starves frontend pods during peak hours, triggering evictions. Design an end-to-end remediation plan: tune pod requests/limits, enforce defaults with a LimitRange, cap total usage with a ResourceQuota, ensure guaranteed QoS, evaluate VerticalPodAutoscaler vs HorizontalPodAutoscaler, and outline validation steps to prevent regressions?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["DoorDash","Netflix"]},{"id":"q-2134","question":"You manage a Databricks/OpenAI-like platform on Kubernetes. A Spark streaming job processes 1TB/day and faces sporadic node preemption causing data loss without checkpoints. Design a fault-tolerant deployment (manifests and configurations) and explain trade-offs between Kubernetes Job vs. StatefulSet, backoff strategies, taints/tolerations, and Spark dynamic allocation. What would you implement and why?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Databricks","OpenAI"]},{"id":"q-2152","question":"On a Kubernetes cluster running Spark jobs via Spark on Kubernetes, a 12-pod job reports intermittent OOM errors during shuffle-heavy stages on medium-sized datasets. Provide a concrete debugging plan: how you verify memory limits, adjust executor/driver memory and overhead, tune JVM GC, and validate changes with metrics and a controlled benchmark run?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Databricks","Oracle"]},{"id":"q-2197","question":"In a Kubernetes cluster running mixed workloads, a critical data-processing job occasionally starves QoS pods during peak load. Provide a concrete plan to enforce SLA guarantees: specify resource requests/limits with appropriate QoS, enable Pod Priority and Preemption, configure HorizontalPodAutoscaler and ClusterAutoscaler, and outline validation steps to prove SLAs hold under load?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["LinkedIn","Two Sigma"]},{"id":"q-2282","question":"Implement namespace-scoped network isolation for namespace 'team-a' using Kubernetes NetworkPolicy. Ensure pods in team-a can reach only the DNS service and a single internal log sink at 'log-sink.logging.svc.cluster.local:9200', and cannot reach other namespaces or external hosts. Provide the manifests (default-deny and allow rules), kubectl commands to apply, and verify using a tester pod with targeted tests for DNS, log sink, and external access?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Databricks","Scale Ai"]},{"id":"q-2304","question":"You're administering a 3-namespace Kubernetes cluster hosting two teams. There are no NetworkPolicies today. Draft and implement a precise plan to enforce per-namespace default-deny, permit intra-namespace traffic for all pods, and allow outbound HTTPS to api.external.com:443, while blocking cross-namespace pod communication. Provide exact YAMLs for the policies, commands to verify with curl from representative pods, and rollback steps?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Hugging Face","Netflix","Twitter"]},{"id":"q-2338","question":"How would you identify memory pressure causing OOMKilled pods in a 3-node Kubernetes cluster (kubectl top, pod events, and logs), compare usage to requests/limits, profile memory if possible, and implement a fix with proper requests/limits, ResourceQuota, and a canary rolling update (maxUnavailable:1) plus a LivenessProbe, then validate with a soak test?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Microsoft","Netflix"]},{"id":"q-2395","question":"In a shared Kubernetes cluster serving teams at Slack, IBM, and Salesforce, design a namespace-per-tenant isolation strategy with quotas and security controls. Include ResourceQuota, LimitRange, NetworkPolicy, PodSecurity admission, and RBAC. Propose a GitOps deployment flow per-namespace and an observability plan. Compare against a single-namespace approach and justify your choice?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["IBM","Salesforce","Slack"]},{"id":"q-2519","question":"In a Kubernetes cluster, a Deployment named web-app with 3 replicas in namespace prod suddenly fails after a ConfigMap update that injects environment variables. Pods crash due to a missing DB_PASSWORD from the ConfigMap. Describe exact steps and commands to diagnose, patch the ConfigMap, and roll out a fix with minimal downtime while ensuring the deployment restarts cleanly?","channel":"cka","subChannel":"general","difficulty":"beginner","tags":["cka"],"companies":["Databricks","Hashicorp","Oracle"]},{"id":"q-2553","question":"In a two-region Kubernetes cluster with 3 nodes per region and a PostgreSQL StatefulSet backed by PVs, deploy a new service image with zero downtime using canary or blue-green. Detail exact rollout steps, traffic shaping, health checks, and data-consistency verification during rollout?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Google","IBM","Square"]},{"id":"q-2584","question":"**Advanced Kubernetes Debugger**: You manage a 12-node cluster on AWS EKS running three microservices. During a surge, a critical deployment experiences OOMKilled. Provide a concrete, end-to-end debugging plan and rollback strategy: include metrics you’d capture, commands you’d run, and exact changes (requests/limits, HPA, PDB, rollout)?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Hashicorp","Meta","Twitter"]},{"id":"q-2656","question":"In a three-AZ Kubernetes cluster, design an operator to keep replicas of Deployments labeled app=svc spread across zones, even during node drains. Describe the CRD you would add, the reconcile logic, and how you would validate distribution with tests. Provide a minimal manifest example?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Databricks","LinkedIn","Netflix"]},{"id":"q-2721","question":"In a 6-node Kubernetes cluster, a critical stateless service is upgraded via RollingUpdate. Ensure availability never drops below 80% of replicas during the upgrade. Outline a concrete rollout plan with specific values for maxUnavailable, maxSurge, and a PodDisruptionBudget, plus readiness probes, preStop hooks, and a rollback strategy. Include how you'd verify rollout status and handle failure?","channel":"cka","subChannel":"general","difficulty":"advanced","tags":["cka"],"companies":["Amazon","Slack","Tesla"]},{"id":"q-874","question":"In a Kubernetes cluster used by Salesforce/Cloudflare/Snap engineers, a Deployment's startup latency rose from 1–2s to 6–8s after introducing an initContainer that runs a health check before the application starts. Describe how you would diagnose, what metrics/logs to collect, and concrete fixes (e.g., moving checks to readiness, caching, parallel init, or canary rollout). Include rollback and validation steps?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Cloudflare","Salesforce","Snap"]},{"id":"q-893","question":"You manage a 3-node Kubernetes control plane backed by an etcd cluster. After a power outage, one etcd member reports corruption. Describe the exact steps to detect the corrupted member, restore from a known-good snapshot, rejoin the cluster, and validate API availability. Include concrete commands, risk notes, and how you would verify DR readiness?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Cloudflare","IBM","Netflix"]},{"id":"q-936","question":"A 3-control-plane Kubernetes cluster on AWS experiences API server latency spikes after a webhook deployment. The admission webhook is malfunctioning and causing slow requests; outline precise steps to identify the failing webhook, safely disable it to restore API responsiveness, validate cluster availability, and prepare a rollback plan with minimal downtime?","channel":"cka","subChannel":"general","difficulty":"intermediate","tags":["cka"],"companies":["Airbnb","Databricks","Google"]},{"id":"q-1066","question":"Inside namespace analytics, schedule a daily batch to process a CSV: use a CronJob (02:00 UTC) to start a Job that runs a Python script from a ConfigMap, reads input from a ConfigMap, uses a Secret for DB credentials to insert results into Postgres service, writes output to a PVC, runs as non-root with a readOnlyRootFilesystem, with resource limits, a backoffLimit of 3, and a 15-minute activeDeadlineSeconds; ensure proper probes?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Goldman Sachs","Microsoft","Snowflake"]},{"id":"q-1312","question":"In a Kubernetes CKAD scenario, you have a Deployment named 'web-server' in namespace 'prod' running a Node.js app. During rolling updates, some pods terminate and restart slowly, causing request timeouts. Outline concrete changes to implement startupProbe, adjust readiness and liveness probes, and add a preStop hook to drain existing connections. Provide a minimal manifest patch showing startupProbe, a readinessProbe, a livenessProbe, and a preStop lifecycle hook that ensures graceful shutdown. How would you approach this?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Discord","MongoDB","Snap"]},{"id":"q-1352","question":"You have a small Python API app to run in Kubernetes. Write a minimal manifest that creates a Deployment with 3 replicas using image myregistry/api:1.0, a readinessProbe httpGet /health on port 8080, a livenessProbe with initialDelaySeconds 15 and periodSeconds 10, and a ClusterIP Service exposing port 8080. Inject config via ConfigMap app-config with LOG_LEVEL. How would you verify and how would you scale to 5 replicas?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Google","Meta","OpenAI"]},{"id":"q-1393","question":"Design a Kubernetes manifest that deploys a stateless app with 3 replicas, uses a ConfigMap and a Secret, attaches a 1Gi PVC for data, includes a HorizontalPodAutoscaler, exposes via ClusterIP, and enforces a NetworkPolicy restricting egress to api.internal.example.com:443. Provide YAML fragments and discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["OpenAI","Two Sigma"]},{"id":"q-1459","question":"Create Kubernetes manifests for a simple API: Deployment using image 'my-api:1.0' that reads PORT from a ConfigMap via env, a ConfigMap with PORT and APP_MODE, readinessProbe and livenessProbe for /healthz on that port, and resource requests/limits. Expose with a Service on port 80 targeting 3000. Describe a rolling update plan?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["IBM","Meta","NVIDIA"]},{"id":"q-1488","question":"Given a Deployment named 'image-processor' in namespace 'prod' with 6 replicas processing images from a Redis queue, design a practical patch to ensure graceful shutdown of in-flight tasks during rollouts, prevent simultaneous pod terminations, and maintain availability during node drains; include a minimal manifest patch adding a preStop script, terminationGracePeriodSeconds, readiness and liveness probes, and a PodDisruptionBudget targeting the deployment. What steps would you take to validate under load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Adobe","Airbnb","Snap"]},{"id":"q-1547","question":"Configure a NetworkPolicy to restrict egress from prod/checkout-service to only reach payments/payment-processor in the payments namespace on port 443, blocking all other egress. Provide a minimal manifest patch and a test strategy to validate allowed and blocked traffic inside the cluster?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Netflix","Oracle"]},{"id":"q-1609","question":"You're deploying inventory-service on Kubernetes. Write YAML to (1) deploy 3 replicas with a ConfigMap for API_ENDPOINT and a Secret for DB_PASSWORD, (2) an InitContainer that runs migrations, (3) readiness and liveness probes, resource requests/limits, and a RollingUpdate strategy with maxUnavailable: 1, (4) a Job to run migrations before the first pod starts, (5) a sidecar log-shipper. Include the key fragments and rationale?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["DoorDash","NVIDIA","Slack"]},{"id":"q-1626","question":"Blue/Green rollout for a CKAD production web service: in namespace 'prod', a Deployment 'web-app' with 3 replicas serves traffic via the Service 'web-app'. Introduce a canary path with a second Deployment 'web-app-canary' and a canary route (via canary Ingress or a second Service) to validate with 10–20% traffic before full switch. Provide a minimal manifest patch showing resource requests/limits, readinessProbe, and a livenessProbe for both deployments, plus how to switch traffic and rollback. Include validation steps under load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["NVIDIA","OpenAI","Oracle"]},{"id":"q-1707","question":"Explain a progressive canary rollout for a 4-replica frontend behind an NGINX Ingress. Use a stable Deployment and a canary Deployment (image frontend:1.2-canary, replicas:1). Patch Ingress to route 10% to canary; later 50% and 100%. Include minimal YAML patches for deployments and a canary Ingress with canary-weight; how would you monitor health and rollback?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Coinbase","Discord"]},{"id":"q-1767","question":"Scenario: In a CKAD scenario, you must roll out a new image for a stateless API 'inventory-api' in namespace 'prod' with 2% traffic to the canary, using vanilla Kubernetes (no service mesh). Outline a practical canary strategy with two Deployments and two Services, explain how you split traffic without a mesh, and provide minimal manifests for the canary Deployment and a canary-facing Service, plus a rollback plan and how you'd validate under load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Google","NVIDIA"]},{"id":"q-1776","question":"Design and implement a CKAD deployment for a 3-replica web app: use a ConfigMap for env vars, define resource requests/limits, add readiness and liveness probes, and configure an HPA with min 3, max 10 and targetCPUUtilizationPercentage 50. Provide the YAML and show verification steps?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Meta","Oracle"]},{"id":"q-1879","question":"You are deploying a Node API behind a Service in Kubernetes with Redis and PostgreSQL. Provide manifests to run API with 4 replicas, Secret for DB creds, ConfigMap for flags, readiness/liveness probes, resource requests/limits, and a RollingUpdate with maxUnavailable=25%, maxSurge=25%. Include how you would test zero-downtime and how HPA would be wired?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Amazon","Discord","Google"]},{"id":"q-1922","question":"CKAD intermediate: In a prod namespace, a Deployment named 'orders-api' with 3 replicas experiences brief outages during image upgrades. Provide a concrete patch for zero-downtime upgrades: set rollingUpdate strategy (maxUnavailable: 0, maxSurge: 1), add a PreStop hook for graceful shutdown, and create a PodDisruptionBudget to protect at least 2 healthy pods during maintenance. Include minimal Deployment and PDB manifests and describe validation under maintenance-like load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Google","Square"]},{"id":"q-1959","question":"In a Kubernetes CKAD scenario, design a real-time log-processor that consumes from Kafka, writes results to Cassandra, restarts gracefully on failure, and preserves at-least-once delivery. Provide a concrete deployment design with InitContainer, ConfigMap, Secret, Probes, HPA, DLQ strategy, and a minimal YAML skeleton. Explain trade-offs and monitoring hooks?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Meta","NVIDIA","Two Sigma"]},{"id":"q-1992","question":"Scenario: A 3-replica Deployment for a Kubernetes CKAD exercise uses a ConfigMap for APP_PORT and LOG_LEVEL and a Secret for DB_PASSWORD. How would you structure manifests to ensure zero-downtime updates, proper health checks, and correct secret/config usage? Provide concrete manifest fragments and a rollout strategy?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Apple","IBM","Meta"]},{"id":"q-2015","question":"Design a CKAD-grade, multi-tenant API gateway canary rollout: implement two Deployments (stable and canary) for api-gateway, share a Service, and use an Ingress canary annotation to route 20% traffic to canary. Use a ConfigMap flag newFeature to toggle the new code path; store TLS certs in a Secret; include readiness/liveness probes, resource requests/limits, and a minimal YAML skeleton. Explain how you would observe traffic split and rollback?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Meta","Uber"]},{"id":"q-2093","question":"You deploy a Kubernetes Deployment named web-app using image registry.example.com/web-app:v1.2 and expose port 8080. Provide a YAML manifest snippet that adds: livenessProbe for /health on 8080 with initialDelaySeconds: 5 and periodSeconds: 10; readinessProbe for /ready with timeoutSeconds: 3; resources: requests cpu: 250m memory: 256Mi; limits cpu: 500m memory: 512Mi; a ConfigMap mounted at /etc/config providing APP_MODE and an env var APP_MODE sourced from that ConfigMap?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Discord","Salesforce"]},{"id":"q-2145","question":"CKAD intermediate: In namespace retail, a Deployment 'inventory' (3 replicas) talks to Postgres service 'inventory-db'. You need a one-time seed of lookup data at Pod startup without delaying traffic. Provide a minimal patch that uses an InitContainer to run a seed SQL script against inventory-db with idempotent INSERTs (ON CONFLICT DO NOTHING), and add resource requests/limits, a readiness probe, and a liveness probe. Explain how you'd validate under load and re-seed behavior on restarts?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Discord","Microsoft","PayPal"]},{"id":"q-2205","question":"CKAD intermediate: A 3-rep StatefulSet named 'cache' in namespace 'prod' must be upgraded with zero downtime. How would you implement a partitioned, rolling update (one pod at a time) using spec.updateStrategy.RollingUpdate with partition, and set readiness/startup probes plus CPU/memory limits? Also provide a minimal PodDisruptionBudget to keep at least 2 pods healthy during maintenance, and outline validation under load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Discord","Salesforce"]},{"id":"q-2237","question":"Design a Kubernetes manifest suite for image registry.example/ckad-demo:1.0 that uses a ConfigMap and a Secret. Include a Deployment with 3 replicas, a Service on port 80, a ConfigMap app-config with APP_MODE=production, and a Secret app-secret with API_KEY=secret123. Mount the ConfigMap at /etc/app and the Secret at /etc/secret, export APP_MODE as an env, include readiness and liveness probes on /healthz, and trigger rolling updates on ConfigMap changes via a checksum/config annotation?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Anthropic","Hashicorp","Oracle"]},{"id":"q-2276","question":"In a CKAD scenario, namespace 'analytics' hosts Deployment 'data-collector' (5 replicas) behind service 'collector-svc' on port 9000. Implement a NetworkPolicy that (1) allows ingress to data-collector only from pods in the analytics namespace labeled app=ingest on port 9000, (2) denies all other inbound traffic, and (3) allows egress to 10.0.0.50:5140. Provide the YAML manifest and a patch to the Deployment to apply the policy. Describe how you'd validate under simulated load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Citadel","Discord","Snowflake"]},{"id":"q-2306","question":"In CKAD terms, design a per-namespace streaming transformer that consumes Redis Stream 'events', computes a rolling 60s window average latency per user, and writes aggregates to PostgreSQL. Replays failed messages to a Redis DLQ. Use an InitContainer to preload an ML model from a Secret, a ConfigMap for thresholds, readiness/liveness probes, and an HPA based on latency. Provide a minimal YAML skeleton and discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Coinbase","Hugging Face","Snap"]},{"id":"q-2366","question":"Design a Kubernetes manifest for a beginner CKAD task: deploy a static site with nginx where index.html is served from a ConfigMap named app-content. Mount TLS certs from Secret tls at /etc/tls. Requirements: Deployment with 2 replicas, runAsNonRoot and readOnlyRootFilesystem, readiness/liveness probes on /healthz, INDEX_FILE env from ConfigMap, and an HPA with CPU target 50%. Provide a minimal YAML skeleton and discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"beginner","tags":["ckad"],"companies":["Bloomberg","Tesla"]},{"id":"q-2427","question":"Design a CKAD-focused per-namespace file-processing service: it watches a PVC-mounted directory for new text files, processes each file, and submits results to a REST API. Use a ConfigMap for FILE_EXT and BATCH_SIZE, a Secret for API creds, an InitContainer to install deps, and a marker-based idempotence scheme to achieve at-least-once with near-exactly-once semantics. Include readiness/liveness probes and a minimal YAML skeleton; discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Adobe","Anthropic","Snap"]},{"id":"q-2453","question":"In a CKAD scenario, design a per-namespace real-time image-resize service that consumes from a Kafka topic, uses an InitContainer to fetch a resize model from a Secret, writes results to PostgreSQL, and preserves at-least-once delivery with idempotent DB writes. Include a ConfigMap for RESIZE_WIDTH/HEIGHT and BATCH_SIZE, readiness/liveness probes, an HPA based on CPU, and a per-namespace NetworkPolicy restricting Kafka ingress and DB egress. Provide a minimal YAML skeleton and discuss trade-offs?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Apple","Hugging Face"]},{"id":"q-2541","question":"In CKAD terms, design a new, per-namespace data-collector that subscribes to an in-cluster message bus (NATS) and writes results to a time-series store, ensuring at-least-once delivery with a DLQ. Include an InitContainer to install runtime deps, a ConfigMap for BATCH_SIZE, a Secret for creds, a Sidecar for TLS cert rotation, readiness/liveness probes, and a NetworkPolicy restricting egress to the TSDB, a DLQ path on a PVC, and a minimal YAML skeleton. Explain trade-offs?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Databricks","Discord","Meta"]},{"id":"q-2614","question":"CKAD advanced: design a per-namespace data-pipeline controller using a CRD NamespaceDataPipeline. Each instance subscribes to a namespaced NATS subject (metrics.{namespace}) and writes batched data to a TSDB. Guarantee at-least-once via a DLQ on a PVC and idempotent writes. Include InitContainer for deps, a TLS-rotation sidecar, a ConfigMap for BATCH_SIZE, a Secret for creds, readiness/liveness probes, and a NetworkPolicy restricting egress to the TSDB. Provide a minimal YAML skeleton and trade-offs?","channel":"ckad","subChannel":"general","difficulty":"advanced","tags":["ckad"],"companies":["Apple","Oracle","Robinhood"]},{"id":"q-858","question":"In a Kubernetes CKAD scenario, you have a Deployment named 'web-app' in namespace 'prod' with 3 replicas; pods frequently OOMKilled under load. Describe a practical debugging plan and provide a minimal manifest patch showing resource requests/limits, a readiness probe, and a liveness probe. Include scaling considerations and how you'd validate the fix under load?","channel":"ckad","subChannel":"general","difficulty":"intermediate","tags":["ckad"],"companies":["Hugging Face","LinkedIn","Snowflake"]},{"id":"q-1180","question":"Design a CKNE-aware per-tenant admission control for a multi-tenant real-time analytics gateway. Downstream CKNE health signals (queue depth, latency, error rate) are exposed via metadata. Propose a per-tenant health score, a dynamic token-bucket policy, and a cross-tenant shedding strategy that preserves fairness and SLA compliance. Include payload schemas, a minute-by-minute control loop, and a minimal sample payload?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Goldman Sachs","Meta","Snap"]},{"id":"q-1198","question":"Design a CKNE-aware per-tenant traffic shaping policy for a real-time collaboration platform (gateway -> engine -> persistence) servicing thousands of tenants with different SLAs. Edge CKNE health signals drive a minute-by-minute token-bucket shedding policy that prioritizes high-SLA tenants while gracefully degrading others; specify payload schemas and provide a minimal test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Discord","Snap"]},{"id":"q-1214","question":"Design a CKNE-aware data lineage policy for a three-stage ETL pipeline (ingest → transform → load) servicing thousands of tenants. Each hop attaches CKNE health in trace metadata. Propose a per-tenant degradation policy that preserves auditability for high‑SLA tenants while shedding heavy lineage data during degradation. Include payload schema, a minute-by-minute decision loop, and a minimal payload example?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Anthropic","Google","Scale Ai"]},{"id":"q-1280","question":"Scenario: A serverless workflow (API gateway -> orchestrator -> worker) serves thousands of tenants. Design a CKNE-aware tracing approach where every hop propagates a CKNE health signal in trace metadata and implement a per-tenant adaptive sampling policy that starts at 3% and scales to 25% during degradation. Include payload schemas, per-tenant health aggregation at the orchestrator, strategies to preserve trace fidelity during micro-bursts, and a minute-by-minute loop mapping health to sampling for the next window; provide a minimal payload example and a test plan?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Discord","Microsoft","Netflix"]},{"id":"q-1289","question":"Design a CKNE-aware canary rollout strategy for a multi-tenant image-resize API (ingest -> process -> deliver). Each tenant's requests carry CKNE health in headers. Propose a per-tenant rollout policy that starts at 5% canary, scales to 40% during healthy conditions, and reverts on degradation, with a minute-by-minute control loop. Include payload schemas, edge aggregation, and a minimal payload example?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Amazon","Meta","NVIDIA"]},{"id":"q-1377","question":"Design a CKNE-aware cross-region cache strategy for a multi-tenant real-time feed service (ingest -> compute -> deliver). Each tenant emits a CKNE health signal attached to requests. Propose per-tenant cache admission, TTLs, and prefetching depth that adapt minute-by-minute based on CKNE health, edge burst traffic, and tenant SLAs. Include payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Instacart","Slack"]},{"id":"q-1442","question":"Context: a multi-tenant mobile app with gateway -> dispatcher -> worker. Each tenant emits CKNE health in requests; design a CKNE-aware per-tenant notification dispatcher that throttles messages by a simple score-to-drop policy: score < 0.7 drops 10%, score < 0.5 drops 30%, otherwise none. Provide payload schemas, minute-by-minute decision loop, a minimal payload example, and a unit test to validate the policy?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Google","NVIDIA","Uber"]},{"id":"q-1554","question":"Design a CKNE-aware per-tenant event routing policy for a real-time analytics pipeline (ingest -> stream-processor -> dashboard) servicing thousands of tenants with varying SLAs. Each hop propagates a CKNE health signal. Propose a per-tenant routing policy that uses CKNE health, queue depth, and SLA tier to determine dynamic fan-out limits, with a minute-by-minute control loop and tenant-aware backpressure. Include payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Citadel","Coinbase","Meta"]},{"id":"q-1595","question":"Design a CKNE-aware per-tenant circuit-breaker policy for a real-time order routing system (gateway -> routing -> fulfillment) serving thousands of merchants. Each hop propagates a CKNE health signal. Propose a per-tenant policy that activates a progressive circuit breaker during degradation based on CKNE health, queue depth, and tenant SLA, with a minute-by-minute control loop; include payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["DoorDash","Tesla"]},{"id":"q-1632","question":"Design a CKNE-aware per-tenant cache TTL policy for a multi-tenant CDN path (gateway -> edge-cache -> origin) where each hop propagates CKNE health signals. Propose how TTLs and cache invalidation granularity vary by tenant health and SLA, with a minute-by-minute control loop. Include payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["OpenAI","PayPal","Snap"]},{"id":"q-1663","question":"Design a CKNE-aware multi-tenant ingestion pipeline (ingest → stream-processor → store) where edge CKNE health signals gate per-tenant throughput: batch size, forwardRaw vs enriched, and backpressure to enrichment services. Provide a minute-by-minute decision loop, per-tenant SLA handling, payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Apple","Databricks","Snowflake"]},{"id":"q-1755","question":"Design a CKNE-aware multi-tenant caching layer for a real-time recommendations pipeline (ingest -> rec-service -> storefront). Edge nodes propagate CKNE health; implement adaptive per-tenant TTLs, prefetch, and anti-stampede guards. Provide payload schemas, minute-by-minute decision loop, and a minimal payload example; include a test plan?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Meta","Slack"]},{"id":"q-1843","question":"Design a CKNE-aware privacy-gating policy for a multi-tenant streaming analytics pipeline (ingest -> processor -> store) that handles PII with per-tenant redaction levels tied to CKNE health. When health degrades, progressively increase redaction, throttle nonessential fields, and gate exports to dashboards. Describe payload schemas, a minute-by-minute control loop, and provide a minimal payload example?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["IBM","MongoDB","Oracle"]},{"id":"q-2169","question":"Design a CKNE-aware per-tenant rate-limiter for a real-time multi-tenant ingestion pipeline (ingest -> transform -> store). Edge CKNE health signals drive per-tenant quotas and burst handling. Propose how quotas are computed, payload schemas, a minute-by-minute control loop, and a minimal payload example; include a test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Lyft","Meta","Snowflake"]},{"id":"q-2289","question":"Design a CKNE-aware per-tenant feature-flag evaluation stack for a real-time personalization engine (ingest -> flag-service -> edges). Each hop propagates CKNE health; implement per-tenant fidelity tiers (A full, B approximate with sampling, C default) driven by SLA and CKNE vector. Ensure deterministic minute-level sampling, edge caching, and per-tenant fallbacks while preserving latency. Provide payload schemas, a minute-by-minute decision loop, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Amazon","PayPal","Two Sigma"]},{"id":"q-2319","question":"Design a CKNE-aware per-tenant resource isolation policy for a real-time multi-tenant data-processing pipeline (ingest -> enrich -> analytics) serving tenants with different SLAs. CKNE health signals propagate at each hop; specify dynamic per-tenant CPU/memory quotas, adaptive throttling, and backpressure during degradation. Include payload schemas, a minute-by-minute decision loop, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Amazon","Citadel","Lyft"]},{"id":"q-2352","question":"Design a CKNE-aware per-tenant deduplication and backpressure policy for a real-time log ingestion pipeline (agents -> collector -> indexer) servicing thousands of tenants. Each hop propagates a CKNE health signal. Propose a per-tenant dedup window, dynamic backpressure thresholds based on CKNE, and a minute-by-minute health-to-throttle loop that preserves high-SLA data while shedding during degradation. Include payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Apple","Bloomberg","IBM"]},{"id":"q-2384","question":"Design a CKNE-aware per-tenant retry and idempotency policy for a streaming ingestion path (ingest -> processor -> warehouse) serving thousands of tenants. Each hop propagates CKNE health. Propose: 1) how to encode CKNE into message headers, 2) per-tenant retry budgets and adaptive backoff with jitter responsive to CKNE and queue depth, 3) a robust idempotency strategy with a tenant-scoped dedupe cache and TTLs aligned to SLA, 4) a minute-by-minute control loop for budget and TTL adjustments, 5) concrete payload schemas and a minimal payload example, 6) a test plan and observability hooks?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Instacart","LinkedIn","Oracle"]},{"id":"q-2422","question":"Design a CKNE-aware cross-tenant telemetry sharing layer: publishers -> broker -> consumers in a real-time multi-tenant pipeline. Each hop adds CKNE health; implement a degradation policy that prioritizes high-SLA tenants, drops low-priority tenants, and tunes per-tenant sampling and backpressure minute-by-minute. Include payload schemas, tenant priorities, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Apple","Meta","Snap"]},{"id":"q-2475","question":"Design a CKNE-aware per-tenant job scheduler for a multi-tenant data pipeline (ingest -> queue -> worker). Each job carries a CKNE health score; implement a minute-by-minute policy to deprioritize high-CKNE tenants, pause new jobs for degraded tenants, and preempt long-running low-priority jobs when total queue depth exceeds a threshold. Include payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Anthropic","Google","MongoDB"]},{"id":"q-2504","question":"In a CKNE-enabled multi-tenant data ingestion pipeline (Ingest -> Processor -> Store), design a per-tenant backpressure mechanism that uses CKNE health to throttle high-SLA tenants vs. low-priority tenants. Edge CKNE signals propagate upstream; implement a minute-by-minute control loop that adjusts per-tenant request rates and queue depths. Include payload schemas, a minimal payload example, per-tenant SLA map, and a concrete test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Google","IBM","OpenAI"]},{"id":"q-2540","question":"Design a CKNE-aware per-tenant autoscaling policy for a multi-tenant streaming pipeline (ingest -> stream-processor -> analytics). Edge and processing nodes propagate CKNE health; specify metrics, scaling rules, throttling, fault isolation, and how you'd test it in a production-like environment?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["Airbnb","Salesforce","Zoom"]},{"id":"q-2604","question":"Design a CKNE-aware per-tenant feature-flag controller for a real-time analytics cockpit (ingest -> processor -> dashboard). Each tenant has features with SLAs. Build a policy: when a tenant’s CKNE health drops, automatically disable non‑critical features, throttle telemetry sampling, and hide non-essential widgets while keeping baseline latency under 150 ms. Include payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Google","Instacart"]},{"id":"q-2659","question":"Design a CKNE-aware multi-tenant batch scheduler for a queue (ingest -> scheduler -> workers). Each job carries tenant_id and a CKNE health score (0–1). Explain how to map CKNE to per-tenant priority, implement a minute-by-minute degradation policy prioritizing high-SLA tenants, and provide a minimal payload example plus a basic test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Airbnb","Hashicorp","Stripe"]},{"id":"q-2673","question":"Design a CKNE-aware per-tenant feature rollout controller for an API gateway serving multiple tenants. Each tenant has a CKNE health score (0–1). Explain how to map CKNE to per-tenant feature exposure (0–100%), implement a minute-by-minute degradation loop that lowers exposure for low-CKNE tenants while keeping high-CKNE tenants fully exposed, and provide payload schemas, a minimal payload example, and a test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Airbnb","MongoDB"]},{"id":"q-2698","question":"Design a CKNE-aware per-tenant API rate limiter at an edge gateway that enforces per-tenant QoS using CKNE scores to scale capacity. How would you map CKNE to per-tenant capacity, implement a minute-by-minute degradation loop, and provide a minimal payload example plus a test plan?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Databricks","Meta","Robinhood"]},{"id":"q-846","question":"Design a real-time CKNE failure detector for a distributed microservice mesh. Specify the data pipeline, latency budget, how you compute p95 latency and error rate, and how you implement replay and backpressure for fault tolerance. Include testing strategies and production validation to demonstrate correctness and resilience?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["Coinbase","Meta","NVIDIA"]},{"id":"q-861","question":"Design an adaptive CKNE-aware tracing and sampling strategy for a real-time order-processing pipeline in a multi-tenant mesh. Explain how CKNE health signals influence sampling decisions, how you preserve trace fidelity under bursts, and how you quantify the overhead and impact on latency. Include concrete data structures and an example workflow?","channel":"ckne","subChannel":"general","difficulty":"intermediate","tags":["ckne"],"companies":["PayPal","Snap","Tesla"]},{"id":"q-954","question":"Scenario: A three-service order flow (API gateway -> inventory -> payment) runs in one region. Design a CKNE-aware tracing approach where each service propagates a CKNE health signal in trace metadata and employs an adaptive sampling policy: base 10% with a health-adjusted factor that can raise sampling to 50% during degradation. Specify data structures for per-service health, the trace metadata payload, and a minute-by-minute workflow to compute health and adjust sampling for the next window. Provide a minimal code snippet showing the payload and health update logic?","channel":"ckne","subChannel":"general","difficulty":"beginner","tags":["ckne"],"companies":["Goldman Sachs","IBM"]},{"id":"q-991","question":"Design a CKNE-aware tracing strategy for a real-time ad bidding pipeline (gateway -> bidding service -> settlement) servicing multi-tenant advertisers. Each leg propagates a CKNE health signal; implement an adaptive sampling policy that scales from 5% baseline to 60% during degradation, with per-tenant health aggregation at the edge. Specify payload schemas, how to preserve trace fidelity under micro-burst traffic, and a minute-by-minute workflow for health-to-sampling decisions; provide a minimal payload example and a test plan?","channel":"ckne","subChannel":"general","difficulty":"advanced","tags":["ckne"],"companies":["NVIDIA","Netflix","Stripe"]},{"id":"q-1121","question":"Scenario: You operate a shared Kubernetes cluster serving multiple product teams. You must prevent cross-namespace data leakage and enforce least-privilege access while remaining auditable and scalable. Describe a concrete strategy using either OPA Gatekeeper or Kyverno for admission control (with at least two constraints), implement namespace RBAC boundaries, apply Calico NetworkPolicy for namespace isolation, and outline a monitoring/audit plan with tests and runbooks. Include example policies and a minimal test commands snippet?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Google","IBM","Snap"]},{"id":"q-1130","question":"You're running a Kubernetes cluster for a web app. A Pod mounting hostPath and running as root was detected in dev. Outline a practical plan to enforce least privilege across namespaces (Baseline/Restricted) using a policy engine (Kyverno or OPA Gatekeeper) and show how you would validate enforcement without disrupting workloads. What steps and files would you use?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["Amazon","Google"]},{"id":"q-1167","question":"Scenario: You operate a multi-cluster Kubernetes data platform (cloud+on‑prem) where a Spark job can access customer data. Design an end-to-end approach to detect, prevent, and respond to data exfiltration attempts from pods across clusters. Include policy design, telemetry signals, enforcement, and incident runbooks; discuss trade-offs?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["Bloomberg","Instacart"]},{"id":"q-1278","question":"Scenario: A fintech data platform runs a multi-tenant data lake on Kubernetes. Each data job uses per-job ServiceAccounts to access restricted cloud storage. A rogue pod tries to exfiltrate data via the bucket. Propose a security approach that binds each pod to a dedicated cloud IAM role (workload identity), enforces namespace-scoped permissions, and provides tamper-evident audit trails. Include detection and response for abnormal egress and a safe rotation plan. What trade-offs?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["DoorDash","Robinhood"]},{"id":"q-1301","question":"You're debugging a Kubernetes deployment in a multi-tenant environment where one namespace's pods delay startup by several minutes. Provide a practical, beginner-friendly diagnostic flow focusing on pod events, init containers, image pulls, and config maps. List concrete kubectl commands you would run and how you’d determine the root cause?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["Anthropic","Netflix","Twitter"]},{"id":"q-1323","question":"In a Kubernetes cluster deploying an ML inference service, models and weights live in a private registry. Outline a practical plan to sign models with cosign, publish attestations, and enforce runtime verification so only attested models can be deployed via GitOps (Argo CD) and an enforcement policy (OPA Gatekeeper or Kyverno). Include concrete commands and sample configuration?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Hugging Face","PayPal","Uber"]},{"id":"q-1356","question":"You manage a Kubernetes cluster hosting regulated data across tenants. Design a practical end-to-end plan to enable at-rest encryption with envelope encryption using a cloud KMS, protect both API server data and etcd data, rotate keys safely, and prove compliance via CI checks. Include concrete commands and sample manifests?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Amazon","Google","Uber"]},{"id":"q-1374","question":"Scenario: A multi-cloud platform runs Kubernetes on EKS and serverless runtimes; ensure only cosign-signed images with verifiable attestations can be deployed across all targets. Describe an end-to-end plan to enforce cross-registry provenance, automate Rekor attestations, and integrate with a GitOps workflow (Argo CD), including concrete commands and sample policy rules?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Discord","Meta","Tesla"]},{"id":"q-1398","question":"In a multi-tenant Kubernetes cluster with images stored in a private OCI registry, design an end-to-end workflow to sign images with cosign, publish attestations, and enforce that only attested images are deployed. Include concrete commands, CI hints, and an admission control policy snippet?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Cloudflare","Discord","Google"]},{"id":"q-1444","question":"Scenario: A poly-cloud serverless stack runs AWS Lambda, GCP Cloud Functions, and Azure Functions. CI/CD signs each function package and its dependencies with cosign and publishes SLSA attestations to a central registry. Deployments must be allowed only if attested across all clouds. Describe an end-to-end plan, including concrete commands and sample configs for signing, attesting, and cross-cloud enforcement?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Amazon","LinkedIn","Netflix"]},{"id":"q-1522","question":"Scenario: In a multi-tenant SaaS, a central migration service applies Flyway SQL scripts to dozens of PostgreSQL instances. How would you implement a concrete plan to sign each migration with cosign in CI, publish attestations to a registry, and enforce at runtime that only attested migrations are executed? Include concrete signing commands, attestation storage approach, and a sample enforcement policy (Kyverno/OPA) plus integration steps with Flyway?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Databricks","Slack","Uber"]},{"id":"q-1551","question":"In a Kubernetes-based data platform hosting a multi-tenant ML feature store exposed via a high-volume API, design a privacy-preserving, end-to-end audit trail to support forensics without exposing PII. Specify architecture, RBAC/ABAC controls, OpenTelemetry instrumentation, log pipeline (Fluentd/Loki), encryption, data redaction, retention, and how you’d run production-scale incident drills. What would you monitor first and why?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["Cloudflare","LinkedIn","Two Sigma"]},{"id":"q-1691","question":"Scenario: A Terraform-driven multi-tenant cloud platform provisions resources across clouds. You must sign every Terraform plan in CI with cosign, publish an attestation to a central registry, and enforce at runtime that only attested plans are applied by the GitOps flow. Describe an end-to-end approach with concrete signing commands, attestation storage layout, an sample OPA policy, and integration steps with the deployment pipeline?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Netflix","Stripe"]},{"id":"q-1734","question":"In a multi-cloud Terraform deployment across AWS, GCP, and Azure, a central registry hosts official modules. Propose a concrete plan to: - sign every module and its dependencies with cosign during CI, - publish attestations to a provenance registry, - enforce at plan/apply time that only attested modules are used via an OPA policy or equivalent gate, including concrete signing commands, storage layout for attestations, and a sample enforcement policy with integration steps?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Airbnb","Apple","Salesforce"]},{"id":"q-1846","question":"In a Tesla-scale Databricks lakehouse on AWS, with Unity Catalog and a Kubernetes data plane, contractors routinely export aggregated datasets to external S3 buckets. Design a detection and response mechanism that distinguishes legitimate exports from exfiltration attempts. Include telemetry (Unity Catalog audit logs, IAM activity, Delta table operations), thresholds, alerting, and an incident playbook?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["Databricks","Tesla"]},{"id":"q-1964","question":"In a multi-tenant notebook service on Kubernetes, each notebook runs in a transient pod and pulls dependencies from a private registry. Design a concrete plan to sign the notebook artifact (notebook.ipynb) and its dependencies with cosign, publish SLSA attestations to a central registry, and enforce at runtime that only attested notebooks and dependencies are allowed to run. Include concrete signing commands, attestations storage, and a sample Kyverno/OPA policy plus integration steps with the notebook runner?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Anthropic","Citadel","Microsoft"]},{"id":"q-2092","question":"In a Kubernetes-based data-analytics platform with two namespaces, dev and prod, design a practical RBAC and Pod Security setup for a new microservice 'data-processor' to ensure it can only read its own ConfigMaps and Secrets, runs as a non-root user, and cannot escalate privileges. Provide concrete manifest fragments (RBAC, ServiceAccount, RoleBinding, PodSecurityContext) and describe how to validate at admission time that all pods comply?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["Adobe","Databricks","DoorDash"]},{"id":"q-2128","question":"In a high-sensitivity environment spanning on-prem and cloud, design a tamper-evident Kubernetes audit logging and incident-response pipeline that preserves evidence from the API server, etcd, and kubelets while enabling automated triage and containment during a breach. Specify data formats, forwarding targets, non-repudiation measures, and failure modes?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["Adobe","PayPal","Tesla"]},{"id":"q-2188","question":"In a monorepo-managed Kubernetes fleet with per-service Helm charts and shared base images, design an attestation model where base image attestations are linked to derived service images via SBOM, and enforcement ensures every deployed chart references an attested base. Describe signing commands, SBOM workflow, attestation storage, and a gating policy (OPA/Kyverno) with CI/CD integration?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Citadel","Zoom"]},{"id":"q-2418","question":"A data ingestion pipeline uses a private artifact registry hosting Spark JARs and Python wheels. Each artifact is cosign-signed with a SBOM attestation; runtime must guarantee that only attested artifacts execute in Spark jobs managed by Airflow on Kubernetes. Describe an end-to-end plan to sign, publish attestations, and enforce runtime checks across the data plane, including concrete cosign commands, attestation storage approach, and a sample Kyverno/OPA policy plus integration steps with Airflow and the registry?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Bloomberg","OpenAI"]},{"id":"q-2508","question":"In a Knative-based FaaS layer on Kubernetes, functions are built as OCI images stored in a private registry and loaded by the function runtime at cold start. Design an end-to-end plan to sign and attest function images, publish attestations to Rekor, and enforce at runtime that only attested functions execute in the Knative namespace. Include concrete cosign commands, attestation storage approach, and a sample Kyverno or OPA policy plus integration steps with the registry and the function loader?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["Google","LinkedIn","Plaid"]},{"id":"q-2537","question":"In a policy-driven edge compute fleet, WASM modules are built per customer and pushed to a private OCI registry. You must ensure only attested modules run on edge devices. Design a concrete plan to sign modules with cosign, publish SLSA attestations, and enforce at runtime that only attested modules are loaded via an admission controller (Kyverno/OPA). Include concrete commands and sample policies?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Anthropic","Google","Instacart"]},{"id":"q-2639","question":"In an AWS EKS cluster with Istio, an attacker attempts DNS tunneling to exfiltrate data from the dev namespace. Design a practical detection pipeline using: (a) eBPF-based DNS egress monitoring, (b) CoreDNS query analytics, (c) Istio telemetry + network policies, and (d) alerting aligned to MITRE techniques. Explain data flow, signals to watch, and how you would validate alerts?","channel":"cks","subChannel":"general","difficulty":"advanced","tags":["cks"],"companies":["Hashicorp","Netflix","Salesforce"]},{"id":"q-2687","question":"Scenario: A Kafka Connect cluster loads dozens of custom connectors from a shared registry. Describe a concrete plan to sign each connector JAR and its JSON configs with cosign, publish attestations to Rekor, and enforce at deployment and runtime that only attested connectors are loaded. Include concrete signing commands, attestation storage layout, and a sample Kyverno/OPA policy and integration steps with Kafka Connect?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["IBM","Instacart","Zoom"]},{"id":"q-920","question":"In a real-time chat service like Discord, deployed on Kubernetes with NVIDIA GPUs for video processing, you introduce a third-party plugin system that runs as WebAssembly modules to apply custom video filters. How would you design a secure plugin sandbox and runtime attestation to prevent leakage of streams or keys, ensure isolation from other plugins, and enable rapid rollback if a plugin behaves unexpectedly in production? Provide concrete approaches and trade-offs?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["Discord","NVIDIA","Zoom"]},{"id":"q-959","question":"Scenario: A service executes user-provided Python plugins inside a container. Design a concrete runtime hardening plan using Linux namespaces, a minimal seccomp profile, and capability bounding, ensuring plugins cannot access host files or network directly while preserving IPC with a controlled channel. Outline exact steps and validation tests?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["PayPal","Tesla"]},{"id":"q-967","question":"Scenario: You manage a microservice app deployed to Kubernetes with CI/CD; you need to prevent tampered container images. **Describe a practical, beginner-friendly plan** to implement image signing and verification using **cosign**, integrate it into a GitHub Actions workflow, and enforce verification at deployment (registry or admission webhook). Include concrete commands?","channel":"cks","subChannel":"general","difficulty":"beginner","tags":["cks"],"companies":["Apple","NVIDIA","Stripe"]},{"id":"q-994","question":"Scenario: A Kubernetes-based ML platform serves multiple teams; outbound data exfiltration is a breach risk. Propose a concrete, end-to-end control plane approach to prevent unauthorized data egress using policy-as-code, Kubernetes NetworkPolicy, and a centralized egress gateway. Include a sample Rego policy for Gatekeeper that enforces a namespace label data-export=allowed and an annotation egress-proxy=https://proxy.internal, and outline testing and GitOps integration?","channel":"cks","subChannel":"general","difficulty":"intermediate","tags":["cks"],"companies":["OpenAI","Zoom"]},{"id":"q-1019","question":"You're operating a CNF-based API gateway cluster that terminates TLS for thousands of tenants across 3 regions. A mandate requires migrating all TLS to post-quantum algorithms with per-tenant keys sourced from an HSM, while delivering zero-downtime upgrades, per-tenant key isolation, and a rollback plan. Outline an end-to-end rollout including (a) inventory and compatibility checks, (b) PQC algorithm and certificate strategy, (c) HSM PKCS#11 integration and key rotation, (d) canary/traffic-mirror rollout and drift detection, (e) observability and rollback criteria?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Goldman Sachs","LinkedIn","PayPal"]},{"id":"q-1040","question":"You're deploying a CNF-based UDP gateway across four data centers. A CVE requires hardware-backed attestation before image execution. Outline a concrete end-to-end rollout plan that (a) signs CNF images with Cosign and SBOMs, (b) enforces attestation via TPM 2.0-based attestation bundles and ImagePolicyWebhook, (c) rolls out region-by-region with traffic mirroring and per-tenant quotas, (d) implements drift detection and automated rollback on attestation failure, and (e) provides observability for attestation metrics and rollback triggers?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Adobe","OpenAI"]},{"id":"q-1071","question":"You're deploying a CNF-based API gateway across 2 Kubernetes clusters. A recent upgrade causes a cross-tenant data bleed under load due to a shared in-memory cache. Outline a beginner-friendly, end-to-end plan to fix and roll out safely: (a) reproduce in staging with two tenants and isolated traffic, (b) implement tenant-scoped cache keys and per-tenant isolation checks, (c) add unit/integration tests for isolation, (d) perform blue/green canary rollout with tenant-based traffic splits, (e) observability: per-tenant SLA metrics and automatic rollback if bleed is detected. Include a minimal code snippet for tenant-scoped cache key?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Discord","Stripe","Two Sigma"]},{"id":"q-1081","question":"You're deploying a CNF-based API gateway that uses workload identities for tenants. Describe an end-to-end plan to enforce identity attestation and per-tenant isolation using SPIRE for SPIFFE IDs, OPA Gatekeeper for policy decisions, and a local Kind testbed. Include (a) issuing and rotating SVIDs, (b) encoding tenant permissions in policies, (c) testing isolation with two tenants, and (d) observability hooks for attestation and policy evaluations?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Lyft","Meta"]},{"id":"q-1107","question":"You're deploying a CNF-based API gateway serving two tenants with strict data-retention and per-tenant log-redaction requirements. Outline an end-to-end plan using SPIRE for workload identities and OPA Gatekeeper for policy decisions to enforce data handling rules while enabling zero-downtime upgrades in a local Kind testbed. Include (a) SVID issuance/rotation tied to tenant IDs, (b) tenant-scoped policies for retention windows and redaction, (c) runtime redaction checks on logs/traces, (d) cross-tenant isolation testing under load, and (e) observability hooks for attestation, policy decisions, and redaction misses?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Google","Meta","Twitter"]},{"id":"q-1187","question":"A beginner CNF certification scenario: you implement a gated CI/CD pipeline for a CNF gateway image. Outline an end-to-end workflow to ensure image provenance before deployment: (a) sign CNF images with Cosign using a KMS-backed key, (b) generate and publish SBOMs, (c) enforce signatures via ImagePolicyWebhook, and (d) surface observability in the monitoring stack for signing success/failure and rollback signals. Provide concrete steps and minimal config snippets?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Instacart","LinkedIn","Tesla"]},{"id":"q-1221","question":"You're operating a CNF-based API gateway deployed across three regions behind a service mesh. Propose a practical upgrade workflow that enforces runtime integrity along with image attestations: (a) sign images with Cosign using a KMS-backed key and publish SBOMs, (b) require TPM/measured-boot attestation plus runtime integrity checks for CNFs, (c) roll out region-by-region with per-tenant canaries and live connection migration, (d) implement drift detection and automatic rollback on attestation/runtime mismatch, (e) surface observability for sign-off, attestation, and rollback triggers. Provide minimal config references?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Netflix","Plaid"]},{"id":"q-1347","question":"Advanced CNF certification scenario: deploy a CNF gateway across three air‑gapped data centers. Design an end‑to‑end plan to ensure image provenance and runtime integrity for updates, covering (a) offline Cosign signing with a TPM‑backed key, (b) SBOM generation and private catalog, (c) per‑tenant policy enforcement via OPA/Gatekeeper, (d) edge‑site offline attestation broker, (e) drift detection with automated rollback, and (f) observability hooks. Include concrete config examples?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Apple","Citadel","Oracle"]},{"id":"q-1511","question":"Beginner CNF certification: You manage a CNF API gateway in a single Kubernetes namespace with a private registry and no external access. Outline an end-to-end plan to enforce SLSA provenance for each image before deployment, including (1) build and sign with Cosign using a KMS-backed key, (2) generate and attach SPDX SBOMs, (3) verify SLSA provenance via ImagePolicyWebhook, (4) canary upgrades with rollback on provenance failure, (5) observability for signing events, SBOM validity, and rollback triggers?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Databricks","Hashicorp","Twitter"]},{"id":"q-1526","question":"In a CNF gateway spanning multiple tenants, implement runtime **per-tenant feature flags** controlled by a central policy server. Describe a concrete plan using SPIRE for tenant identities, OPA for policy evaluation, and a sidecar that hot-reloads policies from a central repo. Include how you detect flag revocation, perform zero-downtime updates, and observability hooks (SBOM provenance, traces, metrics)?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Netflix","Oracle"]},{"id":"q-1544","question":"You're managing a CNF-based API gateway deployed in two regions, serving three tenants with a single global CI/CD pipeline. Propose an end-to-end upgrade and rollback strategy that delivers zero downtime while guaranteeing strict tenant isolation during updates. Include deployment approach (canary vs blue-green), tenant-aware routing, per-tenant metrics and logging, rollback triggers, and how you would validate the plan in staging that mirrors production. Cite concrete tooling choices (Argo Rollouts, Istio/Envoy, OPA, Prometheus) and touchpoints?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["DoorDash","Snowflake"]},{"id":"q-1571","question":"You’re deploying a CNF API gateway across 6 regions and need strict per-tenant isolation with dynamic rate limits and auditable policy changes. Propose an end-to-end plan that uses SPIRE for per-tenant identities, OPA for policy decisions, Envoy with hot xDS updates, and a shared policy catalog. Include how you push canary upgrades, rollback on drift, and observability for quota hits and policy eval latency?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["IBM","Stripe","Uber"]},{"id":"q-1599","question":"Outline a beginner-friendly, end-to-end workflow to enforce image provenance and runtime integrity per-tenant before deployment, using Cosign signing, SPDX SBOMs, and admission controls (ImagePolicyWebhook + Gatekeeper/OPA). Provide concrete steps and minimal config examples?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Apple","IBM"]},{"id":"q-1674","question":"Advanced CNF scenario: deploy a CNF API gateway across three geographies with a private registry and multi-tenancy. Propose an end-to-end plan to enforce per-tenant image provenance and runtime integrity, including (i) per-tenant Cosign signing keys stored in KMS/HSM, (ii) per-tenant SBOMs, (iii) admission controls using ImagePolicyWebhook/OPA, (iv) tenant-aware drift detection and rollback, and (v) observability for signing, attestation, and rollback signals. Provide concrete steps and minimal config examples?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Hashicorp","IBM","Snowflake"]},{"id":"q-1735","question":"Beginner CNF certification: In a multi-tenant Kubernetes cluster hosting CNF gateways in separate namespaces, design an end-to-end image provenance gate: (1) sign images with Cosign using a KMS-backed key and attach SPDX SBOMs, (2) enforce with an ImagePolicyWebhook that requires both signature and SBOM presence, (3) surface a per-tenant visibility badge on deployments, (4) provide minimal config snippets and commands to validate end-to-end in one namespace before rolling out to others?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Google","Hashicorp","Snowflake"]},{"id":"q-1793","question":"In a multi-tenant cluster hosting CNF gateway images, design a beginner end-to-end flow to enforce per-tenant image provenance using Cosign signing with tenant keys, SPDX SBOMs, and a Gatekeeper constraint that reads a per-tenant policy from a ConfigMap. Include minimal YAML for the ConstraintTemplate and Constraint, plus compliant vs noncompliant Deployment manifests, and the exact kubectl steps to validate end-to-end in a single namespace first?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Coinbase","Snap"]},{"id":"q-1812","question":"In a beginner CNF certification scenario, you operate a multi-tenant CNF gateway platform on Kubernetes with a private image registry. Describe a concrete end-to-end workflow to enforce provenance and runtime integrity for every pull: (1) embed build provenance into image labels (commit SHA, CI job, build ID), (2) sign with Cosign using a KMS-backed key and attach SPDX SBOMs, (3) enforce per-tenant policy via Gatekeeper/OPA that rejects images missing provenance or SBOM, and (4) provide a minimal test plan and config snippets to validate in a single namespace before rolling out to others?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Cloudflare","Coinbase","Square"]},{"id":"q-1914","question":"In a multi-tenant CNF gateway managed by a central control plane, design an end-to-end upgrade workflow that uses per-tenant feature flags and a canary rollout with Istio, while enforcing runtime policy with OPA Gatekeeper and SPIRE-based identity. Include: (a) how CNF versions and flags are modeled in CRDs, (b) traffic splitting and health checks for safe canaries, (c) per-tenant policy evaluation, and (d) drift detection with automatic rollback and observability?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Cloudflare","NVIDIA","Twitter"]},{"id":"q-2030","question":"Beginner CNF certification: How would you design an end-to-end workflow in a multi-tenant Kubernetes cluster to enforce SBOM freshness and key-rotation awareness for CNF images, so that when a Cosign key rotates SBOMs are re-generated and re-attested, and a Kyverno policy rejects images signed with old keys or missing SBOMs, with a minimal namespace-scoped test?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["Anthropic","Cloudflare","NVIDIA"]},{"id":"q-2189","question":"Advanced CNF Certification: Design an end-to-end runtime policy and rollout flow to ensure that only CNFs with verifiable provenance and hardware compatibility SBOM attestations can allocate SR-IOV NICs across regions; describe data sources, policy versioning, canary rollout, and rollback, plus a minimal policy snippet?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Apple","OpenAI"]},{"id":"q-2262","question":"How would you implement an end-to-end runtime-provenance strategy for upgrades in a three-region, air-gapped CNF platform hosting stateful gateways across Kubernetes clusters, without pulling new images from the registry? Include image provenance checks, TPM-backed attestation via Sigstore, in-cluster drift detection with eBPF, and a safe, low-downtime rollback plan with minimal manifests?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Discord","NVIDIA","Slack"]},{"id":"q-2362","question":"Beginner CNF certification: In a multi-tenant Kubernetes cluster with a shared private registry, design an end-to-end image provenance gate that adds a freshness check: require Cosign-signed images with SPDX SBOMs, and enforce via Open Policy Agent that SBOM timestamp is within 7 days. Provide minimal constraint template, example annotation, and a one-namespace test plan?","channel":"cnf-certification","subChannel":"general","difficulty":"beginner","tags":["cnf-certification"],"companies":["MongoDB","Snowflake","Uber"]},{"id":"q-2486","question":"Advanced CNF certification: In a four region CNF gateway fleet with a private image registry, design a scalable end-to-end workflow for per tenant image provenance and runtime attestation. Include tenant-specific signing keys, SBOMs, admission controls, and a rollback plan for regional outages. Provide concrete steps and minimal config snippets you would actually use in practice?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Apple","Netflix","Salesforce"]},{"id":"q-2560","question":"Design an end-to-end CNF image provenance and runtime attestation workflow for a globally deployed API gateway across x86_64 and ARM edge nodes with intermittent connectivity. Include per-arch signing and SBOMs, cross-arch attestation claims, an OPA policy enforcing SBOM presence and trusted architecture, and a testing plan with canary rollouts and rollback criteria?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Goldman Sachs","IBM","NVIDIA"]},{"id":"q-2638","question":"In a CNF gateway deployed across multi-cloud Kubernetes clusters, outline an end-to-end plan to guarantee runtime integrity and tenant isolation during updates, by combining hardware-backed attestation (TPM/SEV), SBOM provenance, a private attestation service, SPIRE identities, and policy enforcement with OPA Gatekeeper. Include upgrade testing (canary and rollback) and observability hooks?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Amazon","Oracle"]},{"id":"q-847","question":"You're deploying a CNF gateway (e.g., NGINX CNF) on a 50-node Kubernetes cluster handling streaming traffic. A node eviction hits during peak load. Outline a concrete plan to maintain streaming availability, focusing on (1) graceful drain with preStop, (2) health checks/readiness/liveness, and (3) traffic affinity and pod topology, and (4) observability and rollout strategy?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Netflix","Plaid","Tesla"]},{"id":"q-918","question":"You're rolling out a CNF-based NAT gateway across a 3-region multi-cluster Kubernetes setup. A policy change must be applied without disrupting live traffic. Outline a concrete, end-to-end rollout plan emphasizing (a) shadow canaries with traffic mirroring, (b) region-by-region rollout with per-region SLI targets, (c) policy-state reconciliation and drift detection, (d) rollback conditions and observability instrumentation?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["Citadel","Coinbase"]},{"id":"q-935","question":"You manage CNF-based gateways across 4 regions. A suspected supply-chain compromise requires enforcing in-cluster image attestations before rollout without downtime. Outline an end-to-end plan to (a) sign CNF images with Cosign and SBOMs, (b) enforce signatures via ImagePolicyWebhook, (c) roll out region-by-region with traffic mirroring, (d) implement drift detection and automatic rollback on attestation failure. Include observability?","channel":"cnf-certification","subChannel":"general","difficulty":"intermediate","tags":["cnf-certification"],"companies":["IBM","Tesla","Two Sigma"]},{"id":"q-985","question":"Design a zero-downtime, CNF-based API gateway rollout where per-tenant routing rules update live without dropping connections. Outline end-to-end steps: (a) safe rule distribution, (b) canary-ingress slicing with weighted traffic, (c) drift detection between desired and active routes, (d) observability and rollback?","channel":"cnf-certification","subChannel":"general","difficulty":"advanced","tags":["cnf-certification"],"companies":["Goldman Sachs","Meta","Tesla"]},{"id":"q-1051","question":"CNPA stack: HTTP API writes to PostgreSQL and emits Kafka events. A hot, large users table needs a non-blocking schema change (e.g., adding a new NOT NULL column with default). Propose a production-grade online migration plan that minimizes downtime, keeps Kafka in sync, handles backfill, and describes rollback and validation steps?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Amazon","Instacart","Plaid"]},{"id":"q-1150","question":"In a CNPA stack: HTTP API ingests events, writes to PostgreSQL, and publishes to Kafka; during spikes, retries duplicate processed events. Propose a concrete, end-to-end plan to guarantee idempotent processing and prevent duplicates under retry storms. Include idempotency key strategy, dedup enforcement point, Kafka/DB coordination, and validation?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Netflix","Snap","Twitter"]},{"id":"q-1157","question":"CNPA pipeline uses an HTTP API -> PostgreSQL -> Kafka with Avro schemas in a Schema Registry. A new optional field is added to the events, and some consumers crash when they see older versions. Provide a concrete, zero-downtime plan for schema evolution, including compatibility mode, rollout strategy, topic/consumer changes, backfill approach, rollback, and validation?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Anthropic","Hugging Face","Salesforce"]},{"id":"q-1188","question":"CNPA stack: an HTTP API writes to PostgreSQL and publishes to Kafka. During peak load, duplicate events may be produced due to retries and at-least-once semantics. Describe a concrete end-to-end plan to enforce idempotent processing across HTTP, DB, and Kafka, including dedupe strategy, upsert/constraints, transactional writes, offset tracking, and how you’d validate correctness under load?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Meta","PayPal","Tesla"]},{"id":"q-1283","question":"CNPA stack with HTTP API, PostgreSQL, Kafka, and Redis: a Redis-based rate limiter fronting the API causes legitimate bursts to be 429-throttled during a promo, despite normal traffic. Provide a concrete debugging plan to isolate whether latency or errors come from Redis Lua script, Redis network, the HTTP handler, or downstream services, with exact metrics and concrete fixes and how you’d validate impact?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["DoorDash","PayPal"]},{"id":"q-1383","question":"CNPA stack with HTTP API writing to PostgreSQL, publishing to Kafka, and Elasticsearch dashboards. A nightly backfill misses events, causing dashboards to report incorrect counts. Describe a concrete debugging plan to isolate whether loss occurs in HTTP write/transaction, Postgres-to-Kafka CDC, or Kafka-to-Elasticsearch sink, with exact metrics, sampling, and concrete fixes (idempotent sinks, transactional writes, producer retries, dedup IDs) and how you would verify end-to-end?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Databricks","Robinhood"]},{"id":"q-1401","question":"CNPA stack: HTTP API writes events to PostgreSQL, publishes to Kafka with event_time metadata, and a downstream analytics service reads from Kafka to produce 5-minute windowed counts. After a release, a dashboard shows both late counts and misaligned windows. Provide a concrete debugging plan to determine if the issue is event-time timestamps, Kafka timestamps, consumer windowing, or clock skew across services, including exact metrics, sampling, and fixes (re-timestamp, watermarking, idempotent sinks) and how you would validate end-to-end correctness?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Amazon","Apple"]},{"id":"q-1440","question":"In a CNPA stack with an HTTP API writing to PostgreSQL and publishing to Kafka, add an optional field customer_segment; design a zero-downtime schema evolution and payload versioning plan, including DB changes, Kafka message formats, consumer upgrades, backfill, and validation?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Apple","NVIDIA","Zoom"]},{"id":"q-1586","question":"In a CNPA stack with an HTTP API writing to PostgreSQL, publishing to Kafka, and Elasticsearch/Redis downstream, a burst causes duplicate rows in Postgres and delayed dashboard freshness. Propose a concrete end-to-end exactly-once plan: transactional Kafka producers, Postgres outbox, idempotent Elasticsearch sinks, Redis invalidation, and verification steps, plus rollback if needed?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Airbnb","IBM","Snowflake"]},{"id":"q-1630","question":"In a CNPA stack with an HTTP API, PostgreSQL, Kafka, and Redis, dashboards display stale data for a cohort after a deployment; propose a concrete debugging plan to determine whether drift originates from writes to Postgres, the Kafka sink, or Redis caching, including exact metrics to collect, sampling strategy, and concrete fixes (transactional outbox, idempotent sinks, Redis invalidation, cache-warming) and how you would verify impact?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["NVIDIA","Plaid"]},{"id":"q-1716","question":"CNPA stack: an HTTP API writes to PostgreSQL and publishes to Kafka. A schema evolution adds a new optional field to the event payload stored in Postgres and emitted to Kafka. Propose a concrete migration plan that preserves compatibility, uses a versioned envelope, coordinates writes and reads, validates with end-to-end tests, and provides a safe rollback. Include concrete steps, metrics, and rollback strategy?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Databricks","Google"]},{"id":"q-1732","question":"CNPA stack: HTTP API writes to PostgreSQL, emits to Kafka, and a Redis-backed read cache. A schema change adds a new optional field to the event payload; rollout under peak load leads to some consumers crashing due to compatibility. Provide a concrete, practical rollout and debugging plan to ensure no data loss or outages, including steps, metrics, and concrete changes (schema registry, backward/forward compatibility tests, dual-write, feature flags, cache invalidation) and how you would verify success?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Adobe","Coinbase","MongoDB"]},{"id":"q-1751","question":"CNPA stack: HTTP API → PostgreSQL → Kafka → stream processor → Redis dashboards. A new audit requires end-to-end latency visibility for late events during windows; latency spikes 2–3 minutes. Provide a concrete debugging plan to pinpoint whether the delay lies in HTTP ingress, DB writes, Kafka publish, stream windowing, or Redis caching. Include exact metrics, sampling, and fixes (idempotent sinks, transactional outbox, watermark tuning, checkpointing) and how you’d verify impact?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Goldman Sachs","Instacart","Snowflake"]},{"id":"q-1794","question":"In a CNPA stack—HTTP API, PostgreSQL, Kafka, Redis—latency spikes appear on a new endpoint that touches all components. Outline a practical plan to implement end-to-end tracing with a correlation_id: where to instrument, which metrics to collect (end-to-end latency, per-service latency, queue time, DB time, cache misses), and concrete changes (propagate correlation_id in HTTP, persist it in DB, attach it to Kafka messages, emit trace spans). How would you validate in staging before production?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Bloomberg","Google","Hugging Face"]},{"id":"q-1851","question":"In a CNPA stack: HTTP API ingests events, writes to PostgreSQL, and publishes to Kafka. A new enrichment step guarded by a feature flag calls a 3rd-party service. Design a concrete, end-to-end rollout plan that minimizes risk: per-tenant flag rollout, fallback behavior when the service is down, tracing with correlation IDs, circuit breaker, and backpressure; metrics to monitor (p50/p95 latency, error rate, backlog), rollback criteria, and validation steps?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Discord","Salesforce"]},{"id":"q-1875","question":"CNPA stack: HTTP API writes to PostgreSQL and publishes to Kafka; downstream analytics reads from Kafka and loads into a data warehouse. During deployment, dashboards drift and latency tails widen. Create a concrete, end-to-end debugging plan to isolate whether the root cause is HTTP serialization, DB write latency, Kafka publish, consumer, or ETL/warehouse load, with exact metrics, sampling, and concrete fixes (outbox pattern, idempotent sinks, backpressure, staged deploy) and verification steps?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Hashicorp","Slack","Snowflake"]},{"id":"q-1956","question":"In a CNPA stack: HTTP API ingests events, writes to PostgreSQL, and publishes to Kafka. A new optional field customer_region must be added without downtime or data loss. Describe a concrete, end-to-end migration plan: schema changes, producer/consumer compatibility, backfill strategy, validation checks, and rollback. Include metrics and canary signals to verify success?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["DoorDash","Oracle","Two Sigma"]},{"id":"q-2041","question":"In a CNPA stack (HTTP API -> Postgres -> Kafka -> Redis) you notice tail latency spikes during peak hours. Outline a concrete, beginner-friendly plan to diagnose end-to-end latency using distributed tracing. Include what to instrument, where to insert spans (HTTP handler, DB query, Kafka producer/consumer, Redis ops), how to propagate trace context, minimal metrics, and a simple verification checklist to confirm fixes?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Hashicorp","Lyft","Snap"]},{"id":"q-2351","question":"CNPA pipeline: HTTP API writes to PostgreSQL and publishes events to Kafka. A rollout requires strict per-user ordering across a high-throughput, multi-partition topic; out-of-order deliveries break dashboards. Describe a concrete plan to guarantee per-user ordering while preserving throughput: data-path changes (outbox with transactional writes, per-user partitioning), producer settings (acks=all, enable.idempotence, max.in.flight=1, batch/linger), idempotent sinks, testing (replay, verifications per user), and rollback. Include concrete metrics and verification steps?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Anthropic","Plaid","Twitter"]},{"id":"q-2414","question":"In a CNPA stack where an HTTP API ingests JSON events, writes to PostgreSQL, and publishes to Kafka, a deployment adds a required field 'customerTier' to the payload. Older clients omit it. Outline a concrete, practical debugging plan to locate the failure point and implement a safe rollout using a backward-compatible schema, optional field, or a schema registry with compatibility settings, plus a minimal migration and a feature flag. Include exact steps, data checks, and how you would verify no data loss?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Google","IBM"]},{"id":"q-2450","question":"CNPA stack: HTTP API writes to PostgreSQL and publishes to Kafka. A feature flag routes writes through a shadow sink and a shadow Kafka topic. When enabled, dashboards show delayed data and downstream consumers occasionally duplicate messages due to retries. Provide a concrete debugging plan to verify data correctness and latency impact, including idempotency, outbox, transactional boundaries, and rollback procedures; specify metrics, sampling, and verification steps?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Amazon","Netflix","Robinhood"]},{"id":"q-2489","question":"CNPA stack: HTTP API writes to PostgreSQL and publishes events to Kafka using Avro; a schema migration is requested while dashboards rely on Redis caches updated by a Kafka consumer. Outline a concrete, end-to-end rollout plan that guarantees backward/forward compatibility, zero downtime, and data correctness. Include schema registry strategy, compatibility modes, dual-write/outbox techniques, canary rollout, monitoring, and rollback criteria with concrete change sets?","channel":"cnpa","subChannel":"general","difficulty":"advanced","tags":["cnpa"],"companies":["Amazon","Anthropic","Databricks"]},{"id":"q-2539","question":"CNPA stack with an HTTP API, PostgreSQL, and Kafka: a new schema migration on a hot table runs during peak, causing write latency spikes and intermittent 5xx errors. Provide a concrete debugging plan to isolate whether the bottleneck is the HTTP handler, the migration's locks in PostgreSQL, or the Kafka sink, with exact SQLs, metrics, and concrete fixes (online migrations, lock avoidance, prepared statements, idempotent sinks) and a validation strategy?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["DoorDash","Netflix","Tesla"]},{"id":"q-845","question":"In a MongoDB-backed service, read latency tails spike during peak hours. Provide a concrete, practical debugging plan to determine whether the bottleneck is network, driver, query plan, or index design. Include exact steps, metrics to collect, and concrete changes (indexes, readConcern, pooling) you would apply, plus how you would verify impact?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Anthropic","Lyft","MongoDB"]},{"id":"q-873","question":"A CNPA service receives events over HTTP, writes to PostgreSQL, and publishes to Kafka. During peak hours, read latency spikes. Describe a concrete debugging plan to determine whether the bottleneck is the HTTP handler, the DB query, or the Kafka producer, including exact steps, metrics to collect, and concrete changes (indexes, pooling, prepared statements, idempotent producer) and how you would verify impact?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Databricks","Instacart","NVIDIA"]},{"id":"q-903","question":"In a CNPA stack, an HTTP API writes to Postgres, publishes to Kafka, and a Redis-backed dashboard consumes the stream. During peak load, HTTP latency tails spike. Provide a concrete debugging plan to isolate whether the bottleneck is HTTP, DB, Kafka, producer, consumer, or Redis, with exact metrics, sampling, and concrete changes (pooling, prepared statements, acks, batch sizes, caching strategies) and how you would verify impact?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["Bloomberg","Meta","Tesla"]},{"id":"q-928","question":"In a CNPA stack consisting of an HTTP API, PostgreSQL, Kafka, and Redis, latency tails spike under peak load. Provide a concrete, beginner-friendly plan to enable end-to-end tracing with OpenTelemetry to pinpoint the bottleneck. Include trace propagation, spans for HTTP handler, DB query, Kafka publish/consume, Redis access, and verification steps with a sample end-to-end trace?","channel":"cnpa","subChannel":"general","difficulty":"beginner","tags":["cnpa"],"companies":["Discord","Hashicorp","Netflix"]},{"id":"q-957","question":"CNPA stack with HTTP API, PostgreSQL, Kafka, and Redis dashboards requires a schema evolution: add a new optional field region_id to event records without downtime or breaking producers/consumers. Describe a practical, step-by-step migration plan: DB changes, schema registry versioning, producer/consumer updates, data backfill, testing, and rollback strategies, ensuring end-to-end consistency?","channel":"cnpa","subChannel":"general","difficulty":"intermediate","tags":["cnpa"],"companies":["PayPal","Scale Ai"]},{"id":"q-1193","question":"You have an array A of n positive integers and a threshold T. For a given window length L, define ok(L) as: there exists a subarray of length L with sum <= T. Design an O(n) check for ok(L) using a sliding window, then outline how to find the maximum L with binary search over [1..n], and analyze total time and space. Include edge-case handling and practical optimizations?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Airbnb","Snowflake","Tesla"]},{"id":"q-1254","question":"You're given a DAG G=(V,E) with N nodes and M edges. Edges can be inserted online in batches of size B. Design a dynamic transitive-closure using bitsets to answer reachability queries in O(1). Provide initialization, amortized per-batch update time, and memory. Include two practical optimizations and compare to recomputing the closure after each batch?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Goldman Sachs","Square","Two Sigma"]},{"id":"q-2159","question":"Scenario: In a weighted directed graph with nonnegative weights, you must maintain approximate betweenness centrality for all nodes under batches of edge insertions of size B. Propose a concrete, implementable scheme that (i) initializes estimates, (ii) updates after each batch with amortized cost, (iii) bounds memory, and (iv) provides guaranteed error bounds ε. Include two practical optimizations and compare to re-running Brandes' algorithm after every batch?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["LinkedIn","Snowflake","Two Sigma"]},{"id":"q-2497","question":"Maintain exact single-source shortest paths from a fixed hub h under online edge insertions (batches). Propose a practical scheme that (i) initializes with Dijkstra, (ii) after each batch re-relaxes only affected nodes using a min-heap, (iii) bounds memory, and (iv) gives amortized update time. Include two optimizations and compare to rerunning Dijkstra after every batch. Provide asymptotics in n, m, B?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Meta","Oracle","PayPal"]},{"id":"q-2635","question":"Scenario: A real-time analytics service ingests event messages with a 32-bit integer key. Events arrive in batches of size B every second, and the system must report the number of distinct keys seen so far (cardinality) with a configurable error ε. Propose a practical streaming algorithm and data structure (no deletions) to maintain an ε-approximate cardinality. After each batch, specify (i) per-element insertion time and total batch time, (ii) space usage, (iii) query time for the current estimate, (iv) two optimizations, and (v) how this compares to re-scanning all seen events after every batch. Use asymptotics in n (seen events), B, and ε?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Hashicorp","PayPal","Two Sigma"]},{"id":"q-2655","question":"Design a dynamic (1+ε)-spanner for an undirected weighted graph under online edge insertions in batches of size B. Build a (k,ε)-spanner with O(n^{1+1/k}) edges. After each batch, update only edges that improve distances beyond ε; amortized O(B log n). Query dist via the spanner in O(1). Memory O(n^{1+1/k}). Optimizations: lazy rebuild; reuse tight edges. Compare to rebuilding: faster updates; worst-case near O(n^{1+1/k}) per batch?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Robinhood","Zoom"]},{"id":"q-678","question":"In a directed acyclic graph with N nodes and M edges, all edge costs are nonnegative. Compute the minimum-cost path from S to T. Costs can decrease online; design a strategy to maintain shortest paths with updates, aiming for sublinear re-computation on average. Provide initial complexity and amortized update complexity, plus memory usage and practical optimizations?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Lyft","Meta","Oracle"]},{"id":"q-690","question":"Design a data structure to support two online operations on an integer array A of length N: 1) rangeAdd(l, r, delta) adds delta to A[i] for l <= i <= r, 2) queryMaxSubarray() returns the maximum subarray sum of the current A. Provide a structure that supports both operations in O(log N) time, describe what to store per node, how to merge children, and how to apply a lazy add. Include correctness and complexity considerations?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Goldman Sachs","Microsoft"]},{"id":"q-700","question":"You're building a real-time analytics dashboard that shows the top-k most frequent event types from a high-volume log stream (e.g., clicks, errors). Each event has a string type. Design a data structure and algorithm to maintain the current top-k frequencies with online increments, aiming for roughly O(log k) update time and O(n) memory. Explain how you handle ties and memory growth, and compare with a naive approach that re-sorts after every insert?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Bloomberg","Google"]},{"id":"q-704","question":"Scenario: a directed graph with nonnegative weights and a fixed source S. Each batch updates up to B edges (increases or decreases). Propose a practical dynamic data structure to maintain exact distances from S to all nodes and answer distance queries S→T in polylog time, with sublinear amortized update time. Compare to rerunning Dijkstra after every batch; include expected bounds, memory usage, and practical heuristics?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Salesforce","Snap"]},{"id":"q-709","question":"Given a directed graph with nonnegative weights, a fixed source S, and a stream of online edge weight updates (both increases and decreases), design a dynamic SSSP data structure that maintains exact distances dist(S, v) for all v after each update. Aim for sublinear amortized update time per edge change; specify initial preprocessing, worst-case vs amortized bounds, memory usage, and practical optimizations. Provide a plan for applying this in a traffic-graph scenario with frequent but localized updates?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["OpenAI","PayPal"]},{"id":"q-721","question":"Given a fixed directed graph with nonnegative weights and a single source S, handle a batch of edge-weight decreases (no insertions/deletions). Design a dynamic algorithm to update the exact S→v distances after each batch with sublinear amortized per-edge cost. Specify data structures, provide an amortized bound, and discuss memory and practical optimizations for real-time traffic networks?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Lyft","NVIDIA","Two Sigma"]},{"id":"q-732","question":"Scenario: A data stream yields integers. At each time step, a new value enters a sliding window of fixed size W, and the oldest value leaves. Design a solution to maintain the top-2 most frequent values in the current window with fast updates. Compare a naive O(W) recompute to an augmented structure using a frequency map and a max-heap with lazy deletions. Provide update and query complexities and memory usage?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Meta","Twitter","Two Sigma"]},{"id":"q-740","question":"Scenario: An edge CDN collects response times in milliseconds for every request. Design a beginner-friendly online algorithm to maintain the median latency as new times arrive, using only inserts. Explain the data structure, update steps, and time/space bounds, assuming up to 1e6 entries?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Cloudflare","Coinbase","Oracle"]},{"id":"q-742","question":"In a DAG with N nodes and M edges, nonnegative edge weights. You maintain shortest-path distances from source S to a fixed set of target nodes {T1,...,Tk}. Edge weights can only decrease over time due to updates. After a batch of updates, you should update only the target distances that can improve, avoiding full re-relaxation. Propose a practical algorithm that lazily propagates decreases using the DAG’s topological order, such that total work across updates is sublinear on average. Provide update and query steps, concrete time bounds, and memory usage, plus optimizations?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Microsoft","Robinhood"]},{"id":"q-755","question":"You're maintaining real-time travel times in a citywide road network modeled as a weighted directed graph with nonnegative costs. Costs can only decrease as new data arrives. Design an incremental algorithm to keep the shortest-path distances from a fixed hub S to all nodes up-to-date after each edge-cost decrease, aiming for sublinear amortized update work. Provide initial SSSP complexity, amortized per-decrease update, memory usage, and practical optimization strategies?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Discord","Google","Zoom"]},{"id":"q-766","question":"## Prompt\n\nIn a dynamic directed graph G=(V,E) with nonnegative weights, edge latencies only decrease in batches. Design a practical, production-ready algorithm to maintain a (1+ε)-approximate SSSP tree from a source S under these updates, enabling distance queries dist(S,v) in O(log|V|) time. Target sublinear amortized update in |E| for batch updates, and linear space. Explain data structures, update bounds, and how you bound cascade effects?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Discord","Meta","OpenAI"]},{"id":"q-770","question":"Given a directed graph G=(V,E) with |V|=N and |E|=M, nonnegative weights, support online operations: 1) decreaseWeight(u,v,delta) with delta>0, 2) queryShortestPath(S,T) returning current shortest path length. Updates and queries are interleaved. Propose a data-structure and algorithm that achieves sublinear amortized reprocessing per update, justify amortized bounds, and discuss space and practical optimizations for massive graphs (N up to 1e6, M up to 1e7)?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Citadel","Lyft"]},{"id":"q-779","question":"You're building a real-time analytics component for a fintech platform. You must maintain the number of distinct values in the most recent W events in a streaming fashion. Implement two operations: append(v) to push a new event value, and distinctCount() to return the number of unique values among the last W events. Assume values are integers up to 1e9 and W can be large. Provide a simple approach with its time/memory costs, then describe an amortized-constant-time solution using a hashmap plus a circular buffer, and discuss edge cases (e.g., large W, many duplicates). How would you implement and analyze it?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Databricks","LinkedIn","Robinhood"]},{"id":"q-787","question":"You maintain N players with integer scores in the range 0..10000. You must support two online operations: 1) update(i, s) — set player i's score to s; 2) countLE(X) — return how many players have score <= X. Propose a data structure and algorithm to support both in O(log V) time per operation, where V=10001, and analyze space usage. Include initialization and a brief correctness sketch?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["Apple","Cloudflare","Snowflake"]},{"id":"q-796","question":"Given an array A of length N with integers in range [0, R). You implement a function to count distinct values by inserting each A[i] into a hash set, then return its size. 1) What is the time and space complexity in terms of N and D (distinct values)? 2) If R is small, propose a memory-efficient alternative and analyze its tradeoffs?","channel":"complexity-analysis","subChannel":"general","difficulty":"beginner","tags":["complexity-analysis"],"companies":["DoorDash","Netflix","PayPal"]},{"id":"q-803","question":"In a directed graph G=(V,E) with N nodes, M edges and nonnegative weights, a fixed source S, and an SSSP tree T. Edges can change weight online (increase or decrease), but no edges are added or removed. Propose a concrete, implementable strategy to maintain the SSSP efficiently, including data structures, update rules, and expected time bounds. Provide preprocessing, amortized per-update, and memory usage, plus practical optimizations and a concrete scenario where it shines (e.g., streaming latency updates)?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Instacart","LinkedIn"]},{"id":"q-808","question":"Dynamic path counting in a DAG: maintain the number of S→T paths of length at most L under online edge insertions and deletions. Propose a data structure and amortized update time bound in terms of N, M, L and #updates; discuss memory usage and how to handle large L and modulo arithmetic in practice?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Citadel","Snowflake","Zoom"]},{"id":"q-817","question":"Design a dynamic, multi-source shortest-path maintenance scheme for a directed graph with nonnegative weights. A fixed set of K hub nodes H must always have exact shortest-path distances to all nodes. Edges can be inserted or weights decreased online, in batches of size at most B. Provide initial preprocessing and a full-update algorithm, with (i) initial time, (ii) amortized update time per batch, and (iii) memory usage. Include two practical optimizations and compare to recomputing from scratch after each batch?","channel":"complexity-analysis","subChannel":"general","difficulty":"advanced","tags":["complexity-analysis"],"companies":["Bloomberg","PayPal","Uber"]},{"id":"q-827","question":"Design a dynamic distance labeling scheme for an undirected weighted graph that supports edge insertions and deletions online. Use a fixed hub set H to enable dist(u,v) queries via dist(u,v)=min_h dist(u,h)+dist(h,v) only if H covers all shortest paths. Explain maintenance of hub distances under updates, and bound update/query times and memory usage. Provide two optimizations and a comparison to recomputing from scratch?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Google","Meta","Microsoft"]},{"id":"q-831","question":"In an undirected weighted graph G=(V,E) with nonnegative weights, design a (1+ε)-approximate distance oracle based on a fixed landmark set L (|L|=k). Edges are only inserted online in batches of size B; no deletions. After each batch, specify: (i) preprocessing time and space to build distances from every landmark, (ii) amortized update time per batch to update the oracle, (iii) query time for dist(u,v), (iv) total memory, (v) two practical optimizations, (vi) a comparison to rebuilding all-pairs distances after each batch. Provide concrete asymptotics in terms of n=|V|, m=|E|, k, ε, B?","channel":"complexity-analysis","subChannel":"general","difficulty":"intermediate","tags":["complexity-analysis"],"companies":["Instacart","Microsoft","NVIDIA"]},{"id":"q-1028","question":"Your team runs a multi-tenant SaaS analytics platform on Kubernetes in AWS. Each tenant lives in an isolated namespace with 100+ microservices. Explain how you enforce least privilege RBAC, ephemeral credentials with automatic rotation, secrets management, mTLS/workload identity (SPIFFE/SPIRE), policy-driven runtime enforcement, and a playbook for detecting and responding to cross-tenant data exfiltration?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Airbnb","Google","Meta"]},{"id":"q-1034","question":"In a multi-region SaaS platform running 3,000 microservices on Kubernetes in AWS with a shared data lake, design a secure data-access flow that enforces least privilege, uses workload identity (SPIFFE/SPIRE), ephemeral credentials with rotation, and policy-driven runtime checks. Include tenant isolation, secret management, and a playbook for cross-tenant data exfiltration?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Netflix","PayPal","Zoom"]},{"id":"q-1079","question":"In a multi-tenant rideshare analytics platform with per-tenant clusters and a shared data lake, design a zero-trust data-access pattern that enforces least privilege, uses SPIFFE/SPIRE for workload identity, ephemeral credentials with rotation, and policy-driven runtime checks. Include tenant isolation, secret management, cross-tenant data-exfiltration playbooks, and a DR/IR plan?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Lyft","Snap"]},{"id":"q-1315","question":"Scenario: A Databricks-based analytics platform on AWS feeds Slack alerts. Engineers share notebooks and data assets across teams with minimal controls. Propose a practical, beginner-friendly security plan that enforces least privilege using Unity Catalog RBAC, secrets management with rotation, and a CI check to block secrets in code, plus a 48-hour incident playbook for credential leakage or cross-tenant exposure?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Databricks","Slack"]},{"id":"q-1362","question":"Scenario: A multi-tenant data platform serving Apple, Adobe, and Snowflake runs data jobs in a shared Kubernetes cluster with per-tenant namespaces. An operator could pivot roles to access other tenants. Design a practical access-control plan enforcing least privilege via ABAC, dynamic approvals for sensitive actions, policy-drift detection, and a 24–72 hour rollback/incidence playbook. Include tools like OPA, AWS IAM, Kubernetes RBAC, Vault?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Adobe","Apple","Snowflake"]},{"id":"q-1379","question":"Scenario: A multi-tenant SaaS on Kubernetes in AWS uses a shared MongoDB Atlas cluster and Vault-managed secrets; Oracle reports data. Design a practical plan to enforce least privilege, issue per-tenant Vault DB credentials with short lifetimes, bound to tenant service accounts, include CI checks to block secrets in code, and a 48-hour IR playbook for credential leakage?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Hashicorp","MongoDB","Oracle"]},{"id":"q-1406","question":"In a multi-tenant data platform on AWS with a shared data lake and event bus, outline a practical plan to prevent cross-tenant leakage as data and events flow through data-plane and message-plane. Include (1) ABAC with OPA, (2) SPIRE-like identities and mutual TLS, (3) ephemeral credentials with rotation to data stores and topics, (4) drift detection/remediation, and (5) a 72-hour IR playbook for credential leakage or cross-tenant exposure, with concrete tool choices and trade-offs?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Hashicorp","Plaid","Twitter"]},{"id":"q-1501","question":"In a Slack-like multi-tenant platform deployed on Kubernetes in AWS with a central analytics data lake, a leaked tenant admin token is used to attempt cross-tenant data access via the data API. Design a practical, intermediate-level test plan to validate and harden per-tenant isolation. Include concrete abuse vectors to test, enforcement via RBAC/ABAC/OPA, SPIRE/mTLS, ephemeral credentials, and per-tenant secrets; outline CI checks, observability gates, and a concise IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Citadel","Slack","Square"]},{"id":"q-1529","question":"In a Kubernetes-based multi-tenant analytics platform on AWS where data and logs traverse a shared data plane to a central observability stack, design a production-ready plan to prevent cross-tenant data leakage via log/metric pipelines. Include enforcement with RBAC/ABAC/OPA, SPIRE/mTLS, ephemeral credentials, and per-tenant secrets; detail CI checks, observability gates, and an IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Lyft","Meta","MongoDB"]},{"id":"q-1686","question":"Context: A beginner security analyst joins a multi-tenant SaaS deployed on Google Cloud. The platform runs on GKE and a central data lake; tenants are isolated by per-tenant namespaces with restricted data access. Secrets live in Secret Manager and workloads use Cloud IAM + Workload Identity; CI/CD has a secret-scanning step. Design a practical, beginner-friendly security plan to enforce least privilege, implement secrets rotation, add a CI check to block secrets in code, and prepare a 48-hour incident playbook for credential leakage or cross-tenant exposure. Specify concrete steps, tooling, and success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Google","Instacart","Robinhood"]},{"id":"q-1889","question":"In a global multi-tenant analytics platform on Kubernetes with a shared data lake, design an advanced, concrete plan to verify and harden per-tenant isolation against cross-tenant data access, focusing on dynamic ABAC with OPA, SPIRE/mTLS, ephemeral credentials, and per-tenant secrets. Include abuse vectors, CI gates, observability checks, and an IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Cloudflare","Meta","Zoom"]},{"id":"q-1938","question":"In a beginner security role for a multi-tenant delivery platform on AWS, tenants store order data in per-tenant prefixes under a shared data lake. A developer accidentally attaches a Lambda execution role with broad access to all buckets. Propose a practical, beginner-friendly security plan to enforce least privilege with per-tenant IAM roles and bucket policies, implement per-tenant key rotation via Secrets Manager, and add a CI check to block broad policies before merges, plus a 48-hour incident playbook for credential leakage or cross-tenant exposure. Include concrete steps and success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["DoorDash","Hashicorp","Instacart"]},{"id":"q-2000","question":"In a Kubernetes cluster utilizing MIG partitions for shared NVIDIA GPUs, how would you enforce per-tenant GPU isolation and protect models and data from cross-tenant leakage? Outline architecture, policy controls (RBAC/ABAC/OPA), SPIRE/mTLS, ephemeral credentials, per-tenant secrets; include CI tests, observability gates, and a concise IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["NVIDIA","Square"]},{"id":"q-2091","question":"Design a practical per-tenant data-sharing broker for a SaaS analytics platform that must allow approved external collaborators to access only specific tenant datasets for a limited time. Propose the key-management model (per-tenant CMKs and envelope keys), a policy-driven issuance flow (OPA), service identities (SPIRE), and short-lived credentials. Include rotation cadence, audit/logging, and a concise IR/DR plan with success criteria. Give concrete steps and success metrics?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Amazon","Bloomberg","Databricks"]},{"id":"q-2114","question":"In a multi-tenant SaaS on AWS with a Kubernetes cluster and a shared artifact store, how would you harden the CI/CD pipeline to prevent tenant-confusion and supply-chain abuse during builds and releases? Include architecture: per-tenant runners, SPIRE/mTLS, Vault tenant roles, image signing, OPA policy checks, SBOM validation, per-tenant secret rotation, plus concrete test vectors and an IR/DR plan with measurable success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Apple","Snowflake"]},{"id":"q-2141","question":"In a cloud-native platform deployed to Kubernetes with GitOps pipelines, design an intermediate-level test plan to secure the software supply chain end-to-end, including SBOM provenance (SLSA), per-tenant secrets, and policy enforcement with OPA; outline CI gates, runtime observability, and an IR/DR playbook for a compromised dependency or insertions into the build process?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["Cloudflare","Microsoft","Tesla"]},{"id":"q-2222","question":"In a cloud-native analytics platform where tenants upload datasets and train models against a shared data lake, a feature store isolates tenant data but allows cross-tenant model export. Design a concrete, intermediate-test plan to prevent leakage via cross-tenant feature reuse or model exports, including policy checks (OPA), RBAC/ABAC, per-tenant secrets, and CI/CD gates; include abuse vectors, observability gates, and an IR/DR playbook?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["IBM","Snap","Two Sigma"]},{"id":"q-2297","question":"In a cloud-native SaaS on Kubernetes with a centralized, multi-tenant logging/observability stack (OpenTelemetry + OpenSearch) and a shared data lake, design an intermediate test plan to prevent tenant data leakage through logs and traces. Specify how to ensure per-tenant isolation in logs, enforce redaction and tenant-scoped indexing, ABAC/OPA gates at ingest, SPIRE/mTLS between agents and collectors, ephemeral credentials for exporters, CI checks, observability gates, and a concise IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"intermediate","tags":["comptia-security-plus"],"companies":["DoorDash","Hashicorp","MongoDB"]},{"id":"q-2400","question":"Scenario: A multi-tenant e-commerce platform (Airbnb-like) processes guest and host data in a centralized data lake on AWS. A new data export feature allows hosts to download their own booking data as CSV; misconfigured IAM/bucket policy could leak other tenants' data. Propose a beginner-friendly security plan to enforce least privilege using IAM roles, S3 bucket policies, and KMS, add a CI gate to block secrets in code, and draft a 48-hour IR playbook for credential leakage or cross-tenant exposure; include concrete steps, tooling, and success metrics?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Airbnb","Amazon"]},{"id":"q-2472","question":"Context: A DoorDash-like platform runs on Kubernetes in AWS with a centralized analytics data lake and edge workloads. A rogue build signs a container image that deploys and tries tenant data access via shared APIs. Design an advanced test plan to prevent supply-chain abuse and enforce tenant isolation from build to runtime. Include Sigstore signing with attestations, OPA-based policies, per-tenant secrets, ephemeral credentials via SPIRE, CI image-sign checks, and a concise 60-minute IR/DR playbook with success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["DoorDash","Lyft","OpenAI"]},{"id":"q-2516","question":"Context: A beginner security analyst at a large tech org reviews a Meta/Adobe‑like SaaS portal with a central data lake. Developers push via GitHub Actions; tenants are isolated by per‑tenant RBAC. A tenant API key was leaked and used to probe other tenants. Design a practical, beginner-friendly security plan to enforce least privilege with per‑tenant RBAC, rotate secrets automatically, add a CI gate to block secrets in code, and a 48‑hour IR/DR playbook for credential leakage or cross‑tenant exposure. Include concrete steps, tooling, and success criteria?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Adobe","Meta"]},{"id":"q-2668","question":"In a beginner-friendly AWS multi-tenant SaaS on Scale AI and Amazon, tenants are isolated with per-tenant IAM roles and tenant prefixes in a central data lake. A developer accidentally commits a long‑lived credential to a public repo. Propose concrete steps to enforce least privilege (RBAC), implement automatic secret rotation, add a CI gate to block secrets in code, and craft a 24‑hour IR playbook for credential leakage or cross‑tenant exposure, including tooling?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Amazon","Scale Ai"]},{"id":"q-883","question":"Describe a practical, scalable secure software supply-chain workflow for an automotive platform with OTA-enabled ECUs, leveraging SBOMs, code signing, and runtime attestation. Include concrete steps for CI/CD isolation, secret management, artifact signing, and incident response?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Hashicorp","Salesforce","Tesla"]},{"id":"q-974","question":"Given a global fintech platform delivering real-time risk analytics via a fleet of edge appliances and cloud microservices, outline a practical, end-to-end security workflow for secure software and firmware updates that ensures provenance, integrity, and trust. Include: SBOMs, code signing, runtime attestation for both containers and devices; CI/CD isolation and secrets management; artifact signing and key rotation; secure OTA rollout with canarying and rollback; and incident response playbooks?","channel":"comptia-security-plus","subChannel":"general","difficulty":"advanced","tags":["comptia-security-plus"],"companies":["Goldman Sachs","Meta","Tesla"]},{"id":"q-988","question":"Scenario: A fintech app runs microservices in Kubernetes on AWS and Cloud Run on GCP. You must implement least privilege and dynamic secrets for a data-access service. Outline a practical, beginner-friendly workflow to give the service temporary credentials to a Postgres DB, with a choice between Vault or cloud-native secret managers. Include CI/CD integration, rotation, RBAC, and auditability?","channel":"comptia-security-plus","subChannel":"general","difficulty":"beginner","tags":["comptia-security-plus"],"companies":["Bloomberg","Citadel","Cloudflare"]},{"id":"q-1018","question":"You’re building a mobile camera app that auto-captures frames from a live video feed. Describe a practical, beginner-friendly pipeline to decide whether a frame is usable by applying two simple checks: (1) sharpness via variance of Laplacian, (2) exposure via histogram-based brightness. Explain how thresholds would be chosen, how you'd adapt them across lighting, and provide a minimal code snippet illustrating the core checks?","channel":"computer-vision","subChannel":"general","difficulty":"beginner","tags":["computer-vision"],"companies":["Goldman Sachs","LinkedIn","NVIDIA"]},{"id":"q-1093","question":"Design a privacy-preserving, real-time hand-gesture recognition system for video calls on consumer laptops (720p camera) that distinguishes a small set of gestures (hand-raise, thumbs-up, peace) without exposing facial details. Must run on-device at 30–60 FPS, handle lighting/occlusion, and support federated fine-tuning with differential privacy. Outline architecture, data strategy, and evaluation plan?","channel":"computer-vision","subChannel":"general","difficulty":"intermediate","tags":["computer-vision"],"companies":["Hashicorp","Tesla","Zoom"]},{"id":"q-1108","question":"Design a real-time, on-device hand pose/gesture system for air-drawing in a video-conferencing app. From a monocular 1080p60 stream, infer 2D/3D hand pose with <40ms latency on a CPU, robust to occlusion and varied skin tones, and support at least 5 gestures (draw, erase, next, previous, pointer). Outline data needs, model architecture, latency optimizations, temporal consistency, and evaluation plan?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Adobe","Plaid","Zoom"]},{"id":"q-1270","question":"Design a real-time monocular 3D detector for an assembly line that estimates 6-DoF pose of tools from a single RGB camera, achieving sub-50ms per frame on embedded hardware. Use a lightweight backbone with self-supervised pretraining plus a small labeled set; recover pose via differentiable PnP from 2D-3D correspondences with a temporal filter and reprojection losses. Monitor drift with streaming per-frame errors?","channel":"computer-vision","subChannel":"general","difficulty":"intermediate","tags":["computer-vision"],"companies":["Goldman Sachs","Hashicorp","MongoDB"]},{"id":"q-1417","question":"Given a single RGB camera mounted on an autonomous delivery drone operating in urban environments, design a real-time system to detect and track pedestrians and other vulnerable actors at 30 fps under nighttime and rain conditions. Propose data strategy (synthetic rain + real), model backbone, temporal fusion and latency targets, and safety/failover mechanisms?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Amazon","Cloudflare","Uber"]},{"id":"q-1435","question":"For aerial inspection of solar farms, design a CV system to detect and grade micro-cracks on solar panels from drone video with limited labeled data. Specify an architecture that detects tiny defects, data-augmentation strategies (synthetic crack overlays, texture randomization), domain adaptation, and an edge-friendly output (box, segmentation mask, and severity score). Include evaluation protocol and latency targets?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Bloomberg","MongoDB","Scale Ai"]},{"id":"q-1861","question":"You're given an overhead RGB image of a table with scattered coins. Design a beginner-friendly pipeline to count the number of coins and estimate their approximate denomination from a single image. Include preprocessing, circle/contour detection, radius-based grouping to separate coin types, handling shadows, and a minimal code sketch using OpenCV to detect circular shapes?","channel":"computer-vision","subChannel":"general","difficulty":"beginner","tags":["computer-vision"],"companies":["Goldman Sachs","Instacart","Snowflake"]},{"id":"q-1945","question":"Design a real-time anomaly-detection pipeline for a single RGB camera monitoring an industrial warehouse. Propose a memory-augmented autoencoder approach that runs on an edge device at 15–20 FPS, uses frame-wise reconstruction error plus optical-flow residuals, maintains a fixed-size normal-pattern memory, and includes a drift-adaptation strategy with minimal labeling?","channel":"computer-vision","subChannel":"general","difficulty":"intermediate","tags":["computer-vision"],"companies":["Hashicorp","Hugging Face"]},{"id":"q-2131","question":"In a factory setting, design a real-time 2-view RGB tool-pose tracking system that achieves sub-60ms per frame on edge hardware. Use two synchronized cameras, a lightweight backbone, and a differentiable PnP with 2D-3D correspondences; include cross-view fusion, a temporal filter, and a self-supervised pretraining strategy with synthetic data. How would you validate drift and occlusion resilience?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Adobe","OpenAI","Two Sigma"]},{"id":"q-2242","question":"In a warehouse setting, design an edge-friendly CV pipeline that counts and localizes pallets from a single moving RGB camera mounted on a forklift, using self-supervised depth cues from motion and a lightweight 3D detector to output per-pallet 3D bounding boxes with uncertainty at 30 Hz. Address occlusion, dynamic workers, and domain shift between day and night?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Lyft","Square"]},{"id":"q-2333","question":"Design a beginner-friendly pipeline to detect and count red boxes moving on a conveyor using a single RGB camera in real time. Include HSV color space selection, dual-range red thresholds, noise removal with morphology, contour filtering by area/shape, and a simple line-crossing tracker. Provide a minimal OpenCV.js code snippet for core steps?","channel":"computer-vision","subChannel":"general","difficulty":"beginner","tags":["computer-vision"],"companies":["Coinbase","Google","Tesla"]},{"id":"q-2377","question":"Scenario: fixed overhead RGB camera watches a single shoebox on a shelf. Design a beginner-friendly, non-deep-learning pipeline to decide whether the box is upright within ±10 degrees using contour-based detection. Explain preprocessing, edge/shape heuristics, thresholds, and how to handle perspective distortion and occlusion. Include a minimal Python OpenCV snippet that outputs 'upright' or 'tilted'?","channel":"computer-vision","subChannel":"general","difficulty":"beginner","tags":["computer-vision"],"companies":["Oracle","Scale Ai"]},{"id":"q-2546","question":"Design a real-time monocular hand pose and gesture recognition system for AR UI on a battery-constrained headset. Propose a lightweight architecture (e.g., two-branch network with 2D heatmaps and a temporal encoder), latency target <25 ms per frame on edge hardware, occlusion handling, and 3D hand pose estimation—describe data strategy, loss terms, and evaluation?","channel":"computer-vision","subChannel":"general","difficulty":"intermediate","tags":["computer-vision"],"companies":["Cloudflare","NVIDIA","Snap"]},{"id":"q-456","question":"How would you design a real-time object detection system for a social media platform that processes 10M images/day with 99.9% accuracy and <100ms latency?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Microsoft","Salesforce","Twitter"]},{"id":"q-487","question":"Design a real-time object detection system for DoorDash delivery vehicles that must identify packages, license plates, and traffic signs in varying weather conditions. How would you handle model optimization for edge deployment and ensure 99% accuracy?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["DoorDash","Google"]},{"id":"q-517","question":"Design a real-time object detection system for cryptocurrency trading terminals that must detect and classify multiple monitor types, trading interfaces, and unauthorized screen recording devices with <100ms latency. How would you optimize YOLOv8 for this specific use case?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Amazon","Coinbase","Meta"]},{"id":"q-545","question":"How would you detect if an image contains a face using basic computer vision techniques?","channel":"computer-vision","subChannel":"general","difficulty":"beginner","tags":["computer-vision"],"companies":["Airbnb","NVIDIA","Netflix"]},{"id":"q-570","question":"How would you design a real-time object detection system for Airbnb's property listing photos that can identify amenities and safety violations while processing 10,000 images per hour?","channel":"computer-vision","subChannel":"general","difficulty":"advanced","tags":["computer-vision"],"companies":["Airbnb","Instacart"]},{"id":"q-274","question":"How would you implement a hybrid CNN architecture combining ResNet residual connections with EfficientNet compound scaling for production image classification?","channel":"computer-vision","subChannel":"image-classification","difficulty":"intermediate","tags":["cnn","resnet","efficientnet"],"companies":["Amazon","Google","Meta","Microsoft"]},{"id":"q-253","question":"How does YOLO implement real-time object detection using grid-based prediction and what are the key components of its architecture?","channel":"computer-vision","subChannel":"object-detection","difficulty":"beginner","tags":["yolo","rcnn","detr"],"companies":["Amazon","Google","Meta","Microsoft","NVIDIA","Tesla"]},{"id":"q-200","question":"How does U-Net's skip connection architecture enable precise medical image segmentation?","channel":"computer-vision","subChannel":"segmentation","difficulty":"beginner","tags":["unet","mask-rcnn","sam"],"companies":["Amazon","Google","Meta"]},{"id":"q-228","question":"How would you optimize a real-time medical image segmentation pipeline using SAM with 100ms latency constraint on edge devices?","channel":"computer-vision","subChannel":"segmentation","difficulty":"advanced","tags":["unet","mask-rcnn","sam"],"companies":["Apple","Google","Meta","Microsoft","NVIDIA"]},{"id":"q-1080","question":"In a log-processing pipeline, multiple producers enqueue log entries into a bounded, rate-limited work queue. Implement a beginner-friendly token-bucket rate limiter that allows enqueuing only when a token is available; a background timer refills tokens at a fixed rate up to a max. Producers block when tokens==0; workers process items from the queue. Provide a concrete Go/Python/Java solution and discuss testing?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Bloomberg","Lyft","NVIDIA"]},{"id":"q-1204","question":"In a real-time notification system with thousands of tenants, each tenant's events must be delivered to their own handler in order, while cross-tenant processing happens in parallel. Design a bounded, multi-queue architecture with per-tenant in-order guarantees, backpressure, and graceful shutdown. Compare two backpressure strategies (per-tenant token buckets vs. global credits) and discuss testing and failure scenarios. Provide runnable sketch in Go or Rust?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Scale Ai","Tesla"]},{"id":"q-2080","question":"In a real-time chat system, multiple producers enqueue messages into per-room bounded queues and multiple consumers deliver to connected clients. Design a beginner-friendly concurrency solution (Go, Java, or Python) that guarantees producers block when a room's queue is full and consumers block when the queue is empty, while preserving per-room isolation and simple backpressure. Compare a mutex/condition-based queue versus a channel-based approach for bounded queues?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Discord","Hashicorp"]},{"id":"q-2329","question":"Design a bounded, concurrent router for a real-time analytics platform ingesting events from thousands of clients. Each client's events must be processed in arrival order, while events from different clients may be processed in parallel. The router should implement per-client backpressure and support clients joining/leaving on the fly. Describe two concrete implementations: (A) per-client queues with a central scheduler using locks, (B) per-client lock-free queues with a shared ready-list. Include correctness, failure handling, and a practical test plan?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Google","Instacart","Stripe"]},{"id":"q-2542","question":"Design a two-stage, bounded-concurrency pipeline for real-time video frames from multiple cameras. Stage 1 decodes frames (CPU-bound) using a fixed thread pool; Stage 2 runs an AI-based enhancer asynchronously. Ensure per-frame ordering across all cameras, backpressure to keep queues bounded, support dynamic worker scaling and clean shutdown with cancellation, and provide a runnable minimal sketch in a language of your choice?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Airbnb","Google","Hugging Face"]},{"id":"q-2606","question":"Design an in-process, bounded-concurrency event bus for a streaming service that ingests events from hundreds of producers (per-video id) and delivers to a pool of workers while guaranteeing per-video in-order processing, minimal latency, and backpressure signaling to producers when queues fill. Include a runnable sketch in Go or Rust and discuss testing?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Airbnb","Amazon","LinkedIn"]},{"id":"q-2658","question":"Design a bounded, fair, multi-producer/multi-consumer system for real-time order matching in a crypto exchange. Many producers submit limit orders; multiple workers attempt matches against an in-memory order book. Guarantee bounded memory, backpressure, and starvation-free fairness across producers (e.g., per-producer quotas or weighted fairness). Describe data structures, synchronization, and a minimal runnable Java snippet showing enqueue and a worker loop?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Coinbase","DoorDash","Robinhood"]},{"id":"q-682","question":"In a service handling image uploads, each file triggers a resize and thumbnail generation pipeline. Design a bounded producer-consumer queue in Python using asyncio. Use a Queue with maxsize, a fixed number of worker coroutines, and backpressure so producers await when full. Include clean shutdown and error handling. Provide a runnable minimal example showing enqueue, worker loop, and cancellation?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Discord","Lyft","Snap"]},{"id":"q-687","question":"Write a small Python snippet: create a shared counter initialized to 0, spawn 4 threads that increment it 1000 times each, using a Lock to protect the increment. After joining, print the final value. Explain what happens if the lock is removed and how atomicity is ensured. What value do you expect and why?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Google","Tesla","Two Sigma"]},{"id":"q-696","question":"Context: in a real-time analytics pipeline for a video-conference system, dozens of producers emit events into a shared queue and multiple workers consume them. Implement a bounded, multi-producer, multi-consumer queue with capacity 1024. It must not drop messages while not full, block producers when full, support concurrent consumers, and support a clean shutdown. Describe API, invariants, and a simple synchronization strategy?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Hugging Face","Zoom"]},{"id":"q-708","question":"Design a bounded, concurrent, multi-priority work scheduler for a rendering service: 3 priority levels (0 highest). Producers enqueue tasks into per-priority queues with backpressure; workers drain from the highest non-empty queue (0→1→2). Include aging to prevent starvation and a graceful shutdown sentinel. Provide a runnable minimal Java example?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Adobe","Salesforce","Snap"]},{"id":"q-713","question":"You’re building a web service that enqueues tasks from many request handlers into a shared, fixed-capacity queue consumed by a single worker. Implement a thread-safe bounded queue with a fixed circular buffer, a mutex, not_full and not_empty condition variables, and a shutdown signal. Show enqueue and dequeue semantics and how shutdown behaves?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Adobe","Cloudflare","Tesla"]},{"id":"q-720","question":"Design a bounded worker pool for a high-throughput API gateway that queues work with backpressure. Use a lock-free ring buffer, N workers, and blocking enqueue when full. Include per-task cancellation, timeouts, and metrics. Compare Go, Rust, and Java trade-offs and explain how you’d debug data races and starvation under burst traffic?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Hugging Face","Square"]},{"id":"q-726","question":"Design a concurrent event broker for a live chat service that guarantees per-user in-order delivery while allowing parallel processing across users, using a bounded buffer; describe backpressure handling when the buffer fills, and compare locking versus lock-free approaches in your language of choice?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Meta","Microsoft","Zoom"]},{"id":"q-735","question":"Design a concurrency-safe per-channel message queue for a Discord-like chat service: multiple producers push messages, a single consumer persists them to storage; implement bounded capacity, preserve per-channel order, and handle backpressure. Which synchronization primitives would you use and how would you test edge cases?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Discord","Snowflake"]},{"id":"q-748","question":"You're building a real-time analytics pipeline with many shards. Propose a concurrent, bounded path that preserves per-shard in-order processing while enabling cross-shard parallelism. Design a data structure and protocol (enqueuing, routing, backpressure, and shutdown). Provide a runnable sketch in Go or Rust showing enqueue, per-shard consumer loop, and graceful shutdown?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["NVIDIA","Snap","Twitter"]},{"id":"q-756","question":"Implement a concurrent task-graph executor with dependencies. Tasks form a DAG; a task runs only after all its prerequisites complete. Use a bounded in-flight task pool, per-worker work stealing, and deadlock/backpressure handling. Provide a runnable Go or Rust sketch showing submission, ready-state transitions, worker loop, and graceful shutdown?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Lyft","NVIDIA"]},{"id":"q-759","question":"In a multi-threaded microservice, there is a shared in-memory counter for total processed events. Provide a concrete, beginner-friendly approach to implement a thread-safe increment using language primitives (Java, Go, or Python) and discuss the trade-offs between lock-based vs lock-free solutions when scaling across cores?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Bloomberg","Coinbase","MongoDB"]},{"id":"q-773","question":"In a Go HTTP server, you maintain a per-user quota counter in memory. Multiple requests for different users should advance concurrently, but updates to the same user must be serialized. Design a striped lock (a fixed set of mutexes) to guard a map[string]int, map userID to a bucket via hashing, and explain how you minimize contention, handle hash collisions, and ensure correctness when entries may be evicted?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["MongoDB","NVIDIA","Plaid"]},{"id":"q-780","question":"Implement a lock-free, bounded multi-producer, multi-consumer pipeline with per-symbol partitions and fixed-capacity queues. Producers must acquire credits to enqueue; downstream workers release credits on completion. Explain memory visibility, false sharing avoidance, and a clean shutdown. How do you guarantee in-order per partition and exactly-once processing on failure?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Bloomberg","Microsoft","PayPal"]},{"id":"q-788","question":"In a streaming data pipeline, multiple producers generate frames and a bounded buffer sits between the producer stage and a consumer stage. Implement a backpressure-enabled producer-consumer pair in JavaScript (Node.js) using an async bounded queue that supports multiple producers and multiple consumers. It must guarantee no data loss, bounded memory, and graceful shutdown. How would you test under varying production/consumption rates?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Citadel","Tesla"]},{"id":"q-795","question":"Design a dynamic, concurrency-safe worker pool in Go for a real-time analytics pipeline. Each task type has its own queue; enforce backpressure with bounded channels and implement a central scaler that adjusts worker counts based on observed tail latency and throughput. Provide a runnable skeleton showing enqueue, worker loops, and graceful shutdown?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Databricks","Meta","Microsoft"]},{"id":"q-804","question":"You're building a video-frame ingestion service where frames from thousands of users arrive on a shared input channel. Design a bounded, concurrent dispatcher that routes frames to per-user in-order queues, enabling parallel processing across users. Provide backpressure handling when the global buffer fills, a strategy for reordering out-of-order frames, and a graceful shutdown. Sketch the core data structures and synchronization in Rust or C++ and explain trade-offs?","channel":"concurrency","subChannel":"general","difficulty":"intermediate","tags":["concurrency"],"companies":["Hugging Face","Netflix","Snap"]},{"id":"q-811","question":"In a real-time analytics service, multiple producers enqueue data into a fixed-size circular buffer consumed by multiple workers. Design a beginner-friendly concurrency solution in Java, Go, or Python that guarantees producers block when the buffer is full and consumers block when empty, while maintaining correctness under concurrent access; compare lock-based vs channel-based approaches for this bounded buffer?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Databricks","Oracle","Two Sigma"]},{"id":"q-818","question":"In a build pipeline, N compile tasks run in parallel and must all finish before the linker runs. Design a barrier that blocks each task at barrier.wait() until all N tasks have arrived, then releases them to proceed. Implement two beginner-friendly approaches in Go (or Java/Python): (A) locks/condition variables; (B) channels/futures. Ensure the barrier is reusable for repeated builds, avoids deadlocks, and explain correctness and trade-offs?","channel":"concurrency","subChannel":"general","difficulty":"beginner","tags":["concurrency"],"companies":["Discord","Two Sigma"]},{"id":"q-825","question":"In a real-time chat moderation pipeline for a platform like Discord/LinkedIn, design a dynamic Go worker pool that scales from minWorkers to maxWorkers based on a bounded task queue. The queue should backpressure producers, guarantee per-user fairness, support graceful shutdown, and tolerate occasional misbehaving workers (timeouts). Provide runnable code showing enqueue, worker loop, and scaler?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Discord","LinkedIn"]},{"id":"q-835","question":"Design a multi-tenant dispatcher for a streaming event bus. Producers from each tenant push events into per-tenant input channels; a central dispatcher interleaves tenants fairly, enforces per-tenant rate limits using a token-bucket, and enforces a maximum in-flight events per tenant. Implement in Go using channels; ensure backpressure propagates to producers when quotas or in-flight limits are reached, and support graceful shutdown. Include a runnable minimal example with: per-tenant limiter, dispatcher loop, producer(s), and cancellation?","channel":"concurrency","subChannel":"general","difficulty":"advanced","tags":["concurrency"],"companies":["Airbnb","NVIDIA","Slack"]},{"id":"q-1027","question":"You're running a mixed Consul Connect mesh with Kubernetes services in DC1 and VM-based services in DC2. A new API service in DC1 calls a legacy VM backend behind a firewall via a mesh gateway, but TLS handshakes intermittently fail after a CA rotation. Propose a zero-downtime plan to diagnose, implement automatic CA rotation, and validate end-to-end, including config changes, monitoring, and rollback steps?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Cloudflare","LinkedIn","Microsoft"]},{"id":"q-1178","question":"Across a tri-cloud Consul Connect mesh, a new microservice 'payments-api' in namespace 'payments' must reach a legacy data service 'orderdb' in namespace 'legacy' via a mesh gateway. Propose an end-to-end pattern that enforces strict identity via namespace-scoped Intention defaults, per-service tokens, and gateway ACLs, including resource definitions, deployment steps, and rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Anthropic","DoorDash","Tesla"]},{"id":"q-1293","question":"In a hybrid setup with Consul across two Kubernetes clusters (AWS) and VM-based services on-prem, how would you design cross-cluster service authentication and discovery using Consul Connect with mesh gateways, Namespaces, and ACLs to enforce zero-trust policy and automatic token rotation while preserving DNS-based discovery?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Microsoft","Snowflake"]},{"id":"q-1334","question":"Scenario: You operate a mixed environment with Consul Connect across Kubernetes namespaces dev (auth-service) and prod (user-service). How would you implement an Intentions policy that allows auth-service to call user-service only on port 8080, with default deny for all other traffic, and enable audit logging? Describe the commands, tokens, and how you would verify enforcement from a small client pod?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["NVIDIA","Robinhood","Snap"]},{"id":"q-1348","question":"Scenario: A global application runs in three environments (prod, staging, dev) across Kubernetes in GCP and legacy VM services on‑prem, all using Consul Connect. Design a zero-trust mesh that uses Vault for dynamic secrets, namespace-scoped ACL tokens, and mesh gateways for cross-cluster traffic. Explain how you would rotate credentials automatically, enforce service-to-service ACLs, and validate safety during partial outages?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["IBM","MongoDB"]},{"id":"q-1369","question":"Three-datacenter Consul Connect mesh (DC1 Kubernetes, DC2 VM, DC3 on‑prem) needs zero-downtime TLS credential rotation using Vault PKI and Consul CA integration. Describe a concrete plan including Vault roles, TTLs, renewal cadence, root CA distribution, per-service identity, and a rollback procedure with deployment steps and rollback commands?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Google","MongoDB"]},{"id":"q-1408","question":"Design a zero-downtime certificate rotation strategy for a Consul Connect mesh spanning Kubernetes and VM-based services across two clusters. How would you coordinate CA rotation, per-service identity tokens, and mesh gateway trust so that mutual-TLS remains valid during rotation and new services automatically trust the updated CA without downtime?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Google","Twitter","Two Sigma"]},{"id":"q-1429","question":"In a Consul Connect mesh spanning two Kubernetes clusters, a frontend service in cluster A must call a private external billing API behind a VPC. The API requires mTLS and tenant-scoped API keys that rotate every 24 hours. Design a scalable egress pattern using a Mesh Gateway for outbound traffic, per-service Intentions with explicit allow rules, TLS credential rotation via Vault, and a rollback plan. Include sample manifests, deployment steps, and failure recovery?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Discord","Twitter"]},{"id":"q-1465","question":"In a Consul Connect mesh spanning Kubernetes and VM workloads, an external analytics service must fetch data from internal microservices via mesh gateways but cannot run a sidecar. Design a secure bridge pattern: per-request identity, TLS with a bridge certificate minted by Consul, short-lived credentials rotated automatically, and revocation without downtime. Include concrete resource definitions, deployment steps, and rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Citadel","NVIDIA","Two Sigma"]},{"id":"q-1470","question":"In a multi-cluster Consul Connect mesh spanning Kubernetes and VMs, design a tenant-aware, dynamic access control model where new tenants join/leave at runtime without service downtime. How would you implement per-tenant namespaces, ephemeral tokens, and policy synchronization to enforce zero-trust across tenant boundaries? Include concrete steps and constraints?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Goldman Sachs","Square","Tesla"]},{"id":"q-1525","question":"In a Consul Connect mesh, two services communicate over gRPC. One is in Kubernetes, the other is VM-based without a sidecar. Design a secure bridge via a mesh gateway to enable mutual-TLS and per-request identity, including how to configure the gRPC wiring, gateway policy, and a minimal manifest for the gateway and client. Include a rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Airbnb","IBM","Meta"]},{"id":"q-1669","question":"In a single cluster with two namespaces, dev and prod, a frontend service in dev must call a backend service in prod through a Consul mesh gateway. Design a beginner-friendly end-to-end setup: register both services, enable a mesh gateway in prod, add a route, ensure mTLS with short-lived certs, and include a rollback plan if the gateway fails. Include concrete YAML fragments and commands?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Hashicorp","Hugging Face","Robinhood"]},{"id":"q-1693","question":"In a Consul Connect mesh spanning Kubernetes and VM workloads, how would you implement scalable, per-service access with automatic token rotation and zero-downtime revocation for an external analytics app calling internal services via mesh gateways? Include identity model (SPIFFE IDs), Vault-based token lifecycle, per-service ACLs, mesh-gateway configuration, and a rolling update plus rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Apple","Scale Ai","Square"]},{"id":"q-1795","question":"In a Consul Connect-enabled Kubernetes cluster, a new Python gRPC service must call a legacy on-prem TCP service via a mesh gateway using TCP mode (no HTTP). Draft a minimal setup: (1) service definitions and ACLs, (2) mesh gateway TCP config, (3) per-service identity, tokens, and rotation, (4) a rollback plan if handshake fails?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Oracle","Tesla","Twitter"]},{"id":"q-1849","question":"In a Consul Connect mesh spanning Kubernetes in DC1 and VM-based services in DC2, a compromised service identified by SPIFFE ID spiffe://example.org/compromised-frontend.dc1 must be revoked mesh-wide within minutes. Design a reproducible, low-downtime remediation workflow: revoke tokens, rotate TLS certs, refresh CA bundles on mesh gateways, enforce temporary ACL lockdown, and verify no unauthorized calls remain. Include concrete resource definitions, rotation steps, and rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Apple","Two Sigma"]},{"id":"q-2031","question":"Design a three-region Consul Connect mesh where 500+ services span Kubernetes clusters in US/EU and VM-based services in APAC, with a partner-facing API exposed via a mesh gateway. Describe your approach to: (1) identity and ACL strategy across regions, (2) automatic TLS certificate rotation backed by Vault, (3) WAN federation topology, (4) zero-downtime token revocation, and (5) testing/rollout plan with rollback steps?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Two Sigma","Uber"]},{"id":"q-2065","question":"Scenario: a Kubernetes-based checkout calls inventory. A new canary version inventory-v2 is deployed on both Kubernetes and VM-backed services. Implement a 20% canary split using Consul Connect service-router, gate by a health check, and allow quick rollback to 0% canary. Provide minimal service-router config, deployment labels, and a rollback process with commands to reweight or disable v2?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Amazon","DoorDash","Oracle"]},{"id":"q-2126","question":"Design a per-tenant isolation in a mixed Kubernetes/VM Consul Connect mesh with two tenants (alpha, beta). Implement namespace-scoped ACLs and per-tenant tokens so payments-alpha can access ledger-alpha only, and payments-beta ledger-beta only. Provide service-router rules using identity, a short-lived token rotation plan, and an automated test that verifies cross-tenant access is denied and a rollback path to re-allow access if misconfig is detected?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Snowflake","Tesla","Twitter"]},{"id":"q-2236","question":"In a two-DC Consul Connect mesh (DC1 and DC2), DC1 experiences an outage. Design a reproducible DR workflow to promote DC2 as primary within minutes, preserving TLS identity, ACLs, and service discovery. Include concrete resource definitions for ACLs/Intentions, mesh-gateway config, Vault-integrated token rotation, CA bundle refresh, and a rollback plan with tests and canary semantics?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Hashicorp","Snowflake","Tesla"]},{"id":"q-2321","question":"In a Consul Connect mesh spanning Kubernetes in DC1 and VM workloads in DC2, a rolling upgrade to ingest-api triggers intermittent TLS handshake failures with data-warehouse. Propose a concrete, low-downtime remediation workflow that rotates Vault-backed TLS certs, refreshes CA bundles on mesh gateways, updates ACLs/Intentions to permit only existing paths during the window, and provides a rollback plan. Include example resource definitions and step-by-step commands to verify success?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Apple","Microsoft","MongoDB"]},{"id":"q-2335","question":"Design a Consul Connect mesh that spans two Kubernetes clusters (prod and analytics) and an on‑prem VM fleet. A data-ingest service in prod must securely call downstream processors during business hours only, with per‑request identity binding, zero‑trust ACLs, and automatic token rotation via Vault. Explain how you'd implement cross‑environment identity, time‑bound access, credential rotation without downtime, and auditability. Include concrete config excerpts for both clusters and the VM host?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Amazon","Lyft","Snowflake"]},{"id":"q-2458","question":"Three-DC Consul Connect mesh: in DC1 a real-time feed service is exposed via a mesh gateway; in DC2 a Kubernetes data-collector calls it with SPIFFE IDs and Vault-managed TLS rotation. Design a production-ready flow to enforce least privilege, support automatic certificate rotation, and achieve zero-downtime reconfiguration of Intentions and ACLs, plus rollback. Include concrete resource blocks for ACLs, Intents, mesh-gateway, and Vault rotation steps?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Robinhood","Snowflake"]},{"id":"q-2491","question":"In a Consul Connect mesh spanning Kubernetes and VM workloads, a newly registered VM-based service `reporter` cannot call `data-api` in the Kubernetes cluster after an ACL policy revision. Outline a beginner-friendly, end-to-end debugging workflow to diagnose and fix cross-namespace authorization, including minimal commands, policy fragments, and how to verify per-call identity?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Goldman Sachs","Twitter"]},{"id":"q-2507","question":"In a Consul Connect mesh spanning Kubernetes (DC1) and VM workloads (DC2), a policy change must allow a new service ingestor to reach data-warehouse only under canary conditions. Design a practical, automated canary workflow for ACL/Intentions changes: isolate the canary, run end-to-end tests, validate identity via SPIFFE IDs, rotate tokens automatically, and rollback on failure. Include concrete policy fragments, CI steps, and rollback criteria?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Discord","Microsoft","PayPal"]},{"id":"q-2548","question":"In a two-DC Consul Connect mesh, a new ingest-service in DC1 must securely push time-series data to a real-time analytics sink in DC2. Design a scalable, auditable Intentions workflow that uses SPIFFE IDs, per-service ACLs, and dynamic token rotation, with a canary rollout and rollback plan. Include concrete policy fragments (ACL/Intentions), a test harness, and a rollback trigger?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["MongoDB","Twitter"]},{"id":"q-2684","question":"In a two-DC Consul Connect mesh (DC1 Kubernetes, DC2 VM workloads), a new ingest service in DC1 must call a data-sink in DC2 via a mesh gateway. Design a practical, low-downtime migration that enforces per-service SPIFFE IDs, tenant isolation, and automatic TLS cert rotation. Include concrete Intentions/ACL fragments, canary rollout criteria, rollback triggers, and verification steps across both DCs?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["Cloudflare","Meta","Slack"]},{"id":"q-880","question":"In a Consul Connect-enabled dev cluster, two services run in namespace dev: 'reviews' and 'ratings'. You want to enforce that only reviews can call ratings via the mesh, with all other cross-service calls denied by default. Describe the minimal service-defaults and service-intentions changes needed, and how you would verify using a small client container in dev?","channel":"consul-associate","subChannel":"general","difficulty":"beginner","tags":["consul-associate"],"companies":["Amazon","Google","Square"]},{"id":"q-919","question":"You're operating a multi-datacenter Consul Connect mesh. A new API service in DC1 must reach a legacy monolith in DC2 that cannot run a sidecar. Design a cross-datacenter connectivity pattern using a mesh gateway, per-service Intentions with explicit allow rules, and TLS credential rotation. Include resource definitions, deployment steps, and rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"advanced","tags":["consul-associate"],"companies":["Anthropic","Hugging Face","PayPal"]},{"id":"q-999","question":"Design a cross-datacenter Consul Connect pattern in a three-datacenter setup where api-service.dc1 must reach legacy-db.dc3 (no sidecar). Use a MeshGateway to bridge DC1↔DC3, per-service Intentions with explicit allow rules, and TLS credential rotation. Include concrete resource definitions, deployment steps, and a rollback plan?","channel":"consul-associate","subChannel":"general","difficulty":"intermediate","tags":["consul-associate"],"companies":["NVIDIA","Salesforce","Two Sigma"]},{"id":"q-1077","question":"Design a data ingestion and processing pipeline for a global ride-hailing platform that ingests 5 TB/day of operational events from multiple regional Kafka topics and batch feeds. Requirements: idempotent upserts into a table (Iceberg/Delta), handle late-arriving events, schema evolution, and partition pruning by country/date. Compare Flink vs Spark for streaming, and outline testing, monitoring, and data quality checks?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Amazon","LinkedIn","Lyft"]},{"id":"q-1140","question":"Given daily 1 GB of web logs in JSON lines stored on S3 with fields user_id, timestamp, path, status, and optional referrer, design a beginner-friendly pipeline (Python or Node.js) that deduplicates by timestamp+user_id+path, validates required fields, normalizes timestamp to UTC, and writes date-partitioned Parquet to a data lake; include basic tests and monitoring?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Discord","NVIDIA","Stripe"]},{"id":"q-1272","question":"Design a data pipeline to ingest 2M GPU telemetry events per minute from a global fleet of AI training clusters into a data lake and a feature store. Events include host_id, region, timestamp, metric_type, and value. Requirements: immutable raw Parquet storage partitioned by region/hour; near-real-time metrics and anomaly alerts (1–2 minute latency) via a streaming engine; idempotent upserts into a feature store; schema evolution handling; late-arriving data; cost-aware storage/compute; monitoring and tests; compare Spark vs Flink for streaming components?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["Apple","NVIDIA"]},{"id":"q-1498","question":"Design a global data ingestion and governance pipeline for a real-time ad-tech platform that processes 200k events/sec from 3 cloud regions into a centralized lakehouse. Each event has event_id, tenant_id, timestamp, event_type, and payload. Requirements: enforce data contracts via a central registry (schema + compatibility rules), support schema evolution with automatic catalog updates, and ensure multi-tenant data isolation and access control. Implement partitioning by tenant/date, handle late-arriving data within a 1–2 minute SLA, ensure data lineage and quality checks, and provide rollback semantics for contracts. Compare Iceberg vs Delta as the storage layer, outline testing/monitoring, and describe concrete example schemas and contract definitions. Include how you'd validate end-to-end?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Scale Ai","Tesla"]},{"id":"q-1692","question":"In a social app generating 100 GB/day of newline-delimited JSON events across 3 regions stored in S3, design a beginner-friendly batch pipeline that validates fields (event_id, user_id, timestamp, event_type), derives date, deduplicates by event_id, hashes user_id for privacy, and writes Parquet partitioned by date to a data lake. Compute a daily data-quality score (0-1) based on missing/invalid fields and store it in a metadata table. Outline tooling, testing, and monitoring?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Anthropic","PayPal","Square"]},{"id":"q-1701","question":"Ingest 3 TB/day of regional event data from Kafka (EU/US/APAC) with user_id, session_id, event_type, timestamp, and attributes. Design a privacy-first analytics pipeline: per-region salt pseudonymization of user_id before cross-region joins, idempotent upserts into Apache Iceberg, event-time processing with 15-minute tumbling windows and 1-hour late data, and immutable audits and data contracts. Compare Spark Structured Streaming vs Flink for the streaming layer, and specify GDPR masking after aggregation?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Google","Lyft"]},{"id":"q-1928","question":"Design a cost-aware real-time ingestion pipeline that processes 50 TB/day of clickstream data from multiple sources (Kafka topics and batch feeds) into a lakehouse. Ensure idempotent upserts into an Iceberg/Delta table, handle late-arriving events, and support schema evolution with partition pruning by date and region. Compare Spark Structured Streaming vs Flink for the path and outline data quality checks, testing, and monitoring strategies?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["NVIDIA","Robinhood","Two Sigma"]},{"id":"q-1965","question":"Design a GDPR/CCPA-compliant, multi-source data pipeline for an e-commerce analytics platform. Ingest CDC from OLTP databases plus batch feeds, preserve raw data immutably, and enable per-tenant data isolation. Implement lineage via a metadata catalog, apply privacy techniques (PII redaction and differential privacy for aggregates), and support schema evolution. Compare Flink vs Spark for streaming, outline tests, monitoring, and cost implications across regions?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Google","LinkedIn","Robinhood"]},{"id":"q-1971","question":"You run a product analytics pipeline for a global iOS/Android app (telemetry: user_id, device_id, ts, event, properties). Data must be ingested from a MongoDB Atlas source into a lakehouse on S3 using Apache Iceberg. Design an end-to-end pipeline that supports idempotent upserts, late-arriving events, and schema evolution, while enforcing PII masking and GDPR data deletion requests. Compare using Flink vs Spark for streaming, outline testing, monitoring, and data-quality checks?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["Apple","MongoDB"]},{"id":"q-2272","question":"Design a global telemetry ingest pipeline for 8 regional Kafka clusters, totaling 20 TB/day of JSON events. Build an end-to-end flow into a lakehouse using Iceberg, ensuring idempotent upserts, late-arriving events, and schema evolution for nested JSON; enforce per-country data sovereignty and GDPR deletion requests. Compare Spark Structured Streaming vs Flink, and outline testing, monitoring, and data-quality checks?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Google","Tesla"]},{"id":"q-2433","question":"Design a multi-tenant data ingestion pipeline that streams 40 TB/day of event data from dozens of publishers, each with its own schema and retention policy. Explain how you enforce per-tenant data contracts, support dynamic schema evolution, guarantee idempotent upserts into a lakehouse, handle late-arriving events and GDPR delete requests, and implement cross-region replication with cost controls. Compare Spark Structured Streaming vs Flink for this workload and outline testing and monitoring?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Bloomberg","Salesforce"]},{"id":"q-2460","question":"Design an end-to-end data pipeline for a multi-tenant ride-hailing platform where city-level feature stores power online fraud scoring and offline model training. Ingest 1) trip events, 2) driver status, 3) external pricing data; ensure per-city isolation, schema evolution, and GDPR deletion. Compare Spark Structured Streaming vs Flink for streaming, outline tests and monitoring?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["LinkedIn","Lyft","Uber"]},{"id":"q-2564","question":"You're building a cross-region analytics platform ingesting 200M events/day into a lakehouse. Design a data-contract–driven pipeline with a central schema registry enforcing compatibility, add automated data quality, lineage, and privacy masking for PII, and ensure BI dashboards and model training never see raw sensitive fields. Compare Spark vs Flink for streaming governance and outline testing and monitoring?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["Amazon","Bloomberg","Instacart"]},{"id":"q-457","question":"You need to process 10GB of CSV files daily and load them into a PostgreSQL database. The files contain user activity logs with timestamps, user IDs, and event types. How would you design an efficient ETL pipeline using Python?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Anthropic","Discord","MongoDB"]},{"id":"q-488","question":"You're building a real-time analytics pipeline for a food delivery app. How would you design a data pipeline to process 1M events/day with 5-minute latency, considering data quality, schema evolution, and cost optimization?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["Adobe","Google","Instacart"]},{"id":"q-518","question":"You have a 10GB CSV file with user activity logs that needs to be processed daily. The file contains user_id, timestamp, action_type, and metadata. How would you design a data pipeline to efficiently process this file and load it into a data warehouse?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Adobe","Airbnb","Oracle"]},{"id":"q-571","question":"How would you design a data pipeline to process 1M ride events per minute from Uber's real-time streaming system?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Meta","Twitter","Uber"]},{"id":"q-885","question":"You operate a multi-tenant SaaS analytics platform ingesting per-tenant event streams from Kafka into Snowflake. Each tenant has different event schemas that can evolve independently. Design a data pipeline to enforce per-tenant data contracts, support late-arriving events, and minimize schema drift while controlling storage costs. Include schema versioning, validation, and deployment safety steps?","channel":"data-engineering","subChannel":"general","difficulty":"intermediate","tags":["data-engineering"],"companies":["NVIDIA","Snowflake","Stripe"]},{"id":"q-915","question":"You ingest 200k newline-delimited JSON app events daily into S3. Each event has event_id, user_id, timestamp, event_type, and attributes. Design a beginner-friendly pipeline to deduplicate by event_id, hash user_id for privacy, validate required fields, and write Parquet data partitioned by date in a data lake. Address simple schema drift and testing?","channel":"data-engineering","subChannel":"general","difficulty":"beginner","tags":["data-engineering"],"companies":["Discord","MongoDB","Snap"]},{"id":"q-993","question":"Design a global, multi-tenant data ingestion system for a ride-hailing platform with streams for billing, trips, safety, and promotions. Each tenant defines a data contract; schemas evolve independently; late-arriving events up to 15 minutes must be accepted. Describe architecture using Apache Kafka, Schema Registry (Avro/JSON), an Iceberg/Delta lake sink, and a streaming processor (Flink/Spark). Include data model, schema evolution, backfill handling, testing, and observability?","channel":"data-engineering","subChannel":"general","difficulty":"advanced","tags":["data-engineering"],"companies":["Coinbase","DoorDash","Meta"]},{"id":"q-176","question":"How would you design a data pipeline that handles both batch and streaming workloads for real-time analytics?","channel":"data-engineering","subChannel":"streaming","difficulty":"beginner","tags":["streaming","kafka"],"companies":["Amazon","Google","Meta","Netflix","Uber"]},{"id":"q-222","question":"How would you design a Kafka Streams application to handle exactly-once processing with stateful aggregations while maintaining sub-second latency during peak loads of 100K events/sec?","channel":"data-engineering","subChannel":"streaming","difficulty":"advanced","tags":["kafka","flink","kinesis"],"companies":["Amazon","Confluent","Netflix","Stripe","Uber"]},{"id":"q-248","question":"How would you implement exactly-once processing in a data pipeline when both source (Kafka) and sink (database) can fail, ensuring no duplicate data or data loss during network partitions and system crashes?","channel":"data-engineering","subChannel":"streaming","difficulty":"intermediate","tags":["dag","orchestration","scheduling"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Snowflake"]},{"id":"q-1038","question":"Design a stack that supports push(x), pop(), top(), and getMin() in O(1) time per operation. Duplicates allowed. Explain the approach, discuss invariants and space usage, and provide a compact code sketch (Python or Java)?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Apple","Databricks","Tesla"]},{"id":"q-1213","question":"Design a dynamic autocomplete data structure for a code search tool that stores terms with frequencies; implement insertWord(word), eraseWord(word), and querySuggestions(prefix, k) returning up to k completions starting with prefix, ordered by descending frequency then lexicographically. Discuss a Trie-based design with per-node top-k structures and update costs?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Hashicorp","Hugging Face"]},{"id":"q-2010","question":"Design a dynamic session tracker: each session has an id, a score, and a lastActive timestamp. Support insertSession(id, score, ts), updateScore(id, delta), expireSessionsBefore(ts) removing expired sessions, topK(window, k) returning the k highest scores among active sessions (active = ts >= now - window) breaking ties by id, and medianScore(window) returning the median score among active sessions. Target near O(log n) per update and O(k log n) for topK. Explain data structures and tradeoffs?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Anthropic","Tesla"]},{"id":"q-2087","question":"Design a dynamic leaderboard data structure to support a game with many players: addOrUpdatePlayer(playerId, score), removePlayer(playerId), getTopK(k), and getPlayerRank(playerId). Achieve O(log n) updates and O(log n + k) to fetch the top-k. Explain how you handle duplicates (equal scores) and score changes?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Databricks","Lyft","Plaid"]},{"id":"q-2178","question":"Design a dynamic prefix-aware dictionary that stores strings with counts (duplicates allowed). Implement: addWord(s), removeWord(s) removing one occurrence, queryPrefixCount(p) giving how many distinct words start with p, and kthWordWithPrefix(p, k) returning the k-th word in lexicographic order among words beginning with p. Handle up to 2e5 words, optimize memory and updates; propose a data layout (e.g., compressed Trie) and discuss trade-offs and edge cases?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Anthropic","Google","Two Sigma"]},{"id":"q-2324","question":"Design a dynamic bitset over a universe [1, 10^9] that supports set(l, r), reset(l, r), flip(l, r), sum(l, r), and kthOne(l, r, k). Use a run-length encoded interval map to achieve near O(log M) updates, where M is number of stored intervals. Explain data structure, edge cases, and complexity; provide a robust approach for sparse updates in large N?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Citadel","Google","Hugging Face"]},{"id":"q-2356","question":"Design a dynamic string editor data structure that supports insert at position, delete substring, and substring hash queries with O(log n) edits and O(log n) hash lookups. Propose a rope-based solution using an implicit treap, where each node stores size and a double rolling hash (mod1, mod2). Explain split/merge, hash maintenance, collision mitigation, and memory trade-offs with a realistic editor workload?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Adobe","Citadel","Twitter"]},{"id":"q-2503","question":"Design a lightweight 1D histogram data structure for values in [0, 1_000_000] that supports: insertValue(v), eraseValue(v) (one occurrence), rangeCount(l, r) counting how many values lie in [l, r], and kthValueInRange(l, r, k) returning the k-th smallest value within [l, r]. Aim for O(log M) per operation. Compare Fenwick vs segment-tree-of-frequencies and discuss handling of duplicates and memory?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["OpenAI","Robinhood","Twitter"]},{"id":"q-2666","question":"Design a dynamic string-frequency dictionary that stores strings with counts. Implement: addString(s), removeString(s) (one occurrence), and queryTopK(prefix p, k) that returns the k most frequent strings starting with p, ordered by frequency descending and then lexicographically. Up to 2e5 total insertions. Provide a practical approach and discuss time/memory trade-offs; keep it beginner-friendly?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Airbnb","Netflix","Square"]},{"id":"q-677","question":"Design a data structure to maintain dynamic counters for items, supporting add(key, delta), get(key), and getTopK(k) returning the k keys with highest counts, ties broken by key. In a real-time analytics scenario, such as tracking top active users by event count, describe the structure, updates, and complexity trade-offs?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Bloomberg","Discord","Netflix"]},{"id":"q-686","question":"Design a data structure that maintains a multiset of integers with insert(x), erase(x) for one occurrence, findMedian(), and findKthSmallest(k). Achieve O(log n) time per operation on average. Explain data layout, invariants, and handling duplicates and lazy deletions. For example: insert 1,2,3,4,5; erase 3; what is median and findKthSmallest(2)?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Apple","Oracle","Twitter"]},{"id":"q-697","question":"Design a time-weighted event multiset data structure. Supports insertEvent(ts, w), eraseEvent(ts, w) removing one occurrence, rangeSum(a, b) returning the sum of weights for events with timestamps in [a, b], and findKthWeightInRange(a, b, k) returning the k-th smallest weight among events with timestamps in [a, b]. Target O(log^2 n) per operation; handle duplicate timestamps and weights; discuss memory and trade-offs?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["DoorDash","Plaid","Scale Ai"]},{"id":"q-701","question":"Design a dynamic multiset of 2D points (duplicates allowed) with insertPoint(x,y), erasePoint(x,y) removing one occurrence, rangeCount(x1,y1,x2,y2), and rangeKthX(x1,y1,x2,y2,k) returning the k-th smallest x in the rectangle (tie by y). Target O(log^2 n) per op. Propose a segment tree over x; each node stores an ordered multiset of (y, unique_id). Explain duplicates handling, updates, rangeCount, and rangeKthX via binary search over x?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Apple","Discord","Snap"]},{"id":"q-710","question":"Design a data structure that maintains a dynamic multiset of strings with operations insert(word), erase(word) removing a single occurrence, countWithPrefix(prefix) returning how many words start with the prefix, and mostFrequentWithPrefix(prefix) returning the most frequent word among those starting with the prefix (tie-break lexicographically). What data structure would you implement, and what are the time complexities and invariants?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Apple","Scale Ai","Slack"]},{"id":"q-722","question":"Design a data structure for an array of integers that supports point updates update(i, val) and range maximum subarray sum queries maxSubarray(l, r) in O(log n). Explain the segment tree node data (sum, bestPref, bestSuff, bestSub) and the merge logic, including tie-breaking for leftmost subarray and handling entirely negative ranges. Example: start with [1,-2,3,4,-1], update index 2 to 5, query maxSubarray(0,4)?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Apple","Bloomberg"]},{"id":"q-727","question":"Design a dynamic data structure for weighted 3D points (x,y,t) with weight w. Support insertPoint(x,y,t,w) and erasePoint(x,y,t,w) (duplicates allowed), rangeSum3D(x1,x2,y1,y2,t1,t2) for total weight in the 3D box, and findKthLargestInRange(x1,x2,y1,y2,t1,t2,k) for the k-th largest weight inside the box. Target O(log^3 n) per op; O(log W * log^3 n) for kth; discuss coordinate compression, memory trade-offs, and handling duplicates?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Airbnb","Google","Meta"]},{"id":"q-739","question":"Design a dynamic 2D weighted point data structure that supports insertPoint(x,y,w), erasePoint(x,y,w) (duplicates allowed), rangeSum2D(x1,x2,y1,y2) for the total weight in the rectangle, and kthLargestInRectangle(x1,x2,y1,y2,k) for the k-th largest weight inside the rectangle. Aim for average O(log^2 n) per operation; discuss coordinate compression, memory trade-offs, and duplicates handling?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Citadel","Hugging Face","Snap"]},{"id":"q-746","question":"Design a dynamic multiset of integers that supports insert(x), erase(x) (one occurrence), countInRange(l, r) for the number of elements in [l, r], and kthSmallestInRange(l, r, k) returning the k-th smallest value among elements in [l, r]. Aim for expected O(log n) updates and O(log n * log U) queries. Explain data structure, balance, and duplicates handling?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Airbnb","Google","PayPal"]},{"id":"q-749","question":"Design a dynamic weighted string multiset with insertWord(word, w), eraseWord(word, w) (duplicates allowed), sumByPrefix(prefix) returning total weight of words starting with prefix, and kthLargestWeightInPrefix(prefix, k) returning the k-th largest weight among those words. Outline the data structure, invariants, and expected complexities; discuss handling of long words and memory trade-offs?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Google","IBM"]},{"id":"q-761","question":"Design a dynamic word-frequency histogram for a text stream. Implement addWord(word) to increment frequency, eraseWord(word) to decrement (removing word when count hits zero), getFrequency(word), and topKWords(n) returning the n most frequent distinct words (ties broken lexicographically). Target average O(log m) per update and O(k log m) for topK, where m is the number of distinct words. Explain approach and data structures you would use?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["Adobe","Scale Ai"]},{"id":"q-774","question":"Design a data structure to manage a dynamic multiset of weighted intervals on the real line. Each interval is [l, r] with weight w; duplicates allowed. Implement insertInterval(l, r, w), eraseInterval(l, r, w) (one occurrence), rangeSumAt(x) returning the total weight of all intervals covering point x, and kthLargestWeightAt(x, k) returning the k-th largest weight among intervals covering x. Target average O(log n) update and O(log n) query; discuss coordinate compression, memory trade-offs, and handling duplicates?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Hashicorp","Slack"]},{"id":"q-782","question":"Design a fully dynamic data structure to maintain a set of linear cost functions y = m_i x + b_i. Support: addLine(id, m, b), removeLine(id) (one occurrence per id), queryMin(x) returning the minimum cost at x across active lines, and queryKthMin(x, k) returning the k-th smallest cost at x. Domain x in [Xmin, Xmax]. Aim for near O(log X) per operation; discuss how deletions are handled, precision, and memory trade-offs?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Apple","DoorDash","Tesla"]},{"id":"q-790","question":"Design a persistent 2D dynamic weighted point data structure that supports insertPoint(x,y,w) and erasePoint(x,y,w) (duplicates allowed). Extend with rangeSum2D(x1,x2,y1,y2,version) and kthLargestInRectangle(x1,x2,y1,y2,k,version). Each insertion creates a new version; queries run in O(log^2 n) time. Explain coordinate compression, memory management, and how duplicates are handled across versions?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Amazon","Databricks","Google"]},{"id":"q-801","question":"Design a fixed-window streaming data structure: push(x) appends an integer to the window; if the window exceeds size W, remove the oldest value. Implement getKthSmallest(k) to return the k-th smallest value in the current window in O(log W). Propose a concrete structure (e.g., an order-statistics tree with duplicates) and outline push/evict/getKthSmallest, including how duplicates and memory are handled?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Cloudflare","Plaid","Twitter"]},{"id":"q-805","question":"Design a dynamic forest data structure supporting: addNode(id, value), link(childId, parentId) to attach a child under a parent, cut(childId) to detach a subtree, update(id, delta) to adjust a node's value, pathQuery(u, v) for the sum of values along the path from u to v, and subtreeQuery(u) for the sum of values in the subtree rooted at u. Target amortized O(log n) per operation. Which approach would you pick (Link-Cut Tree vs Euler Tour Tree), and how would you handle edge cases like root changes and duplicate node values?","channel":"data-structures","subChannel":"general","difficulty":"advanced","tags":["data-structures"],"companies":["Lyft","Meta","Netflix"]},{"id":"q-815","question":"Design a dynamic data structure for a bipartite graph with left indices [1..N] and right indices [1..M]. Each edge (u,v) has weight w; duplicates allowed. Support: - addEdge(u,v,w), eraseEdge(u,v,w) (one occurrence) - rangeSum(uL,uR,vL,vR) total weight of edges with u in [uL,uR] and v in [vL,vR] - kthLargestEdgeWeight(uL,uR,vL,vR,k) the k-th largest edge weight in that submatrix. Aim for ~O(log N log M) per operation; discuss compression, duplicates, and memory trade-offs?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Apple","NVIDIA","Snowflake"]},{"id":"q-820","question":"Design a dynamic sequence structure 'RopeArray' storing an integer array supporting insertAt(i, val), eraseAt(i), rangeSum(l, r), rangeMax(l, r), and getAt(i). How would you implement it to achieve average O(log n) per operation using a balanced tree with implicit keys and augmented fields (size, sum, max), and how would you test with an example sequence?","channel":"data-structures","subChannel":"general","difficulty":"intermediate","tags":["data-structures"],"companies":["Amazon","Google","Microsoft"]},{"id":"q-832","question":"Design a data structure to maintain a dynamic multiset of words with three operations: insertWord(word), eraseWord(word) (one occurrence), and getAnagrams(word) that returns all current words that are anagrams of the given word (including duplicates). Explain how you would store the words, how to compute the canonical signature, and the expected time complexity for updates and queries. For example, after insertWord('listen') and insertWord('silent'), getAnagrams('tinsel') should return ['listen','silent']?","channel":"data-structures","subChannel":"general","difficulty":"beginner","tags":["data-structures"],"companies":["NVIDIA","Snowflake","Twitter"]},{"id":"q-1160","question":"How would you implement and maintain an indexing strategy for a database that experiences frequent schema changes and evolving query patterns, while ensuring minimal performance degradation during migrations?","channel":"database","subChannel":"database","difficulty":"intermediate","tags":["database-indexing","schema-migration","performance-optimization","index-lifecycle"],"companies":[]},{"id":"q-1161","question":"How would you implement and maintain an indexing strategy for a database that experiences frequent schema changes and evolving query patterns, ensuring optimal performance without requiring constant manual intervention?","channel":"database","subChannel":"database","difficulty":"intermediate","tags":["adaptive-indexing","database-automation","performance-monitoring","schema-evolution"],"companies":[]},{"id":"q-1162","question":"How would you implement and maintain an indexing strategy for a database that experiences frequent schema changes and evolving query patterns, ensuring optimal performance without requiring constant manual intervention?","channel":"database","subChannel":"database","difficulty":"intermediate","tags":["adaptive-indexing","schema-evolution","automated-maintenance","query-optimization"],"companies":[]},{"id":"q-643","question":"How would you design an indexing strategy for a time-series database that handles both recent data queries and long-term historical analysis, considering the trade-offs between write performance and query efficiency?","channel":"database","subChannel":"database","difficulty":"intermediate","tags":["time-series","index-optimization","partitioning","performance-tuning"],"companies":[]},{"id":"q-651","question":"How would you design an indexing strategy for a table with 10 million rows that has frequent read queries with multiple WHERE conditions, occasional bulk inserts, and needs to support both exact match and range queries on different columns?","channel":"database","subChannel":"database","difficulty":"intermediate","tags":["database-indexing","query-optimization","performance-tuning","composite-indexes","bulk-operations"],"companies":[]},{"id":"q-765","question":"How would you design an indexing strategy for a multi-tenant SaaS application where each tenant has isolated data but queries frequently need to aggregate across tenants for reporting, while ensuring query performance doesn't degrade as the number of tenants scales to thousands?","channel":"database","subChannel":"database","difficulty":"intermediate","tags":["multi-tenancy","index design","performance optimization","scalability"],"companies":[]},{"id":"q-630","question":"Explain the difference between a B-tree index and a hash index, and when would you choose one over the other?","channel":"database","subChannel":"index-types","difficulty":"intermediate","tags":["database-indexing","b-tree","hash-index","query-optimization"],"companies":["Google","Amazon","Microsoft","Meta","Netflix"]},{"id":"da-125","question":"Explain database indexing and when should you use it?","channel":"database","subChannel":"indexing","difficulty":"intermediate","tags":["sql","indexing"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"db-1","question":"Explain the differences between Clustered and Non-Clustered Indexes, including their performance implications, storage characteristics, and when to choose each type in database design?","channel":"database","subChannel":"indexing","difficulty":"beginner","tags":["sql","indexing","perf"],"companies":null},{"id":"q-170","question":"When would you choose a composite index over multiple single-column indexes in a relational database?","channel":"database","subChannel":"indexing","difficulty":"intermediate","tags":["index","optimization"],"companies":["Amazon","Goldman Sachs","Google","Meta","Microsoft"]},{"id":"q-288","question":"What is the main difference between B-tree and hash index in terms of range query performance?","channel":"database","subChannel":"indexing","difficulty":"beginner","tags":["btree","hash-index","composite"],"companies":["Amazon","Google","Meta"]},{"id":"q-365","question":"You're designing a real-time analytics system for Discord that processes millions of message events per minute. Your PostgreSQL database is experiencing severe write contention on the message_events table. How would you design a partitioning strategy using declarative partitioning, and what specific index optimizations would you implement to handle both time-series queries and user-based lookups efficiently?","channel":"database","subChannel":"indexing","difficulty":"advanced","tags":["joins","indexes","normalization","postgres"],"companies":["Amazon","Discord","Google","Netflix","Palantir","Stripe","Uber"]},{"id":"q-409","question":"You're designing a database for an e-commerce platform with frequent queries on (user_id, order_date) and (product_id, category). How would you choose between B-tree and hash indexes, and what composite index strategy would optimize both query patterns?","channel":"database","subChannel":"indexing","difficulty":"intermediate","tags":["btree","hash-index","composite"],"companies":["Gitlab","Tcs","Tempus"]},{"id":"q-420","question":"You're designing a user database for a chat application with 10M users. When would you choose a B-tree index over a hash index for the 'email' column, and what are the performance implications for login queries, user search, and profile updates?","channel":"database","subChannel":"indexing","difficulty":"beginner","tags":["btree","hash-index","composite"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-489","question":"You're designing a database for LinkedIn's feed system. Posts can be queried by user_id, created_at, and engagement_score. How would you optimize the indexing strategy for high-throughput reads and writes?","channel":"database","subChannel":"indexing","difficulty":"advanced","tags":["btree","hash-index","composite"],"companies":["Anthropic","LinkedIn","NVIDIA"]},{"id":"q-572","question":"You're designing a database for a high-frequency trading system. When would you choose a B-tree index over a hash index for composite queries on (symbol, timestamp, price)? What are the specific performance implications?","channel":"database","subChannel":"indexing","difficulty":"advanced","tags":["btree","hash-index","composite"],"companies":["IBM","NVIDIA"]},{"id":"q-599","question":"When would you choose a composite index over multiple single-column indexes, and what are the trade-offs?","channel":"database","subChannel":"indexing-strategies","difficulty":"intermediate","tags":["indexing","performance","query-optimization","database-design"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-618","question":"Explain the difference between clustered and non-clustered indexes and when you would choose each type. Provide a specific example scenario.","channel":"database","subChannel":"indexing-strategies","difficulty":"intermediate","tags":["database","indexing","performance","sql"],"companies":["Google","Microsoft","Amazon","Meta","Apple"]},{"id":"da-129","question":"What is the main difference between SQL and NoSQL databases in terms of data structure?","channel":"database","subChannel":"nosql","difficulty":"beginner","tags":["nosql","mongodb"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-242","question":"How do MongoDB's document structure and SQL's table rows differ in handling user data with varying attributes, and what are the performance implications for common user operations?","channel":"database","subChannel":"nosql","difficulty":"beginner","tags":["mongodb","dynamodb","cassandra","redis"],"companies":null},{"id":"q-331","question":"You're designing a multi-region e-commerce platform using DynamoDB. Your product catalog needs to support 10M items with eventual consistency across regions, but you must handle hot partitioning during flash sales. How would you design your partition key strategy and what trade-offs would you make between read performance and write throughput?","channel":"database","subChannel":"nosql","difficulty":"advanced","tags":["mongodb","dynamodb","cassandra","redis"],"companies":["Hashicorp","Meta","Oracle"]},{"id":"q-268","question":"How would you optimize a time-series analytics query that scans 100M+ rows across multiple date partitions in PostgreSQL when the WHERE clause cannot be pruned effectively due to complex temporal conditions?","channel":"database","subChannel":"query-optimization","difficulty":"advanced","tags":["explain","query-plan","partitioning"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-303","question":"How would you optimize a slow PostgreSQL query that joins 5 tables with millions of rows?","channel":"database","subChannel":"query-optimization","difficulty":"advanced","tags":["joins","indexes","normalization","postgres"],"companies":["Amazon","Google","Meta"]},{"id":"q-343","question":"You have a PostgreSQL table with 100M rows partitioned by date. A query filtering on a specific date range is still slow. What would you check in the EXPLAIN plan and how would you optimize it?","channel":"database","subChannel":"query-optimization","difficulty":"intermediate","tags":["explain","query-plan","partitioning"],"companies":["Affirm","Amazon","Google","Jane Street","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-380","question":"You're optimizing a query that's slow due to a large time-series table. The query filters by timestamp range and device_id. How would you analyze the query plan and what partitioning strategy would you recommend?","channel":"database","subChannel":"query-optimization","difficulty":"intermediate","tags":["explain","query-plan","partitioning"],"companies":["Hrt","Stripe","Tesla"]},{"id":"q-436","question":"You have a 100M row orders table with slow queries. The query plan shows sequential scans despite indexes on customer_id and order_date. How would you diagnose and fix this performance issue?","channel":"database","subChannel":"query-optimization","difficulty":"advanced","tags":["explain","query-plan","partitioning"],"companies":["Bloomberg","Salesforce","Slack"]},{"id":"q-546","question":"You're analyzing a slow query on a partitioned table. The EXPLAIN plan shows a full table scan instead of partition pruning. What could cause this and how would you fix it?","channel":"database","subChannel":"query-optimization","difficulty":"intermediate","tags":["explain","query-plan","partitioning"],"companies":["Microsoft","Tesla"]},{"id":"da-156","question":"What are the key differences between DELETE and TRUNCATE commands in SQL, including their impact on identity columns, foreign key constraints, and performance characteristics?","channel":"database","subChannel":"sql","difficulty":"beginner","tags":["sql","indexing"],"companies":["Amazon","Google","Microsoft","Netflix","Oracle","Snowflake"]},{"id":"q-172","question":"Design a database failover strategy for a high-traffic e-commerce platform using primary-replica PostgreSQL. How would you ensure zero-downtime failover while maintaining data consistency during peak traffic of 10,000 requests/second?","channel":"database","subChannel":"sql","difficulty":"intermediate","tags":["chaos","resilience"],"companies":["Amazon","Bloomberg","Microsoft","Netflix","Uber"]},{"id":"q-458","question":"You have a PostgreSQL database with orders (10M rows) and customers (1M rows). A query joining these tables is slow. How would you optimize it?","channel":"database","subChannel":"sql","difficulty":"intermediate","tags":["joins","indexes","normalization","postgres"],"companies":["Apple","Tesla","Uber"]},{"id":"da-128","question":"You have a banking system where users can transfer money between accounts. Design a transaction to handle a transfer of $500 from Account A (balance: $1000) to Account B (balance: $200). What happens if the system crashes after debiting Account A but before crediting Account B? How would you ensure data consistency?","channel":"database","subChannel":"transactions","difficulty":"intermediate","tags":["acid","transactions"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Stripe"]},{"id":"da-134","question":"You have a banking system where Account A transfers $100 to Account B, but during the transaction, Account B gets deleted by another process. The transfer uses READ COMMITTED isolation. What happens to the $100, and how would you prevent data inconsistency?","channel":"database","subChannel":"transactions","difficulty":"advanced","tags":["acid","transactions"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Stripe"]},{"id":"da-170","question":"You're building a banking system where users can transfer money between accounts. How would you design the transaction handling to ensure no money is lost or created during transfers, especially when the system crashes mid-transfer?","channel":"database","subChannel":"transactions","difficulty":"intermediate","tags":["acid","transactions"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Stripe"]},{"id":"da-172","question":"In a distributed database system, how would you implement a two-phase commit protocol to ensure atomicity across multiple nodes, and what are the key failure scenarios and recovery mechanisms you must handle?","channel":"database","subChannel":"transactions","difficulty":"advanced","tags":["acid","transactions"],"companies":null},{"id":"db-2","question":"How do ACID properties ensure data integrity in a banking transaction where $100 is transferred from Account A to Account B?","channel":"database","subChannel":"transactions","difficulty":"intermediate","tags":["acid","transactions","theory"],"companies":["Amazon","Goldman Sachs","Google","PayPal","Stripe"]},{"id":"q-190","question":"What is the difference between READ COMMITTED and REPEATABLE READ isolation levels in database transactions, and how does MVCC implementation affect their behavior?","channel":"database","subChannel":"transactions","difficulty":"beginner","tags":["acid","isolation-levels","mvcc"],"companies":["Amazon","Databricks","Google","Microsoft","Oracle","Snowflake"]},{"id":"q-317","question":"Explain how MVCC (Multi-Version Concurrency Control) works and how it prevents lost updates in a database system?","channel":"database","subChannel":"transactions","difficulty":"intermediate","tags":["acid","isolation-levels","mvcc"],"companies":["Microsoft","Plaid","Warner Bros"]},{"id":"q-353","question":"You're building a collaborative design tool where multiple users can edit the same document simultaneously. How would you use database transactions and isolation levels to prevent conflicts while maintaining good performance?","channel":"database","subChannel":"transactions","difficulty":"beginner","tags":["acid","isolation-levels","mvcc"],"companies":["Adobe","Amazon","Canva","Epic Games","Google","Meta","Microsoft","Netflix"]},{"id":"q-397","question":"In a high-transaction payment system using PostgreSQL, how would you design a transaction isolation strategy to prevent lost updates while maintaining high concurrency for account transfers?","channel":"database","subChannel":"transactions","difficulty":"advanced","tags":["acid","isolation-levels","mvcc"],"companies":["Amazon","Google","Netflix","PayPal","Square","Stripe"]},{"id":"q-428","question":"You're building a booking system for Airbnb where multiple users can reserve the same property simultaneously. How would you design the transaction handling to prevent double bookings while maintaining high availability?","channel":"database","subChannel":"transactions","difficulty":"intermediate","tags":["acid","isolation-levels","mvcc"],"companies":["Airbnb","Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"q-519","question":"You're designing a high-frequency trading system where transactions must see consistent data snapshots. How would you implement MVCC to handle concurrent reads while preventing write skew anomalies, and what isolation level would you choose?","channel":"database","subChannel":"transactions","difficulty":"advanced","tags":["acid","isolation-levels","mvcc"],"companies":["Lyft","Snap","Tesla"]},{"id":"q-1100","question":"In a Databricks data pipeline, ingest JSON events from S3 into Delta Lake with Auto Loader. Each event has a nested user object (id, email) and an optional pages array. Some events miss user.email. How would you design the pipeline to safely flatten nested fields, handle missing values, and upsert the latest user state into a Delta Silver table using an idempotent MERGE?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["MongoDB","Oracle"]},{"id":"q-1112","question":"Scenario: In a Databricks Delta Live Tables pipeline, ingest JSON events from a Kafka source into a Bronze table, where nested fields drift over time (e.g., payload.user.locale is added later, some events miss payload.user). You then transform to a Silver table used by billing and marketing. How would you implement a robust schema-evolution strategy and gating so new fields are captured without breaking existing downstream queries, and how would you test this in a run?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["DoorDash","Microsoft","Snap"]},{"id":"q-1144","question":"In a Databricks streaming pipeline ingesting tenant-scoped events from Kafka into Delta Lake, events may arrive late by up to 10 minutes. Describe a concrete end-to-end approach to upsert the latest state per (tenant_id, user_id) into a Silver table while preserving the full event history. Include: data model, CDC logic, watermarking and late data handling, an idempotent MERGE strategy, schema evolution handling, and governance considerations with Unity Catalog RBAC and data lineage?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Airbnb","Cloudflare","Databricks"]},{"id":"q-1182","question":"In a multi-tenant Databricks lakehouse ingesting per-tenant telemetry from Kafka and S3 into a unified Silver Delta table, describe an end-to-end CDC strategy to implement SCD2-like history per tenant with idempotent MERGE, handle late data with a watermark, and enforce governance through Unity Catalog RBAC and dynamic PII masking. Include data model, CDC logic, testing plan, and how you’d validate lineage?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Discord","Google","Snowflake"]},{"id":"q-1226","question":"In a Databricks Delta Live Tables pipeline that ingests clickstream events from a cloud bucket into a Delta table, data quality checks are defined via expectations. A batch arrives with corrupted event_time values that would fail the run. Outline a concrete approach to quarantine bad data, feed downstream tables only from valid rows, and store invalids for audit, while still handling late data and deduplication?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Hashicorp","Zoom"]},{"id":"q-1264","question":"In a Databricks job ingesting daily Parquet files from S3 into Delta Lake, a new field 'campaign_id' may appear in newer files while older ones do not. Describe a concrete, beginner-friendly approach to ingest without data loss, allowing downstream joins, and handling the schema change gracefully. Include how to enable Delta schema evolution (mergeSchema/autoMerge), define a compatible target schema, and implement a minimal validation to drop rows missing essential fields?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Anthropic","Coinbase","Uber"]},{"id":"q-1320","question":"You're designing a Delta Live Tables workflow ingesting data from Kafka and an S3 landing zone, with a downstream customer dimension in Delta Lake that uses SCD Type 2. How would you implement idempotent MERGE-based upserts, handle schema drift, and preserve late-arriving data while auditing invalid events and ensuring downstream BI reads only current rows?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["NVIDIA","Robinhood","Two Sigma"]},{"id":"q-1375","question":"In a Databricks streaming job, a Kafka topic emits JSON events for many tenants. The payload schema drifts with new fields; you want a stable Silver Delta table with a canonical schema and history. Describe a beginner-friendly approach to map events to the canonical schema, handle new fields without breaking downstream joins, and perform an idempotent MERGE into Silver by (tenant_id, event_id). Include a concrete mapping rule set and a small MERGE example?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["MongoDB","Oracle","Robinhood"]},{"id":"q-1409","question":"Describe an end-to-end approach for a fraud-detection streaming pipeline using two Kafka topics (transactions, account_updates): Bronze ingest, join to a versioned SCD2 customer_dim, compute risk in Silver, upsert via MERGE with a deterministic key, watermark late data, handle schema drift with Delta Lake evolution, and enforce governance with Unity Catalog RBAC and lineage. Include testing with synthetic late-arriving data?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Hugging Face","Scale Ai","Snowflake"]},{"id":"q-1451","question":"In a Databricks notebook, you need to join a 10M-row Delta Lake 'customers' table with a 10k-row 'segments' reference table to produce a daily marketing audience feed. How would you implement an efficient join strategy in Spark/Delta to minimize shuffle and cost, ensure correctness if segments update, and maintain lineage? Include: join type and hints, caching strategy, refresh cadence, and where to store results?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Databricks","Google","Two Sigma"]},{"id":"q-1540","question":"In a Databricks streaming pipeline ingesting order events from Kafka into Delta Lake, implement a scalable SCD Type 2 for a customer_dim table to preserve history while handling late-arriving updates up to 15 minutes. Describe the data model (bronze/silver), CDC logic using MERGE, watermarking, and schema evolution, plus Unity Catalog RBAC and lineage considerations. Include a minimal code sketch of the MERGE closing an old row and inserting a new version?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Cloudflare","Hugging Face"]},{"id":"q-1555","question":"In a Databricks pipeline ingesting 20 TB/day of Parquet logs on S3 into Delta Lake, design a practical optimization plan to improve read latency for near-real-time dashboards using Delta features like OPTIMIZE, ZORDER, Data Skipping, caching, and Photon. Discuss partitioning strategy, trade-offs, and how you'd validate gains with benchmark metrics?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Databricks","IBM","Oracle"]},{"id":"q-1643","question":"In Databricks, ingest streaming data from Kafka into Delta Lake for 10k IoT devices emitting multiple sensor types (temperature, humidity, pressure). Build a Silver table with the latest per-device per-sensor-type state while preserving full history. The source schema will evolve (new sensors added, some removed). Outline a robust end-to-end approach: data model, CDC/Upsert logic, watermarking for late data, schema evolution strategy, idempotent MERGE, and governance with Unity Catalog RBAC and lineage. Include concrete examples of Bronze->Silver handoff and a dynamic per-tenant view?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Lyft","OpenAI"]},{"id":"q-1658","question":"Design a secure external data sharing workflow in a Databricks environment using Unity Catalog and Delta Sharing to expose aggregated metrics derived from production Delta tables to external partners while maintaining tenant isolation and governance. Include data model, masking strategy, per partner quotas, refresh cadence, and how to monitor and revoke access?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Amazon","Oracle","Twitter"]},{"id":"q-1683","question":"Design a multi-tenant, Databricks-based data pipeline ingesting 1 TB/day of JSON events from Kafka into Delta Lake. Tenants share storage but must be completely isolated; dashboards must mask PII fields per-tenant. Propose an end-to-end pattern using Unity Catalog RBAC, dynamic data masking, Delta Live Tables, and Photon-enabled reads. Include data model, masking rules, handling schema evolution, and how you validate governance and lineage?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Lyft","Netflix","Square"]},{"id":"q-1721","question":"Databricks beginner scenario: A streaming pipeline reads 2 TB/month of JSON events from S3 via Autoloader into Delta Lake. A downstream dashboard shows active_users by hour. Late events arrive up to 10 minutes. Design a practical plan to maintain accurate hourly active_user counts with minimal duplication, including: schema/partitioning for streaming, watermark-based late data handling, an idempotent MERGE into a Silver table, and a simple end-to-end test using synthetic late events. Also outline monitoring steps?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Adobe","Instacart","Plaid"]},{"id":"q-1844","question":"In a Databricks streaming pipeline ingesting events from Kafka into Delta Lake for a 50+ TB/day workload, design a CDC-based upsert to a Silver table with per-tenant sharding, late data handling, and schema evolution. Describe the data model, idempotent MERGE keys, watermark latency model, and Unity Catalog RBAC considerations; propose validation metrics and a minimal reproducible code skeleton?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Databricks","Instacart","MongoDB"]},{"id":"q-1853","question":"In a Databricks job ingesting 50 GB/day of Avro logs from S3 into Delta Lake, design a beginner-friendly Delta Live Tables pipeline to produce Bronze (raw) and Silver (flattened) tables. Include how you flatten the nested field user (id and tier) into Silver, enforce a simple data quality gate (NOT NULL user_id, user_tier in {'free','standard','premium'}), and choose a partitioning strategy by file_date. Explain testing and how you'll measure improvements?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Meta","Robinhood","Stripe"]},{"id":"q-1944","question":"Design a Delta Live Tables pipeline that streams per-tenant user activity from Kafka into Bronze, then updates a per-tenant Silver table implementing SCD Type 2 on a tenant-scoped customer_dim, using a deterministic MERGE for upserts. Tenants can emit out-of-order data and schema drift occurs. Propose concrete config for watermarking, per-tenant constraints, and schema evolution, plus RBAC in Unity Catalog. Include synthetic late-data tests and metrics to validate latency and correctness?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Bloomberg","Citadel","Meta"]},{"id":"q-1990","question":"Beginner: Adobe wants a minimal, end-to-end Databricks pipeline to ingest daily JSON event logs from S3 into Delta Lake. Design the workflow using Auto Loader for a Bronze table, then flatten to a date-partitioned Silver table. Include simple schema-evolution handling, a lightweight data-quality check (required fields, duplicates), and a basic unit test that validates the Silver schema and daily row count?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Adobe","Uber"]},{"id":"q-2033","question":"In a Databricks workflow, ingest 50 GB/day of JSON web logs from S3 into a Bronze Delta table. Propose a beginner-friendly pipeline to populate a Silver table that upserts the latest event per session_id, handles 2-minute late data with a watermark, and validates data quality before write. Include partitioning strategy, a MERGE-based CDC, and Unity Catalog RBAC considerations?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Amazon","Bloomberg","Google"]},{"id":"q-2070","question":"Design an end-to-end Databricks pipeline to ingest a daily JSON feed from an on-prem FTP drop into Delta Lake. Use Auto Loader with schema evolution for changing fields (user_id, event_type, event_ts, ip_address, device, etc.). Describe Bronze to Silver upserts via MERGE, data quality tests, IP masking, and Unity Catalog RBAC with lineage. Be concrete about steps and trade-offs?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["MongoDB","Snowflake"]},{"id":"q-2199","question":"How would you implement a compliant, zero-duplication deletion workflow in a Databricks streaming pipeline that ingests user events from Kafka into Delta Lake? A deletions log triggers row-level removals across Bronze/Silver, handling late data with tombstones, ensuring idempotent MERGE, and preserving an auditable deletion history via a separate Delta table. Describe architecture, data flow, and validation?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Adobe","Databricks","Goldman Sachs"]},{"id":"q-2279","question":"In a Databricks job ingesting 10 GB/day of user event JSON from S3 into Bronze Delta Lake, design a beginner-friendly Silver pipeline that upserts latest user state by user_id using MERGE, enforces Delta constraints (NOT NULL user_id, age > 0), and fails the pipeline on any quality violation. How would you implement and test this end-to-end?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Netflix","Salesforce","Twitter"]},{"id":"q-2397","question":"In a Databricks data pipeline, ingest telemetry from two sources: S3 JSONs (Auto Loader) and a Kafka topic, with evolving schema (device_id, ts, metrics...). Build Bronze, Silver (dedupe by event_id using MERGE CDC), Gold. Implement 2-minute late data watermark, PII masking, and Unity Catalog RBAC with lineage across Bronze/Silver/Gold. Provide concrete steps, partitioning, and a lightweight test plan?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Anthropic","Goldman Sachs","MongoDB"]},{"id":"q-2435","question":"Design a Databricks pipeline ingesting telemetry from S3 (Auto Loader) and a Kafka topic with evolving schema. Build Bronze/Silver/Gold with CDC dedupe by event_id; add a drift-detection layer that scores per-field distribution changes and quarantines anomalous records. Mask PII before Silver/Gold, enforce Unity Catalog RBAC with lineage. Provide concrete steps, partitioning, and a lightweight test plan?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Adobe","Microsoft","Tesla"]},{"id":"q-2577","question":"In a Databricks Delta Live Tables (DLT) pipeline, ingest telemetry from two sources: S3 Auto Loader JSON with evolving schema and a Kafka stream. Implement multi-source dedup by a composite key (device_id, event_id), support up to 5-minute late data with a watermark, mask PII fields, and publish lineage via Unity Catalog RBAC across Bronze Silver Gold. Provide concrete steps, test plan, and rollback path?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"advanced","tags":["databricks-data-engineer"],"companies":["Citadel","NVIDIA","OpenAI"]},{"id":"q-2662","question":"In a Databricks Delta Lake pipeline ingesting telemetry from two sources (S3 JSONs via Auto Loader with evolving schema and a streaming Kafka feed), design a GDPR-style purge workflow to delete or mask PII across Bronze, Silver, and Gold while preserving aggregates and lineage. Describe concrete steps using MERGE/DELETE, specify retention, audit logging, and how you test downstream dashboards and time-travel reads. Include RBAC considerations?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["databricks-data-engineer"],"companies":["Databricks","Snap"]},{"id":"q-906","question":"Scenario: A daily Delta Live Tables (DLT) pipeline ingests clickstream JSONs into a Bronze Delta table and then enriches with a users_dim to a Silver table. Build a beginner-friendly pipeline that: deduplicates by event_id, validates user_id via users_dim, filters to business hours 08:00–18:00, enriches with user fields, and writes to Silver partitioned by event_date. Outline minimal steps and rationale?","channel":"databricks-data-engineer","subChannel":"general","difficulty":"beginner","tags":["databricks-data-engineer"],"companies":["Apple","Netflix","Scale Ai"]},{"id":"q-1125","question":"You're building an incremental dbt model analytics.daily_revenue_by_product sourced from staging.sales_raw. Data can arrive late up to 2 days. How would you implement incremental upserts, reprocess late days without disturbing older data, and validate with tests and a snapshot of product price history? Provide a minimal SQL snippet illustrating the approach?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Apple","Hashicorp"]},{"id":"q-1165","question":"You're designing a dbt analytics pipeline for a ride-sharing platform. The raw events are in `staging.events_raw` with columns like `ride_id`, `city`, `ride_type`, `currency`, `amount`, `occurred_at`, and data can arrive up to 48 hours late. Build a beginner-friendly incremental model `analytics.daily_revenue` that computes USD revenue per day by `date`, `city`, and `ride_type`. Use a daily `pricing.exchange_rates` table to convert from local currency to USD. Describe how you'd implement incremental upserts, late-data handling (2-day window), schema-drift guards, and validation with tests and a snapshot. Also outline tests for late refunds and missing rates?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Google","PayPal"]},{"id":"q-1257","question":"Design a drift-aware dbt workflow across dev/staging/prod in Snowflake. How would you detect schema drift between sources and models using snapshots, tests, and sources, enforce a drift threshold, and automatically fail CI when drift exceeds the threshold? Describe the macro, tests, and alert/rollback mechanism, plus how you version thresholds?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["Goldman Sachs","Google","Meta"]},{"id":"q-1361","question":"Design a beginner-friendly incremental dbt model in Snowflake for a SaaS analytics pipeline: raw events live at staging.event_logs with user_id, event_type (signup, login, purchase), country, occurred_at. Build analytics.daily_metrics that counts distinct active users by day and event_type, enriched by dimensions.countries. Explain incremental logic, late data handling (7 days), schema drift guards, and validation with tests and a snapshot. Also outline an Exposure for the dashboard with lineage?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Snowflake","Zoom"]},{"id":"q-1523","question":"Design an incremental dbt model analytics.daily_engagement for a global multi-tenant app. Raw events sit in staging.user_events with tenant_id, user_id, email, event_type, occurred_at, privacy_flag. Build per-tenant daily distinct-user counts by event_type, redacting emails when privacy_flag is true. Describe incremental strategy, late data window, schema-drift guards, tests and snapshots, and how to expose lineage for dashboards?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Apple","IBM","Tesla"]},{"id":"q-1560","question":"In a dbt project, you ingest raw events into `staging.web_events` with columns: `event_id`, `user_id`, `event_type`, `country`, `occurred_at` (UTC). Build a beginner-friendly incremental model **analytics.daily_active_users** that reports the count of distinct `user_id`s per day, by `country` and `event_type`. Data can arrive up to 2 days late. Describe your incremental logic, how you handle late data with a rolling window, how to guard against schema drift, and how you validate with tests. Also draft an Exposure for a dashboard showing daily active users by country and event_type, including lineage?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Bloomberg","DoorDash","Oracle"]},{"id":"q-1590","question":"Design a two-model incremental dbt flow in Snowflake: 1) analytics.first_session derives first_session_at per user and seeds analytics.users(user_id, first_session_at, country_code, cohort). 2) analytics.daily_metrics counts daily active users by day and event_type, enriched with dimensions.countries. Include late-data handling (3 days), schema-drift guards, tests (not_null, unique), a snapshot, and expose lineage to dashboards?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Amazon","Oracle"]},{"id":"q-1604","question":"In a dbt pipeline powering a fintech analytics platform, raw events arrive at staging.fin_events with customer_id, txn_id, amount, currency, status, event_ts. Design an incremental analytics.transactions model that deduplicates by txn_id across sources, handles late events within a 2-day window, validates currency via a macro-based set per source, guards against schema drift with a dynamic mapping table, and snapshots customers on their first txn for dashboard lineage. Include tests and performance considerations?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["IBM","NVIDIA"]},{"id":"q-1714","question":"In a beginner dbt project for a media site, staging.events_logs(user_id, event_type, occurred_at, region_code) feeds analytics.daily_engagement that counts distinct users per date and event_type, enriched by dimensions.regions on region_code. Build the incremental model with late data window (3 days), include schema-drift guards, and tests (not_null on user_id, occurred_at; unique on (user_id, occurred_at, event_type)). Add a data-contract macro/test ensuring staging.events_logs contains required fields and types, and a snapshot on users to capture region changes. Explain approach, tests, and macro design?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Bloomberg","Cloudflare","Google"]},{"id":"q-1741","question":"You're building a multi-tenant dbt analytics pipeline on Snowflake. Raw events live in staging.events with tenant_id, user_id, event_type, occurred_at. You plan per-tenant analytics schemas (analytics.{tenant}). Design an incremental analytics.daily_metrics per tenant that counts distinct active users by day and event_type, with late data up to 2 days, joining dimensions.tenants and dimensions.countries, including schema-drift guards, tests (not_null, unique), and a snapshot for user_first_seen per tenant. Explain approach for cross-tenant lineage and tenant-scoped materializations?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["IBM","Stripe"]},{"id":"q-1757","question":"Design a dbt analytics layer for three domains: core_events (user actions), memberships (plans), and referrals. Data arrives late (up to 2 days). Propose an architecture that ensures: 1) incremental models with controlled late data backfills, 2) deterministic surrogate keys for cross-domain joins, 3) schema-drift guards via custom tests/macros, 4) automated docs with complete lineage, 5) dashboard-friendly exposure with stable lineage during backfills, and 6) performance considerations for Snowflake or BigQuery (partitioning, clustering). Include a concrete incremental SQL snippet?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["Anthropic","MongoDB","Slack"]},{"id":"q-1808","question":"Design an intermediate dbt workflow for a fintech analytics pipeline on Snowflake. Raw events are in staging.transactions (transaction_id, user_id, amount, currency, occurred_at, status) and staging.users (user_id, country_code, account_status). Build an incremental analytics.daily_finance that sums total_amount and transaction_count by day, currency, country_code, and status, with late data support up to 2 days. Add analytics.users_snapshot as a Type 2 surrogate for changes in country_code/account_status. Expose lineage and discuss a data-contract macro to validate source schemas and auto-generate tests?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Cloudflare","LinkedIn"]},{"id":"q-1823","question":"Scenario: In a beginner dbt project for a gig-economy platform, raw events arrive in staging.event_logs with user_id, session_id, event_type (visit, click, conversion), occurred_at, and region_code. Build an incremental model analytics.daily_events that counts events by day and event_type, enriched by dims.regions on region_code. Use a 2-day late data window, guard against schema drift with not_null and unique tests, and create a data-contract macro to verify staging.event_logs contains the required columns and types before run. What approach would you take?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Amazon","Uber"]},{"id":"q-1909","question":"Design a per-tenant incremental dbt flow in Snowflake that computes analytics.{tenant}.daily_event_summary from staging.events (tenant_id, user_id, event_type, occurred_at, country_code, record_hash). Include enrichment from dimensions.countries, and produce a per-tenant daily count of distinct users by event_type. Implement late data handling (2 days), schema-drift guards, tests (not_null, unique), and a snapshot for analytics.{tenant}.users to capture cohort changes. Explain how you ensure cross-tenant lineage and isolation?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["LinkedIn","Netflix","Twitter"]},{"id":"q-1988","question":"Design an incremental, per-tenant analytics.daily_metrics model in Snowflake with dbt. Staging.events has tenant_id, user_id, event_type, occurred_at, platform, revenue. Build analytics.{tenant}.daily_metrics counting distinct users per day by event_type, with late data tolerance of 2 days. Add a per-tenant feature flag in dimensions.tenants (revenue_enabled) and a macro to include revenue only when true. Enforce isolation via per-tenant schemas, add schema-drift guards, tests (not_null, unique), and a snapshot for analytics.{tenant}.users. Explain cross-tenant lineage and dashboard exposure?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Hugging Face","Snowflake"]},{"id":"q-2013","question":"Design a per-tenant incremental analytics model in Snowflake that materializes analytics.{tenant}.daily_event_engagement from staging.events (tenant_id, user_id, event_type, occurred_at). Enrich with dimensions.regions on country_code. Produce daily counts by event_type and region_name. Add a 3-day late data window, schema-drift guards, tests (not_null, unique), and a snapshot analytics.{tenant}.customers for cohort changes. Explain tenant isolation and cross-tenant lineage via macro-generated per-tenant schemas?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Goldman Sachs","LinkedIn"]},{"id":"q-2097","question":"Design a per-tenant weekly retention pipeline in Snowflake using dbt where raw events live in staging.events with tenant_id, user_id, first_seen_at, occurred_at, and an events.users table. Create analytics.tenant_retention (tenant_id, cohort_week, retention_users, total_users) that computes weekly retention by cohort (first_seen_week) with incremental loading, and late data handling up to 4 days. Also implement analytics.global_retention that aggregates per-tenant retention across tenants with tenant_dim country/plan, ensuring cross-tenant lineage and isolation. Include schema-drift guards, tests (not_null, unique), and a snapshot of analytics.tenants to track cohort definitions. Explain how you ensure isolation and lineage?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Lyft","Snowflake"]},{"id":"q-2133","question":"Scenario: A new multi-tenant event feed named staging.stream_events with columns tenant_id, user_id, event_type, occurred_at, country_code, payload (JSON). Task: implement a dbt incremental model analytics.daily_user_events that, for each day, tenant_id, and event_type, returns the count of distinct users. Enrich with dimensions.countries on country_code. Create a macro to parse payload JSON extracting device and app_version; fallback defaults if missing. Implement late data tolerance of 2 days (i.e., if occurred_at within last 2 days, process in daily batch). Add tests: not_null on tenant_id, user_id, occurred_at; unique on (tenant_id, user_id, occurred_at, event_type). Add a snapshot for analytics.users to capture cohort-country changes. Provide strategy for cross-tenant lineage and isolation in a single schema (no per-tenant schemas)?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["MongoDB","NVIDIA","OpenAI"]},{"id":"q-2173","question":"Design a per-tenant incremental dbt model for a SaaS analytics pipeline on Snowflake. Source staging.user_events (tenant_id, user_id, event_type, event_timestamp, revenue, lifecycle_stage). Build analytics.{tenant}.daily_cohort to compute daily cohorts by lifecycle_stage and event_type with a 2-day late-data window, upserting via MERGE. Add schema-drift guards, tests (not_null, unique), and a per-tenant last_seen_users snapshot. How do you enforce cross-tenant isolation and lineage?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"intermediate","tags":["dbt-analytics-engineer"],"companies":["Hashicorp","NVIDIA"]},{"id":"q-2315","question":"Design a beginner-friendly per-tenant daily_session_summary in Snowflake/dbt. Source: staging.sessions(tenant_id, user_id, session_start). Build analytics.{tenant}.daily_session_summary counting distinct users per tenant per day, with enrichment from dimensions.tenants (plan, region). Implement late data tolerance of 1 day, schema-drift guards, tests (not_null, unique). Include a data-contract macro to validate staging.sessions fields/types and a snapshot on analytics.{tenant}.users. Explain how you ensure cross-tenant isolation and lineage in dashboards?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Bloomberg","Microsoft","Salesforce"]},{"id":"q-2403","question":"Beginner dbt task: from staging.events (event_id, user_id, event_type, occurred_at, region_code, delete_flag) and dimensions.regions, implement an incremental model analytics.daily_event_summary that counts distinct users per day, event_type, country_code (via region_code). Exclude deleted events by default; provide a macro to toggle inclusion for audits. Add tests not_null(event_id, occurred_at) and unique(event_id)?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Discord","Netflix"]},{"id":"q-2442","question":"In a dbt project, given staging.events(user_id, event_type, occurred_at, country_code) and staging.users(user_id, created_at, country_code), implement an incremental model analytics.daily_engagement that counts distinct users per day by event_type and country_code, enriched by dimensions.countries. Include 1-day late data tolerance, schema-drift guards, tests (not_null, unique), and a snapshot on analytics.users to track country changes. Describe lineage and isolation notes?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Cloudflare","Uber"]},{"id":"q-2502","question":"Design a contract-validated, multi-tenant revenue analytics flow in dbt on Snowflake. Raw events live in staging.{tenant}.events (tenant_id, user_id, event_type, amount, currency, occurred_at). Build a incremental analytics.{tenant}.daily_revenue that sums revenue per day/tenant/currency, with 2-day late data tolerance. Create contracts.tenants describing required fields and a macro that fails tests when missing fields. Add per-tenant analytics.{tenant}.users snapshot for status changes. Enforce per-tenant schemas, schema-drift guards, and tests (not_null, unique). Explain cross-tenant lineage and performance considerations?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["Meta","Microsoft","Plaid"]},{"id":"q-2685","question":"Design a beginner dbt task to build an incremental per-tenant analytics.daily_events model in Snowflake from staging.events(tenant_id, user_id, event_type, occurred_at). Include enrichment from dimensions.event_types, a 2-day late-data window, per-tenant isolation via schemas, and schema-drift guards. Add tests (not_null, unique) and a snapshot for analytics.{tenant}.users; also implement a simple data-contract macro to validate staging.events columns/types and a test using it?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"beginner","tags":["dbt-analytics-engineer"],"companies":["Citadel","Instacart","Snowflake"]},{"id":"q-848","question":"You're designing a dbt analytics pipeline for an e-commerce platform. The 'staging.events_raw' table ingests page views and purchases and is partitioned by day. Data arrives both on time and as late as 24 hours. Design a practical incremental dbt model 'analytics.daily_sales' that aggregates revenue by day, category, and customer_segment. Explain how you'd implement incremental logic, handle late data, guard against schema drift, and validate with tests and snapshots?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["Amazon","Anthropic","Tesla"]},{"id":"q-972","question":"In a dbt analytics project deployed to Snowflake, three tenants share a common model but data is isolated by schema prefixes (tenant_a_, tenant_b_, tenant_c_). Describe how you would structure tenancy-aware tests and a test runner to prevent cross-tenant data leakage during PR runs. Include how you configure sources, seeds, and per-tenant test filtering, and how you verify isolation in CI without touching production data?","channel":"dbt-analytics-engineer","subChannel":"general","difficulty":"advanced","tags":["dbt-analytics-engineer"],"companies":["DoorDash","Lyft","OpenAI"]},{"id":"q-1146","question":"You're building a data ingestion service that validates JSON records before persisting them. Validation steps include NonEmpty fields, Email format, and PasswordStrength. Validators must be pluggable: new validators can be added at runtime by registering them into a Registry without touching the core pipeline. Design a minimal interface and registry, and show how to compose and execute the pipeline with a sample config. Include how to add a new validator and run the pipeline, returning the first failure?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["IBM","Square","Tesla"]},{"id":"q-1201","question":"Design a runtime-pluggable per-message transformation system for a streaming pipeline. Messages include metadata that determines the transformation strategy (e.g., enrich, normalize, validate). Implement with a design pattern that lets you add new strategies without modifying the pipeline core. Provide minimal interfaces, a registry, and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["MongoDB","Plaid","Salesforce"]},{"id":"q-1236","question":"Design a runtime-pluggable, per-route transformer and throttling policy system for a real-time event router. Each route maps an event type to a transformer and a rate-limit policy; new transformers (enrich, redact, normalize) and new policies (token-bucket, fixed-window, leaky-bucket) must be addable at runtime without touching the router core. Which pattern would you adopt and how would you structure the minimal interfaces and registry?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Oracle","Snowflake"]},{"id":"q-1258","question":"Design a runtime-extensible notification dispatch system that handles multiple channels (email, SMS, push) where new transports and their backoff strategies can be added at runtime via a registry without touching the core dispatcher. Specify the minimal interfaces, how you register a new transport and a new backoff policy, and show a usage example including adding a WhatsApp transport and a geometric backoff?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Coinbase","DoorDash","Plaid"]},{"id":"q-1300","question":"Design a feature-flag evaluation engine for a large SaaS product. Flags support boolean, percentage rollout, user-segment, and A/B bucket strategies. Create a pluggable evaluator where new strategies can be added at runtime via a Registry without touching the core evaluator. Provide interfaces, a thread-safe registry, and a usage example with versioned strategy lookup?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Discord","Google","Snowflake"]},{"id":"q-2360","question":"Design a Runtime-Configurable Cache Eviction Engine: build a cache component that supports multiple eviction policies (LRU, LFU, Time-based); policies can be swapped at runtime without restarting the app. Use a pattern that lets you add new eviction strategies via a registry. Provide minimal interfaces, how to register/deregister policies, and an example usage that demonstrates policy switch mid-flight with ongoing hits?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Google","Oracle"]},{"id":"q-2513","question":"Design a runtime-extensible data-format converter: the tool accepts files in JSON, CSV, and XML and should support new formats at runtime without touching the core converter. Which design pattern fits, and how would you structure minimal interfaces and a registry? Provide a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Amazon","Google","MongoDB"]},{"id":"q-2603","question":"Design a runtime-pluggable routing layer for a microservice gateway that allows adding new load-balancing and circuit-breaker strategies without touching the gateway core. Endpoints declare a strategy name and optional config; a central registry wires strategy instances to endpoints. Include minimal interfaces, a registry, and a usage example. How would you structure tests for correctness and fault tolerance?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Apple","Tesla","Zoom"]},{"id":"q-2630","question":"In a mobile analytics SDK, events must be emitted in multiple serialization formats (JSON, MessagePack, Protobuf). Design a runtime-pluggable serializer system so new formats can be added without touching the producer. Which design pattern fits, and how would you structure a minimal Serializer interface, a Registry mapping names to serializers, and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Netflix","Robinhood","Tesla"]},{"id":"q-681","question":"You're building a multi-tenant API gateway for a Stripe-like payments service, backed by MongoDB storage and a Twitter-like feed. Each tenant has a per-minute rate limit. Describe a concrete solution using the Decorator pattern to enforce quotas, show how you'd implement atomic Redis updates, discuss burst handling and clock drift, and outline a minimal wrapper skeleton in code?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["MongoDB","Stripe","Twitter"]},{"id":"q-688","question":"You're building a feature flag system where flags can be evaluated as a hard boolean, a percentage rollout, or a targeted user segment. Design the architecture using a design pattern that lets you add new evaluation strategies without changing the caller. Which pattern would you choose and how would you implement it in code? Provide a minimal interface and usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Amazon","Google","Hashicorp"]},{"id":"q-695","question":"You're building a real-time data ingestion pipeline that must apply a sequence of transformations to each record. New transforms (normalization, enrichment, validation, anomaly detection) should be added as plugins without touching producer/consumer code. Design a pluggable Transform pipeline with per-tenant routing and hot-reload of configuration. Provide minimal interface and a usage example, including how to configure a few plugins and compose them for a stream?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Amazon","Google","Twitter"]},{"id":"q-706","question":"You're building an extensible data ingestion framework where new data formats (JSON, Parquet, ORC) must be supported without touching the core ingestion logic. Design the architecture using a pattern that decouples format parsing from the caller and lets you add new format handlers without changing the caller. Which pattern would you choose and how would you implement it in code? Provide a minimal interface and usage example?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Amazon","Google","MongoDB"]},{"id":"q-714","question":"You're building a small HTTP client wrapper that fetches a user profile, but the server occasionally fails. The caller selects a retry policy by name (linear, exponential, jitter) and fetchUser should retry using that policy without changing fetchUser's code. Design the architecture using a design pattern that lets you add new retry strategies without modifying the caller. Provide a minimal interface and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Google","Lyft","Salesforce"]},{"id":"q-717","question":"You're building a real-time event processing pipeline that validates, enriches, and filters events before persisting. New validators, enrichers, and filters must be added at runtime without altering the core processor. Which design pattern enables this extensibility and how would you implement it? Provide minimal interfaces and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Snowflake","Stripe"]},{"id":"q-729","question":"You're building an image-processing pipeline that applies a sequence of filters (blur, brighten, sharpen) to images. The pipeline must run on both CPU and GPU backends, and new backends must be pluggable without touching filter implementations or orchestration code. Design an architecture that decouples filters from backends using a design pattern, enabling adding a backend such as Vulkan without modifying core code. Provide a minimal interface and usage example?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Adobe","NVIDIA"]},{"id":"q-737","question":"In a messaging pipeline used by Zoom and Hugging Face, the system tokenizes, normalizes, and stores messages. You want to support swapping in different normalization strategies (lowercasing, diacritic removal, profanity filtering) without changing the pipeline code. Design the architecture using a design pattern that lets you add new normalization strategies without modifying the pipeline. Provide a minimal interface and a usage example. How would you implement this pattern to make additions painless?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Hugging Face","Zoom"]},{"id":"q-741","question":"In a telemetry alerting system for autonomous fleets, each customer needs a custom alert-threshold strategy for metrics like speed or battery: static value, percentile-based, or rolling window. The aggregator should surface alerts without depending on a concrete strategy. Design the architecture using a design pattern that lets you add new threshold strategies without modifying the aggregator. Provide a minimal interface and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Tesla","Uber"]},{"id":"q-750","question":"You’re building a data export utility that must support multiple formats. New formats can come from external libraries with different APIs. Design an architecture using a pattern that lets you add new formats without changing the core exporter. Which pattern would you use and how would you implement it? Provide a minimal interface and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Anthropic","Cloudflare","Tesla"]},{"id":"q-763","question":"You're designing a data ingestion pipeline where raw inputs pass through optional enhancers (encryption, compression, watermarking) implemented as decorators around a base DataSource. You must add new decorators without touching the core pipeline or existing decorators. Which pattern would you use and how would you implement minimal interfaces to compose them? Provide a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["Instacart","Microsoft","Oracle"]},{"id":"q-769","question":"In a log analytics pipeline, you must support multiple formats (JSON, CSV, Protobuf) and multiple sinks (Elasticsearch, BigQuery, S3). Design an architecture using a design pattern that allows adding new formats or sinks without touching the producer. Provide minimal interfaces and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Lyft","MongoDB","Oracle"]},{"id":"q-781","question":"You're building a CLI tool that supports commands and subcommands, forming a tree (e.g., 'git remote add'). Design a Command interface that treats leaves (actual actions) and composites (groups of commands) uniformly. Implement LeafCommand and CommandGroup using the Composite pattern so a single call can execute a command or print help for a whole subtree without changing client code. Provide a minimal interface and usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Meta","Robinhood"]},{"id":"q-785","question":"You're building a pluggable HTTP request/response transformer pipeline inside a reverse proxy. Each Transformer can add, redact, or modify headers/body. The gateway must load new Transformers at runtime by name without redeploying. Design a minimal interface and registry-driven architecture that supports adding new transformer types without touching the gateway core. Provide a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Airbnb","Cloudflare","Meta"]},{"id":"q-793","question":"You're designing a streaming data processing framework where each record passes through a configurable pipeline of transformation steps. New transformations must be added at runtime without touching the orchestrator, and jobs select steps by name. Which design pattern and minimal interfaces would you use to register, compose, and execute transformations, ensuring type-safety and low churn when adding new steps? Provide a concise usage example?","channel":"design-patterns","subChannel":"general","difficulty":"advanced","tags":["design-patterns"],"companies":["Citadel","LinkedIn","Snowflake"]},{"id":"q-802","question":"Design a unit-test framework runner that supports multiple assertion styles (classic, fluent, should). The goal is to add a new assertion style (e.g., expect) at runtime without modifying the runner core. Which design pattern would be chosen and how would the minimal interfaces and a registry be structured? Provide a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Apple","Salesforce","Scale Ai"]},{"id":"q-809","question":"Design a health-check framework for a fleet-management service. The system aggregates health from multiple subsystems (database, message broker, geolocation API). New checks (checkDiskSpace, checkApiLatency) must be added at runtime without touching the aggregator. Which pattern supports this, and how would you implement minimal interfaces and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Lyft","Salesforce"]},{"id":"q-819","question":"You are designing a log export module for a distributed service. It must support exporting archives to multiple backends (S3, GCS, and an on-prem object store). New backends should be addable at runtime without touching the exporter or consumer code. Design the architecture using a design pattern that lets you plug in new backends via a registry. Provide a minimal interface and usage example?","channel":"design-patterns","subChannel":"general","difficulty":"intermediate","tags":["design-patterns"],"companies":["DoorDash","Netflix","Uber"]},{"id":"q-824","question":"Design a pluggable text-formatting pipeline for a CLI tool. The pipeline should allow new transforms to be added at runtime via a registry, without touching the core pipeline. Use a suitable pattern to compose these transforms in order; provide a minimal interface and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["Bloomberg","Hashicorp","MongoDB"]},{"id":"q-833","question":"You're building a streaming analytics dashboard where widgets render different metrics. New chart renderers can be added at runtime by third-party teams without modifying core widgets. Design the architecture using a design pattern that supports pluggable renderers via a registry. Provide a minimal interface and a usage example?","channel":"design-patterns","subChannel":"general","difficulty":"beginner","tags":["design-patterns"],"companies":["NVIDIA","Netflix","Tesla"]},{"id":"gh-16","question":"What is Infrastructure as Code and why has it become essential for modern DevOps practices?","channel":"devops","subChannel":"automation","difficulty":"beginner","tags":["iac","terraform","ansible"],"companies":["Amazon","Google","Hashicorp","Microsoft","Netflix","Salesforce"]},{"id":"gh-18","question":"What is Ansible and how does it work for infrastructure automation?","channel":"devops","subChannel":"automation","difficulty":"beginner","tags":["iac","terraform","ansible"],"companies":["Amazon Web Services","Google Cloud","Microsoft","Red Hat","Southwest Airlines"]},{"id":"gh-29","question":"What is Configuration Management?","channel":"devops","subChannel":"automation","difficulty":"beginner","tags":["config-mgmt","ansible","chef"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Netflix"]},{"id":"gh-30","question":"What is Puppet and how does it manage infrastructure configuration?","channel":"devops","subChannel":"automation","difficulty":"beginner","tags":["config-mgmt","ansible","chef"],"companies":["Bank Of America","Cisco","Google","Microsoft","Staples"]},{"id":"gh-31","question":"What is Scalability in DevOps?","channel":"devops","subChannel":"automation","difficulty":"advanced","tags":["scale","ha"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-36","question":"How do different backup strategies balance storage efficiency, backup speed, and recovery time?","channel":"devops","subChannel":"automation","difficulty":"intermediate","tags":["backup","dr"],"companies":["Amazon","Google","LinkedIn","Microsoft","Uber"]},{"id":"gh-92","question":"How does a Service Catalog enable self-service infrastructure provisioning in an Internal Developer Platform?","channel":"devops","subChannel":"automation","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Spotify"]},{"id":"q-243","question":"How would you design a zero-downtime deployment strategy using Ansible that includes blue-green infrastructure setup, traffic management, and automated rollback capabilities?","channel":"devops","subChannel":"automation","difficulty":"intermediate","tags":["ansible","puppet","chef"],"companies":["Adobe","Amazon","Google","Microsoft","Netflix","Stripe"]},{"id":"q-269","question":"Compare Ansible, Puppet, and Chef configuration management tools, focusing on their architecture, state management approaches, and ideal use cases for enterprise environments?","channel":"devops","subChannel":"automation","difficulty":"beginner","tags":["ansible","puppet","chef"],"companies":["Adobe","Amazon","Google","IBM","Microsoft","Netflix"]},{"id":"q-304","question":"How would you design a multi-environment configuration management strategy using Ansible that supports development, staging, and production environments with role-based access control?","channel":"devops","subChannel":"automation","difficulty":"advanced","tags":["ansible","puppet","chef"],"companies":["Amazon","Google","Meta"]},{"id":"q-381","question":"You have 10 web servers that all need Nginx installed and configured identically. How would you use Ansible to ensure this configuration is consistent across all servers?","channel":"devops","subChannel":"automation","difficulty":"beginner","tags":["ansible","puppet","chef"],"companies":["Deepmind","Google","MongoDB"]},{"id":"q-421","question":"You're managing infrastructure at scale with Ansible, Puppet, and Chef. How would you design a configuration management strategy that handles secret rotation across 1000+ servers while ensuring zero-downtime deployments?","channel":"devops","subChannel":"automation","difficulty":"intermediate","tags":["ansible","puppet","chef"],"companies":["Discord","Meta","Scale Ai"]},{"id":"q-437","question":"You're migrating from Puppet to Ansible for configuration management. How would you handle idempotency differences and what strategy would you use to ensure zero-downtime during the transition?","channel":"devops","subChannel":"automation","difficulty":"intermediate","tags":["ansible","puppet","chef"],"companies":["Adobe","Amazon","Cloudflare","Hashicorp","IBM","Microsoft","Netflix","Salesforce"]},{"id":"q-459","question":"You're managing infrastructure at scale with Ansible. How would you design a strategy to handle configuration drift across 1000+ servers while ensuring minimal downtime during updates?","channel":"devops","subChannel":"automation","difficulty":"intermediate","tags":["ansible","puppet","chef"],"companies":["Cloudflare","Discord","Tesla"]},{"id":"q-490","question":"You're migrating a 500-server fleet from Puppet to Ansible with zero downtime. How would you design the migration strategy to ensure configuration consistency and rollback capabilities?","channel":"devops","subChannel":"automation","difficulty":"advanced","tags":["ansible","puppet","chef"],"companies":["Adobe","Microsoft","PayPal"]},{"id":"q-573","question":"How would you design a GitOps workflow using Terraform and ArgoCD to manage infrastructure across multiple cloud providers while ensuring zero-downtime deployments?","channel":"devops","subChannel":"automation","difficulty":"intermediate","tags":["ansible","puppet","chef"],"companies":["Cloudflare","OpenAI"]},{"id":"do-2","question":"Compare Blue/Green vs Canary deployment strategies, including traffic routing, monitoring, rollback complexity, and cost implications for a microservices architecture?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["deployment","strategy","cicd","jenkins"],"companies":null},{"id":"gh-1","question":"What are the core principles and practices of DevOps, and how does it bridge the gap between development and operations teams?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["basics"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-10","question":"What is a CI/CD pipeline and how does it automate software delivery?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["cicd","automation"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-102","question":"What is GitHub Actions and how does it work?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Digital Ocean","Goldman Sachs","Google","Microsoft"]},{"id":"gh-104","question":"What is Canary Analysis and how does it work in production deployments?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-11","question":"What is Jenkins and how does it facilitate continuous integration and continuous delivery (CI/CD) in modern software development workflows?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["cicd","automation"],"companies":["Amazon","Deutsche Bank","Goldman Sachs","Microsoft","Netflix"]},{"id":"gh-2","question":"How would you design a DevOps pipeline that reduces deployment time by 60% while improving reliability and security?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["basics"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-3","question":"What is Continuous Integration and how does it improve software development quality?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["basics"],"companies":["Amazon","Google","Microsoft","Netflix","Stripe"]},{"id":"gh-64","question":"What are the four key DORA metrics for measuring DevOps performance and how are they calculated?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["metrics","kpi"],"companies":["Amazon","Google","Hashicorp","Microsoft","Netflix","Salesforce"]},{"id":"gh-67","question":"How does Database DevOps integrate database schema changes into CI/CD pipelines while ensuring data integrity and minimizing downtime?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["db","devops"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Snowflake"]},{"id":"gh-68","question":"How would you implement comprehensive security practices in a DevOps pipeline including SAST/DAST, container security, and secrets management?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["security","network"],"companies":["Amazon","Cloudflare","Google","Microsoft","Netflix","Stripe"]},{"id":"gh-74","question":"How does DevOps culture transform traditional siloed development and operations into collaborative workflows?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["culture","soft-skills"],"companies":["Amazon","Google","LinkedIn","Microsoft","Netflix"]},{"id":"gh-75","question":"What DevOps practices are essential for implementing continuous delivery and fostering team collaboration?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["culture","soft-skills"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-90","question":"What is Blue/Green Deployment?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-177","question":"Explain the key differences between model serving and model deployment in ML systems, including specific technologies, scaling considerations, and real-world implementation patterns?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["mlops","deployment"],"companies":["Amazon","Databricks","Google","Meta","Microsoft","Netflix"]},{"id":"q-194","question":"How would you design a Terragrunt + Atlantis workflow that prevents state lock contention across 50+ microservice environments while maintaining DRY principles?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["dry","terragrunt","atlantis"],"companies":["Airbnb","Coinbase","Databricks","Stripe","Uber"]},{"id":"q-298","question":"Design a large-scale enterprise CI/CD system for an AWS-based application?","channel":"devops","subChannel":"cicd","difficulty":"advanced","tags":["ci-cd","aws","enterprise","containers","automation"],"companies":["Amazon","Google","Microsoft"]},{"id":"q-318","question":"How would you design a GitHub Actions workflow that runs tests in parallel across multiple matrix configurations while ensuring proper artifact management and failure handling?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["github-actions","jenkins","gitlab-ci"],"companies":["DoorDash","LinkedIn","Robinhood"]},{"id":"q-332","question":"You have a GitHub Actions workflow that's failing intermittently due to race conditions when multiple PRs trigger the same deployment pipeline. How would you design a solution to prevent concurrent deployments while maintaining fast feedback for developers?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["github-actions","jenkins","gitlab-ci"],"companies":["Amazon","Hulu","Jane Street"]},{"id":"q-398","question":"You have a GitHub Actions workflow that's failing intermittently due to rate limiting on a third-party API. How would you design a robust retry mechanism with exponential backoff while ensuring the workflow completes within the 6-hour timeout limit?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["github-actions","jenkins","gitlab-ci"],"companies":["Deepmind","Elastic","Webflow"]},{"id":"q-410","question":"You're setting up a CI/CD pipeline for a microservice that needs to run security scans, build a Docker image, and deploy to staging. How would you configure GitHub Actions to fail fast if security vulnerabilities are found, while still allowing the build to proceed for testing?","channel":"devops","subChannel":"cicd","difficulty":"beginner","tags":["github-actions","jenkins","gitlab-ci"],"companies":["Meta","Okta","PayPal"]},{"id":"q-444","question":"You have a GitHub Actions workflow that's failing intermittently due to rate limiting. How would you design a robust CI/CD pipeline that handles API rate limits, implements proper retry logic, and ensures consistent deployments across multiple environments?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["github-actions","jenkins","gitlab-ci"],"companies":["OpenAI","Tesla","Uber"]},{"id":"q-520","question":"You have a GitHub Actions workflow that's failing intermittently due to rate limiting when pulling Docker images. How would you design a robust solution that ensures consistent builds while minimizing costs?","channel":"devops","subChannel":"cicd","difficulty":"intermediate","tags":["github-actions","jenkins","gitlab-ci"],"companies":["Apple","Square"]},{"id":"q-1159","question":"Design a CI/CD pipeline for a containerized application that needs to support multiple environment-specific configurations (dev, staging, prod) while maintaining security best practices. How would you structure the pipeline to handle secrets management, image scanning, and environment-specific deployments without duplicating pipeline code?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["ci-cd","container-security","secrets-management","multi-environment","kubernetes"],"companies":[]},{"id":"q-1339","question":"How would you implement a custom Kubernetes scheduler to handle specialized workloads like GPU-intensive ML jobs, and what components would need to be modified compared to the default scheduler?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["kubernetes","scheduler","custom-scheduler","gpu","ml-workloads"],"companies":[]},{"id":"q-1341","question":"How does Kubernetes handle pod preemption and priority classes when scheduling, and what happens when a high-priority pod needs to be placed but all nodes are at capacity?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["kubernetes","pod-scheduling","preemption","priority-classes","resource-management"],"companies":[]},{"id":"q-2616","question":"How does Kubernetes handle pod scheduling during cluster autoscaling events, and what scheduling considerations come into play when new nodes are added or removed from the cluster?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["kubernetes","autoscaling","pod-scheduling","cluster-autoscaler","devops"],"companies":[]},{"id":"q-640","question":"Design a multi-region CI/CD pipeline for a global SaaS application that must achieve 99.99% uptime with zero-downtime deployments. The pipeline should handle blue-green deployments across 5 regions, implement circuit breakers for regional failures, and maintain data consistency. How would you architect this pipeline and what specific tools and strategies would you use?","channel":"devops","subChannel":"devops","difficulty":"advanced","tags":["multi-region","blue-green","gitops","circuit-breaker","zero-downtime"],"companies":[]},{"id":"q-641","question":"Design a CI/CD pipeline for a monolithic application that needs to be gradually migrated to microservices. How would you structure the pipeline to support both the monolith and new microservices during the transition period, ensuring minimal downtime and feature parity?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["ci-cd","microservices","migration","pipeline-design","monolith"],"companies":[]},{"id":"q-644","question":"How would you design a CI/CD pipeline that implements feature flagging and progressive delivery to enable zero-downtime deployments for a high-traffic e-commerce platform?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["feature-flags","progressive-delivery","zero-downtime","canary-deployment","monitoring"],"companies":[]},{"id":"q-648","question":"How would you implement a custom Kubernetes scheduler to handle specialized workloads like GPU-intensive ML jobs, and what components would need to be modified compared to the default scheduler?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["kubernetes","scheduler","custom-scheduler","gpu","ml-workloads","devops"],"companies":[]},{"id":"q-652","question":"How would you design a CI/CD pipeline that implements progressive delivery with feature flags, ensuring zero-downtime deployments while maintaining data consistency across multiple database services?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["feature-flags","progressive-delivery","database-migration","zero-downtime","blue-green-deployment"],"companies":[]},{"id":"q-654","question":"How would you implement a custom Kubernetes scheduler to handle specific business requirements like cost optimization or geographic placement, and what are the key components you'd need to modify?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["kubernetes","scheduler","custom-scheduler","cost-optimization","devops"],"companies":[]},{"id":"q-655","question":"How would you design a CI/CD pipeline that implements infrastructure-as-code with blue-green deployments while ensuring zero-downtime database schema migrations?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["infrastructure-as-code","blue-green-deployment","database-migration","zero-downtime"],"companies":[]},{"id":"q-656","question":"How would you implement a custom Kubernetes scheduler to prioritize pods based on business criticality levels, and what components would you need to modify or extend?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["kubernetes","custom-scheduler","pod-priority","scheduling-framework","devops"],"companies":[]},{"id":"q-657","question":"How would you design a CI/CD pipeline that implements infrastructure-as-code with immutable infrastructure patterns, ensuring zero-downtime deployments while maintaining compliance and audit trails for a regulated industry?","channel":"devops","subChannel":"devops","difficulty":"intermediate","tags":["infrastructure-as-code","immutable-infrastructure","compliance","zero-downtime","audit-trails"],"companies":[]},{"id":"gh-37","question":"What is Cloud Native Architecture?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["cloud-native","microservices"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-4","question":"What is Docker and how does containerization differ from traditional virtualization in terms of architecture and resource efficiency?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["docker","containers"],"companies":["Amazon","Google","Microsoft","Netflix","PayPal","Uber"]},{"id":"gh-5","question":"Explain the Docker image and container lifecycle, including image layers, copy-on-write, container states, and resource isolation mechanisms?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["docker","containers"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"gh-6","question":"What is a Dockerfile and how does it enable containerized application deployment?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["docker","containers"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Netflix"]},{"id":"q-171","question":"You have a Docker container that keeps crashing and restarting in production. How would you systematically debug this issue without modifying the container image, and what specific Docker commands and monitoring techniques would you use?","channel":"devops","subChannel":"docker","difficulty":"intermediate","tags":["docker","containers"],"companies":["Amazon","Databricks","Google","Microsoft","Netflix","Snowflake"]},{"id":"q-191","question":"What is the purpose of a multi-stage Docker build and how does it reduce final image size?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["dockerfile","compose","multi-stage"],"companies":["Amazon","Capital One","Google","Microsoft","Uber"]},{"id":"q-289","question":"How do you implement multi-stage builds in Docker to optimize image size and security while maintaining build cache efficiency?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["dockerfile","compose","multi-stage"],"companies":["Amazon","Google","Netflix","Spotify","Uber"]},{"id":"q-344","question":"You're deploying a Node.js microservice to production and notice the Docker image is 850MB. How would you optimize it using multi-stage builds, and what are the key trade-offs between image size and build time?","channel":"devops","subChannel":"docker","difficulty":"intermediate","tags":["dockerfile","compose","multi-stage"],"companies":["Aurora","Shopify","Snowflake"]},{"id":"q-354","question":"You need to deploy a Node.js microservice to SAP's production environment. The current Dockerfile is 1.2GB and includes build tools. How would you optimize it using multi-stage builds to reduce the image size under 200MB?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["dockerfile","compose","multi-stage"],"companies":["Apple","Elastic","Sap"]},{"id":"q-670","question":"Given a monorepo with two services: a Node.js API and a Python worker, design multi-stage Dockerfiles to produce minimal production images, using BuildKit secrets for API keys at build time without bake-in. Write a docker-compose.yml to build and run both services on a shared network, mount a logs volume, and run as a non-root user. How would you implement end-to-end?","channel":"devops","subChannel":"docker","difficulty":"intermediate","tags":["dockerfile","compose","multi-stage"],"companies":["Anthropic","Google","LinkedIn"]},{"id":"q-671","question":"Given a minimal FastAPI app (main.py) with requirements.txt, write a 2-stage Dockerfile to build and run it in a slim final image. Ensure the app runs as a non-root user, and the runtime image only contains Python and the app. Create a docker-compose.yml that starts the web service and a Redis cache, exposes port 8000, and reads config from .env. How would you verify the image size and run locally?","channel":"devops","subChannel":"docker","difficulty":"beginner","tags":["dockerfile","compose","multi-stage"],"companies":["Apple","Meta","Salesforce"]},{"id":"gh-27","question":"Design a Git-based collaboration system for a 50-person distributed team. How would you implement branching strategies, conflict resolution, and CI/CD integration to ensure 99.9% uptime while handling 1000+ daily commits?","channel":"devops","subChannel":"gitops","difficulty":"advanced","tags":["git","vcs"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"gh-28","question":"What is Git Branching Strategy?","channel":"devops","subChannel":"gitops","difficulty":"beginner","tags":["git","vcs"],"companies":["Amazon","Goldman Sachs","Google","Meta","Microsoft"]},{"id":"gh-53","question":"What is GitOps and how does it work in practice?","channel":"devops","subChannel":"gitops","difficulty":"beginner","tags":["automation","tools"],"companies":["Amazon Web Services","Gitlab","Google","Microsoft","Netflix"]},{"id":"gh-54","question":"What is ArgoCD and how does it implement GitOps for Kubernetes deployments?","channel":"devops","subChannel":"gitops","difficulty":"beginner","tags":["automation","tools"],"companies":["Amazon","Google","Hashicorp","IBM","Microsoft","Netflix"]},{"id":"q-217","question":"How would you design a GitOps multi-cluster deployment strategy using ArgoCD that handles blue-green deployments with zero-downtime rollback across 50+ clusters while maintaining state consistency?","channel":"devops","subChannel":"gitops","difficulty":"advanced","tags":["argocd","flux","declarative"],"companies":["Amazon","Google","Microsoft","Red Hat","Uber"]},{"id":"q-366","question":"How would you design a GitOps workflow using ArgoCD to deploy a microservices application across multiple environments?","channel":"devops","subChannel":"gitops","difficulty":"intermediate","tags":["gitops","argocd","kubernetes","deployment","automation"],"companies":["Amazon","Google","Meta"]},{"id":"q-429","question":"You're setting up GitOps for a microservices deployment. How would you configure ArgoCD to automatically sync changes from your Git repository to Kubernetes, and what's the difference between declarative and imperative approaches in this context?","channel":"devops","subChannel":"gitops","difficulty":"beginner","tags":["argocd","flux","declarative"],"companies":["Amazon","DoorDash","Google","Hashicorp","Lyft","Microsoft","Netflix"]},{"id":"q-547","question":"You're implementing GitOps for a microservices application. How would you configure ArgoCD to automatically sync changes from your Git repository to Kubernetes, and what would you set as the sync policy to ensure safe deployments?","channel":"devops","subChannel":"gitops","difficulty":"beginner","tags":["argocd","flux","declarative"],"companies":["IBM","NVIDIA","Tesla"]},{"id":"q-668","question":"Describe a practical, automated secret rotation flow in a multi-cluster GitOps setup using ArgoCD and Flux. Include how Vault, ExternalSecret/SealedSecret, and per-cluster Secrets interact, what triggers rotation, how drift is prevented, and how rollback/auditing is handled across clusters?","channel":"devops","subChannel":"gitops","difficulty":"advanced","tags":["argocd","flux","declarative"],"companies":["Apple","Plaid","Zoom"]},{"id":"q-669","question":"You manage a single service deployed to Kubernetes with declarative manifests stored in git. Using both Argo CD and Flux in a GitOps pipeline, describe a practical beginner-friendly approach to implement blue-green or canary deployment, including the minimal YAML you'd configure in Argo CD to promote from staging to prod, and how you'd handle secrets?","channel":"devops","subChannel":"gitops","difficulty":"beginner","tags":["argocd","flux","declarative"],"companies":["Amazon","DoorDash","Google"]},{"id":"q-633","question":"Design a CI/CD pipeline for a microservices application with 10 services, where each service has its own repository. The pipeline must support parallel deployments, canary releases, and automatic rollback on failure. How would you structure the pipeline and what tools would you use?","channel":"devops","subChannel":"kubernetes-devops","difficulty":"advanced","tags":["CI/CD","microservices","kubernetes","gitops","canary-deployments"],"companies":["Google","Netflix","Uber","Spotify","Airbnb"]},{"id":"q-600","question":"Design a CI/CD pipeline for a microservices application that includes automated testing, security scanning, and deployment to multiple environments (dev, staging, prod). What are the key components and how would you ensure zero-downtime deployments?","channel":"devops","subChannel":"pipeline-architecture","difficulty":"intermediate","tags":["CI/CD","microservices","docker","kubernetes","security","zero-downtime"],"companies":["Google","Amazon","Microsoft","Netflix","Uber","Spotify"]},{"id":"q-602","question":"Design a CI/CD pipeline for a microservices application with the following requirements: automated testing, containerization, blue-green deployment, and rollback capabilities. What tools and stages would you include?","channel":"devops","subChannel":"pipeline-architecture","difficulty":"intermediate","tags":["CI/CD","microservices","kubernetes","docker","devops"],"companies":["Google","Amazon","Microsoft","Netflix","Uber","Spotify"]},{"id":"q-613","question":"Design a CI/CD pipeline for a microservices application with 10 services. How would you handle deployment strategies, testing, and rollback mechanisms?","channel":"devops","subChannel":"pipeline-architecture","difficulty":"intermediate","tags":["CI/CD","microservices","kubernetes","docker","deployment"],"companies":["Google","Amazon","Microsoft","Netflix","Uber","Spotify"]},{"id":"q-629","question":"Design a CI/CD pipeline for a microservices application that includes automated testing, security scanning, and multi-environment deployments. What components would you include and how would you structure the pipeline stages?","channel":"devops","subChannel":"pipeline-architecture","difficulty":"intermediate","tags":["cicd","microservices","devops","automation","security"],"companies":["Amazon","Google","Microsoft","Netflix","Spotify"]},{"id":"q-594","question":"How does Kubernetes decide which node to schedule a pod on, and what factors can influence this decision?","channel":"devops","subChannel":"pod-scheduling","difficulty":"intermediate","tags":["kubernetes","scheduling","kube-scheduler","node-selection","resource-management"],"companies":["Google","Amazon","Microsoft","Red Hat","VMware","IBM"]},{"id":"q-608","question":"How does Kubernetes handle pod scheduling when a node becomes resource-constrained, and what mechanisms can be used to ensure critical pods remain running?","channel":"devops","subChannel":"pod-scheduling","difficulty":"intermediate","tags":["kubernetes","scheduling","resource-management","priority","preemption"],"companies":["Google","Netflix","Uber","Amazon","Microsoft"]},{"id":"q-638","question":"Explain how Kubernetes scheduler decides which node to place a pod on, and what factors can cause a pod to remain in a Pending state?","channel":"devops","subChannel":"pod-scheduling","difficulty":"intermediate","tags":["kubernetes","scheduling","pod-management","troubleshooting","resource-allocation"],"companies":["Google","Amazon","Microsoft","Red Hat","VMware"]},{"id":"q-635","question":"How does Kubernetes handle pod scheduling when a node becomes unavailable, and what mechanisms ensure high availability?","channel":"devops","subChannel":"pod-scheduling-failover","difficulty":"intermediate","tags":["kubernetes","scheduling","high-availability","pod-management","failover"],"companies":["Google","Amazon","Microsoft","Red Hat","VMware","IBM"]},{"id":"q-620","question":"Explain the difference between node affinity, node selectors, and taints/tolerations in Kubernetes pod scheduling. When would you use each?","channel":"devops","subChannel":"scheduling-mechanisms","difficulty":"intermediate","tags":["kubernetes","pod-scheduling","node-affinity","taints-tolerations","devops"],"companies":["Google","Amazon","Microsoft","Netflix","Uber"]},{"id":"q-1392","question":"In a three-node Docker Swarm spanning two data centers, deploy a stateless API across an overlay network and ensure zero-downtime upgrades via canary, using update_config (start-first, parallelism 1). Outline the exact sequence of commands to init/join the swarm, create the overlay, deploy the service with update settings, and perform a canary upgrade with health checks. Include a minimal docker-compose snippet showing update_config?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Airbnb","Cloudflare","Two Sigma"]},{"id":"q-1412","question":"Scenario: you’re building a beginner-friendly Docker Compose setup for a FastAPI microservice with PostgreSQL on a single host. Create Dockerfiles for the app and an init script, plus a docker-compose.yml with a named volume for Postgres data, healthchecks, and a startup script that waits for PostgreSQL on port 5432 before starting the app. Explain the exact files, commands, and deployment sequence?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Hugging Face","IBM","Stripe"]},{"id":"q-1443","question":"In a 3-node Swarm across DC-A and DC-B, deploy a stateless API via overlay api-net with DC-affinity (2 replicas in DC-A and 1 in DC-B). Outline the exact CLI steps to init/join, create the overlay, and deploy two services with update_config (order: start-first, parallelism: 1). Describe a canary upgrade process that gradually updates replicas across DCs and validates with health checks. Include a minimal docker-compose snippet showing the two services, the overlay network, and update_config?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Amazon","Netflix"]},{"id":"q-1530","question":"In a three-node Docker Swarm spanning two data centers, deploy a stateless API with TLS termination that uses Vault for dynamic TLS certificate rotation. Use Docker secrets to distribute certs, and implement a lightweight sidecar that refreshes certificates without dropping connections. Configure a canary-style rolling upgrade ensuring zero-downtime during cert rotations. Outline the exact steps: swarm init/join, overlay network creation, stack/deploy with secret handling, and the certificate rotation workflow with health checks?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Amazon","Meta"]},{"id":"q-1579","question":"Design a secret-rotation workflow for a Docker Swarm stack that uses Vault to rotate a TLS cert and a database credential with zero downtime. Use Swarm secrets and a rolling update (start-first). Outline exact steps: swarm init/join, overlay network, create secrets, deploy stack, Vault rotate trigger, service update commands. Include a minimal docker-compose snippet showing secret usage and update_config?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Bloomberg","Databricks","Hashicorp"]},{"id":"q-1628","question":"Scenario: In a 3-node Swarm across two DCs, deploy an API service behind Traefik with image signing enforcement (cosign/DOCKER_CONTENT_TRUST) and implement a canary upgrade to v2 with a 10% traffic split via a separate api-v2-canary service. Outline exact commands, Swarm update_config usage, and docker-compose/service definitions to achieve zero-downtime upgrade and safe rollback?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Cloudflare","IBM","MongoDB"]},{"id":"q-1653","question":"Scenario: On a single host, implement a beginner-friendly Docker Compose stack: a Node.js API that talks to Redis and a Fluent Bit logging service that reads logs from the API via a shared volume and forwards them to stdout. Provide a Dockerfile for the API, a docker-compose.yml with api, redis, fluent-bit, a logs volume, and healthchecks; explain the run steps and verification?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Hugging Face","Instacart","Salesforce"]},{"id":"q-1688","question":"Beginner-level: design a local docker-compose stack for a Node.js API that uses Redis as a cache. Provide a Dockerfile for the API, a startup script that waits for Redis to be reachable on 6379 before starting, and a docker-compose.yml with healthchecks for both services on a shared network. Include exact file contents or minimal snippets, the commands to build and run, and how to validate a cache-hit endpoint?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["DoorDash","MongoDB","Scale Ai"]},{"id":"q-1760","question":"In a three-node Docker Swarm spanning two data centers, introduce a new internal auth service that all APIs depend on. Roll it out with zero downtime and a canary, using update_config (start-first, parallelism 1), Docker Secrets, and a routing shim. Provide exact commands to init/join the swarm, create the overlay, deploy the stack, add the secret, and perform the canary upgrade with health checks?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["LinkedIn","Tesla"]},{"id":"q-1786","question":"In a 3-node Docker Swarm spanning two data centers, deploy a GPU-accelerated model-serving API (TorchServe) using the NVIDIA runtime. Expose it behind an internal overlay network and a simple LB. Ensure zero-downtime upgrades with canary traffic shifts and a pre-warm sidecar to warm the new replica without serving traffic. Outline the steps for swarm init/join, overlay creation, service spec with GPU constraints, secret handling, and a separate migration container if needed. Include a minimal docker-compose snippet showing update_config (start-first, parallelism 1)?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Databricks","Google"]},{"id":"q-1832","question":"Design and implement a zero-downtime upgrade for a 4-node Docker Swarm across two data centers hosting a stateful Redis queue and a stateless worker service. The worker consumes messages encoded in a new proto format; the cluster uses TLS mutual authentication with Vault for cert rotation and Docker Secrets for credentials. Outline exact upgrade steps, a 10% canary rollout, health checks, and a rollback plan, and include a minimal docker-compose snippet showing update_config(order: start-first, parallelism: 1)?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Amazon","Two Sigma","Zoom"]},{"id":"q-1967","question":"In a production Swarm across two data centers with four nodes, implement end-to-end image provenance using Cosign. Sign CI artifacts, publish signatures to a registry, and enforce verification before deploys. Outline the exact steps: key management, signing workflow in CI, signature verification at pull-time, updating services with image digests, and a rollback plan if verification fails. Provide concrete Cosign commands and how to incorporate into a CI/CD pipeline?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Amazon","DoorDash","Microsoft"]},{"id":"q-2016","question":"In a three-node Docker Swarm spanning two data centers, deploy a stateless API behind a Traefik ingress with a Redis-backed cache layer and mutual TLS between services. Use Docker secrets/configs for TLS certs and cache creds. Apply zero-downtime upgrades via update_config (start-first, parallelism 1) and implement a canary upgrade path with health checks and automatic rollback. Outline the exact init/join commands, overlay creation, stack file, and upgrade sequence?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Discord","Twitter"]},{"id":"q-2022","question":"Scenario: On a single host, implement a blue-green deployment for a stateless API behind an Nginx reverse proxy using docker-compose. Start with green (v1) receiving all traffic; deploy blue (v2) and switch traffic to blue only after a 30-second health-check window confirms readiness, ensuring zero downtime. Provide: a) docker-compose.yml with two API services (api-green and api-blue) and Nginx, b) nginx.conf with a switchable upstream, c) a Bash script upgrade.sh that promotes blue by reconfiguring upstream and reloading Nginx, d) exact commands to bring up green, deploy blue, run the upgrade, and verify with curl?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Adobe","Hashicorp","Snowflake"]},{"id":"q-2053","question":"In a four-node **Docker Swarm** spanning two data centers, deploy a stateless API with a Redis-backed rate limiter and a shared cache. Implement a canary upgrade of the API using update_config (start-first, parallelism 1), with a front-door that routes the canary subset using label-based routing. Outline exact commands: swarm init/join, overlay creation, stack deploy, and the health-check-driven promotion. Include a minimal docker-compose snippet showing update_config?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Instacart","Meta"]},{"id":"q-2120","question":"In a two-node Swarm, deploy a TLS-secured stateless API behind an Nginx proxy. Use Docker secrets for TLS certs, a Docker config for API settings, and a small sidecar that ships logs without affecting requests. Attach both services to a single overlay network and perform a canary upgrade with a rolling update (start-first, parallelism 1). Provide exact commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Netflix","Salesforce"]},{"id":"q-2140","question":"In a two-datacenter Docker Swarm (2 nodes per DC), deploy a stateless API behind Traefik with TLS termination on an overlay network across DCs. Implement a canary rollout for a feature-flag change using a Swarm Config and route 10% of traffic to canary via Traefik labels. Outline exact swarm init/join commands, overlay creation, stack deploys, config creation, and the canary upgrade with health checks and rollback criteria?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Apple","Meta"]},{"id":"q-2231","question":"In a four-node docker-dca cluster, deploy a TLS-secured stateless API behind an Nginx edge proxy. Use Docker secrets for TLS certs and a Docker config for API endpoints, and add an Envoy sidecar per app to drive traffic-splitting controlled by a central Consul KV flag. Implement a progressive canary: 10% steps every 30s up to 100%, with health checks and automatic rollback on failure. Provide explicit bootstrap commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Discord","Microsoft","Plaid"]},{"id":"q-2254","question":"Create a local Docker Compose setup with two Python services: web (Flask API) and cache (Redis). Duplicate web as web_canary with CANARY=true. Add an Nginx container as a reverse proxy routing /api to web and /canary to web_canary. Use a named volume for Redis data. Add healthchecks for all three containers and provide a docker-compose.yml skeleton plus exact bootstrap commands to seed data and verify endpoints?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Amazon","Robinhood","Snowflake"]},{"id":"q-2295","question":"In a 5-node Docker Swarm spanning two data centers, deploy a TLS-secured stateless API behind an Nginx edge proxy. Use Docker secrets for TLS certs and a Docker config for per-tenant routing templates, consuming routing directives from a central HTTP API. Attach a per-service Envoy sidecar to drive traffic-splitting with a progressive canary (25% steps every 20s). Ship logs via a separate sidecar to a Loki stack without blocking requests. Implement a zero-downtime upgrade using update_config (start-first, parallelism 1). Provide exact commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Slack","Tesla","Uber"]},{"id":"q-2346","question":"In a 3-node docker-dca cluster spanning two data centers, deploy a TLS-secured stateless API behind an Nginx proxy. Use Docker secrets for TLS certs and a Docker config for API endpoints. Add a shadow sidecar that mirrors 5% of production traffic to a canary version. Implement a canary rollout by adjusting weights in 10% increments using update_config, with health checks and automatic rollback on failure. Provide exact commands and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["Anthropic","Google","NVIDIA"]},{"id":"q-2386","question":"In a two-datacenter Docker Swarm with a TLS-enabled REST API behind an Nginx edge proxy, TLS certs are stored as Docker secrets. Design a zero-downtime rotation workflow: publish a new cert secret api_tls_new without restarting Nginx, upgrade the service with the new secret using a canary approach (update_config: order start-first, parallelism 1), and provide the exact shell commands plus a minimal docker-compose.yml skeleton showing secrets, config, and a small log-shipper sidecar that does not delay requests?","channel":"docker-dca","subChannel":"general","difficulty":"intermediate","tags":["docker-dca"],"companies":["DoorDash","Meta","MongoDB"]},{"id":"q-2432","question":"In a 4-node docker-dca Swarm, deploy a TLS-secured event-processing pipeline behind an Nginx edge proxy. Use Docker secrets for TLS certs, a Docker config for routing rules, and implement a 5% canary upgrade for the Transform service controlled by a Consul KV flag. Provide exact commands and a docker-compose.yml skeleton that demonstrates canary traffic, health checks, and zero-downtime upgrades?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Goldman Sachs","Robinhood","Snowflake"]},{"id":"q-2485","question":"In a three-node Docker Swarm spanning two data centers, deploy a TLS-secured stateless event API behind an Nginx edge proxy. Use Docker secrets for TLS certs and a Docker config for API endpoints. Introduce an SPIRE-based mTLS mesh by wiring an SPIRE agent as a sidecar to each app container to issue short-lived mTLS certs and enforce SPIFFE IDs. Implement automatic certificate rotation every 60 minutes and a progressive canary upgrade (start-first, parallelism 1). Provide exact docker-compose.yml skeleton and bootstrap commands?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["Anthropic","Citadel","Google"]},{"id":"q-2558","question":"In a two-node docker-dca Swarm, deploy a TLS-enabled stateless API behind an Nginx proxy. Use Docker secrets for TLS certs, a Docker config for API endpoints, and a lightweight log-shipper sidecar that adds no latency. Attach both services to a single overlay network. Implement a 5% canary rollout with health checks and automatic rollback on failure. Provide exact commands and a docker-compose.yml skeleton, plus how to measure and enforce performance during the rollout?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Apple","DoorDash","Meta"]},{"id":"q-2588","question":"On a two-node Docker Swarm, deploy a TLS-secured stateless API behind an Nginx edge proxy. Use a Docker secret for TLS certs and a Docker config for API endpoints. Ensure the app runs as a non-root user and add a tiny sidecar that tails logs to a shared volume without affecting requests. Attach both services to an overlay network and implement a rolling update with start_first, parallelism 1. Include exact commands to initialize swarm, create secret/config, build/deploy, and a docker-compose.yml skeleton?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Adobe","Salesforce","Zoom"]},{"id":"q-856","question":"You're running a Docker Swarm with services frontend, api, and worker. A feature-flag config is provided via Docker Config mounted at /etc/flags.json in all containers. You must rotate this config weekly with zero downtime. Describe the exact sequence of commands to create a new config version, rotate the services to use it, and implement a graceful reload inside apps so the new flags are picked up without losing requests. Include any Swarm update options you would tune?","channel":"docker-dca","subChannel":"general","difficulty":"beginner","tags":["docker-dca"],"companies":["Hashicorp","Snowflake"]},{"id":"q-862","question":"In a Docker Swarm with a stateful web app that uses Postgres, you must roll out version 3.2 with a DB schema migration and zero downtime. Propose a concrete upgrade plan that uses a start-first, one-task-at-a-time update, a separate migration container, and post-migration validation. Include exact Swarm commands and a minimal docker-compose snippet showing update_config?","channel":"docker-dca","subChannel":"general","difficulty":"advanced","tags":["docker-dca"],"companies":["IBM","PayPal"]},{"id":"q-1262","question":"You're given an array A of length n and an integer k. You must select exactly k non-overlapping, contiguous subarrays (non-empty) to maximize the sum of all selected elements. Return the maximum sum and the k subarrays (start and end indices). Propose a dynamic programming formulation with states and recurrences, include reconstruction, and discuss time/space complexity?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Hugging Face","Snap"]},{"id":"q-1872","question":"You're given an n x m grid of integers grid[i][j]. From (0,0) to (n-1,m-1) you may move only right or down. You may turn at most K times (a turn is a change between directions). Return the maximum sum along a valid path and the path coordinates. Propose a DP with states dp[i][j][t][dir], recurrences, base cases, and reconstruction, and discuss time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["LinkedIn","MongoDB","Slack"]},{"id":"q-1941","question":"You're given an array A of length n and an integer k. You must select at most k elements such that no two selected elements are adjacent. Return the maximum sum you can obtain and the list of selected indices. Propose a DP formulation with states dp[i][j], recurrences, base cases, and reconstruction?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Apple","Instacart","Meta"]},{"id":"q-2170","question":"You're given an array values[0..n-1] of positive integers. Two players take turns removing either the leftmost or rightmost value and add it to their score. Assuming both play optimally, return A's maximum guaranteed total score and the move sequence (L/R) for each turn. Provide a DP formulation with dp[i][j] as the maximum score difference for subarray i..j, the base cases, and how to reconstruct the moves?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Apple","Lyft","Tesla"]},{"id":"q-2405","question":"You're given an array A[0..n-1], and integers k, L, R. You must select exactly k non-overlapping contiguous subarrays such that each subarray's length is between L and R (inclusive). The sum of all elements in the selected subarrays is maximized. Return the maximum sum and the k subarrays (start and end indices). Provide a DP formulation with states, recurrences, base cases, and reconstruction, and discuss time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Lyft","MongoDB"]},{"id":"q-2434","question":"You're given an array A of length n and an integer k. Choose exactly k non-empty, non-overlapping subarrays. The score of a subarray [l..r] is sum(A[l..r]) - min(A[l..r]). Return the maximum total score and the list of subarray endpoints (l, r). Propose a DP formulation: dp[i][t] = max_{p in [t-1..i-1]} dp[p][t-1] + sum(A[p+1..i]) - min(A[p+1..i]). Base: dp[i][1] = sum(A[1..i]) - min(A[1..i]). Include reconstruction, base cases, and time/space complexity?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Google","Lyft","MongoDB"]},{"id":"q-679","question":"In a warehouse grid of size n x m, each cell has a traversal cost. You can move only right or down from (0,0) to (n-1,m-1). You have a one-time token to halve the cost of exactly one visited cell. Design an O(nm) DP to compute the minimum path cost after optimally using the discount, and describe the recurrences and a space-optimized implementation?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Anthropic","Snap","Twitter"]},{"id":"q-691","question":"You're planning a delivery route along a straight street of n blocks. At block i you can advance up to jumps[i] blocks (at least 1). How many distinct routes reach block n-1 from block 0? If unreachable, return 0. Propose a dynamic-programming approach with dp[i] as ways to reach i and outline its time/space complexity, edge cases, and a brief correctness justification. How would you implement it?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["DoorDash","Meta","Square"]},{"id":"q-693","question":"You're building a daily workout planner. Over n days, you can pick Light (L) or Heavy (H) workouts, with a cooldown: after a Heavy, you must skip the next two days (no workouts). Given integers n and r, how many length-n sequences contain exactly r Heavy workouts and satisfy the cooldown rule? Provide a DP formulation with state dp[i][c][t] (days processed, cooldown days left, heavies used) and outline time/space complexity, edge cases, and a brief correctness justification?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Citadel","Google","Microsoft"]},{"id":"q-703","question":"You're navigating a warehouse grid of size n x m. Each cell (i, j) has a risk value r[i][j] ≥ 0, where higher means less safe. You may move only right or down from (0,0) to (n-1,m-1). Devise a dynamic-programming solution to minimize the maximum risk encountered on the path (i.e., minimize max(r[i][j]) along the path). Define a suitable dp[i][j], give the recurrence and base cases, describe reconstruction of the path, and analyze time/space complexity. How would you handle blocked cells by setting r[i][j] = INF?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["NVIDIA","Scale Ai","Stripe"]},{"id":"q-716","question":"You're building a text formatter. Given a list of word lengths L = [l1, l2, ..., ln] and a maximum line width W, wrap the words into lines so that all lines except the last incur a penalty of (W - usedWidth)^2, where usedWidth = sum of word lengths on the line plus spaces between words. The last line has 0 penalty. Propose a dynamic programming solution with dp[i] representing the minimum penalty for words i..n-1, specify the recurrence, reconstruction method, and time/space complexity. How would you implement it?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Anthropic","DoorDash","Square"]},{"id":"q-718","question":"You're given an array A[1..n] and an integer k. Partition into exactly k contiguous segments. The cost of a segment [t+1..i] is (sum(A[t+1..i]))^2. Return the minimum total cost and the partition indices. Propose DP: P as prefix sums, dp[i][j] = min_{t in [j-1..i-1]} dp[t][j-1] + (P[i]-P[t])^2, with base dp[0][0]=0. Explain reconstruction and discuss naive vs. optimized time, space, and edge cases?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Bloomberg","Netflix"]},{"id":"q-731","question":"You're given an array A of length n with non-negative integers representing daily story points. You must partition the days into consecutive weeks, each containing 2–7 days. The cost of a week is (sum of that week's points − W)^2 where W is a fixed target. Return the minimum total cost to cover all days or INF if impossible. Propose a DP with dp[i] as min cost for first i days; dp[i] = min_{k=2..7, i-k>=0} dp[i-k] + (sum(i-k+1..i) − W)^2, using prefix sums for O(1) range sums. Reconstruct weeks and analyze time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Databricks","Salesforce","Two Sigma"]},{"id":"q-736","question":"You're given an n x m grid. Each cell (i, j) has a color c[i][j] in [0, C-1] and a non-negative cost w[i][j]. Start at (0,0) and move to (n-1,m-1) with only right or down moves. You must visit at least one cell of every color that appears in the grid along your path. Return the minimum total cost to do so, or -1 if impossible. Describe a DP using dp[i][j][mask] where mask tracks visited colors; explain base cases, transitions from top/left, how to reconstruct the path, and complexity?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Microsoft","Netflix","Plaid"]},{"id":"q-743","question":"You're given a list of n task durations L. Partition the tasks into consecutive days (each day at least one task); the cost of a day is the maximum duration on that day. Devise a DP to minimize the total cost across all days. Define dp[i] as the minimum cost for the first i tasks, provide the recurrence, reconstruction method, and complexity. How would you implement it?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Slack","Stripe","Twitter"]},{"id":"q-754","question":"Given a circular array v[1..n] of non-negative values and an integer d≥1, pick a subset of indices such that the cyclic distance between any two chosen indices is at least d. Maximize the sum of selected values; return both the maximum sum and the number of distinct optimal subsets. Constraints: n ≤ 2e5, v[i] ≤ 1e9. Describe an O(n) DP solution with two cases for circularity and how you'd reconstruct counts?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Apple","Netflix","Uber"]},{"id":"q-768","question":"You're given an array A of length n and an integer k. Partition A into exactly k non-empty contiguous subarrays. The cost of a subarray [t+1..i] is (max(A[t+1..i])) * (i-t). Return the minimum total cost and the partition indices. Propose DP: dp[i][j] = min_{t in [j-1..i-1]} dp[t][j-1] + max(A[t+1..i]) * (i-t). Explain reconstruction, base cases, and time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Adobe","Robinhood","Salesforce"]},{"id":"q-777","question":"You're given an array A of non-negative integers representing task durations in order. You must schedule all n tasks into exactly m days. Each day can host a consecutive block with total duration <= D. The cost of a day is (sum of that day's durations)^2. Return the minimum total cost, or -1 if impossible? Propose a DP using dp[i][d] as min cost for first i tasks in d days, with transition dp[i][d] = min_{j<i, prefix[i]-prefix[j] <= D} dp[j][d-1] + (prefix[i]-prefix[j])^2. Include base cases, reconstruction, and time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Amazon","Apple","Robinhood"]},{"id":"q-786","question":"You have an array A of length n. Partition into exactly k non-empty contiguous blocks. The cost of a block [t+1..i] is (max(A[t+1..i])) * (sum(A[t+1..i])). Return the minimum total cost and the partition indices. Propose a DP formulation, reconstruction strategy, and complexity analysis. Assume 1-based indexing?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Coinbase","Scale Ai","Snap"]},{"id":"q-792","question":"You're given an array prices[0..n-1]. You may complete at most k buy-sell transactions (one share at a time, can't hold more than one). Return the maximum profit and the days of each trade. Propose a DP formulation with states cash[i][t] and hold[i][t], include recurrences, base cases, and reconstruction, and discuss time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Goldman Sachs","IBM","MongoDB"]},{"id":"q-797","question":"You have an array of task difficulties A of length n. Partition it into exactly m non-empty contiguous chapters. Each chapter cost equals the maximum difficulty within that chapter. Return the minimum total cost and a valid partition (chapter end indices). Propose a DP: dp[i][j] = min_{t in [j-1..i-1]} dp[t][j-1] + max(A[t..i-1]), with base dp[i][1] = max(A[0..i-1]). Explain reconstruction, base cases, and time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"beginner","tags":["dynamic-programming"],"companies":["Adobe","LinkedIn","OpenAI"]},{"id":"q-806","question":"You're given an n x m grid of digits grid[i][j] in [0..9]. You may move only right or down from (0,0) to (n-1,m-1). Define a path score as the number of times the next cell's digit is strictly larger than the previous cell's digit along the path. Return the maximum score and a valid path (as coordinates or directions). Propose a dynamic programming formulation with recurrences, base cases, and reconstruction, and discuss time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Google","Hugging Face","Tesla"]},{"id":"q-813","question":"You're given a tree with N nodes (N up to 2e5). Each node i has a value val[i]. Find a connected subtree of exactly K nodes that maximizes the sum of values. Return the maximum sum and the node set. Propose a DP formulation with dp[u][s] = max sum of a connected subtree of size s that contains u and lies entirely within u's subtree when the tree is rooted at 1; include reconstruction, base cases, and discuss time/space?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["Google","Microsoft"]},{"id":"q-821","question":"You're given a string s of length n consisting of lowercase letters. Partition s into at most k non-empty contiguous substrings. The cost of a substring is the number of distinct characters in that substring. Return the minimum total cost and one valid partition (end indices). Propose a DP formulation with recurrence dp[i][t] = min_{p in [t-1..i-1]} dp[p][t-1] + cost(p, i-1) where cost(p, q) is the number of distinct letters in s[p..q]. Explain reconstruction, base cases, and time/space complexity?","channel":"dynamic-programming","subChannel":"general","difficulty":"intermediate","tags":["dynamic-programming"],"companies":["Citadel","LinkedIn","Snap"]},{"id":"q-829","question":"You're given an array A of length n and an integer k. Partition A into exactly k non-empty contiguous subarrays. Each subarray's cost is max(A[l..r]) - min(A[l..r]). Return the minimum total cost and one valid partition (end indices). Propose a DP: dp[i][t] = min_{p in [t-1..i-1]} dp[p][t-1] + (max(A[p..i-1]) - min(A[p..i-1])); base: dp[i][1] = max(A[0..i-1]) - min(A[0..i-1]). Explain reconstruction, base cases, and time/space complexity?","channel":"dynamic-programming","subChannel":"general","difficulty":"advanced","tags":["dynamic-programming"],"companies":["DoorDash","Hugging Face","Netflix"]},{"id":"q-235","question":"How do you organize Cypress fixtures for component testing, and what are the key patterns for managing test data dependencies across multiple test suites?","channel":"e2e-testing","subChannel":"cypress","difficulty":"beginner","tags":["cypress","component-testing","fixtures"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"q-1173","question":"Design an end-to-end test for GDPR data deletion in an e-grocery platform: a synthetic user requests account deletion that must purge PII from cart, catalog, checkout, loyalty services, and analytics pipelines across three regions. Describe data setup, purge verification across stores, caches, and search indexes, audit logs, and idempotency after replay; outline concrete tooling (Playwright + REST mocks + Kafka) and how you handle eventual consistency and test isolation?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Apple","DoorDash"]},{"id":"q-1335","question":"Design an end-to-end test strategy for a multi-tenant SaaS app with per-tenant data isolation, dynamic feature flags, and role-based access controls. Outline how you seed tenants, run parallel tests across tenants and roles, validate feature gating, and verify audit logs across services. Include tooling choices, data isolation strategy, and cleanup?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Adobe","Tesla"]},{"id":"q-1420","question":"Design a beginner-friendly E2E test for a CMS bulk-upload feature: a CSV with 100 rows is uploaded; validations run in the background; after completion, 100 pages are created with titles from the CSV; outline specific UI steps, how you verify progress, how you confirm data creation via API, and how you isolate the test to avoid polluting production data?","channel":"e2e-testing","subChannel":"general","difficulty":"beginner","tags":["e2e-testing"],"companies":["Adobe","Twitter"]},{"id":"q-1603","question":"Design an E2E test for a real-time collaborative post editor behind a region-specific feature flag (US vs EU) in a Twitter/Airbnb-like product. The test should cover flag gating, optimistic UI updates, WebSocket propagation across regions, eventual consistency after reconnections, and a rollback path when the flag is disabled. Which tooling and concrete steps would you use?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Airbnb","Twitter"]},{"id":"q-1713","question":"Design an end-to-end test for a Plaid-like bank-link flow embedded in a fintech app, covering OAuth-like redirects, token exchange, and account data pulls. Include how you isolate test data, simulate bank outages, verify idempotent ledger events, and ensure eventual consistency across retries?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Microsoft","Plaid"]},{"id":"q-1746","question":"Design an E2E test for an embeddable form builder widget used by partners (Salesforce, Cloudflare, Coinbase) that runs inside an iframe. It uses postMessage for cross-origin events, drafts in localStorage, and submits to a CORS API. Create an end-to-end test that validates tenant isolation, iframe messaging, draft autosave after brief network blips, and successful submission across two partner domains?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Cloudflare","Coinbase","Salesforce"]},{"id":"q-1893","question":"Design an end-to-end test for a multi-tenant data-export feature: an admin triggers an export via the UI; the export runs in the background, reads tenant-scoped data from multiple services, writes to blob storage, and emails a download link. How would you verify tenant isolation, idempotent retries, and resilience to blob outages without cross-tenant leakage? Include concrete tooling choices and test data strategy?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Airbnb","Microsoft"]},{"id":"q-2213","question":"Design a real-world E2E test for onboarding a new tenant in a multi-region SaaS platform. The flow creates a tenant, provisions resources in 3 regions, configures IAM roles, and activates a 30-day billing trial. Include test data isolation, region failover, eventual consistency checks, idempotent retries, and rollback guarantees. Specify tooling and steps?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Cloudflare","Discord","Goldman Sachs"]},{"id":"q-2302","question":"Design an end-to-end test for a real-time collaboration feature in a chat app with offline support where 3 clients edit the same message concurrently. Include how you simulate network partitions and reconciliations, verify offline queues, message ordering, conflict resolution, and eventual consistency across all clients?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Discord","Google"]},{"id":"q-2367","question":"Design an end-to-end test plan for a multi-tenant, localization-aware onboarding flow in a SaaS app. The flow includes marketing landing in the user's locale, signup that provisions a new workspace, email verification, in-app onboarding tour, and role-based access setup across three regions. Include how you isolate test tenants, seed data, simulate slow networks, verify asynchronous provisioning, and ensure no cross-tenant data leakage. Specify tooling (Playwright or Cypress), data seeding strategies, and validation hooks?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["IBM","LinkedIn"]},{"id":"q-2388","question":"Describe an E2E test for a multi-tenant onboarding flow where a new tenant is provisioned via API, then UI steps (profile, plan, webhooks) must complete across auth, billing, and notification services. Include per-tenant isolation (unique tenantId, separate DB/schema), data lifecycle, cross-service verification via logs/events, and cleanup. Use Playwright, REST, and DB probes?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Adobe","Google","Plaid"]},{"id":"q-2444","question":"Design an end-to-end test for a real-time collaborative editor built on Automerge CRDTs, with three clients editing the same document concurrently, including offline edits, reconnection, and latency across two regions. Describe how you simulate network partitions, verify eventual consistency, ensure idempotent operations, and test presence/undo/redo semantics under reconnect?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["MongoDB","Salesforce","Twitter"]},{"id":"q-2526","question":"Design an advanced E2E test for a real-time order-status pipeline built on a pub/sub backbone (Kafka-like) with a downstream fan-out service. Validate exactly-once delivery, out-of-order processing, regional outages, and per-tenant data isolation; include data generation, replay safety, and latency/throughput metrics under load?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Cloudflare","DoorDash","IBM"]},{"id":"q-2579","question":"Design a beginner-friendly E2E test for a MongoDB-backed admin UI CSV import feature that upserts users by email. The test should cover test data isolation, handling of invalid rows, idempotent re-import, and verification of updated fields in both UI and DB. Include tooling suggestions, concrete steps, and a cleanup strategy?","channel":"e2e-testing","subChannel":"general","difficulty":"beginner","tags":["e2e-testing"],"companies":["Bloomberg","MongoDB"]},{"id":"q-449","question":"How would you design an E2E testing strategy for a distributed edge computing platform that needs to validate functionality across 100+ global data centers with varying network conditions?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Cloudflare","Tesla"]},{"id":"q-460","question":"You're testing a login form with Playwright. The form has email and password fields, and a submit button. How would you write a basic E2E test to verify successful login and redirect to dashboard?","channel":"e2e-testing","subChannel":"general","difficulty":"beginner","tags":["e2e-testing"],"companies":["Lyft","MongoDB","NVIDIA"]},{"id":"q-491","question":"How would you set up a basic E2E test for a login form using Playwright?","channel":"e2e-testing","subChannel":"general","difficulty":"beginner","tags":["e2e-testing"],"companies":["IBM","Lyft","Snowflake"]},{"id":"q-521","question":"You're testing a React app with Playwright. Some tests fail intermittently due to API delays. How would you make your e2e tests more reliable without removing the API dependency?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["Google","Hugging Face"]},{"id":"q-548","question":"How would you design a scalable E2E testing strategy for a microservices architecture with 50+ services, ensuring test isolation and parallel execution while maintaining realistic user journeys?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["LinkedIn","Scale Ai"]},{"id":"q-574","question":"How would you handle flaky E2E tests in a CI/CD pipeline? What strategies would you implement to ensure reliable test execution?","channel":"e2e-testing","subChannel":"general","difficulty":"intermediate","tags":["e2e-testing"],"companies":["LinkedIn","Meta","Oracle"]},{"id":"q-850","question":"In a Stripe-like billing system built on an event-driven microservice architecture, design an E2E test that validates the end-to-end flow from a user initiating a purchase to invoice settlement across regionally distributed services. Include how you verify eventual consistency, idempotency, and state replay safety after a simulated regional outage, with concrete tooling choices and steps?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Google","Stripe"]},{"id":"q-901","question":"Design a beginner-friendly E2E test for a React checkout flow: user visits a product page, adds to cart, proceeds to checkout, fills shipping details, and completes a payment via a sandbox API. Explain how you would ensure test isolation and determinism (seed/reset test data, mock payment endpoint), and show a minimal Playwright script snippet that asserts successful order confirmation and a backend order record?","channel":"e2e-testing","subChannel":"general","difficulty":"beginner","tags":["e2e-testing"],"companies":["MongoDB","Tesla","Twitter"]},{"id":"q-949","question":"Design an end-to-end test plan for a Netflix/Meta-like streaming service that delivers adaptive bitrate video across 4 regions. Include how you model test content, simulate varying network conditions, verify manifest/chunk fetch, DRM/licensing checks, on-device caching, and end-to-end telemetry; specify tooling, data isolation, and how you scale across regions while minimizing flakiness?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Meta","Netflix"]},{"id":"q-979","question":"Design an end-to-end test for a multi-tenant content platform that serves regionally personalized content with feature flags and A/B tests. The platform must ensure per-tenant data isolation, correct content personalization, flag-driven UI, and during regional outages. Outline the test scope, data management, tooling, and steps to verify end-to-end delivery across tenants and regions without flakiness?","channel":"e2e-testing","subChannel":"general","difficulty":"advanced","tags":["e2e-testing"],"companies":["Adobe","Meta","Salesforce"]},{"id":"q-279","question":"What are the key differences between getByRole() and getByText() selectors in Playwright, and when would you choose one over the other for reliable E2E testing?","channel":"e2e-testing","subChannel":"playwright","difficulty":"beginner","tags":["playwright","browser-automation","selectors"],"companies":["Adobe","Amazon","Microsoft","Netflix","Salesforce"]},{"id":"q-208","question":"What is the difference between Selenium WebDriver and Selenium Grid, and when would you use each in your testing strategy?","channel":"e2e-testing","subChannel":"selenium","difficulty":"beginner","tags":["selenium","webdriver","grid"],"companies":["Amazon","Google","Meta"]},{"id":"q-1062","question":"You're stewarding reliability for a 24/7 payments platform with two active regions and strict latency requirements. A recent outage exposed brittle failover and slow triage. Outline a practical plan: governance model (platform vs product teams), incident playbooks, auto-remediation, SRE metrics, release controls, and how you measure ROI while preserving feature velocity?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Citadel","Microsoft","Square"]},{"id":"q-1088","question":"Two squads share a single API surface: Platform Reliability (2 senior + 1 mid) and Growth Feature (4 engineers). Platform incidents increased MTTR by 40% last quarter; Growth feature is 70% complete but depends on unstable APIs and tight external deadlines. How would you structure quarterly planning to protect reliability, reallocate capacity, set SLOs and error budgets, and implement gating (flags, canaries, contracts) to finish the Growth feature without amplifying risk?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Google","Robinhood","Uber"]},{"id":"q-1223","question":"You're steward of a shared data platform used by 8 squads across web, mobile, and ML workloads. A new event schema from one squad breaks downstream contracts and delays analytics dashboards. Propose a governance model: data contracts, versioned schemas, deprecation policy, and a cross-squad escalation process. Include concrete ownership, metrics, and a migration plan that minimizes customer impact while meeting quarterly release goals?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Anthropic","Instacart","Twitter"]},{"id":"q-1309","question":"You're leading engineering for a company with 5 teams (Auth, Billing, Search, Recommendations, Web). A directive requires end-to-end delivery quality: cut post-release hotfix rate by 40% while preserving delivery velocity. Propose a concrete plan to implement dual-track planning (feature vs reliability), allocate capacity (e.g., 60/25/15 split), designate cross-cutting owners (telemetry, incident mgmt, release engineering), and the metrics, rituals, and a 12-week rollout. Include milestones and go/no-go criteria?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Airbnb","DoorDash","Meta"]},{"id":"q-1376","question":"How would you design and implement a platform governance model for 6 product squads relying on a shared platform (auth, billing, search) with a cap of 2 engineers for platform work per sprint, ensuring API stability, a deprecation policy, telemetry, and incident response, plus a 12-week rollout and concrete success metrics? Provide the first 90-day plan and the top trade-offs?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Hugging Face","IBM","Instacart"]},{"id":"q-1614","question":"As owner of a real-time analytics platform used by three customer apps, a silent ETL failure intermittently leaves dashboards stale for 20% of users, skewing revenue KPIs. Provide a concrete remediation plan: 1) 24h incident triage, 2) redesigned monitoring with SLIs/SLOs and automated remediation, 3) cross-team ownership and gating, 4) validation and rollback, 5) a 6-week rollout with milestones and go/no-go criteria. Include concrete metrics and an initial implementation plan?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Lyft","PayPal","Robinhood"]},{"id":"q-1687","question":"You're overseeing five platform teams (Networking, Compute, Auth, Billing, Observability). EU data localization is mandated with a 9-month deadline; design a concrete program to migrate data stores and services with minimal downtime, ensure compliance and data sovereignty, and minimize feature disruption. Include ownership matrix, migration waves, rollback plan, telemetry & auditability, go/no-go criteria, and success metrics?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Cloudflare","Google","LinkedIn"]},{"id":"q-1827","question":"You're the head of platform engineering at a fintech with four squads—Payments, Identity, Analytics, and Notifications. A critical legacy event bus must be replaced with Apache Pulsar. Current availability 99.99% and latency <100 ms, ~2M messages/day, strict regulatory retention for audits. You have 6 weeks, no downtime. Outline a phased migration plan: dependency map, cutover, rollback, telemetry, governance, and success metrics. What plan would you implement to accomplish this within the constraints?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Goldman Sachs","Lyft","Stripe"]},{"id":"q-1856","question":"Oversee a data-platform initiative to consolidate 6 product domains into a centralized data catalog with standardized data contracts. Ambiguous expectations cause data quality issues and flaky dashboards. Design a governance model and a 90-day rollout: schema/versioning, tests, SLIs/SLOs for data, data-contract rituals, and cross-stakeholder engagement. How would you measure ROI and prevent contract creep?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Databricks","Meta","Robinhood"]},{"id":"q-2028","question":"How would you implement a **dual-track leadership ladder** (Technical Leader vs People Manager) in a 4-squad, multi-region organization to deepen technical depth without sacrificing delivery velocity? Propose a 90‑day pilot design, governance, mentoring cadence, and shared OKRs; specify success metrics (**velocity**, turnover, time-to-promotion) and a rollout plan?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Google","Netflix"]},{"id":"q-2047","question":"In a fast-scaling organization aiming for 10x traffic, you manage five squads: Platform, Auth, Payments, Data, and Web, plus centralized SRE. A quarterly objective mandates MTTR down 40%, lead time down 25%, and on-call toil down 20%, without sacrificing velocity. Design an org and governance model, specify roles and rituals, metrics to track, and a 12-week rollout with milestones. Include risk mitigations and rollout gates?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Amazon","Anthropic"]},{"id":"q-2310","question":"You're leading a 6-month program to migrate three critical services to an event-driven architecture using Kafka. Teams span Node.js, Python, and Java, with tight uptime requirements and uneven test coverage. Propose a concrete plan detailing governance, risk-based rollout, cutover strategy, rollback procedures, and the metrics/rituals you would use to stay within SLOs while preserving velocity. Include milestones and go/no-go criteria?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Oracle","Slack","Snap"]},{"id":"q-2428","question":"You're leading an ML Platform group supporting 6 product squads with data privacy and cost constraints. How would you design a chargeback-enabled operating model: cost allocation, guardrails (data access, drift monitoring), and reusable components, plus metrics and a 12-week rollout with milestones and go/no-go criteria?","channel":"engineering-management","subChannel":"general","difficulty":"advanced","tags":["engineering-management"],"companies":["Anthropic","Cloudflare","Meta"]},{"id":"q-2525","question":"How would you implement a cross‑team reliability program that reduces post‑release incidents while preserving feature velocity across three product lines (Auth, Billing, Search) within a 12‑week rollout? Include governance, dual‑track planning, ownership, metrics, rituals, and concrete milestones?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Adobe","DoorDash","OpenAI"]},{"id":"q-2660","question":"You're overseeing four squads across three regions. A unified Authentication Portability Initiative must replace in-house OAuth with a standard provider, risking existing third party integrations. Design a concrete 12 week rollout plan with sequencing, dependency contracts, testing gates, rollback, telemetry, and success criteria. Include cross team governance?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Apple","Bloomberg","Databricks"]},{"id":"q-461","question":"How would you handle a situation where your top engineer wants to work on a different project, but you need them to complete a critical deadline?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Airbnb","Google"]},{"id":"q-492","question":"How would you handle a situation where your top engineer wants to work on a personal project during work hours, claiming it will benefit the company long-term?","channel":"engineering-management","subChannel":"general","difficulty":"intermediate","tags":["engineering-management"],"companies":["Microsoft","Snap"]},{"id":"q-522","question":"You're leading a team of 5 engineers. Two team members disagree on the technical approach for a critical feature. How do you handle this situation while maintaining team morale and meeting the deadline?","channel":"engineering-management","subChannel":"general","difficulty":"beginner","tags":["engineering-management"],"companies":["Google","Instacart"]},{"id":"q-549","question":"How would you handle a situation where your top engineer wants to work on a different project than what the team needs?","channel":"engineering-management","subChannel":"general","difficulty":"beginner","tags":["engineering-management"],"companies":["Apple","Hashicorp","Scale Ai"]},{"id":"q-575","question":"How do you balance technical debt with feature delivery when managing engineering teams?","channel":"engineering-management","subChannel":"general","difficulty":"beginner","tags":["engineering-management"],"companies":["Bloomberg","PayPal"]},{"id":"q-860","question":"When onboarding new engineers to a project with a legacy codebase and a new component library, operating on a 3-week sprint with shared CI, what concrete onboarding plan and gates would you implement in the first 4 weeks to accelerate learning while preserving code quality and preventing regressions?","channel":"engineering-management","subChannel":"general","difficulty":"beginner","tags":["engineering-management"],"companies":["Meta","Stripe"]},{"id":"q-184","question":"You're managing a critical microservices migration from monolith to Kubernetes with 3 teams. Team A (backend services) is 2 weeks behind due to database connection pooling issues, Team B (frontend) is on track but blocked by API contracts, and Team C (DevOps) needs production-ready Helm charts by EOW. How do you resolve the technical dependencies and get the migration back on schedule while maintaining service availability?","channel":"engineering-management","subChannel":"project-management","difficulty":"advanced","tags":["project","planning"],"companies":["Adobe","Amazon","Google","Microsoft","Netflix","Salesforce"]},{"id":"q-211","question":"How would you implement a technical debt repayment framework using the 20% time allocation model while balancing feature delivery deadlines?","channel":"engineering-management","subChannel":"project-management","difficulty":"intermediate","tags":["delegation","mentoring","growth"],"companies":["Google","LinkedIn","Microsoft","Robinhood","Stripe"]},{"id":"q-261","question":"Design a task delegation matrix system for a 15-person engineering team that balances skill development with project delivery SLAs. Include RACI implementation, automated task assignment algorithms, and success metrics. How would you handle edge cases like skill gaps and conflicting priorities?","channel":"engineering-management","subChannel":"team-leadership","difficulty":"beginner","tags":["delegation","mentoring","growth"],"companies":["Amazon","Google","Meta","Microsoft","Salesforce","Stripe"]},{"id":"q-281","question":"How do you influence technical decisions when you're not the technical lead, and what specific strategies do you use to build technical credibility across different stakeholder groups?","channel":"engineering-management","subChannel":"team-leadership","difficulty":"intermediate","tags":["communication","collaboration","influence"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-168","question":"Explain the CSS box model and how box-sizing affects layout calculations. What's the difference between border-box and content-box?","channel":"frontend","subChannel":"css","difficulty":"beginner","tags":["css","styling"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-363","question":"You have a navigation bar with 3 items that should be evenly spaced. The middle item needs to be centered while the first and last items stick to the edges. How would you implement this using CSS Flexbox?","channel":"frontend","subChannel":"css","difficulty":"beginner","tags":["css","flexbox","grid","animations"],"companies":["Amazon","Google","Hrt","Meta","Microsoft","Netflix"]},{"id":"q-408","question":"You're building a responsive dashboard with a complex grid layout that must support dynamic widget resizing, reordering via drag-and-drop, and maintain performance with 100+ widgets. How would you architect the CSS grid system to handle these requirements while ensuring smooth animations and preventing layout thrashing?","channel":"frontend","subChannel":"css","difficulty":"advanced","tags":["css","flexbox","grid","animations"],"companies":["Affirm","Okta","Shopify"]},{"id":"q-661","question":"Design a responsive image gallery using CSS Grid that shows 4 columns on wide screens, 2 on tablets, and 1 on mobile. Each tile contains an image, title, and caption. Add a hover/focus animation that lifts the tile and deepens the shadow, and ensure keyboard accessibility and respect for reduced-motion preferences?","channel":"frontend","subChannel":"css","difficulty":"beginner","tags":["css","flexbox","grid","animations"],"companies":["Amazon","Snap","Square"]},{"id":"q-664","question":"Question: Create a responsive 3-column feature grid using CSS Grid that collapses to a single column on narrow viewports, with a hover animation that gently scales each card and reveals a caption with a slide-in effect, while keeping focus-visible styles and respecting prefers-reduced-motion?","channel":"frontend","subChannel":"css","difficulty":"beginner","tags":["css","flexbox","grid","animations"],"companies":["Apple","Meta"]},{"id":"q-1342","question":"You're building a React application where certain components need to maintain state that persists across page refreshes but doesn't need to be shared globally. How would you implement a state management solution that combines local component state with browser storage, and what are the key considerations for handling synchronization, performance, and potential race conditions?","channel":"frontend","subChannel":"frontend","difficulty":"intermediate","tags":["react-hooks","local-storage","state-persistence","custom-hooks","performance"],"companies":[]},{"id":"q-2125","question":"You're building a React form with multiple controlled inputs that need to maintain their state during component unmounting/remounting (e.g., when switching between tabs in a single-page application). How would you implement a state management solution that preserves form state across component lifecycle changes while avoiding the performance pitfalls of lifting all state to a global store, and what specific React patterns would you use to handle this efficiently?","channel":"frontend","subChannel":"frontend","difficulty":"intermediate","tags":["react-hooks","form-state","persistence","performance","component-lifecycle"],"companies":[]},{"id":"q-2619","question":"You're building a React application with complex state that needs to be shared across multiple components, but you want to avoid the performance overhead of Context API re-renders. How would you implement a custom hook-based state management solution that uses React's useReducer combined with memoization techniques to create a lightweight, performant state management system, and what specific patterns would you use to prevent unnecessary re-renders?","channel":"frontend","subChannel":"frontend","difficulty":"intermediate","tags":["react","state-management","performance","custom-hooks","useReducer"],"companies":[]},{"id":"q-642","question":"You're building a React form with multiple controlled inputs that need to share validation state. How would you implement a custom hook to manage form state and validation logic efficiently, and what are the key considerations for preventing unnecessary re-renders?","channel":"frontend","subChannel":"frontend","difficulty":"intermediate","tags":["react-hooks","form-validation","performance-optimization","custom-hooks","state-management"],"companies":[]},{"id":"q-762","question":"You're building a React dashboard with multiple components that need to share and update real-time data (like stock prices or live metrics). How would you implement a state management solution that minimizes re-renders while ensuring all components receive the latest data, and what specific React patterns would you use to prevent performance bottlenecks?","channel":"frontend","subChannel":"frontend","difficulty":"intermediate","tags":["react-state-management","performance-optimization","context-api","real-time-data","memoization"],"companies":[]},{"id":"fe-2","question":"Explain the JavaScript Event Loop architecture. How do microtasks and macrotasks differ in execution order, and what are the practical implications for async/await code?","channel":"frontend","subChannel":"javascript","difficulty":"beginner","tags":["js","async","core"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"fe-3","question":"Explain JavaScript closures with a practical use case and how they're used in real applications?","channel":"frontend","subChannel":"javascript","difficulty":"intermediate","tags":["js","scope","patterns"],"companies":["Amazon","Google","Meta"]},{"id":"fr-157","question":"What is the difference between `let`, `const`, and `var` in JavaScript, and how do their scoping rules and temporal dead zone affect real-world code?","channel":"frontend","subChannel":"javascript","difficulty":"beginner","tags":["js","core"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"fr-162","question":"Explain how JavaScript's event loop handles microtasks vs macrotasks. What happens when a Promise resolves inside a setTimeout callback?","channel":"frontend","subChannel":"javascript","difficulty":"advanced","tags":["js","core"],"companies":["Airbnb","Google","Meta","Netflix","Stripe"]},{"id":"fr-173","question":"What is the output of this code and explain the event loop behavior: console.log('A'); setTimeout(() => console.log('B'), 0); Promise.resolve().then(() => console.log('C')); Promise.resolve().then(() => console.log('D')); console.log('E'); How do microtask and macrotask queues interact in JavaScript's event loop?","channel":"frontend","subChannel":"javascript","difficulty":"advanced","tags":["js","core"],"companies":["Amazon","Google","Meta","Netflix","Stripe"]},{"id":"q-240","question":"What is a closure in JavaScript and how does it enable data encapsulation?","channel":"frontend","subChannel":"javascript","difficulty":"beginner","tags":["js","es6","closures","promises"],"companies":["Airbnb","Amazon","Apple","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-351","question":"You're building a file upload component that processes multiple files in parallel. How would you implement a concurrent upload queue with a maximum of 3 simultaneous uploads using Promise.allSettled and closures?","channel":"frontend","subChannel":"javascript","difficulty":"intermediate","tags":["js","es6","closures","promises"],"companies":["Amazon","Anthropic","Microsoft"]},{"id":"q-462","question":"Implement a rate-limited API wrapper that queues requests when the limit is reached, using closures to maintain state and promises to handle request ordering?","channel":"frontend","subChannel":"javascript","difficulty":"advanced","tags":["js","es6","closures","promises"],"companies":["Google","Lyft","Scale Ai"]},{"id":"q-550","question":"Explain how closures work in JavaScript and provide a practical example of when you'd use one in a React component?","channel":"frontend","subChannel":"javascript","difficulty":"beginner","tags":["js","es6","closures","promises"],"companies":["Google","Oracle","Tesla"]},{"id":"fr-154","question":"What are the performance implications and layout shift consequences of loading large images without explicit dimensions, and how do modern CSS properties and loading strategies mitigate these issues?","channel":"frontend","subChannel":"performance","difficulty":"beginner","tags":["perf","optimization"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"fr-172","question":"How would you optimize rendering performance for a React component displaying a large list (10,000+ items) with frequent real-time updates?","channel":"frontend","subChannel":"performance","difficulty":"intermediate","tags":["perf","optimization"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-188","question":"How would you implement a performance budget system that automatically detects bundle regressions and enforces lazy-loading boundaries in a large-scale React application?","channel":"frontend","subChannel":"performance","difficulty":"advanced","tags":["lighthouse","bundle","lazy-loading"],"companies":["Airbnb","Atlassian","LinkedIn","Netflix","Uber"]},{"id":"q-301","question":"How would you optimize a React app's bundle size to achieve Lighthouse scores above 90, and what specific tools and metrics would you use to measure success?","channel":"frontend","subChannel":"performance","difficulty":"beginner","tags":["lighthouse","bundle","lazy-loading"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-329","question":"You're tasked with improving a React app's Lighthouse performance score from 65 to 90+. The bundle size is 2.1MB and Time to Interactive is 4.2s. What specific steps would you take to optimize the bundle and implement lazy loading?","channel":"frontend","subChannel":"performance","difficulty":"intermediate","tags":["lighthouse","bundle","lazy-loading"],"companies":["Amazon","Google","Mckinsey","Meta","Microsoft","Netflix","Scale Ai","Stripe"]},{"id":"q-378","question":"You're building a real-time trading dashboard at DE Shaw that needs to display 1000+ rapidly updating price cards. How would you optimize CSS layout and animations to maintain 60fps while cards are being added/removed/updated every 100ms?","channel":"frontend","subChannel":"performance","difficulty":"advanced","tags":["css","flexbox","grid","animations"],"companies":["DE Shaw","Discord","Instacart"]},{"id":"q-390","question":"You're working on a React app that loads slowly. Your Lighthouse performance score is 45. What specific steps would you take to improve it, and how would you implement lazy loading for a heavy component?","channel":"frontend","subChannel":"performance","difficulty":"beginner","tags":["lighthouse","bundle","lazy-loading"],"companies":["Crowdstrike","Deepmind","NVIDIA"]},{"id":"q-395","question":"You're building a complex dashboard with a responsive grid layout that must support dynamic column insertion, reordering, and animated transitions. How would you implement this using CSS Grid and JavaScript while maintaining 60fps performance during large dataset updates (10,000+ items)?","channel":"frontend","subChannel":"performance","difficulty":"advanced","tags":["css","flexbox","grid","animations"],"companies":["Google","Meta","Microsoft","Netflix","Salesforce","Stripe"]},{"id":"q-523","question":"Your React app has a 85 Lighthouse performance score. The bundle analyzer shows a 2.8MB main chunk with heavy libraries like moment.js and lodash. How would you optimize this to reach 95+?","channel":"frontend","subChannel":"performance","difficulty":"intermediate","tags":["lighthouse","bundle","lazy-loading"],"companies":["Cloudflare","Hugging Face","Stripe"]},{"id":"q-576","question":"How would you optimize a React app's Lighthouse score from 65 to 90+ using bundle analysis and lazy loading?","channel":"frontend","subChannel":"performance","difficulty":"beginner","tags":["lighthouse","bundle","lazy-loading"],"companies":["Coinbase","DoorDash"]},{"id":"fe-1","question":"How does React's Virtual DOM diffing algorithm work during reconciliation, and what role do keys play in optimizing list updates?","channel":"frontend","subChannel":"react","difficulty":"intermediate","tags":["react","perf","internals"],"companies":["Airbnb","Google","Meta","Microsoft","Netflix"]},{"id":"fr-161","question":"How would you implement a React hook that tracks component render count and warns when it exceeds a threshold, while avoiding infinite render loops?","channel":"frontend","subChannel":"react","difficulty":"advanced","tags":["react","perf"],"companies":["Airbnb","Microsoft","Netflix","Stripe","Uber"]},{"id":"q-215","question":"How would you implement a custom useDebounce hook that works with React's concurrent features and prevents stale closures?","channel":"frontend","subChannel":"react","difficulty":"intermediate","tags":["react","hooks","context","redux"],"companies":["Amazon","Google","Meta","Microsoft","Uber"]},{"id":"q-239","question":"How would you implement a React useMemo hook to optimize a recursive Fibonacci function with memoization, and what are the key trade-offs between top-down memoization vs bottom-up tabulation in this context?","channel":"frontend","subChannel":"react","difficulty":"intermediate","tags":["dp","memoization","tabulation"],"companies":["Airbnb","Amazon","Apple","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-315","question":"How would you optimize a React app's bundle size and loading performance using lazy loading, code splitting, and webpack optimization strategies?","channel":"frontend","subChannel":"react","difficulty":"intermediate","tags":["lighthouse","bundle","lazy-loading"],"companies":null},{"id":"q-434","question":"You're building a React app with multiple components needing access to user authentication state. When would you choose Context API over Redux, and what are the specific performance implications of each approach?","channel":"frontend","subChannel":"react","difficulty":"intermediate","tags":["react","hooks","context","redux"],"companies":["Anthropic","Microsoft","NVIDIA"]},{"id":"q-595","question":"Explain the difference between useState and useReducer hooks in React and when you would choose one over the other.","channel":"frontend","subChannel":"react-hooks","difficulty":"intermediate","tags":["react","hooks","state-management","useState","useReducer"],"companies":["Meta","Netflix","Airbnb","Uber","Spotify"]},{"id":"q-628","question":"Explain the differences between useState, useReducer, and Context API for state management in React. When would you choose each approach?","channel":"frontend","subChannel":"react-hooks","difficulty":"intermediate","tags":["react","state-management","hooks","context-api"],"companies":["Meta","Netflix","Airbnb","Uber","Spotify"]},{"id":"q-287","question":"How does a Service Worker intercept network requests and implement offline caching strategies?","channel":"frontend","subChannel":"web-apis","difficulty":"intermediate","tags":["dom","fetch","websocket","service-worker"],"companies":["Amazon","Google","Meta"]},{"id":"q-341","question":"You're building a real-time collaborative document editor. How would you implement a service worker to handle offline synchronization, WebSocket reconnection logic, and conflict resolution when multiple users edit simultaneously?","channel":"frontend","subChannel":"web-apis","difficulty":"advanced","tags":["dom","fetch","websocket","service-worker"],"companies":["Affirm","Broadcom","Roblox"]},{"id":"q-419","question":"You're building a real-time food delivery tracking app. How would you implement a WebSocket connection that handles network interruptions and maintains order status updates when the app goes offline?","channel":"frontend","subChannel":"web-apis","difficulty":"intermediate","tags":["dom","fetch","websocket","service-worker"],"companies":["Apple","DoorDash","MongoDB"]},{"id":"q-426","question":"You're building a real-time chat application that needs to work offline. How would you implement a service worker to cache messages and sync them when the user comes back online?","channel":"frontend","subChannel":"web-apis","difficulty":"beginner","tags":["dom","fetch","websocket","service-worker"],"companies":["Anthropic","Hugging Face","Snap"]},{"id":"q-493","question":"You're building a real-time grocery delivery tracking app for Instacart. How would you implement a service worker strategy to handle intermittent connectivity, ensure order updates are delivered via WebSockets, and maintain a consistent UI state across network drops?","channel":"frontend","subChannel":"web-apis","difficulty":"advanced","tags":["dom","fetch","websocket","service-worker"],"companies":["Instacart","MongoDB"]},{"id":"q-663","question":"You’re building a chat UI that loads initial messages with fetch('/api/messages'), caches assets via a service worker, and receives live updates over a WebSocket at '/ws'. Explain and implement: (1) pre-cache strategy and cache invalidation in the SW, (2) a fetch wrapper with timeout and offline fallback to cache, (3) DOM updates for incoming WebSocket messages, (4) reconciliation when offline vs online and message ordering?","channel":"frontend","subChannel":"web-apis","difficulty":"intermediate","tags":["dom","fetch","websocket","service-worker"],"companies":["Coinbase","Google","LinkedIn"]},{"id":"q-665","question":"Design and implement a minimal real-time dashboard that loads initial data via fetch('/api/data?limit=200'), then opens a WebSocket to wss://host/ws for live updates, and uses a Service Worker to cache the app shell and latest API responses for offline use. Describe how you batch DOM updates to minimize reflows, ensure reconnection logic, and implement a cache-first strategy with network fallback. Provide core code snippets and trade-offs?","channel":"frontend","subChannel":"web-apis","difficulty":"advanced","tags":["dom","fetch","websocket","service-worker"],"companies":["Apple","PayPal"]},{"id":"q-1003","question":"Design a multi-region DR plan for a Cloud Run API that reads from Cloud SQL and writes results to Cloud Storage and BigQuery. Define RPO/RTO targets, cross-region replication strategy for Cloud SQL, data residency constraints, traffic failover through a global load balancer, and automated DR tests. Include monitoring, IAM least privilege, and post-failover reconciliation steps?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Netflix","Tesla","Two Sigma"]},{"id":"q-1017","question":"Design a beginner-friendly ingestion workflow on GCP for daily CSV exports delivered via partner-signed URLs into a Cloud Storage bucket. Implement a Cloud Function (Python) triggered on finalization to validate, parse, and load aggregates into BigQuery, with structured Cloud Logging including a correlation_id. Outline per-project isolation (dev/stage/prod), idempotent replay, and a simple test plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Amazon","Snap","Snowflake"]},{"id":"q-1092","question":"Design a cross-tenant, multi-region data ingestion pipeline on Google Cloud to handle telemetry from partner apps. Data arrives as daily compressed NDJSON in per-tenant Cloud Storage buckets. Build end-to-end using Cloud Storage triggers or Pub/Sub, Dataflow (Beam) for parsing/transformations, and BigQuery with per-tenant datasets. Enforce least-privilege IAM, strict per-project isolation, Private Service Connect, data residency, auditable Cloud Logging, and idempotent replay with watermarking. Include architecture, data mapping, and a practical test plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Microsoft","Snowflake"]},{"id":"q-1195","question":"Design a beginner data-retention automation on GCP for a daily CSV export that lands in a shared Cloud Storage bucket and is ingested into a partitioned BigQuery table. Implement per-environment isolation (dev/stage/prod) by separate buckets and datasets. Create a Cloud Scheduler job that triggers a Cloud Function (Python) to apply 30-day retention on storage objects, prune BigQuery partitions, and log using Cloud Logging with a correlation_id. Outline validation tests and rollback plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Coinbase","NVIDIA"]},{"id":"q-1233","question":"Design an advanced, per-tenant data lake on Google Cloud for a SaaS platform serving 100 enterprise customers. Ingest on‑prem JSON logs via Pub/Sub to regional Cloud Storage, and use Dataflow to write per‑tenant BigQuery datasets with CMEK; ensure data locality, least‑privilege IAM, and exfiltration controls via VPC Service Controls and Private Service Connect. Outline observability, auditability, and a test plan for IAM changes and retention?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Anthropic","Lyft","Robinhood"]},{"id":"q-1308","question":"Design a beginner-friendly, cost-aware data ingestion pipeline on GCP for a fleet of devices sending JSON events to Pub/Sub. Create per-environment isolation with separate projects, topics, and BigQuery datasets (dev/stage/prod). Use Dataflow for streaming processing into BigQuery, optimize costs with regional resources, and implement Cloud Billing budgets with alerts plus a simple rollback and test plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Lyft","Netflix"]},{"id":"q-1430","question":"Design a multi-tenant ingestion pipeline on GCP where raw data lands in per-tenant GCS buckets across projects and streams into a single partitioned BigQuery dataset. Implement per-tenant isolation, CMEK, least-privilege IAM, cross-project sharing via authorized views, and end-to-end auditing. Include validation, schema evolution, and cost controls?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Citadel","Discord","MongoDB"]},{"id":"q-1483","question":"Design an advanced, multi-tenant streaming pipeline on GCP for a SaaS analytics product. Tenants across 20 projects publish events to Pub/Sub; a Dataflow streaming job validates per-tenant schemas and writes to per-tenant BigQuery datasets with daily partitions. Include least-privilege IAM, per-tenant service accounts, CMEK for BigQuery, cross-project Private Service Connect, idempotent writes, dead-letter handling, retention, and a complete test plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Airbnb","Cloudflare","Tesla"]},{"id":"q-1519","question":"Design a cross-region data ingestion and analytics pipeline on GCP for a global app. Ingest user events (JSON) from Pub/Sub in two regions, store raw data in regional Cloud Storage buckets with CMEK, process with region-specific Dataflow templates to write per-event aggregates to a partitioned BigQuery dataset, and emit redacted summaries to a separate table. Enforce per-environment isolation, VPC Service Controls, and IAM least privilege. Include end-to-end tests and a rollback plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Citadel","Instacart","OpenAI"]},{"id":"q-1636","question":"Design a region-aware streaming fraud pipeline in Google Cloud for a global payments platform. Ingest via Pub/Sub per region, deduplicate and enrich with Dataflow, score in a Cloud Run service, store raw events in Cloud Storage (CMEK) and scores in BigQuery (partitioned by region/tenant); enforce per-tenant isolation via separate projects and IAM; use VPC Service Controls; implement regional DR and monitoring/alerts. Describe architecture, IAM roles, and testing plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Slack","Square","Tesla"]},{"id":"q-1667","question":"Design a beginner-friendly GCP data pipeline: daily partner CSVs arrive in Cloud Storage via signed URLs; build a Dataflow (Python) batch pipeline to validate, deduplicate by id, and upsert into a date-partitioned BigQuery table. Implement per-env isolation with separate buckets/datasets, a dead-letter path for bad rows, and Cloud Logging correlation_id. Include a simple test plan and rollback steps?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Microsoft","Netflix"]},{"id":"q-1802","question":"Design a beginner-friendly, multi-environment GCP data-ingestion pipeline: partner daily CSV exports arrive via signed URLs into per-environment GCS buckets; build a Node.js Cloud Function that validates final URLs and triggers an Apache Beam Dataflow job to scrub PII and load into a partitioned BigQuery table. Outline IAM least-privilege, testing, and observability?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Cloudflare","Discord"]},{"id":"q-1825","question":"Design a cross-region streaming analytics pipeline on GCP where raw events from EU users must remain data-resident, while derived aggregates are queried globally. Describe your architecture using Pub/Sub, Dataflow (Streaming), BigQuery, and Cloud Storage; enforce least-privilege IAM and per-environment isolation; implement data residency checks and automated rollbacks. How would you validate and test this end-to-end?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Coinbase","Netflix","Tesla"]},{"id":"q-1937","question":"Design an end-to-end, multi-tenant event analytics pipeline on Google Cloud: ingest per-tenant events via Pub/Sub push subscriptions, deduplicate and normalize using Dataflow, store data in dedicated BigQuery datasets per tenant, implement strict per-tenant IAM and VPC boundaries, and ensure full auditability with Cloud Audit Logs/Cloud Logging. Include schema evolution handling with a registry and an accompanying test strategy?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Discord","Snowflake","Two Sigma"]},{"id":"q-2050","question":"Design a cost-conscious, multi-env event-driven pipeline for IoT telemetry on GCP: ingest from Pub/Sub, process with Dataflow (Beam) or Cloud Run, and store in partitioned BigQuery. Enforce per-env isolation with distinct topics/subscriptions and datasets; implement idempotent processing via insertId; provide end-to-end replay tests and a rollback plan. How would you implement monitoring and rollback?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Meta","PayPal","Scale Ai"]},{"id":"q-2077","question":"Advanced multi-tenant data lake on GCP: each tenant has isolated Cloud Storage prefixes and BigQuery datasets. Propose architecture enforcing per-tenant IAM conditions, Private Service Connect and VPC Service Controls for restricted egress, and central metadata with Data Catalog/Dataplex. Include ingestion, isolation validation, auditing, and rollback?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Coinbase","Snowflake"]},{"id":"q-2130","question":"Design a beginner-friendly, cost-conscious GCP ingestion and aggregation workflow for daily partner logs uploaded as signed URLs to Cloud Storage. Provide per-environment isolation (dev/stage/prod) via separate prefixes and BigQuery datasets, a Python Cloud Function triggered on finalization to validate, parse, and load aggregates into a partitioned BigQuery table, and simple monitoring/budget alerts. Address idempotence, a light test plan, and a lightweight reconciliation step?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Slack","Snap"]},{"id":"q-2160","question":"Design a secure, multi-tenant data-sharing and analytics pipeline on Google Cloud for a platform hosting ML models (similar to Hugging Face) where customers upload datasets via signed URLs to per-tenant Cloud Storage buckets, data is processed by Dataflow into per-tenant BigQuery datasets, and model training is kicked off in Vertex AI. Include per-tenant IAM least privilege, VPC Service Controls, data residency constraints, audit logging, and automated cost-guardrails?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Cloudflare","Google","Hugging Face"]},{"id":"q-2206","question":"Design a multi-tenant data ingestion and analytics pipeline on Google Cloud for a SaaS platform. Each customer must have isolated BigQuery datasets; data arrives via Pub/Sub and Cloud Storage, processed by Dataflow, and dashboards read from BigQuery. Explain isolation, schema evolution, cost attribution, auditing, and rollback tests?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Adobe","Discord","Meta"]},{"id":"q-2301","question":"Design a cross-project data ingestion and analytics pipeline on Google Cloud that ingests real-time transaction events from multiple partner systems into a per-partner, per-environment BigQuery dataset with strict data residency, isolation, and audit requirements. Use Pub/Sub or Dataflow for streaming, apply envelope encryption with Cloud KMS, ensure exactly-once processing, enable per-project IAM least privilege, implement automated data retention and DR, and provide a test plan and rollback?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Citadel","Oracle","Scale Ai"]},{"id":"q-2423","question":"Design an advanced, compliant data pipeline on GCP for ingesting patient telemetry from clinics in two regions, ensuring data residency, no public endpoints, and per-project isolation. Build an event-driven flow using Pub/Sub, Private Service Connect, Cloud Run/Functions, Dataflow, and BigQuery. Include IAM least privileges, VPC Service Controls boundaries, audit logging, and a rollback plan?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Adobe","Google"]},{"id":"q-2451","question":"Design a cost-conscious, multi-tenant data pipeline on GCP for a SaaS product. Each tenant's data must live in isolated Cloud Storage buckets and BigQuery datasets with strict IAM boundaries. Implement a daily event ingestion from Pub/Sub to Dataflow, partitioned tables, CMEK encryption, and per-tenant retention. How would you validate tenant isolation, auditability, and rollback?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Apple","MongoDB","Twitter"]},{"id":"q-2580","question":"Design an intermediate-level streaming data pipeline on GCP that ingests user activity from Pub/Sub into a partitioned BigQuery table, applying real-time PII redaction via Data Loss Prevention with per-tenant masking policies stored in a central config. Include CMEK encryption for sensitive fields, tenant-scoped IAM, Cloud Logging audits with correlation_id, and a rollback/replay plan using raw data retained in Cloud Storage. What components, data model, and trade-offs would you choose?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Bloomberg","Cloudflare","Twitter"]},{"id":"q-2627","question":"Design an end-to-end, compliance-focused model-prediction pipeline on GCP for sensitive user data. Data arrives as CSV in per-env Cloud Storage, then is de-identified by Data Loss Prevention and fed to Vertex AI hosting behind Private Service Connect. Enforce per-project isolation, least-privilege IAM, no public endpoints, and robust data lineage. Outline architecture, IAM, perimeters, and a testing plan including replay-safe ingestion and rollback?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-architect"],"companies":["Hugging Face","OpenAI"]},{"id":"q-2675","question":"Design a beginner-friendly, per-tenant data processing pipeline on GCP. Incoming JSON messages arrive on Pub/Sub with a tenant_id. Create an event-driven solution using a Cloud Function (Python) to route records to per-tenant BigQuery datasets, with per-environment isolation (dev/stage/prod) and least-privilege IAM. Include a simple rollback using backups in Cloud Storage?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Adobe","NVIDIA","Snap"]},{"id":"q-878","question":"How would you implement a beginner-friendly, auditable deployment pipeline in Google Cloud for a Cloud Run app that reads from Cloud SQL and writes logs to Cloud Logging, ensuring least-privilege IAM, per-project isolation, and no public endpoints?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Cloudflare","Goldman Sachs","IBM"]},{"id":"q-907","question":"Design a private, regional data pipeline for a global fintech platform: events land in regional Pub/Sub topics, Dataflow performs streaming ETL, results stored in per-region BigQuery, and audit logs go to Cloud Logging. Enforce per-region IAM, least privilege, CMEK, Private Service Connect, and no public egress. Describe data flow, security controls, disaster recovery, and cost implications. How would you implement this pipeline?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-architect"],"companies":["Goldman Sachs","Hugging Face","Instacart"]},{"id":"q-977","question":"In a beginner setup, you deploy a Cloud Run API behind Private Service Connect, with logs going to Cloud Logging and traces to Cloud Trace. Outline a practical observability plan: which metrics, logs, and traces to collect; how to build a useful dashboard; how to configure a low-noise alert for 5xx latency; and a simple test to validate instrumentation and alerting?","channel":"gcp-cloud-architect","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-architect"],"companies":["Instacart","PayPal"]},{"id":"q-1016","question":"Design a regional streaming pipeline for a fintech app: on‑prem and GKE emit events to region Pub/Sub topics; a Dataflow streaming job enforces exactly-once, writes partitioned regional BigQuery tables, and triggers Vertex AI scoring in near real-time. How would you achieve low latency, data residency, schema evolution, and reliable failure recovery?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Adobe","PayPal","Stripe"]},{"id":"q-1055","question":"Design an audit-logging pipeline for a payments platform on GCP with sub-100ms end-to-end write latency at multi-region scale (millions of events/sec). Ingest via Pub/Sub, process with Dataflow (Beam), and sink to BigQuery. Explain how you ensure idempotent writes (insertId), handle schema evolution, and provide auditor access across projects without exposing sensitive data. Also outline retries, backoffs, and monitoring?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Amazon","Stripe"]},{"id":"q-1105","question":"You're building a beginner-friendly ingestion pipeline: external partners upload daily CSVs to a Cloud Storage bucket; design a minimal flow using a Cloud Function triggered on object finalize to parse the CSV and load a daily summary as a single row into BigQuery. Include IAM permissions, how to trigger on new files, and how to ensure idempotent writes?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Discord","Microsoft","Snap"]},{"id":"q-1269","question":"Design a real-time data pipeline on GCP for a multi-tenant SaaS where each tenant's data must reside in a specified region, is encrypted with CMEK, and access is strictly controlled per-tenant using IAM Conditions and VPC Service Controls; use Pub/Sub, Dataflow, and BigQuery, ensure idempotent writes and exactly-once semantics, and include monitoring and incident response steps?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Google","PayPal","Salesforce"]},{"id":"q-1331","question":"Design a beginner-friendly nightly backup workflow on GCP: a Cloud SQL MySQL export writes a dump to a Cloud Storage bucket each night; a Cloud Function triggers on the new object to gzip, append a date stamp, and move it to backups/archived. Include IAM bindings, trigger method on new files, and how to ensure exactly-one backup per day (idempotency)?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Instacart","NVIDIA","Tesla"]},{"id":"q-1370","question":"Design a private, streaming ingestion from on-prem to GCP for sensitive financial logs using Pub/Sub, Dataflow, and BigQuery. Include CMEK, IAM, VPC Service Controls, idempotent writes, exactly-once semantics, failure recovery, and cost considerations. Explain choices and trade-offs?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Databricks","IBM","MongoDB"]},{"id":"q-1413","question":"Design a real-time image ingestion pipeline on GCP for a multi-tenant SaaS app. Customers upload images to per-tenant Cloud Storage buckets with CMEK; a Pub/Sub topic notifies on new uploads; a Dataflow streaming job processes images to extract features and writes results to BigQuery, while archived copies are moved to per-tenant archive buckets. Outline architecture, IAM bindings, service account isolation, exactly-once guarantees (e.g., row_id dedup), and cost/latency trade-offs. Include monitoring and failure-response plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Amazon","Apple","IBM"]},{"id":"q-1479","question":"Design a beginner-friendly pipeline on GCP where a daily vendor JSON health report is uploaded to Cloud Storage; create a Cloud Function triggered by finalize to validate JSON against a schema, write valid rows to BigQuery, and move invalid files to a quarantine bucket with a Pub/Sub notice to the on-call channel. Include IAM roles and idempotent processing?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Google","Hashicorp","Hugging Face"]},{"id":"q-1661","question":"You're architecting a multi-tenant data lake on GCP. Ingest partner feeds via Pub/Sub, land raw into Cloud Storage, process with Dataflow streaming, and emit per-tenant results to dedicated BigQuery datasets with IAM controls. How would you enforce isolation, meet <60s latency, support schema evolution, and implement per-tenant cost governance and quotas? Include IAM bindings, Dataflow templates, and error handling?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Two Sigma","Uber"]},{"id":"q-1677","question":"In a high-value fintech setting, design a globally distributed, PCI-compliant fraud-detection pipeline on GCP that ingests events from on-prem via Private Service Connect, publishes to Pub/Sub, processes with Dataflow, stores raw and processed data in Cloud Storage and BigQuery, and uses Vertex AI for real-time scoring; describe IAM, CMEK, VPC, and failure handling, with idempotency?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Citadel","Robinhood"]},{"id":"q-1708","question":"Daily vendor CSVs arrive in Cloud Storage; design a beginner-friendly ingestion pipeline that first scans each file with Cloud DLP to detect PII, and if PII is found moves the file to a quarantine bucket and publishes an alert; if not, parse the CSV and load daily rows into a partitioned BigQuery table. Include IAM bindings, object-finalize trigger, and idempotent processing?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Databricks","Discord","Uber"]},{"id":"q-1729","question":"Design a beginner-friendly thumbnail pipeline for user-uploaded images in GCP. Images uploaded to Cloud Storage bucket incoming-images should trigger a Cloud Run container to generate two thumbnails (200px and 400px wide) and store them in image-thumbs. Use a Cloud Function to trigger on finalization and invoke the Cloud Run HTTP endpoint. Output names: <orig>_200_<hash>.jpg and <orig>_400_<hash>.jpg to ensure idempotency. IAM: restrict access so only the Cloud Run service account can write to image-thumbs. Include failure handling and a basic test plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Discord","NVIDIA","Snowflake"]},{"id":"q-1864","question":"Given a fintech app ingesting market ticks from multiple regions via Pub/Sub, design an intermediate streaming pipeline to load data into BigQuery with deduplication, schema evolution, and per-region partitioning. Use Dataflow for ETL, ensure exactly-once processing, handle late data up to 2 minutes, implement TTL retention on raw data, and outline monitoring and testing strategy?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Bloomberg","Google","Salesforce"]},{"id":"q-1912","question":"Design a beginner-friendly end-to-end GCP pipeline for user avatars stored in Cloud Storage. On object finalize, a Cloud Function should invoke a Cloud Run service to produce two thumbnails (100x100 and 256x256), store them in avatars-resized with deterministic naming, and publish a log entry. If processing fails, publish a Pub/Sub alert and quarantine the original file. Keep IAM minimal and describe a basic test plan and idempotency strategy?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["MongoDB","Twitter","Zoom"]},{"id":"q-2061","question":"Design an end-to-end cross-region streaming data pipeline on GCP to ingest high-volume telemetry from Pub/Sub into BigQuery with real-time dashboards. Use Dataflow (Beam) for streaming, enable exactly-once processing, implement automatic primary/DR region failover, use multi-region Pub/Sub topics and cross-region BigQuery datasets, handle schema evolution, and provide monitoring and rollback strategies?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Bloomberg","IBM"]},{"id":"q-2100","question":"Design a beginner-friendly, event-driven data validation pipeline on GCP for daily event JSONs uploaded to Cloud Storage: on file finalize, a Cloud Function validates each record against a simple JSON Schema, quarantines invalid files, and writes valid records to a BigQuery table with partitioning by date and a deterministic write-ID to ensure idempotency; explain IAM bindings and a basic test plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Airbnb","Databricks","Stripe"]},{"id":"q-2214","question":"You run a SaaS platform on GCP and collect telemetry per tenant via Pub/Sub. Design an end-to-end, multi-region ingestion and analytics pipeline that ensures tenant isolation, exactly-once processing, and scalable cost control. Requirements: (a) topic/subscription layout with a central ingress project and per-tenant downstream sinks; (b) Dataflow streaming job that deduplicates using event_id, enriches with metadata from a centralized store, and writes to daily-partitioned BigQuery tables per tenant; (c) reliable dead-letter handling; (d) security (IAM, CMEK, and VPC Service Controls); (e) cross-region DR and testing plan. Provide a diagram?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Bloomberg","Discord","IBM"]},{"id":"q-2245","question":"Design a real-time, multi-tenant clickstream pipeline on Google Cloud Platform. Ingest events per-tenant from Pub/Sub, enrich with product catalog data stored in BigQuery, compute 5-minute per-tenant engagement metrics, and persist both raw enriched events and per-tenant aggregates to BigQuery. Address exactly-once semantics, late data, per-tenant IAM, data cataloging, and failure handling with a dead-letter queue?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-cloud-engineer"],"companies":["Plaid","Uber"]},{"id":"q-2365","question":"Design a multi-tenant data ingestion and analytics pipeline on GCP for three customers (Snap, Nvidia, Zoom). Each tenant streams events to Pub/Sub, which Dataflow consumes and writes to a central BigQuery data lake with per-tenant isolation using CMEK-encrypted Cloud Storage staging and BigQuery Authorized Views/Row-Level Security. Include idempotent processing, audit logging, cost controls, and a test plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["NVIDIA","Snap","Zoom"]},{"id":"q-2438","question":"Design a cross-region, multi-tenant analytics pipeline on GCP that ingests streaming user events from Cloud Pub/Sub, processes with Dataflow, and writes per-tenant BigQuery datasets in the region of arrival. Include strict data isolation via IAM and VPC Service Controls, encryption (CMEK), backup, failover, and a canary rollout plan for tenants?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Databricks","DoorDash","Oracle"]},{"id":"q-2549","question":"You receive telemetry events from a mobile app into Pub/Sub. Design a beginner-friendly end-to-end ingestion on GCP: a Pub/Sub topic telemetry, a Cloud Run HTTP push endpoint processes messages, validates JSON schema, and writes to BigQuery telemetry.events using streaming inserts with insertId = event_id to guarantee idempotency. Include IAM bindings, retry strategy, and a basic test plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Airbnb","Amazon","Databricks"]},{"id":"q-2643","question":"Design a beginner-friendly ingestion pipeline on GCP for mobile app analytics. When a JSONL file lands in Cloud Storage (analytics-logs), a Cloud Function validates each event, de-duplicates by event_id, and appends to a date-partitioned BigQuery table analytics.events. Use insertId for idempotency. Minimal IAM, exponential backoff retries, and a basic test plan with sample files and duplicates?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["Bloomberg","DoorDash"]},{"id":"q-2667","question":"Design a real-time telemetry pipeline on Google Cloud Platform for a multi-region ride-hailing fleet that ingests vehicle events from Pub/Sub, deduplicates by event_id, performs streaming enrichment and 1-minute window aggregations in Dataflow, and writes to a region-partitioned BigQuery table with CMEK. Include raw archival in Cloud Storage, cross-region failover, IAM, and a test plan?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-cloud-engineer"],"companies":["Databricks","Two Sigma","Uber"]},{"id":"q-904","question":"How would you configure a Cloud Run (fully managed) service to securely connect to a Cloud SQL PostgreSQL instance using a private connection, including IAM bindings and deployment steps to ensure the app talks via the Cloud SQL socket and never uses the instance's public IP?","channel":"gcp-cloud-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-cloud-engineer"],"companies":["OpenAI","Slack","Snap"]},{"id":"q-1222","question":"Design a real-time ad-click analytics pipeline in GCP that ingests Pub/Sub events via Dataflow into BigQuery, while implementing 90-day TTL on PII data, exporting audits to Cloud Storage, and handling schema evolution, late data, dedup, and rollback. Provide concrete architecture, data models, and operational steps?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Cloudflare","NVIDIA","Snap"]},{"id":"q-1310","question":"You operate a streaming ingestion pipeline: Pub/Sub -> Dataflow -> BigQuery. Daily 50k events with fields user_id, event_type, amount (which can be string). Propose a simple ingestion-time data quality plan that validates JSON schema, coerces types, and routes invalid records to a dead-letter Pub/Sub topic without stopping the pipeline. Include testing with a small sample and how you monitor invalid counts?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Lyft","PayPal","Snap"]},{"id":"q-1319","question":"Design a compliant streaming data platform in GCP for a fintech app that ingests transaction events from Pub/Sub through Dataflow into BigQuery. New fields may appear over time; PII must be detected and masked before analytics, while raw data is retained with restricted access. Outline the end-to-end architecture, data models, schema evolution strategy, PII handling, lineage, retention, and operational monitoring with concrete steps and trade-offs?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Bloomberg","Plaid","Robinhood"]},{"id":"q-1389","question":"You receive streaming JSON events from Pub/Sub. Design a beginner-friendly GCP pipeline using Dataflow (Beam) to parse/flatten events, write to a partitioned BigQuery table (partition by event_date), and deduplicate by event_id. Late data allowed up to 6 hours. Outline architecture, data model, schema-change handling, and monitoring?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["MongoDB","Snap","Square"]},{"id":"q-1415","question":"You are building a multi-source data platform where raw data arrives as Parquet in Google Cloud Storage, Avro via Pub/Sub, and JSON through a partner API; design a two-stage pipeline that normalizes all formats into a canonical Parquet dataset, ingested into a partitioned BigQuery table, with schema drift handling, data validation, and end-to-end lineage. Include how you would implement idempotent ingests, rollback, and cross-team access controls?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Google","Snowflake","Uber"]},{"id":"q-1449","question":"Design a streaming enrichment pipeline: Pub/Sub → Dataflow → BigQuery, with an inline governance layer enforcing per-record rules (required fields, value ranges, cross-field consistency) at ingestion. Records failing rules go to a quarantine Cloud Storage bucket; good records load to a partitioned BigQuery table. Describe data model, rule engine approach, idempotence, backpressure, and testing/monitoring plan?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Apple","Instacart","Uber"]},{"id":"q-1473","question":"In a beginner friendly GCP data pipeline Pub/Sub -> Dataflow -> BigQuery design robust observability and recovery. Describe how to instrument Dataflow with metrics backlog throughput processed failed, implement a dead letter policy that dumps failed JSON messages to Cloud Storage, set retry backoff, and outline a replay path to re ing est from Cloud Storage. Include a concrete alert rule backlog throughput greater than 0.2 for 5 minutes and a brief example snippet?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Goldman Sachs","IBM"]},{"id":"q-1546","question":"In GCP, ingest daily Pub/Sub JSON events through Dataflow into BigQuery. Implement basic observability and reliability: 1) emit metrics for ingested, processed, and failed events; 2) export metrics to Cloud Monitoring and alert on failures for three consecutive checks; 3) deduplicate by event_id; 4) log audits to Cloud Storage. Outline architecture, data model, and concrete steps?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Coinbase","Microsoft","Salesforce"]},{"id":"q-1581","question":"In Pub/Sub, daily JSON events contain PHI fields. Design a beginner-friendly GCP pipeline using Dataflow (Beam) to redact PHI with Cloud DLP before loading into BigQuery, and emit a separate audit log to Cloud Storage. Outline architecture, data model, and validation steps, including how you’d monitor masking failures?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Amazon","Google","Uber"]},{"id":"q-1596","question":"Design a streaming pipeline: Pub/Sub → Dataflow (Beam) → BigQuery. Core: a dynamic data quality gate using a Firestore-stored policy engine that validates fields/types at ingest; invalid records serialized to Cloud Storage as JSONL with metadata; valid records load to BigQuery; publish quality metrics to Cloud Monitoring; handle drift/backpressure without downtime. Explain architecture, data model, and operational steps?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Cloudflare","Hashicorp","Scale Ai"]},{"id":"q-1722","question":"Design a cross-region, real-time analytics pipeline for a fintech on GCP. Ingest 100 GB/day of JSON events from Pub/Sub into BigQuery while ensuring per-tenant data isolation, a 90-day TTL on PII, and a rollback path. Show the architecture, data models, and operational steps for schema evolution, late data, dedup, and cost control?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Apple","Oracle","Tesla"]},{"id":"q-1754","question":"Design a beginner-friendly GCP pipeline to load daily 2-5 GB of newline-delimited JSON logs from Cloud Storage into BigQuery. Keep only the latest 45 days in a partitioned table (partition by load_date, clustered by record_id). Ensure dedup by record_id, handle optional new fields with a permissive schema, allow late data up to 24 hours, and provide basic monitoring. Choose between Dataflow (Beam) or Cloud Functions, justify, and outline data model, transformations, error handling, and testing strategies?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Hugging Face","Lyft","Oracle"]},{"id":"q-1787","question":"Design a data quality framework for a streaming pipeline ingesting 500 GB/day of JSON events from Pub/Sub into BigQuery. Specify in-flight schema validation, per-field checks (types, nullability, ranges), quarantining bad records, and automatic schema evolution with Data Catalog, plus end-to-end monitoring, alerting, and rollback procedures?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Apple","Goldman Sachs","PayPal"]},{"id":"q-1834","question":"Design a cross-tenant data governance pipeline on GCP for 1 TB/day of JSON user activity ingested from Pub/Sub into BigQuery in two regions. Enforce per-tenant privacy with field-level masking, auto-generate end-to-end data lineage with Data Catalog, support backward-compatible schema evolution, and export immutable audit logs to Cloud Storage. Include architecture, data models, rollback plan per-tenant (within 24h), and testing strategy?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Apple","Tesla"]},{"id":"q-1869","question":"Design a GCP streaming pipeline for 2 TB/day of multi-tenant JSON events from Pub/Sub into BigQuery. Create a schema registry in Cloud Storage, validate with a Beam DoFn, quarantine nonconforming records to an invalid dataset, and apply per-tenant DLP masking. Enforce row-level access with BigQuery policies, capture lineage in Data Catalog, and provide rollback and testing plan?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Goldman Sachs","Snap","Uber"]},{"id":"q-1906","question":"Ingest a daily batch of JSON records stored in Cloud Storage into BigQuery via a Dataflow (Beam) job. Build a beginner-friendly pipeline that validates a minimal schema (tenant_id, event_id, event_ts), filters out records missing required fields, and deduplicates by (tenant_id, event_id). Load valid data into a partitioned BigQuery table; write invalid records to a separate GCS errors bucket. Include basic health counters and a simple template?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Amazon","Twitter"]},{"id":"q-1953","question":"Design a GCP data pipeline for a multinational SaaS product that ingests 200 GB/day of JSON telemetry from Pub/Sub into BigQuery across two regions. Requirements: multi-tenant isolation, automatic schema evolution, per-tenant TTL, data quality checks (schema conformance, nulls, duplicates), and an automated rollback path for schema changes. Include data model sketches, partitioning strategy, testing, and rollback steps?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Apple","Hashicorp","NVIDIA"]},{"id":"q-1997","question":"Ingest daily 1 TB JSON exports dumped into Cloud Storage by partner apps. Design a beginner-friendly GCP batch pipeline to load into BigQuery with per-tenant isolation (datasets), ingestion-date partitioning, and a simple schema-evolution strategy (new fields added as nullable columns). Include data-quality checks (nulls, types) and a rollback path to revert a day’s ingest within 24h. Outline architecture and testing?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Databricks","Meta","Uber"]},{"id":"q-2073","question":"On GCP, design an observability-driven pipeline that detects schema drift and data-quality anomalies in near real-time for 1 TB/day of JSON events ingested from Pub/Sub into BigQuery across two regions. Include per-tenant lineage, automatic schema evolution, drift-triggered alerts, a self-healing rollback path, and export of audit trails to Cloud Storage. Provide architecture, data models, tests, and operational playbooks?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Anthropic","Databricks","Google"]},{"id":"q-2153","question":"Design a cost-aware multi-tenant GCP data pipeline for streaming telemetry that ingests 150 GB/day of JSON from Pub/Sub into BigQuery across two regions. Enforce per-tenant access with BigQuery row level security and per-tenant dataset versioning; support backward compatible schema evolution; and export versioned snapshots to Cloud Storage. Include data models, partitioning, testing, rollback, and synthetic-tenant generation?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["NVIDIA","Oracle","Snap"]},{"id":"q-2227","question":"Design a real-time data ingestion pipeline on GCP for a game analytics platform: 60 GB/day of JSON events from Pub/Sub into BigQuery across two regions. Enforce per-tenant isolation with field-level masking, automatic schema evolution, and a per-tenant canary rollout with a controlled rollback path. Include data models, testing strategy, lineage via Data Catalog, and cost controls. Provide a concrete implementation plan?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Discord","Meta","OpenAI"]},{"id":"q-2364","question":"Design a cross-tenant data exchange on GCP: ingest 1 TB/day of JSON telemetry from Pub/Sub into two BigQuery regions. Implement per-tenant isolation via dataset-level access controls and masked views, and use Data Catalog policy tags to enforce visibility. Model a pragmatic two-layer architecture: raw + curated; ensure end-to-end lineage; support controlled data sharing via explicit contracts. Provide a 24h per-tenant rollback plan and testing strategy?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Google","NVIDIA","Zoom"]},{"id":"q-2410","question":"Design an end-to-end GCP data ingestion pipeline for 1-2 TB/day of JSON telemetry streamed from Pub/Sub into BigQuery with per-tenant isolation and strict data residency policies. The solution must mask PII per tenant at ingestion (field-level), use CMEK, and employ DLP where needed. Provide two-region architecture, automatic schema evolution, late data handling, and a rollback/backfill plan, plus end-to-end data lineage in Data Catalog and audit exports. Include architecture, data model, testing, and runbook details?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-data-engineer"],"companies":["Bloomberg","Hugging Face","Tesla"]},{"id":"q-2523","question":"On GCP, design a data mesh for 3 domains (Sales, Product, Ops) where each domain owns its BigQuery data products, publishes a standard schema, and uses Data Catalog and IAM for cross-domain governance. Include an automated lineage from source events to data products, a policy-driven access layer with per-domain permissions, and a testing/rollback plan for schema drift. Provide concrete data models and workflow?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Netflix","Snowflake","Two Sigma"]},{"id":"q-2602","question":"Design a multi-tenant, streaming data platform on GCP for a financial app that ingests 1 TB/day of JSON events via Pub/Sub, processes them with Apache Beam on Dataflow, and writes per-tenant data products to BigQuery across three regions. Explain per-tenant isolation, automatic schema evolution, late data handling, data quality checks, rollback paths, cross-region replication, Data Catalog lineage, and testing strategy?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Microsoft","Robinhood","Stripe"]},{"id":"q-931","question":"A GCP pipeline ingests 1 TB of JSON user activity daily from Pub/Sub into BigQuery via Dataflow. New fields appear over time; you must evolve the schema without downtime. What approach would you use for schema drift and nested fields, and outline concrete steps to implement it?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-data-engineer"],"companies":["Citadel","Discord","Uber"]},{"id":"q-962","question":"In a GCP streaming pipeline, Pub/Sub feeds millions of events into BigQuery via Dataflow. Out-of-order arrivals and duplicates occur. Design an idempotent sink using a staging table and a final partitioned table, leveraging insertId for dedup and a MERGE strategy to upsert into the final table. Outline concrete steps and trade-offs to implement?","channel":"gcp-data-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-data-engineer"],"companies":["Databricks","Google","Instacart"]},{"id":"q-1013","question":"You're managing a multi-tenant SaaS on GCP across five projects connected via Shared VPC. You must enforce per-tenant network isolation, IAM conditions, and budget governance while keeping CI/CD simple. Propose an end-to-end setup using Shared VPC, IAM Conditions, VPC Service Controls, Billing Budgets, and Cloud Asset Inventory, and outline testing and rollback steps?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Discord","DoorDash","Snap"]},{"id":"q-1041","question":"Design a scalable, compliant log routing pipeline on GCP that collects logs from Kubernetes clusters, Cloud Run, and Cloud Functions, redacts PII, and stores in BigQuery with environment separation. Include data flow sinks, IAM and CMEK governance, failure modes and retries, and an end-to-end testing plan?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Databricks","Square"]},{"id":"q-1155","question":"You're deploying a globally distributed service on GKE across three regions, with Cloud Run and Cloud Functions used for specific workloads. Design a deployment pipeline that enforces policy-as-code, encryption at rest via CMEK, drift detection, automated rollback, and cross-region failover. Describe tooling, data planes, tests, and how you handle outages and compliance?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["Adobe","NVIDIA","Zoom"]},{"id":"q-1245","question":"You operate a global chat app with components on **GKE**, **Cloud Run**, and **Cloud Functions**. Latency spikes in one region go unnoticed in aggregated metrics. Design an end-to-end observability approach: (1) how to unify traces across runtimes, (2) how to instrument with **OpenTelemetry** and **OTLP** to a central collector, (3) how to build region-scoped dashboards and **SLO-based alerts**, and (4) how to validate during release with **canary** and **chaos testing**. Include tool choices, sample metrics, and a minimal config sketch?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Discord","Meta","Microsoft"]},{"id":"q-1281","question":"You're maintaining a Cloud Run Python service that reads an API key from Secret Manager. Implement a 30-day secret rotation using Cloud Scheduler to publish a rotation event to Pub/Sub, and enable the Cloud Run instance to fetch updated secret without a restart. Detail the IAM permissions, wiring, and a test plan to verify end-to-end rotation?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Hashicorp","Plaid","Robinhood"]},{"id":"q-1514","question":"Design a cost-aware DR plan for a multi-region GKE + Cloud Run service handling 2M events/min. implement active-active with regional load balancers, Istio-based traffic shifting, CMEK, and a VPC Service Controls perimeter. automate failover tests via Cloud Build + Chaos Mesh; define SLOs and error budgets, observability, and automatic rollback triggers on latency/error breaches?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["Airbnb","Hugging Face","Snap"]},{"id":"q-1664","question":"You’re tasked with enabling per-branch ephemeral environments in Google Cloud Platform for a microservice. Every feature branch should provision an isolated Cloud Run service, a dedicated Cloud SQL instance, and Secrets Manager entries, with least-privilege IAM, a per-branch namespace, and automatic teardown after 48 hours. Outline the exact steps using only GCP-native tools (no external CI), including budgets alerts, health checks, and teardown workflow?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Discord","MongoDB"]},{"id":"q-1685","question":"You’re tasked with cost-aware deployment for a Cloud Run service used by a mobile frontend. Create a policy that (1) halts new deployments when the monthly spend exceeds a threshold, (2) scales traffic to 20% during overages, and (3) posts a Slack alert via a Cloud Function if the bill crosses the threshold. Describe exact steps using only GCP-native tools (Billing Budgets, Cloud Monitoring, Cloud Functions) and outline a minimal end-to-end workflow?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["LinkedIn","Lyft"]},{"id":"q-1763","question":"You operate a large-scale IoT ingestion pipeline on GCP: millions of device events per minute flow from Pub/Sub into Dataflow (Apache Beam) and write to BigQuery. Design an architecture that guarantees near real-time processing with exactly-once semantics, handles late data, and supports schema evolution; include windowing, idempotent writes, backpressure, and a rollback/testing plan for Dataflow job updates. What would you implement and why?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Amazon","Google"]},{"id":"q-1819","question":"You manage a Cloud SQL PostgreSQL instance in GCP and need a reliable disaster-recovery workflow with automated daily backups to Cloud Storage. Explain how to implement this using Cloud Scheduler and a Cloud Function that calls the Cloud SQL Admin API to export to gs://my-backups/sql-dump-YYYYMMDD.sql. Include required IAM roles, bucket lifecycle policies, and how you would validate a restore?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Instacart","Snap","Snowflake"]},{"id":"q-1890","question":"In a multi-tenant SaaS on GCP requiring isolated per-tenant networks with a shared services hub, propose a scalable hub-and-spoke topology using Shared VPC and Private Service Connect. Include least-privilege IAM, per-tenant firewall rules, and Private Google Access. How would you implement policy-as-code, drift detection, automated rollback, and observability across tenants?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Discord","Microsoft"]},{"id":"q-1917","question":"Scenario: A Cloud Run service connects to a Cloud SQL instance. Secrets are stored in Cloud Secret Manager and injected into the container at runtime. Propose a beginner-friendly, low-risk, weekly password rotation workflow that updates the secret, triggers a rolling update with zero downtime, and provides an immutable audit trail. Include the exact GCP services you would use and a minimal 3-step sequence to implement?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Amazon","Databricks"]},{"id":"q-2006","question":"You're delivering a multi-tenant SaaS on GKE and Cloud Run with strict data residency and cost controls. Design an end-to-end pattern using only GCP-native tools to achieve per-tenant isolation, regional data stores, Canary/blue-green deployment, CMEK, VPC Service Controls, and automated rollback on degraded telemetry. Outline architecture, steps, and essential config snippets?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["Apple","PayPal","Tesla"]},{"id":"q-2054","question":"Design a beginner-friendly, GCP-native CI/CD for a Cloud Run service with a private artifact repository. Describe how you would: (a) configure Cloud Build to pull a private repo and push a container image to Artifact Registry, (b) inject a database password from Secret Manager into the Cloud Run container at runtime, and (c) rotate that secret monthly using Cloud Scheduler and a Cloud Function that updates Secret Manager. Include the specific services and minimal steps?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Amazon","Google"]},{"id":"q-2234","question":"Scenario: deploy a Cloud Run service private to a specific VPC using Private Service Connect. Detail the Google-native steps: create a Serverless VPC Access connector, set up a PSC endpoint, configure private DNS, restrict ingress to internal, grant a dedicated service account run.invoker, and validate access from a VM inside the VPC while public access is blocked. What is your minimal rollout plan and validation approach?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Apple","Citadel","Instacart"]},{"id":"q-2252","question":"How would you implement a secure, GCP-native deployment pipeline for a Cloud Run service that uses vulnerability scanning and Binary Authorization to ensure only signed, non-vulnerable images are deployed, including how you would integrate Cloud Build attestation and a guard policy?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Discord","Scale Ai"]},{"id":"q-2332","question":"Scenario: you must implement a real-time pricing and fraud-detection service on GCP that ingests from Pub/Sub, processes with Dataflow, and stores results in BigQuery across multiple regions. Design a production-ready pipeline with canary releases, strict IAM, CMEK, and automated recovery. Include deployment strategy, observability, and teardown plan?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["Airbnb","Plaid"]},{"id":"q-2347","question":"You're running a stateful service on GKE with a Cloud SQL primary in US-CENTRAL1 and a read replica in EUROPE-WEST1. Design an end-to-end deployment and DR strategy that (1) supports blue/green or canary rollouts with automatic rollback based on latency and error-rate gates, (2) enables cross-region failover for compute and DB, (3) enforces CMEK for storage and DB, (4) implements policy-as-code and drift detection, (5) includes observability, outage handling, and compliance. Specify tooling, data planes, tests, and failure handling?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Goldman Sachs","Google","Instacart"]},{"id":"q-2383","question":"You're deploying a new Cloud Run service named text-classifier with a v2 revision. You want a 15% canary for 30 minutes and automatic rollback if latency exceeds 1.5s or error rate exceeds 0.5%. Using only GCP-native tools, specify exact steps to build/push v2 to Artifact Registry, split traffic, configure monitoring alerts, and trigger rollback via traffic changes, including minimal IAM requirements?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Discord","Hugging Face","Two Sigma"]},{"id":"q-2398","question":"You're deploying a beginner-friendly Cloud Run service named text-processor. Using only GCP-native tools, design a minimal CI/CD: build and push the container to Artifact Registry, deploy v1 to Cloud Run, implement a simple traffic split (10% dev, 90% prod) for canary testing, require a manual gate before production via an additional Cloud Build trigger, and outline a rollback plan and minimal IAM?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Apple","Meta","NVIDIA"]},{"id":"q-2480","question":"You're running a high-traffic payment API **payline** on Cloud Run (v2) with multi-region active-active behind a Global HTTP(S) Load Balancer. Design a production deployment plan to roll out v3 with automatic rollback based on latency and error-rate metrics. Use only Google Cloud native tools. Specify: canary strategy (**5% for 15 minutes**), traffic-split commands, monitoring alerts, CMEK/Secrets Manager handling, IAM least privilege, and a rollback workflow across regions?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["MongoDB","Stripe"]},{"id":"q-2642","question":"You're designing a streaming ingestion pipeline on GCP: Pub/Sub topic events.raw, Dataflow job events-ingest-v2 (streaming) writing to BigQuery.events_all. Using only Google-native tools, specify a concrete plan to meet: (1) exactly-once processing semantics; (2) 25% traffic canary for v2 with safe rollback; (3) malformed events go to a Dead-Letter topic; (4) automatic rollback to v1 if latency > 1.8s or error rate > 0.8% persists for 10 minutes; include monitoring, alerting, and a traffic-switching workflow that minimizes data loss?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["OpenAI","Snowflake"]},{"id":"q-2677","question":"You're maintaining a small Cloud Run (fully managed) service named image-processor with a Git-driven CI; you want to enable per-branch deployments to isolated Cloud Run services (e.g., image-processor-dev, image-processor-staging, image-processor-prod) using only GCP-native tools. Outline concrete steps to build and push the container to Artifact Registry, deploy each environment with dedicated service accounts and IAM bindings, configure traffic routing so dev/staging canaries don't affect prod, implement a gating flow to promote from dev to staging to prod via Cloud Build triggers, and specify a rollback workflow if latency > 2s or error rate > 1% persists for 10 minutes. Include minimal Secrets Manager usage and monitoring setup?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-devops-engineer"],"companies":["Google","Snap","Snowflake"]},{"id":"q-877","question":"Design a cross-region disaster recovery plan for a streaming data pipeline on GCP (Pub/Sub, Dataflow, BigQuery) that must survive a regional outage with RTO < 15 minutes and RPO < 5 minutes. The primary region is us-central1; second region is us-east1. Include data paths, failover triggers, data integrity guarantees, and operational testing steps?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-devops-engineer"],"companies":["Amazon","Apple","Scale Ai"]},{"id":"q-921","question":"In a multi-tenant GKE deployment across two GCP projects, you must enforce strict per-tenant network isolation and controlled egress to external services. Design a scalable architecture using Shared VPC, Private Service Connect, and per-tenant firewall policies to ensure tenants only reach whitelisted external endpoints, while preventing cross-tenant access. Include identity management, auditing, drift control, and operational notes for adding new tenants?","channel":"gcp-devops-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-devops-engineer"],"companies":["Amazon","Tesla"]},{"id":"gcp-ml-engineer-data-prep-1768249406549-2","question":"When tracking experiments and model lineage across teams, which combination provides end-to-end provenance and reproducibility?","channel":"gcp-ml-engineer","subChannel":"data-prep","difficulty":"intermediate","tags":["Vertex AI","Metadata","Data Catalog","Experiment Tracking","GKE","Terraform","certification-mcq","domain-weight-16"],"companies":null},{"id":"q-1008","question":"You're building a real-time customer-review sentiment classifier on GCP. Design a beginner-friendly end-to-end pipeline using Vertex AI for training and hosting, Vertex AI Feature Store for online features, Dataflow for ETL, and Pub/Sub for ingestion. Describe data flow, feature materialization cadence, a canary rollout strategy, and basic drift monitoring with rollback triggers. Include cost considerations?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Oracle","Snowflake","Two Sigma"]},{"id":"q-1199","question":"Design a multi-tenant, privacy-preserving online inference and feature materialization pipeline on GCP for a cross-region ride-hailing platform. Each tenant has its own feature schema and data residency needs. Outline how you would manage per-tenant Feature Store namespaces, Canary deployments across tenants, live vs. batch feature materialization, drift/bias monitoring, provenance, and automated rollback with Vertex AI Endpoints, Dataflow, and Pub/Sub. Include concrete rollback criteria and cost considerations?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Amazon","Lyft"]},{"id":"q-1225","question":"Design a beginner-friendly end-to-end GCP pipeline for a price-optimization model. Use Vertex AI for training and hosting, Vertex AI Feature Store for online/offline features, Dataflow for ETL into BigQuery, and Pub/Sub for ingestion. Describe data flow, feature derivation cadence, training trigger cadence, online/offline feature consistency, and a simple rollback strategy if offline metrics degrade. Include a basic cost plan?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Cloudflare","PayPal"]},{"id":"q-1438","question":"Design a production pipeline for a multi-tenant, real-time pricing model on GCP that isolates tenant data, supports per-tenant feature store versions, and enables tenant-scoped A/B testing. Use **Vertex AI**, **Feature Store**, **Pub/Sub**, and **Dataflow** to ingest events, materialize features, serve online predictions, and drive canary rollouts. Include tenancy isolation strategies, encryption at rest and in transit, drift monitoring, and cost-visibility dashboards across tenants?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Citadel","PayPal","Robinhood"]},{"id":"q-1462","question":"You're building a multi-tenant ML platform on GCP where each business unit requires isolated feature stores, per-tenant data locality, and separate budgets. Describe how you'd implement tenant isolation in Vertex AI Feature Store, manage per-tenant data lineage, and enable per-tenant canary model rollouts with drift checks and automated rollback. Include a concrete data path and cost controls?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Amazon","Databricks","Plaid"]},{"id":"q-1557","question":"Design a beginner-friendly end-to-end GCP pipeline for a real-time product-recommendation score using Vertex AI, Dataflow, Pub/Sub, and Vertex AI Feature Store. Include: 1) data validation and schema drift checks at ingestion, 2) per-customer feature isolation via IAM/VPC, 3) online feature materialization cadence and low-latency serving, 4) canary rollout strategy and rollback triggers, 5) practical cost-management tips?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Amazon","NVIDIA","Uber"]},{"id":"q-1694","question":"Design a beginner-friendly GCP ML pipeline for daily demand forecasting: Pub/Sub ingest, Dataflow ETL into BigQuery, Vertex AI training, and a Vertex AI online endpoint. Focus on observability: specify minimal metrics, dashboards, alerts for data drift and latency, and a safe rollback workflow that reverts to a previous model version when drift is detected. Include rough cost notes?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Apple","Square"]},{"id":"q-1799","question":"You run a real-time product risk scoring service on GCP with 50k QPS and 20 ms P95 latency, deployed in NA and EU. Design an end-to-end pipeline using Pub/Sub, Dataflow, Vertex AI, and BigQuery that enforces regional data residency, materializes features per region, serves online predictions with per-request explainability, and supports drift-driven rollback and cost controls. Outline architecture, data flow, and escalation criteria?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Adobe","DoorDash"]},{"id":"q-1837","question":"Design a beginner-friendly GCP ML pipeline to classify customer tickets with privacy in mind: Pub/Sub streams tickets, Dataflow applies DLP redaction and writes to BigQuery, Vertex AI trains a text classifier weekly, deploys a canary Vertex AI online endpoint, and uses drift metrics with alerts. If drift is detected, route traffic to the previous model version; include rough cost notes?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Bloomberg","Cloudflare"]},{"id":"q-1871","question":"Design an end-to-end, privacy-preserving multi-tenant ML pipeline on GCP that isolates customer data, uses Vertex AI for training and hosting, Dataflow for ETL, Pub/Sub for ingestion, and Data Catalog for lineage. Include differential privacy options, KMS-based key management, access controls, audit logging, and a rollback strategy for drift or privacy policy violations. Be concrete about components and data paths?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Anthropic","IBM","Stripe"]},{"id":"q-1940","question":"In a geo-distributed personalization pipeline on GCP, design a geo-canary rollout for a real-time ranking model across regions. Outline end-to-end usage of Vertex AI, Feature Store, Pub/Sub, and Dataflow with online/offline feature separation, drift monitoring, canary criteria, automatic rollback, and per-region cost controls?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Google","LinkedIn","Microsoft"]},{"id":"q-1966","question":"Design a region-aware, real-time update workflow for a multilingual product-support bot on GCP. Ingest user feedback via Pub/Sub; route to per-region Feature Store with Dataflow; train a multilingual NLU model in Vertex AI; deploy per-region canaries with automatic rollback; and implement drift alerts plus strict data residency controls and region-based cost caps?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Apple","Discord"]},{"id":"q-1973","question":"Design a beginner-friendly GCP ML pipeline to moderate user-uploaded product images in a marketplace. Ingest image events via Pub/Sub, Dataflow resizes and extracts safe metadata, stores references in BigQuery; Vertex AI trains a basic image classifier weekly using stored images, deploys an online endpoint with a canary rollout, and monitors drift per-tenant isolation. Include privacy safeguards and rough cost range?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Coinbase","Instacart","Twitter"]},{"id":"q-2020","question":"Design a multi-tenant, region-isolated content ranking system on GCP where each tenant enforces data residency in their region and supports per-tenant feature flags. Build with Vertex AI for model hosting, Vertex Feature Store for per-tenant features, Pub/Sub and Dataflow for streaming feature updates, and BigQuery for offline features. Describe tenant isolation, canary rollouts by tenant, drift detection thresholds, and rollback criteria with minimal impact?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Anthropic","LinkedIn","Uber"]},{"id":"q-2210","question":"Design a geo/tenant-isolated inference pipeline on GCP for a multi-tenant ranking model: each tenant has isolated Feature Store namespaces and a model registry; explain how you would structure Pub/Sub, Dataflow, Feature Store, and Vertex AI to support online/offline features, per-tenant drift monitoring, automatic rollback, and per-tenant cost controls across regions?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Bloomberg","Microsoft","Salesforce"]},{"id":"q-2226","question":"You’re designing a beginner-friendly GCP ML pipeline for a churn classifier with an emphasis on reproducibility and simple drift control, avoiding canaries. Outline data ingestion, dataset versioning, training, evaluation, and a rollback plan using Vertex AI, BigQuery, and Cloud Storage. Include a concrete example of a versioning strategy and a drift-threshold trigger?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Citadel","Netflix","Tesla"]},{"id":"q-2349","question":"You’re building a real-time recommendation model on Google Cloud for a multi-tenant SaaS product where tenants span regulated industries with data residency constraints. Design an end-to-end pipeline using Vertex AI for training and serving, BigQuery for per-tenant datasets, and Feature Store with per-tenant namespaces. Include tenancy-aware drift detection, per-tenant rollback strategy, auditing via Cloud Audit Logs, and cost controls. Describe data flow, governance, and failure modes?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["NVIDIA","Salesforce","Two Sigma"]},{"id":"q-2488","question":"You're building a privacy-preserving, multi-tenant credit-scoring service on GCP. Design a production pipeline using Vertex AI, Feature Store, Dataflow, and BigQuery that enforces per-tenant data isolation, versioned online/offline features, real-time drift detection with tenant-level rollbacks, and data residency constraints while meeting sub-200 ms latency for online predictions?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Databricks","Microsoft"]},{"id":"q-2566","question":"Design a beginner-friendly GCP ML pipeline for a ride-hailing ETA predictor with streaming data. Ingest event data via Pub/Sub, validate and enrich in Dataflow (invalid records go to a dead-letter Pub/Sub), write clean data to BigQuery, and retrain a Vertex AI tabular model daily on the latest validated batch. Include a data-quality gate for schema evolution (new column) and a simple rollback to the previous model version if validation or drift metrics fail. Be concrete about components and thresholds?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Tesla","Two Sigma","Uber"]},{"id":"q-2608","question":"You operate an IoT anomaly-detection system across multiple factories in GCP. Ingest telemetry via Pub/Sub, ETL in Dataflow to BigQuery, train a Vertex AI custom anomaly model with per-plant features, and serve via per-plant endpoints with traffic-splitting. Design versioning for data and models, drift thresholds, a per-plant canary rollout, automated rollback, and cost controls?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Cloudflare","MongoDB","Tesla"]},{"id":"q-2713","question":"You run a geo-distributed video recommendation system on GCP with strict privacy requirements. Design an end-to-end pipeline using Vertex AI, Feature Store, Dataflow, and Pub/Sub to train and serve a real-time ranking model while enforcing per-region data residency, differential privacy for user features, and secure feature materialization. Include drift detection, automatic rollback, and cost controls?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Apple","Microsoft","Netflix"]},{"id":"q-2722","question":"Design a data-governed, per-tenant ML pipeline for a real-time ad-scoring model on GCP. Requirements: enforce data residency and policy controls via Data Catalog and IAM; isolate per-tenant Feature Store namespaces and per-tenant BigQuery datasets; train with Vertex AI Pipelines; route features with Dataflow; provide drift checks, automated rollback per tenant, and audit trails in Cloud Logging. Outline end-to-end, include a sample per-tenant versioning strategy and a rollback trigger?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["LinkedIn","Meta","Robinhood"]},{"id":"q-882","question":"You run a real-time fraud-detection model on GCP at ~25k QPS with sub-20 ms P95 latency. Design a production pipeline using Vertex AI, Feature Store, Pub/Sub, and Dataflow that ingests events, materializes features, serves online predictions, and supports canary rollouts. Include data/model drift monitoring, automated rollback, and cost considerations?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Goldman Sachs","Snap"]},{"id":"q-905","question":"You operate a multi-tenant content recommender service on GCP used by advertisers. Each tenant has its own feature schema and data retention. Design a production ML pipeline (training and online serving) using Vertex AI, Feature Store, Pub/Sub, and Dataflow that supports **per-tenant namespaces**, **policy-based access**, **drift monitoring**, **automated rollback**, and **cost isolation**. Include data leakage prevention, schema evolution, and canary rollouts across regions?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-ml-engineer"],"companies":["Scale Ai","Tesla","Twitter"]},{"id":"q-922","question":"Design a real-time content moderation inference path on GCP that adds a Redis-based in‑memory cache in front of a Vertex AI online endpoint to reduce latency and cost. Outline what you cache (embeddings vs predictions), key schema (e.g., user_id, model_version, language), TTL and eviction policy, and how you invalidate cache on model deploy or feature updates. Include data privacy considerations and a plan for monitoring latency, cache hit rate, and drift. Provide a small Python sketch of the cache lookup?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-ml-engineer"],"companies":["Airbnb","Discord","Snap"]},{"id":"q-955","question":"Design a multi-tenant ML service on GCP that serves diverse customers with strict data isolation and retention policies. Propose a deployment and feature governance pattern using Vertex AI, Feature Store, Private Service Connect, Data Catalog, and Pub/Sub to isolate customer data, manage per-tenant feature lifecycles, perform drift monitoring, and enable tenant-specific canary rollouts with automated rollback and cost controls. Include concrete components, data flow, and rollback criteria?","channel":"gcp-ml-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-ml-engineer"],"companies":["Amazon","Meta"]},{"id":"q-1176","question":"In a multi-tenant GCP security environment, design an ephemeral admin-session workflow for Kubernetes and Cloud Run resources using IAP, Workload Identity Federation, and Binary Authorization. Include policy design, auditability, rollback, and how you'd verify there are no lingering grants after revocation?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Discord","LinkedIn","Microsoft"]},{"id":"q-1194","question":"In a GCP multi-tenant fintech data lake, design end-to-end safeguards to prevent tenant data leakage when multiple tenants share datasets in BigQuery and Cloud Storage. Propose a guardrail that blocks cross-tenant access at the data-product level using IAM Conditions, per-data-product VPC Service Controls, and Private Service Connect to a shared analytics endpoint. Include testing with synthetic misconfigurations and a rollback/audit plan?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Bloomberg","Plaid","Tesla"]},{"id":"q-1325","question":"In a multi-tenant GCP data platform used by Netflix‑like partners, outline a per-session data access model for a partner analytics job using IAM Conditions, Workload Identity Federation, and Access Context Manager to grant time-bounded, least-privilege access; include revocation, auditing, and validation with synthetic data?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Adobe","Microsoft","Netflix"]},{"id":"q-1400","question":"Design a multi-tenant GCP analytics stack (**Shared VPC**, **GKE**, Dataflow, BigQuery) with strict tenant isolation. Propose concrete network/IAM boundaries (per-tenant Namespaces, **NetworkPolicy**, **Private Service Connect**, **IAM Conditions**), a policy-as-code guard (**OPA**) in CI/CD to block cross-tenant paths, and a synthetic verification + rollback plan?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Airbnb","Apple","Slack"]},{"id":"q-1478","question":"In a GCP data-logging pipeline (Pub/Sub -> Dataflow -> BigQuery) for a fintech starter, outline a concrete beginner-friendly plan to ensure data never leaks: dedicated service accounts with least privilege, CMEK on the raw-logs bucket, IAM Conditions restricting access by time and principal, PII masking in outputs, and a CI test that injects synthetic logs to verify redaction and auditability. How would you validate in production?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Goldman Sachs","NVIDIA"]},{"id":"q-1505","question":"Design a secure, auditable cross‑organization data sharing pipeline in GCP: a data lake (Cloud Storage + BigQuery) holds PII-derived features; an external analytics partner connects via Private Service Connect to a synthetic dataset exposed for ad-hoc analysis while no raw PII leaves; specify ACM IAM Conditions per-tenant, PSC endpoints in a Shared VPC, CMEK for both stores, DLP masking prior to export, and automated revocation tests with synthetic data and rollback steps?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["NVIDIA","Scale Ai","Snowflake"]},{"id":"q-1587","question":"In a live GCP analytics stack (Pub/Sub → Dataflow → BigQuery) shared with external partners, you detect a suspected compromise of a Dataflow worker service account. Describe a concrete incident response playbook: containment (disable keys, rotate CMEKs), evidence preservation (export Cloud Audit Logs), access revocation with IAM Conditions, network containment (VPC Service Controls), and a post‑mortem with hardened controls. Include production validation steps?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Databricks","Tesla"]},{"id":"q-1719","question":"In a GCP multi-tenant data lake (BigQuery, Cloud Storage, Dataflow) used by three partners, design a crypto-agile CMEK rotation plan with zero downtime. Detail per-tenant key rings, IAM Conditions, and automatic key versioning; ensure data plane can switch keys without reprocessing. Include Access Context Manager, VPC Service Controls (PSC), DLP masking, and an automated attestations/rollback workflow with synthetic data?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Amazon","Google"]},{"id":"q-1736","question":"A GCP project hosts a Cloud Run API and a BigQuery warehouse. A contractor needs 2 weeks of read-only access to a small BI dataset, without touching production data. Draft a beginner-friendly plan: dedicated read-only service account, a dataset view for masking, IAM Bindings with an IAM Condition for a 14-day window, no public endpoints, and a test plan using a synthetic dataset and audit logs to validate revocation. How would you implement this?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Microsoft","PayPal","Robinhood"]},{"id":"q-1813","question":"In a GCP data science workspace using Vertex AI Workbench and BigQuery, draft a beginner-friendly plan to guarantee per-tenant data isolation across multiple tenants. Include: 1) per-tenant IAM roles/service accounts, 2) dataset-level access controls, 3) network borders via VPC Service Controls or Private Service Connect, 4) a CI test that injects synthetic data and validates isolation, and a safe rollback if misconfig is detected?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Hugging Face","Salesforce","Slack"]},{"id":"q-1830","question":"In a GCP-based security control plane for a fintech platform used by Robinhood and Zoom, external CI/CD pipelines must deploy and validate security baselines without long‑lived credentials. Design a Workload Identity Federation solution that maps external OIDC identities to short‑lived GCP service accounts, enforcing least privilege with IAM Conditions. Include per‑tenant isolation, auditability, token lifetimes, revocation, and automated drift/rollback tests in the CI pipeline?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Robinhood","Zoom"]},{"id":"q-1854","question":"In a GCP analytics platform with BigQuery, Dataflow, and Data Catalog used by multiple tenants, design a per-tenant row-level security model using BigQuery Row Access Policies tied to TenantID, and IAM Conditions for dataset access, complemented by Data Catalog tags and per-tenant CMEK. Include ingestion, query-time enforcement, testing with synthetic tenants, rollback, and auditability?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Google","Meta","Uber"]},{"id":"q-1986","question":"In a GCP-based data science platform shared by three tenants, design a practical beginner-friendly security baseline to prevent cross-tenant access during notebook runs, data prep, and model training. Include: 1) per-tenant Secrets Manager keys with rotation; 2) dataset/bucket access controls and per-tenant IAM/service accounts; 3) a VPC Service Controls perimeter around processing endpoints; 4) a CI check that injects synthetic data and validates isolation; 5) a rollback plan if misconfig detected?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"beginner","tags":["gcp-security-engineer"],"companies":["Anthropic","Databricks","Two Sigma"]},{"id":"q-2002","question":"In a real-time GCP data lake (Pub/Sub -> Dataflow -> BigQuery -> GCS) shared across three brands, design an automated incident containment scenario: when a service account shows anomalous access patterns, how would you instantly restrict access, rotate keys, and quarantine the project while preserving pipelines? Include exact IAM bindings, PSC/VPC controls, CMEK strategies, and rollback tests?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Instacart","Tesla","Twitter"]},{"id":"q-2026","question":"In a shared GCP data lake (Pub/Sub → Dataflow → BigQuery) used by Snowflake, Netflix, and Apple, design a per-brand data isolation and crypto agility strategy. Specify per-brand CMEK key rings, IAM Conditions to enforce least privilege, per-brand VPC Service Controls perimeters, and an automated key rotation and rollback plan with end-to-end tests for authorization, auditing, and data access paths?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Apple","Netflix","Snowflake"]},{"id":"q-2055","question":"In a GCP multi-tenant streaming lake (Pub/Sub -> Dataflow -> BigQuery) shared by three brands, a service account is suspected of exfiltrating data. Design an automated, tenant-aware containment plan that isolates the compromised tenant without interrupting others: 1) tenant-scoped IAM bindings with conditional denies, 2) per-tenant VPC Service Controls and Private Service Connect paths, 3) CMEK-driven key rotation with automatic rotation triggers, 4) pipeline cutover and rollback procedures, 5) synthetic-data canaries and automated validation before full failover?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Apple","Instacart","Zoom"]},{"id":"q-2104","question":"Design a cryptographically verifiable data provenance system for a real-time GCP data lake (Pub/Sub -> Dataflow -> BigQuery -> GCS) shared by three brands. How would you generate per-record provenance tokens, sign them with Cloud KMS, attach them to the data, rotate keys, and verify integrity at read time while preserving pipeline throughput? Include how you'd store proofs, enforce tenant isolation, and rollback tests?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Anthropic","MongoDB","Plaid"]},{"id":"q-2182","question":"In a global GCP security setup hosting a real-time data lake (Pub/Sub -> Dataflow -> BigQuery -> GCS) shared by three brands, a service account in one project shows anomalous access to Secrets Manager and Cloud SQL. Design an automated containment workflow that immediately revokes keys, rotates CMEK-protected secrets, applies IAM Conditions or Deny policies to block access, tightens PSC/VPC Service Controls, and quarantines the affected project while preserving pipelines. Include exact bindings, key rotation steps, and rollback tests?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Apple","Meta"]},{"id":"q-2285","question":"In onboarding a new tenant to a shared GCP data lake (Pub/Sub → Dataflow → BigQuery/Cloud Storage) implement a Just-In-Time (JIT) access flow that temporarily elevates a service account for a 2-hour data load, then revokes it automatically. Include per-tenant IAM bindings with conditions, ACM access policies, PSC endpoints, CMEK management, and an auditable rollback plan?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Lyft","Netflix","Two Sigma"]},{"id":"q-2467","question":"In a GCP-native real-time feature store shared by three brands, design a per-brand access model for streaming features from Pub/Sub through Dataflow to BigQuery. Define exact IAM bindings and conditions, per-brand VPC Service Controls boundaries, Private Service Connect endpoints for feature API calls, and CMEK strategies; include rotation cadence and a rollback test plan that preserves in-flight data?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"advanced","tags":["gcp-security-engineer"],"companies":["Hugging Face","Snowflake","Two Sigma"]},{"id":"q-2628","question":"In a multi-tenant data exchange on GCP, a third-party analytics partner exports data from a shared BigQuery dataset to their project via Data Transfer Service. The dataset contains PII. Design a practical, real-time containment workflow to detect anomalous exports and automatically enforce containment: temporary IAM Conditions on export, rotate partner service account keys, reconfigure PSC/Private Service Connect to the partner, enable VPC Service Controls perimeters, apply CMEK or DLP masking, and establish a rollback/audit procedure with synthetic data checks?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Instacart","Netflix","Zoom"]},{"id":"q-2681","question":"Design a secure onboarding workflow for a shared Vertex AI inference platform used by three financial tenants; how would you implement per-tenant isolation and least privilege, federate client identities via Workload Identity Federation, enforce network egress controls with Private Service Connect and VPC Service Controls, apply CMEK at storage and model artifacts, and verify revocation with automated rollback tests?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Goldman Sachs","Google"]},{"id":"q-863","question":"In a GCP data pipeline that streams PII from Pub/Sub through Dataflow to BigQuery and Cloud Storage, outline a concrete hardening plan: exact IAM bindings with least privilege, VPC Service Controls, private access to API endpoints, encryption key management, access reviews, and monitoring/alerting. Include how you’d validate controls in production?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Adobe","Amazon","Meta"]},{"id":"q-899","question":"In a GCP data pipeline streaming PII from Pub/Sub through Dataflow to BigQuery and Cloud Storage, enable secure cross‑org sharing with an external analytics partner via Private Service Connect. Draft a concrete architecture: least‑privilege IAM bindings per data product, cross‑project scopes, PSC endpoints in a shared VPC, CMEK, DLP masking, and automated validation with synthetic data and revocation tests. End with auditable controls and rollback plan?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Slack","Zoom"]},{"id":"q-911","question":"In a GCP data pipeline that streams PII from Pub/Sub through Dataflow to BigQuery and Cloud Storage across two orgs, enable time-bounded access for an external analytics partner without static credentials. Draft a concrete design using Workload Identity Federation, IAM conditions, ephemeral credentials, Private Service Connect, and VPC Service Controls. Include trust, token lifetimes, auditing, automated revocation tests, and rollback?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Cloudflare","LinkedIn","PayPal"]},{"id":"q-945","question":"In a **Terraform-driven GCP** multi-tenant environment used by **Salesforce** and **Discord**, implement automated guardrails that block any public bucket or dataset and enforce least-privilege IAM at module boundaries. Describe how you’d implement **OPA constraints**, integrate with **CI/CD**, test with synthetic misconfigs, and provide rollback/auditability?","channel":"gcp-security-engineer","subChannel":"general","difficulty":"intermediate","tags":["gcp-security-engineer"],"companies":["Discord","Salesforce"]},{"id":"q-1970","question":"Design a LangChain-based agent that uses Autogen to plan and execute a multi-step inquiry: check stock via /inventory API, fetch ETA via /shipping API, and present a precise delivery window with caveats; implement robust error handling with retries, fallbacks, and timeouts, and show how the agent would adjust its plan if the stock check returns uncertain results?","channel":"generative-ai","subChannel":"agents","difficulty":"intermediate","tags":["langchain","autogen","tool-use","planning"],"companies":["Instacart","Square"]},{"id":"q-2596","question":"Design a beginner-friendly LangChain task: given 'Summarize the top 3 Nvidia and Apple AI news from the last 24h', build a small Autogen-driven plan that uses tools (news API, summarizer, fact-checker). Show the planning steps, how tools are chained, and how you handle tool errors and duplicates. Include a minimal code sketch to register tools and run the plan?","channel":"generative-ai","subChannel":"agents","difficulty":"beginner","tags":["langchain","autogen","tool-use","planning"],"companies":["Apple","NVIDIA","Twitter"]},{"id":"q-430","question":"How would you implement a basic AI agent using LangChain that can use tools to answer user questions about weather data?","channel":"generative-ai","subChannel":"agents","difficulty":"beginner","tags":["langchain","autogen","tool-use","planning"],"companies":["Amazon","Anthropic","Apple","Google","Microsoft","OpenAI","Snowflake"]},{"id":"q-445","question":"You're building a multi-agent system using LangChain and AutoGen for autonomous code generation. How would you design a robust tool-use framework that prevents malicious code execution while maintaining agent autonomy?","channel":"generative-ai","subChannel":"agents","difficulty":"advanced","tags":["langchain","autogen","tool-use","planning"],"companies":["Databricks","Google","Tesla"]},{"id":"q-322","question":"How would you measure and reduce hallucination in a large language model deployed for customer service?","channel":"generative-ai","subChannel":"evaluation","difficulty":"beginner","tags":["hallucination","faithfulness","relevance"],"companies":["Amazon","Google","IBM","Mckinsey","Meta","Microsoft","Salesforce"]},{"id":"q-385","question":"You're building a hallucination detection system for a production LLM service. Design a multi-layered evaluation pipeline that balances false positives/negatives while maintaining sub-100ms latency. How would you implement confidence scoring and fallback mechanisms?","channel":"generative-ai","subChannel":"evaluation","difficulty":"advanced","tags":["hallucination","faithfulness","relevance"],"companies":["Anthropic","Google","Stripe"]},{"id":"q-402","question":"How would you design a hallucination detection system for a medical AI assistant that evaluates faithfulness against verified drug databases while maintaining 99.9% accuracy?","channel":"generative-ai","subChannel":"evaluation","difficulty":"advanced","tags":["hallucination","faithfulness","relevance"],"companies":["Microsoft","Netflix","Veeva"]},{"id":"q-463","question":"How would you evaluate if an LLM's response is faithful to the provided source documents?","channel":"generative-ai","subChannel":"evaluation","difficulty":"beginner","tags":["hallucination","faithfulness","relevance"],"companies":["Cloudflare","Microsoft","PayPal"]},{"id":"q-494","question":"How would you design a comprehensive evaluation framework to detect hallucinations in a large language model deployed for customer support, considering both factual accuracy and faithfulness to provided context?","channel":"generative-ai","subChannel":"evaluation","difficulty":"advanced","tags":["hallucination","faithfulness","relevance"],"companies":["Discord","Meta","Plaid"]},{"id":"q-524","question":"How would you evaluate a generative AI model's tendency to hallucinate when answering factual questions about company policies?","channel":"generative-ai","subChannel":"evaluation","difficulty":"beginner","tags":["hallucination","faithfulness","relevance"],"companies":["Microsoft","Uber"]},{"id":"q-1673","question":"Given a 7B-class LLM, design a practical PEFT plan to add domain-specific knowledge for a banking app using LoRA/QLoRA/adapter techniques. Specify target modules, r, alpha, dropout, quantization approach (e.g., 4-bit), dataset size, batch size, learning rate, and epochs. Explain how you'd validate improvements without harming generalization and how to deploy adapters for on-device inference?","channel":"generative-ai","subChannel":"fine-tuning","difficulty":"intermediate","tags":["lora","qlora","peft","adapter"],"companies":["Apple","Plaid"]},{"id":"q-1780","question":"How would you fine-tune a 7B base model for a live chat assistant using a LoRA adapter (via QLoRA/PEFT) on a 2k-example dataset? Include modules to target, rank, and precision, data prep, training setup, and evaluation strategy. Provide a minimal code snippet to attach a LoRA adapter to a transformer layer?","channel":"generative-ai","subChannel":"fine-tuning","difficulty":"beginner","tags":["lora","qlora","peft","adapter"],"companies":["Airbnb","MongoDB","Zoom"]},{"id":"q-2224","question":"You're fine-tuning a 7B parameter LLM on 1k examples. Configure a LoRA adapter with PEFT/QLora: r=8, lora_alpha=32, lora_dropout=0.1, target_modules=['q_proj','k_proj','v_proj']; enable 4-bit quantization with bitsandbytes. Specify how you would integrate and verify that only adapter weights are trained?","channel":"generative-ai","subChannel":"fine-tuning","difficulty":"beginner","tags":["lora","qlora","peft","adapter"],"companies":["Apple","Two Sigma","Uber"]},{"id":"q-225","question":"When implementing LoRA fine-tuning for a 7B parameter LLM, how do you determine the optimal rank (r) and alpha values to balance performance and memory efficiency while maintaining model quality?","channel":"generative-ai","subChannel":"fine-tuning","difficulty":"intermediate","tags":["lora","qlora","peft","adapter"],"companies":["Amazon","Databricks","Google","Meta","Microsoft","NVIDIA"]},{"id":"q-250","question":"What is LoRA and how does it reduce parameters when fine-tuning large language models?","channel":"generative-ai","subChannel":"fine-tuning","difficulty":"beginner","tags":["lora","qlora","peft","adapter"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"q-197","question":"How would you implement efficient KV caching in a transformer decoder to reduce redundant computation during autoregressive generation?","channel":"generative-ai","subChannel":"llm-fundamentals","difficulty":"intermediate","tags":["transformer","attention","tokenization"],"companies":["Amazon","Google","Meta","Microsoft","OpenAI"]},{"id":"q-308","question":"How does the self-attention mechanism in transformers compute token relationships?","channel":"generative-ai","subChannel":"llm-fundamentals","difficulty":"intermediate","tags":["transformer","attention","tokenization"],"companies":["Amazon","Google","Meta"]},{"id":"q-371","question":"You're designing a custom tokenizer for a multilingual LLM that needs to handle code-switching between English and Chinese. How would you optimize the vocabulary to minimize token count while preserving semantic meaning, and what attention mechanism modifications would you consider?","channel":"generative-ai","subChannel":"llm-fundamentals","difficulty":"advanced","tags":["transformer","attention","tokenization"],"companies":["Meta","Microsoft","NVIDIA"]},{"id":"q-414","question":"Explain how the self-attention mechanism in a transformer works and why it's more effective than RNNs for processing long sequences?","channel":"generative-ai","subChannel":"llm-fundamentals","difficulty":"beginner","tags":["transformer","attention","tokenization"],"companies":["Amazon","Anduril","Google","Meta","Microsoft","NVIDIA","OpenAI","Tesla"]},{"id":"q-577","question":"How would you debug a transformer model where attention weights are becoming uniform across all tokens, leading to poor performance?","channel":"generative-ai","subChannel":"llm-fundamentals","difficulty":"intermediate","tags":["transformer","attention","tokenization"],"companies":["IBM","PayPal"]},{"id":"q-2246","question":"Design a retrieval-augmented QA service for internal engineering docs spanning product manuals, API specs, and code samples. Outline the end-to-end pipeline: chunking strategy, embedding models, vector DB indexing, cross-collection retrieval priors, re-ranking, versioning, provenance, latency targets, and how you'd test and monitor to minimize hallucinations?","channel":"generative-ai","subChannel":"rag","difficulty":"intermediate","tags":["retrieval","embeddings","vector-db","chunking"],"companies":["Meta","Tesla"]},{"id":"q-293","question":"How do you optimize chunking strategies for different document types in RAG systems?","channel":"generative-ai","subChannel":"rag","difficulty":"advanced","tags":["retrieval","embeddings","vector-db","chunking"],"companies":["Amazon","Google","Meta"]},{"id":"q-335","question":"You're building a RAG system for SAP's customer support. How would you chunk a 10-page technical manual to ensure relevant sections are retrieved?","channel":"generative-ai","subChannel":"rag","difficulty":"beginner","tags":["retrieval","embeddings","vector-db","chunking"],"companies":["Amazon","Google","IBM","Microsoft","MongoDB","Planetscale","Sap"]},{"id":"q-438","question":"You're building a RAG system for DoorDash's restaurant search. How would you design a hybrid retrieval strategy combining semantic and keyword search to handle queries like 'cheap Italian delivery near me' while maintaining sub-100ms latency?","channel":"generative-ai","subChannel":"rag","difficulty":"advanced","tags":["retrieval","embeddings","vector-db","chunking"],"companies":["Amazon","Apple","DoorDash","Google","Lyft","Meta","Microsoft","Netflix"]},{"id":"q-551","question":"You're building a RAG system for Discord's message search. Messages have varying lengths, code blocks, and threaded conversations. How would you design your chunking strategy and what embedding model would you choose?","channel":"generative-ai","subChannel":"rag","difficulty":"advanced","tags":["retrieval","embeddings","vector-db","chunking"],"companies":["Discord","Snowflake"]},{"id":"q-1145","question":"Design an online nonlinear ICA pipeline for a 12-mic, 6-camera live broadcast where mixing is nonlinear and time-varying due to the environment. Propose an invertible neural network demixing model, online training with forgetting, a temporal prior to capture dynamics, and strategies for permutation/scale alignment. Include evaluation plan and DSP constraints?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Adobe","Citadel","Coinbase"]},{"id":"q-1208","question":"Design a privacy-preserving, edge-based ICA pipeline to separate overlapping speech captured by a distributed 32‑mic array where raw audio never leaves devices and only anonymized components are aggregated. Describe per‑device whitening with partial channels, online demixing updates, secure aggregation methods, permutation/scale alignment across devices, latency targets, drift handling, and an evaluation plan with synthetic ground truth and transcripts?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Discord","Goldman Sachs","Instacart"]},{"id":"q-1231","question":"In a smart conference room, 6 microphones capture audio while 2 cameras provide synchronized lip-movement visuals. Propose an ICA-based pipeline to jointly separate independent audio sources and align them to speaker identities using visual cues as auxiliary information. Include (i) whitening and joint diagonalization strategy, (ii) how to integrate visual cues into the contrast to improve permutation recovery, (iii) online adaptation for moving speakers, and (iv) a concrete evaluation plan with ground-truth sources and lip-sync metrics?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["LinkedIn","Meta"]},{"id":"q-1316","question":"3-mic wearable office demo: design a beginner ICA pipeline to separate two voices captured by near-field mics in a noisy room. Outline: (i) whiten the 3 channels, (ii) apply a simple real-valued FastICA on short FFT frames to recover two sources, (iii) align permutation across frames via spatial-map correlation, (iv) include a lightweight online update to the demixing matrix for motion, (v) evaluate with synthetic ground truth (SDR/SIR) and a listening check; target <100 ms latency on a low-power CPU?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["NVIDIA","Salesforce"]},{"id":"q-1458","question":"In a 4-channel EEG headset recording a brief resting-state session, design a beginner ICA pipeline to separate neural components from ocular and EMG artifacts. Outline: whitening, choice between real-valued FastICA or Infomax, frame-wise vs block-wise ICA with permutation alignment across windows, online adaptation for impedance drift, and a practical evaluation plan using simulated ground truth sources and known event-related potentials?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Google","Salesforce"]},{"id":"q-1500","question":"In a city-scale IoT deployment for smart buildings, 128 sensors stream heterogeneous, non-stationary signals (temperature, occupancy, vibration) that mix linearly in the cloud. Design a distributed online ICA to recover independent sources in real time, handling missing channels (packet loss), nonstationary mixing, and limited inter-node communication. Include whitening, update rules, drift handling, and an evaluation plan with ground truth?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Airbnb","Databricks","Uber"]},{"id":"q-1601","question":"Intermediate: In a 6-mic conference room capturing two moving speakers with strong reverberation and nonlinear mic distortions (AGC/compression), design a real-time post-nonlinear ICA (PNL-ICA) pipeline to separate sources. Include: a y = g(Ax) model, whitening strategy, online demixing updates, choice between per-bin ICA vs joint diagonalization, handling time-varying mixing, and an evaluation plan using SDR/SIR and transcript-aligned ground truth?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["Adobe","MongoDB","Zoom"]},{"id":"q-1646","question":"Beginner ICA task: A 4-mic array in a small classroom records two overlapping speakers with room reverberation. Design and implement a simple offline ICA pipeline (FastICA) to recover the two voices. Explain preprocessing (centering, whitening), nonlinearity, choice of whitening (PCA vs ZCA), how to align permutations across time windows, and provide a basic SDR/SIR evaluation using ground-truth signals?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Goldman Sachs","Google","OpenAI"]},{"id":"q-1742","question":"With an 8-mic array where each microphone samples at a slightly different rate causing synchronization drift, design a real-time ICA pipeline to separate sources under asynchronous observations. Specify (i) pre-alignment/handling of irregular samples, (ii) whitening strategy, (iii) online demixing updates tolerant to drift, (iv) latency targets and evaluation plan (SDR/SIR) with ground-truth alignment?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Databricks","LinkedIn","Twitter"]},{"id":"q-1768","question":"Beginner ICA task: In a live street interview, a 2-mic smartphone captures two overlapping speakers with wind noise. Design a lightweight online ICA pipeline that runs under 50 ms/frame on a mobile CPU. Include: (i) 20 ms frames with 50% overlap, (ii) whitening, (iii) a simple online ICA (2×2 demixing) with a tanh nonlinearity, (iv) frame-to-frame permutation alignment via cross-frame non-Gaussianity smoothing, (v) evaluation plan with synthetic ground-truth SDR/SIR and listening checks?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Adobe","Amazon","Coinbase"]},{"id":"q-1918","question":"In a 12-mic automotive cabin with three moving talkers and non-stationary noise, design an online convolutive ICA pipeline to separate sources in real time. Specify: whitening and natural-gradient demixing, per-bin vs joint diagonalization choice, online permutation/scale alignment with temporal continuity, drift handling, and an evaluation plan with SDR/SIR and ground-truth transcripts?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Amazon","Coinbase","Lyft"]},{"id":"q-1936","question":"Real-time 4‑mic ICA in a browser: use a 32 ms sliding window at 48 kHz with 50% overlap. Whitening via incremental PCA on 4 channels to produce uncorrelated components. Learn a 4×2 demixing matrix W with online gradient on y = W x using a tanh nonlinearity; adapt the learning rate to keep total latency under 60 ms. Resolve permutation by tracking envelope correlations frame-to-frame and reordering by energy; test with synthetic overlapping voices plus noise and report SDR/SIR?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Cloudflare","Databricks","Netflix"]},{"id":"q-1996","question":"You have 8 EEG channels capturing a simple task. Design an offline ICA pipeline (e.g., FastICA) to separate neural sources from eye-blink and muscle artifacts. Specify preprocessing (bandpass 1–40 Hz, centering), whitening, nonlinearity choice (tanh), number of components, criteria to identify artifact components (correlation with EOG/EMG, topographies), and a basic validation plan using simulated ground-truth sources and reference channels. End with a question mark?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Goldman Sachs","Stripe"]},{"id":"q-2117","question":"In a factory setting, an 8‑mic array is mounted on a mobile robot navigating among moving machinery noise. Design a real-time online convolutive ICA pipeline that (a) uses STFT framing, (b) models a time‑varying mixing matrix M(t), and (c) decides between per‑bin ICA vs joint diagonalization. Propose how to utilize IMU/pose priors to stabilize permutation and scale across time, handle drift, and meet real-time constraints. Include an evaluation plan with SDR/SIR and task‑specific metrics?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Citadel","Hashicorp","Square"]},{"id":"q-2129","question":"Design a real-time online ICA pipeline for a 12‑mic array on a factory floor capturing three moving sound sources (robot arm motors, alarms, and human chatter). The system must tolerate intermittent mic dropout, impulsive noise bursts, and a constrained CPU. Describe frame structure, whitening, demixing updates, per-bin vs joint diagonalization, dropout handling, latency targets, and evaluation plan (ground-truth SDR/SIR)?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["Plaid","Tesla","Twitter"]},{"id":"q-2200","question":"Beginner ICA with reference signal: A 3-mic array records two overlapping speakers in a small classroom with moderate reverberation. Design a lightweight semi-supervised ICA pipeline that uses a short reference sample from the target speaker to bias the demixing toward extracting that voice. Include whitening, online 2×2 demixing, reference-guided permutation, robustness to time-varying noise, latency < 100 ms, and an evaluation plan with ground-truth SDR/SIR and listening checks?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Google","IBM","Stripe"]},{"id":"q-2288","question":"Advanced: A 12-mic drone-mounted array records a busy construction site with Doppler shifts and wind noise. Design a real-time online convolutive ICA pipeline that (a) uses a neural prior to guide non-Gaussianity, (b) models a time-varying mixing matrix with a Kalman filter, (c) handles Doppler and wind, (d) meets latency <80 ms, and (e) provides a robust evaluation plan (SDR/SIR and speaker IDs)?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["Cloudflare","Google","Slack"]},{"id":"q-2416","question":"Design an online audiovisual ICA pipeline for a 6-mic conference room where two speakers move and reverberation is strong. Use lip-tracking from video to provide soft priors that guide permutation and demixing across STFT bins. Specify frame size, whitening, online demixing updates, cross-modal fusion weights, handling time-varying mixing, latency targets, and an evaluation plan (SDR/SIR + listening checks)?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["Adobe","Snap"]},{"id":"q-2443","question":"In a moving car interior, an 8-mic array captures two near-field speakers plus ambient noise. Design a real-time ICA pipeline that adapts to changing geometry, handles intermittent mic dropouts, and preserves source permutations across time. Specify: (i) whitening and online demixing strategy, (ii) how to handle missing channels without reinitialization, (iii) latency target, (iv) evaluation plan with ground-truth SDR and listening checks?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["DoorDash","OpenAI","Zoom"]},{"id":"q-2514","question":"Video-guided ICA with lip cues: 4-mic array records two speakers; synchronized video provides lip-region activity. Propose a beginner-friendly real-time pipeline that uses lip cues as a soft reference to bias 2×2 demixing. Include whitening, online ICA, lip-based bias, frame permutation alignment, latency target (<100 ms), and evaluation plan (SDR/SIR + listening checks)?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Netflix","Scale Ai","Snap"]},{"id":"q-2536","question":"You're building a remote collaboration tool used by Discord/ Coinbase where 4 handheld mics on different users capture speech, but network jitter causes asynchronous sampling and occasional packet loss. Design a practical blind source separation pipeline to recover two active speakers in near real-time. Include: (i) synchronization strategy for misaligned frames, (ii) choice between spatially varying whitening and joint diagonalization, (iii) robustness to packet loss and time-varying mixing, (iv) latency targets, and (v) an evaluation plan with SDR/SIR and listener checks?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["Coinbase","Discord"]},{"id":"q-2545","question":"Beginner ICA in a noisy desk environment: a 4-mic array on a conference desk records two coworkers speaking while a desk fan and HVAC introduce non-stationary noise. Design a real-time ICA pipeline that runs on CPU. Specify: whitening, 2×2 demixing, choice between time-domain vs frequency-domain ICA, online update rule with learning rate, frame-to-frame permutation alignment, strategies for non-stationary noise, latency target, and an evaluation plan (synthetic ground-truth SDR/SIR plus listening checks)?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["Apple","Google","MongoDB"]},{"id":"q-2691","question":"In a 9-mic wearable array mounted on a moving vehicle, three voices drift in and out amid wind noise. Design an online convolutive ICA pipeline to separate sources. Include: (i) STFT framing and whitening, (ii) online demixing for a 9×9 matrix, (iii) nonstationary mixing handling with a forgetting factor, (iv) frame-wise permutation alignment, and (v) a concrete SDR/SIR evaluation plan with ground truth?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["Apple","Coinbase","Databricks"]},{"id":"q-2695","question":"In a web app, you capture stereo audio from a 2-mic headset during a cafe meetup where two people speak and wind noise is occasional. Design a beginner ICA pipeline that runs entirely in the browser (JavaScript/WebAudio) using 50% overlap frames of 1024 samples (assuming 44.1kHz), with (i) whitening, (ii) a simple online 2×2 ICA using tanh nonlinearity, (iii) frame-to-frame permutation alignment based on non-Gaussianity, and (iv) a lightweight adaptation to nonstationary noise. Include latency targets (<80 ms) and a plan to evaluate with ground-truth SDR/SIR and listening checks?","channel":"ica","subChannel":"general","difficulty":"beginner","tags":["ica"],"companies":["NVIDIA","OpenAI","Robinhood"]},{"id":"q-839","question":"**Advanced ICA Challenge**: Given a 3×N mixed signal matrix from sensors, outline a concrete plan to recover independent sources with FastICA. Include whitening steps, nonlinearity choice (e.g., g(u)=tanh(u)), convergence criteria, how you resolve sign/perm ambiguity, and how you compare to PCA on the same data. Include practical inputs and diagnostics?","channel":"ica","subChannel":"general","difficulty":"advanced","tags":["ica"],"companies":["IBM","Two Sigma","Uber"]},{"id":"q-875","question":"Given a 6-channel wearable time-series with non-stationary motion artifacts and slowly varying latent sources, design an online ICA workflow to separate the sources in real time. Specify streaming whitening, adaptive contrast functions, choice of nonlinearity, handling sign and permutation drift, and a robust validation plan against synthetic ground truth and a PCA baseline?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["Citadel","Salesforce"]},{"id":"q-910","question":"An 8-microphone array records a live conference with moving speakers and reverberation. Design a practical convolutive ICA pipeline to separate sources. Include: STFT-based mixing, choice between per-bin ICA vs joint diagonalization, permutation alignment across frequency bins, tracking non-stationary mixing, real-time feasibility, and evaluation plan (SDR/SIR, ground truth)?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["LinkedIn","OpenAI","Robinhood"]},{"id":"q-953","question":"Design a real-time, online complex-valued ICA pipeline to separate RF sources from an 8-antenna receiver in a dynamic multipath environment with drifting mixing. Describe (a) complex whitening + fixed-point ICA update, (b) the complex contrast and convergence rule, (c) tracking sign and permutation drift over time, and (d) a practical evaluation plan with synthetic ground truth and over-the-air tests under DSP constraints?","channel":"ica","subChannel":"general","difficulty":"intermediate","tags":["ica"],"companies":["IBM","Microsoft","NVIDIA"]},{"id":"q-1064","question":"Design and implement a streaming app's episode grid using a UICollectionView with a compositional layout that supports dynamic item sizes, infinite scrolling, and efficient image caching; use a modern iOS approach (Diffable Data Source, NSCache, prefetching), ensure accessibility and testability?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["Apple","Netflix","Tesla"]},{"id":"q-1164","question":"You're building a lightweight notes app for iOS. Implement a NotesEditor view (SwiftUI) that autosaves to disk as the user types, using a 500ms debounce. Persist to a local JSON file in the app's documents directory. Ensure rapid typing doesn't cause multiple disk writes, and provide a minimal unit test that verifies the file content is updated after a debounce period?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Discord","Google","Tesla"]},{"id":"q-1185","question":"In an iOS app, you are displaying a feed of user avatars in a UICollectionView with infinite scrolling. Explain how you would implement incremental image loading that cancels obsolete requests, caches images both in memory and on disk, uses Swift concurrency for loading, and ensures smooth scrolling under memory pressure, including prefetching and memory-pressure handling strategies?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["Airbnb","Bloomberg","Goldman Sachs"]},{"id":"q-1439","question":"In a recipe-list iOS app, implement a multi-select ingredient filter as tappable chips using UIKit. Given a static dataset of recipes with ingredients, create a chips bar that allows selecting multiple ingredients, a Clear button, and a table view that shows only recipes containing all selected ingredients. Ensure accessibility labels and VoiceOver order, and provide a simple unit test validating the filter logic?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Apple","Databricks","DoorDash"]},{"id":"q-1573","question":"Design a beginner iOS feature: a SwiftUI-based **Favorites** screen for a recipe app. It should load a predefined array of Recipe objects from a local JSON file, display in a single-column list, allow tapping to mark/unmark as favorite, persist favorites in **UserDefaults**, and provide a search bar to filter by name **case-insensitively**. Explain how you'd structure the model, storage, and UI, and how you'd test search and persistence?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Airbnb","Google","Netflix"]},{"id":"q-1611","question":"Design and implement an offline-first notes feature for iOS: store locally in SQLite, use a simple CRDT-like merge or last-writer-wins with version vectors for conflict resolution, and sync changes via a WebSocket protocol, all in Swift using async/await. Include data model, conflict handling, and testing strategy?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["OpenAI","Oracle"]},{"id":"q-1681","question":"In an iOS SwiftUI app, build a minimal Notes editor screen: a multiline TextEditor bound to a string. Implement an autosave that triggers 1 second after the user stops typing, saving the draft to UserDefaults under the key 'noteDraft'. Load the draft on view appear. Include a tiny unit test that simulates typing and asserts the draft is saved after the debounce delay?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Airbnb","Google"]},{"id":"q-1718","question":"In a high-traffic iOS app for autonomous telemetry, describe a robust data pipeline that streams sensor data into local batches and uploads to cloud with offline queueing and crash recovery. Include how to handle backpressure, idempotency, and testing. Provide concrete Swift components and a minimal prototype?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["IBM","NVIDIA","Tesla"]},{"id":"q-1951","question":"You're building an **offline-first** image gallery in an iOS app. Describe in detail how you would implement a robust caching and download strategy that supports offline browsing, seamless updates when connectivity returns, and **conflict resolution**. Include data models, caching policy, background downloads, and testing approaches?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["Scale Ai","Tesla","Zoom"]},{"id":"q-2089","question":"Design an offline-first album sync for iOS that uses a CRDT-based OR-Set to resolve conflicts in PhotoMetadata across devices. Include data model, merge rules with per-album ACLs, and a test plan for intermittent connectivity?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["Discord","IBM","Scale Ai"]},{"id":"q-2184","question":"Begin with a concrete scenario: You have an iOS app that shows a list of product names. Build a minimal UIKit view controller that renders the list in a `UITableView` with a search bar. Implement a 300ms **debounce** using Swift's async/await to filter results on a background queue, then present them on the main thread. Filter should be case-insensitive and preserve order. Provide the core view controller code and explain cancellation behavior?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Coinbase","Google","Microsoft"]},{"id":"q-2341","question":"Scenario: Build a real-time presence client for an iOS app used by distributed teams (Zoom/Square/PayPal). Use a WebSocket to publish/receive presence events, implement exponential backoff reconnect, background processing, and idempotent event application; provide a minimal Swift prototype and a test plan for intermittent networks. How would you implement this?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["PayPal","Square","Zoom"]},{"id":"q-2355","question":"Design a live-map ETA/route update system for a rideshare app with intermittent connectivity. Describe an architecture using MapKit, CoreData offline persistence, and CloudKit sync with Swift concurrency. Include conflict resolution strategy, data integrity guarantees, and test plan?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["Amazon","Lyft"]},{"id":"q-2391","question":"Design a high-performance, offline-first photo feed in an iOS app. Render a UICollectionView that shows square image tiles with captions, support infinite scrolling, and prioritize visible cells for image downloads. Implement disk caching (LRU), memory pressure eviction, and offline fallback when network is unavailable. Ensure VoiceOver and Dynamic Type. Provide data model, caching strategy, testing plan, and a concise code sketch for an ImageLoader using async/await?","channel":"ios","subChannel":"general","difficulty":"advanced","tags":["ios"],"companies":["Amazon","Discord","PayPal"]},{"id":"q-2570","question":"**iOS Beginner Challenge:** Build a minimal journaling screen where a user types notes, autosaves to a local file atomically, and displays a list of notes. Ensure data is preserved across app restarts, support background/foreground transitions, and debounce saves to avoid thrashing. Use Swift with SwiftUI, Codable Note, and FileManager in the Documents directory. Include a simple test plan?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Hashicorp","Snap"]},{"id":"q-464","question":"How would you implement a custom UICollectionViewFlowLayout that supports dynamic cell heights and sticky headers while maintaining smooth scrolling performance?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["Amazon","Lyft","Meta"]},{"id":"q-495","question":"How would you implement a simple UITableView with custom cells in iOS using Swift?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Oracle","Snowflake","Uber"]},{"id":"q-525","question":"You're building a food delivery app like DoorDash. How would you implement background location updates to track delivery drivers while balancing battery life and accuracy?","channel":"ios","subChannel":"general","difficulty":"intermediate","tags":["ios"],"companies":["DoorDash","Microsoft","Zoom"]},{"id":"q-578","question":"What's the difference between weak and unowned references in Swift, and when would you use each?","channel":"ios","subChannel":"general","difficulty":"beginner","tags":["ios"],"companies":["Meta","NVIDIA"]},{"id":"q-181","question":"Explain the difference between weak and unowned references in Swift and provide practical use cases for each?","channel":"ios","subChannel":"swift","difficulty":"intermediate","tags":["swift","language"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"q-257","question":"What is optional chaining in Swift and how does it compare to force unwrapping, optional binding, and optional chaining with method calls when accessing nested optional properties?","channel":"ios","subChannel":"swift","difficulty":"beginner","tags":["optionals","protocols","generics"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Stripe"]},{"id":"q-204","question":"How would you optimize a UITableView with 10,000+ complex cells using Auto Layout while maintaining 60fps scrolling and memory efficiency?","channel":"ios","subChannel":"uikit","difficulty":"advanced","tags":["autolayout","tableview","collectionview"],"companies":["Airbnb","Apple","Capital One","Lyft","Uber"]},{"id":"q-232","question":"How does Auto Layout constraint resolution work when creating a UITableView with dynamic cell heights?","channel":"ios","subChannel":"uikit","difficulty":"beginner","tags":["autolayout","tableview","collectionview"],"companies":["Airbnb","Apple","Google","Meta","Uber"]},{"id":"q-1075","question":"You're building a beginner-friendly KCA flow for a data science platform where notebooks run in isolated containers across tenants. The platform uses a cloud CA to issue short-lived client certificates (2 hours) via token-based enrollment (JWT) rather than CSR, with a gateway that validates certs and maps tenants. Describe the enrollment flow, renewal, and how the gateway handles clock skew, expired certs, and failed enrollments. Include a minimal test plan and example API calls?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Databricks","Hugging Face"]},{"id":"q-1082","question":"Design a beginner-friendly KCA workflow focusing on policy-driven certificate issuance for a multi-tenant SaaS gateway: tenants publish per-tenant certificate policies (allowed CNs, key type, validity). A central CA issues short-lived client certs, and edge gateways refresh policy every 60 minutes while rotating certs to avoid handshake failures. Explain enrollment, policy propagation, revocation, and provide a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Adobe","Cloudflare","DoorDash"]},{"id":"q-1120","question":"Explain how you would implement a beginner-friendly KCA workflow for a multi-region SaaS where regional CAs issue per-service client certs with a 2-hour TTL. Include enrollment (CSR-based vs token-based), cross-region policy propagation, revocation, and how to validate certificates at the edge during region failover. Provide a minimal test plan and rollback strategy?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Apple","Databricks","Instacart"]},{"id":"q-1141","question":"For an industrial IoT fleet of 50k field devices with intermittent connectivity, design a KCA workflow where each device boots with a hardware-backed key and obtains a short-lived client certificate from a central CA via an attestation-enabled bootstrap. Include enrollment, attestation checks, certificate lifetimes (6 hours), automatic renewal after reconnection, revocation for decommissioned devices via OCSP/CRLs, offline root fallback, auditing, and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Meta","NVIDIA"]},{"id":"q-1189","question":"Design a beginner-friendly KCA flow for a multi-tenant SaaS gateway that issues per-tenant, short-lived client certificates via token-based enrollment, but enforces per-tenant issuance quotas (e.g., 100 certs/hour with burst to 10x). Explain enrollment, quota enforcement, renewal, revocation, and audit logging, and provide a minimal test plan. Include how you handle quota exhaustion during spikes?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Meta","Slack","Uber"]},{"id":"q-1353","question":"Design a beginner-friendly KCA workflow for a multi-region SaaS API gateway where a primary CA replicates to a secondary region for DR. Explain policy propagation, certificate rollover during failover, cache freshness, cross-region revocation handling, and how edge gateways and clients stay in sync with minimal downtime. Include a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Bloomberg","Databricks","Uber"]},{"id":"q-1527","question":"In a multi-region SaaS gateway architecture, design a KCA-based onboarding flow for both tenants and devices using mTLS. Devices boot with hardware attestation and CSR enrollment to a regional intermediate CA, which signs a 15-minute device cert bound to device identity and tenant policy. Describe enrollment, policy propagation, cross-region trust, cert rotation, revocation (OCSP/CRL), and audit logging. Include failure modes and a concrete 1-page test plan with synthetic onboarding scenarios?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Amazon","Microsoft"]},{"id":"q-1568","question":"Design a global **KCA onboarding** flow for an edge gateway fleet serving 100k tenants. Each gateway boots with a hardware-backed secret and attestation-enabled CSR enrollment, then obtains a short-lived client certificate whose policy extension encodes per-tenant access rules and rate limits. Explain policy refresh, renewal, revocation, and audit trails. Include a concrete example policy and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Adobe","Salesforce"]},{"id":"q-1645","question":"Design a KCA onboarding workflow for 20k edge gateways deployed across regions where each device boots with hardware attestation and a measured firmware hash. The regional CA issues a 12-hour TLS client certificate bound to the firmware hash and attestation identity. Describe enrollment, firmware-hash policy binding, renewal on upgrade, rollback revocation via OCSP/CRLs, offline root fallback, and comprehensive auditing. Include a practical test plan with upgrade/rollback scenarios?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["MongoDB","Scale Ai","Tesla"]},{"id":"q-1651","question":"Design a KCA onboarding and certificate lifecycle flow for 200 edge API gateways deployed across three data centers, serving both internal microservices and external partners. Each gateway boots with a hardware-backed root and attestation-enabled CSR enrollment, then obtains a 10-minute mTLS client certificate from a regional CA. Include enrollment, region-aware policy propagation via a streaming config service, cross-region trust, certificate renewal, revocation via OCSP/CRLs, offline root fallback, auditing, and a concrete 1-page test plan?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["IBM","Meta","Two Sigma"]},{"id":"q-1748","question":"Design a beginner-friendly KCA workflow for a data ingestion platform used by Snowflake, Airbnb, and OpenAI that authenticates per-tenant producers and external partners via mTLS? Implement per-session certificates (30 min) with hardware-backed attestation and policy refresh via streaming config every 15 min. Cover enrollment, renewal, revocation, audit logs, and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Airbnb","OpenAI","Snowflake"]},{"id":"q-1878","question":"In a high-throughput data desk environment, design a KCA-based onboarding for 1,000 streaming data connectors across two data centers. Each connector boots with hardware attestation and obtains a 60-second TLS client certificate from a regional CA, rotated automatically on reconnection. Include enrollment, attestation checks, policy binding to data-stream permissions, cross-region trust, revocation (OCSP/CRLs), offline root fallback, auditing, and a test plan with simulated reconnections and data bursts?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Bloomberg","Two Sigma"]},{"id":"q-1961","question":"Design a KCA onboarding for a global fleet of 5,000 IoT/edge devices migrating from RSA TLS to post-quantum TLS during active operation. Devices boot with hardware-backed keys and attestation-based CSR; regional CAs issue both RSA and PQC certificates with 6-hour lifetimes, automatic renewal on reconnect, and cross-region trust. Include dual-PKI policy binding, revocation (OCSP/CRLs), offline root fallback, auditing, and a 1-page test plan with PQC handshake fallbacks and rollback?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Citadel","LinkedIn","Uber"]},{"id":"q-1978","question":"Design a beginner-friendly KCA workflow for a serverless edge gateway fleet serving 50k tenants. Issue per-tenant client certs with 15-minute lifetimes and a 'scope' extension describing allowed APIs and rate limits. Enrollment via CSR or token; propagate policy to edge nodes via streaming config with staggered renewals. Support per-tenant revocation via OCSP stapling or small CRLs; include audit logs and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Amazon","Google","Lyft"]},{"id":"q-2004","question":"Design a beginner-friendly KCA workflow issuing per-store client certificates with 1h TTL for a distributed edge gateway fleet in 200 retail locations. Enrollment via CSR templates or token-based enrollment; embed per-store API access and rate limits in a certificate policy extension. Include policy propagation via streaming config with staggered renewals, offline revocation via local CRLs, and audit logs. Provide a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Instacart","Snowflake","Two Sigma"]},{"id":"q-2069","question":"Design a KCA onboarding and certificate lifecycle for an autonomous drone fleet deployed across three countries, operating with intermittent connectivity and strict flight-permission policies. Drones boot with hardware-backed keys and attestation; obtain a short-lived TLS client certificate from a regional CA, with policy-bound flight rights, cross-region trust, and automatic certificate rotation on rejoin. Include revocation via OCSP/CRLs, offline root fallback, auditing, and a 1-page test plan with realistic sorties?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Cloudflare","NVIDIA","Oracle"]},{"id":"q-2166","question":"Design a KCA flow for a global microservice mesh where workloads across 4 regions obtain ephemeral TLS client certificates for mTLS. Each workload presents hardware-backed attestation, is issued a short-lived certificate with a SPIFFE ID, and must maintain cross-region trust, rapid revocation, and auditable logs. Include enrollment, attestation, rotation cadence, and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Apple","Lyft","NVIDIA"]},{"id":"q-2181","question":"Design a KCA architecture to issue post-quantum‑ready client certificates for 200 edge gateways across three regions. Gateways boot with hardware-backed roots and attestation, enroll via CSR or token, and receive short-lived certificates with a region policy extension. Include PQC agility, cross-region trust, renewal, revocation (OCSP/CRLs), auditing, and a 1-page test plan?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Amazon","Apple","Google"]},{"id":"q-2217","question":"In a multi-tenant edge compute fabric deployed across 3 regions with 1,000 nodes per region, design a KCA flow where each node boots with a hardware-backed root and attestation, enrolls via a token, and obtains a 120-second TLS client certificate binding to its tenant. Include enrollment, attestation checks, per-tenant policy binding to the data plane, cross-region trust, automatic renewal on reconnect, revocation via OCSP/CRLs, offline root fallback, auditing, and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Google","Tesla"]},{"id":"q-2250","question":"Design a KCA strategy where 5000 edge gateways boot with hardware-backed roots and perform attestation-enabled CSR enrollment to obtain short-lived mTLS certs, but without a centralized CA: use threshold signatures across regional HSM clusters to issue certs, ensuring cross-region trust, policy scoping per gateway, region-aware policy streaming, and robust revocation (gossip-CRLs/OCSP). Include enrollment, renewal, audit, offline scenarios, and a concrete 1-page test plan?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Hashicorp","Robinhood","Tesla"]},{"id":"q-2487","question":"Design a KCA workflow for 100k industrial IoT devices deployed across 5 regions, each with a hardware-backed root and device attestation. Devices enroll via a one-time token and CSR, obtain a 1-year client certificate with per-device scoping, and refresh every 6 hours. Include policy propagation through a streaming config service, cross-region trust, revocation via OCSP/CRLs, offline root fallback, auditing, and a concrete 1-page test plan. Ensure resilience against device compromise and root loss?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Discord","MongoDB","Uber"]},{"id":"q-2500","question":"Design a KCA workflow for 250 edge gateways across four regions that serve multiple tenants with strict data isolation. Gateways boot from hardware-backed roots and attestation-enabled CSR enrollment, then obtain short-lived client certificates whose policy extension enforces per-tenant audience restrictions for a service mesh and cross-tenant data boundaries. Include policy propagation, renewal, revocation (OCSP/CRLs), offline root fallback, auditing, and a 1-page test plan?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Airbnb","Anthropic","Netflix"]},{"id":"q-2640","question":"Design a scalable KCA workflow to issue and rotate mTLS client certificates for 2,500 microservices deployed across six regions, each host with hardware-backed roots and attestation (TPM/TEE). Include CSR enrollment with hardware attestation, region-aware policy propagation via a streaming config service, cross-region trust anchors, short-lived leaf certs (5 minutes), automated renewal, revocation via OCSP/CRLs, offline root fallback, auditing, and a concrete 1-page test plan. Also provide a policy example and a migration path from a legacy PKI?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Airbnb","Coinbase","Lyft"]},{"id":"q-2697","question":"Design a KCA onboarding flow for 50 ephemeral edge workers across a multi-cloud environment where each worker boots from a hardware-backed root of trust (TPM/HSM) and obtains a short-lived TLS client certificate (15 minutes) from a regional CA via attestation-based bootstrap. Include enrollment, attestation checks, automatic renewal on rehydration, revocation, auditing, and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Amazon","Robinhood"]},{"id":"q-844","question":"You have a CSV log with columns: user_id, event_timestamp (ISO 8601), and event_type. Write a Python function using pandas to compute the number of unique active users per day for a given timezone, returning a dict mapping 'YYYY-MM-DD' to count. Explain how you handle timezone normalization and missing data. Provide sample usage?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Coinbase","Databricks"]},{"id":"q-884","question":"You're operating a Kafka-to-Spark streaming job in production and observe sporadic latency spikes; detail a concrete diagnostic plan to identify bottlenecks and a remediation strategy, including metrics, tooling (OpenTelemetry, Prometheus, Grafana), and validation steps?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Anthropic","DoorDash","Google"]},{"id":"q-889","question":"**How would you design a scalable KCA integration** for a multi-tenant SaaS using short-lived client certificates? Central CA in an HSM issues per-tenant CSRs, rotates certificates every 24h, and supports revocation via OCSP stapling and short CRLs with edge caching. Include audit trails, MFA for CA access, and clear renewal, compromise, and revocation workflows?","channel":"kca","subChannel":"general","difficulty":"advanced","tags":["kca"],"companies":["Cloudflare","Microsoft","Plaid"]},{"id":"q-952","question":"How would you design an on-demand KCA for service-to-service mTLS in a multi-tenant data platform (Stripe/Databricks) where a Kubernetes-hosted CA issues per-service CSRs, each cert valid for 5–15 minutes, supports cross-region trust, anomaly-driven revocation via telemetry, and full audit trails; include MFA protection and an offline root fallback?","channel":"kca","subChannel":"general","difficulty":"intermediate","tags":["kca"],"companies":["Databricks","Stripe"]},{"id":"q-997","question":"Design a beginner-friendly KCA flow for a SaaS API gateway where a cloud CA issues per-tenant, short-lived client certificates (4 hours) for app authentication. Outline provisioning (CSR-based enrollment or token-based enrollment), automatic renewal, revocation strategy, and how the gateway validates certs and records audit logs. Include failure modes and a minimal test plan?","channel":"kca","subChannel":"general","difficulty":"beginner","tags":["kca"],"companies":["Bloomberg","Netflix","OpenAI"]},{"id":"q-1138","question":"You are building a real-time KCNA feed service used by Snap, Meta, and Discord-scale clients to publish and deliver announcements across regions with sub-100ms tail latency. Describe the end-to-end architecture, data model for Announcement, ingestion and delivery pipeline, guarantees (at-least-once vs exactly-once), ordering, deduplication, failover, tests, and observability. How would you scale to 10k updates/sec with 99.999% uptime?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Discord","Meta","Snap"]},{"id":"q-1295","question":"**KCNA Consumer Backpressure & Gap Handling**: In a beginner-friendly KCNA consumer, design a pull-based ingestion path that preserves per-topic offsets, guarantees at-least-once processing, and recovers from transient network slowdowns without duplicating messages. Describe the API shape, offset persistence, retry/backoff strategy, and a minimal test plan including a canary scenario?","channel":"kcna","subChannel":"general","difficulty":"beginner","tags":["kcna"],"companies":["Discord","Plaid","Two Sigma"]},{"id":"q-1431","question":"KCNA cross-region, multi-tenant QoS: Propose an architecture and API for declaring per-tenant Topic SLAs (retention, max throughput) and a cross-region offset store with region-local commit logs. How would you implement per-tenant backpressure, quota enforcement, and exactly-once vs at-least-once semantics under regional outages? Include a concrete canary and testing plan?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Oracle","Snowflake"]},{"id":"q-1471","question":"KCNA multi-tenant schema-evolution: design a zero-downtime migration for a KCNA-based event bus used by many tenants where the Event schema evolves from v1 to v2 (add region field, deprecate payload wrapper). How do you enforce backward/forward compatibility, isolate tenants during migration, and validate with canaries? Include tooling, rollback plans, and observability?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["DoorDash","Stripe","Twitter"]},{"id":"q-1503","question":"KCNA cross-region tenancy isolation: design a replication topology where tenants' streams stay regional unless opted into global analytics; implement per-tenant topic partitioning, region-aware routing, and idempotent retries with de-dup. How do you enforce locality, protect privacy in cross-region analytics, and handle failover/lag? Include concrete configs, testing strategy, and rollback plan?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Amazon","NVIDIA"]},{"id":"q-1520","question":"KCNA TTL Retention: design a per-topic TTL policy for KCNA events. How would you store TTL metadata, drop expired events without breaking consumer offsets, handle late-arriving events post-expiry, and observe/verify with canary tests? Provide a minimal API shape for setting TTL per topic, a compact storage layout, and a lightweight cleanup workflow?","channel":"kcna","subChannel":"general","difficulty":"beginner","tags":["kcna"],"companies":["Anthropic","Citadel","Tesla"]},{"id":"q-1602","question":"KCNA key management & tenant isolation: In a **KCNA**-based multi-tenant event bus, each tenant uses a per-tenant envelope encryption key managed by a centralized **KMS**. Design a **zero-downtime** key rotation workflow that rotates tenant keys without breaking consumption, re-encrypts in-flight payloads, and prevents cross-tenant leakage. Include data model (**tenant_id**, key_version, wrapped_key), rollout strategy, rollback, and observability?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Microsoft","Tesla"]},{"id":"q-1698","question":"Design a KCNA privacy-first feed where tenants specify a region (US/EU/APAC) and all data remains local. Use per-tenant envelope encryption with KMS, region-scoped brokers, field-level redaction before delivery, and tenant-aware access controls. Add audit trails and canary tests to prove zero cross-tenant leakage, correct redaction, and retention under burst load?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Anthropic","Tesla","Zoom"]},{"id":"q-1723","question":"KCNA dynamic tenancy fairness under bursty workloads: design a per-tenant ingestion path with token-bucket quotas and fair queuing; detail API surface, quota persistence, backpressure signaling, and dynamic rebalancing from telemetry. How do you validate isolation under burst traffic, and what would your canary rollout look like?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Databricks","NVIDIA"]},{"id":"q-1806","question":"KCNA privacy-by-design: design a tenant-isolated KCNA ingestion and delivery path that enforces per-tenant encryption keys for in-flight and at-rest data, supports on-the-fly key rotation with zero-downtime, and provides auditable access controls for analytic consumers. Describe the KMS integration, key-wrapping strategy, performance impact, and a minimal canary test for rotation?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Adobe","Robinhood","Slack"]},{"id":"q-1901","question":"KCNA Time-Travel Replay for Tenants: Suppose a tenant needs to audit a precise 2-hour window of events without halting live ingestion. Design a tenant-scoped time-travel replay feature in KCNA that allows replaying events from a given timestamp while live streams continue, guarantees exactly-once delivery to downstream analytics, and preserves per-tenant offsets. Describe API shapes, storage layout, consistency guarantees, security controls, and a minimal test plan (canaries)?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Anthropic","Meta","Snowflake"]},{"id":"q-1926","question":"Design a beginner-friendly KCNA feature: tenant-scoped data TTL. Each tenant can configure a TTL (e.g., 24h) for their streams. Describe the API (setTTL on a topic with ttlMs), how per-message metadata is stored, how a background purge runs safely without breaking consumers, and a minimal test plan with canaries?","channel":"kcna","subChannel":"general","difficulty":"beginner","tags":["kcna"],"companies":["LinkedIn","NVIDIA"]},{"id":"q-2025","question":"KCNA ingest fairness at scale: design a per-tenant fair-queuing strategy for bursty producers in a multi-tenant KCNA channel. Implement a two-layer approach with a per-tenant token-bucket at the gateway and a global weighted-round-robin scheduler across tenants to prevent starvation. Define quotas, bounded bursts, and backpressure signaling; outline API contracts, config knobs, and a test plan with synthetic tenants and canaries?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Databricks","Hugging Face","IBM"]},{"id":"q-2101","question":"KCNA policy-driven tenant isolation: design a per-tenant access-control mechanism at the gateway and stream processors that enforces tenant-scoped authorization for ingest and delivery, supports dynamic policy updates with zero-downtime rollout, and provides an auditable per-event trail. Outline the policy model (tenants, roles, actions), integration with a policy engine (e.g., OPA/ABAC), JWT-based identity, and testing strategy including canaries and rollback?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Instacart","NVIDIA","Stripe"]},{"id":"q-2135","question":"KCNA Dead-Letter & Poison Message Handling: For a multi-tenant KCNA deployment, design a per-tenant dead-letter queue strategy that routes malformed events out of the normal path, enforces per-tenant retry budgets, and preserves idempotent replays. Describe DLQ schema, routing rules, retention, operator visibility, and a safe rollout with canaries?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Citadel","Instacart","Meta"]},{"id":"q-2165","question":"KCNA observability & tenant-scoped tracing: design end-to-end observability for a multi-tenant KCNA event bus under burst traffic, preserving tenant data isolation while enabling operations debugging. Specify: how to propagate tenant_id and correlation_id across producer, gateway, and consumer; per-tenant metrics and alerting; privacy-preserving trace UI that exposes only metadata; retention, RBAC, and failure-mode testing with canaries?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Bloomberg","Goldman Sachs","Netflix"]},{"id":"q-2265","question":"Design a KCNA-based privacy-preserving cross-tenant analytics pipeline where tenants publish raw events but analysts receive aggregated metrics only. Tenants can opt into regional sharing with differential privacy, per-tenant encryption keys, and auditable data lineage. Describe topology, data model, access control, latency targets, and a canary rollout plan?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Meta","Netflix","Snap"]},{"id":"q-2311","question":"KCNA Dead-Letter Queue (DLQ): For a beginner-friendly KCNA, design a per-tenant DLQ strategy for messages that fail processing after N retries. Describe API shape to move from main topic to DLQ, retention, how to reprocess, and a simple test canary that demonstrates DLQ routing, backoff between retries, and alerting?","channel":"kcna","subChannel":"general","difficulty":"beginner","tags":["kcna"],"companies":["Meta","Snap"]},{"id":"q-2425","question":"KCNA per-tenant feature flags: design a control plane to selectively enable a new routing path and compression for KCNA streams, with zero-downtime rollout, tenant-scoped rollback, and audit logs; how would you model toggles, propagate config, implement canaries, and validate impact before full enablement?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Lyft","Uber"]},{"id":"q-2464","question":"KCNA retention governance: design a per-tenant data lifecycle in KCNA that enforces tenant-specific retention windows, supports legal holds, and runs background tombstone-based deletions with GC across partitions. Explain metadata storage, purge triggers without breaking at-least-once semantics, observability, and a rollback/hold-release workflow?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Google","Microsoft"]},{"id":"q-2471","question":"KCNA security and governance: design a per-tenant envelope encryption scheme for KCNA payloads using a central KMS. Each tenant has a dedicated DEK wrapped by a tenant-specific KEK. Encrypt payloads at produce time and decrypt only at authorized consumers with per-tenant IAM. Support per-tenant key rotation with zero-downtime re-encryption, and maintain per-tenant audit trails and deletion rules that respect retention. How would you implement lifecycle, performance trade-offs, and backward-compatibility?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Amazon","Goldman Sachs","LinkedIn"]},{"id":"q-2510","question":"KCNA tamper-evident audit trails: design per-tenant verifiable logs for event streams using append-only shards and Merkle proofs, with per-batch signing and external audit proofs. Outline data structures, key rotation, canary rollout, rollback, and a test plan that proves tamper-resistance without leaking tenant data?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Anthropic","Cloudflare","Snap"]},{"id":"q-2565","question":"KCNA per-tenant data isolation with confidential payloads: in a single KCNA cluster serving multiple tenants, design a mechanism to store tenant-scoped payloads with per-tenant encryption keys, support cross-tenant analytics only if opted-in, ensure query isolation, key rotation, and audit trails. Describe API contracts, key management, and performance implications?","channel":"kcna","subChannel":"general","difficulty":"intermediate","tags":["kcna"],"companies":["Coinbase","Microsoft","Plaid"]},{"id":"q-2582","question":"In KCNA, design a per-tenant event-time processing layer that tolerates late events within a tenant-defined latency budget, computes per-tenant 5-minute tumbling window aggregations, and guarantees at-least-once delivery. Describe per-tenant watermarks, state partitioning, late-data handling, fault tolerance, and a test plan to validate SLA compliance?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["DoorDash","Goldman Sachs","Meta"]},{"id":"q-2621","question":"In KCNA, design a per-tenant field-level access control and masking layer that operates in real time on streaming payloads for analytics, ensuring authorized tenants can decrypt unmasked fields while unauthorized tenants only see masked data, without breaking at-least-once semantics or replay safety. Explain data structures, policy evaluation, KMS key management, and testing with canaries?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Amazon","Apple"]},{"id":"q-2710","question":"KCNA per-tenant envelope encryption: design a CMK-backed scheme where each tenant's messages are encrypted at ingest with a tenant KEK wrapped by a KMS CMK, keys rotate monthly, and revocation triggers re-encryption with minimal downtime while preserving canary readers. Describe data model, API surface, rotation and revocation flows, and test strategy?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Adobe","Google"]},{"id":"q-941","question":"Scenario: A global chat platform with 2B MAUs must detect policy-violating content (spam, hate speech) in near real-time while preserving user privacy and multilingual support. Propose an end-to-end pipeline: ingestion, moderation models (rules + ML), latency SLOs (1-2s), privacy safeguards, backpressure handling, retries, and dead-letter queues. Compare on-device vs cloud inference and monitoring?","channel":"kcna","subChannel":"general","difficulty":"advanced","tags":["kcna"],"companies":["Slack","Twitter"]},{"id":"q-1021","question":"Design a globally distributed feature-flag engine for per-user rollout. Provide data model, consistency guarantees, and deployment strategy for scale across multiple regions. Include choice of storage (DynamoDB vs Redis), how regionOverrides and segments are merged, and hot-flip safety. Provide evaluation protocol and a concise pseudocode snippet for evaluateFeature(user, flag, context)?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Coinbase","Meta","Tesla"]},{"id":"q-1042","question":"Design a high-throughput, fault-tolerant event ingestion pipeline for a fraud-detection service. It must guarantee exactly-once processing, deduplicate events across shards, support backpressure, and provide end-to-end observability. Explain the data model for dedup IDs, queue choice (Kafka vs Kinesis), idempotent sinks, and how you'd test failure scenarios?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Meta","Plaid","Twitter"]},{"id":"q-1067","question":"In a Zoom-like multi-tenant Kubernetes cluster, you discover a pod running as root with a hostPath mounted. What concrete remediation plan would you implement using OPA Gatekeeper, Pod Security Standards, RBAC, NetworkPolicies, and image scanning? Include how you would verify tenant isolation with a minimal test that should fail if misconfigured?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Scale Ai","Zoom"]},{"id":"q-1096","question":"In a simple analytics microservice, implement a Python function that reads a CSV file with headers user_id, action, timestamp (timestamp optional) and returns a dict mapping each user_id to the list of unique actions performed in chronological order; if timestamp is missing, preserve input order. Explain your approach and trade-offs?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["OpenAI","Oracle","Twitter"]},{"id":"q-1129","question":"Given a Kubernetes-deployed event-processing service that suddenly drops throughput from 10k to 6k after a feature release, outline a concrete, testable plan to diagnose and restore throughput. Include: **metrics** to monitor, **bottlenecks** to consider, and concrete, incremental changes like **rate limiting**, **backpressure**, and **idempotency**. Provide a safe rollback strategy and canary plan?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Google","Scale Ai","Twitter"]},{"id":"q-1184","question":"You maintain a public API for ride estimates and expect bursts. Implement a per-API-key rate limiter that allows 60 requests per minute using an in-memory structure. Provide the core logic in JavaScript (Node.js) and briefly justify your data structure, test approach, and how you’d handle restart scenarios?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Discord","Lyft","Slack"]},{"id":"q-1227","question":"In a tiny model-usage service, write a Python function that validates a JSON payload with user_id, model_id, action, and timestamp, then inserts a document into MongoDB with a created_at field. Ensure an index exists on (model_id, timestamp) and handle duplicate submissions gracefully (idempotent using a unique composite index on (user_id, model_id, timestamp)). Include basic error handling and a minimal test snippet?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Apple","Hugging Face","MongoDB"]},{"id":"q-1246","question":"Design a policy for a shared Kubernetes cluster serving multiple teams, ensuring namespace isolation, least privilege RBAC, image provenance, Pod Security Standards, and auditable policies. Propose concrete constraints using RBAC, PSP/PSA, NetworkPolicy, and OPA Gatekeeper; include a sample ConstraintTemplate and a test that rejects nonconforming pods. Explain trade-offs and monitoring strategy?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Cloudflare","Google"]},{"id":"q-1298","question":"Design a globally-distributed rate limiter for a high-traffic service with per-tenant limits and cross-region fairness. Compare token-bucket and sliding-window approaches, specify data structures and caching, describe hot-path optimization, failure modes, and testing strategy. Assume latency budgets and real-time enforcement?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Amazon","LinkedIn","Meta"]},{"id":"q-1351","question":"In a Node.js Express API backed by PostgreSQL, POST /search accepts { term } and currently builds a SQL with string concatenation. Describe a secure rewrite using parameterized queries and input validation, including error handling and ensuring the DB user has minimal privileges. How would you structure this to prevent SQL injection?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Adobe","Salesforce","Tesla"]},{"id":"q-1680","question":"In a Kubernetes-deployed microservice that ingests up to 200 events/sec from a queue, each event has event_id and payload. The handler must be idempotent so a duplicate delivery does not write to Postgres. Propose a concrete Redis-based dedup strategy (SETNX with EXPIRE TTL) and outline how you'd implement the dedup path in code, including how you'd handle retries, restarts, and cleanup?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Instacart","NVIDIA","Netflix"]},{"id":"q-1788","question":"You're designing a payment authorization path for a fintech platform (think Plaid) where a mobile tap triggers multiple services: Auth, Fraud/Risk, Ledger, and Notification. How would you ensure idempotent processing, at-least-once retries, and eventual consistency across services? Describe data models, the flow (outbox or saga), and fault tolerance (backoffs, DLQ)?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Lyft","Plaid"]},{"id":"q-1828","question":"You operate a polyglot data stack with MongoDB and Oracle. A global e-commerce app experiences 300–500ms latency on order placement during peak hours, despite modest CPU usage. Propose an end-to-end plan to diagnose and fix, covering data model, indexing, shard/cluster topology, connection pooling, caching, and cross-database consistency. Include concrete knobs you would adjust and how you'd validate impact?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["MongoDB","Oracle"]},{"id":"q-1877","question":"In a multi-tenant data pipeline on Kubernetes serving a PayPal-like payments gateway, events flow from a single Kafka topic to a Spark streaming job that writes to a data lake. How would you enforce per-tenant data isolation, encryption, auditing, and masking while maintaining a 95th percentile latency under peak load?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Databricks","Google","PayPal"]},{"id":"q-1950","question":"You’re asked to implement a simple in-memory rate limiter for a Node.js/Express API: cap each API key at 100 requests per 10 minutes. Provide a minimal in-process solution using a per-key timestamp array, how you prune old entries, and how you handle bursts. Explain trade-offs and a plan to scale to multiple processes?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["DoorDash","Instacart","Meta"]},{"id":"q-2008","question":"In a cloud-native data platform delivering streaming telemetry to a data lake, how would you implement end-to-end data quality and schema drift control for Kafka → Flink → Parquet, when tenants frequently emit extra fields? Describe schema versioning, compatibility, drift detection, and automated remediation to downstream dashboards, with concrete knobs and testing steps?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Databricks","Google"]},{"id":"q-2085","question":"How would you implement compliant per-tenant data erasure in a streaming analytics pipeline that ingests tenant events from Kafka, writes to a data lake as Parquet, and serves BI queries, ensuring immutable data, auditability, and zero-downtime erasure while preserving peak-load latency? Include data-modeling, catalog updates, and operational steps?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Anthropic","Robinhood","Square"]},{"id":"q-2119","question":"In a frontend data pipeline, you receive two sorted arrays of strings representing tags. Write a function mergeUnique(a,b) that returns a new array with all unique elements in sorted order, without mutating inputs. Assume inputs are sorted. Provide a minimal, robust JS implementation and explain its time/space complexity?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Discord","Instacart"]},{"id":"q-2193","question":"In a real-time analytics pipeline processing user activity events from a mobile app, you must support dynamic event schemas, provide backward compatibility, and enforce per-user data retention, while keeping end-to-end latency under 300 ms at 99th percentile. Describe the architecture, data model, schema evolution strategy, validation in-flight, and how you would monitor and test latency?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Hugging Face","MongoDB","Uber"]},{"id":"q-2251","question":"In a Kubernetes-based microservices stack processing per-tenant user events, design a per-tenant dynamic tracing sampling policy to keep OTLP ingestion overhead under 5% while preserving 95th percentile latency visibility for the most latency-sensitive tenants. Discuss config storage, crash-safe fallback, and validation under peak load?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Hugging Face","Plaid","Zoom"]},{"id":"q-2368","question":"You’re building a real-time fraud-detection scoring service for a payments platform that must process 100k transactions per second with sub-20ms latency and strict privacy constraints. Describe an architecture to ingest events, compute per-transaction risk scores, handle backpressure, ensure idempotent processing, and observe the system. Include data model, latency budget, and fault-tolerance trade-offs?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Instacart","OpenAI","PayPal"]},{"id":"q-2408","question":"In a 3-namespace Kubernetes cluster (dev, stage, prod), design a least-privilege model: per-namespace Role/RoleBinding (no ClusterRole bindings), enforce Pod Security Standards:restricted via OPA Gatekeeper, and apply default-deny NetworkPolicies with explicit allows. Explain testing with kubectl can-i and provide minimal YAML samples for Role, RoleBinding, NetworkPolicy, and a Gatekeeper constraint template?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["IBM","LinkedIn"]},{"id":"q-2462","question":"Scenario: You’re maintaining a Kubernetes cluster used by Plaid and Oracle developers. Provide a minimal RBAC setup to grant a ServiceAccount named 'dev-read' in namespace 'dev' read-only access to pods in that namespace. Include the ServiceAccount, a Role with get, list, watch on pods, and a RoleBinding. Explain how you would test it and why this is least privilege?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Oracle","Plaid"]},{"id":"q-2518","question":"Design a zero-trust, multi-region deployment for a real-time chat service on Kubernetes using Istio. The system must support mTLS, service-to-service authentication, PII audit logging, and RBAC for on-call engineers. Describe namespace layout, policy enforcement, DR strategy, and how you’d validate SLOs during a regional outage?","channel":"kcsa","subChannel":"general","difficulty":"advanced","tags":["kcsa"],"companies":["Square","Zoom"]},{"id":"q-2612","question":"In a Kubernetes cluster, a microservice named 'orders' communicates with a MongoDB instance. Describe a beginner-friendly RBAC and Secret strategy to grant only this pod access to MongoDB credentials stored in a Kubernetes Secret. Include minimal YAML references (namespace, ServiceAccount, Role/RoleBinding, Secret mount), explain rotation, and how you would verify a pod can connect without leaking credentials?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Discord","MongoDB","NVIDIA"]},{"id":"q-2674","question":"In a **KCSA** beginner scenario, a namespace hosts a small web app on **Kubernetes**. Under load, pods restart and 5xx errors appear. Describe a concrete, step‑by‑step debugging plan using real commands (kubectl, logs, metrics) and show how you would fix resource limits, readiness/liveness probes, and a minimal **NetworkPolicy** to improve security?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Cloudflare","LinkedIn","NVIDIA"]},{"id":"q-836","question":"You are analyzing a web app's login events, captured as an array of objects: { userId: string, ts: string (ISO 8601) }. Write a JavaScript function that returns the top 3 users by login count in the past 24 hours. Tie-break with alphabetical userId. Provide a concise implementation and explain its time complexity?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Meta","NVIDIA"]},{"id":"q-869","question":"In a high-traffic search suggestions feature, implement a Node.js module that debounces input by 300ms and caches per-query results in memory. Show cache invalidation and handle concurrent requests for the same key without duplicating work. Provide a compact, testable example with usage?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Apple","Lyft","Zoom"]},{"id":"q-912","question":"**KCSA Beginner Question: Inventory Pagination** Scenario: In a warehouse app, design a minimal endpoint GET /items?limit=&offset= that returns items ordered by id, supports pagination, and scales with 10k+ rows. Explain data model, indexing, and error handling. How would you implement a function to validate and apply LIMIT/OFFSET in code, ensuring correctness?","channel":"kcsa","subChannel":"general","difficulty":"beginner","tags":["kcsa"],"companies":["Amazon","Apple"]},{"id":"q-947","question":"You're building a fintech API used by wallets and partner services to transfer funds; describe a secure, scalable auth and data-protection design for REST endpoints, including per-partner API keys, short-lived access tokens with rotating signing keys, mTLS between services, replay protection, and observability to detect abuse, plus failure modes and trade-offs?","channel":"kcsa","subChannel":"general","difficulty":"intermediate","tags":["kcsa"],"companies":["Coinbase","PayPal","Plaid"]},{"id":"gh-56","question":"What are the key deployment strategies in Kubernetes, and how do you configure them considering resource limits, health checks, and rollback scenarios?","channel":"kubernetes","subChannel":"deployments","difficulty":"intermediate","tags":["automation","tools"],"companies":["Amazon","Google","Hashicorp","Microsoft","Netflix","Salesforce"]},{"id":"gh-7","question":"What is Kubernetes and how does it orchestrate containerized applications at scale?","channel":"kubernetes","subChannel":"deployments","difficulty":"beginner","tags":["k8s","orchestration"],"companies":["Airbnb","Amazon","Google","Microsoft","Uber"]},{"id":"gh-8","question":"Design a highly available Kubernetes cluster architecture. What are the main components, their interactions, and how do you ensure 99.95% uptime across multiple availability zones?","channel":"kubernetes","subChannel":"deployments","difficulty":"intermediate","tags":["k8s","orchestration"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-306","question":"How would you implement a canary deployment strategy in Kubernetes to minimize risk during application updates?","channel":"kubernetes","subChannel":"deployments","difficulty":"advanced","tags":["rolling-update","canary","blue-green"],"companies":["Amazon","Google","Meta"]},{"id":"q-334","question":"You're deploying a new version of a microservice in Kubernetes. Describe how you would perform a rolling update and what would you do if the new version starts failing health checks?","channel":"kubernetes","subChannel":"deployments","difficulty":"beginner","tags":["rolling-update","canary","blue-green"],"companies":["Amazon","Cisco","Google","Microsoft","Netflix","New Relic","Stripe"]},{"id":"q-369","question":"You're deploying a critical video processing service at Zoom. During a rolling update, 30% of users experience degraded performance while the new version is being deployed. How would you diagnose and resolve this issue, and what deployment strategy would you recommend instead?","channel":"kubernetes","subChannel":"deployments","difficulty":"intermediate","tags":["rolling-update","canary","blue-green"],"companies":["MongoDB","New Relic","Zoom"]},{"id":"q-465","question":"You're running a production Kubernetes cluster and notice pods are frequently getting OOMKilled despite having sufficient memory limits. How would you diagnose and resolve this issue?","channel":"kubernetes","subChannel":"general","difficulty":"intermediate","tags":["kubernetes"],"companies":["Adobe","Meta","Square"]},{"id":"q-526","question":"You're running a production Kubernetes cluster with 1000+ pods. Your monitoring shows that certain nodes are experiencing high memory pressure, causing pod evictions. How would you diagnose and resolve this issue systematically?","channel":"kubernetes","subChannel":"general","difficulty":"advanced","tags":["kubernetes"],"companies":["Bloomberg","Snap","Two Sigma"]},{"id":"q-552","question":"You're running a production Kubernetes cluster at scale and notice that some pods are experiencing intermittent network timeouts. How would you diagnose and resolve this issue, considering both application-level and cluster-level networking components?","channel":"kubernetes","subChannel":"general","difficulty":"advanced","tags":["kubernetes"],"companies":["Citadel","NVIDIA","Tesla"]},{"id":"q-579","question":"How would you debug a pod that's stuck in CrashLoopBackOff state in a production Kubernetes cluster?","channel":"kubernetes","subChannel":"general","difficulty":"intermediate","tags":["kubernetes"],"companies":["Cloudflare","Tesla"]},{"id":"de-135","question":"You have a Helm chart that needs to deploy different configurations for staging and production environments. The staging environment should use 2 replicas with 512Mi memory limit, while production should use 5 replicas with 2Gi memory limit. How would you structure your values files and templates to handle this requirement?","channel":"kubernetes","subChannel":"helm","difficulty":"intermediate","tags":["helm","k8s"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-49","question":"How does Helm manage Kubernetes application lifecycle through charts, releases, and templates?","channel":"kubernetes","subChannel":"helm","difficulty":"advanced","tags":["k8s","advanced"],"companies":["Amazon","Apple","Google","Hashicorp","Microsoft","Netflix"]},{"id":"q-400","question":"You're deploying a microservice using Helm and notice that the pod keeps crashing with 'ImagePullBackOff' error. The values.yaml specifies 'image.repository: my-service' and 'image.tag: latest'. How would you debug this issue and what's the proper way to configure image pull policies in production?","channel":"kubernetes","subChannel":"helm","difficulty":"intermediate","tags":["charts","values","templating"],"companies":["Adobe","Amazon","Google","Hashicorp","Jane Street","Microsoft","Netflix","Snowflake"]},{"id":"gh-55","question":"How does Tekton provide a cloud-native framework for building CI/CD pipelines on Kubernetes?","channel":"kubernetes","subChannel":"operators","difficulty":"beginner","tags":["automation","tools"],"companies":["Digitalocean","Google","IBM","Microsoft","Red Hat"]},{"id":"q-193","question":"What is the role of a Custom Resource Definition (CRD) in a Kubernetes Operator and how does it enable custom functionality?","channel":"kubernetes","subChannel":"operators","difficulty":"beginner","tags":["crds","controllers","reconciliation"],"companies":["Amazon","Datadog","Google","Microsoft","Prove"]},{"id":"q-291","question":"What is the role of a reconciliation loop in a Kubernetes operator controller?","channel":"kubernetes","subChannel":"operators","difficulty":"beginner","tags":["crds","controllers","reconciliation"],"companies":["Amazon","Google","Meta"]},{"id":"q-346","question":"You're building a Kubernetes operator for a custom resource that manages a fleet of microservices. Your controller is experiencing high memory usage and slow reconciliation loops. How would you design a solution to handle 10,000+ custom resources efficiently while ensuring proper event handling and preventing resource leaks?","channel":"kubernetes","subChannel":"operators","difficulty":"advanced","tags":["crds","controllers","reconciliation"],"companies":["Amazon","Cloudflare","Gitlab","Google","Hashicorp","Microsoft","MongoDB","Workday"]},{"id":"q-383","question":"You're building a Kubernetes operator for a custom database CRD. During reconciliation, you notice the controller is constantly updating the status even when no actual changes occur. How would you implement proper change detection and prevent unnecessary updates?","channel":"kubernetes","subChannel":"operators","difficulty":"intermediate","tags":["crds","controllers","reconciliation"],"companies":["AMD","Google","OpenAI"]},{"id":"gh-100","question":"What is a Sidecar Pattern in Kubernetes?","channel":"kubernetes","subChannel":"pods","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-48","question":"Explain how DaemonSets ensure pod distribution across Kubernetes nodes and describe the controller reconciliation loop that maintains this guarantee?","channel":"kubernetes","subChannel":"pods","difficulty":"advanced","tags":["k8s","advanced"],"companies":["Amazon","Google","Hashicorp","Microsoft","Netflix","Snowflake"]},{"id":"gh-51","question":"What is Container Runtime Interface (CRI) and why is it important in Kubernetes?","channel":"kubernetes","subChannel":"pods","difficulty":"advanced","tags":["k8s","advanced"],"companies":["Amazon","Google","Microsoft"]},{"id":"gh-9","question":"What is a Pod in Kubernetes and why is it considered the smallest deployable unit?","channel":"kubernetes","subChannel":"pods","difficulty":"beginner","tags":["k8s","orchestration"],"companies":["Amazon","Google","LinkedIn","Microsoft","Uber"]},{"id":"q-173","question":"What is a Kubernetes Pod and what is its primary purpose?","channel":"kubernetes","subChannel":"pods","difficulty":"beginner","tags":["pods","containers"],"companies":null},{"id":"q-245","question":"How do init containers differ from sidecar containers in Kubernetes pod lifecycle and resource sharing patterns?","channel":"kubernetes","subChannel":"pods","difficulty":"beginner","tags":["containers","init-containers","sidecars"],"companies":["Amazon","Databricks","Google","Microsoft","Netflix","Stripe"]},{"id":"q-271","question":"Design a zero-downtime database migration system using Kubernetes multi-container pods with init containers, sidecars, and shared volumes. How would you handle schema validation, migration execution, rollback, and coordination while maintaining service availability?","channel":"kubernetes","subChannel":"pods","difficulty":"intermediate","tags":["containers","init-containers","sidecars"],"companies":["Amazon","Databricks","Google","Microsoft","Netflix","Stripe"]},{"id":"q-356","question":"You're deploying a security scanning sidecar with a main application pod. The sidecar must complete its vulnerability scan before the main container starts, then continue monitoring runtime threats. Design this pod configuration with shared volumes, health checks, and graceful shutdown. What key components ensure the security scanning completes before application startup?","channel":"kubernetes","subChannel":"pods","difficulty":"intermediate","tags":["containers","init-containers","sidecars"],"companies":null},{"id":"q-412","question":"You're deploying a web application that needs to run database migrations before the main container starts. How would you configure a Pod with an init container to handle this, and what happens if the init container fails?","channel":"kubernetes","subChannel":"pods","difficulty":"beginner","tags":["containers","init-containers","sidecars"],"companies":["Figma","NVIDIA","Okta"]},{"id":"q-511","question":"How does Kubernetes handle pod scheduling and what factors influence scheduling decisions?","channel":"kubernetes","subChannel":"pods","difficulty":"intermediate","tags":["kubernetes","scheduling","pods","resource-management","container-orchestration"],"companies":["Amazon","Google","Microsoft","Red Hat"]},{"id":"q-636","question":"What are init containers in Kubernetes and how do they differ from regular containers?","channel":"kubernetes","subChannel":"pods","difficulty":"intermediate","tags":["kubernetes","containers","pod-lifecycle","initialization"],"companies":["Amazon","Google","Microsoft"]},{"id":"q-637","question":"What are the key differences between init containers, sidecar containers, and static pods in Kubernetes?","channel":"kubernetes","subChannel":"pods","difficulty":"intermediate","tags":["kubernetes","containers","pod-lifecycle","deployment-patterns"],"companies":["Amazon","Google","Microsoft","Red Hat"]},{"id":"gh-101","question":"What is a Service Mesh Control Plane and how does it manage microservices communication?","channel":"kubernetes","subChannel":"services","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Salesforce","Uber"]},{"id":"gh-50","question":"How does Istio implement service mesh architecture using sidecar proxies, and what are the key components for traffic management, security, and observability?","channel":"kubernetes","subChannel":"services","difficulty":"advanced","tags":["k8s","advanced"],"companies":null},{"id":"q-219","question":"How would you design a zero-downtime service migration strategy using Kubernetes Service selectors and Endpoints controller to avoid connection drops during rolling updates?","channel":"kubernetes","subChannel":"services","difficulty":"advanced","tags":["clusterip","nodeport","loadbalancer","ingress"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-320","question":"You have a microservice deployed in Kubernetes that needs to be accessible both internally within the cluster and externally via a custom domain. How would you configure the service and ingress to achieve this, and what are the trade-offs between using ClusterIP, NodePort, and LoadBalancer service types?","channel":"kubernetes","subChannel":"services","difficulty":"advanced","tags":["clusterip","nodeport","loadbalancer","ingress"],"companies":["Elastic","Snowflake","Zoom"]},{"id":"gh-26","question":"What are the essential Linux commands every DevOps engineer should master for system administration, troubleshooting, and automation?","channel":"linux","subChannel":"commands","difficulty":"beginner","tags":["linux","shell"],"companies":["Amazon","Cloudflare","Google","Hashicorp","Microsoft","Netflix"]},{"id":"q-1000","question":"A Linux host runs several Docker containers; during peak load, API responses slow and some requests time out. Describe a beginner-friendly, concrete diagnostic workflow to (1) confirm whether host saturation is CPU, memory, or I/O, (2) identify the container most responsible, and (3) apply a safe mitigation (e.g., graceful restart or CPU/memory throttling) while monitoring impact. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Instacart","LinkedIn"]},{"id":"q-1142","question":"Scenario: A Linux server hosting a small web service used by customers suddenly shows disk usage climbing on the root filesystem. Provide a beginner-friendly, concrete diagnostic workflow to (1) locate the directories/files consuming the most space, (2) identify top offenders, and (3) apply a safe mitigation (e.g., rotate/compress/archive logs or purge old data) while keeping the service up. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["DoorDash","Google","Salesforce"]},{"id":"q-1205","question":"You're operating a fleet of Linux hosts running a low-latency web API. Intermittent 100–200 ms latency spikes appear under load. Design a practical, end-to-end diagnostic plan using eBPF/BPFtrace or perf, iostat/vmstat, and container metrics to collect data, identify root cause (CPU scheduling, IO wait, or network), and propose minimal-disruption mitigations?","channel":"linux","subChannel":"general","difficulty":"advanced","tags":["linux"],"companies":["Salesforce","Snap"]},{"id":"q-1244","question":"Scenario: A Linux host runs a MongoDB primary in a high-traffic cluster. During peak hours, latency spikes and tail latency increases while CPU and memory appear stable. Using only default tooling, no downtime, describe a concrete, actionable diagnostic workflow to (1) determine if I/O wait, CPU, or memory is the bottleneck, (2) pinpoint the offending disk/device or process, and (3) apply a safe mitigation (e.g., adjust I/O scheduler, tune dirty writeback parameters, or throttle MongoDB) while monitoring impact. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Adobe","Databricks","MongoDB"]},{"id":"q-1266","question":"Context: Linux node in a Kubernetes cluster hosting a high-throughput data ingestion service. Intermittent tail latency spikes (>1s) appear during peak traffic, affecting processing. Without downtime, design a concrete troubleshooting workflow to (1) confirm whether CPU, I/O, or network is the bottleneck, (2) identify the exact subsystem or process responsible, and (3) implement a safe mitigation with minimal impact while maintaining observability. Include exact commands and realistic outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Airbnb","Anthropic","Snowflake"]},{"id":"q-1364","question":"A Linux host running multiple microservices behind NGINX exhibits intermittent latency spikes; new connections occasionally fail and FD usage appears high. Using only default tools, describe a beginner-friendly workflow to (1) confirm FD exhaustion is the bottleneck, (2) identify the offending process by per‑process FD usage, and (3) apply a safe mitigation (increase NOFILE limits, adjust per-service limits, or set a systemd limit) while monitoring impact. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Hugging Face","PayPal","Tesla"]},{"id":"q-1410","question":"On a Linux host, a data-processing job occasionally stalls during bursts. Using only default tooling and no downtime, outline a concrete, beginner-friendly diagnostic flow to (1) confirm CPU, memory, or I/O bound, (2) identify the offending process and its operation, and (3) apply a safe mitigation (e.g., adjust priority, pause/resume, or throttle) with monitoring. Which commands would you run and what outputs would you expect?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Instacart","OpenAI","Square"]},{"id":"q-1424","question":"Scenario: a high-traffic Linux service behind a reverse proxy experiences sporadic tail latency spikes during peak hours. CPU and IO look normal in aggregate. Design a practical end-to-end diagnostic using eBPF to identify root causes quickly: (1) instrument per-request latency across kernel and user-space, (2) attribute latency to network, disk, or app code, (3) propose safe mitigations and validate impact. Include specific probes, sample commands, and expected outputs?","channel":"linux","subChannel":"general","difficulty":"advanced","tags":["linux"],"companies":["Citadel","Cloudflare","Twitter"]},{"id":"q-1445","question":"A Linux host running GPU-accelerated video processing containers shows intermittent frame latency spikes during peak load, with no obvious CPU/memory/I/O bottlenecks. Without downtime, describe a concrete, real-world diagnostic workflow using only default tools to (1) verify if latency is caused by page cache pressure or Transparent Huge Pages, (2) identify the subsystem or container involved, and (3) apply a safe mitigation (e.g., disable THP on affected NUMA nodes, tune swappiness, or pin tasks) with minimal impact and observable results. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Amazon","DoorDash","NVIDIA"]},{"id":"q-1570","question":"A Linux CI node running multiple git builds experiences occasional stalls during parallel jobs. Using only default tooling and no downtime, describe a concrete diagnostic workflow to (1) determine if CPU, memory, or I/O is the bottleneck, (2) identify the specific component (e.g., git, filesystem, network) causing the stall, and (3) apply a safe mitigation (e.g., throttle parallel jobs, adjust I/O scheduler, or raise file descriptor limits) while monitoring impact. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["IBM","Tesla","Uber"]},{"id":"q-1705","question":"You have a Linux host running a Rust-based data-processing daemon that stalls for 30–60 seconds under peak load. Using only default tooling, design a concrete diagnostic workflow to (1) determine if the stall is CPU-, I/O-, or memory-bound, (2) identify the exact subsystem or process causing the stall, and (3) apply a safe mitigation (e.g., cgroup throttling or IO-scheduler tweaks) while preserving observability. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Google","Hugging Face","Lyft"]},{"id":"q-1873","question":"Context: A Linux host runs a Kubernetes-deployed real-time inference service behind Nginx; latency tail spikes occur during bursts. Without downtime, describe a concrete, repeatable workflow to determine whether latency is caused by CPU throttling, memory pressure, or network queueing, identify the offending container/pod, and apply a safe mitigation (e.g., adjust cgroup limits, scale the deployment, or tune kernel parameters) while preserving observability. Include exact commands, expected outputs, and a simple rollback plan?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Airbnb","Hugging Face","Snap"]},{"id":"q-2038","question":"Scenario: A Linux host running a batch queue occasionally fails to start new jobs with 'Too many open files' under moderate load. Without downtime, describe a concrete, beginner-friendly diagnostic workflow using default tools to (1) confirm FD limits are the bottleneck, (2) identify the process or user hitting the limit, and (3) apply a safe mitigation (e.g., raise per-user limits, adjust LimitNOFILE for the service) while keeping service available. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Amazon","Goldman Sachs","Salesforce"]},{"id":"q-2273","question":"Context: A Linux server hosts a real-time ingestion service and a Redis cache on the same box. During peak ingestion, tail latency spikes. Using only default tools, outline a concrete, reproducible workflow to (1) confirm whether CPU, IO, memory, or network is the bottleneck, (2) pinpoint the offender process or disk, and (3) apply a safe mitigation with minimal disruption. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["MongoDB","Slack","Snap"]},{"id":"q-2318","question":"Scenario: A Linux host in a high-throughput UDP ingestion pipeline experiences random latency spikes under peak traffic. Using only default tooling, design a concrete diagnostic workflow to (1) determine whether NIC, IRQs, or queue saturation is the bottleneck, (2) identify the offending interface/driver and IRQ affinity, and (3) apply a safe mitigation (e.g., adjust IRQ affinity, enable RSS, increase Rx queue depth) while preserving observability. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"advanced","tags":["linux"],"companies":["Databricks","Goldman Sachs","MongoDB"]},{"id":"q-2353","question":"A Linux host runs three LXC containers. During peak hours, one container's API latency spikes while others stay responsive. Using only default tooling, describe a concrete, beginner-friendly diagnostic workflow to (1) determine if bottleneck is CPU, memory, or I/O at host or container level, (2) identify the container and the exact process responsible for the spike, and (3) propose a safe mitigation (e.g., adjust container limits, throttle I/O, or relocate service) with observability. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Hashicorp","OpenAI","Snowflake"]},{"id":"q-2426","question":"Context: A Linux host runs multiple containers on a single NIC behind a load balancer. During a traffic spike, a critical API shows increased latency and occasional 5xx errors. Using only default Linux tools, outline a concrete, practical diagnostic workflow to (1) determine whether latency comes from CPU, memory pressure, disk I/O, or network contention, (2) identify the exact container or process responsible, and (3) apply a safe mitigation (e.g., throttle or move to a different cgroup, adjust IO scheduler) with minimal disruption and full observability. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Google","Netflix","Uber"]},{"id":"q-2598","question":"On a Linux host running a high-throughput data-ingestion service, intermittent 5–10s stalls appear under sustained load. Using only default tooling, design a concrete diagnostic workflow to determine whether stalls are caused by Transparent Huge Pages, memory fragmentation, or IO pressure; identify the exact subsystem responsible; and apply a safe mitigation (e.g., disable THP, adjust NUMA binding) while preserving observability. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"advanced","tags":["linux"],"companies":["Coinbase","Databricks","IBM"]},{"id":"q-466","question":"You're debugging a production Linux server where processes are randomly dying with 'Out of memory' errors, but `free -m` shows 8GB available RAM. How would you diagnose and fix this issue?","channel":"linux","subChannel":"general","difficulty":"advanced","tags":["linux"],"companies":["Amazon","Google","Tesla"]},{"id":"q-496","question":"How would you find all processes running on port 8080 and terminate them safely?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Apple","Google"]},{"id":"q-527","question":"How would you find and kill a process that's using port 8080 on a Linux system?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Amazon","Oracle","Two Sigma"]},{"id":"q-553","question":"You're troubleshooting a production server where a critical process keeps getting killed. How would you diagnose if it's an OOM kill versus other issues, and what specific commands would you use to investigate?","channel":"linux","subChannel":"general","difficulty":"intermediate","tags":["linux"],"companies":["Databricks","Scale Ai","Snowflake"]},{"id":"q-580","question":"How would you find all processes using a specific port and terminate one safely?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Hashicorp","MongoDB","Twitter"]},{"id":"q-917","question":"Scenario: a Linux server hosting a web app experiences sporadic high response times during peak hours. Using only default tools and no downtime, describe a concrete, beginner-friendly diagnostic workflow to (1) determine whether CPU, memory, or I/O is the bottleneck, (2) identify the offending process, and (3) apply a safe mitigation (e.g., graceful restart) while monitoring impact. Include exact commands and expected outputs?","channel":"linux","subChannel":"general","difficulty":"beginner","tags":["linux"],"companies":["Microsoft","PayPal","Tesla"]},{"id":"q-1044","question":"On a Linux host running multiple tenant services, peak I/O from a data ingestion daemon saturates the disk, causing latency spikes for others. Propose a production plan to isolate and throttle disk I/O across tenants using systemd slices and cgroup v2 io.max. Include concrete unit and slice snippets, per-device limits for a SATA HDD and NVMe, and a test plan using fio and iostat to verify tail latency improvements under concurrent workloads?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Databricks","Microsoft","Twitter"]},{"id":"q-1056","question":"On a Linux host, a TLS proxy reads its certificate from /etc/ssl/certs/app.crt and currently reloads by restarting, causing brief downtime during renewal. Propose a zero-downtime rotation using a systemd.path trigger that fires on a new cert symlink, with a separate service unit for ExecReload and an atomic update scheme (swap in a new cert, then switch a symlink). Include concrete unit snippets and a test plan?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Apple","Two Sigma"]},{"id":"q-1122","question":"On a Linux host running multiple ML model servers in separate systemd slices using cgroup v2, a spike in one tenant consumes memory and triggers host memory pressure despite per-tenant limits. Propose a production plan to enforce strict isolation and backpressure using memory.max and memory.high, enable group OOM behavior, and implement eviction/playback strategies. Include concrete unit and cgroup settings and a test plan with a reproducible spike and validation steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Hugging Face","MongoDB","Tesla"]},{"id":"q-1230","question":"On a Linux host with a multi-tenant workload sharing a single 10GbE NIC configured with multiple receive queues, one tenant bursts UDP traffic and starves others, causing increased latency and packet loss. Propose a production plan to enforce tenant isolation and fairness using NIC multi-queue, RSS/XPS mapping, IRQ affinity, and per-tenant cgroups (v2) with io.max and tc shaping. Include concrete steps and a test plan with realistic traffic?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["DoorDash","MongoDB","Twitter"]},{"id":"q-1327","question":"On a Linux host running multiple tenants via containers, a CPU-intensive workload from one tenant starves others. Propose a production plan to enforce CPU isolation using cgroup v2 slices, systemd integration, and per-tenant quotas; include concrete settings (cpu.max, cpu.weight), scheduling options (cpuset and isolcpus), and a test plan with realistic workloads and verification steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Bloomberg","Google","IBM"]},{"id":"q-1346","question":"On a production Linux host serving multiple ephemeral monitoring daemons, a heartbeat worker must emit a 5-second ping to a central collector. During peak load, the heartbeat misses several intervals, delaying alerts. Propose a concrete production plan to guarantee cadence and resilience using: (a) per-service CPU isolation and pinning, (b) high-priority timer/RT scheduling or systemd timers with precise tuning, (c) a fallback mechanism for missed heartbeats, and (d) a validation strategy with a controlled load. Include concrete config snippets for systemd, cpuset, and timer settings?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Citadel","LinkedIn"]},{"id":"q-1385","question":"On a Linux host running ephemeral build jobs managed by systemd, a rogue job floods /tmp with tiny files, exhausting inodes and delaying builds. Propose a production plan to isolate and bound /tmp usage and auto-clean stale files. Include: (a) per-service PrivateTmp and tmpfs /tmp with a size cap; (b) systemd-tmpfiles cleanup rules; (c) inode usage monitoring; (d) a reproducible test that proves isolation and cleanup. How would you implement this?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Anthropic","Coinbase","MongoDB"]},{"id":"q-1490","question":"On a Linux server, a small web app runs as a systemd service and reads its port and log level from /etc/myapp/config.yaml. Changes to this file must apply without dropping connections. Design a production-safe hot-reload mechanism using systemd ExecReload, a SIGHUP-based reload, and a small validation script. Include unit file snippets and a test plan with a reproducible config change and verification steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["DoorDash","Google"]},{"id":"q-1497","question":"On a Linux host running multiple tenants in systemd services sharing a single NVMe SSD and a 25Gbps NIC, a new requirement enforces strict I/O isolation to prevent a noisy tenant from stalling others. Propose a production plan to enforce per-tenant I/O isolation and backpressure using: (a) cgroup v2 io.max per service; (b) per-tenant filesystem isolation via PrivateMounts and PrivateDevices with a dedicated data path; (c) I/O scheduler tuning (none/deadline) and per-tenant block IO queuing discipline with tc; (d) a deterministic test plan with a reproducible burst scenario and rollback steps. Include concrete config snippets for systemd units and cgroups?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Discord","MongoDB"]},{"id":"q-1588","question":"On a dual-socket Linux host running PostgreSQL and a data-processor ETL job, both services contend for memory and cache across NUMA nodes; design an advanced plan to enforce NUMA-aware isolation with cpuset/memcg, disable NUMA balancing, set per-node IRQ affinity, and validate via targeted benchmarks. Include concrete commands and a test plan?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Coinbase","Twitter"]},{"id":"q-1665","question":"On a Linux server hosting a latency-sensitive database and a heavy batch analytics job that share a single 2-socket NUMA node and HDD I/O, propose a production plan to guarantee tail latency (P95/P99) for the database while allowing analytics to finish within a bounded window. Include NUMA memory binding, CPU pinning via cpuset/systemd slices, I/O throttling via cgroup v2 io.max, IRQ affinity, and a validation test plan with realistic workloads?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Apple","Snap"]},{"id":"q-1700","question":"On a Linux server running several io_uring-based data ingestors, one worker's backlog bursts, triggering tail latency spikes for others. Propose a production plan to guarantee per-worker latency and fairness using: (a) per-worker io_uring ring sizing and SQ depth; (b) per-worker cgroups v2 with io.max and task limits; (c) backpressure and pacing (budget tokens, per-worker deadlines); (d) monitoring and a reproducible test plan to validate latency under burst?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Apple","Snowflake"]},{"id":"q-1774","question":"On a Linux host, a file-watcher daemon uses inotify to monitor a large directory tree. As load grows, events start being missed and the daemon lags. Propose a production plan to keep event delivery reliable: tune inotify limits, raise file descriptor limits for the service, add a polling fallback or batching, and validate with a reproducible test that simulates churn?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Instacart","Oracle","Snowflake"]},{"id":"q-1807","question":"On a Linux host running a hot-reload deployment workflow for a critical service, mid-deploy tampering could swap the binary between copy and start, breaking integrity. Design a production-grade binary integrity strategy using Linux IMA with runtime attestation. Include how to sign binaries in CI, a signed binary repository, systemd integration (ExecStartPre), policy placement, rollback tests, and how you'd validate against tampering during deployment?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Coinbase","MongoDB"]},{"id":"q-1939","question":"On a Linux host running a web stack (Nginx proxy to a Node/Go app), the app writes to /var/log/webapp/app.log and /var/log/webapp/access.log. Bursts fill the disk and logrotation sometimes fails because the app keeps logs open. Propose a production-grade plan to ensure reliable log rotation with no data loss and no disk-full events. Include concrete logrotate config (copytruncate vs postrotate), systemd unit tweaks (Restart, ExecReload), filesystem layout advice, and a test plan to reproduce a burst and verify rotation completes without downtime?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Cloudflare","Stripe","Uber"]},{"id":"q-1954","question":"On a Linux host running three CI runners as systemd services sharing a single NVMe and a 10 Gb NIC, a long-running build on one runner starves CPU and I/O, delaying others. Propose a production plan to guarantee fair CPU and I/O while preserving peak throughput: (a) per-service CPU limits via cgroup v2 and CPU affinity; (b) per-service I/O throttling with io.max and a suitable I/O scheduler (BFQ/mq-deadline); (c) CPU pinning and IRQ isolation; (d) validation with bursts and latency metrics?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Anthropic","Apple","Databricks"]},{"id":"q-1987","question":"On a Linux host running CI jobs in rootless containers (e.g., Podman) shared across teams, a misconfigured container could attempt host escape. Propose a production hardening plan using user namespaces, rootless mode, Seccomp and AppArmor, and a tight per-job capset + cgroupv2 quotas. Include concrete commands and a rollback plan?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["IBM","Square","Twitter"]},{"id":"q-2014","question":"On a Linux host running multiple GPU-accelerated services in separate systemd slices under containerized workloads, occasional CPU contention causes tail latency spikes and timeouts for one service. Design a production plan to bound latency and preserve throughput without service interruption. Include per-slice isolation (isolcpus, cpuset), systemd slice configuration, cpu.max, and a kernel I/O scheduler tuning (mq-deadline), plus a reproducible test plan with mixed workloads and latency verification steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Adobe","Instacart","OpenAI"]},{"id":"q-2167","question":"On a KVM host running dozens of VMs with memory ballooning enabled, a burst in guest memory growth triggers host memory pressure and occasional OOM events. Propose a production-safe plan to cap balloon aggressiveness, enforce host backpressure, and prevent thrash, using libvirt XML (memory.size/currentMemory, memoryBacking, balloon device) and cgroup v2 memory.max/memory.high, plus a test plan with a reproducible spike and verification steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Google","Instacart","Snap"]},{"id":"q-2223","question":"On a Linux host running multiple CI jobs in containers, CPU contention causes builds to time out. Propose a production-ready plan to bound CPU for CI workloads using cgroup v2 and systemd slices, including concrete unit settings and a test plan with a CPU-bound load?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Citadel","Snowflake","Square"]},{"id":"q-2294","question":"On a Linux host running multi-tenant, rootless containers for isolation, a rogue tenant's DNS resolver leaks queries and inflates latency for others. Propose a production plan to enforce strict per-tenant DNS isolation without compromising performance. Include concrete steps for: (a) per-tenant network namespaces and DNS forwarders, (b) nftables rules to confine DNS to tenant resolvers, (c) container runtime config to prevent cross-tenant DNS leakage, and (d) a reproducible test plan to validate isolation under bursty DNS load?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Adobe","Citadel","Cloudflare"]},{"id":"q-2515","question":"On a Linux host running a multi-tenant container workload sharing a single 25 GbE NIC, one tenant suddenly bursts writes to a shared storage and causes CPU and FD contention, threatening SLA for others. Propose a production plan to enforce strict per-tenant isolation using cgroup v2 (cpu, memory, pids, io.max) and systemd.slice, plus network QoS with tc.clsact and per-tenant TX queues. Include concrete unit snippets and a test plan with reproducible surge and SLA verification?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Databricks","Discord","Instacart"]},{"id":"q-2538","question":"On a Linux host running a multi-tenant data ingestion pipeline and a separate real-time alerting service, ingestion bursts cause alert latency. Propose a production plan to guarantee low-latency alerts while preserving ingestion throughput using: (a) per-service CPU isolation with systemd slices and cpuset, (b) RT scheduling for the alerting process, (c) CPU affinity and NUMA bindings, (d) a validation plan with burst traffic and SLA checks?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Goldman Sachs","PayPal"]},{"id":"q-2573","question":"On a Linux host running a real-time audio processing pipeline that must meet deterministic latency, a single 8-core CPU shows occasional dropouts during peak input. Design a production plan to guarantee latency isolation for the critical path: specify hardware topology, kernel boot params (isolcpus, nohz_full), IRQ affinity, CPU sets, cgroups v2 with a dedicated rt.slice using SCHED_FIFO priorities, memory locking, and a test plan with cyclictest and a synthetic workload simulating 16-channel 192 kHz audio. What exact steps would you take and how would you verify?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Citadel","Square","Two Sigma"]},{"id":"q-2632","question":"On a Linux host running a UDP telemetry ingest pipeline at high pps, tail latency spikes under peak load threaten alerts. Design a production plan to measure end-to-end latency with eBPF (tracepoints, kprobes, uprobes) and BPF maps, identify bottlenecks (kernel vs user-space), implement mitigations (backpressure, NIC tuning, IRQ affinity, NUMA pinning), and validate with a reproducible load test and SLA targets?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Google","OpenAI","Two Sigma"]},{"id":"q-2708","question":"A Linux server hosting a small web app sees frequent 'No space left on device' errors despite disk space being free; df -h shows space but df -i shows inode exhaustion. Outline a beginner-friendly plan to diagnose and fix inode exhaustion, including exact commands to identify hotspots, recommended cleanup or restructuring, and a simple test plan to reproduce and verify the fix?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Anthropic","Instacart","Salesforce"]},{"id":"q-881","question":"On a Linux host, a long-running daemon writes to `/var/log/myapp.log` and is managed by systemd, but log rotation occasionally causes logging to stop after rotation. Propose a practical fix to ensure logging continues after rotation without restarting the daemon. Include the exact approach and a sample logrotate config snippet and testing steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"beginner","tags":["linux-foundation-sysadmin"],"companies":["Apple","Meta","Plaid"]},{"id":"q-888","question":"A production Linux host runs a data-backup agent that uses /var/lib/backup/backup.lock to enforce a single instance. Sometimes a stale lock remains after a crash, blocking new runs; the agent also leaves non-terminating children on stop, risking partial backups. Propose a systemd‑based lifecycle fix: ensure one instance, auto-clean stale lock, and graceful stop with timeout and fallback to kill. Include concrete unit snippets and verification steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"advanced","tags":["linux-foundation-sysadmin"],"companies":["Goldman Sachs","Snowflake"]},{"id":"q-961","question":"On a Linux host, a memory-hungry log-harvester daemon managed by systemd sporadically triggers the kernel OOM killer during peak load, crashing the service and delaying alerts. Propose a production-safe plan to prevent OOM termination while preserving throughput. Include concrete systemd settings (MemoryLimit, MemorySwapMax, OOMScoreAdjust, Restart), kernel tuning (swap, swappiness), and a test plan with a reproducible high-memory scenario and verification steps?","channel":"linux-foundation-sysadmin","subChannel":"general","difficulty":"intermediate","tags":["linux-foundation-sysadmin"],"companies":["Databricks","Two Sigma"]},{"id":"linux-foundation-sysadmin-networking-1768260262823-4","question":"You suspect ARP storms on a host with interface eth1. To quickly verify ARP traffic and identify abnormal ARP activity on that interface, which command should you run?","channel":"linux-foundation-sysadmin","subChannel":"networking","difficulty":"intermediate","tags":["Networking","ARP","Linux","Kubernetes","certification-mcq","domain-weight-12"],"companies":null},{"id":"q-199","question":"When deploying LLM inference with vLLM and Triton Inference Server, how do you handle request batching across multiple GPU nodes while maintaining sub-100ms latency for individual requests?","channel":"llm-ops","subChannel":"deployment","difficulty":"advanced","tags":["vllm","tgi","triton","onnx"],"companies":["Amazon","Google","Meta","Microsoft","NVIDIA"]},{"id":"q-1045","question":"You're running a beginner LLM inference API used by multiple clients. Design a low-cost, safe plan to implement per-tenant model versioning and zero-downtime hot-swapping. Include routing to the correct version, how you deploy new versions, rollback strategy, and observability to verify no regressions before full rollout?","channel":"llm-ops","subChannel":"general","difficulty":"beginner","tags":["llm-ops"],"companies":["Discord","NVIDIA"]},{"id":"q-1060","question":"You're delivering a privacy-preserving, regionally scoped, multi-tenant LLM assistant used by teams across geographies. Describe an end-to-end design that ensures tenant isolation, data residency, and per-tenant policy enforcement while meeting sub-200ms latency for common prompts. Include routing, key management, redaction, auditability, and deletion workflows, plus how you'd validate compliance across regions?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["LinkedIn","NVIDIA","Zoom"]},{"id":"q-1147","question":"You're operating a multi-tenant, multi-region LLM inference service for regulated financial firms. Design an architecture that enforces per-tenant data residency and per-tenant model pools, plus dynamic model-version rollout with feature flags and safe rollback. Describe policy-driven routing, on-the-fly prompt sanitization, observability, and incident response. Include concrete data-plane components and a rollout sequence for a new model version?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Apple","Goldman Sachs","Plaid"]},{"id":"q-1263","question":"You operate a global LLM inference service across three regions and must upgrade models with zero downtime. Describe a concrete plan for rolling upgrades with canaries, traffic-splitting, warm-up, health checks, and fast rollback, ensuring SLA adherence and minimal cold-start impact during the switch?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["Databricks","Slack","Snap"]},{"id":"q-1640","question":"You're launching a beginner-friendly LLM chat in a mobile-first app. To balance latency and privacy, design an edge-first routing: small prompts stay on-device; larger or sensitive prompts go to the cloud. Describe concrete components, a simple policy rule, data flow, and how you'd test it for privacy and latency before release?","channel":"llm-ops","subChannel":"general","difficulty":"beginner","tags":["llm-ops"],"companies":["Amazon","Apple","Snap"]},{"id":"q-1695","question":"Design a compliant, auditable LLM inference pipeline for a regulated financial platform that must deliver per-transaction data lineage, prompt provenance, and immutable end-to-end logging with 24-month retention while keeping latency under 150 ms end-to-end. Describe data flows, encryption, storage, and how you test for data leakage and drift?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Airbnb","Oracle","PayPal"]},{"id":"q-1858","question":"Design a beginner-friendly chat moderation assistant for a live-stream app with edge-first routing: short prompts stay on-device; longer or risk-prone prompts go to cloud. Specify the minimal architecture, a concrete routing rule (token threshold and risk score), data flow with redaction, and a practical test plan to validate latency (<200ms on-device, <600ms cloud) and privacy?","channel":"llm-ops","subChannel":"general","difficulty":"beginner","tags":["llm-ops"],"companies":["Cloudflare","Databricks"]},{"id":"q-2003","question":"Design a tenancy-aware LLM inference layer where 60 regional tenants share a single GPU pool but must enforce per-tenant data isolation, prompt sanitization, and per-tenant model versioning. When burst traffic occurs, guarantee 95th percentile latency under 300 ms while preventing cross-tenant data leakage. Describe data flows, policy evaluation, routing, and rollback?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["Amazon","Google","IBM"]},{"id":"q-2146","question":"Design a hybrid edge-cloud LLM inference system where tenants move between corporate networks and remote locations. How would you enforce per-tenant data locality, latency budgets, model versioning, and graceful failover while keeping observability clear and simple?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["Discord","Google","Snap"]},{"id":"q-2190","question":"Design a lightweight PII redaction gate that runs before prompts reach the model in a multi-tenant LLM-ops pipeline. Describe concrete redaction rules, per-tenant policy config, data locality considerations, and how you'd validate latency and redaction accuracy with a minimal test suite?","channel":"llm-ops","subChannel":"general","difficulty":"beginner","tags":["llm-ops"],"companies":["Databricks","Goldman Sachs","Tesla"]},{"id":"q-2201","question":"Design a per-tenant, order-preserving prompt queue for a multi-tenant LLM chat backend. Ensure prompts from different tenants can be processed in parallel, but each tenant's prompts are served strictly in arrival order. Describe data models, queueing strategy, and a minimal Python asyncio prototype showing enqueue, dispatch, and a mock model call with per-tenant isolation and timeouts?","channel":"llm-ops","subChannel":"general","difficulty":"beginner","tags":["llm-ops"],"companies":["Robinhood","Salesforce","Square"]},{"id":"q-2309","question":"In a multi-tenant LLM-ops gateway used by Oracle and Apple, design a real-time policy-driven guardrail that prevents tenant data leakage via prompt injection, enforces per-tenant data locality and content policies, and streams a tamper-evident audit log. Describe data models, policy evaluation flow, latency targets (<50 ms per prompt), and a minimal Rust/protobuf prototype snippet for policy evaluation?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Apple","Oracle"]},{"id":"q-2339","question":"In a multi-tenant LLM-ops platform that supports dynamic model tiering (tiny/fast vs large/accurate) with per-tenant QoS budgets, how would you design the model selection, routing, and accounting to meet SLAs while minimizing cross-tenant warmups? Include the data model, API surface, and a minimal config snippet showing how tenants express budgets and tier preferences?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Airbnb","MongoDB","Square"]},{"id":"q-2407","question":"Design a per-brand, hot-swappable steering layer between gateway and models. JSON-based policies enforce safety and tone per brand; compute a per-prompt risk score in under 20 ms; policy versions are immutable with a blue/green rollout for rollback; describe data models, policy evaluation flow, rollback strategy, and a minimal Rust/WASM prototype for policy evaluation?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Adobe","Instacart","Lyft"]},{"id":"q-467","question":"You're deploying a LLM inference service that must handle 10,000 RPS with <100ms latency. How would you design the architecture to balance cost, performance, and reliability?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["Discord","Microsoft","Netflix"]},{"id":"q-497","question":"How would you design a distributed inference serving system for LLMs that handles 100K RPS with sub-100ms latency while managing GPU memory fragmentation and ensuring high availability?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Hugging Face","Tesla"]},{"id":"q-528","question":"You're deploying a LLM inference service that must handle 1000 concurrent requests with <500ms latency. Your current setup uses a single GPU with vLLM. How would you architect the system to meet these requirements?","channel":"llm-ops","subChannel":"general","difficulty":"intermediate","tags":["llm-ops"],"companies":["Airbnb","Hugging Face","Robinhood"]},{"id":"q-554","question":"You're deploying a multi-tenant LLM inference service that must handle 10,000 concurrent requests with sub-100ms latency. How would you design the request routing, model loading strategy, and autoscaling to meet these SLAs while optimizing GPU utilization?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["DoorDash","IBM"]},{"id":"q-581","question":"How would you design a production LLM inference pipeline that handles 10K RPS with sub-200ms latency while managing GPU memory fragmentation and cold start issues?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Airbnb","Google"]},{"id":"q-849","question":"You operate a dual-tenant LLM inference service for sensitive internal docs and external users. Design a policy-driven routing and isolation architecture that guarantees tenant data separation, per-tenant model pools, on-the-fly prompt sanitization, and strict latency budgets under burst traffic. Include observability, data handling, and fail-open/closed strategies?","channel":"llm-ops","subChannel":"general","difficulty":"advanced","tags":["llm-ops"],"companies":["Cloudflare","Google","Instacart"]},{"id":"q-252","question":"What are the key techniques and trade-offs for optimizing large language models in production, including quantization strategies and their impact on performance?","channel":"llm-ops","subChannel":"optimization","difficulty":"beginner","tags":["quantization","pruning","distillation"],"companies":["Amazon","Apple","Google","Meta","Microsoft","NVIDIA"]},{"id":"q-422","question":"You're building a production LLM service handling 10K requests/second with transformer models experiencing memory spikes during multi-head attention. How would you optimize memory usage while maintaining throughput and latency requirements?","channel":"llm-ops","subChannel":"optimization","difficulty":"advanced","tags":["transformer","attention","tokenization"],"companies":null},{"id":"q-1177","question":"Scenario: a high-throughput edge service must enforce precise timeouts for thousands of connections. Design a lock-free, per-core timer wheel that manages up to 1,000,000 timers with microsecond granularity on a 4-socket server. API: add_timer(id, due_us, cb, ctx), cancel_timer(id), tick(). Requirements: no global locks, handle cancellation safely, cache-friendly layout, and crash-safe recovery. Include pseudo-code for add_timer, cancel_timer, and tick, plus a microbenchmark plan?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["DoorDash","Robinhood","Tesla"]},{"id":"q-1261","question":"Implement a cache-friendly transpose using tile-based approach for an N×N matrix with N a power of two. Provide a function to transpose A into B using 32×32 blocks, and a minimal test harness validating correctness. Explain how tiling reduces cache misses and how to pick block size relative to L1/L2 caches. Include a microbenchmark plan comparing with a naive transpose?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Microsoft","Plaid","Uber"]},{"id":"q-2105","question":"Design and implement a NUMA-aware per-thread arena allocator in C for a 2-socket server. Features: per-core cache-line-aligned pools, 128-byte padding to prevent false sharing, fast paths for 8/16/32-byte blocks, per-size freelists, and a cross-thread deallocation quarantine. Provide API (init, alloc, free, destroy) plus a minimal test harness and a microbenchmark plan comparing intra- vs inter-NUMA performance against malloc?","channel":"low-level","subChannel":"general","difficulty":"intermediate","tags":["low-level"],"companies":["Databricks","Two Sigma","Uber"]},{"id":"q-2340","question":"Design and implement a tiny fixed-block allocator in C for a 64KB arena to serve 32-byte objects. Provide API: init_allocator(uint8_t* arena, size_t size), void* alloc32(), void free32(void*). Use an in-block free list (8-byte header) and 24-byte payload per block, with the arena head at offset 0. Explain alignment, fragmentation, and how it compares to malloc. Include a minimal test plan?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Goldman Sachs","Meta","Tesla"]},{"id":"q-2361","question":"Design a NUMA-aware fixed-size allocator in C++ for objects of 64 bytes, with per-NUMA-node free lists and a lightweight cross-node migration policy; describe structure, allocation/free, and how to avoid inter-node contention and false sharing; provide a small code sketch and a validation plan?","channel":"low-level","subChannel":"general","difficulty":"intermediate","tags":["low-level"],"companies":["Robinhood","Two Sigma"]},{"id":"q-2448","question":"Design a NUMA-aware, lock-free work-stealing deque with per-NUMA-node local queues and cross-node stealing to sustain high throughput on a 2-socket server. Provide per-slot layout, push/pop/steal pseudo-code with sequence counters, CAS, and memory fences; handle wrap-around; describe hazard pointers/epochs for lifetime management; discuss cache-line padding and validation under cross-NUMA contention?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Microsoft","Plaid"]},{"id":"q-2517","question":"Design a beginner-friendly micro-benchmark in C to quantify false sharing. Create two layouts of per-thread counters: one unpadded (counters reside on the same cache line) and one padded to a full 64-byte cache line. Two threads update their own counter 10,000,000 times each. Measure throughput with clock_gettime(CLOCK_MONOTONIC) and compare results, explaining the impact of padding on cache coherence in multi-core systems?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Netflix","Oracle","Uber"]},{"id":"q-2688","question":"Design and implement a per-thread arena allocator for 16-byte blocks from a fixed 256KB pool. Provide API: init_arena(void* arena, size_t size), void* alloc16(), void free16(void*). Use a global bitmap to track 16,384 blocks and a thread-local cache of 8 blocks to reduce contention. Explain alignment, fragmentation, and how this approach compares to malloc under multi-core contention. Include a minimal test harness?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Coinbase","Google","OpenAI"]},{"id":"q-2699","question":"Design a bounded, lock-free, multi-producer/multi-consumer queue where each slot holds a variable-length payload with a 4-byte length prefix. Use a power-of-two ring, per-slot sequence numbers, and epoch-based reclamation. Provide enqueue/dequeue pseudo-code, describe memory layout, backpressure handling, and a microbenchmark plan under high contention?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Amazon","Netflix","Plaid"]},{"id":"q-684","question":"Design a fixed-size ring buffer in C that stores bytes. Capacity N is a power of two (e.g., 1024). Show how to compute the next index using a mask (idx & (N-1)), and explain full vs empty detection using only head and tail counters. Provide enqueue and dequeue logic for a single-producer/single-consumer scenario?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Discord","Hugging Face","Instacart"]},{"id":"q-692","question":"Design a lock-free ring buffer that supports multiple producers and multiple consumers with bounded capacity. Provide enqueue/dequeue pseudo-code, explain how you avoid ABA, how memory reclamation is handled (hazard pointers or epochs), and why it scales under high contention. Include caveats on cache lines and false sharing. How would you validate under stress?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Anthropic","Databricks","Stripe"]},{"id":"q-694","question":"Design a cache-friendly, per-thread deque work-stealer for a multi-core task executor. Each worker maintains a fixed-size ring buffer for bottom push/pop; thieves steal from the top of other workers via CAS on a top index with a version counter. Explain ABA avoidance, memory ordering, and padding to avoid false sharing. Provide precise pseudo-code and a realistic burst workload scenario?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Coinbase","NVIDIA"]},{"id":"q-707","question":"Design a crash‑consistent, multi‑producer/multi‑consumer ring buffer backed by persistent memory PMEM. How would you ensure last enqueued item durability across power loss, implement recovery, and validate correctness? Provide concise enqueue/dequeue pseudocode with proper flush/fence ordering and discuss failure scenarios?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Airbnb","Coinbase","Two Sigma"]},{"id":"q-712","question":"Design a NUMA-aware in-memory index with per-node shards and a lock-free cross-node coordinator. Provide insertion and lookup with minimal locking, specify data layout (shards, padding, key/value encodings), and memory-order guarantees (fences, atomic ops). Describe a deadlock-free shard rebalancing protocol under high contention and outline tests with realistic Snowflake/Twitter-scale workloads?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Snowflake","Twitter"]},{"id":"q-723","question":"Design a software-based **TLB** for a 4-core 64-bit system with 4KiB pages. Each core has a private 128-entry **TLB** and a global page-table invalidation path. Provide lookup/refill pseudo-code, discuss eviction strategy (LRU vs. random), synchronization via **memory fences**, and cross-core shootdowns. Include a test under memory pressure and explain validation?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Instacart","Oracle","Tesla"]},{"id":"q-725","question":"Design a crash-consistent, in-memory index for a 4-byte key, 8-byte value store on an 8-core Linux machine. Use per-core log-structured segments and a Bloom filter; describe durable append ordering, startup recovery by replaying per-core logs, and validation under power-loss scenarios?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Goldman Sachs","LinkedIn","Plaid"]},{"id":"q-734","question":"Design an epoch-based reclamation scheme for a lock-free stack in a 64-bit, multi-threaded user-space library. Each thread publishes its local epoch; a global clock advances periodically. On pop, move the node to a retire-list with its epoch and reclaim only after all threads have observed an epoch older than that node. Include a stress test with high contention?","channel":"low-level","subChannel":"general","difficulty":"intermediate","tags":["low-level"],"companies":["Google","Lyft","Slack"]},{"id":"q-744","question":"In a 2-socket x86-64 server with MESI coherence, design a lock-free, multi-producer single-consumer ring buffer in shared memory for a 10 Gbps network path. Use per-slot sequence numbers to avoid ABA, with a fixed size N=1<<16 and 64-byte payload slots. Provide slot layout, push/pop pseudo-code with memory fences, discuss wrap-around and backpressure, and outline a minimal microbenchmark to validate throughput and data integrity under contention?","channel":"low-level","subChannel":"general","difficulty":"intermediate","tags":["low-level"],"companies":["Instacart","Tesla","Two Sigma"]},{"id":"q-752","question":"Design a crash-safe, persistent ring buffer in NVRAM for a 2-socket NUMA system with a PCIe NIC. Capacity N=1<<18, 128-byte payloads, per-slot 64-bit sequence to prevent ABA. Slot: [payload|seq|meta]. Enqueue: publish payload, fence, then update seq. Dequeue: verify seq before consume. Durability via per-slot commit log: flush payload and seq, then epoch commit. On crash, recover by replaying committed epochs and validating seq monotonicity. Provide a minimal microbenchmark to validate throughput and correctness under power loss?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Adobe","IBM","Tesla"]},{"id":"q-760","question":"On a dual-socket NUMA server, design a cross-NUMA, zero-copy ring buffer for a 40 Gbps path between two processes where producers live on socket A and a consumer on socket B. Use per-slot 64-bit sequence numbers to prevent ABA, 128-byte payload slots, and capacity 1<<18 with 64-byte alignment. Provide slot layout, enqueue/dequeue pseudo-code with memory fences and cache-line padding, wrap-around and backpressure handling, and outline a minimal microbenchmark to validate throughput and data integrity under cross-socket contention?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Amazon","Google"]},{"id":"q-772","question":"Design a cache-friendly fixed-size object pool for 128-byte blocks used by a multi-threaded producer-consumer path. Implement init, alloc, and free in C for a pool of 1<<20 blocks, each 128 bytes and 64-byte aligned. Use per-thread caches to reduce contention and a global lock-free free-list for overflow. Explain how you avoid false sharing, show the slot layout with a freelist pointer, and outline a microbenchmark to measure throughput and tail latency under contention?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Adobe","Netflix"]},{"id":"q-778","question":"Write a C function sum_prefetch that sums N 64-bit integers from an aligned array of length n using software prefetching to hide memory latency. Use 4-way unrolling and __builtin_prefetch to bring data 256 elements ahead. Ensure correctness for any length. Propose a microbenchmark plan to compare with a naive loop and discuss cache-line utilization and false sharing concerns?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Google","Tesla","Twitter"]},{"id":"q-784","question":"Design a deterministic, time-sliced barrier for a three-stage streaming pipeline on a 2-socket x86-64 system. Each stage runs on a fixed subset of cores; implement a barrier that advances phases only after every core finishes its assigned slice within a bounded time. Explain how to ensure bounded latency under cache-line contention, preserve MESI coherence, and prevent starvation. Provide pseudo-code for enter_barrier for a core and discuss validation?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Bloomberg","OpenAI"]},{"id":"q-794","question":"Design and implement a cache-friendly tiled matrix multiply for two large double-precision matrices stored in row-major order on a two-socket NUMA system. Propose a tile size of 32x32, provide C code for the tiled kernel with boundary handling, explain how to maximize L1/L2 reuse, NUMA locality (first-touch), avoid false sharing, and an inline 4-wide inner-loop unrolling. Include a microbenchmark plan comparing to a naive triple-nested loop and how you would measure throughput and cache behavior?","channel":"low-level","subChannel":"general","difficulty":"intermediate","tags":["low-level"],"companies":["Google","LinkedIn"]},{"id":"q-800","question":"Design a per-core, lock-free memory allocator for a shared-memory, NUMA-aware object store. Each core maintains a 2 MB local heap; allocations first attempt local allocation, with a fast cross-core path using atomic hand-offs; reclaimed memory is managed via hazard pointers and epoch-based reclamation. Provide allocate/free APIs, a sketch of the free-list structure, and a microbenchmark plan that shows fragmentation under steady-state load?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Discord","MongoDB","NVIDIA"]},{"id":"q-810","question":"Design a crash-consistent, bounded, multi-producer/multi-consumer queue backed by non-volatile memory. It must survive power loss, use per-slot sequence numbers to avoid ABA, provide push/pop pseudo-code with proper memory fences, and support epoch-based reclamation to avoid hazard pointers. Outline recovery on boot and a microbenchmark plan?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Airbnb","Plaid","Uber"]},{"id":"q-816","question":"Design a crash-consistent, persistent ring buffer for a multi-producer/multi-consumer event stream backed by non-volatile memory, with 1<<18 slots of 128-byte payloads; explain ABA avoidance via per-slot sequence numbers, two-phase publish with durable commit, and required flush/barrier order; describe recovery and a microbenchmark plan under simulated power loss?","channel":"low-level","subChannel":"general","difficulty":"advanced","tags":["low-level"],"companies":["Robinhood","Tesla"]},{"id":"q-826","question":"Design and implement a tiny spinlock in C11 for a shared memory region. Provide lock() and unlock() using stdatomic.h primitives, ensuring memory_order_acquire on successful lock and memory_order_release on unlock. Include a minimal two-thread test contending for the lock and explain your backoff/yield strategy and fairness limitations?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Goldman Sachs","Google","Snap"]},{"id":"q-834","question":"Design and implement a fixed-size object pool: a pre-allocated buffer partitioned into 1024 blocks of 64 bytes. Provide thread-safe allocate() and free() using a free-list stored in the blocks and a lightweight spinlock guarding the list. Include initialization and a small test snippet with 2 threads. Discuss fragmentation, cache locality, and how you'd validate concurrency?","channel":"low-level","subChannel":"general","difficulty":"beginner","tags":["low-level"],"companies":["Meta","Netflix","Uber"]},{"id":"q-358","question":"You're building a customer churn prediction model. Given a dataset with customer features (age, usage frequency, subscription type), how would you determine whether to use linear regression or logistic regression?","channel":"machine-learning","subChannel":"algorithms","difficulty":"beginner","tags":["regression","classification","clustering"],"companies":["Amazon","Datadog","Robinhood"]},{"id":"q-386","question":"You're building a fraud detection system for a large e-commerce platform. Your initial model using logistic regression has 85% accuracy but high false positives. How would you improve this system using ensemble methods, and what specific trade-offs would you consider between precision and recall?","channel":"machine-learning","subChannel":"algorithms","difficulty":"intermediate","tags":["regression","classification","clustering"],"companies":["Anthropic","Google","OpenAI"]},{"id":"q-201","question":"How does an LSTM cell's forget gate regulate information flow compared to a simple RNN?","channel":"machine-learning","subChannel":"deep-learning","difficulty":"beginner","tags":["lstm","gru","seq2seq"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"q-254","question":"When implementing a bidirectional GRU vs LSTM for sequence labeling, how do gradient clipping thresholds and batch size affect convergence and what are the memory trade-offs?","channel":"machine-learning","subChannel":"deep-learning","difficulty":"intermediate","tags":["lstm","gru","seq2seq"],"companies":["Airbnb","Amazon","Apple","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-415","question":"You're training a CNN for Tesla's Autopilot lane detection. The model works well on clear roads but fails in rainy conditions. How would you modify the architecture and training process to handle weather variations while maintaining real-time performance?","channel":"machine-learning","subChannel":"deep-learning","difficulty":"intermediate","tags":["cnn","rnn","transformer","attention"],"companies":["Coinbase","Epic Games","Tesla"]},{"id":"q-195","question":"How would you implement a canary deployment strategy for a TensorFlow model using MLflow and Kubernetes, ensuring zero downtime and automatic rollback on performance degradation?","channel":"machine-learning","subChannel":"deployment","difficulty":"intermediate","tags":["mlflow","kubeflow","sagemaker"],"companies":["Amazon","Databricks","Google","Microsoft","Uber"]},{"id":"q-227","question":"How would you implement dynamic quantization-aware training with mixed-precision to optimize inference latency while maintaining model accuracy across varying hardware constraints?","channel":"machine-learning","subChannel":"deployment","difficulty":"advanced","tags":["quantization","pruning","distillation"],"companies":["Google","Meta","Microsoft","NVIDIA","Tesla"]},{"id":"q-309","question":"Design a complete MLOps pipeline using MLflow and Kubeflow for a production ML system handling 1M daily predictions with 99.9% availability. What components would you include and how would you ensure model governance and automated retraining?","channel":"machine-learning","subChannel":"deployment","difficulty":"advanced","tags":["mlflow","kubeflow","sagemaker"],"companies":null},{"id":"q-323","question":"How would you design an ML pipeline using Kubeflow that handles model versioning, A/B testing, and automated rollback for a fraud detection system at Coinbase?","channel":"machine-learning","subChannel":"deployment","difficulty":"advanced","tags":["mlflow","kubeflow","sagemaker"],"companies":["Coinbase","Cruise","Tesla"]},{"id":"q-347","question":"You're deploying a machine learning model using MLflow. How would you track experiments and ensure reproducibility when moving from development to production?","channel":"machine-learning","subChannel":"deployment","difficulty":"beginner","tags":["mlflow","kubeflow","sagemaker"],"companies":["MongoDB","Okta","Warner Bros"]},{"id":"q-223","question":"How would you design a production-scale evaluation pipeline that dynamically adjusts metrics based on class imbalance and business priorities while maintaining sub-second latency?","channel":"machine-learning","subChannel":"evaluation","difficulty":"advanced","tags":["precision","recall","auc-roc","f1"],"companies":["Amazon","Google","Meta","Netflix","Stripe"]},{"id":"q-336","question":"You're evaluating a movie recommendation system at Warner Bros. The system has 95% accuracy but poor precision for popular movies. How would you diagnose and fix this issue using precision-recall analysis?","channel":"machine-learning","subChannel":"evaluation","difficulty":"intermediate","tags":["precision","recall","auc-roc","f1"],"companies":["Expedia","Microsoft","Warner Bros"]},{"id":"q-1126","question":"You're deploying a real-time anomaly detector for edge CDN traffic at a cloud provider. Spikes during events cause distribution drift. Propose an online learning approach that adapts without catastrophic forgetting, maintains latency under 30 ms, and keeps calibration. Include data retention policy, drift detection, update rules, and monitoring dashboards?","channel":"machine-learning","subChannel":"general","difficulty":"intermediate","tags":["machine-learning"],"companies":["Cloudflare","Google"]},{"id":"q-1170","question":"You're training a binary classifier on a dataset with 1% positives. After a baseline model, overall accuracy is high but positive precision is very low. Describe a practical plan to diagnose whether the issue is threshold choice or true data imbalance, and implement a minimal pipeline with stratified splits, class weights or resampling, and threshold tuning; outline metrics and validation?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Adobe","Tesla","Two Sigma"]},{"id":"q-1220","question":"You're deploying a single on-device model for real-time video analytics on an edge device with 12 ms per frame latency and 200 MB RAM. The model must perform both object detection and semantic segmentation. Describe a concrete plan to meet latency while preserving accuracy: architecture choices (shared backbone, task heads, feature pyramids), training strategies (loss weighting, distillation, data augmentation), deployment optimizations (quantization, operator fusion, memory layout, early exits), and validation strategy (latency budgets, mAP, mIoU, robustness across weather)?","channel":"machine-learning","subChannel":"general","difficulty":"advanced","tags":["machine-learning"],"companies":["NVIDIA","Tesla","Twitter"]},{"id":"q-1290","question":"You’re training a binary classifier for signup conversion on a dataset with numeric features (age, session_time) and categorical features (device, country). A logistic regression baseline yields high AUROC but poor calibration on holdout. Outline a practical plan to diagnose and fix calibration, comparing Platt scaling and isotonic regression, data preprocessing tweaks, and how you’d validate the fix with a minimal code sketch?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Snowflake","Uber"]},{"id":"q-2291","question":"You're deploying a real-time anomaly detection system for financial transactions using a hybrid model: a fast statistical detector plus a deeper autoencoder. After deployment, you notice an uptick in false positives during major holidays due to seasonal patterns. Design a practical plan to improve robustness: data collection, re-calibration, gating strategy to route events to the appropriate detector, and evaluation metrics including online A/B testing. What changes would you implement and why?","channel":"machine-learning","subChannel":"general","difficulty":"intermediate","tags":["machine-learning"],"companies":["Google","Netflix"]},{"id":"q-2389","question":"You're deploying real-time anomaly detection on telemetry from 50k edge cameras across campuses. Labels are scarce, data drifts with time, and bandwidth to central is limited. Propose a practical architecture and training plan that balances latency, privacy, and accuracy: choose models, training regime (self-supervised + federated updates), deployment (edge vs cloud), monitoring, and evaluation metrics. Be concrete about components and data flow?","channel":"machine-learning","subChannel":"general","difficulty":"advanced","tags":["machine-learning"],"companies":["Adobe","Amazon"]},{"id":"q-2530","question":"Design a production-ready calibration-and-drift pipeline for a binary classifier deployed in a high-stakes domain (e.g., loan approvals). The system must calibrate probabilities in real time as data drifts occur, detect distribution shift, trigger targeted retraining with limited labels, and provide explainability and auditing. Describe architecture, data flow, concrete metrics, and trade-offs?","channel":"machine-learning","subChannel":"general","difficulty":"advanced","tags":["machine-learning"],"companies":["Google","IBM"]},{"id":"q-2682","question":"Describe a minimal, end-to-end churn-prediction pipeline for 25k records with numeric features (tenure, spend), categorical features (region, device_type, plan) and a high-cardinality feature like 'customer_segment_id'. Data drift occurs monthly; outline preprocessing (encode high-cardinality features, handle missing), model choice (logistic vs tree-based) and rationale, evaluation (AUC, calibration), and drift monitoring/retraining cadence with concrete steps?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Goldman Sachs","Tesla","Twitter"]},{"id":"q-2709","question":"In a production ride-recommendation system with a two-tower model for candidate generation at scale, design a concrete plan to handle data skew, long-tail items, and concept drift while maintaining sub-50ms latency per request. Include data/versioning, evaluation, deployment strategies, and monitoring specifics?","channel":"machine-learning","subChannel":"general","difficulty":"advanced","tags":["machine-learning"],"companies":["Meta","Salesforce","Uber"]},{"id":"q-468","question":"You're training a neural network and notice the validation loss starts increasing while training loss continues decreasing. What's happening and how would you diagnose and fix it?","channel":"machine-learning","subChannel":"general","difficulty":"intermediate","tags":["machine-learning"],"companies":["Amazon","Google","OpenAI"]},{"id":"q-498","question":"You're building a simple spam classifier for emails. What's the difference between precision and recall, and which metric would you prioritize if false positives are more costly than false negatives?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Airbnb","IBM","Tesla"]},{"id":"q-529","question":"You're building a customer churn prediction model for a SaaS platform. What are the key steps you'd take from data preprocessing to model evaluation, and which metrics would you prioritize?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Databricks","Salesforce"]},{"id":"q-555","question":"Explain how gradient descent works and why it's fundamental to training neural networks?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Citadel","Microsoft","Tesla"]},{"id":"q-582","question":"Explain the difference between classification and regression in machine learning, and provide a simple example of when to use each?","channel":"machine-learning","subChannel":"general","difficulty":"beginner","tags":["machine-learning"],"companies":["Meta","Oracle"]},{"id":"q-273","question":"What's the difference between hyperparameters and parameters in machine learning, and why is cross-validation important for selecting optimal hyperparameters?","channel":"machine-learning","subChannel":"model-training","difficulty":"beginner","tags":["hyperparameter","cross-validation","regularization"],"companies":["Amazon","Google","Meta","Microsoft"]},{"id":"q-372","question":"You're training a CNN for Snapchat lens effects and notice your validation loss increases after epoch 3 while training loss decreases. What's happening and how would you implement a comprehensive solution including data augmentation, learning rate scheduling, and monitoring strategies?","channel":"machine-learning","subChannel":"model-training","difficulty":"beginner","tags":["hyperparameter","cross-validation","regularization"],"companies":null},{"id":"q-403","question":"You're training a large language model for Notion's AI features. Your model is overfitting on the training data but underperforming on validation. Design a comprehensive regularization strategy that addresses both L1/L2 regularization and more advanced techniques like dropout and early stopping. How would you implement cross-validation to ensure your hyperparameters generalize across different user data distributions?","channel":"machine-learning","subChannel":"model-training","difficulty":"advanced","tags":["hyperparameter","cross-validation","regularization"],"companies":["Chime","Google","Notion"]},{"id":"q-1133","question":"Scenario: a constraint-satisfaction problem with four tasks A,B,C,D each can be in domains {Pending, Running, Done}. Constraints: (A = Done) → (B = Running); (B = Running) → (C = Done); (C = Pending) → (D = Running); and (A = Pending) ∨ (D = Done). Is there a feasible assignment with at least one Running? Explain reasoning and outline a backtracking algorithm with forward-checking and 3-valued propagation to decide arbitrary such constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Bloomberg","Lyft"]},{"id":"q-1253","question":"Scenario: A deployment feature-flag policy defines three booleans F1, F2, F3 with these constraints: F1 -> F2, F2 -> F3, and exactly one of F1 and F3 is true, plus at least one flag is true. Is there a satisfying assignment? If yes, give one; if not, explain why. Then outline a tiny solver that encodes such constraints into a 2-SAT instance using an implication graph and SCC, with pseudocode?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Amazon","Meta","PayPal"]},{"id":"q-1915","question":"You design a tiny policy engine for network ports where each port's state is Allow, Deny, or Unknown (A, D, U). Ports: 80, 443, 22, 23, 8080. Constraints: (80 → 443), (22 ∨ 23), (80 ∨ 22), (443 → ¬23). Is there an assignment with U that does not force any constraint to be definitively false? Explain how you would implement a 3-valued backtracking propagation to verify arbitrary such rule sets?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Cloudflare","LinkedIn","Scale Ai"]},{"id":"q-1930","question":"In a policy graph with rules of the form (P ∧ Q) → R, (P ∧ R) → S, and (Q ∧ S) → T, plus a set of base facts that you may set to true as needed, is there an assignment making T true without violating any rule? Provide an algorithm to compute a minimal witness (smallest base set) and apply it to this instance to show T is derivable with base facts {P, Q}?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Apple","Databricks","Twitter"]},{"id":"q-2088","question":"You manage access flags as a directed graph with A ≤ B meaning 'A true implies B true'. Given a required set T of flags that must be true, decide if a consistent assignment exists and produce the least-true model containing T. Describe a linear-time forward-closure algorithm that propagates truth along edges until a fixpoint, and how it handles cycles. Include a concrete 6-flag example with 7 relations?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Bloomberg","Goldman Sachs","Google"]},{"id":"q-2194","question":"Given a finite Kripke model M = (W, {R_a, R_b}, V) with W = {w1,w2,w3}, agents a,b, and a proposition p with V(p) = {w1,w2}. After a public announcement of p, compute the updated model M|p and determine which worlds satisfy K_a p and K_b p. Then describe a linear-time algorithm using product update and forward closure to handle arbitrary finite models and sequences of announcements, including a small 3-world example to illustrate the steps?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Adobe","Instacart","Salesforce"]},{"id":"q-2286","question":"Design a solver for booleans with two constraint types: XOR constraints (subset S XOR = b) and at-least-k constraints on subsets. How would you decide satisfiability efficiently, encode the mix for a practical solver, and handle contradictions and scalability?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["DoorDash","Instacart","Tesla"]},{"id":"q-2441","question":"In a 3-valued logic with T, F, U, build a tiny circuit: inputs A, B, C; gates: G1 = A AND B; G2 = B OR C; G3 = NOT G1; G4 = G2 AND G3. Is there an assignment to (A, B, C) using U that yields G4 = U after standard Kleene-style tables? Explain how you would implement a small propagator to verify arbitrary such circuits?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Microsoft","NVIDIA","Square"]},{"id":"q-2496","question":"Scenario: four attributes X,Y,Z,W take values 0,1, or U. Constraints: X+Y ≥ 1, Y ≥ Z, Z+W ≥ 1, W ≤ X. Is there an assignment that uses U for at least one variable while satisfying all constraints? Outline a straightforward 3-valued propagation plus backtracking to verify arbitrary such constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Airbnb","Meta","Tesla"]},{"id":"q-2696","question":"Design a tiny policy-engine puzzle: There are flags F1..F5 taking values Enable (E), Disable (D), or Unknown (U). Constraints:\n- If F1 is E then F2 is E.\n- If F2 is D then F3 is D.\n- If F4 is E then F5 is E.\n- At least one of F1 or F3 must be E unless F5 is D.\n- F3 cannot be E if F4 is D.\nIs there an assignment that uses U for at least one flag and satisfies all constraints? Propose a 3-valued propagation with backtracking approach and apply it to the concrete instantiation: F1=U, F2=U, F3=U, F4=D, F5=U; determine feasibility?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Apple","Slack","Tesla"]},{"id":"q-683","question":"You're managing a streaming DAG with tasks A,B,C,D; edges enforce: A before B; B before C; at most one of C or D can occur in a window of size H. Given a log of events with timestamps per task, implement an O(n log n) verifier to determine if the log is valid under these constraints and describe how you'd extend to multiple windows in a distributed system?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Databricks","Google"]},{"id":"q-685","question":"Context: In a data-cleaning pipeline, records have boolean attributes a, b, c, d. Rules are Horn clauses of the form X ∧ Y -> Z. Given a partial assignment, decide if a full assignment exists that satisfies all clauses. Design a linear-time forward-chaining solver, justify its correctness, and discuss incremental updates and cycles with a concrete four-variable example (two rules)?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Adobe","Databricks","Scale Ai"]},{"id":"q-699","question":"You have a Horn-clauses policy language for access control. Given the following rules and facts, determine if allow(alice,read,records) is entailed using forward-chaining to a least fixpoint. Rules: 1) grant(U,act,res) :- haveRole(U,R), privilege(R,act,res). 2) allow(U,act,res) :- grant(U,act,res). Facts: haveRole(alice,dataEngineer). privilege(dataEngineer,read,records). Show your derivation steps?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Adobe","Databricks","MongoDB"]},{"id":"q-702","question":"In a home security system, three sensors—door, window, and motion—report activity as booleans a, b, c for the last minute. The alert should fire only when exactly one sensor is active. How would you implement a function that takes a, b, c and returns true iff exactly one is true, and what are its time and space complexities?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Amazon","Google"]},{"id":"q-715","question":"In a distributed data-processing pipeline across three services Ingest (I), Compute (C), Persist (P), each event carries a 3-element vector clock. Given E1 at I with [2,0,1] and E2 at C with [1,3,0], decide whether E1 happened-before E2, E2 happened-before E1, or they are concurrent. Explain the rule and show the comparison. How would you scale this to N services and detect concurrency in large logs?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Amazon","Bloomberg","Slack"]},{"id":"q-719","question":"Design a tiny solver for three boolean inputs A, B, C given constraints: (A -> B) and (B -> C) and (A or C). Is there an assignment that satisfies all three? Explain your reasoning and outline a simple backtracking approach you would implement to check arbitrary small sets of such implications?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Adobe","Google","Scale Ai"]},{"id":"q-728","question":"Scenario: You’re building a tiny policy engine for feature flags with three booleans A, B, C. Constraints: A implies B, B implies C, and at least two of the three must be true. Is there an assignment that satisfies all constraints? If so, give one example and briefly justify. Then outline a straightforward backtracking approach to enumerate all satisfying assignments for any n flags and any such constraints?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["Amazon","Instacart","Plaid"]},{"id":"q-733","question":"You’re auditing a rule-set for access policy. There are four booleans **X1**, **X2**, **X3**, **X4** with constraints: (X1 -> X2), (X1 ∨ X3), (X2 -> X4), and (X3 -> ¬X4). Is there a satisfying assignment? If yes, provide one; if not, explain why. Then outline a minimal backtracking strategy with unit propagation to verify arbitrary similar constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Microsoft","Netflix","Salesforce"]},{"id":"q-745","question":"You are building a tiny rule engine for feature toggles with three booleans A, B, C. Constraints: A → B, B → ¬C, and at least one of A, B, C must be true. Is there a satisfying assignment? Explain how you would verify it via brute-force backtracking across the eight possibilities and return one concrete example if it exists?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["NVIDIA","Twitter"]},{"id":"q-753","question":"In a feature-flag synthesis task, you have booleans A1..A8. Constraints include: (Ai -> Aj) implications, (Ai XOR Aj) mutual exclusions, and (Ai OR Aj OR Ak) group obligations. The implication graph is acyclic and each node has at most two outgoing edges. Design a backtracking solver that exploits the DAG to decide satisfiability for up to 12 vars. Provide a concrete 8-variable instance and show the solution or UNSAT?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Apple","LinkedIn","Netflix"]},{"id":"q-758","question":"You manage feature flags for a distributed service. Let F1..F5 be booleans with constraints: (F1 -> F2), (F1 ∨ F3), (F2 ⊕ F4), (F3 -> F5), and (F4 -> ¬F5). Is there a satisfying assignment? If yes, provide one; if not, explain. Then outline a minimal backtracking strategy with forward checking to verify arbitrary similar constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Cloudflare","Discord","Snowflake"]},{"id":"q-771","question":"You are validating a tiny access-control policy with two booleans: A = 'account is active', B = 'email verified'. The policy must satisfy: (¬A -> B), (B -> A), and (A ∨ B). Is there a satisfying assignment for A and B? Explain your reasoning and outline a simple backtracking check to verify arbitrary small sets of such clauses?","channel":"math-logic","subChannel":"general","difficulty":"beginner","tags":["math-logic"],"companies":["NVIDIA","PayPal","Snowflake"]},{"id":"q-775","question":"You're building a constraint-solver for a feature-flag system across a microservices deployment. Given four flags S1..S4 with constraints: (S1 -> S2), (S2 -> S3), (S4 -> ¬S1), and (S1 ∨ S4) and (S2 ∨ S4). Is there a satisfying assignment? Provide one concrete assignment and outline a backtracking algorithm with unit propagation to verify arbitrary similar constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Slack","Two Sigma"]},{"id":"q-789","question":"Is there a truth assignment with exactly three flags true that satisfies all constraints A -> B; B -> C; D -> E; F -> G; not C -> H; A -> D? If yes, provide one and justify why it satisfies every implication?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Apple","Square","Twitter"]},{"id":"q-799","question":"Scenario: four booleans P, Q, R, S where P = user has role X, Q = grants read, R = has role Y, S = grants write. Constraints: (P -> Q), (R -> S), (P ∨ R), and at least two of {Q,S} must be true. Is there a satisfying assignment? If yes, give one; if not, explain why. Then outline a minimal backtracking strategy with unit propagation for arbitrary similar constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Cloudflare","DoorDash","Google"]},{"id":"q-812","question":"Instance-based SAT with mixed Horn and parity constraints: Variables A,B,C,D,E,F. Constraints: (A ∧ B) → C; (C ∧ D) → E; A ⊕ D ⊕ F = 1; (B ∧ E) → F. Is there an assignment to A..F that satisfies all constraints? If yes, provide one; then describe how you would build a solver that combines forward-chaining on Horn clauses with Gaussian elimination over GF(2) for parity constraints, including data structures and complexity considerations?","channel":"math-logic","subChannel":"general","difficulty":"advanced","tags":["math-logic"],"companies":["Citadel","Google","Lyft"]},{"id":"q-822","question":"Scenario: a tiny feature-toggle system for a data-annotation pipeline uses five booleans A–E: A = 'data augmentation enabled', B = 'sampling enabled', C = 'privacy mode on', D = 'live monitoring on', E = 'throttle rate limited'. Constraints: (A → B), (B → D), (C → ¬D), (A ∨ C), (D → E), (E → ¬A). Is there a satisfying assignment? If yes, give one; if not, explain why. Then outline a minimal backtracking strategy with unit propagation to verify arbitrary similar constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Databricks","Hugging Face","Square"]},{"id":"q-828","question":"Scenario: a feature-flag set uses four booleans A,B,C,D with rollout statuses True, False, Unknown (U). Constraints: (A → B), (C ∨ D), (A ∨ C), (B → ¬D). Is there a satisfying assignment allowing Unknowns? Explain reasoning and outline a backtracking approach using 3-valued logic to propagate U and verify arbitrary such constraint sets?","channel":"math-logic","subChannel":"general","difficulty":"intermediate","tags":["math-logic"],"companies":["Hashicorp","MongoDB","Robinhood"]},{"id":"q-180","question":"What is the primary purpose of DNS in computer networking and how does it enable internet communication?","channel":"networking","subChannel":"dns","difficulty":"beginner","tags":["dns","resolution"],"companies":["Amazon","Cisco","Google","Meta","Microsoft"]},{"id":"q-1031","question":"In a beginner-friendly scenario, a REST API behind a global CDN shows sporadic 1–2s latency for some users while synthetic tests pass. Outline a practical, end-to-end diagnostic plan to isolate DNS, TLS, caching, and client-network factors, including concrete commands and data you would collect and an initial fix you would try?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Discord","DoorDash","Snowflake"]},{"id":"q-1078","question":"In a small office network, a workstation intermittently cannot reach a public API behind Cloudflare during peak hours while other destinations are responsive; outline a practical, hands-on plan to diagnose using DNS (A/AAAA, TTLs), path tracing, TLS handshakes (ALPN/SNI), and edge routing behavior, with concrete mitigations to test?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Cloudflare","Tesla"]},{"id":"q-1089","question":"In a multi-region Kubernetes deployment using VXLAN overlays for pod networking, you notice intermittent packet loss and high tail latency when pods in region A talk to pods in region B. The overlay adds headers that push MTU beyond 1500 on inter-region links, causing fragmentation in some paths. You can adjust MTU, MSS clamping, and overlay parameters but cannot modify application code. How would you diagnose end-to-end and implement a robust fix that preserves ECMP load balancing and minimizes fragmentation? Provide concrete steps?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["DoorDash","Google"]},{"id":"q-1163","question":"In a two-region service, IPv6 clients report higher latency and intermittent timeouts to a TLS-enabled API when accessed from IPv6 only networks. You cannot modify app code. Outline a practical diagnostic plan focusing on IPv6 path MTU discovery, ICMPv6/firewall behavior, and how to verify with traceroute6, tcpdump, and TLS handshake timings. Propose concrete mitigations like adjusting VPN MTU and enabling IPv4 fallback?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Discord","Robinhood","Snowflake"]},{"id":"q-1232","question":"In a two-region deployment (us-east-1, eu-west-1) with a TLS-enabled API behind a global load balancer, peak hours yield p95 latency spikes to 350 ms while basic tests pass. No app changes allowed. Provide a concrete diagnostic plan to distinguish TLS handshake delays, ALPN/SNI issues, path MTU fragmentation, and load balancer behavior, with exact commands and data you’d collect?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Hashicorp","MongoDB"]},{"id":"q-1461","question":"In a globally distributed API behind a CDN and global load balancer, Asia users report login latency significantly higher than North America, while overall API latency remains acceptable. The client app is mobile and uses TLS with SNI; no code changes are allowed. Outline a concrete diagnostic plan to determine whether ECS (EDNS Client Subnet), DNS TTL, CDN edge variance, or inter-region routing is responsible, including exact commands, data to collect, and initial mitigations to test?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Google","MongoDB","Tesla"]},{"id":"q-1507","question":"In a globally distributed TLS-enabled API for real-time inference, traffic flows from Asia, EU, US behind a global load balancer and regional edge caches. During peak, Asia users see p95 latency spikes while NA remains stable. No code changes allowed. Provide a concrete diagnostic plan to distinguish between (1) TLS handshake/ALPN at edge vs origin, (2) inter-region routing and MTU fragmentation, (3) load balancer ECMP behavior, and (4) CDN edge miss penalties. Include exact commands, data to collect, and initial mitigations to test?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["NVIDIA","OpenAI","PayPal"]},{"id":"q-1671","question":"In a multi-cloud microservice mesh spanning AWS and Azure, inter-region service-to-service calls intermittently fail under load with rising p95 latencies. Tracing shows ECMP paths changing mid-request and MTU-related fragmentation on some hops. Without modifying applications, outline a concrete diagnostic plan and a stabilization strategy addressing PMTUD behavior, IPv4/IPv6 MTU alignment, ICMP blocking, and MTU discovery pitfalls across clouds, while preserving ECMP load balancing?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["Microsoft","Netflix","Oracle"]},{"id":"q-1766","question":"In a global data service using DNS-based global load balancing across us-east-1 and eu-west-1, cross-region reads spike latency during peak hours while intra-region calls remain fast. No app changes allowed. Outline a concrete diagnostic plan to distinguish stale DNS routing, BGP route flaps, and edge TLS termination delays, with exact commands and data you’d collect, interpretation rules, and recommended mitigations?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["Instacart","NVIDIA","Snowflake"]},{"id":"q-2322","question":"In a global deployment using anycast DNS fronting a TLS-terminated API gateway, intermittent TLS handshake stalls and p95 latency spikes appear during peak hours. No app changes allowed. The client path crosses regions via regional load balancers and edge caches. Outline a concrete diagnostic plan to distinguish issues caused by anycast routing, TLS handshakes (ALPN, SNI, 0-RTT), and cross-region MTU/PMTUD behavior, plus a practical mitigation path?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["Airbnb","DoorDash","Snowflake"]},{"id":"q-2382","question":"In a regional microservice mesh with DNS-based routing and TLS termination at edge gateways, intermittent p95 latency spikes occur during peak hours despite healthy synthetic tests. No app changes allowed. Outline a concrete diagnostic plan to distinguish DNS routing churn, edge TLS handshake variability, and backbone path changes, with exact commands, data to collect, and a practical mitigation path?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["Adobe","NVIDIA","Twitter"]},{"id":"q-2394","question":"In a large-scale service mesh across three data centers using VXLAN overlays for pod networking, you observe intermittent packet loss and tail latency spikes when east-west traffic redirection occurs during rebalancing. No app changes allowed. Design a concrete, end-to-end diagnostic plan to isolate whether overlay encapsulation, MTU/PMTUD, or inter-domain routing is the root cause, and outline a robust mitigation path that preserves ECMP and minimizes tunnel churn?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["Airbnb","Snap","Tesla"]},{"id":"q-469","question":"Explain what happens when you type google.com into your browser and press Enter, focusing on the networking layers involved?","channel":"networking","subChannel":"general","difficulty":"beginner","tags":["networking"],"companies":["Discord","Goldman Sachs","IBM"]},{"id":"q-499","question":"How would you design a TCP load balancer that handles 1M concurrent connections with consistent hashing while preventing connection thrashing during backend failures?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["Snowflake","Twitter"]},{"id":"q-583","question":"How would you design a load balancer to handle 1M concurrent connections with sub-10ms latency, considering TCP connection pooling, health checks, and graceful degradation?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["Adobe","Goldman Sachs"]},{"id":"q-926","question":"In a multi-region deployment (US-East, EU-West) for a high-throughput service, end-to-end latency spikes to 150–300 ms during peak hours while synthetic tests pass. You observe edge drops and retransmissions. Provide a practical plan to diagnose and mitigate network-related factors, including path MTU discovery, ECN, TCP congestion control, TLS handshakes, SNI/ALPN behavior, and load-balancing strategy across regions, with data you’d collect and initial fixes?","channel":"networking","subChannel":"general","difficulty":"intermediate","tags":["networking"],"companies":["NVIDIA","Snowflake","Uber"]},{"id":"q-946","question":"In a globally distributed service behind a multi-region, ECMP-enabled load balancer, you observe sporadic high-tail latency while averages look fine. Explain the network mechanisms that could cause tail latency in this setup (per-path RTT variance, path MTU, reordering, retransmissions). Propose a concrete diagnostic workflow using production telemetry (eBPF per-flow histograms, NetFlow/SFlow, MTU checks) and practical remediation steps (MTU tuning, pacing, queue management)?","channel":"networking","subChannel":"general","difficulty":"advanced","tags":["networking"],"companies":["Coinbase","Salesforce","Two Sigma"]},{"id":"gh-72","question":"How would you design and implement network segmentation for a microservices architecture, including Zero Trust principles, east-west traffic monitoring, and compliance requirements?","channel":"networking","subChannel":"load-balancing","difficulty":"advanced","tags":["security","network"],"companies":["Amazon","Cloudflare","Google","Hashicorp","Microsoft","Stripe"]},{"id":"q-186","question":"How would you implement session affinity (sticky sessions) in HAProxy while maintaining high availability, and what are the trade-offs compared to stateless load balancing?","channel":"networking","subChannel":"load-balancing","difficulty":"intermediate","tags":["lb","traffic","nginx","haproxy"],"companies":["Amazon","Bloomberg","Google","Microsoft","Netflix"]},{"id":"sd-1","question":"Explain load balancing strategies and when to use Layer 4 vs Layer 7. How do round-robin, least connections, and IP hash algorithms compare?","channel":"networking","subChannel":"load-balancing","difficulty":"advanced","tags":["infra","scale","networking"],"companies":["Amazon","Cloudflare","Google","Meta","Microsoft","Netflix"]},{"id":"q-203","question":"How does TCP's congestion control algorithm interact with HTTP/2's multiplexing when multiple streams compete for bandwidth?","channel":"networking","subChannel":"tcp-ip","difficulty":"intermediate","tags":["tcp","udp","http2","quic"],"companies":["Amazon Aws","Cloudflare","Google","Microsoft","Netflix"]},{"id":"q-256","question":"How does QUIC solve TCP's head-of-line blocking problem in HTTP/2 multiplexing, and what are the implementation trade-offs?","channel":"networking","subChannel":"tcp-ip","difficulty":"intermediate","tags":["tcp","udp","http2","quic"],"companies":["Amazon","Cloudflare","Google","Meta","Microsoft","Netflix"]},{"id":"q-275","question":"How does QUIC solve HTTP/2's head-of-line blocking issue over TCP, and what are the implementation trade-offs?","channel":"networking","subChannel":"tcp-ip","difficulty":"intermediate","tags":["tcp","udp","http2","quic"],"companies":["Amazon","Google","Meta","Microsoft"]},{"id":"q-1035","question":"In an advanced NLP interview, design an end-to-end multilingual QA system over English, Spanish, and Mandarin medical documents. The user asks in English. Outline architecture, data flow, privacy controls, latency targets, domain adaptation, and an evaluation plan. Include concrete components, trade-offs, and a short example of validation for a high-risk medical claim?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Adobe","IBM","MongoDB"]},{"id":"q-1101","question":"You're building a real-time brand-monitoring NLP service that ingests up to 100k tweets per minute in multiple languages. Design a scalable pipeline to classify sentiment and issue categories (e.g., billing, outages) with <300 ms latency per tweet, handle code-switching and slang, detect and adapt to drift, and provide a rollout plan including testing, monitoring, and rollback?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Stripe","Twitter"]},{"id":"q-1215","question":"Design a beginner-friendly pipeline for a Slack-based support bot that lives in a single workspace. It should: (1) classify Slack messages into intents: 'password_reset', 'access_request', 'billing_issue', 'incident'. (2) retrieve and present the most relevant FAQ article from a 100-article KB in English or Spanish. (3) operate with minimal latency on a shared CPU, and include a simple drift-detection plan and a rollout strategy with a safe fallback. Provide concrete components, data flow, and a short code snippet showing the classifier and retriever?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Slack","Snap"]},{"id":"q-1332","question":"Design an offline-first, on-device NLP pipeline for field technicians in remote areas. The device must classify support requests into hardware, network, or software issues and extract actionable items from multilingual speech transcripts (English, Spanish, Portuguese). Constraints: 256MB RAM, ≤200ms latency per utterance, no network access except periodic OTA updates, privacy-preserving embeddings, and robust drift detection with authenticated weight patches. Provide architecture, models, data handling, evaluation, and rollout plan?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Databricks","Hashicorp","Snap"]},{"id":"q-1394","question":"You are given a multilingual customer support dataset containing code-switching between English and Spanish and occasional emojis. Design an end-to-end NLP solution for intent classification and slot filling, with limited labeled data in the target language. Describe data collection, preprocessing, model choice, and evaluation strategy, including how you'd handle code-switching and emoji semantics?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Google","IBM","Two Sigma"]},{"id":"q-1456","question":"You're deploying a multilingual on-device sentiment and intent classifier for a mobile app, handling English/Spanish with code-switching and emojis. Latency budget: <200 ms on a 2-core device, offline-first. Design an end-to-end pipeline: data flow, model architecture (tiny quantized model plus emoji/slang rules), feature extraction, on-device drift detection, and a practical evaluation plan using ~50 labeled examples for quick adaptation?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Salesforce","Scale Ai","Twitter"]},{"id":"q-1657","question":"Design a beginner-friendly NLP pipeline to extract Date, Money, and Person entities from bilingual English/Spanish Slack-like messages with slang and emojis, using minimal labeled data. Outline preprocessing, tool choices (regex, spaCy), and a concrete evaluation plan with a simple baseline?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Slack","Snap","Two Sigma"]},{"id":"q-1702","question":"Design an end-to-end NLP system to detect safety-critical incidents from real-time chat and voice transcripts in a multi-tenant ride-hailing platform. Include data ingestion, ASR/translation, latency targets, privacy controls, and model versioning/deployment. Compare rule-based vs learned approaches and detail production evaluation (offline metrics plus live A/B and rollback plans)?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Hashicorp","Lyft","Robinhood"]},{"id":"q-2021","question":"Design a multilingual, low-latency NLP pipeline to summarize and extract action items from enterprise meeting transcripts in English, Spanish, and Mandarin. Include language detection, on-device vs cloud trade-offs, privacy controls, and a lifecycle for models and data. How would you evaluate accuracy and latency, and handle update rollouts with minimal downtime?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Google","LinkedIn","Snap"]},{"id":"q-2171","question":"Design a beginner-friendly on-device NLP classifier for a multilingual chat dataset in English and Spanish labeled as spam in a Discord-like app. Outline data prep, feature choices such as character n-grams, a lightweight model (logistic regression), multilingual handling, and a minimal evaluation plan with a held-out test and drift checks. Include a tiny Python snippet to train on a toy dataset?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Amazon","Discord"]},{"id":"q-2249","question":"You're given a dataset of 3,000 English customer-chat messages labeled with 5 intents (greeting, ask_status, report_issue, request_refund, other). Design a minimal on-device NLP classifier that runs in under 40 ms per request on a mid-range smartphone. Specify preprocessing, feature extraction (e.g., bag-of-words vs. embeddings), model choice, and a simple evaluation plan with train/val/test splits and drift checks. Include how you would measure privacy and latency in practice?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["DoorDash","Google","LinkedIn"]},{"id":"q-2314","question":"Design a scalable NLP pipeline that flags hazardous product reviews in a live e-commerce feed, combining real-time abuse detection, policy violation checks, and multilingual support, with privacy constraints and <100 ms latency, plus explainability. Compare embedding-based detectors vs rule-based detectors and outline production evaluation (offline metrics, live A/B, rollback plan)?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Google","Instacart","MongoDB"]},{"id":"q-2370","question":"Design a beginner-friendly NLP workflow to detect copyright-infringing paraphrase in multilingual video captions for a streaming service. Include data collection from captions, a lightweight detector (rule-based plus a small ML model), latency targets, privacy constraints, and how you’d validate with offline metrics and staged rollouts?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Bloomberg","Cloudflare","Netflix"]},{"id":"q-2406","question":"Design a scalable real-time multilingual intent recognition and routing system for chat and voice channels in a global support platform, supporting English, Spanish, and Japanese, with privacy constraints, a retrieval-augmented generation path, and a policy-driven fallback. What architecture, latency targets, privacy controls, and evaluation plan would you propose?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Amazon","Google","Snap"]},{"id":"q-2466","question":"Design a scalable, real-time multilingual NER system for customer support chat that can adapt to domain-specific entities (brands, products) with drift monitoring and minimal latency. Include data pipeline, model choice, schema for entities, evaluation strategy, and privacy/bias considerations?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Oracle","Snap"]},{"id":"q-2574","question":"Design a beginner NLP pipeline for real-time intent classification of chat messages in an e-commerce support setting. Dataset: ~10k English chats labeled with intents: 'billing', 'technical', 'account', 'other'. Build an end-to-end pipeline using a non-neural baseline (TF-IDF + logistic regression). Constraints: inference latency < 50 ms per message, optional stopword handling, easy retraining, and explainability via top contributing tokens per class. Include preprocessing, feature extraction, model choice, evaluation (cross-validation, macro-F1), and a simple staged rollout with monitoring and rollback?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Google","Microsoft","Slack"]},{"id":"q-2661","question":"Design an on-device, privacy-preserving NLP pipeline for a fintech mobile app that handles sensitive user chats. The system should classify intents (**security**, **funding**, **trading**, **account help**), detect high-risk content, support multilingual input (including low-resource languages), deliver latency under 50 ms per message on-device, and support offline model updates. Include data lifecycle, privacy guarantees, model versioning, and a live A/B/rollback plan; compare on-device vs cloud/offload trade-offs?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Hugging Face","Robinhood"]},{"id":"q-2700","question":"Design an end-to-end, real-time NLP system to detect misinformation in multilingual social posts that frequently code-switch between English and another language (e.g., Hindi or Spanish). Include data ingestion, code-switch aware embeddings, latency targets, privacy constraints, cross-language evaluation (macro-F1 and per-language fairness), debiasing, explainability (token attributions), and a staged rollout with monitoring and rollback?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["Google","Meta","Salesforce"]},{"id":"q-2720","question":"Design a beginner NLP classifier for multilingual customer support chats that labels intents: 'billing', 'technical', 'account'. Data may include English-Spanish code-switching and emojis. Use a lightweight pipeline: baseline TF-IDF + logistic regression, augmented with character n-grams and emoji tokens. Add a small rule-based post-filter for profanity. Define data split, macro-F1, per-language eval, and a three-stage rollout with monitoring and rollback?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Meta","MongoDB","Twitter"]},{"id":"q-470","question":"How would you implement a sentiment analysis pipeline for customer reviews that handles negation and domain-specific slang? What preprocessing steps would you prioritize?","channel":"nlp","subChannel":"general","difficulty":"intermediate","tags":["nlp"],"companies":["DoorDash","IBM","Square"]},{"id":"q-500","question":"How would you implement basic text preprocessing for sentiment analysis, including tokenization, stop word removal, and stemming?","channel":"nlp","subChannel":"general","difficulty":"beginner","tags":["nlp"],"companies":["Airbnb","Apple","Google"]},{"id":"q-584","question":"How would you implement a transformer-based model for real-time text generation with attention mechanisms that handle variable-length sequences efficiently?","channel":"nlp","subChannel":"general","difficulty":"advanced","tags":["nlp"],"companies":["Databricks","Lyft","Tesla"]},{"id":"q-229","question":"What is the difference between tokenization and stemming in NLP text preprocessing, and when would you choose lemmatization over stemming?","channel":"nlp","subChannel":"text-processing","difficulty":"beginner","tags":["tokenization","stemming","ner"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-294","question":"How does the attention mechanism in transformers allow the model to handle variable-length sequences without recurrent connections?","channel":"nlp","subChannel":"transformers","difficulty":"intermediate","tags":["cnn","rnn","transformer","attention"],"companies":["Amazon","Google","Meta"]},{"id":"q-1124","question":"On a NUMA‑aware Linux host with a fixed‑size worker pool handling high‑frequency RPCs, cross‑socket memory traffic is a bottleneck. Propose a concrete plan to minimize inter‑socket traffic: pin threads to sockets, allocate per‑socket data, and choose memory policies (numa_bind/numa_alloc_onnode). Include measurement steps and success criteria?","channel":"operating-systems","subChannel":"general","difficulty":"advanced","tags":["operating-systems"],"companies":["Discord","Netflix","Salesforce"]},{"id":"q-1299","question":"Scenario: two threads share a global 32-bit counter. Thread A increments it in a tight loop; Thread B logs the value once per second. Without synchronization, describe a concrete interleaving that yields a stale read or lost update, and explain the cache/coherence mechanics behind it. Then outline the minimal fix and how it ensures atomicity and visibility (e.g., atomic fetch_add or a mutex)?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Instacart","Square"]},{"id":"q-1354","question":"You implement a lock-free ring buffer with two atomics (head, tail) and a data array for inter-thread communication. Describe a concrete interleaving on a weak memory model (e.g., ARM64) where the consumer observes a stale value or an invalid read due to missing ordering. Propose a minimal fix using memory_order_release on the data write/tail update and memory_order_acquire on the consumer read, and sketch a safe patch?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Apple","Hugging Face","Meta"]},{"id":"q-1578","question":"On a Linux server, four worker processes share a 100GB read-only memory-mapped dataset loaded from disk on demand via mmap. Describe the sequence of page fault handling, TLB behavior, and how the kernel page cache and optional swap interact with this pattern. Propose two concrete knobs to maximize throughput without starving others (e.g., MADV_WILLNEED with madvise, NUMA binding with mbind) and how you would measure success?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Coinbase","DoorDash","Two Sigma"]},{"id":"q-1733","question":"Scenario: After a fork, a child writes to a Copy-On-Write (COW) page. Describe the end-to-end kernel steps from the write fault to the point where the parent and child have separate views, including page table updates, TLB changes, and how the private copy is created and isolated from the parent's mapping?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Databricks","Discord","Salesforce"]},{"id":"q-1759","question":"In a multithreaded server, each worker maintains a per-thread stats counter in an array of N 64-byte structs (one per thread). A separate thread periodically sums these counters every second. Without padding, explain a concrete interleaving that leads to cache line false sharing and degraded throughput, and propose a fix (padding, alignas cache-line, or per-thread local counters plus a reduction) that preserves correctness and improves performance?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["DoorDash","Netflix"]},{"id":"q-2084","question":"In a kernel memory allocator with per-core freelists and a global free pool protected by a spinlock, describe a concrete interleaving that yields a use-after-free for a block still in use by a reader, and explain how either hazard pointers or epoch-based reclamation prevents it, including the required memory-order guarantees on x86-64 and how grace periods are enforced?","channel":"operating-systems","subChannel":"general","difficulty":"advanced","tags":["operating-systems"],"companies":["Citadel","Netflix","Oracle"]},{"id":"q-2220","question":"Scenario: A process has two threads: T1 holds a mutex to update a shared log buffer; T2 is blocked trying to acquire the same mutex. A SIGINT is delivered to the process while T1 is in the middle of updating. Explain the sequence of events from signal delivery to termination, including the mutex state, potential race conditions, and safe patterns for signal handling in multi-threaded programs (like async-signal-safe handlers and deferred cleanup via a dedicated signal-handling thread or self-pipe)?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Coinbase","Google","NVIDIA"]},{"id":"q-2280","question":"In a simplified OS with a file-backed page cache, a process repeatedly calls read() to fetch 4 KiB blocks from a file. If a 4 KiB block is not in the file system page cache, describe step-by-step what the kernel does from read() entry to user-space return, including page-cache lookup, block-device reads, potential read-ahead, and how the data ends up in the user buffer. Compare a cache miss vs a cache hit path and timing?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Apple","MongoDB"]},{"id":"q-2404","question":"A multi-threaded app writes to a single file via buffered I/O on a journaling filesystem. Thread A appends 64KiB blocks; Thread B occasionally calls fsync(). Describe the path from write through the page cache to disk, including journal commits and barriers, and give a concrete interleaving where a crash can lose data or leave metadata only. Propose a minimal patch to guarantee durability (e.g., fdatasync after writes)?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Citadel","Databricks","NVIDIA"]},{"id":"q-2477","question":"Explain how Linux handles a write to a 2MB Transparent Huge Page (THP) that only partially overlaps with a 4KB region. Describe the fault path, the split_huge_page path (and khugepaged if applicable), PTE updates, TLB shootdowns, and the performance implications vs pre-splitting THPs?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Meta","Twitter"]},{"id":"q-2544","question":"Scenario: A process memory-maps a 4 KiB device block via mmap with MAP_SHARED and then writes through a user pointer that maps to that page. Explain end-to-end how the kernel handles the write, including the MMU page fault, page cache interaction, dirty bit propagation, writeback, and how the data reaches the physical device, highlighting synchronization with other mappings and potential coherence issues?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Coinbase","OpenAI","PayPal"]},{"id":"q-2714","question":"In a high-throughput key-value service on Linux/x86_64, two threads share a 128-byte struct: a 64-bit value and a 64-bit sequence counter used for a sequence lock (seqlock). Describe how to implement a lock-free reader with a writer that updates the value safely, specifying the exact operation order, required memory barriers, and how you verify correctness under contention. Compare against a mutex approach and practical tradeoffs?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Apple","Instacart","Meta"]},{"id":"q-448","question":"Explain how process scheduling works in an operating system. Which scheduling algorithm would you choose for a real-time system and why?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["DoorDash","OpenAI","Snap"]},{"id":"q-471","question":"You're designing a GPU memory manager for CUDA applications. How would you implement a memory allocator that handles both unified memory and explicit device memory, considering fragmentation, coalescing, and the 48-bit address space limitations?","channel":"operating-systems","subChannel":"general","difficulty":"advanced","tags":["operating-systems"],"companies":["Amazon","NVIDIA"]},{"id":"q-530","question":"A process is stuck in 'D' state (uninterruptible sleep) during I/O operations. How would you debug this, what causes it, and how does it differ from 'Z' zombie state?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Adobe","LinkedIn","Netflix"]},{"id":"q-556","question":"How would you debug a process that's consuming 100% CPU but not responding to signals? What tools and steps would you use?","channel":"operating-systems","subChannel":"general","difficulty":"intermediate","tags":["operating-systems"],"companies":["Snowflake","Two Sigma"]},{"id":"q-585","question":"How would you implement a lock-free concurrent queue using atomic operations and memory barriers? What are the trade-offs between ABA problem solutions?","channel":"operating-systems","subChannel":"general","difficulty":"advanced","tags":["operating-systems"],"companies":["Citadel","OpenAI","Square"]},{"id":"q-927","question":"In a system with a fixed-size circular buffer of size N shared by a producer and a consumer thread, implement a thread-safe producer-consumer solution using semaphores and a mutex in C. Include initialization, edge cases (buffer full/empty), and show how you would test for deadlocks and correctness under concurrent producers/consumers?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Adobe","OpenAI","Scale Ai"]},{"id":"q-995","question":"In a system using paging with a TLB, describe the end-to-end sequence when a 4 KB page accessed by a process is not mapped in RAM, from fault to resume, including the fault handler, page-table walk, TLB update, disk I/O to swap, and how the eviction policy decides which page to replace?","channel":"operating-systems","subChannel":"general","difficulty":"beginner","tags":["operating-systems"],"companies":["Lyft","NVIDIA","Stripe"]},{"id":"q-263","question":"How does demand paging optimize memory utilization in virtual memory systems, what triggers page faults, and which algorithms handle page replacement when physical memory is full?","channel":"operating-systems","subChannel":"memory","difficulty":"intermediate","tags":["virtual-memory","paging","segmentation","cache"],"companies":["Amazon","Apple","Google","Meta","Microsoft","Netflix"]},{"id":"q-1070","question":"You are building a latency-sensitive telemetry ingestion pipeline for autonomous fleets. Design an end-to-end data path from edge sensors to a real-time analytics sink. Include data schemas, serialization (protobuf vs Avro), transport (Kafka vs Kinesis), fault tolerance (idempotency, replay), schema evolution, and testing strategy. Explain trade-offs for throughput, latency, and reliability?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Lyft","NVIDIA","Tesla"]},{"id":"q-1090","question":"Design a real-time notification system for 1M+ concurrent users with end-to-end latency under 200ms. Describe architecture, data model, delivery guarantees, back-pressure, disaster recovery, and how you measure and enforce SLOs?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Apple","Cloudflare","Discord"]},{"id":"q-1115","question":"In a global OTCA stack for a large platform, design a fault-tolerant telemetry pipeline that ingests 10k events/sec, preserves dashboards with sub-30s freshness, and scales regionally. Describe data model, streaming, storage, and tracing choices (e.g., Kafka, OpenTelemetry, ClickHouse/BigQuery), backpressure handling, and testing strategy?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["DoorDash","Meta","Twitter"]},{"id":"q-1171","question":"Design a multi-tenant OTCA telemetry pipeline with per-tenant quotas and privacy masking. Ingest 20k events/sec regionally via Kafka, route by tenant_id, and compute deduplicated aggregates in Flink. Hot metrics in Redis, long-term data in ClickHouse/BigQuery, traces via OpenTelemetry. Include idempotent writes, backpressure handling, and robust testing?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Adobe","Coinbase","Google"]},{"id":"q-1219","question":"In a multi-tenant OTCA pipeline for a global fintech platform, how would you implement tenant-scoped telemetry collection that isolates data, preserves sub-second dashboards, and enforces per-tenant quotas? Describe data model, per-tenant exporters, sampling, storage with row-level security, and testing strategy?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Goldman Sachs","PayPal"]},{"id":"q-1237","question":"In a multi-tenant OTCA telemetry stack for a global SaaS, migrate from a fixed event schema to an evolving, backward/forward-compatible schema while preserving per-tenant data residency. The system must sustain up to 60k events/sec with regional bursts. Describe data model, schema versioning, streaming backbone, storage strategy (raw + materialized), tracing, backpressure handling, tenant-level sampling, and testing plan?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Databricks","MongoDB","Zoom"]},{"id":"q-1286","question":"In a global OTCA stack, design a fault-tolerant telemetry pipeline that ingests 20k events/sec per region from web/mobile clients, enforces per-tenant data residency, and uses adaptive sampling for long-tail tenants. Specify data model, streaming, storage, tracing, backpressure, and a test plan that validates burst behavior, schema evolution, and failover?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Meta","Snowflake"]},{"id":"q-1307","question":"In a global OTCA telemetry stack with three regions, enforce tenant residency by region while enabling real-time global dashboards with sub-500ms latency. Provide the end-to-end ingestion, storage, and aggregation plan, including data model, streaming/backplane choices, per-tenant partitioning, cross-region replication policy, and a validation strategy for residency, schema evolution, and burst traffic?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Cloudflare","Oracle","Twitter"]},{"id":"q-1495","question":"In a two-region OTCA telemetry pipeline for a microservices platform, design a privacy-preserving, adaptive sampling plan for distributed traces that enforces per-tenant data residency, minimizes data egress, and sustains sub-400ms end-to-end latency for dashboards. Detail the trace data model, propagation scheme, sampling algorithm, backpressure handling, and a validation plan?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["LinkedIn","NVIDIA","Snap"]},{"id":"q-1598","question":"Design a lightweight client-side OTCA telemetry exporter for a mobile app used across regions. The exporter must batch events (5 seconds or 1000 events), attach fields: tenant_id, device_id, app_version, event_type, and timestamp; implement offline queuing with local storage, retry with exponential backoff and jitter, and ensure eventual delivery when connectivity returns. Describe data model, batching, retry, and testing plan, plus how you measure correctness and dashboards?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["Anthropic","Tesla","Zoom"]},{"id":"q-1731","question":"In a global OTCA telemetry stack spanning five regions, tenants' raw events must remain within their origin region; only anonymized aggregates cross regions for global dashboards with sub-200ms latency. Design the end-to-end ingestion, streaming, storage, and access controls. Specify data models, de-identification/privacy controls, cross-region aggregation, backpressure, and a testing plan to validate residency, privacy, and latency under burst traffic?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Bloomberg","Microsoft","Oracle"]},{"id":"q-1847","question":"For a global OTCA telemetry stack supporting a multi-tenant ML inference platform, design a region-aware telemetry pipeline that records: tenant_id, model_id, input_hash, latency_ms, outcome, and drift_score. Propose a streaming backbone, OLAP store, and a per-tenant residency policy with SLA-based QoS, plus backpressure, schema evolution, and testing plan?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Hugging Face","Robinhood","Two Sigma"]},{"id":"q-1900","question":"You're building a beginner OTCA telemetry pipeline for a mobile app used in two regions. Design a minimal streaming path that logs only essential fields (tenant_id, event_type, timestamp, latency_ms) and enforces per-tenant data access and privacy (redaction/anonymization). Describe the data model, streaming backbone, storage, and a practical test plan to validate privacy, QoS, and dashboard freshness (<=60s)?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["Apple","Microsoft"]},{"id":"q-1924","question":"Design a beginner OTCA telemetry path for a mobile app where events include tenant_id, event_type, timestamp, latency_ms. Implement a simple on-device dedup using event_id, normalize event_type into a canonical event family, and publish to a single Kafka topic with per-tenant routing to regional storage (Parquet on S3) to meet residency. Describe data model, streaming path, storage layout, and a minimal test plan validating dedup, schema evolution, and cross-region consistency?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["Amazon","Plaid"]},{"id":"q-1999","question":"Design a global OTCA telemetry pipeline for real-time feature experimentation across MongoDB, Netflix, and Nvidia workloads. Each event includes device_id, experiment_id, feature_id, timestamp, latency_ms, and consent_flag. Requirements: per-tenant QoS with adaptive sampling, privacy masking for device_id, versioned schema with backward compatibility, cross-region attribution, and sub-200ms dashboard freshness. Outline data model, streaming backbone, storage layout, backpressure handling, and testing strategy?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["MongoDB","NVIDIA","Netflix"]},{"id":"q-2044","question":"Design a beginner OTCA telemetry path for a mobile app that must enforce per-tenant residency, basic privacy redaction, and monthly cost quotas while handling up to 5k events/sec. Provide a concrete data model (tenant_id, event_type, timestamp, latency_ms, event_id), a streaming topology (on-device dedup, region-specific Kafka, region-local Parquet storage with cross-region replication), and a test plan to verify residency, privacy, dedup, quota enforcement, and dashboard freshness?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["NVIDIA","Oracle","Scale Ai"]},{"id":"q-2157","question":"Design a federated OTCA telemetry pipeline for a multi-tenant platform deployed on AWS, GCP, and Azure, where each event includes tenant_id, service_id, event_type, timestamp, latency_ms, and model_version. Propose a central schema registry with per-tenant Protobuf contracts and strict forward/backward compatibility, a streaming backbone with tenant-scoped topics (e.g., Pulsar), cross-cloud replication, and a compliant data deletion/retention policy. Outline data contracts, backfill strategy, testing, and observability?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Anthropic","Plaid","Salesforce"]},{"id":"q-2198","question":"Design an edge-to-cloud OTCA telemetry pipeline for a latency-sensitive mobile app with intermittent connectivity. Edge devices buffer per-tenant events locally and flush when online; ensure per-tenant isolation, at-least-once delivery with de-dup, and sub-5s dashboard freshness after reconnection. Describe data model, edge processing, local storage, backpressure, retry, and testing plan with failure scenarios?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Hashicorp","Plaid","Two Sigma"]},{"id":"q-2358","question":"You're deploying an edge-to-cloud OTCA telemetry path for a fleet of Nvidia Drive-like autonomous robots across two continents. Each robot emits ~10k events/sec; dashboards must refresh in under 15 seconds with per-tenant residency. Design the data model, streaming backbone, storage format, backpressure, privacy, and testing strategy, including failure scenarios and cross-region replication?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Lyft","NVIDIA","Tesla"]},{"id":"q-2452","question":"In a global OTCA telemetry pipeline for a multi-tenant SaaS platform, design a policy-driven per-tenant data lifecycle (e.g., 7, 30, 90, 365 days) that governs hot storage retention, archiving, and purging. Describe data model changes, a central policy store, per-tenant routing to regional storage, an immutable journaling layer, cross-region archiving, and testing plan for lifecycle transitions?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Citadel","Oracle","Tesla"]},{"id":"q-2473","question":"Design an OTCA path for a real-time trading platform to capture per-venue latency, routing decisions, and queue depth. Stream events to venue-partitioned sinks (per-venue Kafka topics with region routing) and store immutable, versioned logs in regional object stores with a write-once policy. Enforce residency by region, apply in-flight caps and lag throttling, test with synthetic bursts and replay checks?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Citadel","Robinhood"]},{"id":"q-2505","question":"In an OTCA telemetry pipeline spanning multiple regions with edge gateways that reconnect intermittently, design a region-aware, edge-first streaming path that ingests up to 20k events/sec. Include local bounded buffering, per-tenant privacy masking, dedup logic, eventual consistency on reconnect, Avro schema evolution, and cross-region aggregation. Provide data model, streaming backbone, storage plan, backpressure strategy, and a practical test plan?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Cloudflare","Meta","Oracle"]},{"id":"q-2600","question":"Scenario: A fintech-grade, multi-tenant OTCA telemetry pipeline must enforce per-tenant data residency, privacy, and SLA-bounded costs across mobile and embedded devices. Design: on-device aggregation, per-tenant sampling, and cross-region routing. Specify data model, streaming backbone, schema evolution, privacy controls, testing plan, and failure handling with backpressure?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Apple","PayPal","Stripe"]},{"id":"q-2634","question":"You operate a global OTCA telemetry stack for a multi-tenant analytics platform with 1,000 tenants across 5 regions. Design a tamper-evident audit trail path that supports per-tenant data retention and forensic replay while guaranteeing data integrity and compliance. Describe the data model, streaming backbone, immutability strategy, cross-region replication, and testing plan for tamper detection and replay accuracy?","channel":"otca","subChannel":"general","difficulty":"advanced","tags":["otca"],"companies":["Adobe","Microsoft","Scale Ai"]},{"id":"q-838","question":"You’re building a minimal product API in Express. Implement routes GET /products and GET /products/:id that returns 404 when not found. Include input validation, safe DB access via parameterized queries, and robust error handling. Explain your approach and provide a concise code sample?","channel":"otca","subChannel":"general","difficulty":"beginner","tags":["otca"],"companies":["Airbnb","Discord","Meta"]},{"id":"q-908","question":"You have a global edge API gateway handling peak bursts. How would you implement a distributed token-bucket rate limiter per API key across regions using Redis? Outline the Lua script for atomic refill and consume (capacity 1000, refill 50 tokens/sec), how you expose headers, handle clock skew, and fallback when Redis is unavailable?","channel":"otca","subChannel":"general","difficulty":"intermediate","tags":["otca"],"companies":["Bloomberg","Cloudflare","Square"]},{"id":"q-1012","question":"In a churn prediction problem, you have 20k customers and 500 features (mix of binary indicators and continuous metrics). PCA will be used before a logistic regression model to predict churn. Describe an end-to-end plan to (1) handle missing values and mixed data types, (2) scale features appropriately, (3) choose the number of components with cross-validated downstream performance, (4) interpret the top loadings for business insight, and (5) guard against leakage and overfitting in a production pipeline?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["DoorDash","NVIDIA","Plaid"]},{"id":"q-1068","question":"Design an online robust PCA for a streaming fraud-detection pipeline: 50k events/sec, 1000 features with missing values. Describe incremental component updates, robust outlier handling (robust PCA or GoDec variants), missing-data strategy, drift monitoring (eigenvalue gaps, loadings stability, score distribution), and how you’d validate downstream classifier performance under tight memory/time constraints?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Meta","Oracle","Tesla"]},{"id":"q-1104","question":"In a production streaming recommender system, you maintain an incremental PCA basis on 1M users and 3k features, updated hourly. Design an online robust PCA pipeline that adapts to non-stationary covariances, handles missing data, and detects concept drift. Describe algorithm choices (incremental/robust variants, forgetting factors), when to re-train vs update, how to align with past loadings, validation of downstream models after projection, and monitoring for numerical stability?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Airbnb","Goldman Sachs","Google"]},{"id":"q-1128","question":"You have 50k samples, 1k gene-expression features with many missing values. Design an end-to-end Sparse PCA pipeline to produce 40 interpretable components. How would you handle missing data, choose sparsity vs components, validate downstream models, and assess stability and biological coherence of loadings across folds?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Adobe","Twitter","Two Sigma"]},{"id":"q-1153","question":"Design a PPCA-based dimensionality reduction pipeline for real-time telemetry data: 1B feature vectors daily, 500 real-valued features with missing values and skew, to feed a downstream anomaly detector. Explain fitting PPCA with EM, selecting k via BIC on a rolling window, comparing to standard PCA, handling streaming updates with forgetting factors, and validating in production?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Salesforce","Zoom"]},{"id":"q-1175","question":"In a factory IoT setting, 20 devices stream 40 features each (numeric, with occasional missing values). You want a beginner-friendly PCA-based anomaly detector on the edge. Describe how you would handle missing values, decide the number of components, and translate top loadings into actionable maintenance signals for operators, while keeping the model lightweight on-device?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Apple","Twitter"]},{"id":"q-1218","question":"You have a 40-feature numeric customer-survey dataset with some missing values and skewed distributions. You want a beginner-friendly PCA-based feature set for a churn-classification model. Describe preprocessing steps (imputation, transformations, outlier handling), how to choose the number of components, and how to translate top loadings into concrete business signals for a dashboard while keeping the pipeline lightweight?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Databricks","Google"]},{"id":"q-1249","question":"You're building a real-time risk-scoring system for cross-border payments. Data arrives as numeric features with occasional missing values and a few graph-derived signals, streaming at high velocity. You need an incremental PCA that adapts to non-stationary distributions and yields 40 components. Describe how you would: (a) choose/update the number of components under drift, (b) perform online imputation without data leakage, (c) keep loadings interpretable for dashboards, (d) coordinate PCA updates with downstream models to control drift, and (e) design a robust rollback strategy with governance in production?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Cloudflare","Coinbase","Uber"]},{"id":"q-1288","question":"Design a daily-updated PCA-based representation for streaming telemetry with 300 features per vector, many missing values and sparse signals. Outline preprocessing, choice of incremental PCA approach (IPCA vs randomized SVD), when to refresh the basis, how to align new loadings with the existing basis, and how to validate the downstream anomaly detector after dimensionality reduction. Include concrete knobs (batch size, forgetting factor, drift thresholds)?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Google","Snowflake"]},{"id":"q-1306","question":"Imagine a factory IoT setup where edge devices stream 60 numeric features (with occasional missing values). You need a beginner-friendly, PCA-based anomaly detector that runs on-device. Outline a concrete pipeline: (a) how to handle missing values, (b) how to standardize and fit PCA (including incremental options), (c) how to decide the number of components for robust anomaly signals, and (d) how to translate top loadings into actionable operator cues on a dashboard or device indicator?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["MongoDB","Tesla","Uber"]},{"id":"q-1350","question":"Design a privacy-preserving, incremental PCA system for a fintech platform with 2M daily sessions and 150 numeric features, where missing values occur and regulatory privacy requires differential privacy. You must deliver a 50-component representation, support federated updates, monitor drift, and keep loadings interpretable for dashboards. Describe architecture, privacy budget, and an evaluation plan; include a concrete update protocol?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["PayPal","Salesforce"]},{"id":"q-1387","question":"Design an online, privacy-preserving PCA for streaming multi-tenant transaction data to support real-time fraud detection. Specify how to perform incremental PCA with a sliding window, apply differential privacy to loadings, detect drift and decide when to refresh the basis, handle missing values online, allocate the privacy budget, and validate downstream models under DP constraints. Include concrete metrics and thresholds?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Cloudflare","MongoDB","PayPal"]},{"id":"q-1467","question":"In a real-time analytics pipeline that ingests 50k vectors/sec, each with 128 features (some missing), you previously computed offline **PCA** on historical data. Design a streaming, incremental **PCA** approach to (a) decide when to refresh the basis, (b) handle missing values on arrival, (c) monitor component drift, and (d) validate downstream models after dimensionality reduction?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Google","Stripe","Twitter"]},{"id":"q-1512","question":"Design a real-time PCA-based anomaly detector for a streaming platform like Airbnb where a listing's feature vector mixes 150 numeric sensor values and 30 one-hot categorical indicators; describe an incremental PCA workflow that handles mixed data, missing values, and concept drift while maintaining interpretability and low latency; specify preprocessing, component refresh triggers, evaluation, and how to map loadings back to actionable signals?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Airbnb","Salesforce","Scale Ai"]},{"id":"q-1534","question":"Edge PCA on 30 numeric sensors: design a beginner-friendly pipeline that runs on a Raspberry Pi for real-time data compression and anomaly flagging. Describe normalization and simple imputation, how to pick components (explained variance with knee), how to interpret top loadings for operators, and a lightweight drift check over a day with few dependencies?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["IBM","NVIDIA","Uber"]},{"id":"q-1703","question":"Scenario: A healthcare analytics team has a dataset with 100 predictors, mix of continuous and binary indicators, plus missing values. They want to apply PCA to reduce to 5 components for a dashboard and as features for a simple logistic classifier. Describe the exact preprocessing steps, including handling missing data, dealing with binary features during PCA, scaling, and how you would decide the number of components. How would you translate the top loadings into interpretable dashboard signals for clinicians, and how would you validate this approach on a small holdout set?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Google","IBM","Two Sigma"]},{"id":"q-1863","question":"You're building a real-time fraud-detection pipeline that ingests 200 features per transaction, with many outliers and missing values. You choose RPCA to reduce to 6 components for an online anomaly detector and a logistic classifier. Outline concrete steps: how to impute/mangle missing data for RPCA, how to handle heavy tails, how to select k using cross-validated reconstruction error with a complexity penalty, how RPCA would differ from standard PCA in this scenario, how to implement incremental RPCA updates (forgetting factor, windowed EM) in streaming, and how to map top loadings to business signals in a dashboard and detect component drift across batches?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Coinbase","Discord","Snap"]},{"id":"q-2034","question":"Design an online, memory-bounded PCA system for a fleet of 1,000 warehouse-edge sensors producing 200 features per vector. Data include missing values and heavy-tailed noise. Describe end-to-end: (a) how many components to retain under a fixed RAM, (b) how to incrementally update the PCA basis and trigger refresh on drift, (c) online imputation for missing values, (d) how to translate top loadings into actionable alerts for maintenance or throughput, and (e) how to validate offline and online performance. Include concrete metrics and thresholds?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Anthropic","Databricks","Instacart"]},{"id":"q-2106","question":"You have a production-monitoring dataset with 60 numerical telemetry features and 30 binary indicators across 20 services. For a PCA-based compression to 5 components powering a dashboard health score, outline practical preprocessing (imputation, scaling, handling binary features), how to pick the 5 components (explained variance threshold and cross-validated downstream anomaly detection), and how to translate the top loadings into actionable operator signals, keeping the pipeline lightweight?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Hashicorp","PayPal"]},{"id":"q-2164","question":"In a live ad-placement system, a streaming feature set of 320 numeric features arrives at 5 Hz with intermittent missing values and occasional outliers. Design an online, robust PCA (sliding-window, robust covariance, and outlier-resilient loadings) that maintains a 40-component basis. Explain how you would (1) update the basis with drift detection, (2) handle missing data online without corrupting the basis, (3) translate top loadings into low-latency serving signals, and (4) validate downstream models under non-stationarity?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Discord","DoorDash","Twitter"]},{"id":"q-2259","question":"Given a retail analytics dataset with 50 numeric features, including binary flags and ordinal categories, you plan to run PCA on-device to reduce to 6 components for a lightweight gateway in a store. How would you handle mixed data types, imputation, scaling, component selection, and translating top loadings into concrete gateway actions while keeping compute and memory usage low?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Cloudflare","Square","Tesla"]},{"id":"q-2287","question":"How would you implement an incremental PCA-based real-time feature extractor for a streaming ad-click dataset with 120 numeric features (including many missing values) and 40 categorical features encoded via target encoding? Describe preprocessing (imputation, scaling, sparse data handling), incremental PCA update (k choice, forgetting factors), drift detection for explained variance and loadings, and how to translate top loadings into dashboard signals while avoiding leakage of sensitive target info?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["DoorDash","LinkedIn","Snap"]},{"id":"q-2313","question":"Design a privacy-preserving federated PCA-based anomaly detector across multiple regional data centers. Each site has 60k–120k samples with 150 numeric features, with missing data. No raw data leaves sites; a central server learns a global rank-k PCA basis via secure aggregation and incremental updates. Describe the end-to-end protocol: initialization, local imputation, online PCA updates, drift detection, handling non‑IID data, communication budget, and edge deployment constraints. Include concrete metrics and thresholds?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Citadel","Slack","Snap"]},{"id":"q-2372","question":"You operate a Netflix/Plaid-scale real-time content recommender with a 180-feature numeric telemetry table (e.g., interactions, dwell time, buffer events). Data arrives as a stream with occasional missing values. You decide to use an online incremental PCA to maintain a rank-k basis for a lightweight ranking model. Describe how you would: (1) select k under drift; (2) update the PCA online with a forgetting factor and missing-data imputation; (3) detect concept drift and trigger retraining; (4) translate top loadings into actionable signals for the live dashboard; and (5) validate both offline and in production, including a practical rollback plan?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Netflix","Plaid"]},{"id":"q-2506","question":"Scenario: In a fintech analytics gateway used by Plaid, Zoom, and PayPal, you process hourly matrices of 40 numeric features per session. Data are skewed and occasionally missing. Design a beginner PCA pipeline to feed a lightweight anomaly detector on the gateway; describe skew handling, missing value strategy, component selection, and how to translate top loadings into actionable alerts while keeping latency low?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["PayPal","Plaid","Zoom"]},{"id":"q-2534","question":"Design an online PCA system for streaming telemetry with 200 features arriving in minute batches from thousands of devices in a Databricks/Zoom-scale analytics gateway. Explain incremental updates, adaptive component count, online normalization and missing-value handling, drift detection, and how to translate top loadings into real-time alert rules while maintaining sub-second latency?","channel":"pca","subChannel":"general","difficulty":"advanced","tags":["pca"],"companies":["Databricks","Zoom"]},{"id":"q-2672","question":"Scenario: DoorDash handles streaming order features with 120 numeric indicators per event. Data drifts over time and some values are missing skewed. Propose an online PCA-based detector to power a lightweight anomaly alert in real-time routing and fraud checks. Describe incremental PCA approach, window size, handling of missing/skewed data, component selection, and how to translate top loadings into actionable alerts under strict latency constraints?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["DoorDash","Oracle"]},{"id":"q-2705","question":"Scenario: A wearable device streams 70 numerical sensor readings every second into a mobile gateway with 128 KB RAM. You need a beginner-friendly PCA-based anomaly detector running on-device. How would you design the pipeline, including (a) data preprocessing, (b) incremental PCA configuration and update policy, (c) component count strategy under memory limits, and (d) translating top loadings into real-time alerts while keeping latency under 25 ms?","channel":"pca","subChannel":"general","difficulty":"beginner","tags":["pca"],"companies":["Cloudflare","Lyft","Meta"]},{"id":"q-837","question":"In a dataset with 120k samples and 200 features, after standardizing, you fit PCA and observe 95% variance explained by the first 8 components. How would you validate using PCA for downstream linear regression, decide the number of components, and interpret the top loadings? Consider missing values and large-scale data in your answer?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Amazon","Google","Plaid"]},{"id":"q-934","question":"Suppose a streaming analytics pipeline ingests 10k new vectors daily, each with 180 features, and PCA was computed on historical data. Describe an end-to-end approach to decide when to refresh the PCA vs keep the existing basis, how to measure component drift, how to align new loadings with the old basis, how to handle missing values in streaming data, and how to validate downstream models after dimensionality reduction?","channel":"pca","subChannel":"general","difficulty":"intermediate","tags":["pca"],"companies":["Citadel","Discord","Hashicorp"]},{"id":"q-1117","question":"You're adding a real-time AI-powered product search for an Instacart-like app. With 8,000 concurrent users during lunch peak, design a beginner-friendly performance test that isolates the ML inference path from normal search, including cache warming and a clear success criteria?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Instacart","Snap","Zoom"]},{"id":"q-1210","question":"You're introducing a search API for a streaming e-commerce platform during a flash sale; expect 8k concurrent users, target median latency under 120ms and p99 under 250ms. The stack uses Redis caching with a relational DB. Design a beginner-friendly performance test plan to validate this, including tools, test data strategy, ramp pattern, metrics (p50, p90, p95, p99, error rate), and how you identify bottlenecks without affecting production?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["DoorDash","Netflix","Snowflake"]},{"id":"q-1533","question":"You're benchmarking a real-time bidding platform that processes 200k events per second with a 150ms SLA, deployed on Kubernetes with horizontal autoscaling and external services (ad server, fraud check, payment). Design a practical performance testing plan to identify bottlenecks and validate autoscaling, including workload mix, metrics, tools, and failure scenarios?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["IBM","LinkedIn"]},{"id":"q-1697","question":"You're benchmarking a real-time IoT telemetry pipeline: 2M events/sec ingested into Kafka, processed by Spark Structured Streaming, stored to a warehouse, and routed to a ML inference service with a 100ms SLA. Design a performance test plan to validate end-to-end latency, backpressure handling, and autoscaling when downstream latency spikes cause backlog. Include workload mix, metrics, tooling, and failure scenarios?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["Google","MongoDB","PayPal"]},{"id":"q-1712","question":"In a real-time payments microservice stack used by Amazon/PayPal, the /payments/process path calls fraud checks, credit checks, and a settlement queue. Bursts occur daily, with architectures needing to verify latency SLAs, error budgets, and autoscaling. Design a practical performance-test plan: workload model, test harness, metrics, failure scenarios, and how you would handle external-service variability and idempotency?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["Amazon","PayPal"]},{"id":"q-1803","question":"Design a performance-test for a real-time trade-confirmation service handling 150k events/s with an 8 ms SLA (99th percentile) during market open. System: Kubernetes, Kafka, Postgres, Redis; multi-region. Validate autoscaling, backpressure, circuit breakers, and tail latency under bursty traffic and regional failover. Outline workload, metrics, tools, and failure scenarios?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["Amazon","IBM","Robinhood"]},{"id":"q-2107","question":"Design a performance-test plan for a real-time personalized recommender serving 3 regional markets and relying on a centralized feature-flag service to drive A/B experiments. The flag service can throttle or fail under load. Outline traffic modeling, latency/outage injection, metrics (end-to-end P95/P99, throughput, error rate), backpressure handling, circuit-breakers, and graceful degradation criteria to validate SLOs?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Databricks","Tesla","Twitter"]},{"id":"q-2147","question":"You're performance-testing a ride-booking app's driver-matching API used by 3,000 concurrent riders during a rainstorm. The API uses an in-memory queue before dispatching to drivers. Design a beginner-friendly test plan to quantify end-to-end latency, identify bottlenecks in queue processing, and validate the impact of increasing the queue size?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Hugging Face","IBM","Meta"]},{"id":"q-2183","question":"In a ride-hailing app, the trip-matching service accepts ride requests and matches with drivers through an asynchronous pipeline using Kafka, with end-to-end SLA of 500ms for 95th percentile under peak load. Design a performance-testing plan to validate tail latency, backpressure handling, and autoscaling. Include workload mix, metrics, tools, and failure scenarios?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["Meta","Stripe"]},{"id":"q-2247","question":"You're performance testing a serverless data ingestion API on AWS that sees bursts of 1,000 requests per second and where Lambda cold starts spike latency. Design a practical, beginner-friendly test to quantify cold-start impact, isolate bottlenecks in the ingestion path, and propose mitigations (provisioned concurrency, warmups, or VPC endpoints)?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Citadel","Meta"]},{"id":"q-2261","question":"You're evaluating a Kafka-based streaming pipeline (producer -> Kafka -> Flink -> sink) that must sustain 2x peak event rate with 95th percentile end-to-end latency under 300 ms. Design a performance test plan that (a) models bursty arrival with backpressure, (b) validates partition rebalance under scale-out, (c) measures consumer lag and checkpoint impact, and (d) proposes instrumentation and success criteria. How would you execute this end-to-end?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Hashicorp","OpenAI","Scale Ai"]},{"id":"q-2342","question":"You're operating a video-on-demand service with a CDN in front of a regional origin. During a spike to 5x peak traffic, origin latency spikes and startup stalls for many users. Design a beginner-friendly performance test to isolate CDN edge caching vs origin impact, specifying test inputs, metrics, and how you would validate TTL and cache-busting strategies?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Amazon","Google","Uber"]},{"id":"q-2409","question":"Design a performance test for a real-time analytics pipeline: 1–2M events/sec ingested via Kafka, processed by a Spark Structured Streaming job, writes to MongoDB, and dashboards reading from a materialized view. The test must quantify end-to-end tail latency under data skew, backpressure, and autoscaling of Spark workers and MongoDB shards, with realistic burst patterns and data privacy constraints. Outline workload models, metrics, and failure scenarios?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["IBM","MongoDB","Two Sigma"]},{"id":"q-2521","question":"Design a performance test for a Netflix-like live-transcoding pipeline that ingests 50k streams/sec bursts, with a 200ms SLA on first-5-second segment encoding, using Redis for metadata, MongoDB for persistence, and Kubernetes-backed microservices. Outline workload, metrics (p95/p99, tail latency, saturation), bottlenecks, and failure scenarios, including backpressure and circuit-breaking?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["MongoDB","Netflix","Scale Ai"]},{"id":"q-2670","question":"You're performance testing a BI dashboard SaaS that queries a shared data warehouse; during bursts, end-to-end latency increases 2-3x. Design a beginner-friendly test to isolate query latency from rendering; specify workload parameters, metrics, and how you would compare warm vs cold runs and measure tail latency?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Meta","Salesforce","Snowflake"]},{"id":"q-472","question":"You're load testing a high-frequency trading platform that processes 100K requests/second. Your load generator becomes the bottleneck. How would you design a distributed load testing architecture to accurately simulate production traffic patterns?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Goldman Sachs","Two Sigma"]},{"id":"q-501","question":"You're testing a grocery delivery app like Instacart that handles 10,000 concurrent users during peak hours. How would you design a performance testing strategy to identify bottlenecks in the order processing pipeline?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Instacart","Tesla"]},{"id":"q-531","question":"You're load testing a food delivery platform's order processing system. How would you design a performance testing strategy to identify bottlenecks during peak lunch hours (12-2 PM) when order volume increases 10x?","channel":"performance-testing","subChannel":"general","difficulty":"advanced","tags":["performance-testing"],"companies":["Citadel","DoorDash"]},{"id":"q-557","question":"You're load testing a trading platform that processes 10,000 orders/second. Your load generator shows 95th percentile latency at 200ms, but actual users report 2-3 second delays. What's happening and how would you diagnose it?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["NVIDIA","Robinhood"]},{"id":"q-586","question":"How would you measure and optimize the performance of a REST API endpoint that's responding slowly?","channel":"performance-testing","subChannel":"general","difficulty":"beginner","tags":["performance-testing"],"companies":["Plaid","Uber"]},{"id":"q-996","question":"Design a performance testing plan for a MongoDB-backed ride-hailing backend where a single /alloc-trip endpoint coordinates availability updates and cross-service trip allocations under production-like bursts up to 50k RPS; outline how you would identify bottlenecks across DB, services, and network, and concrete steps to reduce tail latency?","channel":"performance-testing","subChannel":"general","difficulty":"intermediate","tags":["performance-testing"],"companies":["Google","MongoDB","Uber"]},{"id":"gh-40","question":"What is Performance Testing and how does it differ from Load and Stress Testing?","channel":"performance-testing","subChannel":"load-testing","difficulty":"beginner","tags":["perf","testing"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-41","question":"What are the different types of performance testing and when would you apply each type in a real-world scenario?","channel":"performance-testing","subChannel":"load-testing","difficulty":"intermediate","tags":["perf","testing"],"companies":["Amazon","Google","Meta"]},{"id":"q-237","question":"How would you design a distributed load testing setup using k6 with multiple cloud regions to simulate 100k concurrent users while avoiding rate limiting and ensuring accurate metrics collection?","channel":"performance-testing","subChannel":"load-testing","difficulty":"intermediate","tags":["jmeter","k6","gatling","locust"],"companies":["Amazon","Google","Netflix","Stripe","Uber"]},{"id":"q-210","question":"How would you implement comprehensive CPU profiling with flame graphs using clinic.js and async hooks to identify performance bottlenecks in a Node.js microservice handling concurrent requests, including production considerations and memory leak detection?","channel":"performance-testing","subChannel":"profiling","difficulty":"intermediate","tags":["cpu-profiling","memory-profiling","flame-graphs"],"companies":["Amazon","Cloudflare","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-280","question":"What is the difference between CPU profiling and memory profiling, and when would you use a flame graph?","channel":"performance-testing","subChannel":"profiling","difficulty":"beginner","tags":["cpu-profiling","memory-profiling","flame-graphs"],"companies":["Amazon","Google","Microsoft","Netflix"]},{"id":"q-1024","question":"You're building a prompt-routing system for a consumer-support assistant serving Apple, Airbnb, and Snap customers. It must decide auto-response, request clarification, or escalate to a human agent, based on intent, risk, PII presence, and policy compliance. Describe end-to-end design, including a 3-template prompt bank (concise, friendly, authoritative), a safety/brand-voice rubric, and a minimal Python prototype that routes auto vs escalate under edge cases. Include a testing plan?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Airbnb","Apple","Snap"]},{"id":"q-1037","question":"You're building a dynamic prompt orchestration system for Scale AI and MongoDB enterprise-support chatbots. It must select from a bank of five templates (concise, empathetic, technical, authoritative, business-friendly) based on user intent, data sensitivity, and risk signals, while applying strict safety guardrails to prevent prompt injection and data leakage. Describe the architecture, routing rules, and a minimal Python prototype that demonstrates template selection and a veto guardrail for edge cases?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["MongoDB","Scale Ai"]},{"id":"q-1119","question":"You're building a beginner-friendly prompt routing module for a multilingual customer-support chatbot serving Snowflake and Airbnb users. Design a two-step prompt selection: first classify intent (order_status, account_help, security_alert), then select a single template from (concise, friendly, authoritative) that preserves safety and privacy. Provide the concrete routing rules and a tiny Python prototype that demonstrates intent classification and template selection with sample inputs?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Airbnb","Snowflake"]},{"id":"q-1137","question":"Design a multilingual prompt evaluation pipeline for a Tesla/Square customer-support bot. It must detect language, route prompts to language-specific template banks, apply safety gates for PII and injection, and track drift metrics to trigger template updates. Provide architecture and a minimal Python prototype that returns a selected template and a veto flag?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Square","Tesla"]},{"id":"q-1206","question":"You're building a prompt lifecycle service for a multi-tenant chat assistant used by Tesla support and MongoDB customers. It must manage versioned templates, canary rollouts, per-tenant experiments, and safe rollback if a new version underperforms or violates safety guards. Describe the architecture, data model, and provide a minimal Python prototype that resolves the tenant's latest approved version and supports rollback via a veto gate?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["MongoDB","Tesla"]},{"id":"q-1217","question":"You're building a budgeted prompt engine for a multilingual support bot. With a 60-token cap for prompts in English and Spanish, design a rule-based condenser that preserves intent, routes to one of three templates (concise, empathetic, clarifying), and rejects unsafe prompts. What would the architecture look like, and provide a minimal Python prototype that compresses input to fit the budget and demonstrates template routing?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Google","Lyft","Microsoft"]},{"id":"q-1317","question":"You’re building a privacy-preserving prompt pipeline for a customer-support chatbot that must operate under GDPR. Outline a design to redact PII (emails, phone numbers) from prompts before feeding them to an LLM, while preserving intent to route to three templates (concise, empathetic, escalate). Include a minimal Python prototype that demonstrates redaction and routing, and discuss edge-case testing and auditability?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Anthropic","Google"]},{"id":"q-1455","question":"You're building a beginner-friendly, client-side prompt calibrator for a real-time support chat used by Tesla, Uber, and Scale AI customers. Design a scoring rubric that evaluates prompts on clarity, safety, and bias risk. Implement a tiny TypeScript prototype that: 1) scores prompts with the rubric, 2) routes to one of three templates (concise, empathetic, authoritative), and 3) flags prompts needing human review. Include basic tests and a sample input?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Scale Ai","Tesla","Uber"]},{"id":"q-1842","question":"Scenario: a large enterprise runs a prompt orchestration layer that routes user prompts to tenant-specific policies and models. You must design a dynamic gating layer that preserves privacy across tenants, adheres to latency budgets, and enforces safety guardrails against prompt injection. Describe the architecture, the routing rules, and provide a minimal Python prototype that demonstrates the gating decision (tenant, model, template) and a pluggable sanitizer?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["Citadel","IBM","Meta"]},{"id":"q-1852","question":"You're building a real-time, multi-tenant prompt router for an all-in-one chat assistant used by Instacart, Tesla, and Netflix employees. The system must route each user prompt to one of three personas: support-centric, revenue-aware, and compliance-oriented, based on user role, prior interactions, context window, and explicit data-sensitivity cues. It should apply a safety veto for prompts that could leak policy or PII, and adjust routing to meet SLA targets. Describe the architecture, routing rules, and provide a minimal Python prototype that demonstrates persona selection and a veto gate for edge cases. How would you measure latency, correctness, and safety?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Instacart","Netflix","Tesla"]},{"id":"q-2032","question":"You're building a multilingual prompt-routing system for a live Discord-like chat platform. Design an approach to detect language, assess safety risk, and route prompts to one of four templates (concise, friendly, formal, safety-first). Include data schemas, routing rules, and a minimal Python prototype that demonstrates language detection, risk scoring, and template selection?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Databricks","Discord"]},{"id":"q-2454","question":"You are building a real-time prompt routing layer for an enterprise AI assistant used by DBAs and developers at a large database platform. The router must assign prompts to one of four modules: 1) code-generation with sandboxed execution, 2) schema design reasoning, 3) performance tuning suggestions, 4) compliance/audit notes. Given a prompt: 'Create an index on users(name, email, last_login) for read-heavy workloads; ensure GDPR minimization and explain any trade-offs', outline the routing signals (intent, data sensitivity, latency), define routing rules, and provide a minimal Python prototype that returns the chosen module and a guardrail message. Include edge-case handling (e.g., conflicting signals)?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Google","Meta","MongoDB"]},{"id":"q-2468","question":"You're building a beginner-friendly prompt evaluation harness for a multilingual customer-support bot that must handle English, Spanish, and Mandarin prompts. Design a minimal, rule-based evaluation that checks if three locales produce semantically equivalent intents for a given user query. Provide a tiny Python prototype that feeds a fixed prompt through a mocked LLM API, compares outputs for a set of intents (order_status, refund, and product_info), and flags mismatches?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Cloudflare","Snap","Tesla"]},{"id":"q-2552","question":"You're building a real-time prompt routing system for a multinational streaming platform's content moderation assistant. It must direct prompts to: 1) automatic policy-compliant reply generation, 2) human escalation, 3) safe-deflection. Enforce jurisdiction-specific guardrails (GDPR, COPPA, local content laws) and data-sensitivity tagging. Propose routing signals, rules, and a minimal Python prototype that returns the chosen module and a jurisdiction-appropriate guardrail message. Include edge-case handling (conflicting signals)?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["Adobe","Netflix","Scale Ai"]},{"id":"q-447","question":"You're building a prompt for a customer service chatbot that needs to extract order details from unstructured user messages. How would you design the prompt to handle variations like 'I need to cancel order #12345' vs 'Can't find my recent purchase 12345' while maintaining high accuracy?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["DoorDash","Google","NVIDIA"]},{"id":"q-450","question":"You're building a prompt optimization system for a large language model API. The system needs to automatically improve prompt performance while maintaining safety constraints. How would you design an architecture that balances prompt effectiveness with content safety, and what metrics would you track?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["Apple","Robinhood","Tesla"]},{"id":"q-473","question":"You're building a chatbot for Instacart's customer service. How would you design a prompt template that handles both order status inquiries and refund requests while maintaining consistent tone and preventing prompt injection?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Goldman Sachs","Instacart","Netflix"]},{"id":"q-502","question":"How would you design a prompt engineering system to handle multi-turn conversations with context windows, ensuring consistent persona adherence while managing token limits and preventing prompt injection attacks?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["IBM","Meta","Zoom"]},{"id":"q-532","question":"You're building a prompt engineering system for a cloud infrastructure tool. How would you design prompts to handle ambiguous user input like 'setup database' while maintaining context and preventing hallucination?","channel":"prompt-engineering","subChannel":"general","difficulty":"intermediate","tags":["prompt-engineering"],"companies":["Google","Hashicorp","Stripe"]},{"id":"q-558","question":"You're building a prompt optimization system for a large language model serving 10M+ daily requests. How would you design a system to automatically detect and mitigate prompt injection attacks while maintaining 99.9% uptime?","channel":"prompt-engineering","subChannel":"general","difficulty":"advanced","tags":["prompt-engineering"],"companies":["Coinbase","NVIDIA","PayPal"]},{"id":"q-587","question":"How would you design a prompt to extract structured data from unstructured text while handling edge cases and ensuring consistent output format?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Meta","Snowflake","Zoom"]},{"id":"q-892","question":"You're building a beginner-friendly prompt evaluation harness for a customer-support chatbot. Given user prompts about orders or refunds, design a lightweight, rule-based template selector that picks among three templates (concise, friendly, authoritative). How would you implement and test this with a tiny Python prototype that scores templates on safety, tone, and length?","channel":"prompt-engineering","subChannel":"general","difficulty":"beginner","tags":["prompt-engineering"],"companies":["Coinbase","Google","Lyft"]},{"id":"q-251","question":"How would you implement a DSPy optimizer to automatically improve few-shot prompts for a classification task using BootstrapFewShot with evaluation metrics?","channel":"prompt-engineering","subChannel":"optimization","difficulty":"intermediate","tags":["prompt-tuning","dspy","automatic-prompting"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"q-198","question":"How would you design a multi-layered guardrail system to prevent prompt injection and jailbreak attacks while maintaining legitimate user functionality, and what are the key trade-offs between security and user experience?","channel":"prompt-engineering","subChannel":"safety","difficulty":"beginner","tags":["jailbreak","guardrails","content-filtering"],"companies":["Amazon","Google","Meta"]},{"id":"q-226","question":"How would you design a prompt-engineering system that dynamically selects between chain-of-thought, few-shot, and zero-shot prompting based on real-time performance metrics and task complexity?","channel":"prompt-engineering","subChannel":"techniques","difficulty":"advanced","tags":["chain-of-thought","few-shot","zero-shot"],"companies":["Amazon","Google","Meta","Microsoft","OpenAI"]},{"id":"q-196","question":"How would you implement a rate-limited async HTTP client using aiohttp and asyncio.Semaphore to handle 1000 requests while respecting API limits?","channel":"python","subChannel":"async","difficulty":"intermediate","tags":["asyncio","aiohttp","concurrency"],"companies":["Amazon","Google","Meta"]},{"id":"q-224","question":"How would you implement a thread-safe singleton in Python with lazy initialization, proper type hints, and discuss the trade-offs between metaclass, decorator, and module-level approaches for a production system?","channel":"python","subChannel":"best-practices","difficulty":"advanced","tags":["pep8","typing","testing"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-178","question":"In Python, when do `is` and `==` return different results, and why does this happen with object identity vs equality?","channel":"python","subChannel":"fundamentals","difficulty":"intermediate","tags":["python","basics"],"companies":["Amazon","Google","Meta","Microsoft","Uber"]},{"id":"q-1085","question":"You’re building a real-time log processor in Python for a messaging app. You receive a stream of JSON lines like {\"user_id\": 123, \"event\": \"message_sent\", \"ts\": 1610000000}. Emit only the first event per (user_id, event) within a 60-second sliding window. Design a memory-bounded, generator-based pipeline that reads from an iterable of strings and yields deduplicated dicts in order. Include edge cases and memory considerations?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Hugging Face","Slack"]},{"id":"q-1149","question":"In a streaming data etl, you're given a JSONL stream where each line is a JSON object with 'host' and 'value'. Implement a memory-efficient Python generator top_n_hosts(n, lines) that yields the top N hosts by total value after consuming the stream, without keeping the entire per-host totals in memory. Use a min-heap to maintain current top-N and skip invalid lines?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Google","Meta"]},{"id":"q-1250","question":"Design an asynchronous NDJSON ingestion pipeline in Python that reads an async iterable of lines, validates each with a Pydantic model, and routes records to per-tenant partitions based on the 'tenant_id'. Implement bounded queues per partition with a total memory cap, guarantee per-tenant in-order processing, and a slower downstream sink that creates backpressure. Provide a test plan with skewed partitions and slow sinks?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Snowflake","Uber"]},{"id":"q-1275","question":"Design an advanced async Python pipeline: NDJSON lines arrive via a TCP socket, each is validated with a Pydantic model, then a deduplication stage uses a sliding-window Bloom filter to emit each event at most once in a 24-hour window. The pipeline must stay memory-bounded, support backpressure to the producer, and allow the downstream sink to pace itself. Provide a concrete test plan with clock skew and bursty traffic?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["DoorDash","Hugging Face","Two Sigma"]},{"id":"q-1285","question":"Given a list of dictionaries representing users, implement normalize_users(users) that returns a list of User dataclass instances with fields id:int, name:str, signup_ts:datetime, is_active:bool. Coerce id from str to int, parse signup_ts ISO8601 strings, interpret is_active from common truthy values, and fill defaults for missing fields. If any record is invalid, raise ValueError with the indices of invalid records?","channel":"python","subChannel":"general","difficulty":"beginner","tags":["python"],"companies":["Apple","MongoDB","Uber"]},{"id":"q-1476","question":"Design an asynchronous Python engine to join two JSON event streams by id within a 5-second window. Streams A and B are async iterables yielding {'id': str, 'ts': int, 'payload': Any}. Emit matched pairs to a sink when both sides have an event with same id within the 5s window. Route late events past the lateness bound to a 'late' sink. Enforce a global memory bound for buffered events and propose a test plan with out-of-order arrivals?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Airbnb","Lyft","Stripe"]},{"id":"q-1639","question":"Write a Python function process_events(file_path) that reads a newline-delimited JSON (JSONL) log file of Stripe-like events. Each line is a JSON object with 'type' (str) and 'data' (dict with 'id' key). The function should return a dict mapping event 'type' to count, skipping lines with missing keys or invalid JSON, and writing errors to a separate errors.log. Make it memory-efficient using a streaming approach?","channel":"python","subChannel":"general","difficulty":"beginner","tags":["python"],"companies":["Hugging Face","Stripe"]},{"id":"q-1675","question":"Design a memory-bounded streaming processor in Python for a JSONL event stream with fields: timestamp, service, event_type, payload. Group by (service, event_type), maintain an in-order per-key queue with bounded capacity, and emit a 60-second rolling histogram of payload sizes per group to a downstream sink. Ensure backpressure via per-key queues and a global memory cap, and provide a test plan with skewed keys and slow sinks?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Airbnb","Amazon","Scale Ai"]},{"id":"q-1783","question":"Implement a memory-efficient Python function top_n_words(filepath, n) that streams a text file line by line to find the n most frequent words. Normalize case, strip punctuation, ignore empty tokens, and return a list of the top n words sorted by frequency. Ensure it never loads the whole file into memory?","channel":"python","subChannel":"general","difficulty":"beginner","tags":["python"],"companies":["Amazon","Google","PayPal"]},{"id":"q-1867","question":"Design a Python async NDJSON ingestion pipeline that reads lines from an async source, validates each line against a versioned Pydantic model, supports hot-reloadable schema versions from a shared config, and guarantees exactly-once delivery with an idempotent sink and per-record deduplication, all while enforcing bounded memory and backpressure. How would you implement it?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Amazon","Netflix"]},{"id":"q-1884","question":"Implement an asynchronous Python data transformer for a JSONL event stream where each line includes a 'version' field. Build transform_stream(input: AsyncIterable[str], schemas: Dict[int, Type[BaseModel]]) that validates each line against its versioned Pydantic model, applies a version-aware field mapping, and outputs transformed JSONL lines to a downstream sink while guaranteeing per-version in-order processing, memory-bounded streaming, and backpressure. Include a small test scaffold showing a v1→v2 migration?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Google","NVIDIA","Tesla"]},{"id":"q-2090","question":"Design a memory-bounded streaming top-K aggregator in Python. Data arrives as an async iterable of numeric events with timestamps. Implement a class that maintains an approximate top-10 using a Count-Min Sketch plus a min-heap, supports a sliding time window, and exposes add(value, ts) and get_top_k() reflecting the current window. Describe API, memory guarantees, and a test plan for bursty traffic?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Anthropic","Databricks","Google"]},{"id":"q-2402","question":"You're building an asyncio Python client for a rate-limited REST API. How would you implement a function fetch_with_backoff(url, method='GET', max_retries=5, session, rate_limiter, metrics) that performs retries with exponential backoff and jitter, enforces a per-endpoint token-bucket rate limit, and records latency and outcomes into metrics? Include a minimal code sketch and a test plan?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Adobe","NVIDIA","Twitter"]},{"id":"q-2704","question":"Design an async Python NDJSON processor that reads lines from an AsyncIterable[str], each line containing a top-level version and payload. It must validate using a dynamic, versioned Pydantic model registry, support on-the-fly hot-swapping of versions without restart, ensure per-version in-order processing, memory-bounded streaming, and backpressure to a downstream sink. Include a test scaffold showing v1→v2 migration and a runtime version swap?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Airbnb","Anthropic","Uber"]},{"id":"q-474","question":"Write a Python function that takes a list of integers and returns the sum of all even numbers. How would you handle edge cases?","channel":"python","subChannel":"general","difficulty":"beginner","tags":["python"],"companies":["Bloomberg","Microsoft","Robinhood"]},{"id":"q-503","question":"How would you implement a distributed rate limiter using Redis with sliding window algorithm to handle 10,000 requests per second across multiple API servers?","channel":"python","subChannel":"general","difficulty":"advanced","tags":["python"],"companies":["Google","LinkedIn","Zoom"]},{"id":"q-559","question":"You're building a data processing pipeline that needs to handle large CSV files efficiently. How would you implement a memory-efficient solution using Python generators to process files that don't fit in RAM?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Apple","Snap","Snowflake"]},{"id":"q-588","question":"How would you implement a rate limiter using Python's asyncio to prevent API abuse while maintaining high throughput?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Apple","DoorDash","MongoDB"]},{"id":"q-852","question":"Design a memory-efficient streaming dedup pipeline in Python that reads a multi-GB log file line-by-line and prints only the first occurrence of each unique line, skipping duplicates, while persisting dedup state across restarts. Use a probabilistic data structure with a configurable false-positive rate and provide a minimal runnable snippet?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["IBM","MongoDB","Robinhood"]},{"id":"q-975","question":"Design an asynchronous NDJSON pipeline in Python that ingests lines of JSON from an async source, validates each line against a Pydantic model, applies a lightweight transform, and yields results to a downstream consumer. Ensure memory stays bounded when the consumer slows by using a bounded asyncio.Queue?","channel":"python","subChannel":"general","difficulty":"intermediate","tags":["python"],"companies":["Amazon","Coinbase","Stripe"]},{"id":"q-1048","question":"You're building a React Native field-ops app with intermittent connectivity that must display offline-first tasks with images and support incremental sync across devices. Describe architecture and provide a small implementation sketch using a local DB (WatermelonDB or Realm), a sync service, and conflict resolution. What edge cases and tests would you include?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["Google","Hugging Face","Tesla"]},{"id":"q-1065","question":"You're building a React Native app for live collaboration that streams up to 4 simultaneous camera feeds using WebRTC. Describe end-to-end architecture for capture, encoding, and transport, how you'd implement backpressure and frame pacing to sustain ~30fps per feed, and provide a small implementation sketch (camera hook + simple backpressure queue) in code?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["Discord","Google","NVIDIA"]},{"id":"q-1098","question":"You're building a React Native app for field technicians that must collect GPS and sensor data in the background every 15 minutes, even when the app is suspended. Describe a cross‑platform architecture using Android WorkManager and iOS BGTaskScheduler, a minimal RN bridge, and a small code sketch of a BackgroundTaskManager that schedules tasks, persists deadlines, and handles results. Include edge cases like battery saver, app termination, and user-initiated cancel?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Google","Microsoft"]},{"id":"q-1265","question":"You're building a real-time collaborative whiteboard in React Native that must support up to 1,000 participants with low latency and offline fallback. Describe an end-to-end architecture using WebRTC data channels for deltas, WebSocket signaling, and a CRDT for merging concurrent strokes. Include data model (stroke encoding, timestamps), backpressure handling, and a small code sketch implementing a delta encoder and an in-app delta queue that feeds an RN Canvas/Skia surface, with clear acceptance criteria?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Cloudflare","Two Sigma"]},{"id":"q-1537","question":"You're building a React Native app for field engineers in regulated environments. It collects GPS and accelerometer telemetry, signs each record with a device key, and batches uploads to a backend with offline-first guarantees and tamper-evident auditing. Describe the architecture, data flow, and provide a small code sketch: a LocalAuditLog module backed by SQLite and TweetNaCl for Ed25519 signing, plus a transport adapter with exponential backoff and key-rotation handling. Include edge cases around tenant isolation, data retention, and replay protection?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["Cloudflare","Goldman Sachs","Zoom"]},{"id":"q-1790","question":"You're building a cross-platform React Native app for field technicians that must display a map with thousands of POIs, support offline caching of tiles and markers, and perform incremental sync of POI updates when online. Design a data layer and UI flow to support offline-first maps, marker clustering, and conflict resolution. Provide a small code sketch for a WatermelonDB/Realm model and a MarkerLayer component, plus test ideas?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["DoorDash","Meta","Oracle"]},{"id":"q-2072","question":"You’re building a beginner-friendly React Native gallery that shows a 2x2 grid of images loaded from a remote JSON endpoint. Describe how you would implement offline-first caching with AsyncStorage, automatic data refresh on reconnect, and a small hook sketch to fetch and cache the image list (including error handling and a simple retry strategy)?","channel":"react-native","subChannel":"general","difficulty":"beginner","tags":["react-native"],"companies":["LinkedIn","Oracle"]},{"id":"q-2292","question":"In a React Native app for real-time dashboards, you must render up to 16 live charts and 8 image tiles refreshed via WebSocket without UI jank. How would you implement a cross-platform image tile pipeline using TurboModules (JSI) that decodes, caches, and delivers tiles on a background thread, while using a virtualization list to keep 60fps? Include a concise native interface sketch and a JS wrapper?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Bloomberg","Databricks"]},{"id":"q-2436","question":"In a DoorDash-style React Native app with intermittent connectivity, how would you implement offline-first order placement using an AsyncStorage-backed Outbox queue, a background worker that processes queued actions when online, and idempotent retry semantics? Provide a minimal enqueueOrder and processQueue sketch?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["DoorDash","Microsoft"]},{"id":"q-2459","question":"You're building a cross-platform React Native app that requires a native module 'TileCache' to prefetch and cache map tiles for offline use. Outline the architecture, how the JavaScript bridge would interact with the iOS and Android implementations, memory management and eviction strategy, and a minimal code sketch showing the JS API and a native bridge interface to fetch a tile by z/x/y within a viewport. Include testing considerations?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["Airbnb","IBM","Snap"]},{"id":"q-2501","question":"You're building a beginner-friendly React Native field-data app for solar installations that allows technicians to submit a checklist and attach photos while offline. Describe an offline-first submission queue design: data model, how to store locally, how to detect connectivity, batching strategy, and a small hook sketch that enqueues items with a retry/backoff policy using AsyncStorage. Include a minimal example snippet for the hook?","channel":"react-native","subChannel":"general","difficulty":"beginner","tags":["react-native"],"companies":["IBM","Tesla"]},{"id":"q-2615","question":"You're building a field-service React Native app that must perform a background data refresh every 15 minutes to fetch location-tagged asset metadata, even when the app is terminated on both Android and iOS. Design a cross-platform background task broker using Android WorkManager and iOS BGTaskScheduler. Specify data models, scheduling constraints (network/battery), how to handle token refresh, and provide a minimal React hook sketch (useBackgroundSync) to start/stop the background work and expose status?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Meta","Microsoft","Two Sigma"]},{"id":"q-2631","question":"You're building a React Native field map app that shows a live route panel with up to 500 waypoints delivered over a WebSocket. Design a component architecture that minimizes re-renders, handles backpressure when bursts occur, and keeps the map smooth at 60fps. Provide a concise implementation sketch for a useLiveWaypoints hook (buffering and a render queue) and a minimal native bridge sketch if needed. What tests would you add to verify performance and correctness?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["Google","Instacart","Meta"]},{"id":"q-475","question":"You're building a React Native app with complex animations that need to run at 60fps. The app has multiple animated components including a custom carousel, gesture-driven interactions, and background video processing. How would you optimize performance to maintain smooth animations?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Amazon","Google","NVIDIA"]},{"id":"q-504","question":"How would you implement a custom button component in React Native that handles both iOS and Android platform-specific styling while maintaining consistent behavior?","channel":"react-native","subChannel":"general","difficulty":"beginner","tags":["react-native"],"companies":["Meta","Robinhood","Stripe"]},{"id":"q-533","question":"How would you optimize a React Native app with 50+ screens that's experiencing slow navigation and memory leaks, particularly on lower-end devices?","channel":"react-native","subChannel":"general","difficulty":"advanced","tags":["react-native"],"companies":["Citadel","PayPal"]},{"id":"q-560","question":"You're building a React Native app that needs to display a list of user profiles with images. The list should be performant with 1000+ items and support pull-to-refresh. How would you implement this using FlatList and what optimizations would you apply?","channel":"react-native","subChannel":"general","difficulty":"intermediate","tags":["react-native"],"companies":["Databricks","Microsoft"]},{"id":"q-589","question":"How do you handle different screen sizes and orientations in React Native?","channel":"react-native","subChannel":"general","difficulty":"beginner","tags":["react-native"],"companies":["Hashicorp","Instacart","LinkedIn"]},{"id":"q-183","question":"What are Native Modules in React Native, when should you use them, and what are the key performance and threading considerations?","channel":"react-native","subChannel":"native-modules","difficulty":"beginner","tags":["native","bridge"],"companies":null},{"id":"q-206","question":"How would you optimize React Native list performance with Hermes and Reanimated when dealing with 10k+ items containing complex animations?","channel":"react-native","subChannel":"performance","difficulty":"advanced","tags":["hermes","reanimated","profiling"],"companies":["Airbnb","Coinbase","Meta","Microsoft","Uber"]},{"id":"q-233","question":"How does the Hermes engine improve React Native app startup performance compared to JavaScriptCore, and what are the specific trade-offs?","channel":"react-native","subChannel":"performance","difficulty":"beginner","tags":["hermes","reanimated","profiling"],"companies":["Airbnb","Meta","Microsoft","Netflix","Salesforce","Shopify"]},{"id":"q-1023","question":"Design a centralized, tamper-evident logging pipeline for 6 RHEL hosts. Include: enable persistent journald, forward logs over TLS to a central collector, configure rotation/retention, protect in transit with certificate-based auth, and a rollback/validation plan that proves delivery during outages. Outline testing steps and failure scenarios?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Apple","Hashicorp"]},{"id":"q-1118","question":"On a RHEL 9 host, a service named app writes to /srv/app/data. After deployment, SELinux denials prevent writes. Without disabling SELinux, outline exact, implementable steps to restore functionality, including identifying the AVC, creating a targeted policy module with audit2allow, loading it, labeling the data directory, and validating the fix with a controlled write and audit checks?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Apple","Hugging Face","Twitter"]},{"id":"q-1148","question":"On a RHEL 8 server, implement a daily backup of /home/userdata to /backup/userdata-YYYYMMDD.tgz, exclude caches and temp dirs, preserve permissions, and generate a sha256 checksum. Schedule at 02:30 via cron and rotate backups to keep last 7 days. Provide commands and a script outline?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Goldman Sachs","Instacart","Lyft"]},{"id":"q-1169","question":"Design and implement an encrypted root on LVM for a production RH host. Boot partition remains unencrypted; the root filesystem sits on a LUKS2 container inside an LVM PV, with a keyfile for unattended boot. Provide a concrete, command-level plan including crypttab, fstab, initramfs (dracut) generation, and grub configuration to ensure the system boots automatically after rotation?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Bloomberg","Plaid","Snap"]},{"id":"q-1192","question":"On a RHEL8 host, set up a minimal Python HTTP server listening on port 8080, accessible only from 192.168.100.0/24. Use a non-root user, a systemd service, SELinux port labeling, and firewalld rules. Provide exact commands to create the service, configure semanage for port 8080, apply firewall rules, and test from a client. Address potential SELinux and port conflict caveats?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Instacart","LinkedIn"]},{"id":"q-1247","question":"On a RHEL 8 server, a web service listens on 127.0.0.1:9090. Configure firewalld to expose port 9090 only to the 10.1.0.0/24 management network, log drops, and persist across reboots. Provide exact commands, and describe test steps using curl from an allowed host and from a non-allowed host?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Amazon","Snap"]},{"id":"q-1399","question":"You manage a RHEL 8 server running SSH on port 22. To improve security, change SSH to listen on port 2222, disable root SSH login, and require key-based authentication, ensuring no downtime for existing sessions. List the exact commands and steps to implement this, including firewall and SELinux considerations, and how you verify connectivity afterwards?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Hugging Face","LinkedIn","MongoDB"]},{"id":"q-1484","question":"On a Red Hat-based host, you must deploy a statically compiled Go web app that runs on port 8080 behind firewalld with SELinux enforcing. The app should auto-start on boot, restart on failure, and log to rsyslog. Propose the concrete steps, files, and exact commands to configure systemd, firewalld, and SELinux contexts, ensuring minimal downtime during deploy?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["Meta","Salesforce"]},{"id":"q-1517","question":"On a RHEL 9 host, a custom service started via systemd fails to write logs to /var/www/app/logs after a patch, with SELinux enforcing. Describe a precise, minimal-risk remediation plan to identify and fix the root cause without broad permission grants, including AVC collection, targeted policy generation, and validation under load. What exact commands and steps would you perform?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Meta","Snap"]},{"id":"q-1577","question":"You must serve a static site from content stored on an NFS mount at /srv/www with SELinux enforcing on a RHEL-based system. The SELinux policy blocks httpd from reading the files. Describe the exact sequence of commands and configurations to allow Apache to serve the site without disabling SELinux or putting the system in permissive mode, including boolean toggles, labeling, and firewall rules?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["Lyft","Meta","Twitter"]},{"id":"q-1610","question":"Configure log rotation for a custom web app log path /var/log/myapp/*.log on a live Linux host. The log is written by a root-owned process and must rotate weekly, keep 4 copies, compress old logs, and after rotation reload nginx to reopen its handles. Provide a minimal, working logrotate.conf snippet and explain how this prevents log loss?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Coinbase","IBM","Plaid"]},{"id":"q-1649","question":"On a RHEL system with root on an LV named /dev/vg_rhel/root, describe a precise method to test a disruptive system update using an LVM snapshot: create a 20G snapshot, apply the update inside the snapshot, validate service health, and rollback by booting the live system from the original LV if needed. Include exact commands?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["Citadel","Google"]},{"id":"q-1679","question":"On a RHEL/CentOS host with two NICs (enp0s3 and enp0s8) connected to two switches, configure a 802.3ad (LACP) bond0 using NetworkManager. Bond should have a static IP 192.0.2.100/24, and both NICs must be slaves. Provide exact nmcli commands to create bond0, add slaves, bring it up, and verify; note required switch settings (LACP active on both ports)?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["Databricks","Scale Ai","Stripe"]},{"id":"q-1738","question":"On a RHEL-based host, configure firewalld to allow SSH access only from 192.0.2.0/24 and deny all other inbound SSH; ensure the changes persist after reboot and can be tested quickly; outline exact commands and verification steps, including revert plan?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Microsoft","Tesla","Zoom"]},{"id":"q-1756","question":"On a freshly provisioned Linux host (RHEL8), configure a new user 'audit' to log in exclusively via SSH key authentication, with a restricted shell rbash so only basic commands are allowed, and grant passwordless sudo to restart the 'auditd' service. Provide exact commands and edits to users, sshd_config, and sudoers, and how you would verify the setup?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Google","IBM"]},{"id":"q-1771","question":"On a RHEL 8 server, you must host a small static site with Nginx, ensure it starts on boot, expose only port 80, and serve content from /var/www/html with correct SELinux context. Provide exact commands to configure Nginx, firewall, and SELinux so SELinux stays enforcing?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Meta","Snap","Snowflake"]},{"id":"q-1804","question":"On a Red Hat-based host, a Python web app writes to /srv/app/data and /var/log/app. After a system update, SELinux denies these writes. Outline an operational plan to diagnose and restore write access without disabling SELinux, including commands for audit review, context restoration, and persistent policy adjustments?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Amazon","Meta","Square"]},{"id":"q-1982","question":"On a Linux host, create a project share at /srv/project where Alice can read/write and Bob can read only, with all others denied. Use POSIX permissions plus ACLs so new files created by Alice inherit Bob's read access. Provide exact commands to: 1) create the group, 2) add users, 3) set up the directory with setgid, 4) apply ACLs (explicit for Bob and default for new files), 5) verify with tests?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["DoorDash","Oracle","Zoom"]},{"id":"q-2225","question":"On a RHEL 9 server running Apache httpd hosting a custom PHP app, uploads are saved to /srv/app/uploads owned by webuser with SELinux type httpd_sys_content_t. After a denied upload, design and implement a minimal SELinux policy module that lets httpd_t write to that directory and create new files, without broad permissive mode. Include exact commands and testing steps?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Lyft","MongoDB"]},{"id":"q-2308","question":"On a fresh RHEL8 host with an unallocated disk at /dev/sdb, describe and implement the steps to (1) create a PV, (2) VG named vg_data, (3) LV named data of 900G, (4) format with XFS, (5) mount at /data with auto-mount in /etc/fstab, and (6) set ownership to user app and proper SELinux context. Provide exact commands?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Google","Hashicorp","Scale Ai"]},{"id":"q-2357","question":"Scenario: You manage a RHEL host where /var/www and /etc live on an LVM-managed XFS volume. Implement a DR restore pipeline: daily incremental backups via rsync to an offsite server, preserving SELinux contexts and ACLs, with integrity verification. Describe the exact commands, systemd timer/unit setup, and restore steps to recover to the latest backup within 60 minutes?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Hashicorp","Oracle"]},{"id":"q-2415","question":"On a fresh RHEL 9 server, implement a small utility that prints the host name and current date, running it as a non-login user via systemd and logging to a dedicated file with rotation. Provide exact commands for user creation, script, service, and log rotation?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Airbnb","Meta"]},{"id":"q-2457","question":"On a Linux host, a Node.js web app runs in a container and connects to Postgres on localhost:5432. After reboot, the web app sometimes starts before Postgres is ready, causing startup failures. Propose a concrete, production-ready systemd-based solution: (1) implement a db-wait.service that blocks until pg_isready reports healthy, (2) make web-app.service depend on and After=db-wait.service, (3) add a lightweight container readiness check for the app, and (4) describe how you would test this reliably?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Apple","Discord"]},{"id":"q-2479","question":"On a RHEL 9 host, a production service writes logs to /var/app/logs with 0777 perms. You must relocate logs to a dedicated 1 TB LVM-backed XFS volume mounted at /var/app/logs with no downtime, ensure SELinux context, and configure logrotate to compress weekly. Provide exact steps including LV creation, filesystem, fstab, SELinux relabeling, and logrotate config?","channel":"rhcsa","subChannel":"general","difficulty":"advanced","tags":["rhcsa"],"companies":["Netflix","PayPal","Two Sigma"]},{"id":"q-2557","question":"On a fresh RHEL 9 host, configure firewalld to allow SSH only from 203.0.113.0/24 and drop SSH from all other sources. Provide exact permanent commands, reload, and verification steps?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Databricks","Hashicorp","MongoDB"]},{"id":"q-2637","question":"On a fresh RHEL 9 host, deploy a tiny Python HTTP health server that binds to 127.0.0.1:9090 and returns 200 OK for GET /health when the file /tmp/healthy exists; otherwise 503. Run this as a non-root systemd service with a dedicated unit file, a startup script, and a log file rotated by logrotate. Provide exact commands to create the non-login user, script, systemd unit, logrotate config, and enable the service. Ensure idempotence?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Anthropic","Hashicorp","OpenAI"]},{"id":"q-2647","question":"On a fresh RHEL 9 server, create a non-root user named devops and configure passwordless sudo for only two commands: /bin/systemctl restart httpd and /usr/bin/journalctl -xe. Restrict SSH login for this user to localhost by updating sshd_config. Provide exact commands and file contents to accomplish this?","channel":"rhcsa","subChannel":"general","difficulty":"beginner","tags":["rhcsa"],"companies":["Microsoft","MongoDB"]},{"id":"q-933","question":"On a fresh RHEL 9 installation with a 120 GB disk, implement an LVM layout: ROOT 40G, HOME 40G, VAR 40G, all using XFS. Create PV, VG, and LVs, format, and mount at /, /home, /var with fstab. Enable and configure firewalld to allow http and https. Ensure SELinux is enforcing. Create a non-root user 'dev' and add to the wheel group with sudo privileges. Show commands and rationale?","channel":"rhcsa","subChannel":"general","difficulty":"intermediate","tags":["rhcsa"],"companies":["Adobe","IBM","Microsoft"]},{"id":"gh-24","question":"What is DevSecOps and how does it differ from traditional DevOps security approaches?","channel":"security","subChannel":"application-security","difficulty":"advanced","tags":["security","devsecops"],"companies":["Amazon","Coinbase","Google","Microsoft","Uber"]},{"id":"gh-44","question":"How do you implement a comprehensive API security strategy that protects against common vulnerabilities while maintaining developer productivity?","channel":"security","subChannel":"application-security","difficulty":"beginner","tags":["api","service-mesh"],"companies":["Amazon","Microsoft","Morgan Stanley","PayPal","Stripe"]},{"id":"gh-69","question":"How does Zero Trust Security implement identity-based access control with micro-segmentation using modern cloud infrastructure and identity providers?","channel":"security","subChannel":"application-security","difficulty":"advanced","tags":["security","network"],"companies":null},{"id":"q-230","question":"How would you implement a Content Security Policy (CSP) with nonce-based inline script protection to prevent XSS while maintaining compatibility with third-party analytics?","channel":"security","subChannel":"application-security","difficulty":"intermediate","tags":["xss","csrf","sqli","ssrf"],"companies":["Airbnb","Google","Microsoft","Stripe","Uber"]},{"id":"q-276","question":"How would you design a secure job scheduling system for a microservices environment that prevents privilege escalation while ensuring reliable execution across distributed services?","channel":"security","subChannel":"application-security","difficulty":"advanced","tags":["systemd","cron","users","permissions"],"companies":null},{"id":"q-283","question":"What is the difference between XSS and CSRF attacks?","channel":"security","subChannel":"application-security","difficulty":"beginner","tags":["xss","csrf","sqli","ssrf"],"companies":["Amazon","Google","Meta"]},{"id":"q-359","question":"You discover a reflected XSS vulnerability in a search feature. The search term is displayed back to the user without sanitization. How would you fix this, and what's the difference between reflected XSS and stored XSS in terms of impact and remediation?","channel":"security","subChannel":"application-security","difficulty":"intermediate","tags":["xss","csrf","sqli","ssrf"],"companies":["Fortinet","Google","Tesla"]},{"id":"q-404","question":"You're building a financial trading platform at Jane Street. How would you design a secure authentication and authorization system that prevents XSS, CSRF, SQLi, and SSRF attacks while maintaining high performance for real-time trading data?","channel":"security","subChannel":"application-security","difficulty":"advanced","tags":["xss","csrf","sqli","ssrf"],"companies":["Canva","Jane Street","Miro"]},{"id":"q-423","question":"You discovered a critical security vulnerability in your team's production system that could expose customer data, but fixing it requires delaying a major product launch. How would you handle this situation, considering CVSS scoring, stakeholder communication, and risk-benefit analysis?","channel":"security","subChannel":"application-security","difficulty":"advanced","tags":["ownership","bias-for-action","customer-obsession"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-202","question":"How do passkeys implement passwordless authentication using public-key cryptography?","channel":"security","subChannel":"authentication","difficulty":"beginner","tags":["mfa","passkeys","zero-trust"],"companies":["Apple","Google","Meta","Microsoft","Okta"]},{"id":"q-241","question":"How would you implement JWT authentication with RS256 signing and refresh token rotation to prevent token replay attacks?","channel":"security","subChannel":"authentication","difficulty":"intermediate","tags":["jwt","oauth2","oidc","saml"],"companies":["Airbnb","Amazon","Apple","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-302","question":"Explain the technical differences between OAuth 2.0 authorization flows and OpenID Connect authentication, including token structures, validation patterns, and security considerations?","channel":"security","subChannel":"authentication","difficulty":"beginner","tags":["jwt","oauth2","oidc","saml"],"companies":["Amazon","Google","Meta","Microsoft","Salesforce","Stripe"]},{"id":"q-324","question":"How would you design a zero-trust video conferencing platform using WebAuthn passkeys with continuous authentication for enterprise users?","channel":"security","subChannel":"authentication","difficulty":"advanced","tags":["mfa","passkeys","zero-trust"],"companies":["Snowflake","Spotify","Zoom"]},{"id":"q-387","question":"Design a zero-trust authentication system for Snowflake's data warehouse that supports MFA, passkeys, and handles 100M+ daily auth requests. How would you prevent replay attacks while ensuring sub-100ms latency?","channel":"security","subChannel":"authentication","difficulty":"advanced","tags":["mfa","passkeys","zero-trust"],"companies":["Infosys","New Relic","Snowflake"]},{"id":"gh-70","question":"How does the TLS 1.3 handshake establish secure communication and what cryptographic mechanisms ensure perfect forward secrecy?","channel":"security","subChannel":"encryption","difficulty":"advanced","tags":["security","network"],"companies":["Amazon","Cloudflare","Google","Hashicorp","Netflix","Square"]},{"id":"q-179","question":"Explain how Perfect Forward Secrecy (PFS) works in TLS, describe the ECDHE key exchange mechanism, and analyze the security trade-offs compared to RSA key exchange?","channel":"security","subChannel":"encryption","difficulty":"intermediate","tags":["encryption","crypto"],"companies":["Amazon","Apple","Cloudflare","Google","Microsoft","Stripe"]},{"id":"q-295","question":"How does AES-256-GCM provide both confidentiality and integrity in a single cryptographic operation?","channel":"security","subChannel":"encryption","difficulty":"advanced","tags":["aes","rsa","tls","hashing"],"companies":["Amazon","Google","Meta"]},{"id":"q-310","question":"How does TLS 1.3 improve security compared to TLS 1.2?","channel":"security","subChannel":"encryption","difficulty":"intermediate","tags":["aes","rsa","tls","hashing"],"companies":["Amazon","Google","Meta"]},{"id":"q-337","question":"Design a secure key exchange system for autonomous vehicle communication between cars and infrastructure. How would you handle forward secrecy and key rotation in a high-mobility environment?","channel":"security","subChannel":"encryption","difficulty":"advanced","tags":["aes","rsa","tls","hashing"],"companies":["Apple","Cruise","Uber"]},{"id":"q-348","question":"You're designing a secure booking system for Expedia that handles payment data. How would you implement a hybrid encryption scheme using RSA for key exchange and AES-256-GCM for data encryption, and what specific security considerations would you address for PCI DSS compliance?","channel":"security","subChannel":"encryption","difficulty":"advanced","tags":["aes","rsa","tls","hashing"],"companies":["Amazon","Apple","Expedia","Microsoft","OpenAI","PayPal","Square","Stripe"]},{"id":"q-1234","question":"You operate a CDN edge platform that lets customers deploy WebAssembly modules for request processing. Outline a practical, auditable approach to securely load, validate, and sandbox these modules, covering authentication (signatures/attestation), host-function access, resource quotas, revocation, and incident response?","channel":"security","subChannel":"general","difficulty":"advanced","tags":["security"],"companies":["Apple","Cloudflare","Microsoft"]},{"id":"q-2613","question":"In a real-time trading platform, how would you implement low-latency, per-request authorization for microservices and WebSocket streams to prevent token replay and privilege escalation, while ensuring global revocation is scalable and auditable?","channel":"security","subChannel":"general","difficulty":"advanced","tags":["security"],"companies":["LinkedIn","Robinhood","Snap"]},{"id":"q-2683","question":"You're building a beginner-friendly secure data ingestion portal where CSV files uploaded by partners feed a Databricks Delta table. Outline a minimal, practical security flow to ensure the upload cannot execute code, is stored securely, and triggers a Databricks job safely. Include concrete steps and a simple code snippet for validating file type and preserving audit trails before queuing the Databricks job?","channel":"security","subChannel":"general","difficulty":"beginner","tags":["security"],"companies":["Databricks","Snap"]},{"id":"q-476","question":"How would you prevent SQL injection in a web application and what are the common attack vectors?","channel":"security","subChannel":"general","difficulty":"beginner","tags":["security"],"companies":["Lyft","Netflix"]},{"id":"q-505","question":"You're building a payment processing API that must handle PCI compliance. How would you design the architecture to ensure sensitive card data never touches your servers while maintaining low latency for payment validation?","channel":"security","subChannel":"general","difficulty":"advanced","tags":["security"],"companies":["Hugging Face","Microsoft","Square"]},{"id":"q-534","question":"How would you implement secure session management in a distributed web application to prevent session hijacking and fixation attacks?","channel":"security","subChannel":"general","difficulty":"intermediate","tags":["security"],"companies":["Airbnb","Google","Two Sigma"]},{"id":"q-561","question":"How would you implement secure session management for a web application using JWT tokens, and what are the key security considerations?","channel":"security","subChannel":"general","difficulty":"intermediate","tags":["security"],"companies":["Discord","MongoDB"]},{"id":"gh-71","question":"How does a Web Application Firewall (WAF) protect against OWASP Top 10 attacks at the application layer?","channel":"security","subChannel":"owasp","difficulty":"advanced","tags":["security","network"],"companies":["Akamai","Amazon","Cloudflare","Google","Microsoft"]},{"id":"q-255","question":"How would you implement OWASP ASVS L3 input validation for a REST API endpoint that accepts JSON payloads with nested objects?","channel":"security","subChannel":"owasp","difficulty":"intermediate","tags":["top10","asvs","samm"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-373","question":"How would you implement a comprehensive defense-in-depth strategy to prevent SQL injection attacks in a modern web application following OWASP Top 10 guidelines?","channel":"security","subChannel":"owasp","difficulty":"beginner","tags":["top10","asvs","samm"],"companies":["Amazon","Cloudflare","Google","Meta","Microsoft","Netflix"]},{"id":"q-1086","question":"Design a Snowflake CDC pattern for an SCD2 customer_dim using a stream on the source table. Explain how to implement an idempotent upsert via MERGE, how deletes are represented, and how to handle late-arriving changes to preserve history?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Google","NVIDIA"]},{"id":"q-1152","question":"Design a least-privilege access layer in Snowflake for a multi-tenant data lake spanning five domains (marketing, sales, finance, analytics, operations). Describe how you would implement ROW ACCESS POLICIES and MASKING POLICIES on a payments table (columns: id, customer_id, amount, card_number, region) to restrict data by region and user role, and how you would audit access?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Adobe","Databricks","Lyft"]},{"id":"q-1239","question":"You’re building a beginner Snowflake task: a large SALES table is routinely queried with date-range filters. Propose a minimal clustering solution to improve pruning. Include exact commands to add a single clustering key on SALE_DATE, how to monitor its effectiveness, and how to decide if reclustering is needed. Keep changes focused and explain validation steps with a simple test?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Hugging Face","PayPal"]},{"id":"q-1337","question":"You're ingesting data daily into table sensor_readings(device_id STRING, ts TIMESTAMP_NTZ, reading FLOAT). You frequently query last 24 hours per device. Propose a beginner-friendly optimization path, choosing clustering key, automatic clustering, or a materialized view, and include an exact query to compute per-device count and average reading for the last 24 hours?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["PayPal","Plaid","Tesla"]},{"id":"q-1475","question":"You have a Snowflake table raw_events with columns file_name STRING, payload VARIANT containing an array at payload:'events'. Each event is an object { event, timestamp, user: { id }, region, amount }. Design a beginner-friendly approach to compute daily total_amount and per-user purchases for the last 7 days using a temporary VIEW and LATERAL FLATTEN. Provide the exact query you would run?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Citadel","Cloudflare","Oracle"]},{"id":"q-1499","question":"You're building a Snowflake-based telemetry store: table `raw.events` (device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) ingests 50M+ rows daily. BI dashboards query per-device event_count and median reading for last 7 days and last 24 hours. Propose a pragmatic optimization, selecting between clustering, automatic clustering, and a rolling aggregate MV/table. Include exact commands to implement your approach and how you would validate improvements?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Databricks","Tesla","Zoom"]},{"id":"q-1567","question":"You're operating a Snowflake telemetry store with table raw.events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) ingesting 100M+ rows daily. BI requires per-device metrics: (a) last hour event_count, (b) last 24h median of payload.metrics.reading. Propose a production-ready strategy: data model, clustering choices vs automatic clustering vs materialized views, and how to validate performance. Include exact commands to implement and how to verify improvements?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Lyft","MongoDB"]},{"id":"q-1605","question":"Design a Snowflake-based near real-time anomaly-detection pipeline for telemetry events. Ingested table: raw_events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). Ingests 200M+ rows/day. You need to detect spikes in a metric inside payload (e.g., latency) on a per-device basis with a 5-minute window and surface alerts to an alerts table when a spike exceeds a dynamic threshold. Describe the architecture using Streams, Tasks, and possibly a Snowflake procedure; include DDLs to create the stream, a task schedule, sample SQL for the rolling compute, and how you would validate the pipeline end-to-end?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Airbnb","Scale Ai","Stripe"]},{"id":"q-1678","question":"You're provisioning a Snowflake warehouse for a multi-tenant SaaS app. All tenant data sits in one table: raw_events (tenant_id STRING, event_ts TIMESTAMP_NTZ, event_type STRING, payload VARIANT) ingesting 40M+ rows/day. Propose a hybrid approach: (1) add a composite clustering key (tenant_id, event_ts) for pruning, and (2) a rolling 7-day materialized view with per-tenant metrics. Include exact SQL commands to implement clustering, MV, and a validation query showing the improvement?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Cloudflare","Goldman Sachs","Snowflake"]},{"id":"q-1711","question":"In a Snowflake telemetry store with table raw.events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) ingesting 50M+ rows daily, implement a production-grade data retention pipeline that archives data older than 365 days to an external stage and keeps only the last 365 days in the main table, while preserving query performance on recent data. Describe the architecture and provide exact SQL commands to create the stage, stream, task, and archive procedure, plus a validation plan?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Microsoft","Oracle","Snap"]},{"id":"q-2024","question":"Implement row-level security for regional data access in Snowflake. Table `customer.sales` has `customer_id`, `region`, `amount`, `last_purchase_ts`. Roles `SA_US` and `SA_EU` should only see rows for their region. Describe and implement a Snowflake ROW ACCESS POLICY using a role-context and attach it to the table, then provide a sample query that would be allowed for role `SA_US` and a test plan to verify enforcement?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Apple","Hugging Face","Microsoft"]},{"id":"q-2048","question":"You're building a multi-tenant analytics warehouse in Snowflake. Ingested events go into raw.events(tenant_id STRING, event_ts TIMESTAMP_NTZ, payload VARIANT). BI needs per-tenant metrics: (a) count of events in the last 15 minutes, (b) 95th percentile of latency from payload.metrics.latency in the last 24 hours. Propose a production-ready architecture: data model (partitioning, clustering), streaming vs batch paths (streams/tasks vs MV), decision between automatic clustering and materialized views, and a concrete validation plan with exact SQL commands?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Amazon","Scale Ai","Square"]},{"id":"q-2229","question":"You're operating a Snowflake event store table raw.events(device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). Implement GDPR-like retention: keep 90 days of live data, purge older rows by archiving to raw.events_archive and deleting from live daily via a Snowflake Task, while preserving time travel history and audits. How would you design the architecture, what SQL would you use, and how would you validate?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Anthropic","Apple","DoorDash"]},{"id":"q-2277","question":"In Snowflake, ingest telemetry into RAW.events (device_id STRING, ts TIMESTAMP_NTZ, payload VARIANT) from two sources: real-time stream and nightly batch. You must deliver per-device 15-minute latency metrics for the last hour (avg_reading, cnt). Propose an end-to-end architecture using a Stream, a Materialized View (MV) or incremental aggregation, and a TASK scheduled every 15 minutes to refresh. Include exact SQL to create the stream, MV, task, and a validation query to prove latency targets?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Discord","Slack","Square"]},{"id":"q-2303","question":"Scenario: a Snowflake account holds prod_sales.customers with PII. External partners at Oracle, Microsoft, and Nvidia need a sanitized subset for analytics. Propose a beginner-friendly data-share plan: masking policies for PII, a region-limited secure view, and an account-based share. Include commands to implement?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Microsoft","NVIDIA","Oracle"]},{"id":"q-2371","question":"You're building a Snowflake-based telemetry store loaded into a shared table raw.events (tenant_id STRING, ts TIMESTAMP_NTZ, device_id STRING, payload VARIANT). Design a production-ready, multi-tenant design ensuring strict data isolation and fast dashboards for last 30 days across hundreds of tenants. Outline RAP rules, masking, secure views or materialized aggregates, and a scalable data-sharing approach. Include concrete SQL patterns and validation steps?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Google","PayPal","Two Sigma"]},{"id":"q-2483","question":"Intermediate Snowflake-core: Ingest 200M rows daily into raw.sessions(user_id STRING, session_id STRING, started_at TIMESTAMP_NTZ, ended_at TIMESTAMP_NTZ, events VARIANT). Need per-user risk scores updated every 5 minutes and dashboards for (a) count of high-risk sessions in last 15 minutes, (b) average duration of high-risk sessions, (c) top 10 user cohorts by risk. Propose a production plan covering data model (raw vs curated), clustering, streaming vs materialized views, schema evolution, validation, and rollback strategy with exact SQL commands?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Airbnb","Amazon"]},{"id":"q-2522","question":"Snowflake-core intermediate: You're ingesting telemetry into Snowflake with raw.events(tenant_id STRING, user_id STRING, ts TIMESTAMP_NTZ, payload VARIANT). In a multi-tenant SaaS, implement per-tenant latency metrics (p95 over last 30 min), dynamic masking of sensitive payload fields for dashboards, and fast per-user drilldown without scanning raw payloads. Propose a production plan: curated schema, partitioning, streaming vs materialized path, VARIANT schema evolution, validation, and rollback with exact commands?","channel":"snowflake-core","subChannel":"general","difficulty":"intermediate","tags":["snowflake-core"],"companies":["Oracle","Robinhood"]},{"id":"q-2591","question":"You're designing a multi-tenant analytics layer in Snowflake. A single table raw.events stores events for all tenants (tenant_id, ts, payload). BI dashboards must be isolated per tenant and support sub-second queries for the last 7 days. Propose and implement a scalable access-control strategy that avoids per-tenant copies, considering growth to hundreds of tenants. Include how you'd implement Row Access Policies, a Secure View, and validation steps with concrete SQL commands?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Adobe","Discord","DoorDash"]},{"id":"q-2592","question":"You operate a multi-tenant Snowflake data lake: table raw.orders(tenant_id STRING, order_id STRING, amount NUMBER, credit_card VARIANT). Implement per-tenant data isolation using row access policies and a masking policy for credit_card so tenants see only last 4 digits. Provide exact SQL to create masking policy, row access policy, a secure view that enforces both policies, and grants. Include how you'd test with two tenants?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["LinkedIn","Robinhood"]},{"id":"q-2657","question":"Beginner Snowflake-core: daily load from staged.orders_stage into production.orders, dedup by order_id. Columns: order_id STRING, customer_id STRING, amount NUMBER, load_date DATE. Write a single MERGE UPSERT that keeps only the latest load_date per order_id. Include exact MERGE SQL and a quick verify query?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["IBM","Oracle","Plaid"]},{"id":"q-2692","question":"You're designing a beginner Snowflake data-curation task: given a raw table RAW.events(device_id string, event_ts timestamp_ntz, payload variant) ingesting 50M rows daily with a JSON payload containing fields event_type, region, and email (PII). Propose a concrete end-to-end approach to surface a curated table ANALYTICS.curated.events_daily with columns device_id, event_type, region, user_email where user_email is masked for external queries via a masking policy. Include exact SQL statements to (1) create the curated table, (2) extract fields, (3) define and apply masking policy on user_email, and (4) create a daily REFRESH TASK at 01:00 UTC that recomputes the curated table from the last 24 hours?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Cloudflare","Discord","NVIDIA"]},{"id":"q-868","question":"Design a cross-account data-sharing solution in Snowflake for a multinational fintech requiring regional affiliates to access a shared dataset containing PII. How would you implement Secure Data Sharing, dynamic data masking, region-specific RBAC, and auditable access, while enabling updates to masking policies without breaking consumer queries?","channel":"snowflake-core","subChannel":"general","difficulty":"advanced","tags":["snowflake-core"],"companies":["Coinbase","Microsoft"]},{"id":"q-981","question":"How would you configure Snowflake so regional analysts can run ad-hoc queries on a shared dataset while ensuring isolation, predictable performance, and cost control? Include concrete settings for: (a) warehouse topology (min/max clusters, auto-suspend/resume), (b) RBAC (roles, grant scopes on databases/schemas/tables), (c) cost governance (resource monitors and credit caps), (d) a sample GRANT script giving REGIONAL_ANALYST access to only SALES and EVENTS schemas, and (e) auditing and reproducibility considerations?","channel":"snowflake-core","subChannel":"general","difficulty":"beginner","tags":["snowflake-core"],"companies":["Anthropic","Cloudflare","Robinhood"]},{"id":"q-305","question":"How would you determine the required capacity for a service expecting 10x traffic growth during a product launch?","channel":"sre","subChannel":"capacity-planning","difficulty":"beginner","tags":["forecasting","autoscaling","load-testing"],"companies":["Amazon","Google","Meta"]},{"id":"sr-131","question":"You're managing a microservices platform with 50 services. Service A has a 95th percentile latency of 200ms and handles 10,000 RPS. It calls Service B (50ms, 5,000 RPS) and Service C (100ms, 3,000 RPS). During Black Friday, you expect 5x traffic. Service A's CPU utilization is currently 60%, memory at 70%. How do you plan capacity to maintain <500ms 95th percentile end-to-end latency?","channel":"sre","subChannel":"capacity-planning","difficulty":"advanced","tags":["capacity","scaling"],"companies":["Amazon","Google","Meta","Microsoft","Uber"]},{"id":"sr-143","question":"Your web application currently handles 1000 requests per minute during peak hours. Each request takes an average of 200ms to process. If you expect traffic to double in the next 6 months, how many additional server instances do you need if each server can handle 50 concurrent requests?","channel":"sre","subChannel":"capacity-planning","difficulty":"beginner","tags":["capacity","scaling"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"sr-149","question":"You're designing capacity planning for a microservices platform handling 10M daily active users. Each user generates 50 API calls/day with 80% during peak hours. How many instances do you need for each service?","channel":"sre","subChannel":"capacity-planning","difficulty":"advanced","tags":["capacity","scaling"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-218","question":"How would you design a chaos engineering experiment to test database failover while maintaining transaction consistency across a microservices architecture?","channel":"sre","subChannel":"chaos-engineering","difficulty":"advanced","tags":["chaos-monkey","litmus","gremlin"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-399","question":"You're using Litmus Chaos to test a microservices application. One of your chaos experiments is causing unexpected cascading failures across multiple services. How would you debug this issue and what specific steps would you take to limit the blast radius?","channel":"sre","subChannel":"chaos-engineering","difficulty":"intermediate","tags":["chaos-monkey","litmus","gremlin"],"companies":["Airtable","Fortinet","Tempus"]},{"id":"sr-146","question":"Design a chaos engineering experiment to test the resilience of a microservices-based e-commerce platform during a database partition event. How would you ensure the experiment doesn't cause customer data loss while still providing meaningful insights?","channel":"sre","subChannel":"chaos-engineering","difficulty":"advanced","tags":["chaos","resilience"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"sr-150","question":"You're implementing chaos engineering for a distributed payment system processing $10M daily transactions. Design a chaos experiment to test resilience against Byzantine failures where 30% of payment validation nodes provide conflicting consensus results. How would you ensure financial accuracy while testing system behavior under adversarial conditions?","channel":"sre","subChannel":"chaos-engineering","difficulty":"advanced","tags":["chaos","resilience"],"companies":["Amazon","Coinbase","Microsoft","Netflix","Stripe"]},{"id":"sr-153","question":"You're implementing chaos engineering for a microservices architecture. Your payment service has a 99.9% SLA. During a chaos experiment, you inject 500ms latency into 20% of requests to the database. The service starts timing out after 1 second. What's the most critical metric to monitor first, and what would indicate the experiment should be stopped immediately?","channel":"sre","subChannel":"chaos-engineering","difficulty":"intermediate","tags":["chaos","resilience"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-477","question":"You're running a chaos experiment in production. How do you determine the blast radius and ensure you don't impact customer experience while still getting meaningful failure data?","channel":"sre","subChannel":"general","difficulty":"intermediate","tags":["sre"],"companies":["Goldman Sachs","Google","LinkedIn"]},{"id":"q-506","question":"You're on-call and receive an alert that a critical service is experiencing 99% error rate. What are your immediate first steps and how do you approach incident response?","channel":"sre","subChannel":"general","difficulty":"beginner","tags":["sre"],"companies":["IBM","Scale Ai"]},{"id":"q-535","question":"You're an SRE at Netflix and notice your CDN cache hit ratio dropped from 95% to 70% during peak hours. How would you diagnose and resolve this issue?","channel":"sre","subChannel":"general","difficulty":"advanced","tags":["sre"],"companies":["Discord","Netflix","Snowflake"]},{"id":"q-590","question":"How would you design a canary deployment strategy for a microservice handling 10K RPS with 99.99% SLA requirements?","channel":"sre","subChannel":"general","difficulty":"advanced","tags":["sre"],"companies":["Bloomberg","Google","Twitter"]},{"id":"gh-65","question":"What is Mean Time to Recovery (MTTR), how do you calculate it, and what specific strategies would you implement to optimize it for SRE teams?","channel":"sre","subChannel":"incident-management","difficulty":"beginner","tags":["metrics","kpi"],"companies":null},{"id":"gh-97","question":"How do you design incident response playbooks that balance automation with human oversight for SRE teams?","channel":"sre","subChannel":"incident-management","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-262","question":"Describe a critical production outage you managed during peak traffic. How did you coordinate the response, communicate with stakeholders, and implement both immediate fixes and long-term preventive measures?","channel":"sre","subChannel":"incident-management","difficulty":"advanced","tags":["situation","task","action","result"],"companies":["Amazon","Cloudflare","Google","Microsoft","Netflix","Oracle"]},{"id":"q-319","question":"You are on-call and receive a high-severity PagerDuty alert for a production service degradation. What are your immediate steps and how do you coordinate with the team?","channel":"sre","subChannel":"incident-management","difficulty":"advanced","tags":["pagerduty","runbooks","postmortem"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Servicenow","Stripe","Wipro"]},{"id":"q-367","question":"You're managing a multi-cluster GitOps setup at Warner Bros with 50+ microservices. ArgoCD suddenly starts showing 'Unknown' sync status for critical services during peak traffic. How would you diagnose and resolve this production incident while ensuring zero downtime?","channel":"sre","subChannel":"incident-management","difficulty":"advanced","tags":["argocd","flux","declarative"],"companies":["Amazon","Google","Hashicorp","LinkedIn","Microsoft","Netflix","Salesforce","Warner Bros"]},{"id":"q-368","question":"You're on-call at Tesla when the vehicle telemetry pipeline shows 95% packet loss. Your PagerDuty alert shows the Kafka cluster is healthy, but the downstream processing service is crashing. What's your immediate triage process and how do you determine if this is a network, application, or data format issue?","channel":"sre","subChannel":"incident-management","difficulty":"intermediate","tags":["pagerduty","runbooks","postmortem"],"companies":["Discord","Tesla","Zscaler"]},{"id":"sr-126","question":"How would you design and implement a comprehensive blameless postmortem process that includes incident response coordination, root cause analysis using 5 Whys and fishbone diagrams, and actionable improvement tracking?","channel":"sre","subChannel":"incident-management","difficulty":"advanced","tags":["incident","postmortem"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"sr-142","question":"You receive a PagerDuty alert at 3 AM: 'Production API is returning 500 errors'. What are your first three steps in handling this incident, and what specific tools and metrics would you use to assess impact and coordinate response?","channel":"sre","subChannel":"incident-management","difficulty":"beginner","tags":["incident","postmortem"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"gh-19","question":"What is monitoring in DevOps and how does it differ from observability?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["observability","monitoring","logging"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-20","question":"Design a comprehensive logging architecture using the ELK Stack with File Beats for a high-traffic e-commerce platform processing 50,000 requests per minute. How would you ensure data integrity and real-time monitoring?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["observability","monitoring","logging"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"gh-21","question":"How does Prometheus implement a pull-based monitoring system, and what are the key components in its architecture?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["observability","monitoring","logging"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-22","question":"What is Grafana and how does it integrate with different data sources for monitoring and visualization?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["observability","monitoring","logging"],"companies":["Airbnb","LinkedIn","Microsoft","Stripe","Uber"]},{"id":"gh-23","question":"Explain the key differences between monitoring and logging in DevOps, and when would you use each?","channel":"sre","subChannel":"observability","difficulty":"intermediate","tags":["observability","monitoring","logging"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-61","question":"What are Service Level Indicators (SLIs) and how do they differ from SLOs?","channel":"sre","subChannel":"observability","difficulty":"intermediate","tags":["sre","reliability"],"companies":["Amazon","Google","Meta"]},{"id":"gh-77","question":"How would you design a comprehensive monitoring strategy for a distributed system, including tool selection, SLI/SLO definition, and alerting implementation?","channel":"sre","subChannel":"observability","difficulty":"intermediate","tags":["monitoring","infra"],"companies":["Amazon","Cloudflare","Google","Meta","Microsoft","Netflix"]},{"id":"gh-78","question":"How would you design a comprehensive monitoring strategy for a production microservices system, including SLI/SLO definitions and alerting thresholds?","channel":"sre","subChannel":"observability","difficulty":"intermediate","tags":["monitoring","infra"],"companies":["Amazon","Cloudflare","Google","Microsoft","Netflix","Stripe"]},{"id":"gh-79","question":"What is Application Performance Monitoring?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["monitoring","infra"],"companies":["Amazon","Datadog","Google","Microsoft","Splunk"]},{"id":"gh-95","question":"What is a Service Level Indicator (SLI)?","channel":"sre","subChannel":"observability","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Citadel","Goldman Sachs","Google","Microsoft"]},{"id":"gh-99","question":"What is Tracing in Observability?","channel":"sre","subChannel":"observability","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Goldman Sachs","Netflix","Stripe","Uber"]},{"id":"q-192","question":"How would you implement OpenTelemetry instrumentation to capture RED metrics (Rate, Errors, Duration) for a microservice using Prometheus as the backend?","channel":"sre","subChannel":"observability","difficulty":"intermediate","tags":["prometheus","grafana","opentelemetry"],"companies":["Chronosphere","Datadog","Grafana Labs","Microsoft","New Relic"]},{"id":"q-244","question":"What is the difference between metrics, logs, and traces in observability, and how do OpenTelemetry collectors correlate them?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["prometheus","grafana","opentelemetry"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"q-345","question":"You're monitoring a streaming service that suddenly experiences 500 errors. How would you use Prometheus and Grafana to quickly identify the root cause?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["prometheus","grafana","opentelemetry"],"companies":["Amazon","Cloudflare","Google","Infosys","Microsoft","Netflix","Warner Bros"]},{"id":"q-382","question":"You're the SRE lead for a rocket launch telemetry system. Prometheus is showing high memory usage on your OpenTelemetry collector during peak launch events, causing metric loss. How would you architect a solution to handle 100K+ metrics/second while ensuring zero data loss during critical launch windows?","channel":"sre","subChannel":"observability","difficulty":"advanced","tags":["prometheus","grafana","opentelemetry"],"companies":["Notion","OpenAI","SpaceX"]},{"id":"q-391","question":"You're an SRE at HashiCorp and your Prometheus alerts are firing every 5 minutes due to a memory leak in a Go service using OpenTelemetry. How would you debug this using the observability stack?","channel":"sre","subChannel":"observability","difficulty":"intermediate","tags":["prometheus","grafana","opentelemetry"],"companies":["Hashicorp","Instacart","Western Digital"]},{"id":"q-411","question":"You're on-call and receive an alert: 'API response time increased from 200ms to 2s over the last 5 minutes'. Using Prometheus, Grafana, and OpenTelemetry, how would you diagnose this issue?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["prometheus","grafana","opentelemetry"],"companies":["Amazon","Cloudflare","Google","Intel","Microsoft","Netflix","Palo Alto Networks","Stripe"]},{"id":"sr-124","question":"How would you implement the four golden signals of monitoring in a production microservices architecture, and what trade-offs would you consider when designing your observability strategy?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["metrics","monitoring"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"sr-133","question":"How do you implement the three pillars of observability (logs, metrics, traces) in a microservices architecture, and what are the key trade-offs between them?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["metrics","monitoring"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Snowflake"]},{"id":"sr-155","question":"What is the difference between metrics, logs, and traces in observability, and when would you use each?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["metrics","monitoring"],"companies":["Amazon","Bloomberg","Datadog","Goldman Sachs","Google","Microsoft","Netflix","New Relic","Splunk","Uber"]},{"id":"sre-1","question":"How would you design and implement SRE monitoring with SLIs, SLOs, and SLAs for a high-traffic e-commerce platform? What specific metrics would you track and how would they drive engineering decisions?","channel":"sre","subChannel":"observability","difficulty":"beginner","tags":["metrics","policy","definitions","observability"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"gh-103","question":"What is a Self-Healing System and how does it work in distributed architectures?","channel":"sre","subChannel":"reliability","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Uber"]},{"id":"gh-35","question":"Design a backup and disaster recovery strategy for a high-availability e-commerce platform processing 10,000 transactions/minute with 99.99% uptime SLA. What are your RTO/RPO targets and how would you implement multi-region failover?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["backup","dr"],"companies":["Amazon","Google","Meta"]},{"id":"gh-59","question":"What is Site Reliability Engineering and how does it differ from traditional operations?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["sre","reliability"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-60","question":"How do you design and implement Service Level Objectives (SLOs) with proper SLI definitions, error budgets, and monitoring strategies?","channel":"sre","subChannel":"reliability","difficulty":"intermediate","tags":["sre","reliability"],"companies":null},{"id":"gh-62","question":"What is an Error Budget and how does it impact SRE decision-making?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["sre","reliability"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-63","question":"What is Toil in Site Reliability Engineering and how should SREs approach managing it?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["sre","reliability"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-93","question":"How do you implement and monitor Service Level Agreements (SLAs) in a distributed system, including specific metrics, tools, and alerting strategies?","channel":"sre","subChannel":"reliability","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Cloudflare","Datadog","Google","Microsoft","Netflix"]},{"id":"gh-94","question":"What is a Service Level Objective (SLO) and how does it differ from an SLA?","channel":"sre","subChannel":"reliability","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-270","question":"Your microservice has a 99.9% availability SLO over 30 days with a 1-hour burn rate alert threshold. If you experience a 10-minute outage at 10% traffic, how much error budget remains and what's the burn rate? Should you alert?","channel":"sre","subChannel":"reliability","difficulty":"intermediate","tags":["slo","sli","error-budget"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"q-290","question":"Explain the relationship between SLIs, SLOs, and SLAs in reliability engineering, including how you would implement error budgets and monitor burn rate?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["slo","sli","error-budget"],"companies":["Amazon","Apple","Cloudflare","Google","Microsoft","Netflix"]},{"id":"q-333","question":"Your SLO for API response time is 99.9% with a 500ms threshold. You're at 99.7% and the error budget is exhausted. The product team wants to ship a new feature that will increase traffic by 20%. How do you handle this situation?","channel":"sre","subChannel":"reliability","difficulty":"intermediate","tags":["slo","sli","error-budget"],"companies":["Atlassian","Databricks","Unity"]},{"id":"q-355","question":"Your SLO is 99.9% for API latency (p95 < 200ms). You're at 99.85% and have 15% error budget remaining. A critical security patch requires 30% traffic shift to new version with unknown latency characteristics. How do you proceed while maintaining service reliability?","channel":"sre","subChannel":"reliability","difficulty":"advanced","tags":["slo","sli","error-budget"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"sr-130","question":"Your web service has an SLO of 99.9% availability over 30 days. You've had 3 outages: 45 minutes, 20 minutes, and 15 minutes. What's your current availability, error budget status, and what immediate actions would you take to prevent SLO breach?","channel":"sre","subChannel":"reliability","difficulty":"intermediate","tags":["slo","sli","error-budget"],"companies":["Amazon","Cloudflare","Google","Meta","Microsoft","Netflix"]},{"id":"sr-147","question":"Your distributed system has 5 microservices with the following failure rates: Service A (0.1%), Service B (0.2%), Service C (0.05%), Service D (0.15%), Service E (0.25%). Design a fault-tolerant architecture to achieve 99.5% SLO with specific implementation details?","channel":"sre","subChannel":"reliability","difficulty":"advanced","tags":["reliability","incident"],"companies":["Amazon","Databricks","Google","Meta","Microsoft","Netflix"]},{"id":"sr-154","question":"Your API serves 10M requests/day with a 99.9% availability SLO and 30-day error budget. After a 4-hour outage affecting 100% of traffic, calculate the remaining error budget and explain how you'd handle post-incident SLO adjustments, error budget recovery strategies, and burn rate monitoring?","channel":"sre","subChannel":"reliability","difficulty":"intermediate","tags":["slo","sli","error-budget"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"sr-169","question":"Your API service has an SLO of 99.9% availability. If you have 5 incidents this month with downtimes of 10min, 5min, 15min, 8min, and 12min, did you meet your SLO and what's the remaining error budget for the rest of the month?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["slo","sli","error-budget"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Salesforce"]},{"id":"sre-2","question":"How do you calculate and manage Error Budgets for a microservices architecture with multiple SLOs, and what strategies do you use for burn rate monitoring and recovery?","channel":"sre","subChannel":"reliability","difficulty":"beginner","tags":["management","concept","risk"],"companies":["Adobe","Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"gh-45","question":"How do rate limiting algorithms like Token Bucket and Leaky Bucket control API request flow and what are their trade-offs?","channel":"system-design","subChannel":"api-design","difficulty":"beginner","tags":["api","service-mesh"],"companies":["Amazon","Google","Microsoft","Stripe","Uber"]},{"id":"q-406","question":"Design a REST API for a cryptocurrency exchange that handles 100,000 trades per day with real-time price updates. How would you ensure data consistency and handle high-frequency trading requests?","channel":"system-design","subChannel":"api-design","difficulty":"beginner","tags":["api","rest","grpc","graphql"],"companies":["Coinbase","Oracle","Twilio"]},{"id":"q-424","question":"Design a RESTful API for a hotel booking system. What endpoints would you create and how would you handle concurrent bookings for the same room?","channel":"system-design","subChannel":"api-design","difficulty":"beginner","tags":["api","rest","grpc","graphql"],"companies":["Airbnb","Amazon","Booking.com","Google","Microsoft"]},{"id":"q-507","question":"Design a unified API gateway for Databricks that supports REST, gRPC, and GraphQL with protocol translation, rate limiting, and unified authentication?","channel":"system-design","subChannel":"api-design","difficulty":"advanced","tags":["api","rest","grpc","graphql"],"companies":["Databricks","IBM"]},{"id":"sy-151","question":"Design a rate limiting API for a multi-tenant SaaS platform where different customers have different rate limits (free: 100 req/hour, premium: 1000 req/hour, enterprise: custom). How would you design the API endpoints and data structures to efficiently track and enforce these limits?","channel":"system-design","subChannel":"api-design","difficulty":"intermediate","tags":["api","rest"],"companies":["Amazon","Google","Microsoft","Stripe","Uber"]},{"id":"q-604","question":"Design a rate limiting system for a public API that can handle 10,000 requests per second with different rate limits for free and paid tiers (100 requests/minute for free, 1000 requests/minute for paid). How would you implement this to ensure fairness and prevent abuse?","channel":"system-design","subChannel":"api-rate-limiting","difficulty":"intermediate","tags":["rate-limiting","api-design","distributed-systems","redis","token-bucket"],"companies":["Twitter","Stripe","GitHub","Google","Amazon"]},{"id":"q-1622","question":"Design a distributed caching system for a high-traffic e-commerce platform that handles 10,000 requests per second with 99.9% availability. How would you handle cache invalidation, consistency, and failover?","channel":"system-design","subChannel":"cache-architecture","difficulty":"intermediate","tags":["distributed-caching","redis","consistency","scalability","high-availability"],"companies":["Amazon","Google","Meta","Netflix","Uber","Airbnb"]},{"id":"q-597","question":"Design a distributed caching system for a global e-commerce platform that handles 100,000 requests per second with 99.9% availability. How would you handle cache consistency, invalidation strategies, and failover across multiple geographic regions?","channel":"system-design","subChannel":"cache-architecture","difficulty":"advanced","tags":["distributed-systems","caching","redis","high-availability","consistency","scalability"],"companies":["Amazon","Google","Meta","Netflix","Uber","Airbnb","Spotify","Twitter"]},{"id":"q-603","question":"Design a distributed caching system for a global e-commerce platform that serves 10 million daily active users. The system must handle product catalog caching with 99.99% availability, sub-millisecond latency for hot items, and cache consistency across multiple data centers.","channel":"system-design","subChannel":"cache-architecture","difficulty":"advanced","tags":["distributed-systems","caching","redis","high-availability","consistency"],"companies":["Amazon","Netflix","Google","Meta","Microsoft","Uber","Airbnb"]},{"id":"q-621","question":"Design a distributed caching system for a social media platform that needs to handle 10 million active users with 99.9% availability. How would you ensure cache consistency across multiple data centers while minimizing latency?","channel":"system-design","subChannel":"cache-architecture","difficulty":"intermediate","tags":["distributed-systems","caching","consistency","scalability","high-availability"],"companies":["Facebook","Twitter","LinkedIn","Reddit","Instagram"]},{"id":"q-634","question":"Design a distributed caching system for a global e-commerce platform that serves 10 million requests per second with 99.99% availability. The system must handle cache consistency across multiple data centers, support cache warming for popular products, and provide graceful degradation when cache nodes fail.","channel":"system-design","subChannel":"cache-architecture","difficulty":"advanced","tags":["distributed-systems","caching","consistency","high-availability","scalability"],"companies":["Amazon","Netflix","Google","Meta","Microsoft","Uber"]},{"id":"q-169","question":"Design a caching strategy for a high-traffic e-commerce platform handling 10,000 RPS. Compare cache-aside vs read-through patterns, including write-through considerations, consistency guarantees, and performance implications. When would you choose each pattern and what are the trade-offs?","channel":"system-design","subChannel":"caching","difficulty":"beginner","tags":["cache","redis"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-213","question":"Design a multi-tier caching strategy for a 99.9% availability e-commerce platform handling 10M requests/day with 100ms P99 latency. How would you implement cache warming, invalidation, and fallback mechanisms?","channel":"system-design","subChannel":"caching","difficulty":"advanced","tags":["cache","redis","memcached","cdn"],"companies":null},{"id":"q-231","question":"How would you design a multi-region CDN cache purging system that guarantees content propagation within 5 seconds while handling 10,000 concurrent invalidations per second?","channel":"system-design","subChannel":"caching","difficulty":"intermediate","tags":["edge","caching","purging"],"companies":["Amazon","Cloudflare","Google","Meta","Microsoft","Netflix"]},{"id":"q-299","question":"How would you design a caching layer for a high-traffic e-commerce website?","channel":"system-design","subChannel":"caching","difficulty":"beginner","tags":["cache","redis","memcached","cdn"],"companies":["Amazon","Google","Meta"]},{"id":"q-392","question":"Design a distributed caching layer for Fortinet's threat intelligence system that serves 50M security devices with real-time malware signatures and threat data. How would you ensure cache consistency across global edge locations while maintaining sub-50ms response times?","channel":"system-design","subChannel":"caching","difficulty":"advanced","tags":["cache","redis","memcached","cdn"],"companies":["Fortinet","Microsoft","Tesla"]},{"id":"q-417","question":"Design a distributed caching strategy for a global e-commerce platform handling 10M daily users with frequent price/inventory updates. How would you ensure cache consistency, handle invalidation, and optimize performance across regions?","channel":"system-design","subChannel":"caching","difficulty":"intermediate","tags":["cache","redis","memcached","cdn"],"companies":null},{"id":"q-441","question":"Design a distributed caching layer for a social media feed serving 10M DAU with 99.9% availability. How would you handle cache invalidation across multiple data centers?","channel":"system-design","subChannel":"caching","difficulty":"advanced","tags":["cache","redis","memcached","cdn"],"companies":["Apple","LinkedIn","Two Sigma"]},{"id":"q-478","question":"Design a caching layer for a product catalog API serving 10K requests/second with 100K products. How would you handle cache invalidation and ensure data consistency?","channel":"system-design","subChannel":"caching","difficulty":"beginner","tags":["cache","redis","memcached","cdn"],"companies":["Instacart","Plaid"]},{"id":"q-601","question":"Design a communication strategy for a microservices-based e-commerce platform where the Order Service needs to notify the Inventory Service, Payment Service, and Notification Service when a new order is placed. How would you handle communication failures and ensure data consistency?","channel":"system-design","subChannel":"distributed-communication","difficulty":"intermediate","tags":["microservices","event-driven","distributed-systems","message-brokers","saga-pattern"],"companies":["Amazon","Netflix","Uber","Spotify","Airbnb"]},{"id":"q-625","question":"Design a distributed rate limiter for a high-traffic API that can handle 10,000 requests per second with per-user, per-IP, and per-endpoint limits. How would you ensure accuracy and prevent race conditions across multiple servers?","channel":"system-design","subChannel":"distributed-rate-limiting","difficulty":"intermediate","tags":["rate-limiting","distributed-systems","api-gateway","redis","token-bucket"],"companies":["Google","Amazon","Meta","Microsoft","Netflix"]},{"id":"gh-43","question":"Design an API Gateway for a high-traffic e-commerce platform handling 10M daily requests. How would you implement rate limiting, circuit breakers, and service discovery while ensuring 99.9% availability and sub-100ms latency?","channel":"system-design","subChannel":"distributed-systems","difficulty":"intermediate","tags":["api","service-mesh"],"companies":null},{"id":"q-189","question":"How would you design a distributed transaction system using the Saga pattern for an e-commerce platform handling inventory, payment, and shipping services, ensuring exactly-once processing and eventual consistency?","channel":"system-design","subChannel":"distributed-systems","difficulty":"beginner","tags":["saga","cqrs","event-sourcing"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-238","question":"How does Raft consensus algorithm ensure leader election and log replication in distributed systems?","channel":"system-design","subChannel":"distributed-systems","difficulty":"beginner","tags":["dist-sys","cap-theorem","consensus"],"companies":["Airbnb","Amazon","Apple","Cockroach Labs","Etcd","Google","Meta","Microsoft","Netflix","Uber"]},{"id":"q-260","question":"Design a scalable Selenium Grid architecture to handle 10,000 concurrent test sessions with 99.9% uptime, ensuring zero memory leaks through automatic session lifecycle management, real-time monitoring, and graceful node failure recovery across multiple data centers?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["selenium","webdriver","grid"],"companies":["Amazon","Google","Microsoft","Netflix","Salesforce","Snowflake"]},{"id":"q-282","question":"Design an event sourcing system for a high-throughput e-commerce platform handling 10,000 orders/second with 99.99% availability. How would you implement the event store, handle versioning, and ensure event ordering while supporting replay and recovery?","channel":"system-design","subChannel":"distributed-systems","difficulty":"intermediate","tags":["event-sourcing","distributed-systems","architecture","cqrs","immutability"],"companies":["Amazon","Databricks","Microsoft","Netflix","Salesforce","Stripe"]},{"id":"q-313","question":"How would you design a distributed chat system like Slack that handles real-time messaging with strong consistency guarantees across global deployments?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["dist-sys","cap-theorem","consensus"],"companies":null},{"id":"q-352","question":"Design a distributed order processing system using Saga pattern for a high-frequency trading platform. How would you handle compensation transactions when a market data feed fails mid-transaction?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["saga","cqrs","event-sourcing"],"companies":["Citadel","Tcs","Western Digital"]},{"id":"q-435","question":"You're building a ride-sharing service similar to Lyft. How would you design the database architecture to handle 10,000 concurrent rides with real-time location updates? What sharding strategy would you use?","channel":"system-design","subChannel":"distributed-systems","difficulty":"beginner","tags":["scaling","sharding","replication"],"companies":["Amazon","Google","Lyft","Meta","Netflix","Salesforce","Uber"]},{"id":"q-536","question":"Design a distributed consensus service for a ride-sharing platform handling 10M concurrent rides with real-time location updates. How do you ensure consistency across geo-distributed data centers while maintaining <100ms latency?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["dist-sys","cap-theorem","consensus"],"companies":["LinkedIn","Lyft","Meta"]},{"id":"q-612","question":"How would you design a communication pattern for a microservices architecture where services need to maintain real-time data consistency while handling high-throughput requests?","channel":"system-design","subChannel":"distributed-systems","difficulty":"intermediate","tags":["microservices","communication-patterns","event-driven","consistency","message-queues"],"companies":["Netflix","Uber","Amazon","Twitter"]},{"id":"sd-2","question":"Design a distributed caching system using Consistent Hashing. How would you handle node failures, load balancing, and ensure minimal data movement when scaling from 10 to 100 nodes?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["hashing","dist-sys","caching"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"sd-3","question":"Explain the CAP Theorem. Can you really 'choose two' and what are the practical tradeoffs?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["theory","dist-sys","database"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"sd-4","question":"Design a database sharding strategy for a social media platform with 100M+ users. How would you handle data distribution, cross-shard queries, and rebalancing?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["db","scale","architecture"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"sd-5","question":"Design a distributed rate limiter for a microservices API handling 10,000 RPS with 99.9% availability using Redis Cluster. How would you handle cache invalidation, circuit breakers, and multi-region consistency?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["security","api","algorithms"],"companies":["Amazon","Cloudflare","Google","Meta","Netflix","Stripe"]},{"id":"sy-132","question":"Design a distributed rate limiting system that can handle 1M+ requests per second across multiple data centers while maintaining consistency and low latency. How would you handle burst traffic, different rate limiting algorithms (token bucket, sliding window), and ensure fair distribution across users?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["api","rest"],"companies":["Amazon","Google","Meta","Microsoft","Uber"]},{"id":"sy-137","question":"Design a distributed system that provides exactly-once processing guarantees for event streams with out-of-order delivery and network partitions. How would you handle idempotency, deduplication, and causal consistency across multiple processing nodes?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["dist-sys","architecture"],"companies":["Goldman Sachs","LinkedIn","Netflix","Stripe","Uber"]},{"id":"sy-138","question":"Design a distributed rate limiting system that can handle 10M requests per minute across 100+ microservices with different rate limit policies per service. How would you ensure high availability, consistency, and sub-millisecond latency while handling failures and scaling?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["api","rest"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"sy-139","question":"Design a rate limiting system for a multi-tenant API serving 100M+ daily calls across 5 regions, supporting tiered rate limits (1000-100K RPS), burst capacity (3x sustained rate), sub-50ms latency, and 99.99% availability using distributed token bucket algorithm?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["api","rest"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"sy-140","question":"Design a rate limiting service that can handle 10 million requests per second with distributed consistency across multiple data centers. The service should support multiple rate limiting strategies (token bucket, sliding window, fixed window) and provide sub-millisecond latency. How would you architect this to handle bursts, prevent thundering herd problems, and ensure accurate global rate limits?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["api","rest"],"companies":["Amazon","Google","Meta","Microsoft","Stripe"]},{"id":"sy-141","question":"Design a globally distributed serverless platform for real-time collaborative document editing with offline support and conflict resolution. How would you handle data consistency, versioning, and low-latency synchronization across AWS regions while maintaining sub-50ms response times?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["infra","scale"],"companies":["Amazon","Dropbox","Google","Meta","Microsoft"]},{"id":"sy-158","question":"Design a distributed rate limiter that can handle 1M requests/second across 100 data centers with <10ms latency. How do you ensure accurate rate limiting while avoiding coordination overhead?","channel":"system-design","subChannel":"distributed-systems","difficulty":"advanced","tags":["dist-sys","architecture"],"companies":["Amazon","Google","Meta","Microsoft","Uber"]},{"id":"q-622","question":"How would you design communication between microservices when you need to ensure data consistency across multiple services, and what patterns would you consider?","channel":"system-design","subChannel":"distributed-transactions","difficulty":"intermediate","tags":["microservices","distributed-systems","data-consistency","saga-pattern","event-driven"],"companies":["Netflix","Amazon","Uber","LinkedIn","Spotify"]},{"id":"gh-14","question":"How would you design a scalable cloud platform architecture that integrates compute, storage, networking, and database services?","channel":"system-design","subChannel":"infrastructure","difficulty":"beginner","tags":["cloud","aws","azure","gcp"],"companies":["Amazon","Google","Meta"]},{"id":"gh-84","question":"Design a cloud-native modernization strategy for a 10M-user monolithic e-commerce platform requiring 99.9% uptime. How would you migrate to microservices while maintaining business continuity and optimizing costs?","channel":"system-design","subChannel":"infrastructure","difficulty":"beginner","tags":["migration","cloud"],"companies":null},{"id":"gh-91","question":"How would you design a comprehensive feature flagging system that supports both server-side and client-side flags with proper performance considerations?","channel":"system-design","subChannel":"infrastructure","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-265","question":"How would you design a unified system monitoring dashboard that aggregates real-time process metrics, system call tracing, and network connection data from htop, strace, and lsof?","channel":"system-design","subChannel":"infrastructure","difficulty":"intermediate","tags":["top","htop","strace","lsof"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-316","question":"How would you design a database architecture to handle 10 million users with 99.99% uptime? What sharding and replication strategies would you use?","channel":"system-design","subChannel":"infrastructure","difficulty":"beginner","tags":["scaling","sharding","replication"],"companies":["Chime","Salesforce","Snowflake"]},{"id":"q-327","question":"Design a simple API rate limiter that can handle 10,000 requests per second. How would you prevent abuse while ensuring legitimate users aren't blocked?","channel":"system-design","subChannel":"infrastructure","difficulty":"beginner","tags":["infra","scale","distributed"],"companies":["Salesforce","Square","Supabase"]},{"id":"q-562","question":"Design a real-time vehicle telemetry system for Tesla's fleet of 10M cars collecting sensor data at 100Hz?","channel":"system-design","subChannel":"infrastructure","difficulty":"advanced","tags":["infra","scale","distributed"],"companies":["Apple","Tesla"]},{"id":"q-619","question":"What is an ambient mesh and how does it differ from traditional service mesh architectures?","channel":"system-design","subChannel":"infrastructure","difficulty":"intermediate","tags":["service-mesh","kubernetes","infrastructure","networking"],"companies":["Google","IBM","Microsoft"]},{"id":"sy-169","question":"Design a URL shortening service that handles 1 billion URLs with 10M daily requests, achieving 99.99% uptime. How would you architect the system for high availability and scalability?","channel":"system-design","subChannel":"infrastructure","difficulty":"beginner","tags":["infra","scale"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-607","question":"Design a communication strategy for a microservices architecture where you have 10+ services that need to exchange data. What patterns would you use and why?","channel":"system-design","subChannel":"inter-service-communication","difficulty":"intermediate","tags":["microservices","communication-patterns","system-design","event-driven","api-gateway"],"companies":["Netflix","Amazon","Uber","Spotify","Airbnb"]},{"id":"q-617","question":"Design a communication pattern for a microservices architecture where an order service needs to notify inventory, payment, and shipping services when a new order is placed. Discuss the trade-offs between synchronous REST calls, message queues, and event streaming.","channel":"system-design","subChannel":"inter-service-communication","difficulty":"intermediate","tags":["microservices","communication-patterns","distributed-systems","message-queues","event-streaming"],"companies":["Netflix","Amazon","Uber","LinkedIn","Spotify"]},{"id":"gh-33","question":"How do different load balancing algorithms distribute traffic across servers, and what are the trade-offs between performance and resource utilization?","channel":"system-design","subChannel":"load-balancing","difficulty":"advanced","tags":["scale","ha"],"companies":["Amazon","Goldman Sachs","Google","Microsoft","Netflix"]},{"id":"q-285","question":"How would you design a load balancer that handles 1M concurrent connections using NGINX vs HAProxy?","channel":"system-design","subChannel":"load-balancing","difficulty":"intermediate","tags":["lb","traffic","nginx","haproxy"],"companies":["Amazon","Google","Meta"]},{"id":"q-376","question":"Design a load balancing system for a global e-commerce platform handling 50M concurrent users during Black Friday sales. How would you ensure zero downtime while handling 10x traffic spikes?","channel":"system-design","subChannel":"load-balancing","difficulty":"advanced","tags":["lb","traffic","nginx","haproxy"],"companies":["Hashicorp","Thoughtworks","Workday"]},{"id":"q-393","question":"Design a global load balancer for Google Cloud that handles 10M concurrent connections with sub-10ms failover across 5 regions. How would you ensure zero-downtime deployments while maintaining 99.999% availability?","channel":"system-design","subChannel":"load-balancing","difficulty":"advanced","tags":["lb","traffic","nginx","haproxy"],"companies":null},{"id":"q-591","question":"How would you design a load balancer for a microservices architecture handling 10,000 requests per second with 99.99% uptime?","channel":"system-design","subChannel":"load-balancing","difficulty":"intermediate","tags":["lb","traffic","nginx","haproxy"],"companies":["Anthropic","Oracle","Tesla"]},{"id":"q-598","question":"Design a load balancer for a high-traffic e-commerce platform that must handle 100,000 requests per second with 99.99% uptime. Explain how you would choose between round-robin, least connections, and weighted round-robin algorithms, and describe your failover strategy.","channel":"system-design","subChannel":"load-balancing-algorithms","difficulty":"intermediate","tags":["load-balancing","system-design","scalability","high-availability","algorithms"],"companies":["Amazon","Google","Meta","Netflix","Microsoft","Uber","Airbnb"]},{"id":"q-605","question":"Design a load balancer for a high-traffic e-commerce website that must handle 100,000 requests per second with 99.99% uptime. Explain which load balancing algorithm you would choose and why, considering factors like session persistence, health checks, and failover mechanisms.","channel":"system-design","subChannel":"load-balancing-algorithms","difficulty":"intermediate","tags":["load-balancing","system-design","scalability","high-availability","algorithms"],"companies":["Amazon","Google","Netflix","Meta","Microsoft","Apple"]},{"id":"q-615","question":"Design a load balancer for a high-traffic e-commerce website. Which load balancing algorithm would you choose and why? How would you handle session persistence and failover?","channel":"system-design","subChannel":"load-balancing-algorithms","difficulty":"intermediate","tags":["load-balancing","system-design","high-availability","scalability","algorithms"],"companies":["Amazon","Google","Meta","Netflix","Uber","Airbnb"]},{"id":"q-609","question":"Design a load balancer for a high-traffic e-commerce platform. Which load balancing algorithm would you choose and why? Consider factors like session persistence, server health, and traffic distribution.","channel":"system-design","subChannel":"load-balancing-strategies","difficulty":"intermediate","tags":["load-balancing","algorithms","system-design","high-availability","scalability"],"companies":["Amazon","Google","Netflix","Meta","Microsoft","Apple"]},{"id":"q-610","question":"Design a load balancer for a high-traffic e-commerce platform. Which load balancing algorithm would you choose and why? Consider the trade-offs between different algorithms.","channel":"system-design","subChannel":"load-balancing-strategies","difficulty":"intermediate","tags":["load-balancing","algorithms","system-design","scalability","high-availability"],"companies":["Amazon","Google","Netflix","Meta","Microsoft"]},{"id":"q-266","question":"Design a distributed message queue system that handles 1M events/sec with exactly-once delivery, sub-second latency, and 99.99% availability. How would you ensure data consistency across partitions while handling consumer failures and network partitions?","channel":"system-design","subChannel":"message-queues","difficulty":"intermediate","tags":["kafka","rabbitmq","sqs","pubsub"],"companies":["Amazon","Google","Meta","Microsoft","Netflix"]},{"id":"q-361","question":"Design a distributed message queue system for processing 10M financial transactions per hour with exactly-once delivery guarantees across multiple data centers. How would you handle message ordering, deduplication, and cross-region consistency?","channel":"system-design","subChannel":"message-queues","difficulty":"advanced","tags":["kafka","rabbitmq","sqs","pubsub"],"companies":["Amazon","Broadcom","Google","Netflix","PayPal","Robinhood","Stripe","Western Digital"]},{"id":"q-432","question":"Design a food delivery app's order processing system using message queues to handle 10,000 orders per minute with exactly-once processing?","channel":"system-design","subChannel":"message-queues","difficulty":"beginner","tags":["kafka","rabbitmq","sqs","pubsub"],"companies":["Lyft","Microsoft","Oracle"]},{"id":"q-623","question":"Design a communication strategy for a microservices architecture where some services need real-time updates while others can tolerate eventual consistency. What patterns would you use and how would you handle service discovery and load balancing?","channel":"system-design","subChannel":"microservices-architecture","difficulty":"intermediate","tags":["microservices","communication-patterns","service-discovery","load-balancing","consistency"],"companies":["Netflix","Amazon","Uber","Spotify","Airbnb"]},{"id":"q-616","question":"Design a CI/CD pipeline for a microservices application with 10 services, each with its own repository. The pipeline must support parallel deployments, canary releases, automated testing, and rollback capabilities. How would you ensure zero-downtime deployments and maintain consistency across services?","channel":"system-design","subChannel":"pipeline-architecture","difficulty":"advanced","tags":["cicd","microservices","kubernetes","devops","gitops"],"companies":["Google","Netflix","Amazon","Microsoft","Uber","Airbnb"]},{"id":"q-631","question":"Design a CI/CD pipeline for a microservices application with 10 services. Each service has its own repository and needs automated testing, security scanning, and deployment to both staging and production environments. How would you ensure fast feedback loops while maintaining quality and security?","channel":"system-design","subChannel":"pipeline-architecture","difficulty":"intermediate","tags":["cicd","microservices","devops","security","automation"],"companies":["Google","Amazon","Microsoft","Netflix","Spotify","Uber"]},{"id":"q-626","question":"Compare and contrast synchronous vs asynchronous communication patterns in microservices, and when would you choose one over the other?","channel":"system-design","subChannel":"service-communication","difficulty":"intermediate","tags":["microservices","communication-patterns","system-design","architecture"],"companies":["Netflix","Amazon","Uber","Spotify","Airbnb"]},{"id":"q-632","question":"Design a communication strategy for a microservices architecture where you have an Order Service, Payment Service, and Inventory Service. The Order Service needs to coordinate with both Payment and Inventory services to process an order. What communication patterns would you use and why?","channel":"system-design","subChannel":"service-communication","difficulty":"intermediate","tags":["microservices","communication-patterns","system-design","async-messaging","api-design"],"companies":["Netflix","Amazon","Uber","Spotify","Airbnb"]},{"id":"q-1343","question":"How would you design a distributed caching system for a real-time analytics platform that processes streaming data from millions of IoT devices, requiring sub-second query responses and handling high write throughput with eventual consistency?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["distributed-caching","real-time-analytics","iot-streaming","eventual-consistency","write-throughput"],"companies":[]},{"id":"q-1621","question":"Design a rate limiting system for a video streaming platform that needs to enforce different limits based on content type and user subscription tier: free users (5 videos/hour, 100MB bandwidth), premium users (50 videos/hour, 1GB bandwidth), and enterprise users (unlimited videos, 10GB bandwidth). How would you implement a multi-dimensional rate limiter that tracks both request count and data transfer while preventing abuse and ensuring fair resource allocation?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","multi-dimensional","video-streaming","redis","sliding-window"],"companies":[]},{"id":"q-1623","question":"Design a rate limiting system for a video streaming platform that needs to handle different rate limits for various user tiers and content types: free users (5 video plays/hour), premium users (50 video plays/hour), and content creators (1000 API calls/hour for upload management). How would you implement a hybrid rate limiter that combines user-based and content-based limits while preventing abuse during peak viewing hours?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","redis","video-streaming","user-tiers","hybrid-rate-limiting"],"companies":[]},{"id":"q-1624","question":"Design a rate limiting system for a video streaming platform that needs to handle different rate limits for various user tiers and content types: free users (5 video plays/hour), premium users (unlimited plays), and API access (1000 req/hour). How would you implement a hybrid rate limiting approach that combines user-based and content-based limits while preventing abuse during peak events like live streams?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","redis","video-streaming","hybrid-limits","adaptive-throttling"],"companies":[]},{"id":"q-1625","question":"How would you design a communication pattern for a microservices architecture where services need to handle both high-volume batch processing and low-latency interactive requests, while ensuring message ordering and exactly-once processing guarantees?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["microservices","message-queues","exactly-once","batch-processing","low-latency"],"companies":[]},{"id":"q-2122","question":"Design a rate limiting system for a mobile banking API that must handle different limits based on transaction type and user risk level: low-risk users (1000 req/day), high-risk users (100 req/day), with stricter limits for sensitive operations like money transfers (10/hour) vs. balance checks (100/hour). How would you implement a dynamic rate limiter that adapts to real-time fraud detection signals?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","redis","fraud-detection","mobile-banking","sliding-window"],"companies":[]},{"id":"q-2123","question":"How would you design a distributed caching system for a real-time analytics platform that processes streaming data from millions of IoT devices, requiring sub-100ms query response times for time-series aggregations while handling high write throughput and data freshness requirements?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["distributed-caching","time-series","iot-analytics","real-time","cache-consistency"],"companies":[]},{"id":"q-2124","question":"Design a rate limiting system for a financial trading platform that needs to enforce different limits based on market volatility: normal markets (1000 req/sec), high volatility (100 req/sec), and flash crash scenarios (10 req/sec). How would you implement a dynamic rate limiter that can automatically adjust limits based on real-time market conditions while preventing manipulation and ensuring fair access for all clients?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","financial-systems","dynamic-algorithms","market-data","circuit-breaker"],"companies":[]},{"id":"q-2618","question":"How would you design a communication pattern for a microservices architecture where services need to handle both high-frequency small messages (like telemetry data) and large payload transfers (like file processing), while optimizing for network bandwidth and preventing message storms?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["microservices","communication-patterns","backpressure","message-queuing","gRPC","rate-limiting"],"companies":[]},{"id":"q-639","question":"How would you design a distributed caching system for a real-time analytics platform that processes streaming event data from millions of IoT devices, requiring sub-10ms query latency for time-series aggregations while handling high write throughput and ensuring data freshness?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["distributed-caching","time-series","real-time-analytics","iot","write-through"],"companies":[]},{"id":"q-645","question":"Design a distributed caching system for a real-time collaborative editing platform (like Google Docs) that must handle concurrent edits from thousands of users on the same document while maintaining strong consistency and sub-10ms latency. How would you handle conflict resolution, version control, and cache invalidation when multiple users edit the same document simultaneously?","channel":"system-design","subChannel":"system-design","difficulty":"advanced","tags":["distributed-caching","real-time-collaboration","conflict-resolution","operational-transformation","strong-consistency"],"companies":[]},{"id":"q-646","question":"Design a rate limiting system for a real-time chat application that needs to handle different types of messages with varying limits: text messages (100/min), file uploads (10/min), and API calls (1000/min). How would you implement a multi-tier rate limiter that prevents spam while ensuring critical messages (like emergency alerts) are always delivered?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","real-time-systems","token-bucket","redis","priority-queues"],"companies":[]},{"id":"q-647","question":"Design a rate limiting system for a video streaming platform that needs to limit API calls based on both user subscription tier (free: 1000 calls/day, premium: 10000 calls/day) AND content type (video uploads: 10/hour, metadata queries: 100/minute). How would you implement multi-dimensional rate limiting while ensuring fair usage and preventing abuse?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","redis","multi-dimensional","api-gateway","token-bucket"],"companies":[]},{"id":"q-649","question":"How would you design a communication pattern for a microservices architecture where services need to handle both request-response interactions and event-driven updates, while ensuring backward compatibility during API version transitions?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["microservices","api-versioning","hybrid-communication","backward-compatibility","system-design"],"companies":[]},{"id":"q-650","question":"Design a rate limiting system for a video streaming platform that needs to limit API calls based on both user tier (free/premium) and content type (metadata vs video transcoding requests). How would you implement a multi-dimensional rate limiter that can handle 100K concurrent users with different limits per content type while preventing abuse and ensuring fair resource allocation?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","redis","multi-dimensional","user-tiers","content-aware"],"companies":[]},{"id":"q-658","question":"How would you design a multi-tier distributed caching strategy for a microservices architecture where different services have varying access patterns and consistency requirements?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["distributed-caching","microservices","cache-tiers","consistency-patterns"],"companies":[]},{"id":"q-764","question":"Design a rate limiting system for a content delivery network (CDN) that needs to enforce different limits based on content type and geographic region. Static assets (images, CSS) can serve 10,000 req/min globally, while dynamic API endpoints have stricter limits: 1,000 req/min per region for North America, 500 req/min for Europe, and 200 req/min for Asia-Pacific. How would you implement a hierarchical rate limiter that balances global fairness with regional capacity constraints?","channel":"system-design","subChannel":"system-design","difficulty":"intermediate","tags":["rate-limiting","cdn","geo-distribution","redis","hierarchical-limits"],"companies":[]},{"id":"q-1015","question":"Design a TensorFlow 2.x data pipeline for a document-classification model trained on 8 GPUs with MirroredStrategy. Data comes from two sources: TFRecords with image_raw and a CSV with per-record numeric metadata. Build a single tf.data pipeline that yields a dict {'image': image_tensor, 'meta': meta_tensor}, with image decoded and resized to 224x224 and scaled to [0,1], metadata normalized, deterministic per-epoch shuffling with a fixed seed, interleaving sources with parallelism, caching, and prefetching. Then implement gradient accumulation to reach a global batch size of 1024 while per-replica batch size is 128, and outline reproducibility checks and simple throughput measurements. Provide key code blocks?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Bloomberg","Meta","Microsoft"]},{"id":"q-1072","question":"You're deploying a TensorFlow 2.x recommender with dense features and a massive sparse feature 'item_id'. The item vocabulary comes from Redis and updates in real time without downtime. Describe a practical serving approach that keeps latency under 20 ms, handles unseen IDs gracefully, and updates embeddings without restarting the service. Include a minimal code sketch showing how to load a Redis-backed vocabulary into a tf.lookup MutableHashTable and map incoming IDs to embeddings inside a tf.function?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Adobe","Tesla"]},{"id":"q-1103","question":"Scenario: You have a dataset of short audio clips stored as WAV files in data/train/{class}/*.wav. As a beginner TensorFlow developer, implement a minimal end-to-end solution: (1) a tf.data pipeline that reads file paths and infers the label from the parent directory, (2) loads WAVs as mono, (3) pads/trims to 16000 samples, (4) normalizes to [-1,1], (5) shuffles with a fixed seed, (6) caches and prefetches, (7) batches 32. Then define a tiny Conv1D classifier for 2 classes and show how to train with model.fit using the pipeline. Include only the essential code blocks?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Meta","MongoDB"]},{"id":"q-1168","question":"You’re building a TensorFlow model that jointly processes images and captions stored in a CSV with columns: image_path, caption, label. Implement an efficient tf.data pipeline that (1) reads the CSV, (2) loads and decodes images from disk with aspect-ratio-preserving resize to 224x224, (3) tokenizes captions using a saved BPE tokenizer loaded from a file, (4) pads captions to the max length within each batch, (5) caches, (6) shuffles with a fixed seed, (7) batches 64, (8) runs under a multi-GPU distribution strategy. Provide the core code blocks and discuss performance trade-offs?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Amazon","Google"]},{"id":"q-1252","question":"You are training a large Transformer-based recommender model on multiple GPUs with a custom training loop in TensorFlow 2.x. The per-device batch is 256, but you want an effective global batch of 4096. Describe and implement how to use gradient accumulation to achieve this, including how to adjust the learning rate, BN handling, and mixed-precision considerations?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["NVIDIA","Snap"]},{"id":"q-1384","question":"Describe how to deploy a TensorFlow model that accepts variable-length input sequences in production without a fixed max_length. Include input signatures, RaggedTensor usage, encoder compatibility, dynamic batching (bucketed), memory/latency considerations, and validation strategy with latency and throughput benchmarks?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Microsoft","Snowflake"]},{"id":"q-1422","question":"You have a local dataset of 50k JPEG images organized as train/{class} and val/{class}. Build a beginner-friendly TensorFlow 2.x data pipeline that trains a simple CNN on CPU. Describe and implement reading files with tf.data, decoding JPEG, resizing to 128x128, applying basic augmentations, and batching with prefetch. Include a method to verify input throughput keeps the trainer busy?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Amazon","Google"]},{"id":"q-1454","question":"Given a directory of JPEG images and a CSV file with two columns (path, label) for a 2-class image classification task, design a beginner-friendly tf.data pipeline that keeps the GPU busy on a single GPU. What steps would you include and provide a minimal code snippet using cache, shuffle, batch, and prefetch?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Amazon","Apple","Zoom"]},{"id":"q-1510","question":"You're deploying a browser-based image classifier in a product used by large-scale apps (e.g., Discord, Instacart, Lyft). Describe end-to-end how to convert a trained Keras MobileNetV2 model to TensorFlow.js, including quantization choices and asset packaging, and how to serve it from a static site. Then provide a minimal JavaScript snippet to load the model, preprocess a 224x224 HTMLImageElement, and output the top-3 class labels?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Discord","Instacart","Lyft"]},{"id":"q-1518","question":"On a workstation with 4 GPUs, train a multi-modal model (image + text) using a shared backbone and two heads. How would you implement a single training loop that (1) accumulates gradients to emulate a larger global batch, (2) applies per-branch loss weights to form a stable total loss under mixed-precision, and (3) keeps data sharding synchronized across GPUs? Provide a minimal code sketch?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["OpenAI","Snowflake","Tesla"]},{"id":"q-1629","question":"You are building a real-time fraud-detection model in TensorFlow 2.x. Data arrives as streaming JSON with dense features and a high-cardinality categorical field. Design a streaming tf.data pipeline that hashes the categorical feature, caches preprocessed tensors, and trains with focal loss on a single-GPU setup. Provide a minimal, runnable dataset builder and focal loss snippet that demonstrates the end-to-end flow?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["IBM","Salesforce","Tesla"]},{"id":"q-1739","question":"You’re deploying a multi-tenant vision model behind TensorFlow Serving on Kubernetes. Tenants share a single model graph but require different input normalization pipelines (e.g., color space, resizing strategy, and augmentation). How would you design a solution that isolates per-tenant preprocessing without duplicating the model, keeps latency under 50 ms per inference, and allows updating tenant parameters without redeploying the model?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Adobe","Google","Oracle"]},{"id":"q-1752","question":"In training a Transformer language model on 4 GPUs with mixed precision using tf.distribute.MirroredStrategy, how would you implement gradient accumulation to simulate a larger global batch while keeping updates stable? Provide a minimal code snippet showing an accumulation loop and the cadence for applying the optimizer?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["DoorDash","Microsoft","MongoDB"]},{"id":"q-1840","question":"How would you implement a multi‑objective training loop in TensorFlow 2.x that trains a model with a primary cross-entropy loss and an auxiliary contrastive loss under tf.distribute.MultiWorkerMirroredStrategy, ensuring stable convergence via dynamic loss weighting (GradNorm), per-batch gradient normalization, and proper sequence masking for variable-length inputs?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["IBM","NVIDIA","Twitter"]},{"id":"q-1859","question":"You are training a graph neural network for molecular property prediction with graphs of varying sizes. The dataset is stored as sharded TFRecord files on GCS and you want multi-GPU training (2–4 GPUs). Propose a tf.data pipeline that buckets graphs by node count, pads to the bucket max, preserves per-epoch shuffling, and integrates with tf.distribute.Strategy. Describe the approach and provide a minimal batching sketch?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Cloudflare","Google","Lyft"]},{"id":"q-2081","question":"In a distributed training setup with tf.distribute.MultiWorkerMirroredStrategy across 8 workers, how would you guarantee deterministic sharding and data order for a long-text Transformer training run? Include how to set global_batch_size, per_replica_batch_size, dataset sharding via input_context/experimental_distribute_dataset, and seeds/flags to ensure reproducibility; avoid hidden data leakage across workers?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Citadel","Cloudflare"]},{"id":"q-2098","question":"Using TensorFlow 2.x, implement a gradient accumulation training loop inside tf.distribute.MirroredStrategy to achieve an effective batch size of 1024 with per-step batch 64 on multi-GPU. Include how you would apply loss scaling for mixed precision, and how to accumulate and apply gradients every 'accum_steps' steps. Provide skeleton code for the train_step and train_loop, and explain memory and determinism considerations?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Discord","Google"]},{"id":"q-2161","question":"You're deploying a Transformer-based sequence model for a real-time recommender in TensorFlow Serving behind a microservice API. Clients send requests with variable-length sequences up to 1024 tokens. You need dynamic batching, mixed precision, and memory-efficient attention to meet P95 latency under 30 ms at 1k RPS, plus canary rollouts. Outline the end-to-end approach: preprocessing, model packaging, dynamic batching strategy, memory management, and benchmarking. Which TF APIs and Serving config would you use, and what are the trade-offs?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Databricks","DoorDash"]},{"id":"q-2258","question":"You're deploying a real-time sentence-embedding model in TensorFlow 2.x behind TensorFlow Serving on Kubernetes for a high-throughput API. How would you architect deterministic dynamic batching to coalesce requests with varying sequence lengths, ensuring tail latency stays under 20 ms while preserving embedding quality, including choices between TF Serving batching versus a custom batching layer, handling bucketing/padding, and validation/rollback plans?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Goldman Sachs","Meta","Oracle"]},{"id":"q-2283","question":"You have a small image classifier trained in Keras on 28x28 grayscale images and you need to deploy on-device with limited compute. Describe a concrete, end-to-end plan to convert the model to TensorFlow Lite, choose between post-training quantization and quantization-aware training, and how you would validate that accuracy loss on-device remains acceptable after quantization?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Discord","Lyft","Scale Ai"]},{"id":"q-2437","question":"You run a real-time multi-tenant anomaly-detection service for network traffic. Each tenant can generate up to 1k events/sec; you need strong isolation and predictable tail latency. How would you architect the inference path in TensorFlow Serving to support per-tenant routing, tenant-specific weights, dynamic batching, and model versioning in production?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Cloudflare","Coinbase"]},{"id":"q-2512","question":"In a production TensorFlow 2.x service, you need to feed a model with high-throughput image batches stored in a remote bucket. The pipeline must support deterministic preprocessing, robust caching, and minimal CPU-GPU contention. Describe how you would construct a tf.data pipeline using interleave/map with num_parallel_calls, cache, shuffle, batch, prefetch, and AUTOTUNE. Include a minimal example demonstrating the pipeline for 256x256 RGB images and explain how you would measure throughput and tune parameters?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["LinkedIn","Microsoft","Salesforce"]},{"id":"q-2528","question":"You're deploying a streaming text-generation Transformer behind TensorFlow Serving; latency budget is 50 ms per token under burst traffic. Outline a practical approach to streaming inference in TensorFlow that reuses attention keys/values across tokens, chooses decoding strategy (greedy, top-k/top-p) with cache, and maintains per-session state across requests. Include concrete components, data shapes, and potential pitfalls like cache invalidation and memory growth?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Hugging Face","Instacart","Lyft"]},{"id":"q-2543","question":"You are training a simple image classifier in TensorFlow 2.x. The training loop stalls because the input pipeline is the bottleneck; dataset is 1M TFRecord images on GCS. How would you construct a tf.data pipeline with parallel reads, caching, and prefetch to keep GPUs busy? Provide a code snippet showing the pipeline steps and parameter choices?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["DoorDash","Oracle","Snowflake"]},{"id":"q-2651","question":"Given a 10k-image dataset and a simple CNN in TensorFlow 2.x, describe a reproducible training setup on a single GPU that enforces determinism across runs, including seed initialization, environment flags for deterministic ops, and pragmatic caveats; provide a minimal code-free plan and an outline of what you'd verify in tests?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Anthropic","Lyft"]},{"id":"q-857","question":"You’re deploying a multimodal TensorFlow 2.x Keras model that consumes an image [N,224,224,3] and a text embedding [N,128] to TensorFlow Serving on Kubernetes. Explain how to export a SavedModel with a serving_default signature that accepts a dict input {'image': ..., 'text': ...} and a separate 'predict_dense' signature for A/B testing. Include concrete input signatures, how to create concrete_functions, and how to manage versioning for backward compatibility?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["DoorDash","Meta","Slack"]},{"id":"q-943","question":"You're running distributed TensorFlow training with tf.distribute.MultiWorkerMirroredStrategy across 8 workers. Intermittent batch loss suggests non-deterministic per-worker data sharding and uneven batch boundaries. Describe a concrete fix: deterministic sharding, fixed seeds, per-replica batch sizing, and validation steps; specify exact API calls, TF_CONFIG handling, and how you'll verify convergence is repeatable?","channel":"tensorflow-developer","subChannel":"general","difficulty":"advanced","tags":["tensorflow-developer"],"companies":["Airbnb","Citadel"]},{"id":"q-966","question":"How would you deploy a text classifier in TF2 that must support vocab expansion without retraining? Provide a single SavedModel with two signatures: 'predict' for input {'texts': tf.Tensor<String>} and 'extend_vocab' for {'new_tokens': tf.Tensor<String>, 'vectors': tf.Tensor<float>}. Explain embedding resizing, token→id mapping, stateful management, and versioning; include a minimal code outline?","channel":"tensorflow-developer","subChannel":"general","difficulty":"intermediate","tags":["tensorflow-developer"],"companies":["Citadel","Hugging Face","Oracle"]},{"id":"q-992","question":"You’re building a beginner TensorFlow image classifier. The dataset sits under data/train with subfolders per class (e.g., cat/dog). Write a minimal tf.data pipeline that (1) reads image files with automatic label inference, (2) decodes and resizes to 224x224, (3) scales pixels to [0,1], (4) shuffles with a fixed seed for reproducibility, (5) batches 32, and (6) caches to speed training. Include the key code blocks?","channel":"tensorflow-developer","subChannel":"general","difficulty":"beginner","tags":["tensorflow-developer"],"companies":["Anthropic","Snowflake","Tesla"]},{"id":"do-3","question":"What is Infrastructure as Code (IaC) and why is Terraform preferred over manual infrastructure management?","channel":"terraform","subChannel":"basics","difficulty":"beginner","tags":["infra","automation","terraform"],"companies":["Airbnb","Amazon","Google","Meta","Microsoft","Netflix","Stripe","Uber"]},{"id":"gh-17","question":"What is Terraform and how does it implement Infrastructure as Code (IaC) workflows?","channel":"terraform","subChannel":"basics","difficulty":"beginner","tags":["iac","terraform","ansible"],"companies":["Airbnb","Databricks","Goldman Sachs","Microsoft","Snowflake"]},{"id":"de-137","question":"You have a Terraform configuration that creates an AWS S3 bucket. After running 'terraform apply', you realize you need to add versioning to the bucket. What's the safest way to modify your existing infrastructure?","channel":"terraform","subChannel":"best-practices","difficulty":"beginner","tags":["terraform","iac"],"companies":["Amazon","Google","Hashicorp","Microsoft","Netflix"]},{"id":"q-272","question":"How would you implement a DRY Terraform configuration using Terragrunt and Atlantis for multi-environment deployments?","channel":"terraform","subChannel":"best-practices","difficulty":"intermediate","tags":["dry","terragrunt","atlantis"],"companies":["Amazon","Google","Netflix","Stripe"]},{"id":"q-284","question":"Design a production-grade Terraform architecture for a multi-environment AWS infrastructure with 100+ resources, including state management, CI/CD integration, and security controls. How would you handle state locking, workspace strategy, and deployment validation?","channel":"terraform","subChannel":"best-practices","difficulty":"advanced","tags":["infrastructure-as-code","automation","best-practices"],"companies":["Amazon","Google","Hashicorp","Microsoft","Netflix","Snowflake"]},{"id":"q-1049","question":"In a multi-account AWS setup, a core Terraform module is versioned in a private registry and consumed by 12 workspaces. A regional failover requires a safe rollback to the previous core module version without drift. Describe the end-to-end strategy, including version pinning, CI validation, and state/rollback mechanisms?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["Citadel","Goldman Sachs","Lyft"]},{"id":"q-1197","question":"In a multi-account AWS setup, a single Terraform repo provisions VPCs and IAM roles per environment using provider aliases. A governance rule requires per-environment tagging and automatic drift detection that blocks non-Terraform changes. Describe a concrete pattern to enforce per-account isolation, tagging, and drift guardrails, including provider aliasing, remote state per environment, and a PR-based drift test workflow?","channel":"terraform","subChannel":"general","difficulty":"intermediate","tags":["terraform"],"companies":["Citadel","Twitter"]},{"id":"q-1271","question":"You have a Terraform project that provisions an AWS VPC and a small app stack. You want developers to run the same config against their own environments using per-environment secrets (DB_PASSWORD, APP_SSH_KEY) that are never stored in git. Outline a minimal structure (files, vars, and commands) to supply these secrets safely, and explain how you prevent secrets from triggering plan changes or drift?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["LinkedIn","Microsoft","Twitter"]},{"id":"q-1615","question":"You have a VPC with public and private subnets and an Internet Gateway. You want to optionally provision a NAT Gateway in the public subnet based on a boolean var create_nat_gateway (default true). How would you implement conditional creation of the Elastic IP, NAT Gateway, and the private route to 0.0.0.0/0 using Terraform 0.12+ syntax? Explain how you handle plan stability for existing deployments when the flag toggles?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Meta","Plaid","Snap"]},{"id":"q-1666","question":"You manage a Terraform project that already provisions a VPC with a public subnet, a private subnet, and an EC2 instance in the private subnet via a NAT Gateway. Add a feature flag to optionally create an RDS instance in the private subnet, but only when var.create_rds is true. Ensure running 'terraform apply' in non-prod environments does not touch the RDS resource. Describe the exact Terraform changes you would make, including the variable declaration, the RDS resource, and any dependencies, with a minimal disruption?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["NVIDIA","Two Sigma"]},{"id":"q-2099","question":"Create a minimal Terraform setup using provider aliases for Cloudflare and IBM Cloud to provision a single IBM Cloud VM and a Cloudflare A record for example.com, such that the A record always points to the VM's public IP. Outline folder structure, how to reference outputs, and how updates occur with no downtime?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Cloudflare","IBM","Stripe"]},{"id":"q-2255","question":"Design a scalable onboarding workflow for per-tenant environments in Terraform that provisions AWS VPCs via a shared module while giving each tenant an isolated Terraform Cloud workspace and remote state. Explain how you'd enforce per-tenant tagging, IAM least privilege, and network boundaries, and how you handle tenant retirement with drift-aware teardown?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["Discord","Microsoft","Snap"]},{"id":"q-2330","question":"You have a Terraform project that currently stores state locally; describe how you would switch to an S3 backend with DynamoDB locking, migrate the existing state safely, and adjust team workflows to prevent drift during the transition?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Citadel","Databricks","Meta"]},{"id":"q-2499","question":"In a Terraform project spanning IBM Cloud resources with a shared network module across three environments dev staging prod, implement per environment state isolation and drift aware deployments. Describe a backend strategy using remote backends per environment, provider aliases for IBM Cloud, and CI gates with Open Policy Agent that block applies when drift is detected or policies fail. Include concrete backend config and a minimal drift check approach?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["IBM","Meta"]},{"id":"q-2626","question":"You have a Terraform repo that uses a private git module for VPC networking across two environments. A new module tag was pushed, but CI fails due to registry access. Explain how you'd pin module versions, add a local fallback path, and validate with 'terraform init' and 'terraform plan' for both environments. Provide a minimal config snippet showing module source and version pinning?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Microsoft","Snap","Uber"]},{"id":"q-479","question":"You're managing a multi-region infrastructure with 50+ Terraform modules. How would you design a strategy to handle state locking, drift detection, and safe deployments across regions while minimizing downtime?","channel":"terraform","subChannel":"general","difficulty":"advanced","tags":["terraform"],"companies":["Microsoft","Uber"]},{"id":"q-508","question":"You have a Terraform configuration that creates multiple EC2 instances across different availability zones. How would you implement a blue-green deployment strategy using Terraform workspaces and what are the key considerations?","channel":"terraform","subChannel":"general","difficulty":"intermediate","tags":["terraform"],"companies":["Citadel","PayPal","Scale Ai"]},{"id":"q-563","question":"You're deploying a simple web application using Terraform. How would you create an AWS EC2 instance with a security group that allows HTTP traffic on port 80?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Coinbase","Discord","Slack"]},{"id":"q-592","question":"How would you use Terraform variables to manage different environments (dev/staging/prod) while keeping your configuration DRY?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Oracle","Snowflake","Stripe"]},{"id":"q-854","question":"In a Terraform Cloud setup spanning AWS and GCP, you must enforce a cross-cloud policy: every resource must carry a non-empty 'cost-center' tag and new regions must not auto-create default VPCs. How would you implement drift detection, policy gating, and automatic remediation across workspaces without downtime?","channel":"terraform","subChannel":"general","difficulty":"intermediate","tags":["terraform"],"companies":["Amazon","Discord","Google"]},{"id":"q-983","question":"In a Terraform project that provisions an AWS S3 bucket, add a new boolean variable enable_sse to toggle server-side encryption; when enable_sse is true, the bucket should have server-side encryption AES256 enabled. How would you implement this in the bucket resource using Terraform 0.12+ syntax, ensuring existing deployments remain stable and the plan doesn't force unnecessary changes?","channel":"terraform","subChannel":"general","difficulty":"beginner","tags":["terraform"],"companies":["Microsoft","Tesla"]},{"id":"gh-105","question":"What is Infrastructure Drift and how do you detect and prevent it?","channel":"terraform","subChannel":"state-management","difficulty":"advanced","tags":["advanced","cloud"],"companies":["Amazon","Google","Microsoft","Netflix","Stripe"]},{"id":"q-175","question":"You have a Terraform configuration with multiple developers working on the same infrastructure. How would you implement remote state locking to prevent state corruption and enable team collaboration?","channel":"terraform","subChannel":"state-management","difficulty":"intermediate","tags":["state","backend"],"companies":["Amazon","Google","Microsoft","Stripe","Uber"]},{"id":"q-221","question":"How would you implement a zero-downtime blue-green deployment strategy using Terraform workspaces, remote state locking, and Atlantis for production-scale microservices?","channel":"terraform","subChannel":"state-management","difficulty":"advanced","tags":["dry","terragrunt","atlantis"],"companies":["Amazon","Google Cloud","Microsoft","Stripe","Uber"]},{"id":"q-247","question":"How does Terraform remote state prevent conflicts when multiple team members work on the same infrastructure, and what are the key mechanisms involved?","channel":"terraform","subChannel":"state-management","difficulty":"beginner","tags":["remote-state","locking","workspaces"],"companies":["Amazon","Hashicorp","Microsoft","Netflix","Stripe"]},{"id":"q-1009","question":"Scenario: a single repo provisions per-tenant AWS resources for many tenants. To isolate state without workspaces, use a shared S3 backend with a DynamoDB lock table and a tenant-scoped key. Describe the backend config (bucket, region, dynamodb_table, key pattern per tenant), how you detect drift across tenants, and CI gating for dev-to-prod promotions (plan -out, tests, approve, apply)?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Amazon","Coinbase","Databricks"]},{"id":"q-1154","question":"In a Terraform module that provisions an AWS RDS instance, operators sometimes alter maintenance_window directly in AWS, creating drift. You want Terraform to ignore external changes to maintenance_window but still apply code-driven updates (e.g., allocated_storage). How would you implement this using a lifecycle block? Provide a minimal aws_db_instance snippet showing ignore_changes for maintenance_window and outline tradeoffs?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Salesforce","Scale Ai","Tesla"]},{"id":"q-1172","question":"In AWS us-east-1, you must provision 3 private subnets in a single VPC across 3 AZs using one module. Define a map variable with AZs and CIDRs, create the subnets with for_each, attach appropriate tags, and output the subnet IDs. Then show how to reference these IDs in a resource that requires subnet_id (eg NAT Gateway) and justify for_each vs count?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Netflix","Oracle"]},{"id":"q-1202","question":"Scenario: you must bring under Terraform management a set of existing AWS S3 buckets across teams. Some buckets already exist and must be imported; you will manage encryption and versioning with a single module using for_each over a bucket map. How would you import, structure, and promote changes safely via CI?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Amazon","NVIDIA","Robinhood"]},{"id":"q-1338","question":"Scenario: You maintain a Terraform repo that provisions an AWS RDS primary in us-east-1 and an optional cross-region read replica in us-west-2 controlled by a feature flag var.enable_replica. If the flag is false, Terraform should destroy the replica on apply. How would you implement the cross-region provider, conditional resource creation, and safe destruction without leaving dangling resources? Provide code patterns for the provider alias and the replica resource?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Hugging Face","NVIDIA","Netflix"]},{"id":"q-1367","question":"You\\'re managing 100 AWS subnets defined in a single Terraform module using for_each. A drift occurred in one subnet\\'s route_table_id because it was changed outside Terraform. Describe exactly how you\\'d detect the drift and fix only that resource with zero-drift impact, using Terraform CLI commands. Include the exact commands to init plan, taint the resource, apply with -target, and re-run a full plan to confirm drift-free state?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Databricks","IBM","Meta"]},{"id":"q-1584","question":"You are refactoring a Terraform repo and moving an existing AWS S3 bucket resource from the root module into a submodule named storage. Describe exact steps and commands to relocate the resource in state without recreation, including the state mv addresses and a follow-up plan?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Coinbase","Instacart","Zoom"]},{"id":"q-1655","question":"You discover an AWS VPC and related resources created outside Terraform and want to bring them under a new network module without recreating. Describe exact steps to import the VPC, its subnets (for_each), and a peering connection into the module, including the resource addresses you’ll use, how to handle multiple subnets, and how to verify with a plan that nothing changes?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Lyft","PayPal"]},{"id":"q-1740","question":"Advanced cross-repo Terraform: network state is stored in a dedicated repo/backends across envs. The application repo imports VPC IDs and subnets via data terraform_remote_state. Explain how you would structure backends and modules to avoid drift, ensure isolation between dev/staging/prod, and orchestrate safe promotions from dev to prod with exact commands for init/plan/apply per environment. Include how you would test drift and rollback?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Adobe","Databricks"]},{"id":"q-1749","question":"You manage a single Terraform repo deploying a shared 'network' module into two AWS accounts (prod and dev) using provider aliases. Describe how you structure the provider blocks, module calls, and var-files to deploy both environments without duplicating code. Include exact commands to init, plan, and apply for each environment, ensuring isolation and no cross-env state changes?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Robinhood","Slack","Snap"]},{"id":"q-1805","question":"How would you implement input validation in a Terraform module to enforce that a variable instance_count is between 1 and 5 and that a string region is one of an allowed set? Provide the approach and expected Terraform behavior during plan and apply?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Netflix","Square","Tesla"]},{"id":"q-1836","question":"Scenario: A Terraform repo uses a module named network that creates subnets via for_each over a map of subnet_id to name. Three subnets were created manually (subnet-aaa, subnet-bbb, subnet-ccc) and must be brought under Terraform control without changing the rest. Describe exact steps and commands to import only these existing subnets into the module state, update the for_each map to include their IDs, and verify drift with a full plan. Include init/plan/apply commands and how to handle dependent resources (route tables, tags)?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["NVIDIA","Plaid"]},{"id":"q-1888","question":"Scenario: You manage 120 Cloudflare DNS records in a single Terraform module with for_each. A drift occurred in one record's IP address due to a manual edit in Cloudflare. Describe a surgical plan to detect drift and fix only that resource with zero-drift impact, using Terraform CLI commands. Include the exact commands to init, plan, taint the resource, apply with -target, and re-run a full plan to confirm drift-free state?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Citadel","Microsoft","Plaid"]},{"id":"q-1948","question":"You’re extending a VPC Terraform module to support both a managed NAT Gateway and a NAT instance via a single input variable named nat_type with default 'gateway'. How would you implement this with conditional resources, including the variable, two resource blocks, and outputs? Provide the minimal code and explain how you’d validate with terraform plan to ensure no changes when nat_type remains 'gateway'?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Google","PayPal","Twitter"]},{"id":"q-1976","question":"In a monorepo, a Terraform module provisions a multi-tenant AWS VPC setup with per-tenant isolation via tenant_id. A new tenant must be added without touching existing ones. Describe exact steps and commands to add the tenant using a fresh state segment, ensure no drift to others, and verify with targeted and full plans. Include backend-key organization and apply sequencing?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Goldman Sachs","LinkedIn"]},{"id":"q-2037","question":"You're provisioning a private API gateway via a custom Terraform provider. Implement a blue/green deployment pattern with two gateway instances and an atomic traffic switch using a single Terraform plan. Describe the exact structure (for_each, modules, and a central traffic_split map), how to prevent unintended recreation, and the precise steps and commands to drift-check and promote from blue to green in prod with minimal blast radius?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Netflix","PayPal"]},{"id":"q-2049","question":"You maintain a per-environment security module that provisions an AWS Security Group with environment-scoped rules (dev, stg, prod). An external admin altered prod inbound 22 to 0.0.0.0/0, causing drift. Describe exact steps to detect the drift and fix prod only, using Terraform CLI with init/plan/apply and a -target run, then re-run a full plan to confirm no drift. Provide the exact resource address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Meta","Tesla"]},{"id":"q-2079","question":"In a Terraform repo using a single VPC module across AWS prod and canary via provider aliases, a CIDR change would recreate many subnets. Describe a blue/green deployment strategy that updates production with zero downtime. Include exact commands to initialize, create/apply canary resources with targeted -target, run drift checks, then migrate canary state to prod with terraform state mv and finalize with a full plan?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Bloomberg","Netflix"]},{"id":"q-2127","question":"You manage a Terraform repo with a Google Cloud IAM bindings module using for_each over projects. A drift occurred when a member was removed in prod outside Terraform. Describe exact steps to detect the drift and fix only prod binding using Terraform CLI with init/plan/apply and a -target run, then re-run a full plan to confirm no drift. Include the exact resource address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Adobe","Google","Slack"]},{"id":"q-2168","question":"You find an AWS IAM role that was created outside Terraform and you want to bring it under Terraform control without changing its attached policies—what exact steps and commands would you use to import it, reconcile the state, and apply only the intended configuration?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Adobe","Cloudflare","Coinbase"]},{"id":"q-2212","question":"Scenario: You manage a single Terraform module that provisions a Google Cloud DNS managed zone with multiple A records across environments (dev, staging, prod) using a map variable and a for_each on google_dns_record_set. A drift occurred in prod: the IP of prod A record was manually changed outside Terraform. Describe exact steps to detect drift and fix only prod's A record using Terraform CLI with init/plan/apply and a -target run, then re-run a full plan to confirm drift-free state. Include the exact resource address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Databricks","Meta","NVIDIA"]},{"id":"q-2238","question":"Scenario: A Terraform module manages an AWS VPC with subnets created outside Terraform. The module uses an aws_vpc.main and a for_each map aws_subnet.subnets keyed by environment (prod, staging). Provide exact commands to import the existing VPC and each subnet without recreation, including their addresses, then run terraform plan to confirm no drift and describe handling if drift is detected?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Hashicorp","Snowflake"]},{"id":"q-2337","question":"You maintain a Terraform repo that builds a multi-account AWS network using a root module with per-account submodules. In prod, a single security group rule was manually changed outside Terraform. Describe exact steps to detect drift, fix only prod with a targeted apply, and then re-run a full plan. Include the exact resource address to target and how to ensure prod drift is isolated from other accounts?","channel":"terraform-associate","subChannel":"general","difficulty":"advanced","tags":["terraform-associate"],"companies":["Meta","Zoom"]},{"id":"q-2345","question":"A Terraform repo provisions a VPC, subnets, and an EC2 instance. A central tagging policy is required via a local default_tags map and an optional input var.tags, applied to all resources without duplicating tags. Describe exact steps to implement this, including how to merge tags in each resource, how to handle resources that cannot be tagged, and how to validate with a full plan before apply. Include a minimal code snippet showing how to declare the variables and apply tags to the VPC and EC2 instance?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Hashicorp","LinkedIn"]},{"id":"q-2470","question":"You find an existing Google Compute Engine instance named \\\"web-1\\\" that wasn't created by Terraform but should be managed by the Terraform module at path modules/infra with a for_each over a map of instances. Describe exact steps to import that instance into state, including the resource address you would use for the import, what to add to the code to match the real-world instance, and how you verify drift with a full terraform plan after the import (no recreation)?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Google","MongoDB"]},{"id":"q-2563","question":"You manage a Terraform repo deploying a shared AWS network module across three accounts with for_each over accounts to create security groups. In account \"acct-b\" a security group rule’s CIDR was manually changed outside Terraform. Describe exact steps to detect drift and fix only that resource with zero-drift impact, using Terraform CLI init/plan -detailed-exitcode, then a -target run, and finally a full plan. Include the exact resource address to target?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Amazon","Meta","Twitter"]},{"id":"q-2610","question":"In a Terraform repo with two modules, networking (producing vpc_id and public_subnet_ids as outputs) and app (consuming them), configure and use a remote state data source to read networking's outputs from a remote backend and reference them in app. Provide exact code for data 'terraform_remote_state' and show how to use outputs.vpc_id and outputs.public_subnet_ids in the app module. Assume AWS S3 backend?","channel":"terraform-associate","subChannel":"general","difficulty":"beginner","tags":["terraform-associate"],"companies":["Apple","Databricks","Slack"]},{"id":"q-864","question":"In Terraform, you need to manage two AWS accounts in a single repo: prod (provider aws.prod) and audit (provider aws.audit). Create an S3 bucket in prod and a cross-account IAM policy in audit that grants read access to that bucket. How do you configure aliased providers, reference the prod bucket ARN from the audit module, and enforce deterministic apply order (e.g., data fetch before policy) with minimal risk? Include a minimal config outline?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Snap","Snowflake","Square"]},{"id":"q-960","question":"In a single Terraform repo that provisions prod, staging, and dev AWS environments, how would you configure a single S3 backend to isolate state for each environment without using separate Terraform workspaces? Provide the exact backend config (bucket, region, dynamodb_lock_table, key per env) and explain how you would promote changes from dev to prod, including drift handling and CI plan/apply steps?","channel":"terraform-associate","subChannel":"general","difficulty":"intermediate","tags":["terraform-associate"],"companies":["Bloomberg","Google","Hashicorp"]},{"id":"q-1033","question":"You build a small service that fetches user profiles via GET /api/users/{id} and caches results in memory for 5 minutes. Write concrete tests that verify: (1) a cache miss calls the API, stores the result with TTL; (2) a cache hit returns cached value without API call; (3) TTL expiry triggers a fresh API call to refresh cache. Provide example test code?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["Google","Lyft","Netflix"]},{"id":"q-1076","question":"You implement a debounce utility in frontend code: debounce(fn, wait) returns a wrapper that ensures fn is called at most once per wait ms, using the last invocation's arguments. Write a focused test that demonstrates rapid successive calls do not trigger fn more than once, and that a final call after waiting triggers with the latest args?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["DoorDash","Instacart","Oracle"]},{"id":"q-1287","question":"Design a testing strategy for a real-time data pipeline built with Apache Flink processing millions of events per second, ensuring exactly-once semantics across sources and sinks, handling out-of-order and late data, with stateful operators and checkpointing. Outline how you'd structure unit, integration, and end-to-end tests, simulate late data and failures, verify sink idempotence and recovery guarantees, and specify concrete tools and success criteria?","channel":"testing","subChannel":"general","difficulty":"advanced","tags":["testing"],"companies":["Amazon","Google","Scale Ai"]},{"id":"q-2150","question":"You're deploying a Delta Lake ingestion pipeline on Databricks: a Spark Structured Streaming job reads from a Kafka topic (Avro, Schema Registry), upserts into a partitioned Delta table, and downstream queries rely on the latest state. Design a test plan that (1) proves exactly-once semantics and idempotent upserts under retry/replay, (2) validates safe schema evolution without breaking downstream code, and (3) detects data quality regressions (missing keys, late data) within a 24h window. Include artifacts, environments, and metrics?","channel":"testing","subChannel":"general","difficulty":"advanced","tags":["testing"],"companies":["Databricks","IBM","Scale Ai"]},{"id":"q-2219","question":"Design a practical test plan for a new endpoint '/upload-csv' that accepts CSV files up to 5MB. The CSV must have **headers**; on success, records are inserted into PostgreSQL table `users(id, name, email)` and 201 is returned. Provide concrete unit tests, integration tests, and end-to-end tests. Include edge cases: empty file, invalid header, invalid email, duplicates, and simulate **DB** outage?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["LinkedIn","Snowflake"]},{"id":"q-2419","question":"In a MongoDB-backed user service, introduce an optional field 'lastLogin' (timestamp) to user documents without breaking existing clients. Provide a concrete, beginner-friendly test plan covering: (1) ensuring existing docs are untouched, (2) new docs set lastLogin correctly, (3) queries that filter on lastLogin behave as expected, (4) rollback path if migration fails. Include test data, tooling, and a minimal CI flow?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["IBM","MongoDB","Twitter"]},{"id":"q-2601","question":"You're adding a multilingual contact form to a React app that saves submissions to a REST API. The UI has client-side validation and the API may respond with 400 or 429 errors. Describe a beginner-friendly, concrete test plan to verify correctness, localization, and resilience in CI. Include test types, data sets, and example test cases?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["IBM","Microsoft"]},{"id":"q-2649","question":"You're implementing a background data export feature: when a user clicks Export, the system queues a request to generate a CSV of that user's data and deliver it to a downstream REST API. Propose a beginner-friendly, concrete test plan that covers correctness, idempotency, retry/backoff behavior, and data privacy (redaction of PII). Include test types, data sets, and example test cases?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["Google","IBM","Square"]},{"id":"q-480","question":"How would you design a comprehensive testing strategy for a distributed microservices architecture handling 10M requests/day, ensuring 99.9% uptime while maintaining fast CI/CD pipelines?","channel":"testing","subChannel":"general","difficulty":"advanced","tags":["testing"],"companies":["Bloomberg","LinkedIn","NVIDIA"]},{"id":"q-509","question":"How would you test a REST API endpoint that creates a user account, including validation, error handling, and database integration?","channel":"testing","subChannel":"general","difficulty":"intermediate","tags":["testing"],"companies":["Amazon","Coinbase","Tesla"]},{"id":"q-537","question":"You're testing a real-time chat application that uses WebSockets. How would you design a test strategy to verify message ordering, connection resilience, and concurrent user scenarios?","channel":"testing","subChannel":"general","difficulty":"intermediate","tags":["testing"],"companies":["NVIDIA","Twitter","Two Sigma"]},{"id":"q-593","question":"How would you test a function that makes HTTP requests to an external API? What testing strategies would you use?","channel":"testing","subChannel":"general","difficulty":"intermediate","tags":["testing"],"companies":["Apple","Microsoft"]},{"id":"q-990","question":"You maintain a Node.js API function getUser(userId) that reads from MongoDB via Mongoose and caches the result in an in-memory TTL cache (60s). Write a practical test plan and code to verify: (1) first call hits DB and caches, (2) second call returns cached value, (3) after TTL expires a new DB hit occurs and cache updates, (4) DB error propagates to caller. Use Jest with minimal mocks and demonstrate time-control?","channel":"testing","subChannel":"general","difficulty":"beginner","tags":["testing"],"companies":["Airbnb","MongoDB"]},{"id":"q-259","question":"How would you design integration tests for a Saga pattern implementation across 5 microservices to ensure exactly-once transaction processing and proper compensation handling during partial failures?","channel":"testing","subChannel":"integration-testing","difficulty":"advanced","tags":["api-testing","database-testing","mocking"],"companies":["Airbnb","Amazon","LinkedIn","Netflix","Spotify","Twitter","Uber"]},{"id":"q-207","question":"How would you implement a test-driven development workflow for a REST API endpoint using Jest and Supertest, following the red-green-refactor cycle with proper test organization and mocking strategies?","channel":"testing","subChannel":"tdd","difficulty":"intermediate","tags":["test-driven","red-green-refactor","test-first"],"companies":["Amazon","Google","Meta","Microsoft","Netflix","Stripe"]},{"id":"q-360","question":"You're building a simple calculator class. Write a failing test first, then implement the add method using TDD. What's the red-green-refactor cycle?","channel":"testing","subChannel":"tdd","difficulty":"beginner","tags":["test-driven","red-green-refactor","test-first"],"companies":["Amazon","Anthropic","Google","Meta","Microsoft","Salesforce","Stripe"]},{"id":"q-405","question":"You're building a real-time collaborative drawing feature where multiple users can simultaneously edit a canvas. How would you apply TDD to test the conflict resolution mechanism when two users edit the same element at the same time?","channel":"testing","subChannel":"tdd","difficulty":"intermediate","tags":["test-driven","red-green-refactor","test-first"],"companies":["Canva","Unity","Zoom"]},{"id":"q-234","question":"How would you design a scalable test architecture for a microservices application handling 10,000+ concurrent tests across multiple environments while ensuring test isolation, performance, and CI/CD integration?","channel":"testing","subChannel":"test-strategies","difficulty":"advanced","tags":["jest","mocha","pytest","junit"],"companies":["Amazon","Google","Microsoft","Netflix","Salesforce","Stripe"]},{"id":"q-278","question":"How would you design a comprehensive testing strategy for a microservices architecture that scales to handle millions of requests per second while ensuring 99.99% availability?","channel":"testing","subChannel":"test-strategies","difficulty":"advanced","tags":["test-pyramid","coverage","mutation-testing"],"companies":["Amazon","Google","Meta","Netflix"]},{"id":"q-325","question":"How would you implement mutation testing to validate your test suite's quality and what's the relationship between mutation testing and code coverage?","channel":"testing","subChannel":"test-strategies","difficulty":"advanced","tags":["test-pyramid","coverage","mutation-testing"],"companies":["Epic Systems","Jane Street","Western Digital"]},{"id":"q-349","question":"You're building a distributed event streaming platform similar to Kafka. How would you design a comprehensive testing strategy that ensures message ordering guarantees, exactly-once semantics, and fault tolerance across a cluster of brokers?","channel":"testing","subChannel":"test-strategies","difficulty":"advanced","tags":["test-pyramid","coverage","mutation-testing"],"companies":["Confluent","Epic Games","Meta"]},{"id":"q-374","question":"You're testing a ServiceNow form validation module. How would you structure your test pyramid and what coverage metrics would you track?","channel":"testing","subChannel":"test-strategies","difficulty":"beginner","tags":["test-pyramid","coverage","mutation-testing"],"companies":["Amazon","Google","Microsoft","Netflix","Okta","Salesforce","Servicenow"]},{"id":"q-416","question":"You're building a React component library. How would you structure your test pyramid and what specific coverage metrics would you target for each layer?","channel":"testing","subChannel":"test-strategies","difficulty":"beginner","tags":["test-pyramid","coverage","mutation-testing"],"companies":["Broadcom","Hugging Face","Meta"]},{"id":"q-296","question":"In Jest, how would you implement advanced mocking patterns including sequential return values, async behavior, and proper mock lifecycle management for comprehensive test coverage?","channel":"testing","subChannel":"unit-testing","difficulty":"intermediate","tags":["jest","mocha","pytest","junit"],"companies":["Google","Meta","Netflix","Salesforce","Stripe"]},{"id":"q-311","question":"How do you mock a function in Jest that's called within another function under test?","channel":"testing","subChannel":"unit-testing","difficulty":"advanced","tags":["jest","mocha","pytest","junit"],"companies":["Amazon","Google","Meta"]},{"id":"q-338","question":"You're testing a React component that fetches user data from an API. How would you write a unit test using Jest to mock the API call and verify the component renders the user's name correctly?","channel":"testing","subChannel":"unit-testing","difficulty":"beginner","tags":["jest","mocha","pytest","junit"],"companies":["Cisco","Hulu","Postman"]},{"id":"q-388","question":"You're testing a REST API endpoint that returns user data. Write a basic unit test using Jest that verifies the endpoint returns a 200 status code and the response contains a 'name' field. What's the most important assertion to include?","channel":"testing","subChannel":"unit-testing","difficulty":"beginner","tags":["jest","mocha","pytest","junit"],"companies":["Postman","Retool","Supabase"]},{"id":"q-1002","question":"In a Unix environment logs are stored in /var/log/app/*.log with lines formatted as timestamp|user|action|resource. Write a practical one-liner using standard UNIX tools to output the top 5 users by total actions in the last 24 hours. Explain how you would handle log rotation and malformed lines?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Amazon","Robinhood"]},{"id":"q-1032","question":"How would you capture stdout and stderr of a simple shell command into separate log files while still displaying live output in the terminal? Provide a concrete Bash command and brief justification?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Databricks","LinkedIn","Uber"]},{"id":"q-1069","question":"In a Unix environment, logs live under /var/log/metrics/*.log and are hourly rotated to .log and .log.gz. Each line is like [YYYY-MM-DD HH:MM:SS] LEVEL: message. Propose a robust, portable approach (one-liner preferred) to output the number of ERROR events per hour for the last 6 hours, handling missing files and rotations without external dependencies?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Adobe","Meta","Plaid"]},{"id":"q-1123","question":"In a Unix environment, multiple services write JSON logs under /var/log/diag/*.log and rotated hourly to *.log.gz. Each line is a JSON object like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"ERROR\",\"msg\":\"...\"}; Propose a robust one-liner (no external dependencies beyond standard UNIX tools) to output the number of ERROR events per hour for the last 4 hours, aggregated across all services, and tolerant of missing files and rotated archives?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Apple","OpenAI","Snap"]},{"id":"q-1139","question":"Scenario: JSON logs at /var/log/diag/*.log, rotated hourly to *.log.gz. Each line: {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"ERROR\",\"msg\":\"...\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) that outputs the per-hour 99th percentile of message length for the last 4 hours, across all services, deduplicating identical messages per hour, and tolerant of missing files and gz archives?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["OpenAI","Plaid","Uber"]},{"id":"q-1224","question":"Scenario: In a Unix cluster, logs are emitted as JSON lines under /var/log/cluster/*/*.log and rotated hourly to *.log.gz. Each line contains {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"tenant\":\"tenant-id\",\"svc\":\"service\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}; Propose a robust one-liner (no external deps beyond standard UNIX tools) to identify the top 3 tenants by error rate (ERROR / total events) for the last 6 hours, aggregated across all services, tolerant of missing files and gz archives?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Apple","Coinbase","PayPal"]},{"id":"q-1241","question":"Scenario: In a multi-user Unix workspace, /home contains subdirectories per user with various files. Some are large, some are temporary. Provide a robust one-liner (no external dependencies) that lists the five largest regular files under /home (recursively), excluding hidden files and symlinks, printing each as 'size<TAB>path' with sizes in human-readable form. Explain how you ensure safety against spaces and newlines in filenames?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Databricks","Lyft","Tesla"]},{"id":"q-1492","question":"Scenario: multiple services log JSON lines to /var/log/app/*.log, rotated hourly to *.log.gz. Each line looks like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lat_ms\":123}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to compute the per-service average lat_ms in the last 2 hours, across all logs, tolerant of missing and gzipped files, ignoring malformed lines. Output: 'service avg_ms'?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["PayPal","Salesforce"]},{"id":"q-1809","question":"Scenario: A Linux host logs auth events into /var/log/auth/{service}/*.log and rotates hourly to *.log.gz. Each line is JSON like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"user\":\"alice\",\"action\":\"LOGIN\",\"result\":\"FAIL\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to print the top 5 users by failed login rate in the last 2 hours, aggregated across all services, tolerant of missing files and gz archives, and resilient to malformed lines?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Databricks","Robinhood","Tesla"]},{"id":"q-1903","question":"Propose a robust one-liner to compute the number of ERROR lines per hour for the previous 12 hours, aggregating across all logs in /var/log/app and across both uncompressed (*.log) and rotated/compressed files (*.log.*, *.log.gz). The one-liner must tolerate missing files and gz archives and rely only on standard UNIX tools?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Databricks","Slack","Stripe"]},{"id":"q-1975","question":"In a Unix host, logs live in /logs/web/*.log and are rotated hourly to *.log.gz. Each line is a JSON object like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"ip\":\"1.2.3.4\",\"method\":\"GET\",\"path\":\"/api\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to output the top 5 IPs by request count in the last 4 hours, aggregating across all files and rotations, tolerant of missing files and compressed archives?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Google","Snap","Twitter"]},{"id":"q-2094","question":"Scenario: In a Unix CI environment, logs are written by build agents under /build/logs/*/*.log and rotated hourly to *.log.gz. Each line is BUILDID|TIMESTAMP|STEP|STATUS|MESSAGE. Propose a robust one-liner (no external deps beyond standard UNIX tools) to report, for the last 24 hours, per BUILDID and per hour, the count of failed steps (STATUS != 'SUCCESS'), aggregating across all agents and rotations and tolerating missing files and compressed archives?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Apple","IBM"]},{"id":"q-2253","question":"Scenario: /var/run contains *.pid per daemon (service name == filename). Some daemons crash leaving stale PIDs. Propose a robust one-liner (no external deps) that lists service:pid pairs for all PID files whose PID is not running, tolerant of missing files and spaces in names?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Coinbase","Microsoft"]},{"id":"q-2293","question":"In a Unix environment, logs live under /var/log/app-logs/*.log and rotations *.log.gz. Each line is a JSON object like {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"dur_ms\":1234,\"ev\":\"RUN\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) to compute, per service, the mean and 95th percentile of dur_ms for the last 2 hours, aggregating across all files and rotations, tolerating missing files and archives?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Google","IBM","Zoom"]},{"id":"q-2396","question":"In a Unix host, multiple services write JSON log lines to /logs/app/*/*.log, rotated hourly to *.log.gz. Each line is {\"ts\":\"YYYY-MM-DDTHH:MM:SSZ\",\"svc\":\"name\",\"lvl\":\"LEVEL\",\"msg\":\"...\"}. Propose a robust one-liner (no external deps beyond standard UNIX tools) that outputs a per-minute histogram of WARN or ERROR events for the last 60 minutes, aggregated across all files and rotations, tolerant of missing files and gz archives, and resilient to out-of-order timestamps within the minute bucket?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Apple","Databricks","PayPal"]},{"id":"q-2420","question":"Scenario: In a Unix host, logs live under /var/log/app-logs with daily rotation and filenames like app-<service>-YYYYMMDD.log. Write a robust one-liner (no external deps beyond standard UNIX tools) that prints a total count of lines containing the word 'ERROR' across all log files modified in the last 3 days. The command must gracefully skip missing/unreadable files and work across many services?","channel":"unix","subChannel":"general","difficulty":"beginner","tags":["unix"],"companies":["Coinbase","MongoDB","Tesla"]},{"id":"q-2520","question":"In a fleet of Linux hosts, log files sit under /var/log/relay/*/*.log and rotated to *.log.gz. Each line is a JSON object with fields ts (YYYY-MM-DDTHH:MM:SSZ), svc, lat_ms. Write a robust one-liner (no external deps beyond standard UNIX tools) that outputs each service's 95th percentile latency over the last 15 minutes, aggregating across all files and rotations, tolerant of missing/unreadable files and out-of-order lines?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Google","Lyft","Twitter"]},{"id":"q-2624","question":"In a Linux host running dozens of worker processes inside containers, build a POSIX-friendly watchdog using only standard UNIX tools to detect any worker that has spent uninterruptible sleep (D state) for more than 60 seconds within a 10-minute window and restart such workers with exponential backoff. Use /proc to derive each worker's startup command from /proc/$pid/cmdline and /proc/cgroups to avoid system processes?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Amazon","Google","Zoom"]},{"id":"q-481","question":"You're debugging a production system where processes are hanging. Using only Unix tools, how would you identify which processes are blocked on I/O, what they're waiting for, and safely terminate them without causing data corruption?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Snap","Snowflake"]},{"id":"q-510","question":"You're debugging a production issue where a process is stuck in uninterruptible sleep (D state). How would you identify and handle this situation?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["OpenAI","Tesla"]},{"id":"q-538","question":"You notice a process is consuming excessive CPU on a production server. How would you diagnose and troubleshoot this issue using Unix commands?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Citadel","Goldman Sachs","Microsoft"]},{"id":"q-564","question":"You're debugging a production system where processes are hanging. Using only Unix tools, how would you identify which processes are stuck in uninterruptible sleep (D state) and what could be causing this?","channel":"unix","subChannel":"general","difficulty":"advanced","tags":["unix"],"companies":["Adobe","OpenAI","Square"]},{"id":"q-894","question":"On a Linux host, /var/log/myapp.log is written by multiple processes. Implement a robust log rotation that triggers when the file reaches 100MB, keeps 7 rotated files, compresses older logs, and ensures no log loss while writers continue. Describe the approach, commands, and failure modes for concurrent writers?","channel":"unix","subChannel":"general","difficulty":"intermediate","tags":["unix"],"companies":["Coinbase","Discord","OpenAI"]},{"id":"q-264","question":"How do Unix pipes enable inter-process communication and what are their performance implications?","channel":"unix","subChannel":"system-programming","difficulty":"beginner","tags":["posix","signals","pipes","sockets"],"companies":["Amazon","Apple","Google","Meta","Microsoft"]},{"id":"q-1014","question":"Scenario: A multinational SaaS runs Vault with cross-region replication. Tenants use Vault's DB Secrets Engine for Postgres with per-tenant roles and short TTLs. Explain how you would enforce per-tenant rotation and immediate revocation on disable, while preventing cross-tenant leakage during DR failover?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Adobe","Goldman Sachs","MongoDB"]},{"id":"q-1054","question":"Scenario: A multi-tenant SaaS stores per-tenant API keys in Vault KV v2 with versioning. A rotation accidentally overwrote the previous key. Describe exactly how you would recover the previous version, which Vault paths and commands you'd use, and what policy controls you would enforce to prevent accidental deletions and ensure auditability?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Coinbase","Hugging Face","Tesla"]},{"id":"q-1094","question":"Design a per-tenant envelope encryption workflow using Vault Transit: each tenant has a dedicated Transit key; generate a per-tenant DEK, encrypt data with the DEK, and store the ciphertext. How would you rotate the Transit key without downtime, rewrap existing ciphertext to the new version, handle tenant disablement (revocation), and maintain end-to-end auditability? Include specific Vault paths and commands?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Hashicorp","Instacart","Twitter"]},{"id":"q-1174","question":"In a multi-tenant SaaS, you store per-tenant data encryption keys in Vault Transit with a dedicated key per tenant and daily rotation. Describe how you would implement per-tenant key lifecycle (creation, rotation without downtime, data rewrap for existing data, and immediate revocation when a tenant is disabled) and how you would monitor/audit across services?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Hugging Face","MongoDB","Stripe"]},{"id":"q-1229","question":"You’re building a multi-tenant SaaS and plan to encrypt data at rest using Vault's Transit engine. Describe how you would enable Transit, create per-tenant keys, implement encryption/decryption via the API, and perform zero-downtime key rotation while preserving ciphertext integrity. Include policy considerations to prevent data leakage and how you validate rotation?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Hugging Face","Meta","Tesla"]},{"id":"q-1357","question":"In a microservices setup, Vault's PKI engine issues short-lived TLS certs for mTLS. Describe end-to-end: enable PKI at path pki, create a role microservice-tls with allowed_domains='service.local', allow_subdomains=true, max_ttl='24h', issue a cert for 'auth-service.service.local', and revoke certificates immediately when a service is decommissioned and audit issuance?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Bloomberg","Lyft","MongoDB"]},{"id":"q-1405","question":"In a multi-DC Vault deployment used by a large enterprise, explain how to provision ephemeral SSH access to hosts using Vault's SSH secrets engine. Include how to establish a per-user role, certificate TTLs, revocation on logout or compromise, host-key rotation, and how to audit all SSH sessions end-to-end?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["LinkedIn","Uber"]},{"id":"q-1446","question":"You operate a multi-tenant app using Vault's Transit engine to encrypt per-tenant data with keys at transit/tenant-{tenant_id}. When retiring a tenant, you must rotate its key, re-encrypt historical data, and keep live traffic unaffected. Describe a concrete plan for key rotation, re-encryption strategy, audit logging, and rollback safeguards to preserve decryptability?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Snowflake","Tesla","Twitter"]},{"id":"q-1504","question":"Scenario: A project alpha needs read-only access to its KV v2 secrets at secret/data/projects/alpha and secret/metadata/projects/alpha. Define a policy 'alpha-read' granting read to those endpoints with renewable=false and a max TTL of 1h, and show the token issuance command to grant this policy. How would you verify expiry and revoke immediately if the project ends?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["NVIDIA","Snowflake"]},{"id":"q-1569","question":"Describe how you would configure Vault audit logging and per-tenant access controls for a SaaS that issues per-tenant Snowflake credentials via Vault's database secrets engine. Include audit setup, tenant-scoped policies, how to attach tenant metadata to credentials, and a test plan to verify instant revocation and tenant-specific logs?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Meta","NVIDIA","Snowflake"]},{"id":"q-1676","question":"You manage a SaaS that grants per-tenant SSH access to a fleet using Vault's SSH Secret Engine. Each tenant receives ephemeral SSH credentials valid for 24 hours, scoped to a per-tenant username and a dedicated user CA. Outline the operational steps to configure the engine, define a per-tenant role, enforce 24h TTLs, and ensure immediate revocation on de-provisioning, including how you would issue credentials and revoke them?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Airbnb","Anthropic","Two Sigma"]},{"id":"q-1699","question":"Design a practical key-rotation workflow using Vault Transit to rotate a dataset encryption key (DEK) for a Databricks workflow that writes Parquet files to S3, ensuring zero-downtime encryption for new data, safe rewrap of existing ciphertext, and immediate revocation if a cluster is decommissioned. Include exact Vault commands and rollout plan?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Databricks","Zoom"]},{"id":"q-1855","question":"You run a multi-tenant Databricks data platform using Vault's database secret engine to issue per-tenant Snowflake credentials with short TTLs. Design a rotation strategy that rotates credentials across 1000+ concurrent jobs with zero downtime, and enables immediate revocation when a tenant is disabled. Include Vault role config, rotation/renewal workflow, and how to propagate changes to active notebooks/clusters?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Databricks","Two Sigma"]},{"id":"q-1894","question":"You're operating a Slack/Anthropic-scale SaaS using Vault for **Kubernetes** workloads. Each namespace gets a dedicated Vault role via the Kubernetes auth method, issuing per-pod credentials with a TTL of 15 minutes. Describe how you would implement scalable rotation and safe renewal to avoid expired secrets, including: (a) agentless vs agent-based rotation strategies, (b) pre-rotation and lease renewal coordination across many pods, (c) handling long-running jobs that outlive their pods, and (d) auditing, revocation on namespace deletion, and potential upgrade paths. Include concrete Vault commands and config snippets?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Anthropic","Slack"]},{"id":"q-1896","question":"Design an end-to-end workflow using Vault's SSH secrets engine to issue ephemeral SSH credentials for a multi-tenant Linux fleet. Include per-tenant roles under ssh/roles/tenant-*, TTL 10m, a renewal approach that preserves live sessions, and an immediate revocation path on tenant disable. Include audit scopes and concrete Vault commands?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Bloomberg","LinkedIn","NVIDIA"]},{"id":"q-2007","question":"In a multi-tenant SaaS running on Kubernetes, each tenant's app pods authenticate to Vault via Kubernetes auth. Design an end-to-end plan to ensure per-tenant secret isolation, automatic rotation with no downtime, and immediate revocation when a tenant is disabled. Include: (1) Identity/Group mapping per tenant, (2) per-tenant DB role in the Vault DB secret engine with TTL, (3) rotation flow and how apps will access new creds without downtime, and (4) tenant disable revoke process and audit considerations. Provide sample policies and a rollout plan?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Oracle","Plaid","Square"]},{"id":"q-2023","question":"You’re deploying a small SaaS that requires per-service mTLS authentication. Using Vault's PKI engine, design an issuance flow to deliver short-lived client certificates (TTL 15m) to services, with immediate revocation on termination. Specify role config, issuance path, revocation process, and how you'd validate revocation in a running cluster?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Instacart","OpenAI","Snap"]},{"id":"q-2108","question":"Design a PCI-DSS compliant multi-tenant Vault deployment across three regions. Each tenant retrieves per-tenant DB credentials via Vault with strict isolation, rotation, and revocation. Provide an isolation strategy (namespaces vs path-based), a cross-region DR plan, a zero-downtime rotation workflow, and concrete Vault commands and policies for rollout and rollback?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Goldman Sachs","LinkedIn","Tesla"]},{"id":"q-2156","question":"You're running a multi-region Vault deployment for a SaaS with tenants isolated in namespaces. Plan a cross-region DR replication that preserves tenant isolation and minimizes downtime. Include: 1) namespace/policy design across regions, 2) replication topology and sync behavior, 3) tenant token/lease revocation paths during failover, 4) a failover testing plan with realistic success metrics?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Apple","Coinbase","Discord"]},{"id":"q-2215","question":"In a multi-tenant Vault Enterprise deployment with per-tenant namespaces, a tenant’s OIDC token was compromised. Outline a practical incident response plan to immediately revoke leases, rotate the tenant’s DB and KV secrets, and restore access with minimal downtime. Include concrete Vault commands, rollback checks, and validation steps?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Adobe","NVIDIA"]},{"id":"q-2281","question":"Design a per-tenant SSH dynamic credential workflow using Vault's SSH Secrets Engine for a multi-tenant SaaS with a shared bastion host. Include per-tenant roles, TTL controls, instant revocation on tenant disable, and robust audit/logging considerations?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Google","LinkedIn","Salesforce"]},{"id":"q-2299","question":"You’re running a multi-tenant data processing platform used by Zoom and Airbnb. Vault's AWS secret engine issues per-tenant ephemeral IAM credentials for short-lived compute jobs in Kubernetes. Explain how you would: 1) map tenants to Vault roles, 2) enforce 15–30 minute TTLs with rotation without downtime, 3) revoke immediately when a tenant is disabled, and 4) audit and validate end-to-end?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Airbnb","Zoom"]},{"id":"q-2401","question":"You’re running a multi-tenant Vault Enterprise deployment with per-tenant namespaces. Data is envelope-encrypted with Vault Transit using a per-tenant key set. A tenant requests a routine key rotation due to suspected compromise. Provide a concrete, low-downtime rotation plan that (1) uses Transit key versions and rewrap, (2) ensures in-flight requests finish safely, (3) validates data integrity after rotation, and (4) defines rollback steps if issues are detected. Include concrete Vault commands and timing guidelines?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Google","MongoDB"]},{"id":"q-2575","question":"In a Vault Enterprise deployment with per-tenant namespaces, design a secure workflow using the SSH secret engine to grant ephemeral SSH access to a fleet of jump hosts for each tenant. Host keys rotate every 24 hours. Describe (a) per-tenant ssh roles and constraints, (b) the rotation flow that avoids breaking in-flight sessions, including how to handle known_hosts and key distributions, (c) immediate revocation when a tenant is disabled, and (d) validation of tenant isolation after rotation. Include concrete Vault paths and commands?","channel":"vault-associate","subChannel":"general","difficulty":"advanced","tags":["vault-associate"],"companies":["Adobe","DoorDash","Two Sigma"]},{"id":"q-2595","question":"Scenario: You run Vault KV v2 under path secret/ with per-tenant data: secret/data/tenants/{tenant}/api-key. Implement a low-downtime rotation for a tenant's API key: enable KV v2, rotate by creating a new version, ensure clients fetch the latest by default, and securely destroy older versions when a tenant is disabled. Include concrete Vault commands and policy considerations?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Hugging Face","Scale Ai","Twitter"]},{"id":"q-2641","question":"In a Vault Enterprise deployment with per-tenant namespaces on Kubernetes, each tenant's workloads authenticate via Kubernetes auth and obtain per-tenant PostgreSQL credentials through the DB secrets engine. A tenant is disabled while pods may be running. Describe a concrete, low-downtime rotation plan that revokes all tenant leases, rotates credentials, ensures in-flight requests complete safely, and validates post-rotation isolation. Include concrete Vault commands and timing guidelines?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Robinhood","Snowflake","Uber"]},{"id":"q-2707","question":"You're setting up Vault in a multi-tenant environment. Beginner task: enable a file audit device to log all requests to /var/log/vault_audit.log in JSON format, set up OS logrotate to safely rotate the audit log without losing data, and demonstrate how to verify in the audit log that a tenant's read of their own secret is recorded without exposing secret contents. Include exact Vault commands for enabling the audit, a sample logrotate config snippet, and a sample CLI session that would generate an audit entry?","channel":"vault-associate","subChannel":"general","difficulty":"beginner","tags":["vault-associate"],"companies":["Amazon","Citadel","Twitter"]},{"id":"q-971","question":"In a PCI-compliant SaaS, each tenant uses Vault's database secret engine with a dedicated role to issue per-tenant PostgreSQL credentials. TTLs are short (1h). Describe how you would configure per-tenant roles, handle rotation without downtime, and ensure immediate revocation when a tenant is disabled?","channel":"vault-associate","subChannel":"general","difficulty":"intermediate","tags":["vault-associate"],"companies":["Anthropic","Google","Stripe"]}]