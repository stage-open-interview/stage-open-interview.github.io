{"questions":[{"id":"q-1066","question":"Inside namespace analytics, schedule a daily batch to process a CSV: use a CronJob (02:00 UTC) to start a Job that runs a Python script from a ConfigMap, reads input from a ConfigMap, uses a Secret for DB credentials to insert results into Postgres service, writes output to a PVC, runs as non-root with a readOnlyRootFilesystem, with resource limits, a backoffLimit of 3, and a 15-minute activeDeadlineSeconds; ensure proper probes?","answer":"Configure a CronJob in namespace analytics to launch a Job daily at 02:00 UTC. The Job runs a Python script sourced from a ConfigMap, reads input from another ConfigMap, uses a Secret for Postgres cre","explanation":"## Why This Is Asked\nThis problem tests advanced CKAD skills: combining CronJob and Job for batch work, wiring Script and data through ConfigMaps, securing credentials with Secrets, persisting output via PVCs, and enforcing pod security and lifecycle constraints. It also covers resource sizing, retries, timeouts, and basic health checks in a production-like scenario.\n\n## Key Concepts\n- CronJob, Job, PodSecurityContext\n- ConfigMap for scripts and input data\n- Secret for credentials\n- PersistentVolumeClaim for outputs\n- Probes, activeDeadlineSeconds, backoffLimit\n- runAsNonRoot, readOnlyRootFilesystem, resources\n\n## Code Example\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: daily-ingest\nspec:\n  schedule: '0 2 * * *'\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: ingest\n            image: myrepo/ingest:latest\n            volumeMounts:\n            - mountPath: /scripts\n              name: script\n          volumes:\n          - name: script\n            configMap:\n              name: ingest-scripts\n          restartPolicy: OnFailure\n```\n\n## Follow-up Questions\n- How would you test cron execution and idempotence?\n- How to rotate db credentials without redeploying the Job?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Microsoft","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T21:26:32.694Z","createdAt":"2026-01-12T21:26:32.694Z"},{"id":"q-1312","question":"In a Kubernetes CKAD scenario, you have a Deployment named 'web-server' in namespace 'prod' running a Node.js app. During rolling updates, some pods terminate and restart slowly, causing request timeouts. Outline concrete changes to implement startupProbe, adjust readiness and liveness probes, and add a preStop hook to drain existing connections. Provide a minimal manifest patch showing startupProbe, a readinessProbe, a livenessProbe, and a preStop lifecycle hook that ensures graceful shutdown. How would you approach this?","answer":"Implement startupProbe to delay liveness checks until the app is ready, tune readiness and liveness probes for fast detection and avoid unnecessary restarts, and add a preStop hook to signal graceful ","explanation":"## Why This Is Asked\n\nTests practical lifecycle management and how probes interact with rolling updates to avoid downtime.\n\n## Key Concepts\n\n- StartupProbe, readinessProbe, livenessProbe\n- preStop and terminationGracePeriodSeconds\n- Graceful shutdown patterns in Node.js apps\n\n## Code Example\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-server\n  namespace: prod\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: web-server\n        image: node:18\n        ports:\n        - containerPort: 3000\n        startupProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n          failureThreshold: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n          initialDelaySeconds: 30\n          periodSeconds: 15\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"bash\",\"-lc\",\"sleep 5\"]\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n          limits:\n            cpu: \"500m\"\n            memory: \"512Mi\"\n      terminationGracePeriodSeconds: 30\n```\n\n## Follow-up Questions\n\n- How would you adjust thresholds for busy traffic?\n- How would you verify graceful shutdown under load?\n- What pitfalls exist with preStop sleeps in container runtimes?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","MongoDB","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T10:37:57.488Z","createdAt":"2026-01-13T10:37:57.488Z"},{"id":"q-1352","question":"You have a small Python API app to run in Kubernetes. Write a minimal manifest that creates a Deployment with 3 replicas using image myregistry/api:1.0, a readinessProbe httpGet /health on port 8080, a livenessProbe with initialDelaySeconds 15 and periodSeconds 10, and a ClusterIP Service exposing port 8080. Inject config via ConfigMap app-config with LOG_LEVEL. How would you verify and how would you scale to 5 replicas?","answer":"Create Deployment with 3 replicas using image myregistry/api:1.0, add readinessProbe httpGet /health on 8080 and livenessProbe with initialDelaySeconds 15 and periodSeconds 10. Expose a ClusterIP Serv","explanation":"## Why This Is Asked\n\nThis tests converting a real-world requirement into Kubernetes manifests and basic operational steps, including probes and config injection.\n\n## Key Concepts\n\n- Deployment and ReplicaSet lifecycle\n- Readiness and liveness probes\n- ConfigMap-based configuration\n- Service exposure and ports\n\n## Code Example\n\n```javascript\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: myregistry/api:1.0\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: app-config\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 10\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: api-svc\nspec:\n  type: ClusterIP\n  selector:\n    app: api\n  ports:\n  - port: 8080\n    targetPort: 8080\n```\n\n## Follow-up Questions\n\n- How would you extend configuration with Secrets for sensitive data?\n- How would you monitor readiness/liveness in a multi-container pod?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T13:09:01.337Z","createdAt":"2026-01-13T13:09:01.337Z"},{"id":"q-1393","question":"Design a Kubernetes manifest that deploys a stateless app with 3 replicas, uses a ConfigMap and a Secret, attaches a 1Gi PVC for data, includes a HorizontalPodAutoscaler, exposes via ClusterIP, and enforces a NetworkPolicy restricting egress to api.internal.example.com:443. Provide YAML fragments and discuss trade-offs?","answer":"Deployment: 3 replicas; EnvFrom: ConfigMap and Secret; volume: 1Gi PVC mounted at /data; HPA: min 3, max 10, targetCPUUtilizationPercentage: 60; Service: ClusterIP exposing port 8080; NetworkPolicy: a","explanation":"## Why This Is Asked\nExplores practical CKAD competencies: deployment orchestration, config/secret management, volumes, autoscaling, service exposure, and network isolation.\n\n## Key Concepts\n- Deployment, PVC, ConfigMap/Secret, HPA\n- ClusterIP service semantics\n- NetworkPolicy gating and its limitations\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: app\n        image: my-app:latest\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: my-app-config\n        - secretRef:\n            name: my-app-creds\n        volumeMounts:\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: data-pvc\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-pvc\nspec:\n  accessModes: [\"ReadWriteOnce\"]\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: standard\n---\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: my-app-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: my-app\n  minReplicas: 3\n  maxReplicas: 10\n  targetCPUUtilizationPercentage: 60\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-app\nspec:\n  selector:\n    app: my-app\n  ports:\n  - port: 8080\n    targetPort: 8080\n  type: ClusterIP\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-egress-api\nspec:\n  podSelector:\n    matchLabels:\n      app: my-app\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - ipBlock:\n        cidr: 203.0.113.0/24\n    ports:\n    - protocol: TCP\n      port: 443\n```\n\n## Follow-up Questions\n- How would you test this in a cluster with a restricted CNI?\n- What changes if the app becomes stateful or requires dynamic PVC provisioning?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T14:53:41.455Z","createdAt":"2026-01-13T14:53:41.455Z"},{"id":"q-1459","question":"Create Kubernetes manifests for a simple API: Deployment using image 'my-api:1.0' that reads PORT from a ConfigMap via env, a ConfigMap with PORT and APP_MODE, readinessProbe and livenessProbe for /healthz on that port, and resource requests/limits. Expose with a Service on port 80 targeting 3000. Describe a rolling update plan?","answer":"Provide YAML manifests showing a Deployment that uses image my-api:1.0, reads PORT from a ConfigMap, includes a ConfigMap named api-config with PORT and APP_MODE, readiness and liveness probes for /he","explanation":"## Why This Is Asked\\nPractical CKAD tasks using ConfigMaps, env injection, probes, resources, and Service exposure. Demonstrates understanding of update strategies.\\n\\n## Key Concepts\\n- ConfigMap usage via env/ENV\\n- Probes: readiness and liveness\\n- Resource requests/limits\\n- Service exposure and port mapping\\n- Rolling updates and rollout verification\\n\\n## Code Example\\n\\n```javascript\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: api-config\\ndata:\\n  PORT: 3000\\n  APP_MODE: prod\\n\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: api\\nspec:\\n  replicas: 2\\n  selector:\\n    matchLabels:\\n      app: api\\ntemplate:\\n  metadata:\\n    labels:\\n      app: api\\n  spec:\\n    containers:\\n    - name: api\\n      image: my-api:1.0\\n      ports:\\n      - containerPort: 3000\\n      env:\\n      - name: PORT\\n        valueFrom:\\n          configMapKeyRef:\\n            name: api-config\\n            key: PORT\\n      readinessProbe:\\n        httpGet:\\n          path: /healthz\\n          port: 3000\\n        initialDelaySeconds: 5\\n        periodSeconds: 10\\n      livenessProbe:\\n        httpGet:\\n          path: /healthz\\n          port: 3000\\n        initialDelaySeconds: 15\\n        periodSeconds: 20\\n      resources:\\n        requests:\\n          cpu: 100m\\n          memory: 128Mi\\n        limits:\\n          cpu: 200m\\n          memory: 256Mi\\n\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: api-service\\nspec:\\n  selector:\\n    app: api\\n  ports:\\n  - port: 80\\n    targetPort: 3000\\n```\\n\\n## Follow-up Questions\\n- How would you reload config without pod restart?\\n- How do you verify a rolling update complete with zero downtime?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Meta","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T17:55:04.986Z","createdAt":"2026-01-13T17:55:04.986Z"},{"id":"q-1488","question":"Given a Deployment named 'image-processor' in namespace 'prod' with 6 replicas processing images from a Redis queue, design a practical patch to ensure graceful shutdown of in-flight tasks during rollouts, prevent simultaneous pod terminations, and maintain availability during node drains; include a minimal manifest patch adding a preStop script, terminationGracePeriodSeconds, readiness and liveness probes, and a PodDisruptionBudget targeting the deployment. What steps would you take to validate under load?","answer":"Patch deployment to add lifecycle preStop draining, a 90s terminationGracePeriodSeconds, readinessProbe /ready and livenessProbe /healthz, plus resource requests/limits; create PodDisruptionBudget wit","explanation":"## Why This Is Asked\nTests practical mastery of graceful rollouts, pod eviction safety, and CKAD patterns beyond basics.\n\n## Key Concepts\n- PreStop draining\n- terminationGracePeriodSeconds\n- Probes\n- PodDisruptionBudget\n- Node drains under load\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: image-processor\n  namespace: prod\nspec:\n  replicas: 6\n  template:\n    spec:\n      containers:\n      - name: image-processor\n        image: myimage\n        ports:\n        - containerPort: 8080\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"/bin/sh\",\"-c\",\"/usr/local/bin/drain-tasks.sh\"]\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 2\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n          timeoutSeconds: 2\n      terminationGracePeriodSeconds: 90\n---\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: image-processor-pdb\n  namespace: prod\nspec:\n  minAvailable: 4\n  selector:\n    matchLabels:\n      app: image-processor\n      tier: backend\n```\n\n## Follow-up Questions\n- How would you adapt for multiple queues or different backoff strategies?\n- How would you monitor backlog during drains?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Airbnb","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:00:31.030Z","createdAt":"2026-01-13T19:00:31.030Z"},{"id":"q-1547","question":"Configure a NetworkPolicy to restrict egress from prod/checkout-service to only reach payments/payment-processor in the payments namespace on port 443, blocking all other egress. Provide a minimal manifest patch and a test strategy to validate allowed and blocked traffic inside the cluster?","answer":"Implement a NetworkPolicy in the prod namespace that allows egress from pods labeled app=checkout-service to payments/payment-processor:443 only, while denying all other egress. Apply the minimal YAML patch and validate using a test pod.","explanation":"## Why This Is Asked\n\nThis task evaluates practical NetworkPolicy design, cross-namespace scoping, and cluster service DNS testing under real workloads.\n\n## Key Concepts\n\n- Kubernetes NetworkPolicy\n- namespaceSelector and podSelector\n- DNS resolution for cluster services\n- Egress testing with a dedicated test pod\n- Labeling prerequisites for policy scope\n\n## Code Example\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: checkout-egress-restrict\n  namespace: prod\nspec:\n  podSelector:\n    matchLabels:\n      app: checkout-service\n  policyTypes:\n  - Egress\n  egress:\n```","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:30:52.600Z","createdAt":"2026-01-13T21:36:32.747Z"},{"id":"q-1609","question":"You're deploying inventory-service on Kubernetes. Write YAML to (1) deploy 3 replicas with a ConfigMap for API_ENDPOINT and a Secret for DB_PASSWORD, (2) an InitContainer that runs migrations, (3) readiness and liveness probes, resource requests/limits, and a RollingUpdate strategy with maxUnavailable: 1, (4) a Job to run migrations before the first pod starts, (5) a sidecar log-shipper. Include the key fragments and rationale?","answer":"Provide a compact YAML snippet for a Deployment creating 3 replicas of inventory-service, sourcing API_ENDPOINT from a ConfigMap and DB_PASSWORD from a Secret, with an InitContainer running migrations","explanation":"## Why This Is Asked\n\nTests ability to compose Kubernetes primitives for real-world apps: Deployment, ConfigMap, Secret, InitContainer, Probes, Resources, and Jobs. It also probes rollout safety and migration orchestration.\n\n## Key Concepts\n\n- ConfigMap and Secret integration into Pods\n- InitContainer for pre-start tasks (migrations)\n- Liveness/readiness probes and resource management\n- RollingUpdate strategy with maxUnavailable\n- Separate Job for one-off migrations to ensure DB schema readiness\n\n## Code Example\n\n```javascript\nconst deployment = {\n  apiVersion: 'apps/v1',\n  kind: 'Deployment',\n  metadata: { name: 'inventory-service' },\n  spec: { /* ... */ }\n};\n```\n\n## Follow-up Questions\n\n- How would you implement a canary rollout with traffic splitting?\n- How would you validate migrations and rollback on failure?","diagram":"flowchart TD\n  A[Deployment: inventory-service] --> B[ConfigMap: API_ENDPOINT]\n  A --> C[Secret: DB_PASSWORD]\n  A --> D[InitContainer: migrations]\n  A --> E[Readiness/Liveness probes]\n  A --> F[RollingUpdate: maxUnavailable: 1]\n  G[Job: migrations] --> A","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","NVIDIA","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T02:35:11.694Z","createdAt":"2026-01-14T02:35:11.694Z"},{"id":"q-1626","question":"Blue/Green rollout for a CKAD production web service: in namespace 'prod', a Deployment 'web-app' with 3 replicas serves traffic via the Service 'web-app'. Introduce a canary path with a second Deployment 'web-app-canary' and a canary route (via canary Ingress or a second Service) to validate with 10–20% traffic before full switch. Provide a minimal manifest patch showing resource requests/limits, readinessProbe, and a livenessProbe for both deployments, plus how to switch traffic and rollback. Include validation steps under load?","answer":"Use a blue/green pattern: keep web-app as blue; deploy web-app-canary with 10–20% traffic behind the same Service via a canary route (Ingress canary or a second Service) and implement identical resour","explanation":"## Why This Is Asked\n\nTests practical rollout skills: blue/green pattern, traffic routing, probes, and quick rollback under load.\n\n## Key Concepts\n- Blue/Green deployment\n- Canary routing with minimal impact\n- Resource requests/limits, readinessProbe, livenessProbe\n- Service selector switching for traffic control\n\n## Code Example\n\n```yaml\n# Deployment patch for canary\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app-canary\n  namespace: prod\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n      - name: app\n        image: myorg/web-app:canary\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n          limits:\n            cpu: \"300m\"\n            memory: \"256Mi\"\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n```\n\n```yaml\n# Service patched for blue/green traffic routing\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-app\n  namespace: prod\nspec:\n  selector:\n    app: web-app-blue\n  ports:\n  - port: 80\n    targetPort: 8080\n```\n\n```yaml\n# Canary Service (optional, if using separate service for canary routing)\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-app-canary-svc\n  namespace: prod\nspec:\n  selector:\n    app: web-app-canary\n  ports:\n  - port: 80\n    targetPort: 8080\n```\n\n## Follow-up Questions\n- How would you automate switching traffic and rollback if issues appear?\n- How would you validate stability under a 50k RPS load during the canary phase?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","OpenAI","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:14:20.574Z","createdAt":"2026-01-14T04:14:20.576Z"},{"id":"q-1707","question":"Explain a progressive canary rollout for a 4-replica frontend behind an NGINX Ingress. Use a stable Deployment and a canary Deployment (image frontend:1.2-canary, replicas:1). Patch Ingress to route 10% to canary; later 50% and 100%. Include minimal YAML patches for deployments and a canary Ingress with canary-weight; how would you monitor health and rollback?","answer":"Implement a two-Deployment canary: stable frontend with 4 replicas and canary frontend-canary with 1 replica image frontend:1.2-canary. Create an Ingress canary rule with nginx.ingress.kubernetes.io/c","explanation":"## Why This Is Asked\nTests ability to design a safe, observable canary rollout using standard Kubernetes resources and NGINX Ingress without a service mesh.\n\n## Key Concepts\n- Canary deployment pattern with separate Deployments\n- Ingress annotations (nginx.ingress.kubernetes.io/canary, canary-weight)\n- Progressive traffic shifting and rollback strategy\n- Observability: latency, error rate, saturation\n\n## Code Example\n```yaml\n# Stable Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  namespace: prod\n  labels:\n    app: frontend\n    version: stable\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: frontend\n      version: stable\n  template:\n    metadata:\n      labels:\n        app: frontend\n        version: stable\n    spec:\n      containers:\n      - name: frontend\n        image: frontend:latest\n        ports:\n        - containerPort: 80\n```\n```yaml\n# Canary Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-canary\n  namespace: prod\n  labels:\n    app: frontend\n    version: canary\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: frontend\n      version: canary\n  template:\n    metadata:\n      labels:\n        app: frontend\n        version: canary\n    spec:\n      containers:\n      - name: frontend\n        image: frontend:1.2-canary\n        ports:\n        - containerPort: 80\n```\n```yaml\n# Ingress (production path)\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: frontend-ingress\n  namespace: prod\n  annotations:\n    # enable canary routing for the canary backend\n    nginx.ingress.kubernetes.io/canary: \"true\"\n    nginx.ingress.kubernetes.io/canary-weight: \"10\"\nspec:\n  rules:\n  - host: \"frontend.example.com\"\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: frontend-canary\n            port:\n              number: 80\n```\n\n## Follow-up Questions\n- How would you promote canary to 50% and then 100% safely?\n- What metrics would you monitor to decide progression or rollback?\n- How would you rollback if the canary fails without affecting stable users?","diagram":"flowchart TD\n  A[User Request] --> B[Ingress]\n  B --> C[Stable Backend (frontend)]\n  B --> D[Canary Route (frontend-canary)]\n  D --> E[Canary Pods]\n  C --> F[User Response]\n  E --> F","difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:42:45.036Z","createdAt":"2026-01-14T07:42:45.036Z"},{"id":"q-1767","question":"Scenario: In a CKAD scenario, you must roll out a new image for a stateless API 'inventory-api' in namespace 'prod' with 2% traffic to the canary, using vanilla Kubernetes (no service mesh). Outline a practical canary strategy with two Deployments and two Services, explain how you split traffic without a mesh, and provide minimal manifests for the canary Deployment and a canary-facing Service, plus a rollback plan and how you'd validate under load?","answer":"Two Deployments: stable and canary with distinct labels; two Services: inventory-svc (stable) and inventory-svc-canary (canary). Vanilla Kubernetes lacks built-in weighted routing, so use an Ingress c","explanation":"## Why This Is Asked\n\nTests practical canary workflows in vanilla Kubernetes, a common CKAD constraint.\n\n## Key Concepts\n\n- Canary rollout without service mesh\n- Dual deployments and service selectors\n- Ingress-based weighted routing\n- Rollback and validation under load\n\n## Code Example\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inventory-api-canary\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: inventory-api\n      version: canary\n  template:\n    metadata:\n      labels:\n        app: inventory-api\n        version: canary\n    spec:\n      containers:\n      - name: inventory-api\n        image: repo/inventory-api:canary\n        ports:\n        - containerPort: 80\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 15\n          periodSeconds: 20\n```\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: inventory-svc-canary\nspec:\n  selector:\n    app: inventory-api\n    version: canary\n  ports:\n  - port: 80\n    targetPort: 80\n```\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: inventory-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/canary: \"true\"\n    nginx.ingress.kubernetes.io/canary-weight: \"2\"\nspec:\n  rules:\n  - host: inventory.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: inventory-svc\n            port:\n              number: 80\n```\n\n## Follow-up Questions\n\n- How would you automate canary promotions or rollbacks?\n- What metrics would you monitor to detect canary issues? ","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:51:06.854Z","createdAt":"2026-01-14T09:51:06.854Z"},{"id":"q-1776","question":"Design and implement a CKAD deployment for a 3-replica web app: use a ConfigMap for env vars, define resource requests/limits, add readiness and liveness probes, and configure an HPA with min 3, max 10 and targetCPUUtilizationPercentage 50. Provide the YAML and show verification steps?","answer":"Define a Deployment with 3 replicas, envFrom: configMapRef: name: web-env, resources: requests: cpu: 100m, memory: 128Mi; limits: cpu: 250m, memory: 256Mi; readinessProbe and livenessProbe for /; HPA:","explanation":"## Why This Is Asked\nTests practical CKAD skills: Deployments, ConfigMaps, resources, probes, and autoscaling with verification.\n\n## Key Concepts\n- Deployment, ConfigMap, resources, probes, HPA\n- Rollout verification, kubectl, metrics-server\n- Safe rollout strategies\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: web-app\n    spec:\n      containers:\n      - name: web\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: web-env\n        resources:\n          requests:\n            memory: \"128Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 15\n          periodSeconds: 20\n```\n\n## Follow-up Questions\n- How would you adapt this for a multi-tenant cluster? \n- What changes if the app uses a sidecar log collector?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:39:53.916Z","createdAt":"2026-01-14T10:39:53.916Z"},{"id":"q-1879","question":"You are deploying a Node API behind a Service in Kubernetes with Redis and PostgreSQL. Provide manifests to run API with 4 replicas, Secret for DB creds, ConfigMap for flags, readiness/liveness probes, resource requests/limits, and a RollingUpdate with maxUnavailable=25%, maxSurge=25%. Include how you would test zero-downtime and how HPA would be wired?","answer":"Provide a Deployment for the API with 4 replicas, a Secret for DB credentials, a ConfigMap for feature flags, readinessProbe and livenessProbe, and resource requests/limits. Use a RollingUpdate with m","explanation":"## Why This Is Asked\nThis tests end-to-end Kubernetes app design under CKAD constraints: deployments, secrets, configmaps, probes, resource budgeting, rolling updates, and autoscaling. It emphasizes practical, scalable patterns rather than theory.\n\n## Key Concepts\n- Deployments and rollout strategies\n- Secrets and ConfigMaps\n- Probes and health checks\n- Resources and limits\n- HorizontalPodAutoscaler\n- StatefulSets and PVCs for Redis/PostgreSQL\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: yourrepo/api:latest\n        ports:\n        - containerPort: 3000\n        envFrom:\n        - secretRef:\n            name: db-creds\n        - configMapRef:\n            name: api-flags\n        resources:\n          requests:\n            cpu: \"200m\"\n            memory: \"256Mi\"\n          limits:\n            cpu: \"500m\"\n            memory: \"512Mi\"\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 15\n          periodSeconds: 20\n```\n\n## Follow-up Questions\n- How would you monitor Redis cache misses and cache hits?\n- How would you handle DB credential rotation without downtime?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Discord","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T15:39:49.169Z","createdAt":"2026-01-14T15:39:49.169Z"},{"id":"q-1922","question":"CKAD intermediate: In a prod namespace, a Deployment named 'orders-api' with 3 replicas experiences brief outages during image upgrades. Provide a concrete patch for zero-downtime upgrades: set rollingUpdate strategy (maxUnavailable: 0, maxSurge: 1), add a PreStop hook for graceful shutdown, and create a PodDisruptionBudget to protect at least 2 healthy pods during maintenance. Include minimal Deployment and PDB manifests and describe validation under maintenance-like load?","answer":"Patch should enforce RollingUpdate with maxUnavailable 0 and maxSurge 1, add a PreStop hook that gracefully drains connections, and deploy a PodDisruptionBudget requiring at least 2 available pods. Va","explanation":"## Why This Is Asked\nTests ability to perform zero-downtime upgrades in a CKAD-like scenario with real-world constraints.\n\n## Key Concepts\n- Deployment strategy: RollingUpdate with maxUnavailable and maxSurge\n- Graceful shutdown via PreStop lifecycle hook\n- PodDisruptionBudget to protect minimum availability\n\n## Code Example\n```yaml\n# Deployment patch focusing on zero-downtime upgrade\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: orders-api\n  namespace: prod\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: \"0\"\n      maxSurge: \"1\"\n  template:\n    spec:\n      containers:\n      - name: orders-api\n        image: orders-api:1.2\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"/bin/sh\",\"-c\",\"/app/graceful-stop.sh\"]\n```\n```yaml\n# PodDisruptionBudget to allow maintenance without dropping below 2 pods\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: orders-api-pdb\n  namespace: prod\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: orders-api\n```\n\n## Follow-up Questions\n- How would you test the grace period and ensure no in-flight requests are dropped?\n- How would you adapt this pattern for a canary upgrade strategy?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T17:35:33.832Z","createdAt":"2026-01-14T17:35:33.833Z"},{"id":"q-1959","question":"In a Kubernetes CKAD scenario, design a real-time log-processor that consumes from Kafka, writes results to Cassandra, restarts gracefully on failure, and preserves at-least-once delivery. Provide a concrete deployment design with InitContainer, ConfigMap, Secret, Probes, HPA, DLQ strategy, and a minimal YAML skeleton. Explain trade-offs and monitoring hooks?","answer":"Deploy a Deployment with 3 replicas, resource requests/limits, and readiness/liveness probes. Use an InitContainer to fetch schema, a ConfigMap for topics, a Secret for credentials, and a Kafka consum","explanation":"## Why This Is Asked\nThis question probes practical CKAD skills: building a resilient streaming app, managing config and secrets, and validating delivery semantics. It also tests how to reason about when to use InitContainers, DLQ, and Kubernetes primitives under real workloads.\n\n## Key Concepts\n- Deployment vs StatefulSet\n- InitContainers and init data fetch\n- ConfigMap and Secret usage\n- Kafka offset management and retries\n- Dead Letter Queue strategy\n- Probes, HPA, NetworkPolicy\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: log-processor\nspec:\n  replicas: 3\n  template:\n    spec:\n      initContainers:\n        - name: fetch-schema\n          image: busybox\n          command: [\"sh\",\"-c\",\"echo fetch-schema\"]\n      containers:\n        - name: processor\n          image: myrepo/log-processor:latest\n          envFrom:\n            - configMapRef:\n                name: log-processor-config\n```\n\n## Follow-up Questions\n- How would you implement exactly-once semantics in this pipeline?\n- How would you monitor lag and alert on Kafka consumer delays?\n","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","NVIDIA","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T18:54:45.563Z","createdAt":"2026-01-14T18:54:45.563Z"},{"id":"q-1992","question":"Scenario: A 3-replica Deployment for a Kubernetes CKAD exercise uses a ConfigMap for APP_PORT and LOG_LEVEL and a Secret for DB_PASSWORD. How would you structure manifests to ensure zero-downtime updates, proper health checks, and correct secret/config usage? Provide concrete manifest fragments and a rollout strategy?","answer":"Use a Deployment with rollingUpdate (maxUnavailable: 0, maxSurge: 1) and 3 replicas. Provision a ConfigMap (APP_PORT: 8080, LOG_LEVEL: info) and a Secret (DB_PASSWORD: mypassword) with stringData. In ","explanation":"## Why This Is Asked\n\nEvaluates CKAD fundamentals: ConfigMap/Secret usage, zero-downtime rolling updates, and health checks.\n\n## Key Concepts\n\n- ConfigMap vs Secret usage\n- envFrom vs volumeMounts\n- Readiness/Liveness probes\n- RollingUpdate strategy\n- Resource requests/limits\n\n## Code Example\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  APP_PORT: 8080\n  LOG_LEVEL: info\n\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secrets\nstringData:\n  DB_PASSWORD: mypassword\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: my-registry/api:latest\n        envFrom:\n        - configMapRef:\n            name: app-config\n        - secretRef:\n            name: app-secrets\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n```\n\n## Follow-up Questions\n\n- What would you change to mount the secret as a file instead of env vars?\n- How would you observe rollout status in CI?","diagram":"flowchart TD\n  Deployment --> ConfigMap\n  Deployment --> Secret\n  Deployment --> Probes\n  Probes --> RollingUpdate","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","IBM","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T19:46:46.570Z","createdAt":"2026-01-14T19:46:46.571Z"},{"id":"q-2015","question":"Design a CKAD-grade, multi-tenant API gateway canary rollout: implement two Deployments (stable and canary) for api-gateway, share a Service, and use an Ingress canary annotation to route 20% traffic to canary. Use a ConfigMap flag newFeature to toggle the new code path; store TLS certs in a Secret; include readiness/liveness probes, resource requests/limits, and a minimal YAML skeleton. Explain how you would observe traffic split and rollback?","answer":"Two Deployments (stable and canary) for api-gateway behind one Service; use Ingress canary annotations to route ~20% traffic to canary; a ConfigMap flag newFeature toggles the new code path; a Secret ","explanation":"## Why This Is Asked\nTests practical CKAD skills: multi-resource coordination, canary rollout, and observability.\n\n## Key Concepts\n- Deployments and Services for versioned workloads\n- Ingress canary routing and traffic splits\n- ConfigMaps and Secrets for feature flags and TLS\n- Readiness and Liveness probes\n- Resource requests/limits and RBAC basics\n- Observability and rollback procedures\n\n## Code Example\n```yaml\n# Deployment skeletons for stable and canary\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-stable\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: api\n        version: stable\n    spec:\n      containers:\n        - name: gateway\n          image: myrepo/api-gateway:stable\n          ports:\n            - containerPort: 8080\n          resources:\n            requests:\n              cpu: \"100m\"\n              memory: \"128Mi\"\n            limits:\n              cpu: \"500m\"\n              memory: \"256Mi\"\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 5\n            periodSeconds: 10\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 15\n            periodSeconds: 20\n```\n```yaml\n# Canary Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-canary\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: api\n        version: canary\n    spec:\n      containers:\n        - name: gateway\n          image: myrepo/api-gateway:canary\n          ports:\n            - containerPort: 8080\n          resources:\n            requests:\n              cpu: \"100m\"\n              memory: \"128Mi\"\n            limits:\n              cpu: \"600m\"\n              memory: \"256Mi\"\n```\n```yaml\n# Shared Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: api-service\nspec:\n  selector:\n    app: api\n  ports:\n    - port: 80\n      targetPort: 8080\n```\n```yaml\n# Ingress with canary rule (nginx-ingress)\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: api-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/canary: \"true\"\n    nginx.ingress.kubernetes.io/canary-by-header: \"Canary\"\n    nginx.ingress.kubernetes.io/canary-weight: \"20\"\nspec:\n  rules:\n  - host: api.example.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: api-service\n            port:\n              number: 80\n```\n```yaml\n# ConfigMap: feature flag\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: api-config\ndata:\n  newFeature: \"true\"\n```\n```yaml\n# Secret: TLS\napiVersion: v1\nkind: Secret\ntype: kubernetes.io/tls\nmetadata:\n  name: api-tls\ndata:\n  tls.crt: <base64-cert>\n  tls.key: <base64-key>\n```\n\n## Follow-up Questions\n- How would you verify the traffic split and rollback safely?\n- What metrics would you monitor to detect issues with the canary?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T20:52:30.972Z","createdAt":"2026-01-14T20:52:30.972Z"},{"id":"q-2093","question":"You deploy a Kubernetes Deployment named web-app using image registry.example.com/web-app:v1.2 and expose port 8080. Provide a YAML manifest snippet that adds: livenessProbe for /health on 8080 with initialDelaySeconds: 5 and periodSeconds: 10; readinessProbe for /ready with timeoutSeconds: 3; resources: requests cpu: 250m memory: 256Mi; limits cpu: 500m memory: 512Mi; a ConfigMap mounted at /etc/config providing APP_MODE and an env var APP_MODE sourced from that ConfigMap?","answer":"Configure a Deployment for web-app (image registry.example.com/web-app:v1.2) with 3 replicas on port 8080. Add livenessProbe: httpGet /health on 8080, initialDelaySeconds: 5, periodSeconds: 10; readinessProbe: httpGet /ready on 8080 with timeoutSeconds: 3; resources: requests cpu: 250m memory: 256Mi, limits cpu: 500m memory: 512Mi; ConfigMap mounted at /etc/config providing APP_MODE and env var APP_MODE sourced from that ConfigMap.","explanation":"## Why This Is Asked\nTests CKAD fundamentals in a practical way: probes, resources, and ConfigMap usage. It checks that the candidate can translate requirements into Kubernetes primitives and reason about reliability.\n\n## Key Concepts\n- Liveness vs readiness probes\n- Resource requests and limits\n- ConfigMaps as configuration and env binding\n- Volume mounts and envFrom/KeyRefs\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: web-app\n    spec:\n      containers:\n      - name: web-app\n        image: registry.example.com/web-app:v1.2\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          timeoutSeconds: 3\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n        env:\n        - name: APP_MODE\n          valueFrom:\n            configMapKeyRef:\n              name: web-app-config\n              key: APP_MODE\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: web-app-config\n```","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:13:59.723Z","createdAt":"2026-01-14T23:42:58.629Z"},{"id":"q-2145","question":"CKAD intermediate: In namespace retail, a Deployment 'inventory' (3 replicas) talks to Postgres service 'inventory-db'. You need a one-time seed of lookup data at Pod startup without delaying traffic. Provide a minimal patch that uses an InitContainer to run a seed SQL script against inventory-db with idempotent INSERTs (ON CONFLICT DO NOTHING), and add resource requests/limits, a readiness probe, and a liveness probe. Explain how you'd validate under load and re-seed behavior on restarts?","answer":"Use an InitContainer to run a seed SQL against inventory-db before app pods start, with idempotent INSERTs (ON CONFLICT DO NOTHING). Patch Deployment to mount the seed script from a ConfigMap, set res","explanation":"## Why This Is Asked\nTests InitContainer sequencing, one-time data seeding, ConfigMap usage, and ensuring idempotent seeds and probes work with startup order.\n\n## Key Concepts\n- InitContainers run before app containers\n- Seed data via idempotent SQL (ON CONFLICT DO NOTHING)\n- Mount seed script from ConfigMap\n- Probes and resource requests/limits to keep health\n- Validation under load and restart scenarios\n\n## Code Example\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inventory\n  namespace: retail\nspec:\n  replicas: 3\n  template:\n    spec:\n      initContainers:\n      - name: seed-data\n        image: postgres:15\n        command: [\"psql\", \"-h\", \"inventory-db\", \"-U\", \"seeduser\", \"-d\", \"inventory\"]\n        args: [\"-f\", \"/seed/init.sql\"]\n        volumeMounts:\n        - name: seed-scripts\n          mountPath: /seed\n        env:\n        - name: PGPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: inventory-db-secret\n              key: password\n      containers:\n      - name: app\n        image: inventory-app:latest\n        env:\n        - name: DB_HOST\n          value: \"inventory-db\"\n        - name: DB_USER\n          value: \"seeduser\"\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: inventory-db-secret\n              key: password\n      volumes:\n      - name: seed-scripts\n        configMap:\n          name: inventory-seed-scripts\n```\n\n## Follow-up Questions\n- How would you avoid reseeding when pods are rescheduled and during upgrades?\n- How would you test seed idempotency and monitor seed success during rolling updates?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:25:43.380Z","createdAt":"2026-01-15T04:25:43.380Z"},{"id":"q-858","question":"In a Kubernetes CKAD scenario, you have a Deployment named 'web-app' in namespace 'prod' with 3 replicas; pods frequently OOMKilled under load. Describe a practical debugging plan and provide a minimal manifest patch showing resource requests/limits, a readiness probe, and a liveness probe. Include scaling considerations and how you'd validate the fix under load?","answer":"First, inspect recent pod events and previous logs to confirm OOMKilled, then verify container resources and limits. Set requests/limits (e.g., 500m CPU, 512Mi memory; limit 1Gi). Add a readiness prob","explanation":"## Why This Is Asked\nCKAD candidates must diagnose real issues with limited tools. This tests debugging flow, resource tuning, and readiness/liveness strategies.\n\n## Key Concepts\n- OOMKilled diagnosis through pod events and logs\n- Resource requests/limits tuning and safety margins\n- Probes (readiness and liveness) to recover from bad states\n\n## Code Example\n````yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n  namespace: prod\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: app\n        image: myrepo/web-app:latest\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"1000m\"\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n````\n\n## Follow-up Questions\n- How would you validate changes in a staging environment before production?\n- What trade-offs exist between higher requests/limits and cluster density?","diagram":"flowchart TD\n  A[Pod Events] --> B{OOMKilled?}\n  B -->|Yes| C[Inspect Resources & Probes]\n  B -->|No| D[Monitor under load]\n  C --> E[Adjust requests/limits & Probes]\n  E --> F[Rollout restart]\n  F --> G[Validate under load]","difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","LinkedIn","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:40:43.301Z","createdAt":"2026-01-12T13:40:43.301Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Apple","Coinbase","Discord","DoorDash","Goldman Sachs","Google","Hugging Face","IBM","LinkedIn","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Salesforce","Slack","Snap","Snowflake","Square","Two Sigma","Uber"],"stats":{"total":20,"beginner":5,"intermediate":9,"advanced":6,"newThisWeek":20}}