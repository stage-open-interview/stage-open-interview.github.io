{"questions":[{"id":"q-1066","question":"Inside namespace analytics, schedule a daily batch to process a CSV: use a CronJob (02:00 UTC) to start a Job that runs a Python script from a ConfigMap, reads input from a ConfigMap, uses a Secret for DB credentials to insert results into Postgres service, writes output to a PVC, runs as non-root with a readOnlyRootFilesystem, with resource limits, a backoffLimit of 3, and a 15-minute activeDeadlineSeconds; ensure proper probes?","answer":"Configure a CronJob in namespace analytics to launch a Job daily at 02:00 UTC. The Job runs a Python script sourced from a ConfigMap, reads input from another ConfigMap, uses a Secret for Postgres cre","explanation":"## Why This Is Asked\nThis problem tests advanced CKAD skills: combining CronJob and Job for batch work, wiring Script and data through ConfigMaps, securing credentials with Secrets, persisting output via PVCs, and enforcing pod security and lifecycle constraints. It also covers resource sizing, retries, timeouts, and basic health checks in a production-like scenario.\n\n## Key Concepts\n- CronJob, Job, PodSecurityContext\n- ConfigMap for scripts and input data\n- Secret for credentials\n- PersistentVolumeClaim for outputs\n- Probes, activeDeadlineSeconds, backoffLimit\n- runAsNonRoot, readOnlyRootFilesystem, resources\n\n## Code Example\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: daily-ingest\nspec:\n  schedule: '0 2 * * *'\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: ingest\n            image: myrepo/ingest:latest\n            volumeMounts:\n            - mountPath: /scripts\n              name: script\n          volumes:\n          - name: script\n            configMap:\n              name: ingest-scripts\n          restartPolicy: OnFailure\n```\n\n## Follow-up Questions\n- How would you test cron execution and idempotence?\n- How to rotate db credentials without redeploying the Job?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Microsoft","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T21:26:32.694Z","createdAt":"2026-01-12T21:26:32.694Z"},{"id":"q-1312","question":"In a Kubernetes CKAD scenario, you have a Deployment named 'web-server' in namespace 'prod' running a Node.js app. During rolling updates, some pods terminate and restart slowly, causing request timeouts. Outline concrete changes to implement startupProbe, adjust readiness and liveness probes, and add a preStop hook to drain existing connections. Provide a minimal manifest patch showing startupProbe, a readinessProbe, a livenessProbe, and a preStop lifecycle hook that ensures graceful shutdown. How would you approach this?","answer":"Implement startupProbe to delay liveness checks until the app is ready, tune readiness and liveness probes for fast detection and avoid unnecessary restarts, and add a preStop hook to signal graceful ","explanation":"## Why This Is Asked\n\nTests practical lifecycle management and how probes interact with rolling updates to avoid downtime.\n\n## Key Concepts\n\n- StartupProbe, readinessProbe, livenessProbe\n- preStop and terminationGracePeriodSeconds\n- Graceful shutdown patterns in Node.js apps\n\n## Code Example\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-server\n  namespace: prod\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: web-server\n        image: node:18\n        ports:\n        - containerPort: 3000\n        startupProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n          failureThreshold: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n          initialDelaySeconds: 30\n          periodSeconds: 15\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"bash\",\"-lc\",\"sleep 5\"]\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n          limits:\n            cpu: \"500m\"\n            memory: \"512Mi\"\n      terminationGracePeriodSeconds: 30\n```\n\n## Follow-up Questions\n\n- How would you adjust thresholds for busy traffic?\n- How would you verify graceful shutdown under load?\n- What pitfalls exist with preStop sleeps in container runtimes?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","MongoDB","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T10:37:57.488Z","createdAt":"2026-01-13T10:37:57.488Z"},{"id":"q-1352","question":"You have a small Python API app to run in Kubernetes. Write a minimal manifest that creates a Deployment with 3 replicas using image myregistry/api:1.0, a readinessProbe httpGet /health on port 8080, a livenessProbe with initialDelaySeconds 15 and periodSeconds 10, and a ClusterIP Service exposing port 8080. Inject config via ConfigMap app-config with LOG_LEVEL. How would you verify and how would you scale to 5 replicas?","answer":"Create Deployment with 3 replicas using image myregistry/api:1.0, add readinessProbe httpGet /health on 8080 and livenessProbe with initialDelaySeconds 15 and periodSeconds 10. Expose a ClusterIP Serv","explanation":"## Why This Is Asked\n\nThis tests converting a real-world requirement into Kubernetes manifests and basic operational steps, including probes and config injection.\n\n## Key Concepts\n\n- Deployment and ReplicaSet lifecycle\n- Readiness and liveness probes\n- ConfigMap-based configuration\n- Service exposure and ports\n\n## Code Example\n\n```javascript\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: myregistry/api:1.0\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: app-config\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 10\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: api-svc\nspec:\n  type: ClusterIP\n  selector:\n    app: api\n  ports:\n  - port: 8080\n    targetPort: 8080\n```\n\n## Follow-up Questions\n\n- How would you extend configuration with Secrets for sensitive data?\n- How would you monitor readiness/liveness in a multi-container pod?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T13:09:01.337Z","createdAt":"2026-01-13T13:09:01.337Z"},{"id":"q-1393","question":"Design a Kubernetes manifest that deploys a stateless app with 3 replicas, uses a ConfigMap and a Secret, attaches a 1Gi PVC for data, includes a HorizontalPodAutoscaler, exposes via ClusterIP, and enforces a NetworkPolicy restricting egress to api.internal.example.com:443. Provide YAML fragments and discuss trade-offs?","answer":"Deployment: 3 replicas; EnvFrom: ConfigMap and Secret; volume: 1Gi PVC mounted at /data; HPA: min 3, max 10, targetCPUUtilizationPercentage: 60; Service: ClusterIP exposing port 8080; NetworkPolicy: a","explanation":"## Why This Is Asked\nExplores practical CKAD competencies: deployment orchestration, config/secret management, volumes, autoscaling, service exposure, and network isolation.\n\n## Key Concepts\n- Deployment, PVC, ConfigMap/Secret, HPA\n- ClusterIP service semantics\n- NetworkPolicy gating and its limitations\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: app\n        image: my-app:latest\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: my-app-config\n        - secretRef:\n            name: my-app-creds\n        volumeMounts:\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: data-pvc\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-pvc\nspec:\n  accessModes: [\"ReadWriteOnce\"]\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: standard\n---\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: my-app-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: my-app\n  minReplicas: 3\n  maxReplicas: 10\n  targetCPUUtilizationPercentage: 60\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-app\nspec:\n  selector:\n    app: my-app\n  ports:\n  - port: 8080\n    targetPort: 8080\n  type: ClusterIP\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-egress-api\nspec:\n  podSelector:\n    matchLabels:\n      app: my-app\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - ipBlock:\n        cidr: 203.0.113.0/24\n    ports:\n    - protocol: TCP\n      port: 443\n```\n\n## Follow-up Questions\n- How would you test this in a cluster with a restricted CNI?\n- What changes if the app becomes stateful or requires dynamic PVC provisioning?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T14:53:41.455Z","createdAt":"2026-01-13T14:53:41.455Z"},{"id":"q-1459","question":"Create Kubernetes manifests for a simple API: Deployment using image 'my-api:1.0' that reads PORT from a ConfigMap via env, a ConfigMap with PORT and APP_MODE, readinessProbe and livenessProbe for /healthz on that port, and resource requests/limits. Expose with a Service on port 80 targeting 3000. Describe a rolling update plan?","answer":"Provide YAML manifests showing a Deployment that uses image my-api:1.0, reads PORT from a ConfigMap, includes a ConfigMap named api-config with PORT and APP_MODE, readiness and liveness probes for /he","explanation":"## Why This Is Asked\\nPractical CKAD tasks using ConfigMaps, env injection, probes, resources, and Service exposure. Demonstrates understanding of update strategies.\\n\\n## Key Concepts\\n- ConfigMap usage via env/ENV\\n- Probes: readiness and liveness\\n- Resource requests/limits\\n- Service exposure and port mapping\\n- Rolling updates and rollout verification\\n\\n## Code Example\\n\\n```javascript\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: api-config\\ndata:\\n  PORT: 3000\\n  APP_MODE: prod\\n\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: api\\nspec:\\n  replicas: 2\\n  selector:\\n    matchLabels:\\n      app: api\\ntemplate:\\n  metadata:\\n    labels:\\n      app: api\\n  spec:\\n    containers:\\n    - name: api\\n      image: my-api:1.0\\n      ports:\\n      - containerPort: 3000\\n      env:\\n      - name: PORT\\n        valueFrom:\\n          configMapKeyRef:\\n            name: api-config\\n            key: PORT\\n      readinessProbe:\\n        httpGet:\\n          path: /healthz\\n          port: 3000\\n        initialDelaySeconds: 5\\n        periodSeconds: 10\\n      livenessProbe:\\n        httpGet:\\n          path: /healthz\\n          port: 3000\\n        initialDelaySeconds: 15\\n        periodSeconds: 20\\n      resources:\\n        requests:\\n          cpu: 100m\\n          memory: 128Mi\\n        limits:\\n          cpu: 200m\\n          memory: 256Mi\\n\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: api-service\\nspec:\\n  selector:\\n    app: api\\n  ports:\\n  - port: 80\\n    targetPort: 3000\\n```\\n\\n## Follow-up Questions\\n- How would you reload config without pod restart?\\n- How do you verify a rolling update complete with zero downtime?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Meta","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T17:55:04.986Z","createdAt":"2026-01-13T17:55:04.986Z"},{"id":"q-1488","question":"Given a Deployment named 'image-processor' in namespace 'prod' with 6 replicas processing images from a Redis queue, design a practical patch to ensure graceful shutdown of in-flight tasks during rollouts, prevent simultaneous pod terminations, and maintain availability during node drains; include a minimal manifest patch adding a preStop script, terminationGracePeriodSeconds, readiness and liveness probes, and a PodDisruptionBudget targeting the deployment. What steps would you take to validate under load?","answer":"Patch deployment to add lifecycle preStop draining, a 90s terminationGracePeriodSeconds, readinessProbe /ready and livenessProbe /healthz, plus resource requests/limits; create PodDisruptionBudget wit","explanation":"## Why This Is Asked\nTests practical mastery of graceful rollouts, pod eviction safety, and CKAD patterns beyond basics.\n\n## Key Concepts\n- PreStop draining\n- terminationGracePeriodSeconds\n- Probes\n- PodDisruptionBudget\n- Node drains under load\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: image-processor\n  namespace: prod\nspec:\n  replicas: 6\n  template:\n    spec:\n      containers:\n      - name: image-processor\n        image: myimage\n        ports:\n        - containerPort: 8080\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"/bin/sh\",\"-c\",\"/usr/local/bin/drain-tasks.sh\"]\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 2\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n          timeoutSeconds: 2\n      terminationGracePeriodSeconds: 90\n---\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: image-processor-pdb\n  namespace: prod\nspec:\n  minAvailable: 4\n  selector:\n    matchLabels:\n      app: image-processor\n      tier: backend\n```\n\n## Follow-up Questions\n- How would you adapt for multiple queues or different backoff strategies?\n- How would you monitor backlog during drains?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Airbnb","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:00:31.030Z","createdAt":"2026-01-13T19:00:31.030Z"},{"id":"q-1547","question":"Configure a NetworkPolicy to restrict egress from prod/checkout-service to only reach payments/payment-processor in the payments namespace on port 443, blocking all other egress. Provide a minimal manifest patch and a test strategy to validate allowed and blocked traffic inside the cluster?","answer":"Implement a NetworkPolicy in the prod namespace that allows egress from pods labeled app=checkout-service to payments/payment-processor:443 only, while denying all other egress. Apply the minimal YAML patch and validate using a test pod.","explanation":"## Why This Is Asked\n\nThis task evaluates practical NetworkPolicy design, cross-namespace scoping, and cluster service DNS testing under real workloads.\n\n## Key Concepts\n\n- Kubernetes NetworkPolicy\n- namespaceSelector and podSelector\n- DNS resolution for cluster services\n- Egress testing with a dedicated test pod\n- Labeling prerequisites for policy scope\n\n## Code Example\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: checkout-egress-restrict\n  namespace: prod\nspec:\n  podSelector:\n    matchLabels:\n      app: checkout-service\n  policyTypes:\n  - Egress\n  egress:\n```","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:30:52.600Z","createdAt":"2026-01-13T21:36:32.747Z"},{"id":"q-1609","question":"You're deploying inventory-service on Kubernetes. Write YAML to (1) deploy 3 replicas with a ConfigMap for API_ENDPOINT and a Secret for DB_PASSWORD, (2) an InitContainer that runs migrations, (3) readiness and liveness probes, resource requests/limits, and a RollingUpdate strategy with maxUnavailable: 1, (4) a Job to run migrations before the first pod starts, (5) a sidecar log-shipper. Include the key fragments and rationale?","answer":"Provide a compact YAML snippet for a Deployment creating 3 replicas of inventory-service, sourcing API_ENDPOINT from a ConfigMap and DB_PASSWORD from a Secret, with an InitContainer running migrations","explanation":"## Why This Is Asked\n\nTests ability to compose Kubernetes primitives for real-world apps: Deployment, ConfigMap, Secret, InitContainer, Probes, Resources, and Jobs. It also probes rollout safety and migration orchestration.\n\n## Key Concepts\n\n- ConfigMap and Secret integration into Pods\n- InitContainer for pre-start tasks (migrations)\n- Liveness/readiness probes and resource management\n- RollingUpdate strategy with maxUnavailable\n- Separate Job for one-off migrations to ensure DB schema readiness\n\n## Code Example\n\n```javascript\nconst deployment = {\n  apiVersion: 'apps/v1',\n  kind: 'Deployment',\n  metadata: { name: 'inventory-service' },\n  spec: { /* ... */ }\n};\n```\n\n## Follow-up Questions\n\n- How would you implement a canary rollout with traffic splitting?\n- How would you validate migrations and rollback on failure?","diagram":"flowchart TD\n  A[Deployment: inventory-service] --> B[ConfigMap: API_ENDPOINT]\n  A --> C[Secret: DB_PASSWORD]\n  A --> D[InitContainer: migrations]\n  A --> E[Readiness/Liveness probes]\n  A --> F[RollingUpdate: maxUnavailable: 1]\n  G[Job: migrations] --> A","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","NVIDIA","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T02:35:11.694Z","createdAt":"2026-01-14T02:35:11.694Z"},{"id":"q-1626","question":"Blue/Green rollout for a CKAD production web service: in namespace 'prod', a Deployment 'web-app' with 3 replicas serves traffic via the Service 'web-app'. Introduce a canary path with a second Deployment 'web-app-canary' and a canary route (via canary Ingress or a second Service) to validate with 10–20% traffic before full switch. Provide a minimal manifest patch showing resource requests/limits, readinessProbe, and a livenessProbe for both deployments, plus how to switch traffic and rollback. Include validation steps under load?","answer":"Use a blue/green pattern: keep web-app as blue; deploy web-app-canary with 10–20% traffic behind the same Service via a canary route (Ingress canary or a second Service) and implement identical resour","explanation":"## Why This Is Asked\n\nTests practical rollout skills: blue/green pattern, traffic routing, probes, and quick rollback under load.\n\n## Key Concepts\n- Blue/Green deployment\n- Canary routing with minimal impact\n- Resource requests/limits, readinessProbe, livenessProbe\n- Service selector switching for traffic control\n\n## Code Example\n\n```yaml\n# Deployment patch for canary\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app-canary\n  namespace: prod\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n      - name: app\n        image: myorg/web-app:canary\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n          limits:\n            cpu: \"300m\"\n            memory: \"256Mi\"\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n```\n\n```yaml\n# Service patched for blue/green traffic routing\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-app\n  namespace: prod\nspec:\n  selector:\n    app: web-app-blue\n  ports:\n  - port: 80\n    targetPort: 8080\n```\n\n```yaml\n# Canary Service (optional, if using separate service for canary routing)\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-app-canary-svc\n  namespace: prod\nspec:\n  selector:\n    app: web-app-canary\n  ports:\n  - port: 80\n    targetPort: 8080\n```\n\n## Follow-up Questions\n- How would you automate switching traffic and rollback if issues appear?\n- How would you validate stability under a 50k RPS load during the canary phase?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","OpenAI","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:14:20.574Z","createdAt":"2026-01-14T04:14:20.576Z"},{"id":"q-1707","question":"Explain a progressive canary rollout for a 4-replica frontend behind an NGINX Ingress. Use a stable Deployment and a canary Deployment (image frontend:1.2-canary, replicas:1). Patch Ingress to route 10% to canary; later 50% and 100%. Include minimal YAML patches for deployments and a canary Ingress with canary-weight; how would you monitor health and rollback?","answer":"Implement a two-Deployment canary: stable frontend with 4 replicas and canary frontend-canary with 1 replica image frontend:1.2-canary. Create an Ingress canary rule with nginx.ingress.kubernetes.io/c","explanation":"## Why This Is Asked\nTests ability to design a safe, observable canary rollout using standard Kubernetes resources and NGINX Ingress without a service mesh.\n\n## Key Concepts\n- Canary deployment pattern with separate Deployments\n- Ingress annotations (nginx.ingress.kubernetes.io/canary, canary-weight)\n- Progressive traffic shifting and rollback strategy\n- Observability: latency, error rate, saturation\n\n## Code Example\n```yaml\n# Stable Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  namespace: prod\n  labels:\n    app: frontend\n    version: stable\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: frontend\n      version: stable\n  template:\n    metadata:\n      labels:\n        app: frontend\n        version: stable\n    spec:\n      containers:\n      - name: frontend\n        image: frontend:latest\n        ports:\n        - containerPort: 80\n```\n```yaml\n# Canary Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-canary\n  namespace: prod\n  labels:\n    app: frontend\n    version: canary\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: frontend\n      version: canary\n  template:\n    metadata:\n      labels:\n        app: frontend\n        version: canary\n    spec:\n      containers:\n      - name: frontend\n        image: frontend:1.2-canary\n        ports:\n        - containerPort: 80\n```\n```yaml\n# Ingress (production path)\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: frontend-ingress\n  namespace: prod\n  annotations:\n    # enable canary routing for the canary backend\n    nginx.ingress.kubernetes.io/canary: \"true\"\n    nginx.ingress.kubernetes.io/canary-weight: \"10\"\nspec:\n  rules:\n  - host: \"frontend.example.com\"\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: frontend-canary\n            port:\n              number: 80\n```\n\n## Follow-up Questions\n- How would you promote canary to 50% and then 100% safely?\n- What metrics would you monitor to decide progression or rollback?\n- How would you rollback if the canary fails without affecting stable users?","diagram":"flowchart TD\n  A[User Request] --> B[Ingress]\n  B --> C[Stable Backend (frontend)]\n  B --> D[Canary Route (frontend-canary)]\n  D --> E[Canary Pods]\n  C --> F[User Response]\n  E --> F","difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:42:45.036Z","createdAt":"2026-01-14T07:42:45.036Z"},{"id":"q-1767","question":"Scenario: In a CKAD scenario, you must roll out a new image for a stateless API 'inventory-api' in namespace 'prod' with 2% traffic to the canary, using vanilla Kubernetes (no service mesh). Outline a practical canary strategy with two Deployments and two Services, explain how you split traffic without a mesh, and provide minimal manifests for the canary Deployment and a canary-facing Service, plus a rollback plan and how you'd validate under load?","answer":"Two Deployments: stable and canary with distinct labels; two Services: inventory-svc (stable) and inventory-svc-canary (canary). Vanilla Kubernetes lacks built-in weighted routing, so use an Ingress c","explanation":"## Why This Is Asked\n\nTests practical canary workflows in vanilla Kubernetes, a common CKAD constraint.\n\n## Key Concepts\n\n- Canary rollout without service mesh\n- Dual deployments and service selectors\n- Ingress-based weighted routing\n- Rollback and validation under load\n\n## Code Example\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inventory-api-canary\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: inventory-api\n      version: canary\n  template:\n    metadata:\n      labels:\n        app: inventory-api\n        version: canary\n    spec:\n      containers:\n      - name: inventory-api\n        image: repo/inventory-api:canary\n        ports:\n        - containerPort: 80\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 15\n          periodSeconds: 20\n```\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: inventory-svc-canary\nspec:\n  selector:\n    app: inventory-api\n    version: canary\n  ports:\n  - port: 80\n    targetPort: 80\n```\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: inventory-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/canary: \"true\"\n    nginx.ingress.kubernetes.io/canary-weight: \"2\"\nspec:\n  rules:\n  - host: inventory.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: inventory-svc\n            port:\n              number: 80\n```\n\n## Follow-up Questions\n\n- How would you automate canary promotions or rollbacks?\n- What metrics would you monitor to detect canary issues? ","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:51:06.854Z","createdAt":"2026-01-14T09:51:06.854Z"},{"id":"q-1776","question":"Design and implement a CKAD deployment for a 3-replica web app: use a ConfigMap for env vars, define resource requests/limits, add readiness and liveness probes, and configure an HPA with min 3, max 10 and targetCPUUtilizationPercentage 50. Provide the YAML and show verification steps?","answer":"Define a Deployment with 3 replicas, envFrom: configMapRef: name: web-env, resources: requests: cpu: 100m, memory: 128Mi; limits: cpu: 250m, memory: 256Mi; readinessProbe and livenessProbe for /; HPA:","explanation":"## Why This Is Asked\nTests practical CKAD skills: Deployments, ConfigMaps, resources, probes, and autoscaling with verification.\n\n## Key Concepts\n- Deployment, ConfigMap, resources, probes, HPA\n- Rollout verification, kubectl, metrics-server\n- Safe rollout strategies\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: web-app\n    spec:\n      containers:\n      - name: web\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: web-env\n        resources:\n          requests:\n            memory: \"128Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 15\n          periodSeconds: 20\n```\n\n## Follow-up Questions\n- How would you adapt this for a multi-tenant cluster? \n- What changes if the app uses a sidecar log collector?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:39:53.916Z","createdAt":"2026-01-14T10:39:53.916Z"},{"id":"q-1879","question":"You are deploying a Node API behind a Service in Kubernetes with Redis and PostgreSQL. Provide manifests to run API with 4 replicas, Secret for DB creds, ConfigMap for flags, readiness/liveness probes, resource requests/limits, and a RollingUpdate with maxUnavailable=25%, maxSurge=25%. Include how you would test zero-downtime and how HPA would be wired?","answer":"Provide a Deployment for the API with 4 replicas, a Secret for DB credentials, a ConfigMap for feature flags, readinessProbe and livenessProbe, and resource requests/limits. Use a RollingUpdate with m","explanation":"## Why This Is Asked\nThis tests end-to-end Kubernetes app design under CKAD constraints: deployments, secrets, configmaps, probes, resource budgeting, rolling updates, and autoscaling. It emphasizes practical, scalable patterns rather than theory.\n\n## Key Concepts\n- Deployments and rollout strategies\n- Secrets and ConfigMaps\n- Probes and health checks\n- Resources and limits\n- HorizontalPodAutoscaler\n- StatefulSets and PVCs for Redis/PostgreSQL\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: yourrepo/api:latest\n        ports:\n        - containerPort: 3000\n        envFrom:\n        - secretRef:\n            name: db-creds\n        - configMapRef:\n            name: api-flags\n        resources:\n          requests:\n            cpu: \"200m\"\n            memory: \"256Mi\"\n          limits:\n            cpu: \"500m\"\n            memory: \"512Mi\"\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 15\n          periodSeconds: 20\n```\n\n## Follow-up Questions\n- How would you monitor Redis cache misses and cache hits?\n- How would you handle DB credential rotation without downtime?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Discord","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T15:39:49.169Z","createdAt":"2026-01-14T15:39:49.169Z"},{"id":"q-1922","question":"CKAD intermediate: In a prod namespace, a Deployment named 'orders-api' with 3 replicas experiences brief outages during image upgrades. Provide a concrete patch for zero-downtime upgrades: set rollingUpdate strategy (maxUnavailable: 0, maxSurge: 1), add a PreStop hook for graceful shutdown, and create a PodDisruptionBudget to protect at least 2 healthy pods during maintenance. Include minimal Deployment and PDB manifests and describe validation under maintenance-like load?","answer":"Patch should enforce RollingUpdate with maxUnavailable 0 and maxSurge 1, add a PreStop hook that gracefully drains connections, and deploy a PodDisruptionBudget requiring at least 2 available pods. Va","explanation":"## Why This Is Asked\nTests ability to perform zero-downtime upgrades in a CKAD-like scenario with real-world constraints.\n\n## Key Concepts\n- Deployment strategy: RollingUpdate with maxUnavailable and maxSurge\n- Graceful shutdown via PreStop lifecycle hook\n- PodDisruptionBudget to protect minimum availability\n\n## Code Example\n```yaml\n# Deployment patch focusing on zero-downtime upgrade\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: orders-api\n  namespace: prod\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: \"0\"\n      maxSurge: \"1\"\n  template:\n    spec:\n      containers:\n      - name: orders-api\n        image: orders-api:1.2\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"/bin/sh\",\"-c\",\"/app/graceful-stop.sh\"]\n```\n```yaml\n# PodDisruptionBudget to allow maintenance without dropping below 2 pods\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: orders-api-pdb\n  namespace: prod\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: orders-api\n```\n\n## Follow-up Questions\n- How would you test the grace period and ensure no in-flight requests are dropped?\n- How would you adapt this pattern for a canary upgrade strategy?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T17:35:33.832Z","createdAt":"2026-01-14T17:35:33.833Z"},{"id":"q-1959","question":"In a Kubernetes CKAD scenario, design a real-time log-processor that consumes from Kafka, writes results to Cassandra, restarts gracefully on failure, and preserves at-least-once delivery. Provide a concrete deployment design with InitContainer, ConfigMap, Secret, Probes, HPA, DLQ strategy, and a minimal YAML skeleton. Explain trade-offs and monitoring hooks?","answer":"Deploy a Deployment with 3 replicas, resource requests/limits, and readiness/liveness probes. Use an InitContainer to fetch schema, a ConfigMap for topics, a Secret for credentials, and a Kafka consum","explanation":"## Why This Is Asked\nThis question probes practical CKAD skills: building a resilient streaming app, managing config and secrets, and validating delivery semantics. It also tests how to reason about when to use InitContainers, DLQ, and Kubernetes primitives under real workloads.\n\n## Key Concepts\n- Deployment vs StatefulSet\n- InitContainers and init data fetch\n- ConfigMap and Secret usage\n- Kafka offset management and retries\n- Dead Letter Queue strategy\n- Probes, HPA, NetworkPolicy\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: log-processor\nspec:\n  replicas: 3\n  template:\n    spec:\n      initContainers:\n        - name: fetch-schema\n          image: busybox\n          command: [\"sh\",\"-c\",\"echo fetch-schema\"]\n      containers:\n        - name: processor\n          image: myrepo/log-processor:latest\n          envFrom:\n            - configMapRef:\n                name: log-processor-config\n```\n\n## Follow-up Questions\n- How would you implement exactly-once semantics in this pipeline?\n- How would you monitor lag and alert on Kafka consumer delays?\n","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","NVIDIA","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T18:54:45.563Z","createdAt":"2026-01-14T18:54:45.563Z"},{"id":"q-1992","question":"Scenario: A 3-replica Deployment for a Kubernetes CKAD exercise uses a ConfigMap for APP_PORT and LOG_LEVEL and a Secret for DB_PASSWORD. How would you structure manifests to ensure zero-downtime updates, proper health checks, and correct secret/config usage? Provide concrete manifest fragments and a rollout strategy?","answer":"Use a Deployment with rollingUpdate (maxUnavailable: 0, maxSurge: 1) and 3 replicas. Provision a ConfigMap (APP_PORT: 8080, LOG_LEVEL: info) and a Secret (DB_PASSWORD: mypassword) with stringData. In ","explanation":"## Why This Is Asked\n\nEvaluates CKAD fundamentals: ConfigMap/Secret usage, zero-downtime rolling updates, and health checks.\n\n## Key Concepts\n\n- ConfigMap vs Secret usage\n- envFrom vs volumeMounts\n- Readiness/Liveness probes\n- RollingUpdate strategy\n- Resource requests/limits\n\n## Code Example\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  APP_PORT: 8080\n  LOG_LEVEL: info\n\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secrets\nstringData:\n  DB_PASSWORD: mypassword\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: my-registry/api:latest\n        envFrom:\n        - configMapRef:\n            name: app-config\n        - secretRef:\n            name: app-secrets\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n```\n\n## Follow-up Questions\n\n- What would you change to mount the secret as a file instead of env vars?\n- How would you observe rollout status in CI?","diagram":"flowchart TD\n  Deployment --> ConfigMap\n  Deployment --> Secret\n  Deployment --> Probes\n  Probes --> RollingUpdate","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","IBM","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T19:46:46.570Z","createdAt":"2026-01-14T19:46:46.571Z"},{"id":"q-2015","question":"Design a CKAD-grade, multi-tenant API gateway canary rollout: implement two Deployments (stable and canary) for api-gateway, share a Service, and use an Ingress canary annotation to route 20% traffic to canary. Use a ConfigMap flag newFeature to toggle the new code path; store TLS certs in a Secret; include readiness/liveness probes, resource requests/limits, and a minimal YAML skeleton. Explain how you would observe traffic split and rollback?","answer":"Two Deployments (stable and canary) for api-gateway behind one Service; use Ingress canary annotations to route ~20% traffic to canary; a ConfigMap flag newFeature toggles the new code path; a Secret ","explanation":"## Why This Is Asked\nTests practical CKAD skills: multi-resource coordination, canary rollout, and observability.\n\n## Key Concepts\n- Deployments and Services for versioned workloads\n- Ingress canary routing and traffic splits\n- ConfigMaps and Secrets for feature flags and TLS\n- Readiness and Liveness probes\n- Resource requests/limits and RBAC basics\n- Observability and rollback procedures\n\n## Code Example\n```yaml\n# Deployment skeletons for stable and canary\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-stable\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: api\n        version: stable\n    spec:\n      containers:\n        - name: gateway\n          image: myrepo/api-gateway:stable\n          ports:\n            - containerPort: 8080\n          resources:\n            requests:\n              cpu: \"100m\"\n              memory: \"128Mi\"\n            limits:\n              cpu: \"500m\"\n              memory: \"256Mi\"\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 5\n            periodSeconds: 10\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 15\n            periodSeconds: 20\n```\n```yaml\n# Canary Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-canary\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: api\n        version: canary\n    spec:\n      containers:\n        - name: gateway\n          image: myrepo/api-gateway:canary\n          ports:\n            - containerPort: 8080\n          resources:\n            requests:\n              cpu: \"100m\"\n              memory: \"128Mi\"\n            limits:\n              cpu: \"600m\"\n              memory: \"256Mi\"\n```\n```yaml\n# Shared Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: api-service\nspec:\n  selector:\n    app: api\n  ports:\n    - port: 80\n      targetPort: 8080\n```\n```yaml\n# Ingress with canary rule (nginx-ingress)\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: api-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/canary: \"true\"\n    nginx.ingress.kubernetes.io/canary-by-header: \"Canary\"\n    nginx.ingress.kubernetes.io/canary-weight: \"20\"\nspec:\n  rules:\n  - host: api.example.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: api-service\n            port:\n              number: 80\n```\n```yaml\n# ConfigMap: feature flag\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: api-config\ndata:\n  newFeature: \"true\"\n```\n```yaml\n# Secret: TLS\napiVersion: v1\nkind: Secret\ntype: kubernetes.io/tls\nmetadata:\n  name: api-tls\ndata:\n  tls.crt: <base64-cert>\n  tls.key: <base64-key>\n```\n\n## Follow-up Questions\n- How would you verify the traffic split and rollback safely?\n- What metrics would you monitor to detect issues with the canary?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T20:52:30.972Z","createdAt":"2026-01-14T20:52:30.972Z"},{"id":"q-2093","question":"You deploy a Kubernetes Deployment named web-app using image registry.example.com/web-app:v1.2 and expose port 8080. Provide a YAML manifest snippet that adds: livenessProbe for /health on 8080 with initialDelaySeconds: 5 and periodSeconds: 10; readinessProbe for /ready with timeoutSeconds: 3; resources: requests cpu: 250m memory: 256Mi; limits cpu: 500m memory: 512Mi; a ConfigMap mounted at /etc/config providing APP_MODE and an env var APP_MODE sourced from that ConfigMap?","answer":"Configure a Deployment for web-app (image registry.example.com/web-app:v1.2) with 3 replicas on port 8080. Add livenessProbe: httpGet /health on 8080, initialDelaySeconds: 5, periodSeconds: 10; readinessProbe: httpGet /ready on 8080 with timeoutSeconds: 3; resources: requests cpu: 250m memory: 256Mi, limits cpu: 500m memory: 512Mi; ConfigMap mounted at /etc/config providing APP_MODE and env var APP_MODE sourced from that ConfigMap.","explanation":"## Why This Is Asked\nTests CKAD fundamentals in a practical way: probes, resources, and ConfigMap usage. It checks that the candidate can translate requirements into Kubernetes primitives and reason about reliability.\n\n## Key Concepts\n- Liveness vs readiness probes\n- Resource requests and limits\n- ConfigMaps as configuration and env binding\n- Volume mounts and envFrom/KeyRefs\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: web-app\n    spec:\n      containers:\n      - name: web-app\n        image: registry.example.com/web-app:v1.2\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          timeoutSeconds: 3\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n        env:\n        - name: APP_MODE\n          valueFrom:\n            configMapKeyRef:\n              name: web-app-config\n              key: APP_MODE\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: web-app-config\n```","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:13:59.723Z","createdAt":"2026-01-14T23:42:58.629Z"},{"id":"q-2145","question":"CKAD intermediate: In namespace retail, a Deployment 'inventory' (3 replicas) talks to Postgres service 'inventory-db'. You need a one-time seed of lookup data at Pod startup without delaying traffic. Provide a minimal patch that uses an InitContainer to run a seed SQL script against inventory-db with idempotent INSERTs (ON CONFLICT DO NOTHING), and add resource requests/limits, a readiness probe, and a liveness probe. Explain how you'd validate under load and re-seed behavior on restarts?","answer":"Use an InitContainer to run a seed SQL against inventory-db before app pods start, with idempotent INSERTs (ON CONFLICT DO NOTHING). Patch Deployment to mount the seed script from a ConfigMap, set res","explanation":"## Why This Is Asked\nTests InitContainer sequencing, one-time data seeding, ConfigMap usage, and ensuring idempotent seeds and probes work with startup order.\n\n## Key Concepts\n- InitContainers run before app containers\n- Seed data via idempotent SQL (ON CONFLICT DO NOTHING)\n- Mount seed script from ConfigMap\n- Probes and resource requests/limits to keep health\n- Validation under load and restart scenarios\n\n## Code Example\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inventory\n  namespace: retail\nspec:\n  replicas: 3\n  template:\n    spec:\n      initContainers:\n      - name: seed-data\n        image: postgres:15\n        command: [\"psql\", \"-h\", \"inventory-db\", \"-U\", \"seeduser\", \"-d\", \"inventory\"]\n        args: [\"-f\", \"/seed/init.sql\"]\n        volumeMounts:\n        - name: seed-scripts\n          mountPath: /seed\n        env:\n        - name: PGPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: inventory-db-secret\n              key: password\n      containers:\n      - name: app\n        image: inventory-app:latest\n        env:\n        - name: DB_HOST\n          value: \"inventory-db\"\n        - name: DB_USER\n          value: \"seeduser\"\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: inventory-db-secret\n              key: password\n      volumes:\n      - name: seed-scripts\n        configMap:\n          name: inventory-seed-scripts\n```\n\n## Follow-up Questions\n- How would you avoid reseeding when pods are rescheduled and during upgrades?\n- How would you test seed idempotency and monitor seed success during rolling updates?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:25:43.380Z","createdAt":"2026-01-15T04:25:43.380Z"},{"id":"q-2205","question":"CKAD intermediate: A 3-rep StatefulSet named 'cache' in namespace 'prod' must be upgraded with zero downtime. How would you implement a partitioned, rolling update (one pod at a time) using spec.updateStrategy.RollingUpdate with partition, and set readiness/startup probes plus CPU/memory limits? Also provide a minimal PodDisruptionBudget to keep at least 2 pods healthy during maintenance, and outline validation under load?","answer":"Use a partitioned RollingUpdate: set spec.updateStrategy.rollingUpdate.partition: 2 so upgrades begin with the highest ordinal Pod (2) and proceed one at a time, preserving at least 2 pods. Add readin","explanation":"## Why This Is Asked\nTests partitioned StatefulSet upgrades, PDB, and probes in a real CKAD scenario.\n\n## Key Concepts\n- StatefulSet partitioned RollingUpdate\n- PodDisruptionBudget for minimum availability\n- readinessProbe and startupProbe usage\n- Resource requests/limits in containers\n\n## Code Example\n```javascript\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: cache\n  namespace: prod\nspec:\n  serviceName: \"cache\"\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      partition: 2\n  template:\n    metadata:\n      labels:\n        app: cache\n    spec:\n      containers:\n      - name: cache\n        image: myrepo/cache:2.0\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n          limits:\n            cpu: \"300m\"\n            memory: \"256Mi\"\n        readinessProbe:\n          exec:\n            command: [\"bash\",\"-lc\",\"redis-cli PING\"]\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        startupProbe:\n          exec:\n            command: [\"bash\",\"-lc\",\"true\"]\n          initialDelaySeconds: 10\n          periodSeconds: 30\n```\n\n```javascript\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: cache-pdb\n  namespace: prod\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: cache\n```\n\n## Diagram\nflowchart TD\n  A[Partitioned Upgrade] --> B[Staged Pods Updated]\n\n## Follow-up Questions\n- How would you revert if the upgrade fails?\n- How would you monitor for anomalies during the upgrade?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T07:36:07.318Z","createdAt":"2026-01-15T07:36:07.318Z"},{"id":"q-2237","question":"Design a Kubernetes manifest suite for image registry.example/ckad-demo:1.0 that uses a ConfigMap and a Secret. Include a Deployment with 3 replicas, a Service on port 80, a ConfigMap app-config with APP_MODE=production, and a Secret app-secret with API_KEY=secret123. Mount the ConfigMap at /etc/app and the Secret at /etc/secret, export APP_MODE as an env, include readiness and liveness probes on /healthz, and trigger rolling updates on ConfigMap changes via a checksum/config annotation?","answer":"Provide Kubernetes manifests for a Deployment of registry.example/ckad-demo:1.0 with 3 replicas, a ConfigMap app-config (APP_MODE=production) and a Secret app-secret (API_KEY=secret123). Mount /etc/ap","explanation":"## Why This Is Asked\nTests practical CKAD skills: using ConfigMaps and Secrets in a pod, mounting as volumes, exporting env vars, health checks, and Service exposure. Also checks ability to prompt rollout on CM changes, a common DevOps trap.\n\n## Key Concepts\n- ConfigMap and Secret usage in Pods\n- Volume mounts and envFrom/env\n- Probes: readiness and liveness\n- Rolling updates via annotation\n- Service exposure and port mapping\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ckad-demo\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: app\n        image: registry.example/ckad-demo:1.0\n        env:\n        - name: APP_MODE\n          valueFrom:\n            configMapKeyRef:\n              name: app-config\n              key: APP_MODE\n        volumeMounts:\n        - name: app-config\n          mountPath: /etc/app\n        - name: app-secret\n          mountPath: /etc/secret\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n      volumes:\n      - name: app-config\n        configMap:\n          name: app-config\n      - name: app-secret\n        secret:\n          secretName: app-secret\n```\n\n## Follow-up Questions\n- How would you update the config without downtime in a multi-tenant cluster?\n- What are the security considerations for mounting secrets as files?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Hashicorp","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T08:55:00.789Z","createdAt":"2026-01-15T08:55:00.789Z"},{"id":"q-2276","question":"In a CKAD scenario, namespace 'analytics' hosts Deployment 'data-collector' (5 replicas) behind service 'collector-svc' on port 9000. Implement a NetworkPolicy that (1) allows ingress to data-collector only from pods in the analytics namespace labeled app=ingest on port 9000, (2) denies all other inbound traffic, and (3) allows egress to 10.0.0.50:5140. Provide the YAML manifest and a patch to the Deployment to apply the policy. Describe how you'd validate under simulated load?","answer":"Create a NetworkPolicy in analytics: podSelector: {app: data-collector}, policyTypes: Ingress,Egress. Ingress: from: {podSelector: {app: ingest}}, ports: [{protocol: TCP, port: 9000}]. Egress: to: {ip","explanation":"## Why This Is Asked\nTests practical network isolation using NetworkPolicy, ensuring correct pod labeling and cross-namespace traffic control, plus validation under load.\n\n## Key Concepts\n- NetworkPolicy scoping and podSelector behavior within a namespace\n- Ingress vs Egress rules and port matching\n- Synchronizing Deployment labels with Policy selectors and Service selectors\n- Validation via test pods and kubectl events\n\n## Code Example\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: analytics-data-collector-np\n  namespace: analytics\nspec:\n  podSelector:\n    matchLabels:\n      app: data-collector\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: ingest\n    ports:\n    - protocol: TCP\n      port: 9000\n  egress:\n  - to:\n    - ipBlock:\n        cidr: 10.0.0.50/32\n    ports:\n    - protocol: TCP\n      port: 5140\n```\n\n## Follow-up Questions\n- How would you extend this for DNS-based egress lets and audits?\n- What pitfalls can arise with different CNI implementations when enforcing NetworkPolicy?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Discord","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T10:37:29.665Z","createdAt":"2026-01-15T10:37:29.665Z"},{"id":"q-2306","question":"In CKAD terms, design a per-namespace streaming transformer that consumes Redis Stream 'events', computes a rolling 60s window average latency per user, and writes aggregates to PostgreSQL. Replays failed messages to a Redis DLQ. Use an InitContainer to preload an ML model from a Secret, a ConfigMap for thresholds, readiness/liveness probes, and an HPA based on latency. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"The transformer uses a Deployment per namespace with an InitContainer that loads a model from a Secret into a shared volume. The app subscribes to Redis Stream 'events', computes 60s rolling averages ","explanation":"## Why This Is Asked\n\nTests CKAD-level streaming design using Redis Streams, per-namespace isolation, and observable patterns. Highlights InitContainer model preload, Secrets/ConfigMaps usage, DLQ strategy, and latency-based scaling.\n\n## Key Concepts\n\n- CKAD primitives: ConfigMap, Secret, InitContainer, Probes, HPA\n- Messaging and durability: Redis Streams DLQ\n- Data plane: PostgreSQL aggregates, idempotent writes\n\n## Code Example\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: streaming-transformer\n  namespace: finance\nspec:\n  replicas: 2\n  template:\n    spec:\n      initContainers:\n      - name: preload-model\n        image: busybox\n        command: [\"sh\",\"-c\",\"echo loading model; sleep 1\"]\n        volumeMounts:\n        - name: model\n          mountPath: /models\n      containers:\n      - name: transformer\n        image: registry.example/stream-transformer:latest\n        env:\n        - name: REDIS_STREAM\n          value: events\n        - name: POSTGRES_URL\n          valueFrom:\n            secretKeyRef:\n              name: pg-credentials\n              key: url\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n      volumes:\n      - name: model\n        emptyDir: {}\n```\n\n## Follow-up Questions\n\n- How would you implement idempotent writes to PostgreSQL in this setup?\n- How would you monitor latency and set HPA thresholds?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Hugging Face","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T11:34:24.300Z","createdAt":"2026-01-15T11:34:24.300Z"},{"id":"q-2366","question":"Design a Kubernetes manifest for a beginner CKAD task: deploy a static site with nginx where index.html is served from a ConfigMap named app-content. Mount TLS certs from Secret tls at /etc/tls. Requirements: Deployment with 2 replicas, runAsNonRoot and readOnlyRootFilesystem, readiness/liveness probes on /healthz, INDEX_FILE env from ConfigMap, and an HPA with CPU target 50%. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Create a Deployment for nginx with 2 replicas, runAsNonRoot and readOnlyRootFilesystem. Mount ConfigMap app-content at /usr/share/nginx/html and Secret tls at /etc/tls as files. Expose INDEX_FILE via ","explanation":"## Why This Is Asked\nTests mounting of ConfigMaps and Secrets, securityContext, health checks, and CPU-based scaling in a realistic, small app.\n\n## Key Concepts\n- ConfigMap as files and env\n- Secret mounted as files\n- SecurityContext (runAsNonRoot, readOnlyRootFilesystem)\n- Probes (readiness, liveness)\n- HorizontalPodAutoscaler on CPU\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-static\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx-static\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:stable-alpine\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: content\n          mountPath: /usr/share/nginx/html\n        - name: tls\n          mountPath: /etc/tls\n          readOnly: true\n        env:\n        - name: INDEX_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: app-config\n              key: INDEX_FILE\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 80\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 80\n          initialDelaySeconds: 15\n          periodSeconds: 20\n      volumes:\n      - name: content\n        configMap:\n          name: app-content\n      - name: tls\n        secret:\n          secretName: tls\n```\n\n## Follow-up Questions\n- How would you verify that readOnlyRootFilesystem is enforced?\n- How would you rotate the TLS secret without restarting pods?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T14:55:37.884Z","createdAt":"2026-01-15T14:55:37.884Z"},{"id":"q-2427","question":"Design a CKAD-focused per-namespace file-processing service: it watches a PVC-mounted directory for new text files, processes each file, and submits results to a REST API. Use a ConfigMap for FILE_EXT and BATCH_SIZE, a Secret for API creds, an InitContainer to install deps, and a marker-based idempotence scheme to achieve at-least-once with near-exactly-once semantics. Include readiness/liveness probes and a minimal YAML skeleton; discuss trade-offs?","answer":"An ideal answer would describe a Deployment with a Python worker reading /workspace/files from a per-namespace PVC, an InitContainer for dependencies, a ConfigMap to set FILE_EXT='txt', BATCH_SIZE=100","explanation":"## Why This Is Asked\nThis question probes per-namespace isolation, file-based events, and Kubernetes primitives in CKAD scope. It also tests InitContainers, readiness probes, and a practical idempotence pattern for small-scale data pipelines.\n\n## Key Concepts\n- Kubernetes primitives: Deployment, PVC, ConfigMap, Secret\n- InitContainers and probes\n- Idempotent file processing using marker files\n- In-namespace REST egress with retry/backoff\n- Observability: logs and metrics\n\n## Code Example\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: file-processor-config\ndata:\n  file_ext: \"txt\"\n  batch_size: \"100\"\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: file-processor-creds\ntype: Opaque\ndata:\n  api_token: <base64>\n```\n\n## Follow-up Questions\n- How would you guarantee exactly-once semantics on restart?\n- How would you scale across many namespaces while avoiding cross-namespace leakage?","diagram":"flowchart TD\n  A[Namespace] --> B[Worker Pod]\n  B --> C[PVC: data]\n  B --> D[ConfigMap: options]\n  B --> E[Secret: creds]","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Anthropic","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T17:47:10.074Z","createdAt":"2026-01-15T17:47:10.074Z"},{"id":"q-2453","question":"In a CKAD scenario, design a per-namespace real-time image-resize service that consumes from a Kafka topic, uses an InitContainer to fetch a resize model from a Secret, writes results to PostgreSQL, and preserves at-least-once delivery with idempotent DB writes. Include a ConfigMap for RESIZE_WIDTH/HEIGHT and BATCH_SIZE, readiness/liveness probes, an HPA based on CPU, and a per-namespace NetworkPolicy restricting Kafka ingress and DB egress. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Deploy a per-namespace image-resize processor that subscribes to a Kafka topic, with an InitContainer fetching a resize model from a Secret. Write results to PostgreSQL via an idempotent upsert on (im","explanation":"## Why This Is Asked\nTests practical CKAD mastery: per-namespace isolation, InitContainer usage, secret/config management, and production-grade guarantees.\n\n## Key Concepts\n- InitContainer for model pull from Secret\n- ConfigMap and Secret usage for runtime parameters\n- Probes and HPA for reliability and scale\n- Idempotent DB writes to achieve at-least-once delivery\n- NetworkPolicy to enforce namespace boundaries\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ns-image-resize\n  namespace: <ns>\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: image-resize\n    spec:\n      initContainers:\n        - name: fetch-model\n          image: alpine:3.18\n          command: [\"sh\", \"-c\", \"download-model.sh\"]\n          volumeMounts:\n            - name: model\n              mountPath: /models\n      containers:\n        - name: processor\n          image: registry/ns-image-resize:latest\n          ports:\n            - containerPort: 8080\n          envFrom:\n            - secretRef: { name: resize-secret }\n      volumes:\n        - name: model\n          emptyDir: {}\n      readinessProbe:\n        httpGet:\n          path: /healthz\n          port: 8080\n        initialDelaySeconds: 5\n        periodSeconds: 10\n      livenessProbe:\n        httpGet:\n          path: /healthz\n          port: 8080\n        initialDelaySeconds: 15\n        periodSeconds: 20\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - weight: 100\n              podAffinityTerm:\n                labelSelector:\n                  matchLabels:\n                    app: image-resize\n                topologyKey: \"kubernetes.io/hostname\"\n  ```\n\n## Follow-up Questions\n- How would you monitor idempotency and retry behavior?\n- What changes if Kafka offset management moves to a separate transactional store?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T18:56:11.109Z","createdAt":"2026-01-15T18:56:11.109Z"},{"id":"q-2541","question":"In CKAD terms, design a new, per-namespace data-collector that subscribes to an in-cluster message bus (NATS) and writes results to a time-series store, ensuring at-least-once delivery with a DLQ. Include an InitContainer to install runtime deps, a ConfigMap for BATCH_SIZE, a Secret for creds, a Sidecar for TLS cert rotation, readiness/liveness probes, and a NetworkPolicy restricting egress to the TSDB, a DLQ path on a PVC, and a minimal YAML skeleton. Explain trade-offs?","answer":"Design a per-namespace data-collector that subscribes to NATS, batches events, and writes to InfluxDB with at-least-once delivery and a DLQ. Use an InitContainer to install runtime dependencies, a ConfigMap for BATCH_SIZE configuration, and a Secret for credentials. Include a sidecar for TLS certificate rotation, implement readiness and liveness probes, and restrict egress traffic to the time-series database using a NetworkPolicy. Mount a PVC for the dead-letter queue path and provide a minimal YAML skeleton for deployment.","explanation":"## Why This Is Asked\nThis question tests comprehensive CKAD skills including per-namespace isolation, configuration management with ConfigMaps and Secrets, InitContainers for dependency setup, sidecar patterns for auxiliary services, health probes for reliability, RBAC for security, and NetworkPolicy for traffic control.\n\n## Key Concepts\n- InitContainers for runtime dependency installation\n- ConfigMaps and Secrets for configuration and credential management\n- Sidecar containers for TLS certificate rotation\n- Readiness and liveness probes for health monitoring\n- Namespace isolation and NetworkPolicy for security boundaries\n- Dead-letter queue implementation on PVC storage\n- At-least-once delivery semantics with batch processing\n\n## Code Example\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: ns-collector\n  namespace: target-namespace\nspec:\n  initContainers:\n  - name: install-deps\n    image: alpine:latest\n    command: ['sh', '-c', 'apk add --no-cache nats-cli']\n  containers:\n  - name: collector\n    image: collector:latest\n    envFrom:\n    - configMapRef:\n        name: ns-collector-config\n    - secretRef:\n        name: ns-collector-creds\n    readinessProbe:\n      httpGet:\n        path: /health\n        port: 8080\n    livenessProbe:\n      httpGet:\n        path: /health\n        port: 8080\n    volumeMounts:\n    - name: dlq-pvc\n      mountPath: /dlq\n  - name: tls-sidecar\n    image: tls-sidecar:latest\n    volumeMounts:\n    - name: tls-certs\n      mountPath: /etc/tls\n  volumes:\n  - name: dlq-pvc\n    persistentVolumeClaim:\n      claimName: dlq-pvc\n  - name: tls-certs\n    secret:\n      secretName: tls-certs\n```\n\n## Trade-offs\n- **At-least-once vs exactly-once**: At-least-once provides simpler implementation but may cause duplicate processing\n- **Sidecar overhead**: TLS rotation adds resource consumption but improves security\n- **PVC vs temporary storage**: PVC provides persistence but increases complexity and cost\n- **NetworkPolicy restrictions**: Improve security but may impact debugging and monitoring capabilities","diagram":"flowchart TD\n  NS[Namespace] --> DC[Data Collector Pod]\n  DC --> NB[NATS Subscription]\n  DC --> TS[InfluxDB]\n  DC --> DLQ[DLQ PVC]","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Discord","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T04:25:02.327Z","createdAt":"2026-01-15T21:49:37.961Z"},{"id":"q-2614","question":"CKAD advanced: design a per-namespace data-pipeline controller using a CRD NamespaceDataPipeline. Each instance subscribes to a namespaced NATS subject (metrics.{namespace}) and writes batched data to a TSDB. Guarantee at-least-once via a DLQ on a PVC and idempotent writes. Include InitContainer for deps, a TLS-rotation sidecar, a ConfigMap for BATCH_SIZE, a Secret for creds, readiness/liveness probes, and a NetworkPolicy restricting egress to the TSDB. Provide a minimal YAML skeleton and trade-offs?","answer":"Design a NamespaceDataPipeline Custom Resource Definition that creates per-namespace data pipeline controllers. Each instance subscribes to a namespaced NATS subject (metrics.{namespace}) and writes batched data to a time-series database. Ensure at-least-once delivery through a dead-letter queue on a persistent volume claim with idempotent write semantics.","explanation":"## Why This Is Asked\nTests your ability to design Kubernetes-native data pipelines with CRD-driven control, per-namespace isolation, and robust delivery guarantees.\n\n## Key Concepts\n- Custom Resources and controllers\n- Idempotent writes and dead-letter queue design\n- InitContainers, sidecars, and TLS rotation\n- RBAC, NetworkPolicy, and health probes\n\n## Code Example\n```yaml\n# Minimal skeleton showing CRD and Deployment structure\n```\n\n## Follow-up Questions\n- How would you implement exactly-once semantics in a stateless controller?\n- How would you test CRD upgrades and canary rollouts?","diagram":"flowchart TD\nA[CRD NamespaceDataPipeline] --> B[Controller watches CRs]\nB --> C[NATS client subscribes to metrics.*]\nC --> D[Batcher writes to TSDB]\nD --> E{DLQ PVC}\nE --> F[Retry/Dead-lettering]\n","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Oracle","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:00:43.754Z","createdAt":"2026-01-16T02:44:23.261Z"},{"id":"q-858","question":"In a Kubernetes CKAD scenario, you have a Deployment named 'web-app' in namespace 'prod' with 3 replicas; pods frequently OOMKilled under load. Describe a practical debugging plan and provide a minimal manifest patch showing resource requests/limits, a readiness probe, and a liveness probe. Include scaling considerations and how you'd validate the fix under load?","answer":"First, inspect recent pod events and previous logs to confirm OOMKilled, then verify container resources and limits. Set requests/limits (e.g., 500m CPU, 512Mi memory; limit 1Gi). Add a readiness prob","explanation":"## Why This Is Asked\nCKAD candidates must diagnose real issues with limited tools. This tests debugging flow, resource tuning, and readiness/liveness strategies.\n\n## Key Concepts\n- OOMKilled diagnosis through pod events and logs\n- Resource requests/limits tuning and safety margins\n- Probes (readiness and liveness) to recover from bad states\n\n## Code Example\n````yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n  namespace: prod\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: app\n        image: myrepo/web-app:latest\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"1000m\"\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n````\n\n## Follow-up Questions\n- How would you validate changes in a staging environment before production?\n- What trade-offs exist between higher requests/limits and cluster density?","diagram":"flowchart TD\n  A[Pod Events] --> B{OOMKilled?}\n  B -->|Yes| C[Inspect Resources & Probes]\n  B -->|No| D[Monitor under load]\n  C --> E[Adjust requests/limits & Probes]\n  E --> F[Rollout restart]\n  F --> G[Validate under load]","difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","LinkedIn","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:40:43.301Z","createdAt":"2026-01-12T13:40:43.301Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","LinkedIn","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Robinhood","Salesforce","Slack","Snap","Snowflake","Square","Tesla","Two Sigma","Uber"],"stats":{"total":29,"beginner":7,"intermediate":11,"advanced":11,"newThisWeek":29}}