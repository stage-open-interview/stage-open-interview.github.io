{"questions":[{"id":"q-1066","question":"Inside namespace analytics, schedule a daily batch to process a CSV: use a CronJob (02:00 UTC) to start a Job that runs a Python script from a ConfigMap, reads input from a ConfigMap, uses a Secret for DB credentials to insert results into Postgres service, writes output to a PVC, runs as non-root with a readOnlyRootFilesystem, with resource limits, a backoffLimit of 3, and a 15-minute activeDeadlineSeconds; ensure proper probes?","answer":"Configure a CronJob in namespace analytics to launch a Job daily at 02:00 UTC. The Job runs a Python script sourced from a ConfigMap, reads input from another ConfigMap, uses a Secret for Postgres cre","explanation":"## Why This Is Asked\nThis problem tests advanced CKAD skills: combining CronJob and Job for batch work, wiring Script and data through ConfigMaps, securing credentials with Secrets, persisting output via PVCs, and enforcing pod security and lifecycle constraints. It also covers resource sizing, retries, timeouts, and basic health checks in a production-like scenario.\n\n## Key Concepts\n- CronJob, Job, PodSecurityContext\n- ConfigMap for scripts and input data\n- Secret for credentials\n- PersistentVolumeClaim for outputs\n- Probes, activeDeadlineSeconds, backoffLimit\n- runAsNonRoot, readOnlyRootFilesystem, resources\n\n## Code Example\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: daily-ingest\nspec:\n  schedule: '0 2 * * *'\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: ingest\n            image: myrepo/ingest:latest\n            volumeMounts:\n            - mountPath: /scripts\n              name: script\n          volumes:\n          - name: script\n            configMap:\n              name: ingest-scripts\n          restartPolicy: OnFailure\n```\n\n## Follow-up Questions\n- How would you test cron execution and idempotence?\n- How to rotate db credentials without redeploying the Job?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Microsoft","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:26:32.694Z","createdAt":"2026-01-12T21:26:32.694Z"},{"id":"q-1312","question":"In a Kubernetes CKAD scenario, you have a Deployment named 'web-server' in namespace 'prod' running a Node.js app. During rolling updates, some pods terminate and restart slowly, causing request timeouts. Outline concrete changes to implement startupProbe, adjust readiness and liveness probes, and add a preStop hook to drain existing connections. Provide a minimal manifest patch showing startupProbe, a readinessProbe, a livenessProbe, and a preStop lifecycle hook that ensures graceful shutdown. How would you approach this?","answer":"Implement startupProbe to delay liveness checks until the app is ready, tune readiness and liveness probes for fast detection and avoid unnecessary restarts, and add a preStop hook to signal graceful ","explanation":"## Why This Is Asked\n\nTests practical lifecycle management and how probes interact with rolling updates to avoid downtime.\n\n## Key Concepts\n\n- StartupProbe, readinessProbe, livenessProbe\n- preStop and terminationGracePeriodSeconds\n- Graceful shutdown patterns in Node.js apps\n\n## Code Example\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-server\n  namespace: prod\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: web-server\n        image: node:18\n        ports:\n        - containerPort: 3000\n        startupProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n          failureThreshold: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n          initialDelaySeconds: 30\n          periodSeconds: 15\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"bash\",\"-lc\",\"sleep 5\"]\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n          limits:\n            cpu: \"500m\"\n            memory: \"512Mi\"\n      terminationGracePeriodSeconds: 30\n```\n\n## Follow-up Questions\n\n- How would you adjust thresholds for busy traffic?\n- How would you verify graceful shutdown under load?\n- What pitfalls exist with preStop sleeps in container runtimes?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","MongoDB","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T10:37:57.488Z","createdAt":"2026-01-13T10:37:57.488Z"},{"id":"q-1352","question":"You have a small Python API app to run in Kubernetes. Write a minimal manifest that creates a Deployment with 3 replicas using image myregistry/api:1.0, a readinessProbe httpGet /health on port 8080, a livenessProbe with initialDelaySeconds 15 and periodSeconds 10, and a ClusterIP Service exposing port 8080. Inject config via ConfigMap app-config with LOG_LEVEL. How would you verify and how would you scale to 5 replicas?","answer":"Create Deployment with 3 replicas using image myregistry/api:1.0, add readinessProbe httpGet /health on 8080 and livenessProbe with initialDelaySeconds 15 and periodSeconds 10. Expose a ClusterIP Serv","explanation":"## Why This Is Asked\n\nThis tests converting a real-world requirement into Kubernetes manifests and basic operational steps, including probes and config injection.\n\n## Key Concepts\n\n- Deployment and ReplicaSet lifecycle\n- Readiness and liveness probes\n- ConfigMap-based configuration\n- Service exposure and ports\n\n## Code Example\n\n```javascript\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: myregistry/api:1.0\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: app-config\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 10\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: api-svc\nspec:\n  type: ClusterIP\n  selector:\n    app: api\n  ports:\n  - port: 8080\n    targetPort: 8080\n```\n\n## Follow-up Questions\n\n- How would you extend configuration with Secrets for sensitive data?\n- How would you monitor readiness/liveness in a multi-container pod?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T13:09:01.337Z","createdAt":"2026-01-13T13:09:01.337Z"},{"id":"q-1393","question":"Design a Kubernetes manifest that deploys a stateless app with 3 replicas, uses a ConfigMap and a Secret, attaches a 1Gi PVC for data, includes a HorizontalPodAutoscaler, exposes via ClusterIP, and enforces a NetworkPolicy restricting egress to api.internal.example.com:443. Provide YAML fragments and discuss trade-offs?","answer":"Deployment: 3 replicas; EnvFrom: ConfigMap and Secret; volume: 1Gi PVC mounted at /data; HPA: min 3, max 10, targetCPUUtilizationPercentage: 60; Service: ClusterIP exposing port 8080; NetworkPolicy: a","explanation":"## Why This Is Asked\nExplores practical CKAD competencies: deployment orchestration, config/secret management, volumes, autoscaling, service exposure, and network isolation.\n\n## Key Concepts\n- Deployment, PVC, ConfigMap/Secret, HPA\n- ClusterIP service semantics\n- NetworkPolicy gating and its limitations\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: app\n        image: my-app:latest\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: my-app-config\n        - secretRef:\n            name: my-app-creds\n        volumeMounts:\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: data-pvc\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-pvc\nspec:\n  accessModes: [\"ReadWriteOnce\"]\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: standard\n---\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: my-app-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: my-app\n  minReplicas: 3\n  maxReplicas: 10\n  targetCPUUtilizationPercentage: 60\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-app\nspec:\n  selector:\n    app: my-app\n  ports:\n  - port: 8080\n    targetPort: 8080\n  type: ClusterIP\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-egress-api\nspec:\n  podSelector:\n    matchLabels:\n      app: my-app\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - ipBlock:\n        cidr: 203.0.113.0/24\n    ports:\n    - protocol: TCP\n      port: 443\n```\n\n## Follow-up Questions\n- How would you test this in a cluster with a restricted CNI?\n- What changes if the app becomes stateful or requires dynamic PVC provisioning?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T14:53:41.455Z","createdAt":"2026-01-13T14:53:41.455Z"},{"id":"q-1459","question":"Create Kubernetes manifests for a simple API: Deployment using image 'my-api:1.0' that reads PORT from a ConfigMap via env, a ConfigMap with PORT and APP_MODE, readinessProbe and livenessProbe for /healthz on that port, and resource requests/limits. Expose with a Service on port 80 targeting 3000. Describe a rolling update plan?","answer":"Provide YAML manifests showing a Deployment that uses image my-api:1.0, reads PORT from a ConfigMap, includes a ConfigMap named api-config with PORT and APP_MODE, readiness and liveness probes for /he","explanation":"## Why This Is Asked\\nPractical CKAD tasks using ConfigMaps, env injection, probes, resources, and Service exposure. Demonstrates understanding of update strategies.\\n\\n## Key Concepts\\n- ConfigMap usage via env/ENV\\n- Probes: readiness and liveness\\n- Resource requests/limits\\n- Service exposure and port mapping\\n- Rolling updates and rollout verification\\n\\n## Code Example\\n\\n```javascript\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: api-config\\ndata:\\n  PORT: 3000\\n  APP_MODE: prod\\n\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: api\\nspec:\\n  replicas: 2\\n  selector:\\n    matchLabels:\\n      app: api\\ntemplate:\\n  metadata:\\n    labels:\\n      app: api\\n  spec:\\n    containers:\\n    - name: api\\n      image: my-api:1.0\\n      ports:\\n      - containerPort: 3000\\n      env:\\n      - name: PORT\\n        valueFrom:\\n          configMapKeyRef:\\n            name: api-config\\n            key: PORT\\n      readinessProbe:\\n        httpGet:\\n          path: /healthz\\n          port: 3000\\n        initialDelaySeconds: 5\\n        periodSeconds: 10\\n      livenessProbe:\\n        httpGet:\\n          path: /healthz\\n          port: 3000\\n        initialDelaySeconds: 15\\n        periodSeconds: 20\\n      resources:\\n        requests:\\n          cpu: 100m\\n          memory: 128Mi\\n        limits:\\n          cpu: 200m\\n          memory: 256Mi\\n\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: api-service\\nspec:\\n  selector:\\n    app: api\\n  ports:\\n  - port: 80\\n    targetPort: 3000\\n```\\n\\n## Follow-up Questions\\n- How would you reload config without pod restart?\\n- How do you verify a rolling update complete with zero downtime?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Meta","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T17:55:04.986Z","createdAt":"2026-01-13T17:55:04.986Z"},{"id":"q-1488","question":"Given a Deployment named 'image-processor' in namespace 'prod' with 6 replicas processing images from a Redis queue, design a practical patch to ensure graceful shutdown of in-flight tasks during rollouts, prevent simultaneous pod terminations, and maintain availability during node drains; include a minimal manifest patch adding a preStop script, terminationGracePeriodSeconds, readiness and liveness probes, and a PodDisruptionBudget targeting the deployment. What steps would you take to validate under load?","answer":"Patch deployment to add lifecycle preStop draining, a 90s terminationGracePeriodSeconds, readinessProbe /ready and livenessProbe /healthz, plus resource requests/limits; create PodDisruptionBudget wit","explanation":"## Why This Is Asked\nTests practical mastery of graceful rollouts, pod eviction safety, and CKAD patterns beyond basics.\n\n## Key Concepts\n- PreStop draining\n- terminationGracePeriodSeconds\n- Probes\n- PodDisruptionBudget\n- Node drains under load\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: image-processor\n  namespace: prod\nspec:\n  replicas: 6\n  template:\n    spec:\n      containers:\n      - name: image-processor\n        image: myimage\n        ports:\n        - containerPort: 8080\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"/bin/sh\",\"-c\",\"/usr/local/bin/drain-tasks.sh\"]\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 2\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n          timeoutSeconds: 2\n      terminationGracePeriodSeconds: 90\n---\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: image-processor-pdb\n  namespace: prod\nspec:\n  minAvailable: 4\n  selector:\n    matchLabels:\n      app: image-processor\n      tier: backend\n```\n\n## Follow-up Questions\n- How would you adapt for multiple queues or different backoff strategies?\n- How would you monitor backlog during drains?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Airbnb","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T19:00:31.030Z","createdAt":"2026-01-13T19:00:31.030Z"},{"id":"q-1547","question":"Configure a NetworkPolicy to restrict egress from prod/checkout-service to only reach payments/payment-processor in the payments namespace on port 443, blocking all other egress. Provide a minimal manifest patch and a test strategy to validate allowed and blocked traffic inside the cluster?","answer":"Implement a NetworkPolicy in the prod namespace that allows egress from pods labeled app=checkout-service to payments/payment-processor:443 only, while denying all other egress. Apply the minimal YAML patch and validate using a test pod.","explanation":"## Why This Is Asked\n\nThis task evaluates practical NetworkPolicy design, cross-namespace scoping, and cluster service DNS testing under real workloads.\n\n## Key Concepts\n\n- Kubernetes NetworkPolicy\n- namespaceSelector and podSelector\n- DNS resolution for cluster services\n- Egress testing with a dedicated test pod\n- Labeling prerequisites for policy scope\n\n## Code Example\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: checkout-egress-restrict\n  namespace: prod\nspec:\n  podSelector:\n    matchLabels:\n      app: checkout-service\n  policyTypes:\n  - Egress\n  egress:\n```","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:30:52.600Z","createdAt":"2026-01-13T21:36:32.747Z"},{"id":"q-1609","question":"You're deploying inventory-service on Kubernetes. Write YAML to (1) deploy 3 replicas with a ConfigMap for API_ENDPOINT and a Secret for DB_PASSWORD, (2) an InitContainer that runs migrations, (3) readiness and liveness probes, resource requests/limits, and a RollingUpdate strategy with maxUnavailable: 1, (4) a Job to run migrations before the first pod starts, (5) a sidecar log-shipper. Include the key fragments and rationale?","answer":"Provide a compact YAML snippet for a Deployment creating 3 replicas of inventory-service, sourcing API_ENDPOINT from a ConfigMap and DB_PASSWORD from a Secret, with an InitContainer running migrations","explanation":"## Why This Is Asked\n\nTests ability to compose Kubernetes primitives for real-world apps: Deployment, ConfigMap, Secret, InitContainer, Probes, Resources, and Jobs. It also probes rollout safety and migration orchestration.\n\n## Key Concepts\n\n- ConfigMap and Secret integration into Pods\n- InitContainer for pre-start tasks (migrations)\n- Liveness/readiness probes and resource management\n- RollingUpdate strategy with maxUnavailable\n- Separate Job for one-off migrations to ensure DB schema readiness\n\n## Code Example\n\n```javascript\nconst deployment = {\n  apiVersion: 'apps/v1',\n  kind: 'Deployment',\n  metadata: { name: 'inventory-service' },\n  spec: { /* ... */ }\n};\n```\n\n## Follow-up Questions\n\n- How would you implement a canary rollout with traffic splitting?\n- How would you validate migrations and rollback on failure?","diagram":"flowchart TD\n  A[Deployment: inventory-service] --> B[ConfigMap: API_ENDPOINT]\n  A --> C[Secret: DB_PASSWORD]\n  A --> D[InitContainer: migrations]\n  A --> E[Readiness/Liveness probes]\n  A --> F[RollingUpdate: maxUnavailable: 1]\n  G[Job: migrations] --> A","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","NVIDIA","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T02:35:11.694Z","createdAt":"2026-01-14T02:35:11.694Z"},{"id":"q-1626","question":"Blue/Green rollout for a CKAD production web service: in namespace 'prod', a Deployment 'web-app' with 3 replicas serves traffic via the Service 'web-app'. Introduce a canary path with a second Deployment 'web-app-canary' and a canary route (via canary Ingress or a second Service) to validate with 10–20% traffic before full switch. Provide a minimal manifest patch showing resource requests/limits, readinessProbe, and a livenessProbe for both deployments, plus how to switch traffic and rollback. Include validation steps under load?","answer":"Use a blue/green pattern: keep web-app as blue; deploy web-app-canary with 10–20% traffic behind the same Service via a canary route (Ingress canary or a second Service) and implement identical resour","explanation":"## Why This Is Asked\n\nTests practical rollout skills: blue/green pattern, traffic routing, probes, and quick rollback under load.\n\n## Key Concepts\n- Blue/Green deployment\n- Canary routing with minimal impact\n- Resource requests/limits, readinessProbe, livenessProbe\n- Service selector switching for traffic control\n\n## Code Example\n\n```yaml\n# Deployment patch for canary\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app-canary\n  namespace: prod\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n      - name: app\n        image: myorg/web-app:canary\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n          limits:\n            cpu: \"300m\"\n            memory: \"256Mi\"\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n```\n\n```yaml\n# Service patched for blue/green traffic routing\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-app\n  namespace: prod\nspec:\n  selector:\n    app: web-app-blue\n  ports:\n  - port: 80\n    targetPort: 8080\n```\n\n```yaml\n# Canary Service (optional, if using separate service for canary routing)\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-app-canary-svc\n  namespace: prod\nspec:\n  selector:\n    app: web-app-canary\n  ports:\n  - port: 80\n    targetPort: 8080\n```\n\n## Follow-up Questions\n- How would you automate switching traffic and rollback if issues appear?\n- How would you validate stability under a 50k RPS load during the canary phase?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","OpenAI","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:14:20.574Z","createdAt":"2026-01-14T04:14:20.576Z"},{"id":"q-1707","question":"Explain a progressive canary rollout for a 4-replica frontend behind an NGINX Ingress. Use a stable Deployment and a canary Deployment (image frontend:1.2-canary, replicas:1). Patch Ingress to route 10% to canary; later 50% and 100%. Include minimal YAML patches for deployments and a canary Ingress with canary-weight; how would you monitor health and rollback?","answer":"Implement a two-Deployment canary: stable frontend with 4 replicas and canary frontend-canary with 1 replica image frontend:1.2-canary. Create an Ingress canary rule with nginx.ingress.kubernetes.io/c","explanation":"## Why This Is Asked\nTests ability to design a safe, observable canary rollout using standard Kubernetes resources and NGINX Ingress without a service mesh.\n\n## Key Concepts\n- Canary deployment pattern with separate Deployments\n- Ingress annotations (nginx.ingress.kubernetes.io/canary, canary-weight)\n- Progressive traffic shifting and rollback strategy\n- Observability: latency, error rate, saturation\n\n## Code Example\n```yaml\n# Stable Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  namespace: prod\n  labels:\n    app: frontend\n    version: stable\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: frontend\n      version: stable\n  template:\n    metadata:\n      labels:\n        app: frontend\n        version: stable\n    spec:\n      containers:\n      - name: frontend\n        image: frontend:latest\n        ports:\n        - containerPort: 80\n```\n```yaml\n# Canary Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-canary\n  namespace: prod\n  labels:\n    app: frontend\n    version: canary\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: frontend\n      version: canary\n  template:\n    metadata:\n      labels:\n        app: frontend\n        version: canary\n    spec:\n      containers:\n      - name: frontend\n        image: frontend:1.2-canary\n        ports:\n        - containerPort: 80\n```\n```yaml\n# Ingress (production path)\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: frontend-ingress\n  namespace: prod\n  annotations:\n    # enable canary routing for the canary backend\n    nginx.ingress.kubernetes.io/canary: \"true\"\n    nginx.ingress.kubernetes.io/canary-weight: \"10\"\nspec:\n  rules:\n  - host: \"frontend.example.com\"\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: frontend-canary\n            port:\n              number: 80\n```\n\n## Follow-up Questions\n- How would you promote canary to 50% and then 100% safely?\n- What metrics would you monitor to decide progression or rollback?\n- How would you rollback if the canary fails without affecting stable users?","diagram":"flowchart TD\n  A[User Request] --> B[Ingress]\n  B --> C[Stable Backend (frontend)]\n  B --> D[Canary Route (frontend-canary)]\n  D --> E[Canary Pods]\n  C --> F[User Response]\n  E --> F","difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:42:45.036Z","createdAt":"2026-01-14T07:42:45.036Z"},{"id":"q-1767","question":"Scenario: In a CKAD scenario, you must roll out a new image for a stateless API 'inventory-api' in namespace 'prod' with 2% traffic to the canary, using vanilla Kubernetes (no service mesh). Outline a practical canary strategy with two Deployments and two Services, explain how you split traffic without a mesh, and provide minimal manifests for the canary Deployment and a canary-facing Service, plus a rollback plan and how you'd validate under load?","answer":"Two Deployments: stable and canary with distinct labels; two Services: inventory-svc (stable) and inventory-svc-canary (canary). Vanilla Kubernetes lacks built-in weighted routing, so use an Ingress controller with weighted backend configuration.\n\n## Strategy\n1. Deploy stable version (98% traffic)\n2. Deploy canary version (2% traffic) \n3. Use Ingress annotations for weighted routing\n4. Monitor and validate under load\n5. Rollback by updating Ingress weights\n\n## Core Manifests\n\n```yaml\n# Stable Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inventory-api-stable\n  namespace: prod\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: inventory-api\n      version: stable\n  template:\n    metadata:\n      labels:\n        app: inventory-api\n        version: stable\n    spec:\n      containers:\n      - name: inventory-api\n        image: repo/inventory-api:v1.0\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n\n---\n# Canary Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inventory-api-canary\n  namespace: prod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: inventory-api\n      version: canary\n  template:\n    metadata:\n      labels:\n        app: inventory-api\n        version: canary\n    spec:\n      containers:\n      - name: inventory-api\n        image: repo/inventory-api:canary\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n\n---\n# Stable Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: inventory-svc-stable\n  namespace: prod\nspec:\n  selector:\n    app: inventory-api\n    version: stable\n  ports:\n  - port: 80\n    targetPort: 8080\n\n---\n# Canary Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: inventory-svc-canary\n  namespace: prod\nspec:\n  selector:\n    app: inventory-api\n    version: canary\n  ports:\n  - port: 80\n    targetPort: 8080\n\n---\n# Ingress with Weighted Routing\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: inventory-api-ingress\n  namespace: prod\n  annotations:\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTP\"\n    # NGINX Ingress weighted routing\n    nginx.ingress.kubernetes.io/canary: \"true\"\n    nginx.ingress.kubernetes.io/canary-weight: \"2\"\n    # Alternative: Traefik weighted service\n    # traefik.ingress.kubernetes.io/router.middlewares: \"canary-weight@kubernetescrd\"\nspec:\n  rules:\n  - host: inventory-api.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: inventory-svc-stable\n            port:\n              number: 80\n```\n\n## Rollback Plan\n1. **Immediate rollback**: Update Ingress annotation `nginx.ingress.kubernetes.io/canary-weight: \"0\"`\n2. **Full rollback**: Delete canary Deployment and Service\n3. **Rollback command**: `kubectl patch ingress inventory-api-ingress -n prod -p '{\"metadata\":{\"annotations\":{\"nginx.ingress.kubernetes.io/canary-weight\":\"0\"}}}'`\n\n## Validation Under Load\n```bash\n# Generate load with ApacheBench\nab -n 1000 -c 10 http://inventory-api.example.com/health\n\n# Monitor traffic split\nkubectl logs -n prod -l version=stable --tail=50\nkubectl logs -n prod -l version=canary --tail=50\n\n# Check metrics\nkubectl top pods -n prod --l version=canary\nkubectl get pods -n prod -l version=canary -w\n\n# Validate canary receives ~2% of requests\nfor i in {1..100}; do curl -s http://inventory-api.example.com/health | grep -q \"canary\" && echo \"Canary hit: $i\"; done\n```","explanation":"## Why This Is Asked\n\nTests practical canary workflows in vanilla Kubernetes, a common CKAD constraint requiring understanding of traffic splitting without service mesh.\n\n## Key Concepts\n\n- Canary rollout without service mesh using Ingress controllers\n- Dual deployments with version-specific labels and selectors\n- Ingress-based weighted routing annotations (NGINX/Traefik)\n- Service isolation for stable and canary versions\n- Rollback strategies using Ingress annotation updates\n- Load validation using traffic generation and monitoring\n\n## Implementation Details\n\nThe solution provides complete, production-ready manifests showing:\n1. **Stable deployment** with higher replica count handling 98% traffic\n2. **Canary deployment** with single replica for 2% traffic\n3. **Separate services** enabling selective traffic routing\n4. **Ingress configuration** with weighted routing using NGINX annotations\n5. **Rollback commands** for immediate traffic re-routing\n6. **Load validation** using ab, kubectl monitoring, and hit counting\n\nThis demonstrates practical CKAD-level knowledge of vanilla Kubernetes canary patterns, proper resource management, and operational validation techniques.","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":["canary deployment","weighted routing","ingress controller","service mesh","vanilla kubernetes","rollback plan","traffic splitting","load validation","version labels","readiness probe","replica count","backend configuration"],"voiceSuitable":true,"isNew":true,"lastUpdated":"2026-01-21T05:01:09.780Z","createdAt":"2026-01-14T09:51:06.854Z"},{"id":"q-1776","question":"Design and implement a CKAD deployment for a 3-replica web app: use a ConfigMap for env vars, define resource requests/limits, add readiness and liveness probes, and configure an HPA with min 3, max 10 and targetCPUUtilizationPercentage 50. Provide the YAML and show verification steps?","answer":"Define a Deployment with 3 replicas, envFrom: configMapRef: name: web-env, resources: requests: cpu: 100m, memory: 128Mi; limits: cpu: 250m, memory: 256Mi; readinessProbe and livenessProbe for /; HPA:","explanation":"## Why This Is Asked\nTests practical CKAD skills: Deployments, ConfigMaps, resources, probes, and autoscaling with verification.\n\n## Key Concepts\n- Deployment, ConfigMap, resources, probes, HPA\n- Rollout verification, kubectl, metrics-server\n- Safe rollout strategies\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: web-app\n    spec:\n      containers:\n      - name: web\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: web-env\n        resources:\n          requests:\n            memory: \"128Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 15\n          periodSeconds: 20\n```\n\n## Follow-up Questions\n- How would you adapt this for a multi-tenant cluster? \n- What changes if the app uses a sidecar log collector?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:39:53.916Z","createdAt":"2026-01-14T10:39:53.916Z"},{"id":"q-1879","question":"You are deploying a Node API behind a Service in Kubernetes with Redis and PostgreSQL. Provide manifests to run API with 4 replicas, Secret for DB creds, ConfigMap for flags, readiness/liveness probes, resource requests/limits, and a RollingUpdate with maxUnavailable=25%, maxSurge=25%. Include how you would test zero-downtime and how HPA would be wired?","answer":"Provide a Deployment for the API with 4 replicas, a Secret for DB credentials, a ConfigMap for feature flags, readinessProbe and livenessProbe, and resource requests/limits. Use a RollingUpdate with m","explanation":"## Why This Is Asked\nThis tests end-to-end Kubernetes app design under CKAD constraints: deployments, secrets, configmaps, probes, resource budgeting, rolling updates, and autoscaling. It emphasizes practical, scalable patterns rather than theory.\n\n## Key Concepts\n- Deployments and rollout strategies\n- Secrets and ConfigMaps\n- Probes and health checks\n- Resources and limits\n- HorizontalPodAutoscaler\n- StatefulSets and PVCs for Redis/PostgreSQL\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: yourrepo/api:latest\n        ports:\n        - containerPort: 3000\n        envFrom:\n        - secretRef:\n            name: db-creds\n        - configMapRef:\n            name: api-flags\n        resources:\n          requests:\n            cpu: \"200m\"\n            memory: \"256Mi\"\n          limits:\n            cpu: \"500m\"\n            memory: \"512Mi\"\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 15\n          periodSeconds: 20\n```\n\n## Follow-up Questions\n- How would you monitor Redis cache misses and cache hits?\n- How would you handle DB credential rotation without downtime?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Discord","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T15:39:49.169Z","createdAt":"2026-01-14T15:39:49.169Z"},{"id":"q-1922","question":"CKAD intermediate: In a prod namespace, a Deployment named 'orders-api' with 3 replicas experiences brief outages during image upgrades. Provide a concrete patch for zero-downtime upgrades: set rollingUpdate strategy (maxUnavailable: 0, maxSurge: 1), add a PreStop hook for graceful shutdown, and create a PodDisruptionBudget to protect at least 2 healthy pods during maintenance. Include minimal Deployment and PDB manifests and describe validation under maintenance-like load?","answer":"Patch should enforce RollingUpdate with maxUnavailable 0 and maxSurge 1, add a PreStop hook that gracefully drains connections, and deploy a PodDisruptionBudget requiring at least 2 available pods. Va","explanation":"## Why This Is Asked\nTests ability to perform zero-downtime upgrades in a CKAD-like scenario with real-world constraints.\n\n## Key Concepts\n- Deployment strategy: RollingUpdate with maxUnavailable and maxSurge\n- Graceful shutdown via PreStop lifecycle hook\n- PodDisruptionBudget to protect minimum availability\n\n## Code Example\n```yaml\n# Deployment patch focusing on zero-downtime upgrade\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: orders-api\n  namespace: prod\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: \"0\"\n      maxSurge: \"1\"\n  template:\n    spec:\n      containers:\n      - name: orders-api\n        image: orders-api:1.2\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"/bin/sh\",\"-c\",\"/app/graceful-stop.sh\"]\n```\n```yaml\n# PodDisruptionBudget to allow maintenance without dropping below 2 pods\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: orders-api-pdb\n  namespace: prod\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: orders-api\n```\n\n## Follow-up Questions\n- How would you test the grace period and ensure no in-flight requests are dropped?\n- How would you adapt this pattern for a canary upgrade strategy?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T17:35:33.832Z","createdAt":"2026-01-14T17:35:33.833Z"},{"id":"q-1959","question":"In a Kubernetes CKAD scenario, design a real-time log-processor that consumes from Kafka, writes results to Cassandra, restarts gracefully on failure, and preserves at-least-once delivery. Provide a concrete deployment design with InitContainer, ConfigMap, Secret, Probes, HPA, DLQ strategy, and a minimal YAML skeleton. Explain trade-offs and monitoring hooks?","answer":"Deploy a Deployment with 3 replicas, resource requests/limits, and readiness/liveness probes. Use an InitContainer to fetch schema, a ConfigMap for topics, a Secret for credentials, and a Kafka consum","explanation":"## Why This Is Asked\nThis question probes practical CKAD skills: building a resilient streaming app, managing config and secrets, and validating delivery semantics. It also tests how to reason about when to use InitContainers, DLQ, and Kubernetes primitives under real workloads.\n\n## Key Concepts\n- Deployment vs StatefulSet\n- InitContainers and init data fetch\n- ConfigMap and Secret usage\n- Kafka offset management and retries\n- Dead Letter Queue strategy\n- Probes, HPA, NetworkPolicy\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: log-processor\nspec:\n  replicas: 3\n  template:\n    spec:\n      initContainers:\n        - name: fetch-schema\n          image: busybox\n          command: [\"sh\",\"-c\",\"echo fetch-schema\"]\n      containers:\n        - name: processor\n          image: myrepo/log-processor:latest\n          envFrom:\n            - configMapRef:\n                name: log-processor-config\n```\n\n## Follow-up Questions\n- How would you implement exactly-once semantics in this pipeline?\n- How would you monitor lag and alert on Kafka consumer delays?\n","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","NVIDIA","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T18:54:45.563Z","createdAt":"2026-01-14T18:54:45.563Z"},{"id":"q-1992","question":"Scenario: A 3-replica Deployment for a Kubernetes CKAD exercise uses a ConfigMap for APP_PORT and LOG_LEVEL and a Secret for DB_PASSWORD. How would you structure manifests to ensure zero-downtime updates, proper health checks, and correct secret/config usage? Provide concrete manifest fragments and a rollout strategy?","answer":"Use a Deployment with rollingUpdate (maxUnavailable: 0, maxSurge: 1) and 3 replicas. Provision a ConfigMap (APP_PORT: 8080, LOG_LEVEL: info) and a Secret (DB_PASSWORD: mypassword) with stringData. In ","explanation":"## Why This Is Asked\n\nEvaluates CKAD fundamentals: ConfigMap/Secret usage, zero-downtime rolling updates, and health checks.\n\n## Key Concepts\n\n- ConfigMap vs Secret usage\n- envFrom vs volumeMounts\n- Readiness/Liveness probes\n- RollingUpdate strategy\n- Resource requests/limits\n\n## Code Example\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  APP_PORT: 8080\n  LOG_LEVEL: info\n\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secrets\nstringData:\n  DB_PASSWORD: mypassword\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: my-registry/api:latest\n        envFrom:\n        - configMapRef:\n            name: app-config\n        - secretRef:\n            name: app-secrets\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n```\n\n## Follow-up Questions\n\n- What would you change to mount the secret as a file instead of env vars?\n- How would you observe rollout status in CI?","diagram":"flowchart TD\n  Deployment --> ConfigMap\n  Deployment --> Secret\n  Deployment --> Probes\n  Probes --> RollingUpdate","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","IBM","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T19:46:46.570Z","createdAt":"2026-01-14T19:46:46.571Z"},{"id":"q-2015","question":"Design a CKAD-grade, multi-tenant API gateway canary rollout: implement two Deployments (stable and canary) for api-gateway, share a Service, and use an Ingress canary annotation to route 20% traffic to canary. Use a ConfigMap flag newFeature to toggle the new code path; store TLS certs in a Secret; include readiness/liveness probes, resource requests/limits, and a minimal YAML skeleton. Explain how you would observe traffic split and rollback?","answer":"Two Deployments (stable and canary) for api-gateway behind one Service; use Ingress canary annotations to route ~20% traffic to canary; a ConfigMap flag newFeature toggles the new code path; a Secret ","explanation":"## Why This Is Asked\nTests practical CKAD skills: multi-resource coordination, canary rollout, and observability.\n\n## Key Concepts\n- Deployments and Services for versioned workloads\n- Ingress canary routing and traffic splits\n- ConfigMaps and Secrets for feature flags and TLS\n- Readiness and Liveness probes\n- Resource requests/limits and RBAC basics\n- Observability and rollback procedures\n\n## Code Example\n```yaml\n# Deployment skeletons for stable and canary\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-stable\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: api\n        version: stable\n    spec:\n      containers:\n        - name: gateway\n          image: myrepo/api-gateway:stable\n          ports:\n            - containerPort: 8080\n          resources:\n            requests:\n              cpu: \"100m\"\n              memory: \"128Mi\"\n            limits:\n              cpu: \"500m\"\n              memory: \"256Mi\"\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 5\n            periodSeconds: 10\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 15\n            periodSeconds: 20\n```\n```yaml\n# Canary Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-canary\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: api\n        version: canary\n    spec:\n      containers:\n        - name: gateway\n          image: myrepo/api-gateway:canary\n          ports:\n            - containerPort: 8080\n          resources:\n            requests:\n              cpu: \"100m\"\n              memory: \"128Mi\"\n            limits:\n              cpu: \"600m\"\n              memory: \"256Mi\"\n```\n```yaml\n# Shared Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: api-service\nspec:\n  selector:\n    app: api\n  ports:\n    - port: 80\n      targetPort: 8080\n```\n```yaml\n# Ingress with canary rule (nginx-ingress)\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: api-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/canary: \"true\"\n    nginx.ingress.kubernetes.io/canary-by-header: \"Canary\"\n    nginx.ingress.kubernetes.io/canary-weight: \"20\"\nspec:\n  rules:\n  - host: api.example.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: api-service\n            port:\n              number: 80\n```\n```yaml\n# ConfigMap: feature flag\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: api-config\ndata:\n  newFeature: \"true\"\n```\n```yaml\n# Secret: TLS\napiVersion: v1\nkind: Secret\ntype: kubernetes.io/tls\nmetadata:\n  name: api-tls\ndata:\n  tls.crt: <base64-cert>\n  tls.key: <base64-key>\n```\n\n## Follow-up Questions\n- How would you verify the traffic split and rollback safely?\n- What metrics would you monitor to detect issues with the canary?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T20:52:30.972Z","createdAt":"2026-01-14T20:52:30.972Z"},{"id":"q-2093","question":"You deploy a Kubernetes Deployment named web-app using image registry.example.com/web-app:v1.2 and expose port 8080. Provide a YAML manifest snippet that adds: livenessProbe for /health on 8080 with initialDelaySeconds: 5 and periodSeconds: 10; readinessProbe for /ready with timeoutSeconds: 3; resources: requests cpu: 250m memory: 256Mi; limits cpu: 500m memory: 512Mi; a ConfigMap mounted at /etc/config providing APP_MODE and an env var APP_MODE sourced from that ConfigMap?","answer":"Configure a Deployment for web-app (image registry.example.com/web-app:v1.2) with 3 replicas on port 8080. Add livenessProbe: httpGet /health on 8080, initialDelaySeconds: 5, periodSeconds: 10; readinessProbe: httpGet /ready on 8080 with timeoutSeconds: 3; resources: requests cpu: 250m memory: 256Mi, limits cpu: 500m memory: 512Mi; ConfigMap mounted at /etc/config providing APP_MODE and env var APP_MODE sourced from that ConfigMap.","explanation":"## Why This Is Asked\nTests CKAD fundamentals in a practical way: probes, resources, and ConfigMap usage. It checks that the candidate can translate requirements into Kubernetes primitives and reason about reliability.\n\n## Key Concepts\n- Liveness vs readiness probes\n- Resource requests and limits\n- ConfigMaps as configuration and env binding\n- Volume mounts and envFrom/KeyRefs\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: web-app\n    spec:\n      containers:\n      - name: web-app\n        image: registry.example.com/web-app:v1.2\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          timeoutSeconds: 3\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n        env:\n        - name: APP_MODE\n          valueFrom:\n            configMapKeyRef:\n              name: web-app-config\n              key: APP_MODE\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: web-app-config\n```","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:13:59.723Z","createdAt":"2026-01-14T23:42:58.629Z"},{"id":"q-2145","question":"CKAD intermediate: In namespace retail, a Deployment 'inventory' (3 replicas) talks to Postgres service 'inventory-db'. You need a one-time seed of lookup data at Pod startup without delaying traffic. Provide a minimal patch that uses an InitContainer to run a seed SQL script against inventory-db with idempotent INSERTs (ON CONFLICT DO NOTHING), and add resource requests/limits, a readiness probe, and a liveness probe. Explain how you'd validate under load and re-seed behavior on restarts?","answer":"Use an InitContainer to run a seed SQL against inventory-db before app pods start, with idempotent INSERTs (ON CONFLICT DO NOTHING). Patch Deployment to mount the seed script from a ConfigMap, set res","explanation":"## Why This Is Asked\nTests InitContainer sequencing, one-time data seeding, ConfigMap usage, and ensuring idempotent seeds and probes work with startup order.\n\n## Key Concepts\n- InitContainers run before app containers\n- Seed data via idempotent SQL (ON CONFLICT DO NOTHING)\n- Mount seed script from ConfigMap\n- Probes and resource requests/limits to keep health\n- Validation under load and restart scenarios\n\n## Code Example\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inventory\n  namespace: retail\nspec:\n  replicas: 3\n  template:\n    spec:\n      initContainers:\n      - name: seed-data\n        image: postgres:15\n        command: [\"psql\", \"-h\", \"inventory-db\", \"-U\", \"seeduser\", \"-d\", \"inventory\"]\n        args: [\"-f\", \"/seed/init.sql\"]\n        volumeMounts:\n        - name: seed-scripts\n          mountPath: /seed\n        env:\n        - name: PGPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: inventory-db-secret\n              key: password\n      containers:\n      - name: app\n        image: inventory-app:latest\n        env:\n        - name: DB_HOST\n          value: \"inventory-db\"\n        - name: DB_USER\n          value: \"seeduser\"\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: inventory-db-secret\n              key: password\n      volumes:\n      - name: seed-scripts\n        configMap:\n          name: inventory-seed-scripts\n```\n\n## Follow-up Questions\n- How would you avoid reseeding when pods are rescheduled and during upgrades?\n- How would you test seed idempotency and monitor seed success during rolling updates?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:25:43.380Z","createdAt":"2026-01-15T04:25:43.380Z"},{"id":"q-2205","question":"CKAD intermediate: A 3-rep StatefulSet named 'cache' in namespace 'prod' must be upgraded with zero downtime. How would you implement a partitioned, rolling update (one pod at a time) using spec.updateStrategy.RollingUpdate with partition, and set readiness/startup probes plus CPU/memory limits? Also provide a minimal PodDisruptionBudget to keep at least 2 pods healthy during maintenance, and outline validation under load?","answer":"Use a partitioned RollingUpdate: set spec.updateStrategy.rollingUpdate.partition: 2 so upgrades begin with the highest ordinal Pod (2) and proceed one at a time, preserving at least 2 pods. Add readin","explanation":"## Why This Is Asked\nTests partitioned StatefulSet upgrades, PDB, and probes in a real CKAD scenario.\n\n## Key Concepts\n- StatefulSet partitioned RollingUpdate\n- PodDisruptionBudget for minimum availability\n- readinessProbe and startupProbe usage\n- Resource requests/limits in containers\n\n## Code Example\n```javascript\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: cache\n  namespace: prod\nspec:\n  serviceName: \"cache\"\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      partition: 2\n  template:\n    metadata:\n      labels:\n        app: cache\n    spec:\n      containers:\n      - name: cache\n        image: myrepo/cache:2.0\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n          limits:\n            cpu: \"300m\"\n            memory: \"256Mi\"\n        readinessProbe:\n          exec:\n            command: [\"bash\",\"-lc\",\"redis-cli PING\"]\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        startupProbe:\n          exec:\n            command: [\"bash\",\"-lc\",\"true\"]\n          initialDelaySeconds: 10\n          periodSeconds: 30\n```\n\n```javascript\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: cache-pdb\n  namespace: prod\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: cache\n```\n\n## Diagram\nflowchart TD\n  A[Partitioned Upgrade] --> B[Staged Pods Updated]\n\n## Follow-up Questions\n- How would you revert if the upgrade fails?\n- How would you monitor for anomalies during the upgrade?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T07:36:07.318Z","createdAt":"2026-01-15T07:36:07.318Z"},{"id":"q-2237","question":"Design a Kubernetes manifest suite for image registry.example/ckad-demo:1.0 that uses a ConfigMap and a Secret. Include a Deployment with 3 replicas, a Service on port 80, a ConfigMap app-config with APP_MODE=production, and a Secret app-secret with API_KEY=secret123. Mount the ConfigMap at /etc/app and the Secret at /etc/secret, export APP_MODE as an env, include readiness and liveness probes on /healthz, and trigger rolling updates on ConfigMap changes via a checksum/config annotation?","answer":"Provide Kubernetes manifests for a Deployment of registry.example/ckad-demo:1.0 with 3 replicas, a ConfigMap app-config (APP_MODE=production) and a Secret app-secret (API_KEY=secret123). Mount /etc/ap","explanation":"## Why This Is Asked\nTests practical CKAD skills: using ConfigMaps and Secrets in a pod, mounting as volumes, exporting env vars, health checks, and Service exposure. Also checks ability to prompt rollout on CM changes, a common DevOps trap.\n\n## Key Concepts\n- ConfigMap and Secret usage in Pods\n- Volume mounts and envFrom/env\n- Probes: readiness and liveness\n- Rolling updates via annotation\n- Service exposure and port mapping\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ckad-demo\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: app\n        image: registry.example/ckad-demo:1.0\n        env:\n        - name: APP_MODE\n          valueFrom:\n            configMapKeyRef:\n              name: app-config\n              key: APP_MODE\n        volumeMounts:\n        - name: app-config\n          mountPath: /etc/app\n        - name: app-secret\n          mountPath: /etc/secret\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n      volumes:\n      - name: app-config\n        configMap:\n          name: app-config\n      - name: app-secret\n        secret:\n          secretName: app-secret\n```\n\n## Follow-up Questions\n- How would you update the config without downtime in a multi-tenant cluster?\n- What are the security considerations for mounting secrets as files?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Hashicorp","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T08:55:00.789Z","createdAt":"2026-01-15T08:55:00.789Z"},{"id":"q-2276","question":"In a CKAD scenario, namespace 'analytics' hosts Deployment 'data-collector' (5 replicas) behind service 'collector-svc' on port 9000. Implement a NetworkPolicy that (1) allows ingress to data-collector only from pods in the analytics namespace labeled app=ingest on port 9000, (2) denies all other inbound traffic, and (3) allows egress to 10.0.0.50:5140. Provide the YAML manifest and a patch to the Deployment to apply the policy. Describe how you'd validate under simulated load?","answer":"Create a NetworkPolicy in analytics: podSelector: {app: data-collector}, policyTypes: Ingress,Egress. Ingress: from: {podSelector: {app: ingest}}, ports: [{protocol: TCP, port: 9000}]. Egress: to: {ip","explanation":"## Why This Is Asked\nTests practical network isolation using NetworkPolicy, ensuring correct pod labeling and cross-namespace traffic control, plus validation under load.\n\n## Key Concepts\n- NetworkPolicy scoping and podSelector behavior within a namespace\n- Ingress vs Egress rules and port matching\n- Synchronizing Deployment labels with Policy selectors and Service selectors\n- Validation via test pods and kubectl events\n\n## Code Example\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: analytics-data-collector-np\n  namespace: analytics\nspec:\n  podSelector:\n    matchLabels:\n      app: data-collector\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: ingest\n    ports:\n    - protocol: TCP\n      port: 9000\n  egress:\n  - to:\n    - ipBlock:\n        cidr: 10.0.0.50/32\n    ports:\n    - protocol: TCP\n      port: 5140\n```\n\n## Follow-up Questions\n- How would you extend this for DNS-based egress lets and audits?\n- What pitfalls can arise with different CNI implementations when enforcing NetworkPolicy?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Discord","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T10:37:29.665Z","createdAt":"2026-01-15T10:37:29.665Z"},{"id":"q-2306","question":"In CKAD terms, design a per-namespace streaming transformer that consumes Redis Stream 'events', computes a rolling 60s window average latency per user, and writes aggregates to PostgreSQL. Replays failed messages to a Redis DLQ. Use an InitContainer to preload an ML model from a Secret, a ConfigMap for thresholds, readiness/liveness probes, and an HPA based on latency. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"The transformer uses a Deployment per namespace with an InitContainer that loads a model from a Secret into a shared volume. The app subscribes to Redis Stream 'events', computes 60s rolling averages ","explanation":"## Why This Is Asked\n\nTests CKAD-level streaming design using Redis Streams, per-namespace isolation, and observable patterns. Highlights InitContainer model preload, Secrets/ConfigMaps usage, DLQ strategy, and latency-based scaling.\n\n## Key Concepts\n\n- CKAD primitives: ConfigMap, Secret, InitContainer, Probes, HPA\n- Messaging and durability: Redis Streams DLQ\n- Data plane: PostgreSQL aggregates, idempotent writes\n\n## Code Example\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: streaming-transformer\n  namespace: finance\nspec:\n  replicas: 2\n  template:\n    spec:\n      initContainers:\n      - name: preload-model\n        image: busybox\n        command: [\"sh\",\"-c\",\"echo loading model; sleep 1\"]\n        volumeMounts:\n        - name: model\n          mountPath: /models\n      containers:\n      - name: transformer\n        image: registry.example/stream-transformer:latest\n        env:\n        - name: REDIS_STREAM\n          value: events\n        - name: POSTGRES_URL\n          valueFrom:\n            secretKeyRef:\n              name: pg-credentials\n              key: url\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n      volumes:\n      - name: model\n        emptyDir: {}\n```\n\n## Follow-up Questions\n\n- How would you implement idempotent writes to PostgreSQL in this setup?\n- How would you monitor latency and set HPA thresholds?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Hugging Face","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T11:34:24.300Z","createdAt":"2026-01-15T11:34:24.300Z"},{"id":"q-2366","question":"Design a Kubernetes manifest for a beginner CKAD task: deploy a static site with nginx where index.html is served from a ConfigMap named app-content. Mount TLS certs from Secret tls at /etc/tls. Requirements: Deployment with 2 replicas, runAsNonRoot and readOnlyRootFilesystem, readiness/liveness probes on /healthz, INDEX_FILE env from ConfigMap, and an HPA with CPU target 50%. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Create a Deployment for nginx with 2 replicas, runAsNonRoot and readOnlyRootFilesystem. Mount ConfigMap app-content at /usr/share/nginx/html and Secret tls at /etc/tls as files. Expose INDEX_FILE via ","explanation":"## Why This Is Asked\nTests mounting of ConfigMaps and Secrets, securityContext, health checks, and CPU-based scaling in a realistic, small app.\n\n## Key Concepts\n- ConfigMap as files and env\n- Secret mounted as files\n- SecurityContext (runAsNonRoot, readOnlyRootFilesystem)\n- Probes (readiness, liveness)\n- HorizontalPodAutoscaler on CPU\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-static\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx-static\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:stable-alpine\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: content\n          mountPath: /usr/share/nginx/html\n        - name: tls\n          mountPath: /etc/tls\n          readOnly: true\n        env:\n        - name: INDEX_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: app-config\n              key: INDEX_FILE\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 80\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 80\n          initialDelaySeconds: 15\n          periodSeconds: 20\n      volumes:\n      - name: content\n        configMap:\n          name: app-content\n      - name: tls\n        secret:\n          secretName: tls\n```\n\n## Follow-up Questions\n- How would you verify that readOnlyRootFilesystem is enforced?\n- How would you rotate the TLS secret without restarting pods?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T14:55:37.884Z","createdAt":"2026-01-15T14:55:37.884Z"},{"id":"q-2427","question":"Design a CKAD-focused per-namespace file-processing service: it watches a PVC-mounted directory for new text files, processes each file, and submits results to a REST API. Use a ConfigMap for FILE_EXT and BATCH_SIZE, a Secret for API creds, an InitContainer to install deps, and a marker-based idempotence scheme to achieve at-least-once with near-exactly-once semantics. Include readiness/liveness probes and a minimal YAML skeleton; discuss trade-offs?","answer":"An ideal answer would describe a Deployment with a Python worker reading /workspace/files from a per-namespace PVC, an InitContainer for dependencies, a ConfigMap to set FILE_EXT='txt', BATCH_SIZE=100","explanation":"## Why This Is Asked\nThis question probes per-namespace isolation, file-based events, and Kubernetes primitives in CKAD scope. It also tests InitContainers, readiness probes, and a practical idempotence pattern for small-scale data pipelines.\n\n## Key Concepts\n- Kubernetes primitives: Deployment, PVC, ConfigMap, Secret\n- InitContainers and probes\n- Idempotent file processing using marker files\n- In-namespace REST egress with retry/backoff\n- Observability: logs and metrics\n\n## Code Example\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: file-processor-config\ndata:\n  file_ext: \"txt\"\n  batch_size: \"100\"\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: file-processor-creds\ntype: Opaque\ndata:\n  api_token: <base64>\n```\n\n## Follow-up Questions\n- How would you guarantee exactly-once semantics on restart?\n- How would you scale across many namespaces while avoiding cross-namespace leakage?","diagram":"flowchart TD\n  A[Namespace] --> B[Worker Pod]\n  B --> C[PVC: data]\n  B --> D[ConfigMap: options]\n  B --> E[Secret: creds]","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Anthropic","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T17:47:10.074Z","createdAt":"2026-01-15T17:47:10.074Z"},{"id":"q-2453","question":"In a CKAD scenario, design a per-namespace real-time image-resize service that consumes from a Kafka topic, uses an InitContainer to fetch a resize model from a Secret, writes results to PostgreSQL, and preserves at-least-once delivery with idempotent DB writes. Include a ConfigMap for RESIZE_WIDTH/HEIGHT and BATCH_SIZE, readiness/liveness probes, an HPA based on CPU, and a per-namespace NetworkPolicy restricting Kafka ingress and DB egress. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Deploy a per-namespace image-resize processor that subscribes to a Kafka topic, with an InitContainer fetching a resize model from a Secret. Write results to PostgreSQL via an idempotent upsert on (im","explanation":"## Why This Is Asked\nTests practical CKAD mastery: per-namespace isolation, InitContainer usage, secret/config management, and production-grade guarantees.\n\n## Key Concepts\n- InitContainer for model pull from Secret\n- ConfigMap and Secret usage for runtime parameters\n- Probes and HPA for reliability and scale\n- Idempotent DB writes to achieve at-least-once delivery\n- NetworkPolicy to enforce namespace boundaries\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ns-image-resize\n  namespace: <ns>\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: image-resize\n    spec:\n      initContainers:\n        - name: fetch-model\n          image: alpine:3.18\n          command: [\"sh\", \"-c\", \"download-model.sh\"]\n          volumeMounts:\n            - name: model\n              mountPath: /models\n      containers:\n        - name: processor\n          image: registry/ns-image-resize:latest\n          ports:\n            - containerPort: 8080\n          envFrom:\n            - secretRef: { name: resize-secret }\n      volumes:\n        - name: model\n          emptyDir: {}\n      readinessProbe:\n        httpGet:\n          path: /healthz\n          port: 8080\n        initialDelaySeconds: 5\n        periodSeconds: 10\n      livenessProbe:\n        httpGet:\n          path: /healthz\n          port: 8080\n        initialDelaySeconds: 15\n        periodSeconds: 20\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - weight: 100\n              podAffinityTerm:\n                labelSelector:\n                  matchLabels:\n                    app: image-resize\n                topologyKey: \"kubernetes.io/hostname\"\n  ```\n\n## Follow-up Questions\n- How would you monitor idempotency and retry behavior?\n- What changes if Kafka offset management moves to a separate transactional store?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T18:56:11.109Z","createdAt":"2026-01-15T18:56:11.109Z"},{"id":"q-2541","question":"In CKAD terms, design a new, per-namespace data-collector that subscribes to an in-cluster message bus (NATS) and writes results to a time-series store, ensuring at-least-once delivery with a DLQ. Include an InitContainer to install runtime deps, a ConfigMap for BATCH_SIZE, a Secret for creds, a Sidecar for TLS cert rotation, readiness/liveness probes, and a NetworkPolicy restricting egress to the TSDB, a DLQ path on a PVC, and a minimal YAML skeleton. Explain trade-offs?","answer":"Design a per-namespace data-collector that subscribes to NATS, batches events, and writes to InfluxDB with at-least-once delivery and a DLQ. Use an InitContainer to install runtime dependencies, a ConfigMap for BATCH_SIZE configuration, and a Secret for credentials. Include a sidecar for TLS certificate rotation, implement readiness and liveness probes, and restrict egress traffic to the time-series database using a NetworkPolicy. Mount a PVC for the dead-letter queue path and provide a minimal YAML skeleton for deployment.","explanation":"## Why This Is Asked\nThis question tests comprehensive CKAD skills including per-namespace isolation, configuration management with ConfigMaps and Secrets, InitContainers for dependency setup, sidecar patterns for auxiliary services, health probes for reliability, RBAC for security, and NetworkPolicy for traffic control.\n\n## Key Concepts\n- InitContainers for runtime dependency installation\n- ConfigMaps and Secrets for configuration and credential management\n- Sidecar containers for TLS certificate rotation\n- Readiness and liveness probes for health monitoring\n- Namespace isolation and NetworkPolicy for security boundaries\n- Dead-letter queue implementation on PVC storage\n- At-least-once delivery semantics with batch processing\n\n## Code Example\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: ns-collector\n  namespace: target-namespace\nspec:\n  initContainers:\n  - name: install-deps\n    image: alpine:latest\n    command: ['sh', '-c', 'apk add --no-cache nats-cli']\n  containers:\n  - name: collector\n    image: collector:latest\n    envFrom:\n    - configMapRef:\n        name: ns-collector-config\n    - secretRef:\n        name: ns-collector-creds\n    readinessProbe:\n      httpGet:\n        path: /health\n        port: 8080\n    livenessProbe:\n      httpGet:\n        path: /health\n        port: 8080\n    volumeMounts:\n    - name: dlq-pvc\n      mountPath: /dlq\n  - name: tls-sidecar\n    image: tls-sidecar:latest\n    volumeMounts:\n    - name: tls-certs\n      mountPath: /etc/tls\n  volumes:\n  - name: dlq-pvc\n    persistentVolumeClaim:\n      claimName: dlq-pvc\n  - name: tls-certs\n    secret:\n      secretName: tls-certs\n```\n\n## Trade-offs\n- **At-least-once vs exactly-once**: At-least-once provides simpler implementation but may cause duplicate processing\n- **Sidecar overhead**: TLS rotation adds resource consumption but improves security\n- **PVC vs temporary storage**: PVC provides persistence but increases complexity and cost\n- **NetworkPolicy restrictions**: Improve security but may impact debugging and monitoring capabilities","diagram":"flowchart TD\n  NS[Namespace] --> DC[Data Collector Pod]\n  DC --> NB[NATS Subscription]\n  DC --> TS[InfluxDB]\n  DC --> DLQ[DLQ PVC]","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Discord","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T04:25:02.327Z","createdAt":"2026-01-15T21:49:37.961Z"},{"id":"q-2614","question":"CKAD advanced: design a per-namespace data-pipeline controller using a CRD NamespaceDataPipeline. Each instance subscribes to a namespaced NATS subject (metrics.{namespace}) and writes batched data to a TSDB. Guarantee at-least-once via a DLQ on a PVC and idempotent writes. Include InitContainer for deps, a TLS-rotation sidecar, a ConfigMap for BATCH_SIZE, a Secret for creds, readiness/liveness probes, and a NetworkPolicy restricting egress to the TSDB. Provide a minimal YAML skeleton and trade-offs?","answer":"Design a NamespaceDataPipeline Custom Resource Definition that creates per-namespace data pipeline controllers. Each instance subscribes to a namespaced NATS subject (metrics.{namespace}) and writes batched data to a time-series database. Ensure at-least-once delivery through a dead-letter queue on a persistent volume claim with idempotent write semantics.","explanation":"## Why This Is Asked\nTests your ability to design Kubernetes-native data pipelines with CRD-driven control, per-namespace isolation, and robust delivery guarantees.\n\n## Key Concepts\n- Custom Resources and controllers\n- Idempotent writes and dead-letter queue design\n- InitContainers, sidecars, and TLS rotation\n- RBAC, NetworkPolicy, and health probes\n\n## Code Example\n```yaml\n# Minimal skeleton showing CRD and Deployment structure\n```\n\n## Follow-up Questions\n- How would you implement exactly-once semantics in a stateless controller?\n- How would you test CRD upgrades and canary rollouts?","diagram":"flowchart TD\nA[CRD NamespaceDataPipeline] --> B[Controller watches CRs]\nB --> C[NATS client subscribes to metrics.*]\nC --> D[Batcher writes to TSDB]\nD --> E{DLQ PVC}\nE --> F[Retry/Dead-lettering]\n","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Oracle","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:00:43.754Z","createdAt":"2026-01-16T02:44:23.261Z"},{"id":"q-2767","question":"CKAD advanced: Design a per-namespace policy-enforcement flow using a MutatingAdmissionWebhook that auto-injects resource requests/limits into Pods lacking them. The webhook reads defaults from a ConfigMap (e.g., default_cpu: 125m, default_memory: 128Mi) with per-namespace overrides; a Secret stores a token to fetch policy updates from an in-cluster policy store. Include a minimal webhook Deployment, MutatingWebhookConfiguration, and a per-namespace test namespace. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Implement a MutatingAdmissionWebhook that injects resource requests/limits on Pods missing them. Defaults come from a ConfigMap (e.g., 125m CPU, 128Mi memory) with per-namespace overrides; a Secret st","explanation":"Why This Is Asked\n- Tests mutating admission logic, config-driven defaults, and namespace overrides.\nKey Concepts\n- MutatingAdmissionWebhook, ConfigMap defaults with per-namespace overrides, Secret-based credentials for policy store, InitContainer for runtime deps, and a NetworkPolicy to restrict webhook traffic.\nCode Example\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: policy-defaults\ndata:\n  default_cpu: \"125m\"\n  default_memory: \"128Mi\"\n```\n```\nyaml\napiVersion: admissionregistration.k8s.io/v1\nkind: MutatingWebhookConfiguration\nmetadata:\n  name: pod-resource-defaults\nwebhooks:\n  - name: default-resources.example.com\n    clientConfig:\n      service:\n        name: policy-webhook\n        namespace: default\n        path: \"/mutate\"\n    rules:\n      - apiGroups: [\"\"]\n        apiVersions: [\"v1\"]\n        resources: [\"pods\"]\n        operations: [\"CREATE\"]\n```\nFollow-up Questions\n- How to test idempotency across pod restarts?\n- How to handle conflicting per-namespace overrides and ensure observability?","diagram":"flowchart TD\nA[Namespace] --> B[MutatingAdmissionWebhook]\nB --> C[ConfigMap: defaults]\nB --> D[Secret: policy token]\nB --> E[Policy store]\nF[Webhook Deployment] --> B\nG[NetworkPolicy] --> B","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T10:56:28.677Z","createdAt":"2026-01-16T10:56:28.677Z"},{"id":"q-2782","question":"CKAD intermediate: In namespace 'prod', a Deployment 'payments-api' with multiple replicas risks resource contention under peak load. You must enforce per-namespace resource discipline. Create a ResourceQuota and a LimitRange; provide minimal manifests and describe validation under load?","answer":"Define a ResourceQuota prod-resource-quota to cap total pods, CPU, and memory, plus a LimitRange to enforce default pod requests/limits. This prevents surge from starving the cluster and keeps neighbo","explanation":"## Why This Is Asked\nTests practical enforcement of multi-tenant isolation and Kubernetes quotas in a real prod-like namespace.\n\n## Key Concepts\n- ResourceQuota and hard limits\n- LimitRange defaults and constraints\n- Validation via kubectl describe quota and continuity tests\n\n## Code Example\n```\nyaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: prod-resource-quota\n  namespace: prod\nspec:\n  hard:\n    pods: \"20\"\n    requests.cpu: \"40\"\n    requests.memory: \"160Gi\"\n    limits.cpu: \"80\"\n    limits.memory: \"320Gi\"\n```\n```\nyaml\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: prod-defaults\n  namespace: prod\nspec:\n  limits:\n  - default:\n      cpu: 200m\n      memory: 256Mi\n    defaultRequest:\n      cpu: 100m\n      memory: 128Mi\n    type: Pod\n```\n\n## Follow-up Questions\n- How would you monitor quota usage and alert on breaches in a cluster-wide monitoring stack?\n- How would you adapt quotas for environments like dev/staging while preserving prod isolation?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Scale Ai","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T11:39:36.693Z","createdAt":"2026-01-16T11:39:36.693Z"},{"id":"q-2881","question":"In a CKAD scenario, you need to isolate a payment microservice from other pods while allowing only approved traffic within namespace 'finance'. The Deployment 'payment' has 3 replicas. Write a NetworkPolicy that (a) allows Ingress to payment only from pods labeled app=frontend in the same namespace, (b) allows Egress from payment only to pods labeled app=db on port 5432 and to pods labeled app=cache on port 6379. Provide the minimal policy manifest and describe how to validate under load?","answer":"Create a NetworkPolicy named payment-allow in namespace finance selecting app=payment; ingress only from pods labeled app=frontend in the same namespace; egress only to pods labeled app=db on port 543","explanation":"## Why This Is Asked\nTests practical use of NetworkPolicy to enforce zero-trust interactions between microservices and their dependents, a common production pattern.\n\n## Key Concepts\n- NetworkPolicy basics: podSelector, ingress/egress, policyTypes\n- namespace-scoped rules and label selectors\n- Egress to specific app Pods on defined ports\n- Validation: simulate allowed and blocked traffic, inspect policy state\n\n## Code Example\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: payment-allow\n  namespace: finance\nspec:\n  podSelector:\n    matchLabels:\n      app: payment\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n  egress:\n  - to:\n    - podSelector:\n        matchLabels:\n          app: db\n    ports:\n    - protocol: TCP\n      port: 5432\n  - to:\n    - podSelector:\n        matchLabels:\n          app: cache\n    ports:\n    - protocol: TCP\n      port: 6379\n```\n\n## Follow-up Questions\n- How would you adapt if payment also needs cross-namespace database access?\n- What runtime and network-layer observability would you enable to detect policy violations?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Airbnb","Amazon"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T15:51:46.254Z","createdAt":"2026-01-16T15:51:46.254Z"},{"id":"q-2950","question":"Design a per-namespace Kubernetes CronJob in CKAD terms that runs every 10 minutes and pings a TARGET_URL from a ConfigMap, using an API_TOKEN from a Secret to authenticate, posting a heartbeat payload with a timestamp. Persist the last status in a namespace ConfigMap heartbeat-status and write run logs to a PVC at /logs. Use an InitContainer to install curl, mount the logs volume, add readiness/liveness probes on the Job's Pod, and provide a minimal YAML skeleton. What trade-offs exist?","answer":"Create a per-namespace CronJob every 10 minutes. Pod runs a small script that reads TARGET_URL from ConfigMap cron-config and API_TOKEN from Secret target-creds, posts a heartbeat payload with a times","explanation":"## Why This Is Asked\n\nTests CKAD fundamentals for per-namespace automation, using CronJobs, ConfigMaps, Secrets, InitContainers, and PVC-backed logs in a realistic heartbeat scenario.\n\n## Key Concepts\n\n- CronJob scheduling and Job templates\n- ConfigMap and Secret usage for config and creds\n- InitContainer for one-time setup\n- Probes for container health\n- PersistentVolumeClaim for logs\n\n## Code Example\n\n```javascript\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: heartbeat-cron\n  namespace: default\nspec:\n  schedule: \"*/10 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          initContainers:\n          - name: install-deps\n            image: alpine:3.19\n            command: [\"sh\",\"-c\",\"apk add --no-cache curl\"]\n          containers:\n          - name: heartbeater\n            image: alpine:3.19\n            command: [\"sh\",\"-c\",\"curl -sS -X POST -H \\\"Authorization: Bearer $API_TOKEN\\\" $TARGET_URL/heartbeat -d '{\\\"ts\\\": \\\"$(date +%s)\\\"}'\"]\n            envFrom:\n            - configMapRef:\n                name: cron-config\n            - secretRef:\n                name: target-creds\n            volumeMounts:\n            - name: logs\n              mountPath: /logs\n            readinessProbe:\n              exec:\n                command: [\"sh\",\"-c\",\"test -f /logs/ready\"]\n              initialDelaySeconds: 5\n              periodSeconds: 10\n            livenessProbe:\n              httpGet:\n                path: /healthz\n                port: 80\n              initialDelaySeconds: 15\n              periodSeconds: 20\n          restartPolicy: OnFailure\n          volumes:\n          - name: logs\n            persistentVolumeClaim:\n              claimName: heartbeat-logs\n```\n\n## Follow-up Questions\n\n- How would you rotate the API token without recreating the CronJob?\n- What failure modes and retries would you implement?","diagram":"flowchart TD\n  CronJob[Heartbeat CronJob] --> JobPod[Job Pod]\n  JobPod --> ConfigMap[cron-config]\n  JobPod --> Secret[target-creds]\n  JobPod --> Endpoint[Target URL]\n  Endpoint --> Response[HTTP Response]","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Salesforce","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T18:53:45.750Z","createdAt":"2026-01-16T18:53:45.750Z"},{"id":"q-3044","question":"In CKAD terms, design a per-namespace CronJob-based worker that watches a PVC-backed /data directory for new CSV files, processes them in batches (configurable via a ConfigMap named app-config with BATCH_SIZE), converts to JSON, and POSTS to a cluster-internal endpoint at http://data-portal:8080/api/v1/ingest. Use an InitContainer to install csvkit, a Secret for API credentials, a Sidecar for TLS cert rotation, readiness/liveness probes, a Namespace-scoped NetworkPolicy restricting egress to the internal endpoint, and a minimal YAML skeleton. Discuss trade-offs?","answer":"Deploy a per-namespace CronJob that spawns a Job every 5 minutes to read CSVs from a PVC at /data, batch-process up to BATCH_SIZE files, convert to JSON, and POST to http://data-portal:8080/api/v1/ingest.","explanation":"## Why This Is Asked\nTests CKAD workflow patterns: CronJobs, batch processing, and secret management within a namespace; validates knowledge of init containers, sidecars, and network hardening.\n\n## Key Concepts\n- CronJob scheduling in a namespace\n- InitContainer dependency installation\n- ConfigMap for BATCH_SIZE\n- Secret for API credentials\n- Sidecar for TLS cert rotation\n- NetworkPolicy for internal egress controls\n- Probes for readiness and liveness\n- Idempotency vs at-least-once semantics\n\n## Code Example\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: csv-ingest\nspec:\n  schedule: \"*/5 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          initContainers:\n          - name: install-csvkit\n            image: python:3.9-alpine\n            command: ['sh', '-c', 'pip install csvkit && cp -r /usr/local/lib/python3.9/site-packages/* /shared/']\n            volumeMounts:\n            - name: shared-libs\n              mountPath: /shared\n          containers:\n          - name: csv-processor\n            image: python:3.9-alpine\n            command: ['sh', '-c', 'python /app/process.py']\n            env:\n            - name: BATCH_SIZE\n              valueFrom:\n                configMapKeyRef:\n                  name: app-config\n                  key: BATCH_SIZE\n            - name: API_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: api-credentials\n                  key: token\n            volumeMounts:\n            - name: data-pvc\n              mountPath: /data\n            - name: shared-libs\n              mountPath: /usr/local/lib/python3.9/site-packages\n            readinessProbe:\n              httpGet:\n                path: /health\n                port: 8080\n              initialDelaySeconds: 10\n            livenessProbe:\n              httpGet:\n                path: /health\n                port: 8080\n              initialDelaySeconds: 30\n          - name: cert-rotator\n            image: cert-manager/cert-manager-rotator\n            volumeMounts:\n            - name: certs\n              mountPath: /etc/ssl/certs\n          volumes:\n          - name: data-pvc\n            persistentVolumeClaim:\n              claimName: data-pvc\n          - name: shared-libs\n            emptyDir: {}\n          - name: certs\n            emptyDir: {}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  BATCH_SIZE: \"10\"\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: api-credentials\ntype: Opaque\ndata:\n  token: <base64-encoded-token>\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: csv-ingest-netpol\nspec:\n  podSelector:\n    matchLabels:\n      app: csv-ingest\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - podSelector:\n        matchLabels:\n          app: data-portal\n    ports:\n    - protocol: TCP\n      port: 8080\n```\n\n## Trade-offs\n- **CronJob vs Deployment**: CronJobs provide scheduled execution but lack continuous availability; Deployments offer always-on processing but require custom scheduling logic.\n- **InitContainer overhead**: Installing dependencies at runtime increases startup time vs pre-built images, but provides flexibility for version updates.\n- **Sidecar complexity**: TLS cert rotation adds operational overhead but ensures security compliance; alternative is using cert-manager injection.\n- **Batch processing**: Larger batches improve throughput but increase memory usage and failure impact; smaller batches provide better isolation but higher API call frequency.\n- **NetworkPolicy scope**: Namespace-scoped policies provide good isolation but require per-namespace management; cluster-wide policies simplify administration but reduce granular control.\n- **Storage patterns**: PVC-backed storage ensures data persistence across pod restarts but requires careful volume management; temporary storage simplifies cleanup but risks data loss during failures.","diagram":"flowchart TD\n  A[CronJob] --> B[Job] --> C[//data CSVs/]\n  B --> D[Transform to JSON]\n  D --> E[POST to data-portal]\n  E --> F[Success]\n  F --> G[Cleanup / data]\n  G --> H[Observability: metrics & logs]","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Discord","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T05:28:53.579Z","createdAt":"2026-01-16T22:39:32.265Z"},{"id":"q-3158","question":"Design a per-namespace CKAD microservice that exposes a REST endpoint and supports dynamic feature flags controlled by a ConfigMap. The ConfigMap contains JSON features; a Secret stores per-namespace API keys. Include an InitContainer to install runtime dependencies, a sidecar that watches the ConfigMap for changes and signals the main container to reload config, readiness/liveness probes, an HPA based on 95th percentile latency, and a PVC-backed log store. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Deploy a per-namespace Deployment with an InitContainer that installs runtime deps; main app reads features.json from a ConfigMap and API keys from a Secret; a sidecar watches the ConfigMap for change","explanation":"## Why This Is Asked\n\nTests ability to integrate ConfigMap-driven feature flags, per-namespace isolation, hot-reload semantics, and production considerations like latency-based scaling and persistent logs.\n\n## Key Concepts\n- ConfigMap-driven feature flags with JSON payloads\n- Secret usage for per-namespace API keys\n- InitContainer for dependencies\n- ConfigMap reload sidecar pattern (hot reload via signals)\n- Readiness/Liveness probes\n- HPA based on latency\n- PVC-backed logs\n\n## Code Example\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ns-microservice\n  namespace: example-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ns-microservice\n  template:\n    metadata:\n      labels:\n        app: ns-microservice\n    spec:\n      initContainers:\n      - name: deps\n        image: alpine:3.18\n        command: [\"sh\", \"-c\", \"apk add --no-cache curl jq && mkdir -p /logs\"]\n        volumeMounts:\n        - name: logs\n          mountPath: /logs\n      containers:\n      - name: app\n        image: my-org/ns-microservice:latest\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - secretRef:\n            name: ns-api-keys\n        volumeMounts:\n        - name: logs\n          mountPath: /logs\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n      - name: config-reload\n        image: busybox\n        command: [\"sh\", \"-c\", \"sleep infinity\"]\n        volumeMounts:\n        - name: config\n          mountPath: /config\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: ns-logs-pvc\n      - name: config\n        configMap:\n          name: features-config\n```\n\n## Follow-up Questions\n- How would you implement a graceful failover if the ConfigMap reload signal fails to reach the main app?\n- What are the trade-offs of hot-reload versus full pod restart for feature flag changes?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Meta","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T04:53:35.004Z","createdAt":"2026-01-17T04:53:35.005Z"},{"id":"q-3246","question":"In namespace analytics, Deployment 'reporter' with 4 replicas is failing to fetch tokens because a restrictive NetworkPolicy blocks egress to api.analytics.internal:443. Provide a minimal patch to allow egress to that endpoint, preserving other rules, and describe how you would verify connectivity under load and rollback if needed?","answer":"Patch the NetworkPolicy to add an egress rule for TCP 443 to api.analytics.internal from pods labeled app=reporter in namespace analytics, keeping existing rules. Apply, then verify with a test pod cu","explanation":"## Why This Is Asked\n\nTests practical NetworkPolicy understanding, especially egress limitations and debugging under load.\n\n## Key Concepts\n\n- NetworkPolicy\n- Egress rules\n- IPBlock vs DNS constraints\n- Load validation\n- Rollback strategy\n\n## Code Example\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: reporter-egress\n  namespace: analytics\nspec:\n  podSelector:\n    matchLabels:\n      app: reporter\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - ipBlock:\n        cidr: 10.2.0.0/16\n    ports:\n    - protocol: TCP\n      port: 443\n```\n\n## Follow-up Questions\n\n- How would you test egress connectivity under load?\n- What risks exist with overly permissive egress policies, and how would you mitigate them?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T08:42:05.171Z","createdAt":"2026-01-17T08:42:05.171Z"},{"id":"q-3338","question":"In CKAD terms, design a per-namespace Deployment that runs a small REST API (example Flask app) with config from a ConfigMap named app-config and an API key from Secret named api-credentials. Include an InitContainer to install dependencies, a sidecar to rotate TLS certs from a Secret named tls-certs, readiness/liveness probes, a Namespace NetworkPolicy restricting egress to DEST_ENDPOINT, and a minimal YAML skeleton. Discuss trade-offs?","answer":"Outline: a Deployment with 1) ConfigMap app-config providing DEST_ENDPOINT and LOG_LEVEL as env vars, 2) Secret api-credentials for API_KEY, 3) InitContainer to install Python deps, 4) main REST API c","explanation":"## Why This Is Asked\nTests ability to compose CKAD tasks involving ConfigMaps, Secrets, InitContainers, sidecars, network policies, and probes in a beginner-credible scenario.\n\n## Key Concepts\n- InitContainer for bootstrapping deps\n- Sidecar for TLS cert rotation\n- ConfigMap/Secret usage for config and credentials\n- Namespace NetworkPolicy for egress control\n- Readiness/Liveness probes and non-root/container security\n\n## Code Example\n```yaml\n# Minimal skeleton highlights (not full manifest)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-deploy\n  namespace: payments\nspec:\n  template:\n    spec:\n      initContainers:\n        - name: installer\n          image: python:3.9-slim\n          command: [\"sh\", \"-c\", \"pip install flask\"]\n      containers:\n        - name: api\n          image: my-registry/api:latest\n          env:\n            - name: DEST_ENDPOINT\n              valueFrom:\n                configMapKeyRef:\n                  name: app-config\n                  key: DEST_ENDPOINT\n            - name: LOG_LEVEL\n              valueFrom:\n                configMapKeyRef:\n                  name: app-config\n                  key: LOG_LEVEL\n            - name: API_KEY\n              valueFrom:\n                secretKeyRef:\n                  name: api-credentials\n                  key: API_KEY\n      volumes:\n        - name: tls\n          secret:\n            secretName: tls-certs\n      # probes, securityContext, etc.\n```\n\n## Follow-up Questions\n- How would you test the NetworkPolicy in CI? \n- What are the risks of cert rotation on service availability?","diagram":"flowchart TD\n  A[Deployment] --> B[InitContainer: install deps]\n  A --> C[Main container: REST API]\n  A --> D[Sidecar: TLS rotation]\n  B --> E[ConfigMap: app-config]\n  B --> F[Secret: api-credentials]\n  A --> G[NetworkPolicy: allow DEST_ENDPOINT]","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T12:57:57.293Z","createdAt":"2026-01-17T12:57:57.293Z"},{"id":"q-3410","question":"Design a per-namespace log-forwarder in CKAD terms: a Deployment that tails logs from a PVC-mounted /logs directory produced by apps in the namespace, aggregates log counts by severity for the last minute, and POSTs a heartbeat payload plus the count to a REST endpoint defined in a per-namespace ConfigMap ENDPOINT using API_TOKEN from a Secret. On failure, write payloads to a DLQ directory in the same PVC. Include an InitContainer to install curl and jq, readiness/liveness probes, and a minimal YAML skeleton; discuss trade-offs?","answer":"Deploy a per-namespace log-forwarder: a single-rep Deployment that mounts a PVC at /logs and /logs/dlq. InitContainer installs curl/jq. The forwarder tails new lines in /logs, classifies by severity, ","explanation":"## Why This Is Asked\nTests CKAD patterns for per-namespace resources, log pipelines, and robust error handling with limited tooling.\n\n## Key Concepts\n- Namespace-scoped Deployments and PVCs\n- InitContainers for bootstrap tooling\n- ConfigMap/Secret for endpoint and credentials\n- Simple in-Pod state via streaming tail and per-minute aggregation\n- DLQ on PVC for failed forwards\n- Readiness/Liveness probes\n\n## Code Example\n\n```yaml\n# skeleton Deployment (high level)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: log-forwarder\n  namespace: <namespace>\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: log-forwarder\n    spec:\n      initContainers:\n        - name: installer\n          image: alpine:3.18\n          command: [\"sh\", \"-c\", \"apk add --no-cache curl jq\"]\n      containers:\n        - name: forwarder\n          image: some-forwarder:latest\n          volumeMounts:\n            - name: logs\n              mountPath: /logs\n            - name: dlq\n              mountPath: /logs/dlq\n          env:\n            - name: ENDPOINT\n              valueFrom:\n                configMapKeyRef:\n                  name: log-forwarder-config\n                  key: ENDPOINT\n            - name: API_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: log-forwarder-secret\n                  key: API_TOKEN\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 15\n            periodSeconds: 10\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 30\n            periodSeconds: 10\n      volumes:\n        - name: logs\n          persistentVolumeClaim:\n            claimName: logs-pvc\n        - name: dlq\n          persistentVolumeClaim:\n            claimName: logs-dlq-pvc\n```\n\n## Follow-up Questions\n- How would you scale and parallelize across namespaces?\n- How to secure endpoint access and rotate API_TOKEN?","diagram":"flowchart TD\n  N[Namespace PVC: /logs] --> I[InitContainer: install curl/jq]\n  I --> F[Forwarder: tail /logs, aggregate last-minute counts by severity]\n  F --> E[POST to ENDPOINT with TOKEN from Secret]\n  E -->|success| S[Healthy]\n  E -->|failure| DLQ[DLQ: /logs/dlq]","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T15:29:15.253Z","createdAt":"2026-01-17T15:29:15.253Z"},{"id":"q-3480","question":"In CKAD terms, design a per-namespace Deployment that serves a static landing page with Nginx. The site content lives in a PVC mounted at /usr/share/nginx/html. Use a ConfigMap named site-config with SITE_TITLE and FOOTER, and a Secret named site-auth for htpasswd-based basic authentication. Include an InitContainer to pre-populate index.html from /config data, a readiness/liveness probe for /. Provide a Namespace NetworkPolicy restricting egress to an internal CDN at 10.0.0.0/16:443. Include a minimal YAML skeleton and discuss trade-offs?","answer":"Design a per-namespace Deployment: Nginx serves a static site from a PVC mounted at /usr/share/nginx/html. InitContainer runs alpine, reads /config/SITE_TITLE and /config/FOOTER from the ConfigMap mou","explanation":"## Why This Is Asked\nTests ability to compose common CKAD primitives: Deployment, InitContainer writing into a PVC, ConfigMap/Secret handling, basic auth, and network controls at namespace scope.\n\n## Key Concepts\n- InitContainer writes dynamic index.html from ConfigMap data\n- PVC for content, ConfigMap for config, Secret for credentials\n- Readiness/Liveness probes, TLS/basics via htpasswd\n- Namespace NetworkPolicy restricting external access\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: site\n  namespace: ns-example\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: site\n  template:\n    metadata:\n      labels:\n        app: site\n    spec:\n      initContainers:\n      - name: gen-index\n        image: alpine:3.18\n        command: [\"/bin/sh\", \"-c\", \"SITE_TITLE=$(cat /config/SITE_TITLE); FOOTER=$(cat /config/FOOTER); cat > /usr/share/nginx/html/index.html <<EOF\\n<html><head><title>$SITE_TITLE</title></head><body><h1>$SITE_TITLE</h1><p>$FOOTER</p></body></html>\\nEOF\"]\n        volumeMounts:\n        - name: html\n          mountPath: /usr/share/nginx/html\n        - name: config\n          mountPath: /config\n      containers:\n      - name: nginx\n        image: nginx:1.25-alpine\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: html\n          mountPath: /usr/share/nginx/html\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 15\n          periodSeconds: 20\n      volumes:\n      - name: html\n        persistentVolumeClaim:\n          claimName: site-content\n      - name: config\n        configMap:\n          name: site-config\n      - name: auth\n        secret:\n          secretName: site-auth\n```\n\n## Follow-up Questions\n- How would you rotate the htpasswd file without downtime?\n- How would you test the readiness probe in CI/CD?","diagram":"flowchart TD\n  A[Namespace ns-example] --> B[Deployment site]\n  B --> C[InitContainer gen-index]\n  B --> D[Main container nginx]\n  B --> E[ConfigMap: site-config]\n  B --> F[Secret: site-auth]\n  B --> G[PVC: site-content]\n  B --> H[NetworkPolicy: restrict egress to CDN]","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Oracle","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T18:39:21.788Z","createdAt":"2026-01-17T18:39:21.788Z"},{"id":"q-3548","question":"In CKAD terms, design a per-namespace Deployment that serves a tiny REST API (Flask) backed by a local SQLite DB stored on a PVC. Use a ConfigMap app-config to toggle ENABLE_FEATURE and set API_TIMEOUT, and a Secret api-credentials for basic-auth. Include an InitContainer to install deps, a TLS cert rotation sidecar, readiness/liveness probes, and a NamespaceNetworkPolicy restricting egress to the internal auth-service host. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Deployment per-namespace serves a Flask REST API backed by a SQLite DB on a PVC. InitContainer installs Python, Flask, sqlite3; main container uses ConfigMap app-config (ENABLE_FEATURE, API_TIMEOUT) a","explanation":"## Why This Is Asked\nTests CKAD ability to combine common primitives: PVC-backed data, ConfigMap-driven behavior, Secrets for credentials, InitContainers, TLS rotation sidecar, probes, and per-namespace NetworkPolicy in a cohesive, beginner-friendly scenario.\n\n## Key Concepts\n- ConfigMap/Secret usage for runtime config and credentials\n- PVC-backed storage and data lifecycle\n- InitContainer for dependencies\n- TLS cert rotation sidecar\n- Readiness/Liveness probes and health checks\n- NamespaceNetworkPolicy scoping\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ns-api\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: ns-api\n    spec:\n      volumes:\n        - name: data\n          persistentVolumeClaim:\n            claimName: ns-app-data\n      initContainers:\n        - name: install-deps\n          image: python:3.11\n          command: [\"bash\",\"-lc\",\"pip install flask sqlite3\"]\n      containers:\n        - name: api\n          image: myregistry/ns-api:latest\n          ports:\n            - containerPort: 8080\n          env:\n            - name: ENABLE_FEATURE\n              valueFrom:\n                configMapKeyRef:\n                  name: app-config\n                  key: ENABLE_FEATURE\n            - name: API_TIMEOUT\n              valueFrom:\n                configMapKeyRef:\n                  name: app-config\n                  key: API_TIMEOUT\n          volumeMounts:\n            - name: data\n              mountPath: /data\n      - name: tls-sidecar\n        image: quay.io/cert_manager/tls-sidecar:latest\n        # sidecar config omitted for brevity\n      terminationGracePeriodSeconds: 30\n      restartPolicy: Always\n```\n\n## Follow-up Questions\n- How would you test config flag changes without rolling pods?\n- What are trade-offs of storing DB in a PVC vs ephemeral storage?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Snap","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T20:45:21.505Z","createdAt":"2026-01-17T20:45:21.505Z"},{"id":"q-3637","question":"In CKAD terms, design a per-namespace Deployment that handles file-based ingestion from a PVC mounted /input: it should validate each JSON file against a schema stored in a ConfigMap named schema-config, normalize valid records to a consistent JSON, and POST them to http://data-portal:8080/api/v1/ingest using a Bearer token sourced from a Secret named api-tokens. Include an InitContainer to install jq and a validator, a Sidecar for TLS cert rotation, readiness/liveness probes, a Namespace NetworkPolicy restricting egress to the internal endpoint, and a minimal YAML skeleton. Discuss trade-offs?","answer":"Implement a Deployment that monitors /input for JSON files, validates each against a schema stored in schema-config, normalizes valid records to consistent JSON, and POSTs them to http://data-portal:8080/api/v1/ingest using a Bearer token from the api-tokens Secret. Include an InitContainer to install jq and validation tools, a sidecar for TLS certificate rotation, proper readiness/liveness probes, and a Namespace NetworkPolicy restricting egress to the internal endpoint.","explanation":"## Why This Is Asked\nCKAD candidates often struggle with end-to-end file ingestion using Kubernetes primitives. This question tests volume access, ConfigMap/Secret usage, InitContainers, probes, NetworkPolicy, and retry strategies in a comprehensive scenario.\n\n## Key Concepts\n- ConfigMaps/Secrets for configuration and credentials\n- InitContainers for tooling setup\n- In-cluster networking and security policies\n- File-based ingestion with idempotent POST operations\n- Multi-container pod patterns\n\n## Code Example\n```javascript\n// Pseudo-implementation sketch\nconst payload = readFile('/input/file.json');\nvalidateSchema(payload, getSchema('schema-config'));\nconst normalized = normalize(payload);\nsendToEndpoint('http://data-portal:8080/api/v1/ingest', normalized);\n```\n\n## Trade-offs\n- **File watching vs. event-driven**: File watching is simpler but less efficient than event-based systems\n- **In-cluster validation**: Validates locally but increases pod resource usage\n- **NetworkPolicy restrictions**: Improves security but adds complexity to egress management\n- **Sidecar overhead**: TLS rotation adds reliability but increases resource consumption","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T04:01:50.948Z","createdAt":"2026-01-18T02:41:31.376Z"},{"id":"q-3702","question":"In namespace data, create a CronJob named daily-analytics that runs at 02:00 daily using image analytics-runner:2.0. It must mount a ConfigMap analytics-config (DB_HOST, DB_NAME, QUERY_TIMEOUT) and a Secret analytics-creds (DB_USER, DB_PASSWORD). Enforce no overlaps (concurrencyPolicy: Forbid), backoffLimit: 3, TTLSecondsAfterFinished: 86400, successfulJobsHistoryLimit: 3, and annotate the PodTemplate with a checksum/config to reload on changes. How would you verify this under simulated load and when the config changes?","answer":"Configure a CronJob named daily-analytics in namespace data using image analytics-runner:2.0, schedule 0 2 * * *, with ConfigMap analytics-config (DB_HOST, DB_NAME, QUERY_TIMEOUT) and Secret analytics","explanation":"## Why This Is Asked\nTests ability to design a batch CronJob workflow with secrets/config, idempotent restarts, and lifecycle controls. It also probes config-driven restarts and observable validation under load.\n\n## Key Concepts\n- CronJob scheduling and lifecycle\n- ConfigMap and Secret usage in Jobs\n- PodTemplate annotations for config reloads\n- Job history, TTL, and backoff behavior\n- Validation under simulated load\n\n## Code Example\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: daily-analytics\n  namespace: data\nspec:\n  schedule: \"0 2 * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: analytics\n            image: analytics-runner:2.0\n            envFrom:\n            - configMapRef:\n                name: analytics-config\n            - secretRef:\n                name: analytics-creds\n          restartPolicy: OnFailure\n```\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: analytics-config\n  namespace: data\ndata:\n  DB_HOST: \"db.internal\"\n  DB_NAME: \"analytics\"\n  QUERY_TIMEOUT: \"30s\"\n```\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: analytics-creds\n  namespace: data\ntype: Opaque\nstringData:\n  DB_USER: \"analytics\"\n  DB_PASSWORD: \"s3cr3t\"\n```\n\n## Follow-up Questions\n- How would you test idempotency of cron jobs across config changes?\n- How would you monitor this to detect overdue runs?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Snap","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T06:42:43.612Z","createdAt":"2026-01-18T06:42:43.613Z"},{"id":"q-3798","question":"In CKAD terms, design a per-namespace log-collector Deployment that mounts /logs from a PVC, tails app.log for ERROR lines, batches them (configurable via a ConfigMap named app-config with BATCH_SIZE), and POSTs JSON payloads to an internal endpoint at http://analytics.local:9000/ingest using credentials from a Secret named api-credentials. Use an InitContainer to install a minimal Python runtime + requests, a Sidecar for TLS cert rotation, readiness/liveness probes, and a Namespace NetworkPolicy restricting egress to the internal endpoint. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Implement a per-namespace Deployment that mounts /logs from a PVC, uses inotify-based watcher (Python) to tail app.log, filters lines matching ERROR, batches them (BATCH_SIZE from app-config), and pos","explanation":"## Why This Is Asked\nTests practical use of per-namespace isolation for log processing, dynamic config via ConfigMap, secret-based credentials, and a robust data path with batching, retries, and a DLQ. Requires InitContainer setup, sidecars for TLS, and proper networking constraints.\n\n## Key Concepts\n- ConfigMap/Secret usage for runtime config and sensitive data\n- InitContainer for dependencies\n- Sidecar for TLS cert rotation\n- Readiness/Liveness probes\n- NetworkPolicy scoping to internal endpoint\n- DLQ on PVC for failed payloads\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: log-collector\n  namespace: default\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: log-collector\n    spec:\n      initContainers:\n        - name: init-deps\n          image: python:3.11-slim\n          command: [\"sh\", \"-c\", \"pip install --no-cache-dir requests\"]\n          volumeMounts:\n            - name: logs\n              mountPath: /logs\n      containers:\n        - name: collector\n          image: my org/log-collector:latest\n          volumeMounts:\n            - name: logs\n              mountPath: /logs\n          env:\n            - name: BATCH_SIZE\n              valueFrom:\n                configMapKeyRef:\n                  name: app-config\n                  key: BATCH_SIZE\n        - name: tls-sidecar\n          image: myorg/tls-sidecar:latest\n          volumeMounts:\n            - name: tls\n              mountPath: /tls\n      volumes:\n        - name: logs\n          persistentVolumeClaim:\n            claimName: logs-pvc\n        - name: tls\n          secret:\n            secretName: tls-certs\n        - name: dlq\n          persistentVolumeClaim:\n            claimName: log-dlq-pvc\n      # NetworkPolicy defined in separate manifest\n```\n\n## Follow-up Questions\n- How would you handle backpressure if analytics.local is slow to ingest?\n- What changes would you make for multi-tenant isolation across namespaces?","diagram":"flowchart TD\n  PVC[Logs PVC] --> Watcher[Watcher]\n  Watcher --> Batcher[Batcher]\n  Batcher --> API[Analytics API]\n  API --> DLQ[DLQ PVC]","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T09:43:55.795Z","createdAt":"2026-01-18T09:43:55.795Z"},{"id":"q-3915","question":"In CKAD terms, design a per-namespace Deployment that exposes a tiny HTTP API to store and retrieve JSON blobs using a PVC-backed path. Endpoints: POST /store with {id, blob}, GET /store/{id}. Use a ConfigMap named app-config with MAX_BLOB_SIZE, and a Secret named api-credentials. Include an InitContainer to install a minimal store tool, a Sidecar for TLS cert rotation, readiness/liveness probes, and a Namespace NetworkPolicy restricting egress to an internal auth service. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Implementation would create a Deployment with a small HTTP API that stores JSON blobs in a PVC-backed directory. It validates payloads against a schema, writes files named by id, and serves /store/{id","explanation":"## Why This Is Asked\nTests ability to compose Kubernetes primitives (PVC, ConfigMap, Secret, InitContainer), simple HTTP API, file-based persistence, TLS rotation, and network controls at CKAD level. Encourages thinking about concurrency, idempotence, and failure modes.\n\n## Key Concepts\n- PVC-backed file store\n- InitContainer bootstrap\n- ConfigMap and Secret usage\n- TLS cert rotation sidecar\n- Readiness/Liveness probes\n- Namespace NetworkPolicy\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blob-store\nspec:\n  template:\n    spec:\n      containers:\n      - name: api\n        image: example/api:latest\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet: {path: /health, port: 8080}\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        volumeMounts:\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: blob-pvc\n```\n\n## Follow-up Questions\n- How would you enforce idempotence for POST /store?\n- How would you simulate pod rescheduling without data loss?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T14:54:44.685Z","createdAt":"2026-01-18T14:54:44.685Z"},{"id":"q-4031","question":"In a CKAD scenario, in namespace dataops, create a CronJob named db-backup that runs daily at 02:00 UTC to back up a PostgreSQL database. Use a ConfigMap backup-config for BACKUP_DIR and DB_HOST and a Secret db-credentials for DB_USER and DB_PASSWORD. Mount PVC backup-pvc at /backups and write backups as /backups/<dbname>-YYYYMMDD.sql. Enforce concurrencyPolicy: Forbid, keep 7 successful jobs, and set activeDeadlineSeconds: 7200. Provide manifests and a straightforward validation and rollback plan?","answer":"Create a CronJob manifest and related resources to back up PostgreSQL daily. Use ConfigMap backup-config for BACKUP_DIR and DB_HOST, Secret db-credentials for DB_USER/DB_PASSWORD, mount PVC backup-pvc at /backups, and write backups as /backups/<dbname>-YYYYMMDD.sql. Enforce concurrencyPolicy: Forbid, keep 7 successful jobs, and set activeDeadlineSeconds: 7200.","explanation":"## Why This Is Asked\nTests ability to compose Kubernetes primitives for practical automation: CronJobs, Jobs, ConfigMaps, Secrets, and PVCs, plus robust scheduling and rollback.\n\n## Key Concepts\n- CronJob scheduling and jobTemplate usage\n- ConfigMap and Secret as environment sources\n- PVC mounting in Jobs for persistent backups\n- Concurrency policy and job history controls\n- Rollback and validation workflows\n\n## Code Example\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: db-backup\n  namespace: dataops\nspec:\n  schedule: \"0 2 * * *\"\n  concurrencyPolicy: Forbid\n  successfulJobsHistoryLimit: 7\n  activeDeadlineSeconds: 7200\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: postgres-backup\n            image: postgres:15\n            env:\n            - name: BACKUP_DIR\n              valueFrom:\n                configMapKeyRef:\n                  name: backup-config\n                  key: BACKUP_DIR\n            - name: DB_HOST\n              valueFrom:\n                configMapKeyRef:\n                  name: backup-config\n                  key: DB_HOST\n            - name: DB_USER\n              valueFrom:\n                secretKeyRef:\n                  name: db-credentials\n                  key: DB_USER\n            - name: DB_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: db-credentials\n                  key: DB_PASSWORD\n            command:\n            - /bin/bash\n            - -c\n            - |\n              DATE=$(date +%Y%m%d)\n              BACKUP_FILE=\"/backups/${DB_NAME}-${DATE}.sql\"\n              pg_dump -h $DB_HOST -U $DB_USER -d $DB_NAME > $BACKUP_FILE\n            volumeMounts:\n            - name: backup-storage\n              mountPath: /backups\n          volumes:\n          - name: backup-storage\n            persistentVolumeClaim:\n              claimName: backup-pvc\n          restartPolicy: OnFailure\n```\n\n## Validation Plan\n1. Check CronJob creation: `kubectl get cronjob db-backup -n dataops`\n2. Verify schedule: `kubectl describe cronjob db-backup -n dataops`\n3. Test manual job: `kubectl create job --from=cronjob/db-backup test-backup -n dataops`\n4. Monitor job completion: `kubectl get jobs -n dataops -w`\n5. Validate backup files: `kubectl exec -it <pod-name> -n dataops -- ls -la /backups`\n\n## Rollback Plan\n1. Delete CronJob: `kubectl delete cronjob db-backup -n dataops`\n2. Clean up test jobs: `kubectl delete jobs -l job-name=db-backup -n dataops`\n3. Remove backup files if needed: Access PVC and delete specific backup files\n4. Recreate with corrected manifest if issues persist","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T05:05:22.693Z","createdAt":"2026-01-18T20:44:03.265Z"},{"id":"q-4049","question":"In CKAD terms, implement a per-namespace BackupPlan pattern (CRD) that triggers backups of a StatefulSet's PVC to a per-namespace S3-like bucket. Define CRD fields for targetStatefulSet, sourcePVC, destBucket, destPrefix; show a minimal Job-based workflow with an InitContainer to install awscli, a Secret for AWS creds, a ConfigMap for BUCKET/PREFIX, and a PVC for logs. Include readiness/liveness probes and a minimal YAML skeleton; discuss trade-offs?","answer":"Implement a per-namespace BackupPlan CRD (name, targetStatefulSet, sourcePVC, destBucket, destPrefix). A lightweight controller watches BackupPlan in the namespace and creates a Job per plan named bac","explanation":"## Why This Is Asked\nTests ability to design Kubernetes-native backup patterns using CRD + Job orchestration, with proper use of InitContainers, Secrets, ConfigMaps, and PVs. Encourages thinking about per-namespace scoping, fault-tolerance, and minimal operator logic.\n\n## Key Concepts\n- CustomResourceDefinition and a namespace-scoped controller\n- Jobs, InitContainers, and data packaging (tarball) for backups\n- Secrets for credentials and ConfigMaps for configuration\n- PVC usage for temporary/log data and readiness/liveness probes\n- Idempotency, retries, and failure semantics\n\n## Code Example\n```yaml\n# CRD skeleton (minimal)\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: backplans.example.com\nspec:\n  group: example.com\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n  scope: Namespaced\n  names:\n    plural: backplans\n    singular: backplan\n    kind: BackupPlan\n\n---\n# Minimal Job skeleton per BackupPlan\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: backup-<plan>\nspec:\n  template:\n    spec:\n      initContainers:\n        - name: awscli\n          image: <image-with-awscli>\n          command: [\"sh\", \"-c\", \"aws --version\"]\n      containers:\n        - name: tar-and-upload\n          image: <tar-awsupload-image>\n          command: [\"/bin/sh\", \"-c\", \"tar czf /logs/backup.tar.gz -C /data . && aws s3 cp /logs/backup.tar.gz s3://<bucket>/<prefix>/ backup-<plan>.tar.gz\"]\n          volumeMounts:\n            - name: data\n              mountPath: /data\n            - name: logs\n              mountPath: /logs\n      volumes:\n        - name: data\n          persistentVolumeClaim:\n            claimName: <source-pvc>\n        - name: logs\n          emptyDir: {}\n      restartPolicy: OnFailure\n      readinessProbe:\n        tcpSocket:\n          port: 9100\n      livenessProbe:\n        httpGet:\n          path: /healthz\n          port: 8080\n```\n\n## Follow-up Questions\n- How would you handle partial failures or large PVCs to avoid timeouts?\n- What changes to support multi-region backups and encryption at rest?","diagram":"flowchart TD\n  A[BackupPlan Created in Namespace] --> B[Controller detects Plan]\n  B --> C[Create Job backup-PLAN]\n  C --> D[Job InitContainer installs awscli]\n  D --> E[Main Container tarballs PVC data]\n  E --> F[Uploads to s3://bucket/prefix/plan.tar.gz]\n  F --> G[Status update in CRD / heartbeat ConfigMap]","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Meta","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T21:36:36.687Z","createdAt":"2026-01-18T21:36:36.687Z"},{"id":"q-4184","question":"Design a per-namespace synthetic-traffic generator Deployment in CKAD terms: it reads TARGET_URL, PAYLOAD, and HEADERS from a Namespace ConfigMap, and a TOKEN from a Secret; an InitContainer installs curl. It should generate requests at a configurable rate, log each attempt to a PVC at /logs, and on failure move the payload to a DLQ on the same PVC. Include readiness/liveness probes and a CPU-based HPA; provide a minimal YAML skeleton and discuss trade-offs?","answer":"Use a Deployment with envFrom ConfigMap for TARGET_URL, PAYLOAD, HEADERS and a Secret TOKEN. An InitContainer installs curl (apk add --no-cache curl). A worker loop hits TARGET_URL at a configurable r","explanation":"## Why This Is Asked\nTests CKAD proficiency in namespace scoping, ConfigMap/Secret wiring, InitContainer usage, persistent logging, DLQ handling, and monitoring hooks.\n\n## Key Concepts\n- Namespace-scoped ConfigMaps and Secrets\n- InitContainer to install curl\n- Deployment with a worker loop\n- PVC-backed logs and DLQ\n- Readiness/Liveness probes\n- HorizontalPodAutoscaler by CPU\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: traffic-gen\n  namespace: example-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: traffic-gen\n  template:\n    metadata:\n      labels:\n        app: traffic-gen\n    spec:\n      initContainers:\n      - name: install-curl\n        image: alpine:3.18\n        command: [\"sh\", \"-c\", \"apk add --no-cache curl && mkdir -p /logs/dlq\"]\n        volumeMounts:\n        - name: logs\n          mountPath: /logs\n      containers:\n      - name: worker\n        image: curlimages/curl:8.1.2\n        envFrom:\n        - configMapRef:\n            name: traffic-config\n        - secretRef:\n            name: traffic-secret\n        volumeMounts:\n        - name: logs\n          mountPath: /logs\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        livenessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: traffic-logs-pvc\n```\n","diagram":"flowchart TD\n  NS[Namespace] --> D[Deployment: traffic-gen]\n  D --> IC[InitContainer: install curl]\n  D --> W[Worker: traffic generator]\n  W --> Logs[/logs]\n  W --> DLQ[/logs/dlq]\n  W --> Probes[Probes]\n  W --> HPA[HPA: CPU]","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Twitter","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T07:01:40.590Z","createdAt":"2026-01-19T07:01:40.590Z"},{"id":"q-4196","question":"CKAD intermediate: In namespace prod, the 4-replica Deployment 'payments-api' must talk to an upstream processor at payments.proc.svc:443 using mTLS. Create minimal Kubernetes manifests to mount a Secret tls-secret containing client cert/key and CA, configure the container to use those certs, and restrict egress to only that endpoint via a NetworkPolicy. Show the patches and explain how you'd verify TLS and cert rotation?","answer":"Mount tls-secret at /certs in payments-api, set TLS_PATH=/certs, and configure the client to present the certificate when connecting to payments.proc.svc:443; add a NetworkPolicy that restricts egress","explanation":"## Why This Is Asked\nTests secret handling, TLS/mTLS basics, mounting certs, and correct network isolation for a real-world integration.\n\n## Key Concepts\n- Secrets and volume mounts\n- TLS/Certificates and mTLS basics\n- NetworkPolicy egress controls\n- Rollout and secret rotation strategies\n\n## Code Example\n```javascript\napiVersion: v1\nkind: Secret\nmetadata:\n  name: tls-secret\ntype: Opaque\ndata:\n  tls.crt: <base64>\n  tls.key: <base64>\n  ca.crt: <base64>\n```\n\n```javascript\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: payments-api\n  namespace: prod\nspec:\n  template:\n    spec:\n      containers:\n      - name: payments\n        image: payments-api:latest\n        env:\n        - name: TLS_PATH\n          value: /certs\n        volumeMounts:\n        - name: tls\n          mountPath: /certs\n      volumes:\n      - name: tls\n        secret:\n          secretName: tls-secret\n```\n\n```javascript\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: payments-egress\n  namespace: prod\nspec:\n  podSelector:\n    matchLabels:\n      app: payments-api\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - ipBlock:\n        cidr: 203.0.113.45/32\n    ports:\n    - protocol: TCP\n      port: 443\n```\n\n> Note: DNS-based egress restrictions are not widely supported in NetworkPolicy; use the processor’s IP in the ipBlock.","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","IBM","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T08:07:27.645Z","createdAt":"2026-01-19T08:07:27.647Z"},{"id":"q-4282","question":"In CKAD terms, design a per-namespace Job that migrates data from a PVC /data/migrate by reading CSVs with csvkit (InitContainer installs tools), sending batched JSON to http://data-portal:8080/api/v1/migrate. Configure MIG_BATCH_SIZE in mig-config ConfigMap and API token in mig-creds Secret. Use a PVC-stored marker for idempotence, set backoffLimit, and add readiness/liveness probes. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Draft a per-namespace Job that migrates data from a PVC /data/migrate by reading CSVs with csvkit (InitContainer installs tools), sending batched JSON to http://data-portal:8080/api/v1/migrate. MIG_BA","explanation":"Why This Is Asked\n- Validates CKAD basics: Jobs, InitContainers, ConfigMaps, Secrets, PVCs, probes in a practical data-migrate flow.\n- Introduces idempotence with a marker, a common real-world concern.\n\nKey Concepts\n- Job lifecycle and completion guarantees\n- InitContainer usage for tooling installation\n- Runtime config from ConfigMaps and Secrets\n- PVC-based idempotence markers and backoff settings\n- Probes and minimal YAML scaffolding\n\nCode Example\n```yaml\n# Minimal Job skeleton (pseudo)\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: data-migrate\nspec:\n  backoffLimit: 3\n  template:\n    spec:\n      initContainers:\n      - name: csvkit-install\n        image: alpine:latest\n        command: [\"sh\", \"-c\", \"apk add --no-cache python3 py3-pip && pip3 install csvkit\"]\n      containers:\n      - name: migrator\n        image: curlimages/curl:7.88.0\n        command: [\"sh\", \"-c\", \"python3 migrate.py\"]\n        env:\n        - name: MIG_BATCH_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: mig-config\n              key: MIG_BATCH_SIZE\n        - name: API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: mig-creds\n              key: api-token\n        volumeMounts:\n        - name: data\n          mountPath: /data/migrate\n        - name: marker\n          mountPath: /data/marker\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: migrate-pvc\n      - name: marker\n        persistentVolumeClaim:\n          claimName: migrate-marker-pvc\n      restartPolicy: OnFailure\n      # probes would be defined on the migrator container\n```\n\nFollow-up Questions\n- How would you test idempotence across retries?\n- How would you handle partial CSV failures and ensure exactly-once semantics?","diagram":"flowchart TD\n  A[Start] --> B[InitContainer: install csvkit]\n  B --> C[Read /data/migrate]\n  C --> D[Batch to /api/v1/migrate]\n  D --> E[Write marker to PVC]\n  E --> F[Job completes]","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T11:32:11.040Z","createdAt":"2026-01-19T11:32:11.040Z"},{"id":"q-4418","question":"In CKAD terms, design a per-namespace Deployment that runs a rate-limited API proxy inside the cluster. It should read its rate limit from a ConfigMap named app-config, authenticate upstream calls with a Secret named api-credentials, and initialize dependencies in an InitContainer. Include a TLS cert-rotation sidecar using a Secret named tls-certs, readiness/liveness probes, and a Namespace NetworkPolicy restricting egress to upstream.internal:8080. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Design a per-namespace Deployment that runs a rate-limited API proxy. It uses a ConfigMap app-config with RPS, a Secret api-credentials for upstream API key, and an InitContainer to install deps. A TL","explanation":"## Why This Is Asked\nTests understanding of combining Kubernetes primitives for a practical, beginner-to-intermediate task: ConfigMaps and Secrets for config and credentials, InitContainers for bootstrapping, a TLS rotation sidecar for security, probes for reliability, and a Namespace NetworkPolicy to constrain egress.\n\n## Key Concepts\n- ConfigMap and Secret usage in a single pod\n- InitContainer for bootstrapping dependencies\n- Sidecar for TLS cert rotation\n- Readiness and Liveness probes\n- Namespace NetworkPolicy to constrain egress\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: proxy\n  namespace: default\nspec:\n  replicas: 1\n  template:\n    spec:\n      initContainers:\n      - name: install-deps\n        image: alpine:3.18\n        command: [\"/bin/sh\",\"-lc\",\"apk add --no-cache python3 py3-pip && pip3 install fastapi uvicorn httpx\"]\n        volumeMounts:\n        - name: config\n          mountPath: /config\n      containers:\n      - name: proxy\n        image: myorg/proxy:latest\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: app-config\n        - secretRef:\n            name: api-credentials\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n      volumes:\n      - name: config\n        configMap:\n          name: app-config\n      - name: certs\n        secret:\n          secretName: tls-certs\n```\n\n## Follow-up Questions\n- How would you test the rate limiter under burst traffic?\n- How would you rotate keys without downtime?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Uber","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T17:59:24.422Z","createdAt":"2026-01-19T17:59:24.422Z"},{"id":"q-4509","question":"CKAD intermediate: In namespace 'prod', a Deployment named 'image-processor' with 4 replicas experiences spikes under load. Propose and implement a minimal patch to: (a) add resource requests/limits and health probes; (b) configure an HPA targeting 75% CPU (min 3, max 10 replicas); (c) add a PodDisruptionBudget to keep at least 3 healthy pods; (d) tune rolling updates (maxUnavailable:1). Then describe how you'd validate under simulated load and rollback if needed?","answer":"Implement a comprehensive patch to the Deployment by adding resource requests/limits (cpu: 100m/500m, memory: 128Mi/512Mi), configuring readinessProbe and livenessProbe with /healthz endpoint, deploying an HPA with minReplicas: 3, maxReplicas: 10, targetCPUUtilizationPercent: 75, creating a PodDisruptionBudget with minAvailable: 3, and setting rollingUpdate strategy with maxUnavailable: 1. Validate the configuration using load testing tools like k6 or hey while monitoring HPA scaling behavior and pod health metrics, and rollback via kubectl rollout undo deployment/image-processor if issues arise.","explanation":"## Why This Is Asked\n\nAssesses practical CKAD mastery of production-ready deployments, autoscaling configuration, and operational safety measures in Kubernetes environments.\n\n## Key Concepts\n\n- Resource requests/limits for predictable performance and resource allocation\n- Readiness and liveness probes for health monitoring and traffic management\n- HorizontalPodAutoscaler for dynamic scaling based on CPU utilization\n- PodDisruptionBudget for availability guarantees during maintenance\n- RollingUpdate strategy for controlled deployments with zero downtime\n\n## Code Example\n\n```yaml\nresources:\n  requests:\n    cpu: \"100m\"\n    memory: \"128Mi\"\n  limits:\n    cpu: \"500m\"\n    memory: \"512Mi\"\nreadinessProbe:\n  httpGet:\n    path: /healthz\n    port: 8080\n  initialDelaySeconds: 5\n  periodSeconds: 10\nlivenessProbe:\n  httpGet:\n    path: /healthz\n    port: 8080\n  initialDelaySeconds: 15\n  periodSeconds: 20\n```\n\n```yaml\n# HPA Configuration\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: image-processor-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: image-processor\n  minReplicas: 3\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 75\n```","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","MongoDB","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T07:38:47.157Z","createdAt":"2026-01-19T21:49:54.992Z"},{"id":"q-4635","question":"In CKAD terms, design a per-namespace Deployment that serves a static site via nginx. Use a PVC-backed volume mounted at /usr/share/nginx/html, an InitContainer to fetch assets from an internal artifact registry based on a ConfigMap site-config (INDEX_FILE, THEME), and a Secret site-basic-auth for HTTP basic auth. Include readiness/liveness probes, and a Namespace NetworkPolicy that restricts egress to the asset registry and internal DNS. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Create a Deployment with nginx:alpine, mount a PVC at /usr/share/nginx/html, and an InitContainer that downloads assets from the internal registry based on site-config (INDEX_FILE, THEME) writing into","explanation":"## Why This Is Asked\nTests ability to compose CKAD primitives: ConfigMaps for runtime knobs, Secrets for credentials, InitContainers for bootstrapping assets, PVCs for content, and NetworkPolicy for egress control, all in a per-namespace scope.\n\n## Key Concepts\n- InitContainers, ConfigMaps, Secrets, PVCs\n- Liveness/Readiness probes\n- NetworkPolicy, egress controls\n- Static site hosting with nginx\n\n## Code Example\n```yaml\n# Minimal Deployment skeleton (conceptual)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: static-site\nspec:\n  template:\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        volumeMounts:\n        - mountPath: /usr/share/nginx/html\n          name: html\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n      initContainers:\n      - name: fetch-assets\n        image: appropriate/cull\n        # placeholder for fetch logic using site-config\n        volumeMounts:\n        - mountPath: /html\n          name: html\n      volumes:\n      - name: html\n        persistentVolumeClaim:\n          claimName: site-pvc\n      # ... Secrets/ConfigMaps mounted as envs or files\n```\n\n## Follow-up Questions\n- How would you handle asset updates without redeploying pods?\n- What are the security implications of mounting secrets vs. env vars in this setup?","diagram":"flowchart TD\n  A[ConfigMap site-config] --> B[InitContainer downloads assets]\n  B --> C[HTML content in PVC mounted at /usr/share/nginx/html]\n  C --> D[Nginx serves content]\n  A --> E[Secret site-basic-auth for HTTP auth]","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","DoorDash","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T05:50:44.714Z","createdAt":"2026-01-20T05:50:44.714Z"},{"id":"q-4791","question":"CKAD intermediate: In namespace prod, Deployment payments-api with 4 replicas uses a vendor-base image that runs as root. Provide a minimal patch to enforce a strict security posture: container securityContext (runAsNonRoot: true, runAsUser: 1000, readOnlyRootFilesystem: true, allowPrivilegeEscalation: false, capabilities: drop: ['ALL']); PodSecurityContext (fsGroup: 2000); and include imagePullSecrets if the registry is private. Explain how you would validate under load and rollback if issues arise?","answer":"Patch the Deployment payments-api in prod to add a pod securityContext and container securityContext enforcing non-root operation, read-only root FS, and dropped capabilities; e.g.: RunAsNonRoot: true","explanation":"## Why This Is Asked\nThis checks practical hardening and CKAD adherence under real workflows, not theory. It spans securityContext, PodSecurityContext, imagePullSecrets, and rollback strategies.\n\n## Key Concepts\n- PodSecurityContext and containerSecurityContext\n- runAsNonRoot, runAsUser, readOnlyRootFilesystem, capabilities\n- imagePullSecrets and rollout rollback\n- Load testing and safe rollback procedures\n\n## Code Example\n```yaml\n# patch example (conceptual)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: payments-api\n  namespace: prod\nspec:\n  template:\n    spec:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 1000\n        fsGroup: 2000\n        readOnlyRootFilesystem: true\n      containers:\n      - name: payments-api\n        image: vendor/payments-api:latest\n        securityContext:\n          allowPrivilegeEscalation: false\n          runAsNonRoot: true\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop: ['ALL']\n```\n\n## Follow-up Questions\n- How would you adapt if the vendor image requires a non-root user different from 1000?\n- What metrics and rollback threshold would you use to safely roll this out across clusters?","diagram":"flowchart TD\n  A[Identify risk] --> B[Apply patch]\n  B --> C[Validate in staging]\n  C --> D[Rollout to prod]\n  D --> E[Monitor & rollback if issues]","difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Scale Ai","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T13:13:20.529Z","createdAt":"2026-01-20T13:13:20.529Z"},{"id":"q-4932","question":"In namespace payments, Deployment 'invoice-processor' (3 replicas) uses a ConfigMap and Secret to call an external invoicing API. A rolling upgrade to invoice-processor:2.1 fails health checks under load due to slow startup. Provide a minimal patch to: (1) add a startupProbe and adjust the readinessProbe, (2) add an initContainer to pre-warm a local cache from the ConfigMap, (3) add a PodDisruptionBudget minAvailable: 2. Explain verification under load and rollback steps?","answer":"Patch: add startupProbe httpGet:/healthz:8080 with timeout 2, period 5, failureThreshold 12; set readinessProbe initialDelaySeconds 15, timeoutSeconds 2, periodSeconds 5; add initContainer that fetche","explanation":"## Why This Is Asked\n\nTests practical CKAD debugging: handling slow-start upgrades with probes, init containers for bootstrapping state, and PD&B safeguards to prevent downtime during maintenance.\n\n## Key Concepts\n\n- startupProbe and readinessProbe tuning\n- initContainer for pre-warming caches\n- PodDisruptionBudget for upgrade safety\n- RollingUpdate strategy and rollback procedures\n\n## Code Example\n\n```yaml\n# Minimal patch sketch (Deployment snippet)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: invoice-processor\n  namespace: payments\nspec:\n  replicas: 3\n  template:\n    spec:\n      initContainers:\n      - name: init-cache\n        image: invoice-processor:2.1\n        command: [\"sh\", \"-c\", \"cp /config/* /var/cache/ || true\"]\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        - name: cache\n          mountPath: /var/cache\n      containers:\n      - name: app\n        image: invoice-processor:2.1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n      volumes:\n      - name: config\n        secret:\n          secretName: app-secret\n      - name: cache\n        emptyDir: {}\n```\n\n## Follow-up Questions\n\n- How would you adapt this for a cluster with limited CPU headroom?\n- What metrics would you monitor to confirm startup health and cache warmth?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Salesforce","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T20:22:29.820Z","createdAt":"2026-01-20T20:22:29.820Z"},{"id":"q-4982","question":"In CKAD terms, design a per-namespace caching API deployment that exposes a small REST service on port 8080 and serves cached responses to other pods. It reads CACHE_SIZE and TIMEOUT from a ConfigMap named service-config, and a backend API key from Secret named backend-credentials. Include InitContainer to install dependencies, a Sidecar for TLS cert rotation, readiness/liveness probes, and a Namespace NetworkPolicy restricting egress to backend.svc.cluster.local:8443. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Design a per-namespace Deployment named cache-api that serves on port 8080, utilizing a ConfigMap for CACHE_SIZE and TIMEOUT configuration and a Secret for API_KEY credentials. An InitContainer handles dependency installation, while a sidecar manages TLS certificate rotation. Include readiness and liveness probes for health monitoring, and implement a NetworkPolicy restricting egress traffic to backend.svc.cluster.local:8443.","explanation":"## Why This Is Asked\nTests CKAD fundamentals for multi-component deployments: ConfigMaps/Secrets, InitContainers, sidecars, probes, and namespace-scoped NetworkPolicies in a practical caching service scenario.\n\n## Key Concepts\n- ConfigMap/Secret integration for runtime configuration and credentials\n- InitContainer for bootstrapping dependencies\n- Sidecar pattern for TLS certificate rotation\n- Readiness/Liveness probes for service health monitoring\n- Namespace NetworkPolicy controlling egress traffic\n- Minimal YAML skeleton suitable for live CKAD exam tasks\n\n## Code Example\n```yaml\n<minimal Dep","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Snap","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:52:05.970Z","createdAt":"2026-01-20T22:35:13.844Z"},{"id":"q-5074","question":"Design a per-namespace CKAD-ready Deployment that acts as a synthetic feature-flag tester: it reads a list of feature endpoints from a Namespace ConfigMap named feature-endpoints, and an API_TOKEN from a Secret. InitContainer installs httpie. The Pod pings each endpoint at a configurable interval, writes per-endpoint results to /logs on a PVC, and updates a last-run summary in feat-test-status ConfigMap. Expose /metrics with per-feature latency; include readiness/liveness probes and a CPU-based HPA. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Design a per-namespace CKAD-ready Deployment that acts as a synthetic feature-flag tester: it reads a list of feature endpoints from a Namespace ConfigMap named feature-endpoints, and an API_TOKEN fro","explanation":"Why This Is Asked\n- Tests ability to wire ConfigMaps and Secrets across a namespace, plus InitContainer usage and lifecycle hooks.\n- Evaluates observability via a metrics endpoint and logs on a PVC, with a simple autoscale path via HPA.\n- Probes, resource requests, and minimal skeletons show readiness for real-world CKAD constraints.\n\nKey Concepts\n- ConfigMap/Secret integration, InitContainer tooling, PVC-backed logs, custom metrics exposure, readiness/liveness, HPA kinship, minimal YAML.\n\nCode Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: feature-tester\n  namespace: test-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: feature-tester\n  template:\n    metadata:\n      labels:\n        app: feature-tester\n    spec:\n      initContainers:\n      - name: init-tools\n        image: alpine:3.18\n        command: [\"sh\", \"-c\", \"apk add --no-cache httpie jq && mkdir -p /logs\"]\n        volumeMounts:\n        - name: logs\n          mountPath: /logs\n      containers:\n      - name: tester\n        image: alpine:3.18\n        command: [\"sh\", \"-c\", \"while true; do for url in $(cat /etc/feature-endpoints/endpoints); do resp=$(http --ignore-stdin GET \"$url\" Authorization:\"Bearer $(cat /secret/api_token)\"); echo \"$url -> $resp\" >> /logs/$(date +%s).log; done; sleep 60; done\"]\n        env:\n        - name: ENDPOINTS_CONFIG\n          value: \"/etc/feature-endpoints/endpoints\"\n        - name: TOKEN_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: api-token\n              key: token\n        volumeMounts:\n        - name: logs\n          mountPath: /logs\n        ports:\n        - containerPort: 8080\n          name: metrics\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n          limits:\n            cpu: \"500m\"\n            memory: \"256Mi\"\n        volumeMounts:\n        - name: logs\n          mountPath: /logs\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: feature-tester-logs\n      - name: feature-endpoints\n        configMap:\n          name: feature-endpoints\n      - name: secret\n        secret:\n          secretName: api-token\n      imagePullSecrets:\n      - name: regcred\n  \n# Note: This is a minimal skeleton; real manifests should separate concerns (InitContainer, metrics, status storage) and handle errors gracefully.\n```","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Robinhood","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T04:34:48.297Z","createdAt":"2026-01-21T04:34:48.297Z"},{"id":"q-5116","question":"CKAD intermediate: In namespace inventory, a Deployment stock-service with 3 replicas mounts an emptyDir at /data and relies on a ConfigMap stock-config and a Secret db-credentials. Implement an InitContainer that waits for a Postgres DB at postgresql.inventory.svc:5432 and seeds data from /seed/init.sql (mounted from a ConfigMap). Ensure InitContainer runs before the app, add readiness and liveness probes for /health, and provide a minimal manifest patch plus the steps to validate under load and rollback?","answer":"Add an InitContainer named seed-db to the stock-service Deployment that waits for postgresql.inventory.svc:5432 and runs psql -f /seed/init.sql using credentials from Secret db-credentials; mount /see","explanation":"## Why This Is Asked\nTests ability to coordinate InitContainers with main containers, use ConfigMap/Secret mounted data, and implement health checks for reliable startups.\n\n## Key Concepts\n- InitContainers sequencing\n- ConfigMap/Secret mounts\n- Probes for startup health\n- Rollback and load testing\n\n## Code Example\n```yaml\n# fragment patch adding initContainer and volumes\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: stock-service\n  namespace: inventory\nspec:\n  template:\n    spec:\n      initContainers:\n      - name: seed-db\n        image: postgres:15\n        command:\n        - bash\n        - -lc\n        - until pg_isready -h postgresql.inventory.svc -p 5432; do sleep 2; done; psql -h postgresql.inventory.svc -U dbuser -d inventory -f /seed/init.sql\n        env:\n        - name: PGPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-credentials\n              key: password\n        volumeMounts:\n        - name: seed-files\n          mountPath: /seed\n      containers:\n      - name: stock-service\n        image: my-reg/stock-service:latest\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n      volumes:\n      - name: seed-files\n        configMap:\n          name: stock-seed-config\n```\n\n## Follow-up Questions\n- How would you ensure idempotent seeds across restarts?\n- How would you scale this pattern for multiple environments?\n","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Airbnb","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T07:03:38.938Z","createdAt":"2026-01-21T07:03:38.938Z"},{"id":"q-858","question":"In a Kubernetes CKAD scenario, you have a Deployment named 'web-app' in namespace 'prod' with 3 replicas; pods frequently OOMKilled under load. Describe a practical debugging plan and provide a minimal manifest patch showing resource requests/limits, a readiness probe, and a liveness probe. Include scaling considerations and how you'd validate the fix under load?","answer":"First, inspect recent pod events and previous logs to confirm OOMKilled, then verify container resources and limits. Set requests/limits (e.g., 500m CPU, 512Mi memory; limit 1Gi). Add a readiness prob","explanation":"## Why This Is Asked\nCKAD candidates must diagnose real issues with limited tools. This tests debugging flow, resource tuning, and readiness/liveness strategies.\n\n## Key Concepts\n- OOMKilled diagnosis through pod events and logs\n- Resource requests/limits tuning and safety margins\n- Probes (readiness and liveness) to recover from bad states\n\n## Code Example\n````yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n  namespace: prod\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: app\n        image: myrepo/web-app:latest\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"1000m\"\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n````\n\n## Follow-up Questions\n- How would you validate changes in a staging environment before production?\n- What trade-offs exist between higher requests/limits and cluster density?","diagram":"flowchart TD\n  A[Pod Events] --> B{OOMKilled?}\n  B -->|Yes| C[Inspect Resources & Probes]\n  B -->|No| D[Monitor under load]\n  C --> E[Adjust requests/limits & Probes]\n  E --> F[Rollout restart]\n  F --> G[Validate under load]","difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","LinkedIn","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:40:43.301Z","createdAt":"2026-01-12T13:40:43.301Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":57,"beginner":19,"intermediate":21,"advanced":17,"newThisWeek":44}}