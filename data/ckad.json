{"questions":[{"id":"ckad-app-deployment-1768221857337-0","question":"You are updating a 3-replica stateless web app and require zero-downtime during rollouts. Which Deployment strategy configuration best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"strategy: type: Recreate\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"strategy: type: RollingUpdate, rollingUpdate: maxUnavailable: 0, maxSurge: 1\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"strategy: type: RollingUpdate, rollingUpdate: maxUnavailable: 1, maxSurge: 0\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"strategy: type: RollingUpdate, rollingUpdate: maxUnavailable: 2, maxSurge: 0\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is B because RollingUpdate with maxUnavailable: 0 and maxSurge: 1 ensures at most one extra pod is created during the rollout and no pod becomes unavailable, achieving zero-downtime updates.\n\n## Why Other Options Are Wrong\n- Option A: Recreate terminates all old pods before new ones are started, causing downtime.\n- Option C: maxUnavailable: 1 can momentarily reduce available replicas during the rollout.\n- Option D: maxUnavailable: 2 allows two pods to be unavailable simultaneously, increasing risk of downtime.\n\n## Key Concepts\n- RollingUpdate strategy\n- maxUnavailable and maxSurge controls\n- Zero-downtime deployment practices\n\n## Real-World Application\n- Use this pattern in customer-facing services during CI/CD deployments to minimize user impact while rolling out updates.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","kubectl","Deployment","RollingUpdate","EKS","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T12:44:17.338Z","createdAt":"2026-01-12 12:44:17"},{"id":"ckad-app-deployment-1768221857337-1","question":"You have a Deployment that consumes configuration from a ConfigMap. You want updates to the ConfigMap to be automatically visible in the running pods without restarting them. Which approach achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Populate container environment variables directly from ConfigMap keys using envFrom\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Mount the ConfigMap as a volume and read values from the mounted files\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a Secret instead of ConfigMap to reflect changes automatically\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Inline the values in the Pod spec\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because mounting the ConfigMap as a volume exposes the config as files inside the container. When the ConfigMap changes, the volume contents update automatically in the running Pod, without requiring a restart. Environment variables sourced from a ConfigMap are fixed at container startup and do not refresh.\n\n## Why Other Options Are Wrong\n- Option A: EnvFrom creates environment variables at startup; updates to the ConfigMap do not propagate to running containers.\n- Option C: Secrets rotate with explicit restarts and are not automatically updated in mounted form like a ConfigMap volume.\n- Option D: Inlining values makes configuration immutable at pod creation time and cannot reflect dynamic changes.\n\n## Key Concepts\n- ConfigMap volumes vs environment variables\n- Live-updating configuration via mounted ConfigMap\n- Pod lifecycle considerations when config changes\n\n## Real-World Application\n- For dynamic configuration in microservices, prefer ConfigMap volumes so operators can update config without recreating pods; plan a rollout or automated restart if needed for drift guarantees.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","ConfigMap","Volumes","kubectl","EKS","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T12:44:17.833Z","createdAt":"2026-01-12 12:44:18"},{"id":"ckad-app-deployment-1768221857337-2","question":"Your app exposes /healthz on port 8080 and you want readiness to prevent traffic to unhealthy pods until ready. Which readinessProbe configuration matches the required settings: path /healthz, port 8080, initialDelaySeconds 15, periodSeconds 10, failureThreshold 3?","answer":"[{\"id\":\"a\",\"text\":\"readinessProbe: httpGet: path: /healthz, port: 8080, initialDelaySeconds: 15, periodSeconds: 10, failureThreshold: 3\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"readinessProbe: httpGet: path: /healthz, port: 8080, initialDelaySeconds: 15, periodSeconds: 5, failureThreshold: 3\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"readinessProbe: httpGet: path: /health, port: 8080, initialDelaySeconds: 15, periodSeconds: 10, failureThreshold: 3\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"readinessProbe: tcpSocket: port: 8080, initialDelaySeconds: 15, periodSeconds: 10, failureThreshold: 3\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because it precisely matches the specified readiness probe: an HTTP GET to /healthz on port 8080 with an initial delay of 15 seconds, a 10-second period, and a failure threshold of 3.\n\n## Why Other Options Are Wrong\n- Option B: Period is 5 seconds, which does not meet the 10-second requirement.\n- Option C: Path is /health, which is not the expected endpoint.\n- Option D: Uses a TCP probe instead of HTTP GET, which does not check the HTTP readiness endpoint.\n\n## Key Concepts\n- Readiness probes vs liveness probes\n- HTTP GET probes and endpoint selection\n- Timing controls: initialDelaySeconds, periodSeconds, failureThreshold\n\n## Real-World Application\n- Correct readiness probing prevents routing traffic to unready pods during startup or transient failures, improving user experience and stability during deployments.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","ReadinessProbe","HTTP","kubectl","Deployment","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T12:44:18.298Z","createdAt":"2026-01-12 12:44:18"},{"id":"ckad-app-design-1768193364551-0","question":"You are deploying a web app with a Deployment of 3 replicas. You want updates to roll out without any downtime. Which Deployment strategy configuration achieves zero downtime during rolling updates?","answer":"[{\"id\":\"a\",\"text\":\"maxUnavailable=0, maxSurge=1\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"maxUnavailable=1, maxSurge=1\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"maxUnavailable=0, maxSurge=0\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"strategy: Recreate\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption A is correct because rolling updates configured with maxUnavailable: 0 and maxSurge: 1 allow updating one extra pod while keeping all existing pods available. This avoids any downtime during the rollout.\n\n## Why Other Options Are Wrong\n\n- Option B may cause temporary unavailability because one pod is taken down while another starts up.\n- Option C with maxUnavailable: 0 and maxSurge: 0 cannot deploy new pods to replace old ones, leading to stalled updates and potential downtime.\n- Option D uses the Recreate strategy which stops all pods before starting new ones, causing downtime.\n\n## Key Concepts\n\n- Kubernetes Deployment\n- RollingUpdate strategy\n- maxUnavailable\n- maxSurge\n\n## Real-World Application\n\nUse this pattern when deploying user-facing web services that require continuous availability during deployments; pair with readiness probes to ensure traffic only reaches healthy pods.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Deployment","RollingUpdate","zero-downtime","CKAD","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-design","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:49:24.553Z","createdAt":"2026-01-12 04:49:24"},{"id":"ckad-app-design-1768193364551-1","question":"Your team wants to terminate TLS at the Ingress controller for a service exposed by a Kubernetes Ingress. Which combination of resources is required to achieve this in a typical setup?","answer":"[{\"id\":\"a\",\"text\":\"Deployment, Service, and Ingress with TLS secret in the same namespace\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Deployment, ConfigMap, and Ingress\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"StatefulSet, Ingress, and a TLS secret in a different namespace\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"DaemonSet with TLS secret only\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption A is correct because TLS termination at the Ingress requires a TLS secret in the same namespace and a Service exposing the pods you want to reach; the Ingress references the secret to terminate TLS and routes traffic to the backend Service.\n\n## Why Other Options Are Wrong\n\n- Option B lacks a TLS secret; simply using a Deployment and ConfigMap does not enable TLS termination.\n- Option C places the TLS secret in a different namespace; TLS secrets must be in the same namespace as the Ingress to be used.\n- Option D omits the necessary Ingress and Service wiring; only a TLS secret won't expose the workload.\n\n## Key Concepts\n\n- Kubernetes Ingress\n- TLS secrets\n- Ingress Controller\n- Service backends\n\n## Real-World Application\n\nIn production, create a TLS secret (e.g., tls-secret) in the app's namespace, configure Ingress with tls:\n  - hosts: [example.com]\n    secretName: tls-secret\nand expose the app via a ClusterIP Service; this enables end-to-end TLS termination at the Ingress layer.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Ingress","TLS","Secrets","Networking","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-design","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:49:24.966Z","createdAt":"2026-01-12 04:49:25"},{"id":"ckad-app-design-1768193364551-2","question":"You have a microservice that writes user-uploaded files to disk and you want the data to persist beyond the life of any single Pod and be accessible to all replicas. Which Kubernetes resource is appropriate to attach to the Pod to provide persistent storage?","answer":"[{\"id\":\"a\",\"text\":\"EmptyDir\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"ConfigMap\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"PersistentVolumeClaim\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"HostPath\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption C is correct because PersistentVolumeClaims provide durable storage that outlives individual Pods and can be attached to multiple Pods when using a matching StorageClass or access mode.\n\n## Why Other Options Are Wrong\n\n- Option A (EmptyDir) is ephemeral and vanishes when the Pod is deleted.\n- Option B (ConfigMap) is for configuration data, not for file storage persistence.\n- Option D (HostPath) ties storage to a specific node and is not portable across nodes.\n\n## Key Concepts\n\n- PersistentVolume\n- PersistentVolumeClaim\n- StorageClass and access modes\n\n## Real-World Application\n\nUse a PVC for user-upload directories to ensure data persists across Pod rescheduling and is accessible to all replicas behind a Service.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","PersistentVolume","PVC","Storage","CKAD","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-design","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:49:25.352Z","createdAt":"2026-01-12 04:49:25"},{"id":"ckad-app-environment-1768151848059-0","question":"Which approach protects sensitive data by avoiding passing the secret as an environment variable, while still allowing the application to read the secret?","answer":"[{\"id\":\"a\",\"text\":\"Mount the Secret as files in a volume and have the application read the password from the mounted file\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Continue to inject the Secret as an environment variable\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Copy the Secret into the container image\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store the Secret in a ConfigMap and read it from the application\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\n- a\n\n## Why Other Options Are Wrong\n\n- b: Exposing secrets in environment variables makes them visible in process lists and logs, violating best practices for secret handling.\n- c: Copying secrets into the image risks leakage across image layers and in registries.\n- d: ConfigMaps are intended for non-sensitive configuration data; secrets should use Secret objects, and storing them in a ConfigMap is not secure.\n\n## Key Concepts\n\n- Kubernetes Secrets vs ConfigMaps\n- Secrets mounted as files vs as environment variables\n- Secret-handling best practices in Pod specs\n\n## Real-World Application\n\n- In production, mount sensitive credentials as files from Secrets and configure apps to read from file paths, reducing exposure in the process environment.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Secrets","Security","CKAD","certification-mcq","domain-weight-25"],"channel":"ckad","subChannel":"app-environment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:17:28.060Z","createdAt":"2026-01-11 17:17:28"},{"id":"ckad-app-environment-1768151848059-1","question":"To securely supply a TLS certificate and its private key to a Pod, which approach is correct?","answer":"[{\"id\":\"a\",\"text\":\"Mount the TLS certificate and private key from a Secret as files into the container\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Pass the certificate and key as environment variables to the container\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store the certificate in a ConfigMap and mount as files\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Bake the certificate into the container image\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\n- a\n\n## Why Other Options Are Wrong\n\n- b: Exposing private keys via environment variables risks leakage through process listings and logs.\n- c: ConfigMaps are not intended for sensitive data; Secrets should be used for TLS material.\n- d: Including certs in the image makes rotation and revocation harder and risks distribution of secret material.\n\n## Key Concepts\n\n- Kubernetes Secrets for TLS material\n- Mounting Secrets as files vs exposing via env vars\n- Secret rotation considerations\n\n## Real-World Application\n\n- Use Secrets to store TLS certs/keys and mount them as files at standard paths for TLS-enabled services; enables safer rotation and access control.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","TLS","Secrets","CKAD","certification-mcq","domain-weight-25"],"channel":"ckad","subChannel":"app-environment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:17:28.472Z","createdAt":"2026-01-11 17:17:28"},{"id":"ckad-app-environment-1768151848059-2","question":"Which Pod spec snippet properly ensures a container runs as a non-root user and cannot escalate privileges, while also enforcing read-only root filesystem?","answer":"[{\"id\":\"a\",\"text\":\"securityContext: { runAsNonRoot: true, allowPrivilegeEscalation: false, readOnlyRootFilesystem: true }\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"securityContext: { runAsUser: 0 }\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"securityContext: { allowPrivilegeEscalation: true }\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"securityContext: { privileged: true }\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\n- a\n\n## Why Other Options Are Wrong\n\n- b: Running as UID 0 means the container is effectively root, defeating non-root enforcement.\n- c: AllowPrivilegeEscalation: true allows privilege escalation.\n- d: Privileged: true grants extensive host privileges and defeats isolation.\n\n## Key Concepts\n\n- Pod securityContext basics\n- runAsNonRoot, readOnlyRootFilesystem, and privilegeEscalation controls\n- Privilege separation in containers\n\n## Real-World Application\n\n- Applying these settings reduces attack surface by preventing root access and restricting file system writes, aligning with production-hardening practices.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Security","PodSecurity","CKAD","certification-mcq","domain-weight-25"],"channel":"ckad","subChannel":"app-environment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:17:28.886Z","createdAt":"2026-01-11 17:17:28"},{"id":"q-858","question":"In a Kubernetes CKAD scenario, you have a Deployment named 'web-app' in namespace 'prod' with 3 replicas; pods frequently OOMKilled under load. Describe a practical debugging plan and provide a minimal manifest patch showing resource requests/limits, a readiness probe, and a liveness probe. Include scaling considerations and how you'd validate the fix under load?","answer":"First, inspect recent pod events and previous logs to confirm OOMKilled, then verify container resources and limits. Set requests/limits (e.g., 500m CPU, 512Mi memory; limit 1Gi). Add a readiness prob","explanation":"## Why This Is Asked\nCKAD candidates must diagnose real issues with limited tools. This tests debugging flow, resource tuning, and readiness/liveness strategies.\n\n## Key Concepts\n- OOMKilled diagnosis through pod events and logs\n- Resource requests/limits tuning and safety margins\n- Probes (readiness and liveness) to recover from bad states\n\n## Code Example\n````yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n  namespace: prod\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: app\n        image: myrepo/web-app:latest\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"1000m\"\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n````\n\n## Follow-up Questions\n- How would you validate changes in a staging environment before production?\n- What trade-offs exist between higher requests/limits and cluster density?","diagram":"flowchart TD\n  A[Pod Events] --> B{OOMKilled?}\n  B -->|Yes| C[Inspect Resources & Probes]\n  B -->|No| D[Monitor under load]\n  C --> E[Adjust requests/limits & Probes]\n  E --> F[Rollout restart]\n  F --> G[Validate under load]","difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","LinkedIn","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:40:43.301Z","createdAt":"2026-01-12T13:40:43.301Z"},{"id":"ckad-services-networking-1768231917982-0","question":"To provide stable DNS names per pod for a stateful application and enable direct addressing by name within the cluster, which combination would you use?","answer":"[{\"id\":\"a\",\"text\":\"StatefulSet with a headless service (clusterIP: None)\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Deployment with a ClusterIP service\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"DaemonSet with a NodePort service\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"ReplicaSet with a standard ClusterIP service\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is A because a StatefulSet combined with a headless service (clusterIP: None) yields stable network identities and DNS per pod (e.g., web-0, web-1). The headless service returns per-pod DNS A records, enabling direct addressing.\n\n## Why Other Options Are Wrong\n- Option B is incorrect because a standard ClusterIP service from a Deployment provides a single virtual IP, not per-pod DNS identities.\n- Option C is incorrect; a DaemonSet with NodePort yields disparate nodes and no stable per-pod DNS identity.\n- Option D is incorrect because a ReplicaSet with a standard ClusterIP service also uses a single service IP, not stable per-pod DNS names.\n\n## Key Concepts\n- StatefulSet and headless service (clusterIP: None)\n- Pod DNS naming and per-pod addressing\n- Service DNS resolution within a cluster\n\n## Real-World Application\n- Useful for apps requiring stable network identities across restarts, such as databases or clustered services that rely on per-pod DNS names for replication.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Networking","StatefulSet","DNS","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"services-networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:31:57.984Z","createdAt":"2026-01-12 15:31:58"},{"id":"ckad-services-networking-1768231917982-1","question":"Your frontend pods in namespace frontend must only talk to the backend API in namespace backend on port 8080, and not access other namespaces. Which resource enforces this rule?","answer":"[{\"id\":\"a\",\"text\":\"A NetworkPolicy in the frontend namespace allowing egress to pods in backend on port 8080\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"PodSecurityPolicy\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"ResourceQuota\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Service\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is A because a NetworkPolicy scoped to the frontend namespace can restrict egress to only the backend pods on port 8080 (assuming a default deny behavior is in place). This enforces cross-namespace communication policies.\n\n## Why Other Options Are Wrong\n- Option B is incorrect because PodSecurityPolicy controls pod security contexts, not network access between namespaces.\n- Option C is incorrect because ResourceQuota restricts resource usage, not network traffic.\n- Option D is incorrect because a Service abstracts access to a set of pods but does not enforce cross-namespace egress restrictions.\n\n## Key Concepts\n- NetworkPolicy scope per namespace\n- Egress rules and default-deny patterns\n- Namespace-based isolation\n\n## Real-World Application\n- Helps enforce microservice boundaries and reduce blast-radius in production environments.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Networking","NetworkPolicy","Namespace","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"services-networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:31:58.337Z","createdAt":"2026-01-12 15:31:58"},{"id":"ckad-services-networking-1768231917982-2","question":"To balance requests across pods and maintain client session affinity, which Service configuration is required?","answer":"[{\"id\":\"a\",\"text\":\"spec.sessionAffinity: ClientIP in the Service\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"StatefulSet with a Headless Service\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"DaemonSet with a NodePort service\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"ExternalName service\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is A because setting spec.sessionAffinity to ClientIP makes the Service route a client's requests to the same pod, providing simple session stickiness across requests.\n\n## Why Other Options Are Wrong\n- Option B offers stable per-pod identities but does not provide load-balanced session affinity.\n- Option C is unrelated to service-level load balancing.\n- Option D maps to an external name and does not provide internal session affinity.\n\n## Key Concepts\n- Service sessionAffinity (ClientIP)\n- Basic internal load balancing behavior\n- Pod distribution across a Service\n\n## Real-World Application\n- Maintains consistent user sessions for login or cart state without external session stores.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Networking","Service","LoadBalancer","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"services-networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:31:58.683Z","createdAt":"2026-01-12 15:31:58"},{"id":"ckad-services-networking-1768231917982-3","question":"You want to expose a service externally with TLS termination at the Ingress controller. Which Kubernetes object defines the routing rules and TLS configuration?","answer":"[{\"id\":\"a\",\"text\":\"Ingress\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Service\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Endpoint\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"ConfigMap\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is A because the Ingress resource defines host/path routing rules and TLS configurations, and TLS termination is typically performed by an Ingress Controller.\n\n## Why Other Options Are Wrong\n- Option B describes a basic Service, which does not define TLS termination or routing rules to external traffic.\n- Option C Endpoints represent the actual pods backing a Service, not routing rules.\n- Option D ConfigMap stores configuration data, not routing or TLS settings for external access.\n\n## Key Concepts\n- Ingress resource and TLS configuration\n- Ingress Controller role\n- External exposure of services\n\n## Real-World Application\n- Enables HTTPS termination and central routing for multiple services behind a single entry point.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Networking","Ingress","TLS","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"services-networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:31:58.811Z","createdAt":"2026-01-12 15:31:58"},{"id":"ckad-services-networking-1768231917982-4","question":"From a pod in namespace frontend, you want to call the billing service in namespace billing; what is the fully qualified DNS name you should use for cross-namespace service discovery?","answer":"[{\"id\":\"a\",\"text\":\"billing.billing.svc.cluster.local\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"billing.svc.cluster.local\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"billing.frontend.svc.cluster.local\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"billing.cluster.local\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is A because cross-namespace service discovery uses the pattern <service>.<namespace>.svc.cluster.local, so billing.billing.svc.cluster.local resolves to the billing service in the billing namespace.\n\n## Why Other Options Are Wrong\n- Option B omits the namespace, which is required for cross-namespace resolution.\n- Option C incorrectly uses the frontend namespace in the DNS name.\n- Option D is not a valid cluster DNS name for a service.\n\n## Key Concepts\n- Kubernetes DNS hierarchy: <service>.<namespace>.svc.cluster.local\n- Service discovery across namespaces\n\n## Real-World Application\n- Enables clean service calls across multiple namespaces without hard-coding IPs.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","DNS","ServiceDiscovery","CrossNamespace","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"services-networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:31:58.937Z","createdAt":"2026-01-12 15:31:58"}],"subChannels":["app-deployment","app-design","app-environment","general","services-networking"],"companies":["Hugging Face","LinkedIn","Snowflake"],"stats":{"total":15,"beginner":0,"intermediate":15,"advanced":0,"newThisWeek":15}}