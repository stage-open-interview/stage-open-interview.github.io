{"questions":[{"id":"q-1066","question":"Inside namespace analytics, schedule a daily batch to process a CSV: use a CronJob (02:00 UTC) to start a Job that runs a Python script from a ConfigMap, reads input from a ConfigMap, uses a Secret for DB credentials to insert results into Postgres service, writes output to a PVC, runs as non-root with a readOnlyRootFilesystem, with resource limits, a backoffLimit of 3, and a 15-minute activeDeadlineSeconds; ensure proper probes?","answer":"Configure a CronJob in namespace analytics to launch a Job daily at 02:00 UTC. The Job runs a Python script sourced from a ConfigMap, reads input from another ConfigMap, uses a Secret for Postgres cre","explanation":"## Why This Is Asked\nThis problem tests advanced CKAD skills: combining CronJob and Job for batch work, wiring Script and data through ConfigMaps, securing credentials with Secrets, persisting output via PVCs, and enforcing pod security and lifecycle constraints. It also covers resource sizing, retries, timeouts, and basic health checks in a production-like scenario.\n\n## Key Concepts\n- CronJob, Job, PodSecurityContext\n- ConfigMap for scripts and input data\n- Secret for credentials\n- PersistentVolumeClaim for outputs\n- Probes, activeDeadlineSeconds, backoffLimit\n- runAsNonRoot, readOnlyRootFilesystem, resources\n\n## Code Example\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: daily-ingest\nspec:\n  schedule: '0 2 * * *'\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: ingest\n            image: myrepo/ingest:latest\n            volumeMounts:\n            - mountPath: /scripts\n              name: script\n          volumes:\n          - name: script\n            configMap:\n              name: ingest-scripts\n          restartPolicy: OnFailure\n```\n\n## Follow-up Questions\n- How would you test cron execution and idempotence?\n- How to rotate db credentials without redeploying the Job?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Microsoft","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:26:32.694Z","createdAt":"2026-01-12T21:26:32.694Z"},{"id":"q-1312","question":"In a Kubernetes CKAD scenario, you have a Deployment named 'web-server' in namespace 'prod' running a Node.js app. During rolling updates, some pods terminate and restart slowly, causing request timeouts. Outline concrete changes to implement startupProbe, adjust readiness and liveness probes, and add a preStop hook to drain existing connections. Provide a minimal manifest patch showing startupProbe, a readinessProbe, a livenessProbe, and a preStop lifecycle hook that ensures graceful shutdown. How would you approach this?","answer":"Implement startupProbe to delay liveness checks until the app is ready, tune readiness and liveness probes for fast detection and avoid unnecessary restarts, and add a preStop hook to signal graceful ","explanation":"## Why This Is Asked\n\nTests practical lifecycle management and how probes interact with rolling updates to avoid downtime.\n\n## Key Concepts\n\n- StartupProbe, readinessProbe, livenessProbe\n- preStop and terminationGracePeriodSeconds\n- Graceful shutdown patterns in Node.js apps\n\n## Code Example\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-server\n  namespace: prod\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: web-server\n        image: node:18\n        ports:\n        - containerPort: 3000\n        startupProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n          failureThreshold: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n          initialDelaySeconds: 30\n          periodSeconds: 15\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"bash\",\"-lc\",\"sleep 5\"]\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n          limits:\n            cpu: \"500m\"\n            memory: \"512Mi\"\n      terminationGracePeriodSeconds: 30\n```\n\n## Follow-up Questions\n\n- How would you adjust thresholds for busy traffic?\n- How would you verify graceful shutdown under load?\n- What pitfalls exist with preStop sleeps in container runtimes?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","MongoDB","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T10:37:57.488Z","createdAt":"2026-01-13T10:37:57.488Z"},{"id":"q-1352","question":"You have a small Python API app to run in Kubernetes. Write a minimal manifest that creates a Deployment with 3 replicas using image myregistry/api:1.0, a readinessProbe httpGet /health on port 8080, a livenessProbe with initialDelaySeconds 15 and periodSeconds 10, and a ClusterIP Service exposing port 8080. Inject config via ConfigMap app-config with LOG_LEVEL. How would you verify and how would you scale to 5 replicas?","answer":"Create Deployment with 3 replicas using image myregistry/api:1.0, add readinessProbe httpGet /health on 8080 and livenessProbe with initialDelaySeconds 15 and periodSeconds 10. Expose a ClusterIP Serv","explanation":"## Why This Is Asked\n\nThis tests converting a real-world requirement into Kubernetes manifests and basic operational steps, including probes and config injection.\n\n## Key Concepts\n\n- Deployment and ReplicaSet lifecycle\n- Readiness and liveness probes\n- ConfigMap-based configuration\n- Service exposure and ports\n\n## Code Example\n\n```javascript\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: myregistry/api:1.0\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: app-config\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 10\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: api-svc\nspec:\n  type: ClusterIP\n  selector:\n    app: api\n  ports:\n  - port: 8080\n    targetPort: 8080\n```\n\n## Follow-up Questions\n\n- How would you extend configuration with Secrets for sensitive data?\n- How would you monitor readiness/liveness in a multi-container pod?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T13:09:01.337Z","createdAt":"2026-01-13T13:09:01.337Z"},{"id":"q-1393","question":"Design a Kubernetes manifest that deploys a stateless app with 3 replicas, uses a ConfigMap and a Secret, attaches a 1Gi PVC for data, includes a HorizontalPodAutoscaler, exposes via ClusterIP, and enforces a NetworkPolicy restricting egress to api.internal.example.com:443. Provide YAML fragments and discuss trade-offs?","answer":"Deployment: 3 replicas; EnvFrom: ConfigMap and Secret; volume: 1Gi PVC mounted at /data; HPA: min 3, max 10, targetCPUUtilizationPercentage: 60; Service: ClusterIP exposing port 8080; NetworkPolicy: a","explanation":"## Why This Is Asked\nExplores practical CKAD competencies: deployment orchestration, config/secret management, volumes, autoscaling, service exposure, and network isolation.\n\n## Key Concepts\n- Deployment, PVC, ConfigMap/Secret, HPA\n- ClusterIP service semantics\n- NetworkPolicy gating and its limitations\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: app\n        image: my-app:latest\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: my-app-config\n        - secretRef:\n            name: my-app-creds\n        volumeMounts:\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: data-pvc\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-pvc\nspec:\n  accessModes: [\"ReadWriteOnce\"]\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: standard\n---\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: my-app-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: my-app\n  minReplicas: 3\n  maxReplicas: 10\n  targetCPUUtilizationPercentage: 60\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-app\nspec:\n  selector:\n    app: my-app\n  ports:\n  - port: 8080\n    targetPort: 8080\n  type: ClusterIP\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-egress-api\nspec:\n  podSelector:\n    matchLabels:\n      app: my-app\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - ipBlock:\n        cidr: 203.0.113.0/24\n    ports:\n    - protocol: TCP\n      port: 443\n```\n\n## Follow-up Questions\n- How would you test this in a cluster with a restricted CNI?\n- What changes if the app becomes stateful or requires dynamic PVC provisioning?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T14:53:41.455Z","createdAt":"2026-01-13T14:53:41.455Z"},{"id":"q-1459","question":"Create Kubernetes manifests for a simple API: Deployment using image 'my-api:1.0' that reads PORT from a ConfigMap via env, a ConfigMap with PORT and APP_MODE, readinessProbe and livenessProbe for /healthz on that port, and resource requests/limits. Expose with a Service on port 80 targeting 3000. Describe a rolling update plan?","answer":"Provide YAML manifests showing a Deployment that uses image my-api:1.0, reads PORT from a ConfigMap, includes a ConfigMap named api-config with PORT and APP_MODE, readiness and liveness probes for /he","explanation":"## Why This Is Asked\\nPractical CKAD tasks using ConfigMaps, env injection, probes, resources, and Service exposure. Demonstrates understanding of update strategies.\\n\\n## Key Concepts\\n- ConfigMap usage via env/ENV\\n- Probes: readiness and liveness\\n- Resource requests/limits\\n- Service exposure and port mapping\\n- Rolling updates and rollout verification\\n\\n## Code Example\\n\\n```javascript\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: api-config\\ndata:\\n  PORT: 3000\\n  APP_MODE: prod\\n\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: api\\nspec:\\n  replicas: 2\\n  selector:\\n    matchLabels:\\n      app: api\\ntemplate:\\n  metadata:\\n    labels:\\n      app: api\\n  spec:\\n    containers:\\n    - name: api\\n      image: my-api:1.0\\n      ports:\\n      - containerPort: 3000\\n      env:\\n      - name: PORT\\n        valueFrom:\\n          configMapKeyRef:\\n            name: api-config\\n            key: PORT\\n      readinessProbe:\\n        httpGet:\\n          path: /healthz\\n          port: 3000\\n        initialDelaySeconds: 5\\n        periodSeconds: 10\\n      livenessProbe:\\n        httpGet:\\n          path: /healthz\\n          port: 3000\\n        initialDelaySeconds: 15\\n        periodSeconds: 20\\n      resources:\\n        requests:\\n          cpu: 100m\\n          memory: 128Mi\\n        limits:\\n          cpu: 200m\\n          memory: 256Mi\\n\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: api-service\\nspec:\\n  selector:\\n    app: api\\n  ports:\\n  - port: 80\\n    targetPort: 3000\\n```\\n\\n## Follow-up Questions\\n- How would you reload config without pod restart?\\n- How do you verify a rolling update complete with zero downtime?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Meta","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T17:55:04.986Z","createdAt":"2026-01-13T17:55:04.986Z"},{"id":"q-1488","question":"Given a Deployment named 'image-processor' in namespace 'prod' with 6 replicas processing images from a Redis queue, design a practical patch to ensure graceful shutdown of in-flight tasks during rollouts, prevent simultaneous pod terminations, and maintain availability during node drains; include a minimal manifest patch adding a preStop script, terminationGracePeriodSeconds, readiness and liveness probes, and a PodDisruptionBudget targeting the deployment. What steps would you take to validate under load?","answer":"Patch deployment to add lifecycle preStop draining, a 90s terminationGracePeriodSeconds, readinessProbe /ready and livenessProbe /healthz, plus resource requests/limits; create PodDisruptionBudget wit","explanation":"## Why This Is Asked\nTests practical mastery of graceful rollouts, pod eviction safety, and CKAD patterns beyond basics.\n\n## Key Concepts\n- PreStop draining\n- terminationGracePeriodSeconds\n- Probes\n- PodDisruptionBudget\n- Node drains under load\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: image-processor\n  namespace: prod\nspec:\n  replicas: 6\n  template:\n    spec:\n      containers:\n      - name: image-processor\n        image: myimage\n        ports:\n        - containerPort: 8080\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"/bin/sh\",\"-c\",\"/usr/local/bin/drain-tasks.sh\"]\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 2\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n          timeoutSeconds: 2\n      terminationGracePeriodSeconds: 90\n---\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: image-processor-pdb\n  namespace: prod\nspec:\n  minAvailable: 4\n  selector:\n    matchLabels:\n      app: image-processor\n      tier: backend\n```\n\n## Follow-up Questions\n- How would you adapt for multiple queues or different backoff strategies?\n- How would you monitor backlog during drains?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Airbnb","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T19:00:31.030Z","createdAt":"2026-01-13T19:00:31.030Z"},{"id":"q-1547","question":"Configure a NetworkPolicy to restrict egress from prod/checkout-service to only reach payments/payment-processor in the payments namespace on port 443, blocking all other egress. Provide a minimal manifest patch and a test strategy to validate allowed and blocked traffic inside the cluster?","answer":"Implement a NetworkPolicy in the prod namespace that allows egress from pods labeled app=checkout-service to payments/payment-processor:443 only, while denying all other egress. Apply the minimal YAML patch and validate using a test pod.","explanation":"## Why This Is Asked\n\nThis task evaluates practical NetworkPolicy design, cross-namespace scoping, and cluster service DNS testing under real workloads.\n\n## Key Concepts\n\n- Kubernetes NetworkPolicy\n- namespaceSelector and podSelector\n- DNS resolution for cluster services\n- Egress testing with a dedicated test pod\n- Labeling prerequisites for policy scope\n\n## Code Example\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: checkout-egress-restrict\n  namespace: prod\nspec:\n  podSelector:\n    matchLabels:\n      app: checkout-service\n  policyTypes:\n  - Egress\n  egress:\n```","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:30:52.600Z","createdAt":"2026-01-13T21:36:32.747Z"},{"id":"q-1609","question":"You're deploying inventory-service on Kubernetes. Write YAML to (1) deploy 3 replicas with a ConfigMap for API_ENDPOINT and a Secret for DB_PASSWORD, (2) an InitContainer that runs migrations, (3) readiness and liveness probes, resource requests/limits, and a RollingUpdate strategy with maxUnavailable: 1, (4) a Job to run migrations before the first pod starts, (5) a sidecar log-shipper. Include the key fragments and rationale?","answer":"Provide a compact YAML snippet for a Deployment creating 3 replicas of inventory-service, sourcing API_ENDPOINT from a ConfigMap and DB_PASSWORD from a Secret, with an InitContainer running migrations","explanation":"## Why This Is Asked\n\nTests ability to compose Kubernetes primitives for real-world apps: Deployment, ConfigMap, Secret, InitContainer, Probes, Resources, and Jobs. It also probes rollout safety and migration orchestration.\n\n## Key Concepts\n\n- ConfigMap and Secret integration into Pods\n- InitContainer for pre-start tasks (migrations)\n- Liveness/readiness probes and resource management\n- RollingUpdate strategy with maxUnavailable\n- Separate Job for one-off migrations to ensure DB schema readiness\n\n## Code Example\n\n```javascript\nconst deployment = {\n  apiVersion: 'apps/v1',\n  kind: 'Deployment',\n  metadata: { name: 'inventory-service' },\n  spec: { /* ... */ }\n};\n```\n\n## Follow-up Questions\n\n- How would you implement a canary rollout with traffic splitting?\n- How would you validate migrations and rollback on failure?","diagram":"flowchart TD\n  A[Deployment: inventory-service] --> B[ConfigMap: API_ENDPOINT]\n  A --> C[Secret: DB_PASSWORD]\n  A --> D[InitContainer: migrations]\n  A --> E[Readiness/Liveness probes]\n  A --> F[RollingUpdate: maxUnavailable: 1]\n  G[Job: migrations] --> A","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","NVIDIA","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T02:35:11.694Z","createdAt":"2026-01-14T02:35:11.694Z"},{"id":"q-1626","question":"Blue/Green rollout for a CKAD production web service: in namespace 'prod', a Deployment 'web-app' with 3 replicas serves traffic via the Service 'web-app'. Introduce a canary path with a second Deployment 'web-app-canary' and a canary route (via canary Ingress or a second Service) to validate with 10–20% traffic before full switch. Provide a minimal manifest patch showing resource requests/limits, readinessProbe, and a livenessProbe for both deployments, plus how to switch traffic and rollback. Include validation steps under load?","answer":"Use a blue/green pattern: keep web-app as blue; deploy web-app-canary with 10–20% traffic behind the same Service via a canary route (Ingress canary or a second Service) and implement identical resour","explanation":"## Why This Is Asked\n\nTests practical rollout skills: blue/green pattern, traffic routing, probes, and quick rollback under load.\n\n## Key Concepts\n- Blue/Green deployment\n- Canary routing with minimal impact\n- Resource requests/limits, readinessProbe, livenessProbe\n- Service selector switching for traffic control\n\n## Code Example\n\n```yaml\n# Deployment patch for canary\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app-canary\n  namespace: prod\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n      - name: app\n        image: myorg/web-app:canary\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n          limits:\n            cpu: \"300m\"\n            memory: \"256Mi\"\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n```\n\n```yaml\n# Service patched for blue/green traffic routing\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-app\n  namespace: prod\nspec:\n  selector:\n    app: web-app-blue\n  ports:\n  - port: 80\n    targetPort: 8080\n```\n\n```yaml\n# Canary Service (optional, if using separate service for canary routing)\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-app-canary-svc\n  namespace: prod\nspec:\n  selector:\n    app: web-app-canary\n  ports:\n  - port: 80\n    targetPort: 8080\n```\n\n## Follow-up Questions\n- How would you automate switching traffic and rollback if issues appear?\n- How would you validate stability under a 50k RPS load during the canary phase?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","OpenAI","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T04:14:20.574Z","createdAt":"2026-01-14T04:14:20.576Z"},{"id":"q-1707","question":"Explain a progressive canary rollout for a 4-replica frontend behind an NGINX Ingress. Use a stable Deployment and a canary Deployment (image frontend:1.2-canary, replicas:1). Patch Ingress to route 10% to canary; later 50% and 100%. Include minimal YAML patches for deployments and a canary Ingress with canary-weight; how would you monitor health and rollback?","answer":"Implement a two-Deployment canary: stable frontend with 4 replicas and canary frontend-canary with 1 replica image frontend:1.2-canary. Create an Ingress canary rule with nginx.ingress.kubernetes.io/c","explanation":"## Why This Is Asked\nTests ability to design a safe, observable canary rollout using standard Kubernetes resources and NGINX Ingress without a service mesh.\n\n## Key Concepts\n- Canary deployment pattern with separate Deployments\n- Ingress annotations (nginx.ingress.kubernetes.io/canary, canary-weight)\n- Progressive traffic shifting and rollback strategy\n- Observability: latency, error rate, saturation\n\n## Code Example\n```yaml\n# Stable Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  namespace: prod\n  labels:\n    app: frontend\n    version: stable\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: frontend\n      version: stable\n  template:\n    metadata:\n      labels:\n        app: frontend\n        version: stable\n    spec:\n      containers:\n      - name: frontend\n        image: frontend:latest\n        ports:\n        - containerPort: 80\n```\n```yaml\n# Canary Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-canary\n  namespace: prod\n  labels:\n    app: frontend\n    version: canary\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: frontend\n      version: canary\n  template:\n    metadata:\n      labels:\n        app: frontend\n        version: canary\n    spec:\n      containers:\n      - name: frontend\n        image: frontend:1.2-canary\n        ports:\n        - containerPort: 80\n```\n```yaml\n# Ingress (production path)\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: frontend-ingress\n  namespace: prod\n  annotations:\n    # enable canary routing for the canary backend\n    nginx.ingress.kubernetes.io/canary: \"true\"\n    nginx.ingress.kubernetes.io/canary-weight: \"10\"\nspec:\n  rules:\n  - host: \"frontend.example.com\"\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: frontend-canary\n            port:\n              number: 80\n```\n\n## Follow-up Questions\n- How would you promote canary to 50% and then 100% safely?\n- What metrics would you monitor to decide progression or rollback?\n- How would you rollback if the canary fails without affecting stable users?","diagram":"flowchart TD\n  A[User Request] --> B[Ingress]\n  B --> C[Stable Backend (frontend)]\n  B --> D[Canary Route (frontend-canary)]\n  D --> E[Canary Pods]\n  C --> F[User Response]\n  E --> F","difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T07:42:45.036Z","createdAt":"2026-01-14T07:42:45.036Z"},{"id":"q-1767","question":"Scenario: In a CKAD scenario, you must roll out a new image for a stateless API 'inventory-api' in namespace 'prod' with 2% traffic to the canary, using vanilla Kubernetes (no service mesh). Outline a practical canary strategy with two Deployments and two Services, explain how you split traffic without a mesh, and provide minimal manifests for the canary Deployment and a canary-facing Service, plus a rollback plan and how you'd validate under load?","answer":"Two Deployments: stable and canary with distinct labels; two Services: inventory-svc (stable) and inventory-svc-canary (canary). Vanilla Kubernetes lacks built-in weighted routing, so use an Ingress controller with weighted backend configuration.\n\n## Strategy\n1. Deploy stable version (98% traffic)\n2. Deploy canary version (2% traffic) \n3. Use Ingress annotations for weighted routing\n4. Monitor and validate under load\n5. Rollback by updating Ingress weights\n\n## Core Manifests\n\n```yaml\n# Stable Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inventory-api-stable\n  namespace: prod\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: inventory-api\n      version: stable\n  template:\n    metadata:\n      labels:\n        app: inventory-api\n        version: stable\n    spec:\n      containers:\n      - name: inventory-api\n        image: repo/inventory-api:v1.0\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n\n---\n# Canary Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inventory-api-canary\n  namespace: prod\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: inventory-api\n      version: canary\n  template:\n    metadata:\n      labels:\n        app: inventory-api\n        version: canary\n    spec:\n      containers:\n      - name: inventory-api\n        image: repo/inventory-api:canary\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n\n---\n# Stable Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: inventory-svc-stable\n  namespace: prod\nspec:\n  selector:\n    app: inventory-api\n    version: stable\n  ports:\n  - port: 80\n    targetPort: 8080\n\n---\n# Canary Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: inventory-svc-canary\n  namespace: prod\nspec:\n  selector:\n    app: inventory-api\n    version: canary\n  ports:\n  - port: 80\n    targetPort: 8080\n\n---\n# Ingress with Weighted Routing\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: inventory-api-ingress\n  namespace: prod\n  annotations:\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTP\"\n    # NGINX Ingress weighted routing\n    nginx.ingress.kubernetes.io/canary: \"true\"\n    nginx.ingress.kubernetes.io/canary-weight: \"2\"\n    # Alternative: Traefik weighted service\n    # traefik.ingress.kubernetes.io/router.middlewares: \"canary-weight@kubernetescrd\"\nspec:\n  rules:\n  - host: inventory-api.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: inventory-svc-stable\n            port:\n              number: 80\n```\n\n## Rollback Plan\n1. **Immediate rollback**: Update Ingress annotation `nginx.ingress.kubernetes.io/canary-weight: \"0\"`\n2. **Full rollback**: Delete canary Deployment and Service\n3. **Rollback command**: `kubectl patch ingress inventory-api-ingress -n prod -p '{\"metadata\":{\"annotations\":{\"nginx.ingress.kubernetes.io/canary-weight\":\"0\"}}}'`\n\n## Validation Under Load\n```bash\n# Generate load with ApacheBench\nab -n 1000 -c 10 http://inventory-api.example.com/health\n\n# Monitor traffic split\nkubectl logs -n prod -l version=stable --tail=50\nkubectl logs -n prod -l version=canary --tail=50\n\n# Check metrics\nkubectl top pods -n prod --l version=canary\nkubectl get pods -n prod -l version=canary -w\n\n# Validate canary receives ~2% of requests\nfor i in {1..100}; do curl -s http://inventory-api.example.com/health | grep -q \"canary\" && echo \"Canary hit: $i\"; done\n```","explanation":"## Why This Is Asked\n\nTests practical canary workflows in vanilla Kubernetes, a common CKAD constraint requiring understanding of traffic splitting without service mesh.\n\n## Key Concepts\n\n- Canary rollout without service mesh using Ingress controllers\n- Dual deployments with version-specific labels and selectors\n- Ingress-based weighted routing annotations (NGINX/Traefik)\n- Service isolation for stable and canary versions\n- Rollback strategies using Ingress annotation updates\n- Load validation using traffic generation and monitoring\n\n## Implementation Details\n\nThe solution provides complete, production-ready manifests showing:\n1. **Stable deployment** with higher replica count handling 98% traffic\n2. **Canary deployment** with single replica for 2% traffic\n3. **Separate services** enabling selective traffic routing\n4. **Ingress configuration** with weighted routing using NGINX annotations\n5. **Rollback commands** for immediate traffic re-routing\n6. **Load validation** using ab, kubectl monitoring, and hit counting\n\nThis demonstrates practical CKAD-level knowledge of vanilla Kubernetes canary patterns, proper resource management, and operational validation techniques.","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":["canary deployment","weighted routing","ingress controller","service mesh","vanilla kubernetes","rollback plan","traffic splitting","load validation","version labels","readiness probe","replica count","backend configuration"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-21T05:01:09.780Z","createdAt":"2026-01-14T09:51:06.854Z"},{"id":"q-1776","question":"Design and implement a CKAD deployment for a 3-replica web app: use a ConfigMap for env vars, define resource requests/limits, add readiness and liveness probes, and configure an HPA with min 3, max 10 and targetCPUUtilizationPercentage 50. Provide the YAML and show verification steps?","answer":"Define a Deployment with 3 replicas, envFrom: configMapRef: name: web-env, resources: requests: cpu: 100m, memory: 128Mi; limits: cpu: 250m, memory: 256Mi; readinessProbe and livenessProbe for /; HPA:","explanation":"## Why This Is Asked\nTests practical CKAD skills: Deployments, ConfigMaps, resources, probes, and autoscaling with verification.\n\n## Key Concepts\n- Deployment, ConfigMap, resources, probes, HPA\n- Rollout verification, kubectl, metrics-server\n- Safe rollout strategies\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: web-app\n    spec:\n      containers:\n      - name: web\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: web-env\n        resources:\n          requests:\n            memory: \"128Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 15\n          periodSeconds: 20\n```\n\n## Follow-up Questions\n- How would you adapt this for a multi-tenant cluster? \n- What changes if the app uses a sidecar log collector?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T10:39:53.916Z","createdAt":"2026-01-14T10:39:53.916Z"},{"id":"q-1879","question":"You are deploying a Node API behind a Service in Kubernetes with Redis and PostgreSQL. Provide manifests to run API with 4 replicas, Secret for DB creds, ConfigMap for flags, readiness/liveness probes, resource requests/limits, and a RollingUpdate with maxUnavailable=25%, maxSurge=25%. Include how you would test zero-downtime and how HPA would be wired?","answer":"Provide a Deployment for the API with 4 replicas, a Secret for DB credentials, a ConfigMap for feature flags, readinessProbe and livenessProbe, and resource requests/limits. Use a RollingUpdate with m","explanation":"## Why This Is Asked\nThis tests end-to-end Kubernetes app design under CKAD constraints: deployments, secrets, configmaps, probes, resource budgeting, rolling updates, and autoscaling. It emphasizes practical, scalable patterns rather than theory.\n\n## Key Concepts\n- Deployments and rollout strategies\n- Secrets and ConfigMaps\n- Probes and health checks\n- Resources and limits\n- HorizontalPodAutoscaler\n- StatefulSets and PVCs for Redis/PostgreSQL\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 25%\n      maxSurge: 25%\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: yourrepo/api:latest\n        ports:\n        - containerPort: 3000\n        envFrom:\n        - secretRef:\n            name: db-creds\n        - configMapRef:\n            name: api-flags\n        resources:\n          requests:\n            cpu: \"200m\"\n            memory: \"256Mi\"\n          limits:\n            cpu: \"500m\"\n            memory: \"512Mi\"\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 15\n          periodSeconds: 20\n```\n\n## Follow-up Questions\n- How would you monitor Redis cache misses and cache hits?\n- How would you handle DB credential rotation without downtime?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Discord","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T15:39:49.169Z","createdAt":"2026-01-14T15:39:49.169Z"},{"id":"q-1922","question":"CKAD intermediate: In a prod namespace, a Deployment named 'orders-api' with 3 replicas experiences brief outages during image upgrades. Provide a concrete patch for zero-downtime upgrades: set rollingUpdate strategy (maxUnavailable: 0, maxSurge: 1), add a PreStop hook for graceful shutdown, and create a PodDisruptionBudget to protect at least 2 healthy pods during maintenance. Include minimal Deployment and PDB manifests and describe validation under maintenance-like load?","answer":"Patch should enforce RollingUpdate with maxUnavailable 0 and maxSurge 1, add a PreStop hook that gracefully drains connections, and deploy a PodDisruptionBudget requiring at least 2 available pods. Va","explanation":"## Why This Is Asked\nTests ability to perform zero-downtime upgrades in a CKAD-like scenario with real-world constraints.\n\n## Key Concepts\n- Deployment strategy: RollingUpdate with maxUnavailable and maxSurge\n- Graceful shutdown via PreStop lifecycle hook\n- PodDisruptionBudget to protect minimum availability\n\n## Code Example\n```yaml\n# Deployment patch focusing on zero-downtime upgrade\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: orders-api\n  namespace: prod\nspec:\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: \"0\"\n      maxSurge: \"1\"\n  template:\n    spec:\n      containers:\n      - name: orders-api\n        image: orders-api:1.2\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"/bin/sh\",\"-c\",\"/app/graceful-stop.sh\"]\n```\n```yaml\n# PodDisruptionBudget to allow maintenance without dropping below 2 pods\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: orders-api-pdb\n  namespace: prod\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: orders-api\n```\n\n## Follow-up Questions\n- How would you test the grace period and ensure no in-flight requests are dropped?\n- How would you adapt this pattern for a canary upgrade strategy?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T17:35:33.832Z","createdAt":"2026-01-14T17:35:33.833Z"},{"id":"q-1959","question":"In a Kubernetes CKAD scenario, design a real-time log-processor that consumes from Kafka, writes results to Cassandra, restarts gracefully on failure, and preserves at-least-once delivery. Provide a concrete deployment design with InitContainer, ConfigMap, Secret, Probes, HPA, DLQ strategy, and a minimal YAML skeleton. Explain trade-offs and monitoring hooks?","answer":"Deploy a Deployment with 3 replicas, resource requests/limits, and readiness/liveness probes. Use an InitContainer to fetch schema, a ConfigMap for topics, a Secret for credentials, and a Kafka consum","explanation":"## Why This Is Asked\nThis question probes practical CKAD skills: building a resilient streaming app, managing config and secrets, and validating delivery semantics. It also tests how to reason about when to use InitContainers, DLQ, and Kubernetes primitives under real workloads.\n\n## Key Concepts\n- Deployment vs StatefulSet\n- InitContainers and init data fetch\n- ConfigMap and Secret usage\n- Kafka offset management and retries\n- Dead Letter Queue strategy\n- Probes, HPA, NetworkPolicy\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: log-processor\nspec:\n  replicas: 3\n  template:\n    spec:\n      initContainers:\n        - name: fetch-schema\n          image: busybox\n          command: [\"sh\",\"-c\",\"echo fetch-schema\"]\n      containers:\n        - name: processor\n          image: myrepo/log-processor:latest\n          envFrom:\n            - configMapRef:\n                name: log-processor-config\n```\n\n## Follow-up Questions\n- How would you implement exactly-once semantics in this pipeline?\n- How would you monitor lag and alert on Kafka consumer delays?\n","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","NVIDIA","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T18:54:45.563Z","createdAt":"2026-01-14T18:54:45.563Z"},{"id":"q-1992","question":"Scenario: A 3-replica Deployment for a Kubernetes CKAD exercise uses a ConfigMap for APP_PORT and LOG_LEVEL and a Secret for DB_PASSWORD. How would you structure manifests to ensure zero-downtime updates, proper health checks, and correct secret/config usage? Provide concrete manifest fragments and a rollout strategy?","answer":"Use a Deployment with rollingUpdate (maxUnavailable: 0, maxSurge: 1) and 3 replicas. Provision a ConfigMap (APP_PORT: 8080, LOG_LEVEL: info) and a Secret (DB_PASSWORD: mypassword) with stringData. In ","explanation":"## Why This Is Asked\n\nEvaluates CKAD fundamentals: ConfigMap/Secret usage, zero-downtime rolling updates, and health checks.\n\n## Key Concepts\n\n- ConfigMap vs Secret usage\n- envFrom vs volumeMounts\n- Readiness/Liveness probes\n- RollingUpdate strategy\n- Resource requests/limits\n\n## Code Example\n\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  APP_PORT: 8080\n  LOG_LEVEL: info\n\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secrets\nstringData:\n  DB_PASSWORD: mypassword\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: my-registry/api:latest\n        envFrom:\n        - configMapRef:\n            name: app-config\n        - secretRef:\n            name: app-secrets\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n          limits:\n            cpu: 500m\n            memory: 256Mi\n```\n\n## Follow-up Questions\n\n- What would you change to mount the secret as a file instead of env vars?\n- How would you observe rollout status in CI?","diagram":"flowchart TD\n  Deployment --> ConfigMap\n  Deployment --> Secret\n  Deployment --> Probes\n  Probes --> RollingUpdate","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","IBM","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T19:46:46.570Z","createdAt":"2026-01-14T19:46:46.571Z"},{"id":"q-2015","question":"Design a CKAD-grade, multi-tenant API gateway canary rollout: implement two Deployments (stable and canary) for api-gateway, share a Service, and use an Ingress canary annotation to route 20% traffic to canary. Use a ConfigMap flag newFeature to toggle the new code path; store TLS certs in a Secret; include readiness/liveness probes, resource requests/limits, and a minimal YAML skeleton. Explain how you would observe traffic split and rollback?","answer":"Two Deployments (stable and canary) for api-gateway behind one Service; use Ingress canary annotations to route ~20% traffic to canary; a ConfigMap flag newFeature toggles the new code path; a Secret ","explanation":"## Why This Is Asked\nTests practical CKAD skills: multi-resource coordination, canary rollout, and observability.\n\n## Key Concepts\n- Deployments and Services for versioned workloads\n- Ingress canary routing and traffic splits\n- ConfigMaps and Secrets for feature flags and TLS\n- Readiness and Liveness probes\n- Resource requests/limits and RBAC basics\n- Observability and rollback procedures\n\n## Code Example\n```yaml\n# Deployment skeletons for stable and canary\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-stable\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: api\n        version: stable\n    spec:\n      containers:\n        - name: gateway\n          image: myrepo/api-gateway:stable\n          ports:\n            - containerPort: 8080\n          resources:\n            requests:\n              cpu: \"100m\"\n              memory: \"128Mi\"\n            limits:\n              cpu: \"500m\"\n              memory: \"256Mi\"\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 5\n            periodSeconds: 10\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 15\n            periodSeconds: 20\n```\n```yaml\n# Canary Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-canary\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: api\n        version: canary\n    spec:\n      containers:\n        - name: gateway\n          image: myrepo/api-gateway:canary\n          ports:\n            - containerPort: 8080\n          resources:\n            requests:\n              cpu: \"100m\"\n              memory: \"128Mi\"\n            limits:\n              cpu: \"600m\"\n              memory: \"256Mi\"\n```\n```yaml\n# Shared Service\napiVersion: v1\nkind: Service\nmetadata:\n  name: api-service\nspec:\n  selector:\n    app: api\n  ports:\n    - port: 80\n      targetPort: 8080\n```\n```yaml\n# Ingress with canary rule (nginx-ingress)\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: api-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/canary: \"true\"\n    nginx.ingress.kubernetes.io/canary-by-header: \"Canary\"\n    nginx.ingress.kubernetes.io/canary-weight: \"20\"\nspec:\n  rules:\n  - host: api.example.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: api-service\n            port:\n              number: 80\n```\n```yaml\n# ConfigMap: feature flag\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: api-config\ndata:\n  newFeature: \"true\"\n```\n```yaml\n# Secret: TLS\napiVersion: v1\nkind: Secret\ntype: kubernetes.io/tls\nmetadata:\n  name: api-tls\ndata:\n  tls.crt: <base64-cert>\n  tls.key: <base64-key>\n```\n\n## Follow-up Questions\n- How would you verify the traffic split and rollback safely?\n- What metrics would you monitor to detect issues with the canary?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T20:52:30.972Z","createdAt":"2026-01-14T20:52:30.972Z"},{"id":"q-2093","question":"You deploy a Kubernetes Deployment named web-app using image registry.example.com/web-app:v1.2 and expose port 8080. Provide a YAML manifest snippet that adds: livenessProbe for /health on 8080 with initialDelaySeconds: 5 and periodSeconds: 10; readinessProbe for /ready with timeoutSeconds: 3; resources: requests cpu: 250m memory: 256Mi; limits cpu: 500m memory: 512Mi; a ConfigMap mounted at /etc/config providing APP_MODE and an env var APP_MODE sourced from that ConfigMap?","answer":"Configure a Deployment for web-app (image registry.example.com/web-app:v1.2) with 3 replicas on port 8080. Add livenessProbe: httpGet /health on 8080, initialDelaySeconds: 5, periodSeconds: 10; readinessProbe: httpGet /ready on 8080 with timeoutSeconds: 3; resources: requests cpu: 250m memory: 256Mi, limits cpu: 500m memory: 512Mi; ConfigMap mounted at /etc/config providing APP_MODE and env var APP_MODE sourced from that ConfigMap.","explanation":"## Why This Is Asked\nTests CKAD fundamentals in a practical way: probes, resources, and ConfigMap usage. It checks that the candidate can translate requirements into Kubernetes primitives and reason about reliability.\n\n## Key Concepts\n- Liveness vs readiness probes\n- Resource requests and limits\n- ConfigMaps as configuration and env binding\n- Volume mounts and envFrom/KeyRefs\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: web-app\n    spec:\n      containers:\n      - name: web-app\n        image: registry.example.com/web-app:v1.2\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          timeoutSeconds: 3\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n        env:\n        - name: APP_MODE\n          valueFrom:\n            configMapKeyRef:\n              name: web-app-config\n              key: APP_MODE\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: web-app-config\n```","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T04:13:59.723Z","createdAt":"2026-01-14T23:42:58.629Z"},{"id":"q-2145","question":"CKAD intermediate: In namespace retail, a Deployment 'inventory' (3 replicas) talks to Postgres service 'inventory-db'. You need a one-time seed of lookup data at Pod startup without delaying traffic. Provide a minimal patch that uses an InitContainer to run a seed SQL script against inventory-db with idempotent INSERTs (ON CONFLICT DO NOTHING), and add resource requests/limits, a readiness probe, and a liveness probe. Explain how you'd validate under load and re-seed behavior on restarts?","answer":"Use an InitContainer to run a seed SQL against inventory-db before app pods start, with idempotent INSERTs (ON CONFLICT DO NOTHING). Patch Deployment to mount the seed script from a ConfigMap, set res","explanation":"## Why This Is Asked\nTests InitContainer sequencing, one-time data seeding, ConfigMap usage, and ensuring idempotent seeds and probes work with startup order.\n\n## Key Concepts\n- InitContainers run before app containers\n- Seed data via idempotent SQL (ON CONFLICT DO NOTHING)\n- Mount seed script from ConfigMap\n- Probes and resource requests/limits to keep health\n- Validation under load and restart scenarios\n\n## Code Example\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inventory\n  namespace: retail\nspec:\n  replicas: 3\n  template:\n    spec:\n      initContainers:\n      - name: seed-data\n        image: postgres:15\n        command: [\"psql\", \"-h\", \"inventory-db\", \"-U\", \"seeduser\", \"-d\", \"inventory\"]\n        args: [\"-f\", \"/seed/init.sql\"]\n        volumeMounts:\n        - name: seed-scripts\n          mountPath: /seed\n        env:\n        - name: PGPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: inventory-db-secret\n              key: password\n      containers:\n      - name: app\n        image: inventory-app:latest\n        env:\n        - name: DB_HOST\n          value: \"inventory-db\"\n        - name: DB_USER\n          value: \"seeduser\"\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: inventory-db-secret\n              key: password\n      volumes:\n      - name: seed-scripts\n        configMap:\n          name: inventory-seed-scripts\n```\n\n## Follow-up Questions\n- How would you avoid reseeding when pods are rescheduled and during upgrades?\n- How would you test seed idempotency and monitor seed success during rolling updates?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T04:25:43.380Z","createdAt":"2026-01-15T04:25:43.380Z"},{"id":"q-2205","question":"CKAD intermediate: A 3-rep StatefulSet named 'cache' in namespace 'prod' must be upgraded with zero downtime. How would you implement a partitioned, rolling update (one pod at a time) using spec.updateStrategy.RollingUpdate with partition, and set readiness/startup probes plus CPU/memory limits? Also provide a minimal PodDisruptionBudget to keep at least 2 pods healthy during maintenance, and outline validation under load?","answer":"Use a partitioned RollingUpdate: set spec.updateStrategy.rollingUpdate.partition: 2 so upgrades begin with the highest ordinal Pod (2) and proceed one at a time, preserving at least 2 pods. Add readin","explanation":"## Why This Is Asked\nTests partitioned StatefulSet upgrades, PDB, and probes in a real CKAD scenario.\n\n## Key Concepts\n- StatefulSet partitioned RollingUpdate\n- PodDisruptionBudget for minimum availability\n- readinessProbe and startupProbe usage\n- Resource requests/limits in containers\n\n## Code Example\n```javascript\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: cache\n  namespace: prod\nspec:\n  serviceName: \"cache\"\n  replicas: 3\n  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      partition: 2\n  template:\n    metadata:\n      labels:\n        app: cache\n    spec:\n      containers:\n      - name: cache\n        image: myrepo/cache:2.0\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n          limits:\n            cpu: \"300m\"\n            memory: \"256Mi\"\n        readinessProbe:\n          exec:\n            command: [\"bash\",\"-lc\",\"redis-cli PING\"]\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        startupProbe:\n          exec:\n            command: [\"bash\",\"-lc\",\"true\"]\n          initialDelaySeconds: 10\n          periodSeconds: 30\n```\n\n```javascript\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: cache-pdb\n  namespace: prod\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: cache\n```\n\n## Diagram\nflowchart TD\n  A[Partitioned Upgrade] --> B[Staged Pods Updated]\n\n## Follow-up Questions\n- How would you revert if the upgrade fails?\n- How would you monitor for anomalies during the upgrade?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T07:36:07.318Z","createdAt":"2026-01-15T07:36:07.318Z"},{"id":"q-2237","question":"Design a Kubernetes manifest suite for image registry.example/ckad-demo:1.0 that uses a ConfigMap and a Secret. Include a Deployment with 3 replicas, a Service on port 80, a ConfigMap app-config with APP_MODE=production, and a Secret app-secret with API_KEY=secret123. Mount the ConfigMap at /etc/app and the Secret at /etc/secret, export APP_MODE as an env, include readiness and liveness probes on /healthz, and trigger rolling updates on ConfigMap changes via a checksum/config annotation?","answer":"Provide Kubernetes manifests for a Deployment of registry.example/ckad-demo:1.0 with 3 replicas, a ConfigMap app-config (APP_MODE=production) and a Secret app-secret (API_KEY=secret123). Mount /etc/ap","explanation":"## Why This Is Asked\nTests practical CKAD skills: using ConfigMaps and Secrets in a pod, mounting as volumes, exporting env vars, health checks, and Service exposure. Also checks ability to prompt rollout on CM changes, a common DevOps trap.\n\n## Key Concepts\n- ConfigMap and Secret usage in Pods\n- Volume mounts and envFrom/env\n- Probes: readiness and liveness\n- Rolling updates via annotation\n- Service exposure and port mapping\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ckad-demo\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: app\n        image: registry.example/ckad-demo:1.0\n        env:\n        - name: APP_MODE\n          valueFrom:\n            configMapKeyRef:\n              name: app-config\n              key: APP_MODE\n        volumeMounts:\n        - name: app-config\n          mountPath: /etc/app\n        - name: app-secret\n          mountPath: /etc/secret\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n      volumes:\n      - name: app-config\n        configMap:\n          name: app-config\n      - name: app-secret\n        secret:\n          secretName: app-secret\n```\n\n## Follow-up Questions\n- How would you update the config without downtime in a multi-tenant cluster?\n- What are the security considerations for mounting secrets as files?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Hashicorp","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T08:55:00.789Z","createdAt":"2026-01-15T08:55:00.789Z"},{"id":"q-2276","question":"In a CKAD scenario, namespace 'analytics' hosts Deployment 'data-collector' (5 replicas) behind service 'collector-svc' on port 9000. Implement a NetworkPolicy that (1) allows ingress to data-collector only from pods in the analytics namespace labeled app=ingest on port 9000, (2) denies all other inbound traffic, and (3) allows egress to 10.0.0.50:5140. Provide the YAML manifest and a patch to the Deployment to apply the policy. Describe how you'd validate under simulated load?","answer":"Create a NetworkPolicy in analytics: podSelector: {app: data-collector}, policyTypes: Ingress,Egress. Ingress: from: {podSelector: {app: ingest}}, ports: [{protocol: TCP, port: 9000}]. Egress: to: {ip","explanation":"## Why This Is Asked\nTests practical network isolation using NetworkPolicy, ensuring correct pod labeling and cross-namespace traffic control, plus validation under load.\n\n## Key Concepts\n- NetworkPolicy scoping and podSelector behavior within a namespace\n- Ingress vs Egress rules and port matching\n- Synchronizing Deployment labels with Policy selectors and Service selectors\n- Validation via test pods and kubectl events\n\n## Code Example\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: analytics-data-collector-np\n  namespace: analytics\nspec:\n  podSelector:\n    matchLabels:\n      app: data-collector\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: ingest\n    ports:\n    - protocol: TCP\n      port: 9000\n  egress:\n  - to:\n    - ipBlock:\n        cidr: 10.0.0.50/32\n    ports:\n    - protocol: TCP\n      port: 5140\n```\n\n## Follow-up Questions\n- How would you extend this for DNS-based egress lets and audits?\n- What pitfalls can arise with different CNI implementations when enforcing NetworkPolicy?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Discord","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T10:37:29.665Z","createdAt":"2026-01-15T10:37:29.665Z"},{"id":"q-2306","question":"In CKAD terms, design a per-namespace streaming transformer that consumes Redis Stream 'events', computes a rolling 60s window average latency per user, and writes aggregates to PostgreSQL. Replays failed messages to a Redis DLQ. Use an InitContainer to preload an ML model from a Secret, a ConfigMap for thresholds, readiness/liveness probes, and an HPA based on latency. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"The transformer uses a Deployment per namespace with an InitContainer that loads a model from a Secret into a shared volume. The app subscribes to Redis Stream 'events', computes 60s rolling averages ","explanation":"## Why This Is Asked\n\nTests CKAD-level streaming design using Redis Streams, per-namespace isolation, and observable patterns. Highlights InitContainer model preload, Secrets/ConfigMaps usage, DLQ strategy, and latency-based scaling.\n\n## Key Concepts\n\n- CKAD primitives: ConfigMap, Secret, InitContainer, Probes, HPA\n- Messaging and durability: Redis Streams DLQ\n- Data plane: PostgreSQL aggregates, idempotent writes\n\n## Code Example\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: streaming-transformer\n  namespace: finance\nspec:\n  replicas: 2\n  template:\n    spec:\n      initContainers:\n      - name: preload-model\n        image: busybox\n        command: [\"sh\",\"-c\",\"echo loading model; sleep 1\"]\n        volumeMounts:\n        - name: model\n          mountPath: /models\n      containers:\n      - name: transformer\n        image: registry.example/stream-transformer:latest\n        env:\n        - name: REDIS_STREAM\n          value: events\n        - name: POSTGRES_URL\n          valueFrom:\n            secretKeyRef:\n              name: pg-credentials\n              key: url\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n      volumes:\n      - name: model\n        emptyDir: {}\n```\n\n## Follow-up Questions\n\n- How would you implement idempotent writes to PostgreSQL in this setup?\n- How would you monitor latency and set HPA thresholds?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Hugging Face","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T11:34:24.300Z","createdAt":"2026-01-15T11:34:24.300Z"},{"id":"q-2366","question":"Design a Kubernetes manifest for a beginner CKAD task: deploy a static site with nginx where index.html is served from a ConfigMap named app-content. Mount TLS certs from Secret tls at /etc/tls. Requirements: Deployment with 2 replicas, runAsNonRoot and readOnlyRootFilesystem, readiness/liveness probes on /healthz, INDEX_FILE env from ConfigMap, and an HPA with CPU target 50%. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Create a Deployment for nginx with 2 replicas, runAsNonRoot and readOnlyRootFilesystem. Mount ConfigMap app-content at /usr/share/nginx/html and Secret tls at /etc/tls as files. Expose INDEX_FILE via ","explanation":"## Why This Is Asked\nTests mounting of ConfigMaps and Secrets, securityContext, health checks, and CPU-based scaling in a realistic, small app.\n\n## Key Concepts\n- ConfigMap as files and env\n- Secret mounted as files\n- SecurityContext (runAsNonRoot, readOnlyRootFilesystem)\n- Probes (readiness, liveness)\n- HorizontalPodAutoscaler on CPU\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-static\nspec:\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: nginx-static\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:stable-alpine\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: content\n          mountPath: /usr/share/nginx/html\n        - name: tls\n          mountPath: /etc/tls\n          readOnly: true\n        env:\n        - name: INDEX_FILE\n          valueFrom:\n            configMapKeyRef:\n              name: app-config\n              key: INDEX_FILE\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 80\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 80\n          initialDelaySeconds: 15\n          periodSeconds: 20\n      volumes:\n      - name: content\n        configMap:\n          name: app-content\n      - name: tls\n        secret:\n          secretName: tls\n```\n\n## Follow-up Questions\n- How would you verify that readOnlyRootFilesystem is enforced?\n- How would you rotate the TLS secret without restarting pods?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T14:55:37.884Z","createdAt":"2026-01-15T14:55:37.884Z"},{"id":"q-2427","question":"Design a CKAD-focused per-namespace file-processing service: it watches a PVC-mounted directory for new text files, processes each file, and submits results to a REST API. Use a ConfigMap for FILE_EXT and BATCH_SIZE, a Secret for API creds, an InitContainer to install deps, and a marker-based idempotence scheme to achieve at-least-once with near-exactly-once semantics. Include readiness/liveness probes and a minimal YAML skeleton; discuss trade-offs?","answer":"An ideal answer would describe a Deployment with a Python worker reading /workspace/files from a per-namespace PVC, an InitContainer for dependencies, a ConfigMap to set FILE_EXT='txt', BATCH_SIZE=100","explanation":"## Why This Is Asked\nThis question probes per-namespace isolation, file-based events, and Kubernetes primitives in CKAD scope. It also tests InitContainers, readiness probes, and a practical idempotence pattern for small-scale data pipelines.\n\n## Key Concepts\n- Kubernetes primitives: Deployment, PVC, ConfigMap, Secret\n- InitContainers and probes\n- Idempotent file processing using marker files\n- In-namespace REST egress with retry/backoff\n- Observability: logs and metrics\n\n## Code Example\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: file-processor-config\ndata:\n  file_ext: \"txt\"\n  batch_size: \"100\"\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: file-processor-creds\ntype: Opaque\ndata:\n  api_token: <base64>\n```\n\n## Follow-up Questions\n- How would you guarantee exactly-once semantics on restart?\n- How would you scale across many namespaces while avoiding cross-namespace leakage?","diagram":"flowchart TD\n  A[Namespace] --> B[Worker Pod]\n  B --> C[PVC: data]\n  B --> D[ConfigMap: options]\n  B --> E[Secret: creds]","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Anthropic","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T17:47:10.074Z","createdAt":"2026-01-15T17:47:10.074Z"},{"id":"q-2453","question":"In a CKAD scenario, design a per-namespace real-time image-resize service that consumes from a Kafka topic, uses an InitContainer to fetch a resize model from a Secret, writes results to PostgreSQL, and preserves at-least-once delivery with idempotent DB writes. Include a ConfigMap for RESIZE_WIDTH/HEIGHT and BATCH_SIZE, readiness/liveness probes, an HPA based on CPU, and a per-namespace NetworkPolicy restricting Kafka ingress and DB egress. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Deploy a per-namespace image-resize processor that subscribes to a Kafka topic, with an InitContainer fetching a resize model from a Secret. Write results to PostgreSQL via an idempotent upsert on (im","explanation":"## Why This Is Asked\nTests practical CKAD mastery: per-namespace isolation, InitContainer usage, secret/config management, and production-grade guarantees.\n\n## Key Concepts\n- InitContainer for model pull from Secret\n- ConfigMap and Secret usage for runtime parameters\n- Probes and HPA for reliability and scale\n- Idempotent DB writes to achieve at-least-once delivery\n- NetworkPolicy to enforce namespace boundaries\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ns-image-resize\n  namespace: <ns>\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: image-resize\n    spec:\n      initContainers:\n        - name: fetch-model\n          image: alpine:3.18\n          command: [\"sh\", \"-c\", \"download-model.sh\"]\n          volumeMounts:\n            - name: model\n              mountPath: /models\n      containers:\n        - name: processor\n          image: registry/ns-image-resize:latest\n          ports:\n            - containerPort: 8080\n          envFrom:\n            - secretRef: { name: resize-secret }\n      volumes:\n        - name: model\n          emptyDir: {}\n      readinessProbe:\n        httpGet:\n          path: /healthz\n          port: 8080\n        initialDelaySeconds: 5\n        periodSeconds: 10\n      livenessProbe:\n        httpGet:\n          path: /healthz\n          port: 8080\n        initialDelaySeconds: 15\n        periodSeconds: 20\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n            - weight: 100\n              podAffinityTerm:\n                labelSelector:\n                  matchLabels:\n                    app: image-resize\n                topologyKey: \"kubernetes.io/hostname\"\n  ```\n\n## Follow-up Questions\n- How would you monitor idempotency and retry behavior?\n- What changes if Kafka offset management moves to a separate transactional store?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T18:56:11.109Z","createdAt":"2026-01-15T18:56:11.109Z"},{"id":"q-2541","question":"In CKAD terms, design a new, per-namespace data-collector that subscribes to an in-cluster message bus (NATS) and writes results to a time-series store, ensuring at-least-once delivery with a DLQ. Include an InitContainer to install runtime deps, a ConfigMap for BATCH_SIZE, a Secret for creds, a Sidecar for TLS cert rotation, readiness/liveness probes, and a NetworkPolicy restricting egress to the TSDB, a DLQ path on a PVC, and a minimal YAML skeleton. Explain trade-offs?","answer":"Design a per-namespace data-collector that subscribes to NATS, batches events, and writes to InfluxDB with at-least-once delivery and a DLQ. Use an InitContainer to install runtime dependencies, a ConfigMap for BATCH_SIZE configuration, and a Secret for credentials. Include a sidecar for TLS certificate rotation, implement readiness and liveness probes, and restrict egress traffic to the time-series database using a NetworkPolicy. Mount a PVC for the dead-letter queue path and provide a minimal YAML skeleton for deployment.","explanation":"## Why This Is Asked\nThis question tests comprehensive CKAD skills including per-namespace isolation, configuration management with ConfigMaps and Secrets, InitContainers for dependency setup, sidecar patterns for auxiliary services, health probes for reliability, RBAC for security, and NetworkPolicy for traffic control.\n\n## Key Concepts\n- InitContainers for runtime dependency installation\n- ConfigMaps and Secrets for configuration and credential management\n- Sidecar containers for TLS certificate rotation\n- Readiness and liveness probes for health monitoring\n- Namespace isolation and NetworkPolicy for security boundaries\n- Dead-letter queue implementation on PVC storage\n- At-least-once delivery semantics with batch processing\n\n## Code Example\n```yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: ns-collector\n  namespace: target-namespace\nspec:\n  initContainers:\n  - name: install-deps\n    image: alpine:latest\n    command: ['sh', '-c', 'apk add --no-cache nats-cli']\n  containers:\n  - name: collector\n    image: collector:latest\n    envFrom:\n    - configMapRef:\n        name: ns-collector-config\n    - secretRef:\n        name: ns-collector-creds\n    readinessProbe:\n      httpGet:\n        path: /health\n        port: 8080\n    livenessProbe:\n      httpGet:\n        path: /health\n        port: 8080\n    volumeMounts:\n    - name: dlq-pvc\n      mountPath: /dlq\n  - name: tls-sidecar\n    image: tls-sidecar:latest\n    volumeMounts:\n    - name: tls-certs\n      mountPath: /etc/tls\n  volumes:\n  - name: dlq-pvc\n    persistentVolumeClaim:\n      claimName: dlq-pvc\n  - name: tls-certs\n    secret:\n      secretName: tls-certs\n```\n\n## Trade-offs\n- **At-least-once vs exactly-once**: At-least-once provides simpler implementation but may cause duplicate processing\n- **Sidecar overhead**: TLS rotation adds resource consumption but improves security\n- **PVC vs temporary storage**: PVC provides persistence but increases complexity and cost\n- **NetworkPolicy restrictions**: Improve security but may impact debugging and monitoring capabilities","diagram":"flowchart TD\n  NS[Namespace] --> DC[Data Collector Pod]\n  DC --> NB[NATS Subscription]\n  DC --> TS[InfluxDB]\n  DC --> DLQ[DLQ PVC]","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Discord","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T04:25:02.327Z","createdAt":"2026-01-15T21:49:37.961Z"},{"id":"q-2614","question":"CKAD advanced: design a per-namespace data-pipeline controller using a CRD NamespaceDataPipeline. Each instance subscribes to a namespaced NATS subject (metrics.{namespace}) and writes batched data to a TSDB. Guarantee at-least-once via a DLQ on a PVC and idempotent writes. Include InitContainer for deps, a TLS-rotation sidecar, a ConfigMap for BATCH_SIZE, a Secret for creds, readiness/liveness probes, and a NetworkPolicy restricting egress to the TSDB. Provide a minimal YAML skeleton and trade-offs?","answer":"Design a NamespaceDataPipeline Custom Resource Definition that creates per-namespace data pipeline controllers. Each instance subscribes to a namespaced NATS subject (metrics.{namespace}) and writes batched data to a time-series database. Ensure at-least-once delivery through a dead-letter queue on a persistent volume claim with idempotent write semantics.","explanation":"## Why This Is Asked\nTests your ability to design Kubernetes-native data pipelines with CRD-driven control, per-namespace isolation, and robust delivery guarantees.\n\n## Key Concepts\n- Custom Resources and controllers\n- Idempotent writes and dead-letter queue design\n- InitContainers, sidecars, and TLS rotation\n- RBAC, NetworkPolicy, and health probes\n\n## Code Example\n```yaml\n# Minimal skeleton showing CRD and Deployment structure\n```\n\n## Follow-up Questions\n- How would you implement exactly-once semantics in a stateless controller?\n- How would you test CRD upgrades and canary rollouts?","diagram":"flowchart TD\nA[CRD NamespaceDataPipeline] --> B[Controller watches CRs]\nB --> C[NATS client subscribes to metrics.*]\nC --> D[Batcher writes to TSDB]\nD --> E{DLQ PVC}\nE --> F[Retry/Dead-lettering]\n","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Oracle","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:00:43.754Z","createdAt":"2026-01-16T02:44:23.261Z"},{"id":"q-2767","question":"CKAD advanced: Design a per-namespace policy-enforcement flow using a MutatingAdmissionWebhook that auto-injects resource requests/limits into Pods lacking them. The webhook reads defaults from a ConfigMap (e.g., default_cpu: 125m, default_memory: 128Mi) with per-namespace overrides; a Secret stores a token to fetch policy updates from an in-cluster policy store. Include a minimal webhook Deployment, MutatingWebhookConfiguration, and a per-namespace test namespace. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Implement a MutatingAdmissionWebhook that injects resource requests/limits on Pods missing them. Defaults come from a ConfigMap (e.g., 125m CPU, 128Mi memory) with per-namespace overrides; a Secret st","explanation":"Why This Is Asked\n- Tests mutating admission logic, config-driven defaults, and namespace overrides.\nKey Concepts\n- MutatingAdmissionWebhook, ConfigMap defaults with per-namespace overrides, Secret-based credentials for policy store, InitContainer for runtime deps, and a NetworkPolicy to restrict webhook traffic.\nCode Example\n```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: policy-defaults\ndata:\n  default_cpu: \"125m\"\n  default_memory: \"128Mi\"\n```\n```\nyaml\napiVersion: admissionregistration.k8s.io/v1\nkind: MutatingWebhookConfiguration\nmetadata:\n  name: pod-resource-defaults\nwebhooks:\n  - name: default-resources.example.com\n    clientConfig:\n      service:\n        name: policy-webhook\n        namespace: default\n        path: \"/mutate\"\n    rules:\n      - apiGroups: [\"\"]\n        apiVersions: [\"v1\"]\n        resources: [\"pods\"]\n        operations: [\"CREATE\"]\n```\nFollow-up Questions\n- How to test idempotency across pod restarts?\n- How to handle conflicting per-namespace overrides and ensure observability?","diagram":"flowchart TD\nA[Namespace] --> B[MutatingAdmissionWebhook]\nB --> C[ConfigMap: defaults]\nB --> D[Secret: policy token]\nB --> E[Policy store]\nF[Webhook Deployment] --> B\nG[NetworkPolicy] --> B","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T10:56:28.677Z","createdAt":"2026-01-16T10:56:28.677Z"},{"id":"q-2782","question":"CKAD intermediate: In namespace 'prod', a Deployment 'payments-api' with multiple replicas risks resource contention under peak load. You must enforce per-namespace resource discipline. Create a ResourceQuota and a LimitRange; provide minimal manifests and describe validation under load?","answer":"Define a ResourceQuota prod-resource-quota to cap total pods, CPU, and memory, plus a LimitRange to enforce default pod requests/limits. This prevents surge from starving the cluster and keeps neighbo","explanation":"## Why This Is Asked\nTests practical enforcement of multi-tenant isolation and Kubernetes quotas in a real prod-like namespace.\n\n## Key Concepts\n- ResourceQuota and hard limits\n- LimitRange defaults and constraints\n- Validation via kubectl describe quota and continuity tests\n\n## Code Example\n```\nyaml\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: prod-resource-quota\n  namespace: prod\nspec:\n  hard:\n    pods: \"20\"\n    requests.cpu: \"40\"\n    requests.memory: \"160Gi\"\n    limits.cpu: \"80\"\n    limits.memory: \"320Gi\"\n```\n```\nyaml\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: prod-defaults\n  namespace: prod\nspec:\n  limits:\n  - default:\n      cpu: 200m\n      memory: 256Mi\n    defaultRequest:\n      cpu: 100m\n      memory: 128Mi\n    type: Pod\n```\n\n## Follow-up Questions\n- How would you monitor quota usage and alert on breaches in a cluster-wide monitoring stack?\n- How would you adapt quotas for environments like dev/staging while preserving prod isolation?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Scale Ai","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T11:39:36.693Z","createdAt":"2026-01-16T11:39:36.693Z"},{"id":"q-2881","question":"In a CKAD scenario, you need to isolate a payment microservice from other pods while allowing only approved traffic within namespace 'finance'. The Deployment 'payment' has 3 replicas. Write a NetworkPolicy that (a) allows Ingress to payment only from pods labeled app=frontend in the same namespace, (b) allows Egress from payment only to pods labeled app=db on port 5432 and to pods labeled app=cache on port 6379. Provide the minimal policy manifest and describe how to validate under load?","answer":"Create a NetworkPolicy named payment-allow in namespace finance selecting app=payment; ingress only from pods labeled app=frontend in the same namespace; egress only to pods labeled app=db on port 543","explanation":"## Why This Is Asked\nTests practical use of NetworkPolicy to enforce zero-trust interactions between microservices and their dependents, a common production pattern.\n\n## Key Concepts\n- NetworkPolicy basics: podSelector, ingress/egress, policyTypes\n- namespace-scoped rules and label selectors\n- Egress to specific app Pods on defined ports\n- Validation: simulate allowed and blocked traffic, inspect policy state\n\n## Code Example\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: payment-allow\n  namespace: finance\nspec:\n  podSelector:\n    matchLabels:\n      app: payment\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  - from:\n    - podSelector:\n        matchLabels:\n          app: frontend\n  egress:\n  - to:\n    - podSelector:\n        matchLabels:\n          app: db\n    ports:\n    - protocol: TCP\n      port: 5432\n  - to:\n    - podSelector:\n        matchLabels:\n          app: cache\n    ports:\n    - protocol: TCP\n      port: 6379\n```\n\n## Follow-up Questions\n- How would you adapt if payment also needs cross-namespace database access?\n- What runtime and network-layer observability would you enable to detect policy violations?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Airbnb","Amazon"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T15:51:46.254Z","createdAt":"2026-01-16T15:51:46.254Z"},{"id":"q-2950","question":"Design a per-namespace Kubernetes CronJob in CKAD terms that runs every 10 minutes and pings a TARGET_URL from a ConfigMap, using an API_TOKEN from a Secret to authenticate, posting a heartbeat payload with a timestamp. Persist the last status in a namespace ConfigMap heartbeat-status and write run logs to a PVC at /logs. Use an InitContainer to install curl, mount the logs volume, add readiness/liveness probes on the Job's Pod, and provide a minimal YAML skeleton. What trade-offs exist?","answer":"Create a per-namespace CronJob every 10 minutes. Pod runs a small script that reads TARGET_URL from ConfigMap cron-config and API_TOKEN from Secret target-creds, posts a heartbeat payload with a times","explanation":"## Why This Is Asked\n\nTests CKAD fundamentals for per-namespace automation, using CronJobs, ConfigMaps, Secrets, InitContainers, and PVC-backed logs in a realistic heartbeat scenario.\n\n## Key Concepts\n\n- CronJob scheduling and Job templates\n- ConfigMap and Secret usage for config and creds\n- InitContainer for one-time setup\n- Probes for container health\n- PersistentVolumeClaim for logs\n\n## Code Example\n\n```javascript\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: heartbeat-cron\n  namespace: default\nspec:\n  schedule: \"*/10 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          initContainers:\n          - name: install-deps\n            image: alpine:3.19\n            command: [\"sh\",\"-c\",\"apk add --no-cache curl\"]\n          containers:\n          - name: heartbeater\n            image: alpine:3.19\n            command: [\"sh\",\"-c\",\"curl -sS -X POST -H \\\"Authorization: Bearer $API_TOKEN\\\" $TARGET_URL/heartbeat -d '{\\\"ts\\\": \\\"$(date +%s)\\\"}'\"]\n            envFrom:\n            - configMapRef:\n                name: cron-config\n            - secretRef:\n                name: target-creds\n            volumeMounts:\n            - name: logs\n              mountPath: /logs\n            readinessProbe:\n              exec:\n                command: [\"sh\",\"-c\",\"test -f /logs/ready\"]\n              initialDelaySeconds: 5\n              periodSeconds: 10\n            livenessProbe:\n              httpGet:\n                path: /healthz\n                port: 80\n              initialDelaySeconds: 15\n              periodSeconds: 20\n          restartPolicy: OnFailure\n          volumes:\n          - name: logs\n            persistentVolumeClaim:\n              claimName: heartbeat-logs\n```\n\n## Follow-up Questions\n\n- How would you rotate the API token without recreating the CronJob?\n- What failure modes and retries would you implement?","diagram":"flowchart TD\n  CronJob[Heartbeat CronJob] --> JobPod[Job Pod]\n  JobPod --> ConfigMap[cron-config]\n  JobPod --> Secret[target-creds]\n  JobPod --> Endpoint[Target URL]\n  Endpoint --> Response[HTTP Response]","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Salesforce","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T18:53:45.750Z","createdAt":"2026-01-16T18:53:45.750Z"},{"id":"q-3044","question":"In CKAD terms, design a per-namespace CronJob-based worker that watches a PVC-backed /data directory for new CSV files, processes them in batches (configurable via a ConfigMap named app-config with BATCH_SIZE), converts to JSON, and POSTS to a cluster-internal endpoint at http://data-portal:8080/api/v1/ingest. Use an InitContainer to install csvkit, a Secret for API credentials, a Sidecar for TLS cert rotation, readiness/liveness probes, a Namespace-scoped NetworkPolicy restricting egress to the internal endpoint, and a minimal YAML skeleton. Discuss trade-offs?","answer":"Deploy a per-namespace CronJob that spawns a Job every 5 minutes to read CSVs from a PVC at /data, batch-process up to BATCH_SIZE files, convert to JSON, and POST to http://data-portal:8080/api/v1/ingest.","explanation":"## Why This Is Asked\nTests CKAD workflow patterns: CronJobs, batch processing, and secret management within a namespace; validates knowledge of init containers, sidecars, and network hardening.\n\n## Key Concepts\n- CronJob scheduling in a namespace\n- InitContainer dependency installation\n- ConfigMap for BATCH_SIZE\n- Secret for API credentials\n- Sidecar for TLS cert rotation\n- NetworkPolicy for internal egress controls\n- Probes for readiness and liveness\n- Idempotency vs at-least-once semantics\n\n## Code Example\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: csv-ingest\nspec:\n  schedule: \"*/5 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          initContainers:\n          - name: install-csvkit\n            image: python:3.9-alpine\n            command: ['sh', '-c', 'pip install csvkit && cp -r /usr/local/lib/python3.9/site-packages/* /shared/']\n            volumeMounts:\n            - name: shared-libs\n              mountPath: /shared\n          containers:\n          - name: csv-processor\n            image: python:3.9-alpine\n            command: ['sh', '-c', 'python /app/process.py']\n            env:\n            - name: BATCH_SIZE\n              valueFrom:\n                configMapKeyRef:\n                  name: app-config\n                  key: BATCH_SIZE\n            - name: API_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: api-credentials\n                  key: token\n            volumeMounts:\n            - name: data-pvc\n              mountPath: /data\n            - name: shared-libs\n              mountPath: /usr/local/lib/python3.9/site-packages\n            readinessProbe:\n              httpGet:\n                path: /health\n                port: 8080\n              initialDelaySeconds: 10\n            livenessProbe:\n              httpGet:\n                path: /health\n                port: 8080\n              initialDelaySeconds: 30\n          - name: cert-rotator\n            image: cert-manager/cert-manager-rotator\n            volumeMounts:\n            - name: certs\n              mountPath: /etc/ssl/certs\n          volumes:\n          - name: data-pvc\n            persistentVolumeClaim:\n              claimName: data-pvc\n          - name: shared-libs\n            emptyDir: {}\n          - name: certs\n            emptyDir: {}\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\ndata:\n  BATCH_SIZE: \"10\"\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: api-credentials\ntype: Opaque\ndata:\n  token: <base64-encoded-token>\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: csv-ingest-netpol\nspec:\n  podSelector:\n    matchLabels:\n      app: csv-ingest\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - podSelector:\n        matchLabels:\n          app: data-portal\n    ports:\n    - protocol: TCP\n      port: 8080\n```\n\n## Trade-offs\n- **CronJob vs Deployment**: CronJobs provide scheduled execution but lack continuous availability; Deployments offer always-on processing but require custom scheduling logic.\n- **InitContainer overhead**: Installing dependencies at runtime increases startup time vs pre-built images, but provides flexibility for version updates.\n- **Sidecar complexity**: TLS cert rotation adds operational overhead but ensures security compliance; alternative is using cert-manager injection.\n- **Batch processing**: Larger batches improve throughput but increase memory usage and failure impact; smaller batches provide better isolation but higher API call frequency.\n- **NetworkPolicy scope**: Namespace-scoped policies provide good isolation but require per-namespace management; cluster-wide policies simplify administration but reduce granular control.\n- **Storage patterns**: PVC-backed storage ensures data persistence across pod restarts but requires careful volume management; temporary storage simplifies cleanup but risks data loss during failures.","diagram":"flowchart TD\n  A[CronJob] --> B[Job] --> C[//data CSVs/]\n  B --> D[Transform to JSON]\n  D --> E[POST to data-portal]\n  E --> F[Success]\n  F --> G[Cleanup / data]\n  G --> H[Observability: metrics & logs]","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Discord","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T05:28:53.579Z","createdAt":"2026-01-16T22:39:32.265Z"},{"id":"q-3158","question":"Design a per-namespace CKAD microservice that exposes a REST endpoint and supports dynamic feature flags controlled by a ConfigMap. The ConfigMap contains JSON features; a Secret stores per-namespace API keys. Include an InitContainer to install runtime dependencies, a sidecar that watches the ConfigMap for changes and signals the main container to reload config, readiness/liveness probes, an HPA based on 95th percentile latency, and a PVC-backed log store. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Deploy a per-namespace Deployment with an InitContainer that installs runtime deps; main app reads features.json from a ConfigMap and API keys from a Secret; a sidecar watches the ConfigMap for change","explanation":"## Why This Is Asked\n\nTests ability to integrate ConfigMap-driven feature flags, per-namespace isolation, hot-reload semantics, and production considerations like latency-based scaling and persistent logs.\n\n## Key Concepts\n- ConfigMap-driven feature flags with JSON payloads\n- Secret usage for per-namespace API keys\n- InitContainer for dependencies\n- ConfigMap reload sidecar pattern (hot reload via signals)\n- Readiness/Liveness probes\n- HPA based on latency\n- PVC-backed logs\n\n## Code Example\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ns-microservice\n  namespace: example-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ns-microservice\n  template:\n    metadata:\n      labels:\n        app: ns-microservice\n    spec:\n      initContainers:\n      - name: deps\n        image: alpine:3.18\n        command: [\"sh\", \"-c\", \"apk add --no-cache curl jq && mkdir -p /logs\"]\n        volumeMounts:\n        - name: logs\n          mountPath: /logs\n      containers:\n      - name: app\n        image: my-org/ns-microservice:latest\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - secretRef:\n            name: ns-api-keys\n        volumeMounts:\n        - name: logs\n          mountPath: /logs\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n      - name: config-reload\n        image: busybox\n        command: [\"sh\", \"-c\", \"sleep infinity\"]\n        volumeMounts:\n        - name: config\n          mountPath: /config\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: ns-logs-pvc\n      - name: config\n        configMap:\n          name: features-config\n```\n\n## Follow-up Questions\n- How would you implement a graceful failover if the ConfigMap reload signal fails to reach the main app?\n- What are the trade-offs of hot-reload versus full pod restart for feature flag changes?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Meta","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:53:35.004Z","createdAt":"2026-01-17T04:53:35.005Z"},{"id":"q-3246","question":"In namespace analytics, Deployment 'reporter' with 4 replicas is failing to fetch tokens because a restrictive NetworkPolicy blocks egress to api.analytics.internal:443. Provide a minimal patch to allow egress to that endpoint, preserving other rules, and describe how you would verify connectivity under load and rollback if needed?","answer":"Patch the NetworkPolicy to add an egress rule for TCP 443 to api.analytics.internal from pods labeled app=reporter in namespace analytics, keeping existing rules. Apply, then verify with a test pod cu","explanation":"## Why This Is Asked\n\nTests practical NetworkPolicy understanding, especially egress limitations and debugging under load.\n\n## Key Concepts\n\n- NetworkPolicy\n- Egress rules\n- IPBlock vs DNS constraints\n- Load validation\n- Rollback strategy\n\n## Code Example\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: reporter-egress\n  namespace: analytics\nspec:\n  podSelector:\n    matchLabels:\n      app: reporter\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - ipBlock:\n        cidr: 10.2.0.0/16\n    ports:\n    - protocol: TCP\n      port: 443\n```\n\n## Follow-up Questions\n\n- How would you test egress connectivity under load?\n- What risks exist with overly permissive egress policies, and how would you mitigate them?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T08:42:05.171Z","createdAt":"2026-01-17T08:42:05.171Z"},{"id":"q-3338","question":"In CKAD terms, design a per-namespace Deployment that runs a small REST API (example Flask app) with config from a ConfigMap named app-config and an API key from Secret named api-credentials. Include an InitContainer to install dependencies, a sidecar to rotate TLS certs from a Secret named tls-certs, readiness/liveness probes, a Namespace NetworkPolicy restricting egress to DEST_ENDPOINT, and a minimal YAML skeleton. Discuss trade-offs?","answer":"Outline: a Deployment with 1) ConfigMap app-config providing DEST_ENDPOINT and LOG_LEVEL as env vars, 2) Secret api-credentials for API_KEY, 3) InitContainer to install Python deps, 4) main REST API c","explanation":"## Why This Is Asked\nTests ability to compose CKAD tasks involving ConfigMaps, Secrets, InitContainers, sidecars, network policies, and probes in a beginner-credible scenario.\n\n## Key Concepts\n- InitContainer for bootstrapping deps\n- Sidecar for TLS cert rotation\n- ConfigMap/Secret usage for config and credentials\n- Namespace NetworkPolicy for egress control\n- Readiness/Liveness probes and non-root/container security\n\n## Code Example\n```yaml\n# Minimal skeleton highlights (not full manifest)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-deploy\n  namespace: payments\nspec:\n  template:\n    spec:\n      initContainers:\n        - name: installer\n          image: python:3.9-slim\n          command: [\"sh\", \"-c\", \"pip install flask\"]\n      containers:\n        - name: api\n          image: my-registry/api:latest\n          env:\n            - name: DEST_ENDPOINT\n              valueFrom:\n                configMapKeyRef:\n                  name: app-config\n                  key: DEST_ENDPOINT\n            - name: LOG_LEVEL\n              valueFrom:\n                configMapKeyRef:\n                  name: app-config\n                  key: LOG_LEVEL\n            - name: API_KEY\n              valueFrom:\n                secretKeyRef:\n                  name: api-credentials\n                  key: API_KEY\n      volumes:\n        - name: tls\n          secret:\n            secretName: tls-certs\n      # probes, securityContext, etc.\n```\n\n## Follow-up Questions\n- How would you test the NetworkPolicy in CI? \n- What are the risks of cert rotation on service availability?","diagram":"flowchart TD\n  A[Deployment] --> B[InitContainer: install deps]\n  A --> C[Main container: REST API]\n  A --> D[Sidecar: TLS rotation]\n  B --> E[ConfigMap: app-config]\n  B --> F[Secret: api-credentials]\n  A --> G[NetworkPolicy: allow DEST_ENDPOINT]","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T12:57:57.293Z","createdAt":"2026-01-17T12:57:57.293Z"},{"id":"q-3410","question":"Design a per-namespace log-forwarder in CKAD terms: a Deployment that tails logs from a PVC-mounted /logs directory produced by apps in the namespace, aggregates log counts by severity for the last minute, and POSTs a heartbeat payload plus the count to a REST endpoint defined in a per-namespace ConfigMap ENDPOINT using API_TOKEN from a Secret. On failure, write payloads to a DLQ directory in the same PVC. Include an InitContainer to install curl and jq, readiness/liveness probes, and a minimal YAML skeleton; discuss trade-offs?","answer":"Deploy a per-namespace log-forwarder: a single-rep Deployment that mounts a PVC at /logs and /logs/dlq. InitContainer installs curl/jq. The forwarder tails new lines in /logs, classifies by severity, ","explanation":"## Why This Is Asked\nTests CKAD patterns for per-namespace resources, log pipelines, and robust error handling with limited tooling.\n\n## Key Concepts\n- Namespace-scoped Deployments and PVCs\n- InitContainers for bootstrap tooling\n- ConfigMap/Secret for endpoint and credentials\n- Simple in-Pod state via streaming tail and per-minute aggregation\n- DLQ on PVC for failed forwards\n- Readiness/Liveness probes\n\n## Code Example\n\n```yaml\n# skeleton Deployment (high level)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: log-forwarder\n  namespace: <namespace>\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: log-forwarder\n    spec:\n      initContainers:\n        - name: installer\n          image: alpine:3.18\n          command: [\"sh\", \"-c\", \"apk add --no-cache curl jq\"]\n      containers:\n        - name: forwarder\n          image: some-forwarder:latest\n          volumeMounts:\n            - name: logs\n              mountPath: /logs\n            - name: dlq\n              mountPath: /logs/dlq\n          env:\n            - name: ENDPOINT\n              valueFrom:\n                configMapKeyRef:\n                  name: log-forwarder-config\n                  key: ENDPOINT\n            - name: API_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: log-forwarder-secret\n                  key: API_TOKEN\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 15\n            periodSeconds: 10\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 30\n            periodSeconds: 10\n      volumes:\n        - name: logs\n          persistentVolumeClaim:\n            claimName: logs-pvc\n        - name: dlq\n          persistentVolumeClaim:\n            claimName: logs-dlq-pvc\n```\n\n## Follow-up Questions\n- How would you scale and parallelize across namespaces?\n- How to secure endpoint access and rotate API_TOKEN?","diagram":"flowchart TD\n  N[Namespace PVC: /logs] --> I[InitContainer: install curl/jq]\n  I --> F[Forwarder: tail /logs, aggregate last-minute counts by severity]\n  F --> E[POST to ENDPOINT with TOKEN from Secret]\n  E -->|success| S[Healthy]\n  E -->|failure| DLQ[DLQ: /logs/dlq]","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T15:29:15.253Z","createdAt":"2026-01-17T15:29:15.253Z"},{"id":"q-3480","question":"In CKAD terms, design a per-namespace Deployment that serves a static landing page with Nginx. The site content lives in a PVC mounted at /usr/share/nginx/html. Use a ConfigMap named site-config with SITE_TITLE and FOOTER, and a Secret named site-auth for htpasswd-based basic authentication. Include an InitContainer to pre-populate index.html from /config data, a readiness/liveness probe for /. Provide a Namespace NetworkPolicy restricting egress to an internal CDN at 10.0.0.0/16:443. Include a minimal YAML skeleton and discuss trade-offs?","answer":"Design a per-namespace Deployment: Nginx serves a static site from a PVC mounted at /usr/share/nginx/html. InitContainer runs alpine, reads /config/SITE_TITLE and /config/FOOTER from the ConfigMap mou","explanation":"## Why This Is Asked\nTests ability to compose common CKAD primitives: Deployment, InitContainer writing into a PVC, ConfigMap/Secret handling, basic auth, and network controls at namespace scope.\n\n## Key Concepts\n- InitContainer writes dynamic index.html from ConfigMap data\n- PVC for content, ConfigMap for config, Secret for credentials\n- Readiness/Liveness probes, TLS/basics via htpasswd\n- Namespace NetworkPolicy restricting external access\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: site\n  namespace: ns-example\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: site\n  template:\n    metadata:\n      labels:\n        app: site\n    spec:\n      initContainers:\n      - name: gen-index\n        image: alpine:3.18\n        command: [\"/bin/sh\", \"-c\", \"SITE_TITLE=$(cat /config/SITE_TITLE); FOOTER=$(cat /config/FOOTER); cat > /usr/share/nginx/html/index.html <<EOF\\n<html><head><title>$SITE_TITLE</title></head><body><h1>$SITE_TITLE</h1><p>$FOOTER</p></body></html>\\nEOF\"]\n        volumeMounts:\n        - name: html\n          mountPath: /usr/share/nginx/html\n        - name: config\n          mountPath: /config\n      containers:\n      - name: nginx\n        image: nginx:1.25-alpine\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - name: html\n          mountPath: /usr/share/nginx/html\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 15\n          periodSeconds: 20\n      volumes:\n      - name: html\n        persistentVolumeClaim:\n          claimName: site-content\n      - name: config\n        configMap:\n          name: site-config\n      - name: auth\n        secret:\n          secretName: site-auth\n```\n\n## Follow-up Questions\n- How would you rotate the htpasswd file without downtime?\n- How would you test the readiness probe in CI/CD?","diagram":"flowchart TD\n  A[Namespace ns-example] --> B[Deployment site]\n  B --> C[InitContainer gen-index]\n  B --> D[Main container nginx]\n  B --> E[ConfigMap: site-config]\n  B --> F[Secret: site-auth]\n  B --> G[PVC: site-content]\n  B --> H[NetworkPolicy: restrict egress to CDN]","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Oracle","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T18:39:21.788Z","createdAt":"2026-01-17T18:39:21.788Z"},{"id":"q-3548","question":"In CKAD terms, design a per-namespace Deployment that serves a tiny REST API (Flask) backed by a local SQLite DB stored on a PVC. Use a ConfigMap app-config to toggle ENABLE_FEATURE and set API_TIMEOUT, and a Secret api-credentials for basic-auth. Include an InitContainer to install deps, a TLS cert rotation sidecar, readiness/liveness probes, and a NamespaceNetworkPolicy restricting egress to the internal auth-service host. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Deployment per-namespace serves a Flask REST API backed by a SQLite DB on a PVC. InitContainer installs Python, Flask, sqlite3; main container uses ConfigMap app-config (ENABLE_FEATURE, API_TIMEOUT) a","explanation":"## Why This Is Asked\nTests CKAD ability to combine common primitives: PVC-backed data, ConfigMap-driven behavior, Secrets for credentials, InitContainers, TLS rotation sidecar, probes, and per-namespace NetworkPolicy in a cohesive, beginner-friendly scenario.\n\n## Key Concepts\n- ConfigMap/Secret usage for runtime config and credentials\n- PVC-backed storage and data lifecycle\n- InitContainer for dependencies\n- TLS cert rotation sidecar\n- Readiness/Liveness probes and health checks\n- NamespaceNetworkPolicy scoping\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ns-api\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: ns-api\n    spec:\n      volumes:\n        - name: data\n          persistentVolumeClaim:\n            claimName: ns-app-data\n      initContainers:\n        - name: install-deps\n          image: python:3.11\n          command: [\"bash\",\"-lc\",\"pip install flask sqlite3\"]\n      containers:\n        - name: api\n          image: myregistry/ns-api:latest\n          ports:\n            - containerPort: 8080\n          env:\n            - name: ENABLE_FEATURE\n              valueFrom:\n                configMapKeyRef:\n                  name: app-config\n                  key: ENABLE_FEATURE\n            - name: API_TIMEOUT\n              valueFrom:\n                configMapKeyRef:\n                  name: app-config\n                  key: API_TIMEOUT\n          volumeMounts:\n            - name: data\n              mountPath: /data\n      - name: tls-sidecar\n        image: quay.io/cert_manager/tls-sidecar:latest\n        # sidecar config omitted for brevity\n      terminationGracePeriodSeconds: 30\n      restartPolicy: Always\n```\n\n## Follow-up Questions\n- How would you test config flag changes without rolling pods?\n- What are trade-offs of storing DB in a PVC vs ephemeral storage?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Snap","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T20:45:21.505Z","createdAt":"2026-01-17T20:45:21.505Z"},{"id":"q-3637","question":"In CKAD terms, design a per-namespace Deployment that handles file-based ingestion from a PVC mounted /input: it should validate each JSON file against a schema stored in a ConfigMap named schema-config, normalize valid records to a consistent JSON, and POST them to http://data-portal:8080/api/v1/ingest using a Bearer token sourced from a Secret named api-tokens. Include an InitContainer to install jq and a validator, a Sidecar for TLS cert rotation, readiness/liveness probes, a Namespace NetworkPolicy restricting egress to the internal endpoint, and a minimal YAML skeleton. Discuss trade-offs?","answer":"Implement a Deployment that monitors /input for JSON files, validates each against a schema stored in schema-config, normalizes valid records to consistent JSON, and POSTs them to http://data-portal:8080/api/v1/ingest using a Bearer token from the api-tokens Secret. Include an InitContainer to install jq and validation tools, a sidecar for TLS certificate rotation, proper readiness/liveness probes, and a Namespace NetworkPolicy restricting egress to the internal endpoint.","explanation":"## Why This Is Asked\nCKAD candidates often struggle with end-to-end file ingestion using Kubernetes primitives. This question tests volume access, ConfigMap/Secret usage, InitContainers, probes, NetworkPolicy, and retry strategies in a comprehensive scenario.\n\n## Key Concepts\n- ConfigMaps/Secrets for configuration and credentials\n- InitContainers for tooling setup\n- In-cluster networking and security policies\n- File-based ingestion with idempotent POST operations\n- Multi-container pod patterns\n\n## Code Example\n```javascript\n// Pseudo-implementation sketch\nconst payload = readFile('/input/file.json');\nvalidateSchema(payload, getSchema('schema-config'));\nconst normalized = normalize(payload);\nsendToEndpoint('http://data-portal:8080/api/v1/ingest', normalized);\n```\n\n## Trade-offs\n- **File watching vs. event-driven**: File watching is simpler but less efficient than event-based systems\n- **In-cluster validation**: Validates locally but increases pod resource usage\n- **NetworkPolicy restrictions**: Improves security but adds complexity to egress management\n- **Sidecar overhead**: TLS rotation adds reliability but increases resource consumption","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T04:01:50.948Z","createdAt":"2026-01-18T02:41:31.376Z"},{"id":"q-3702","question":"In namespace data, create a CronJob named daily-analytics that runs at 02:00 daily using image analytics-runner:2.0. It must mount a ConfigMap analytics-config (DB_HOST, DB_NAME, QUERY_TIMEOUT) and a Secret analytics-creds (DB_USER, DB_PASSWORD). Enforce no overlaps (concurrencyPolicy: Forbid), backoffLimit: 3, TTLSecondsAfterFinished: 86400, successfulJobsHistoryLimit: 3, and annotate the PodTemplate with a checksum/config to reload on changes. How would you verify this under simulated load and when the config changes?","answer":"Configure a CronJob named daily-analytics in namespace data using image analytics-runner:2.0, schedule 0 2 * * *, with ConfigMap analytics-config (DB_HOST, DB_NAME, QUERY_TIMEOUT) and Secret analytics","explanation":"## Why This Is Asked\nTests ability to design a batch CronJob workflow with secrets/config, idempotent restarts, and lifecycle controls. It also probes config-driven restarts and observable validation under load.\n\n## Key Concepts\n- CronJob scheduling and lifecycle\n- ConfigMap and Secret usage in Jobs\n- PodTemplate annotations for config reloads\n- Job history, TTL, and backoff behavior\n- Validation under simulated load\n\n## Code Example\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: daily-analytics\n  namespace: data\nspec:\n  schedule: \"0 2 * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: analytics\n            image: analytics-runner:2.0\n            envFrom:\n            - configMapRef:\n                name: analytics-config\n            - secretRef:\n                name: analytics-creds\n          restartPolicy: OnFailure\n```\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: analytics-config\n  namespace: data\ndata:\n  DB_HOST: \"db.internal\"\n  DB_NAME: \"analytics\"\n  QUERY_TIMEOUT: \"30s\"\n```\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: analytics-creds\n  namespace: data\ntype: Opaque\nstringData:\n  DB_USER: \"analytics\"\n  DB_PASSWORD: \"s3cr3t\"\n```\n\n## Follow-up Questions\n- How would you test idempotency of cron jobs across config changes?\n- How would you monitor this to detect overdue runs?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Snap","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T06:42:43.612Z","createdAt":"2026-01-18T06:42:43.613Z"},{"id":"q-3798","question":"In CKAD terms, design a per-namespace log-collector Deployment that mounts /logs from a PVC, tails app.log for ERROR lines, batches them (configurable via a ConfigMap named app-config with BATCH_SIZE), and POSTs JSON payloads to an internal endpoint at http://analytics.local:9000/ingest using credentials from a Secret named api-credentials. Use an InitContainer to install a minimal Python runtime + requests, a Sidecar for TLS cert rotation, readiness/liveness probes, and a Namespace NetworkPolicy restricting egress to the internal endpoint. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Implement a per-namespace Deployment that mounts /logs from a PVC, uses inotify-based watcher (Python) to tail app.log, filters lines matching ERROR, batches them (BATCH_SIZE from app-config), and pos","explanation":"## Why This Is Asked\nTests practical use of per-namespace isolation for log processing, dynamic config via ConfigMap, secret-based credentials, and a robust data path with batching, retries, and a DLQ. Requires InitContainer setup, sidecars for TLS, and proper networking constraints.\n\n## Key Concepts\n- ConfigMap/Secret usage for runtime config and sensitive data\n- InitContainer for dependencies\n- Sidecar for TLS cert rotation\n- Readiness/Liveness probes\n- NetworkPolicy scoping to internal endpoint\n- DLQ on PVC for failed payloads\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: log-collector\n  namespace: default\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: log-collector\n    spec:\n      initContainers:\n        - name: init-deps\n          image: python:3.11-slim\n          command: [\"sh\", \"-c\", \"pip install --no-cache-dir requests\"]\n          volumeMounts:\n            - name: logs\n              mountPath: /logs\n      containers:\n        - name: collector\n          image: my org/log-collector:latest\n          volumeMounts:\n            - name: logs\n              mountPath: /logs\n          env:\n            - name: BATCH_SIZE\n              valueFrom:\n                configMapKeyRef:\n                  name: app-config\n                  key: BATCH_SIZE\n        - name: tls-sidecar\n          image: myorg/tls-sidecar:latest\n          volumeMounts:\n            - name: tls\n              mountPath: /tls\n      volumes:\n        - name: logs\n          persistentVolumeClaim:\n            claimName: logs-pvc\n        - name: tls\n          secret:\n            secretName: tls-certs\n        - name: dlq\n          persistentVolumeClaim:\n            claimName: log-dlq-pvc\n      # NetworkPolicy defined in separate manifest\n```\n\n## Follow-up Questions\n- How would you handle backpressure if analytics.local is slow to ingest?\n- What changes would you make for multi-tenant isolation across namespaces?","diagram":"flowchart TD\n  PVC[Logs PVC] --> Watcher[Watcher]\n  Watcher --> Batcher[Batcher]\n  Batcher --> API[Analytics API]\n  API --> DLQ[DLQ PVC]","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T09:43:55.795Z","createdAt":"2026-01-18T09:43:55.795Z"},{"id":"q-3915","question":"In CKAD terms, design a per-namespace Deployment that exposes a tiny HTTP API to store and retrieve JSON blobs using a PVC-backed path. Endpoints: POST /store with {id, blob}, GET /store/{id}. Use a ConfigMap named app-config with MAX_BLOB_SIZE, and a Secret named api-credentials. Include an InitContainer to install a minimal store tool, a Sidecar for TLS cert rotation, readiness/liveness probes, and a Namespace NetworkPolicy restricting egress to an internal auth service. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Implementation would create a Deployment with a small HTTP API that stores JSON blobs in a PVC-backed directory. It validates payloads against a schema, writes files named by id, and serves /store/{id","explanation":"## Why This Is Asked\nTests ability to compose Kubernetes primitives (PVC, ConfigMap, Secret, InitContainer), simple HTTP API, file-based persistence, TLS rotation, and network controls at CKAD level. Encourages thinking about concurrency, idempotence, and failure modes.\n\n## Key Concepts\n- PVC-backed file store\n- InitContainer bootstrap\n- ConfigMap and Secret usage\n- TLS cert rotation sidecar\n- Readiness/Liveness probes\n- Namespace NetworkPolicy\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: blob-store\nspec:\n  template:\n    spec:\n      containers:\n      - name: api\n        image: example/api:latest\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet: {path: /health, port: 8080}\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        volumeMounts:\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: blob-pvc\n```\n\n## Follow-up Questions\n- How would you enforce idempotence for POST /store?\n- How would you simulate pod rescheduling without data loss?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T14:54:44.685Z","createdAt":"2026-01-18T14:54:44.685Z"},{"id":"q-4031","question":"In a CKAD scenario, in namespace dataops, create a CronJob named db-backup that runs daily at 02:00 UTC to back up a PostgreSQL database. Use a ConfigMap backup-config for BACKUP_DIR and DB_HOST and a Secret db-credentials for DB_USER and DB_PASSWORD. Mount PVC backup-pvc at /backups and write backups as /backups/<dbname>-YYYYMMDD.sql. Enforce concurrencyPolicy: Forbid, keep 7 successful jobs, and set activeDeadlineSeconds: 7200. Provide manifests and a straightforward validation and rollback plan?","answer":"Create a CronJob manifest and related resources to back up PostgreSQL daily. Use ConfigMap backup-config for BACKUP_DIR and DB_HOST, Secret db-credentials for DB_USER/DB_PASSWORD, mount PVC backup-pvc at /backups, and write backups as /backups/<dbname>-YYYYMMDD.sql. Enforce concurrencyPolicy: Forbid, keep 7 successful jobs, and set activeDeadlineSeconds: 7200.","explanation":"## Why This Is Asked\nTests ability to compose Kubernetes primitives for practical automation: CronJobs, Jobs, ConfigMaps, Secrets, and PVCs, plus robust scheduling and rollback.\n\n## Key Concepts\n- CronJob scheduling and jobTemplate usage\n- ConfigMap and Secret as environment sources\n- PVC mounting in Jobs for persistent backups\n- Concurrency policy and job history controls\n- Rollback and validation workflows\n\n## Code Example\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: db-backup\n  namespace: dataops\nspec:\n  schedule: \"0 2 * * *\"\n  concurrencyPolicy: Forbid\n  successfulJobsHistoryLimit: 7\n  activeDeadlineSeconds: 7200\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: postgres-backup\n            image: postgres:15\n            env:\n            - name: BACKUP_DIR\n              valueFrom:\n                configMapKeyRef:\n                  name: backup-config\n                  key: BACKUP_DIR\n            - name: DB_HOST\n              valueFrom:\n                configMapKeyRef:\n                  name: backup-config\n                  key: DB_HOST\n            - name: DB_USER\n              valueFrom:\n                secretKeyRef:\n                  name: db-credentials\n                  key: DB_USER\n            - name: DB_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: db-credentials\n                  key: DB_PASSWORD\n            command:\n            - /bin/bash\n            - -c\n            - |\n              DATE=$(date +%Y%m%d)\n              BACKUP_FILE=\"/backups/${DB_NAME}-${DATE}.sql\"\n              pg_dump -h $DB_HOST -U $DB_USER -d $DB_NAME > $BACKUP_FILE\n            volumeMounts:\n            - name: backup-storage\n              mountPath: /backups\n          volumes:\n          - name: backup-storage\n            persistentVolumeClaim:\n              claimName: backup-pvc\n          restartPolicy: OnFailure\n```\n\n## Validation Plan\n1. Check CronJob creation: `kubectl get cronjob db-backup -n dataops`\n2. Verify schedule: `kubectl describe cronjob db-backup -n dataops`\n3. Test manual job: `kubectl create job --from=cronjob/db-backup test-backup -n dataops`\n4. Monitor job completion: `kubectl get jobs -n dataops -w`\n5. Validate backup files: `kubectl exec -it <pod-name> -n dataops -- ls -la /backups`\n\n## Rollback Plan\n1. Delete CronJob: `kubectl delete cronjob db-backup -n dataops`\n2. Clean up test jobs: `kubectl delete jobs -l job-name=db-backup -n dataops`\n3. Remove backup files if needed: Access PVC and delete specific backup files\n4. Recreate with corrected manifest if issues persist","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T05:05:22.693Z","createdAt":"2026-01-18T20:44:03.265Z"},{"id":"q-4049","question":"In CKAD terms, implement a per-namespace BackupPlan pattern (CRD) that triggers backups of a StatefulSet's PVC to a per-namespace S3-like bucket. Define CRD fields for targetStatefulSet, sourcePVC, destBucket, destPrefix; show a minimal Job-based workflow with an InitContainer to install awscli, a Secret for AWS creds, a ConfigMap for BUCKET/PREFIX, and a PVC for logs. Include readiness/liveness probes and a minimal YAML skeleton; discuss trade-offs?","answer":"Implement a per-namespace BackupPlan CRD (name, targetStatefulSet, sourcePVC, destBucket, destPrefix). A lightweight controller watches BackupPlan in the namespace and creates a Job per plan named bac","explanation":"## Why This Is Asked\nTests ability to design Kubernetes-native backup patterns using CRD + Job orchestration, with proper use of InitContainers, Secrets, ConfigMaps, and PVs. Encourages thinking about per-namespace scoping, fault-tolerance, and minimal operator logic.\n\n## Key Concepts\n- CustomResourceDefinition and a namespace-scoped controller\n- Jobs, InitContainers, and data packaging (tarball) for backups\n- Secrets for credentials and ConfigMaps for configuration\n- PVC usage for temporary/log data and readiness/liveness probes\n- Idempotency, retries, and failure semantics\n\n## Code Example\n```yaml\n# CRD skeleton (minimal)\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: backplans.example.com\nspec:\n  group: example.com\n  versions:\n    - name: v1\n      served: true\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n  scope: Namespaced\n  names:\n    plural: backplans\n    singular: backplan\n    kind: BackupPlan\n\n---\n# Minimal Job skeleton per BackupPlan\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: backup-<plan>\nspec:\n  template:\n    spec:\n      initContainers:\n        - name: awscli\n          image: <image-with-awscli>\n          command: [\"sh\", \"-c\", \"aws --version\"]\n      containers:\n        - name: tar-and-upload\n          image: <tar-awsupload-image>\n          command: [\"/bin/sh\", \"-c\", \"tar czf /logs/backup.tar.gz -C /data . && aws s3 cp /logs/backup.tar.gz s3://<bucket>/<prefix>/ backup-<plan>.tar.gz\"]\n          volumeMounts:\n            - name: data\n              mountPath: /data\n            - name: logs\n              mountPath: /logs\n      volumes:\n        - name: data\n          persistentVolumeClaim:\n            claimName: <source-pvc>\n        - name: logs\n          emptyDir: {}\n      restartPolicy: OnFailure\n      readinessProbe:\n        tcpSocket:\n          port: 9100\n      livenessProbe:\n        httpGet:\n          path: /healthz\n          port: 8080\n```\n\n## Follow-up Questions\n- How would you handle partial failures or large PVCs to avoid timeouts?\n- What changes to support multi-region backups and encryption at rest?","diagram":"flowchart TD\n  A[BackupPlan Created in Namespace] --> B[Controller detects Plan]\n  B --> C[Create Job backup-PLAN]\n  C --> D[Job InitContainer installs awscli]\n  D --> E[Main Container tarballs PVC data]\n  E --> F[Uploads to s3://bucket/prefix/plan.tar.gz]\n  F --> G[Status update in CRD / heartbeat ConfigMap]","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Meta","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T21:36:36.687Z","createdAt":"2026-01-18T21:36:36.687Z"},{"id":"q-4184","question":"Design a per-namespace synthetic-traffic generator Deployment in CKAD terms: it reads TARGET_URL, PAYLOAD, and HEADERS from a Namespace ConfigMap, and a TOKEN from a Secret; an InitContainer installs curl. It should generate requests at a configurable rate, log each attempt to a PVC at /logs, and on failure move the payload to a DLQ on the same PVC. Include readiness/liveness probes and a CPU-based HPA; provide a minimal YAML skeleton and discuss trade-offs?","answer":"Use a Deployment with envFrom ConfigMap for TARGET_URL, PAYLOAD, HEADERS and a Secret TOKEN. An InitContainer installs curl (apk add --no-cache curl). A worker loop hits TARGET_URL at a configurable r","explanation":"## Why This Is Asked\nTests CKAD proficiency in namespace scoping, ConfigMap/Secret wiring, InitContainer usage, persistent logging, DLQ handling, and monitoring hooks.\n\n## Key Concepts\n- Namespace-scoped ConfigMaps and Secrets\n- InitContainer to install curl\n- Deployment with a worker loop\n- PVC-backed logs and DLQ\n- Readiness/Liveness probes\n- HorizontalPodAutoscaler by CPU\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: traffic-gen\n  namespace: example-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: traffic-gen\n  template:\n    metadata:\n      labels:\n        app: traffic-gen\n    spec:\n      initContainers:\n      - name: install-curl\n        image: alpine:3.18\n        command: [\"sh\", \"-c\", \"apk add --no-cache curl && mkdir -p /logs/dlq\"]\n        volumeMounts:\n        - name: logs\n          mountPath: /logs\n      containers:\n      - name: worker\n        image: curlimages/curl:8.1.2\n        envFrom:\n        - configMapRef:\n            name: traffic-config\n        - secretRef:\n            name: traffic-secret\n        volumeMounts:\n        - name: logs\n          mountPath: /logs\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        livenessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: traffic-logs-pvc\n```\n","diagram":"flowchart TD\n  NS[Namespace] --> D[Deployment: traffic-gen]\n  D --> IC[InitContainer: install curl]\n  D --> W[Worker: traffic generator]\n  W --> Logs[/logs]\n  W --> DLQ[/logs/dlq]\n  W --> Probes[Probes]\n  W --> HPA[HPA: CPU]","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Twitter","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T07:01:40.590Z","createdAt":"2026-01-19T07:01:40.590Z"},{"id":"q-4196","question":"CKAD intermediate: In namespace prod, the 4-replica Deployment 'payments-api' must talk to an upstream processor at payments.proc.svc:443 using mTLS. Create minimal Kubernetes manifests to mount a Secret tls-secret containing client cert/key and CA, configure the container to use those certs, and restrict egress to only that endpoint via a NetworkPolicy. Show the patches and explain how you'd verify TLS and cert rotation?","answer":"Mount tls-secret at /certs in payments-api, set TLS_PATH=/certs, and configure the client to present the certificate when connecting to payments.proc.svc:443; add a NetworkPolicy that restricts egress","explanation":"## Why This Is Asked\nTests secret handling, TLS/mTLS basics, mounting certs, and correct network isolation for a real-world integration.\n\n## Key Concepts\n- Secrets and volume mounts\n- TLS/Certificates and mTLS basics\n- NetworkPolicy egress controls\n- Rollout and secret rotation strategies\n\n## Code Example\n```javascript\napiVersion: v1\nkind: Secret\nmetadata:\n  name: tls-secret\ntype: Opaque\ndata:\n  tls.crt: <base64>\n  tls.key: <base64>\n  ca.crt: <base64>\n```\n\n```javascript\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: payments-api\n  namespace: prod\nspec:\n  template:\n    spec:\n      containers:\n      - name: payments\n        image: payments-api:latest\n        env:\n        - name: TLS_PATH\n          value: /certs\n        volumeMounts:\n        - name: tls\n          mountPath: /certs\n      volumes:\n      - name: tls\n        secret:\n          secretName: tls-secret\n```\n\n```javascript\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: payments-egress\n  namespace: prod\nspec:\n  podSelector:\n    matchLabels:\n      app: payments-api\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - ipBlock:\n        cidr: 203.0.113.45/32\n    ports:\n    - protocol: TCP\n      port: 443\n```\n\n> Note: DNS-based egress restrictions are not widely supported in NetworkPolicy; use the processor’s IP in the ipBlock.","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","IBM","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T08:07:27.645Z","createdAt":"2026-01-19T08:07:27.647Z"},{"id":"q-4282","question":"In CKAD terms, design a per-namespace Job that migrates data from a PVC /data/migrate by reading CSVs with csvkit (InitContainer installs tools), sending batched JSON to http://data-portal:8080/api/v1/migrate. Configure MIG_BATCH_SIZE in mig-config ConfigMap and API token in mig-creds Secret. Use a PVC-stored marker for idempotence, set backoffLimit, and add readiness/liveness probes. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Draft a per-namespace Job that migrates data from a PVC /data/migrate by reading CSVs with csvkit (InitContainer installs tools), sending batched JSON to http://data-portal:8080/api/v1/migrate. MIG_BA","explanation":"Why This Is Asked\n- Validates CKAD basics: Jobs, InitContainers, ConfigMaps, Secrets, PVCs, probes in a practical data-migrate flow.\n- Introduces idempotence with a marker, a common real-world concern.\n\nKey Concepts\n- Job lifecycle and completion guarantees\n- InitContainer usage for tooling installation\n- Runtime config from ConfigMaps and Secrets\n- PVC-based idempotence markers and backoff settings\n- Probes and minimal YAML scaffolding\n\nCode Example\n```yaml\n# Minimal Job skeleton (pseudo)\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: data-migrate\nspec:\n  backoffLimit: 3\n  template:\n    spec:\n      initContainers:\n      - name: csvkit-install\n        image: alpine:latest\n        command: [\"sh\", \"-c\", \"apk add --no-cache python3 py3-pip && pip3 install csvkit\"]\n      containers:\n      - name: migrator\n        image: curlimages/curl:7.88.0\n        command: [\"sh\", \"-c\", \"python3 migrate.py\"]\n        env:\n        - name: MIG_BATCH_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: mig-config\n              key: MIG_BATCH_SIZE\n        - name: API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: mig-creds\n              key: api-token\n        volumeMounts:\n        - name: data\n          mountPath: /data/migrate\n        - name: marker\n          mountPath: /data/marker\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: migrate-pvc\n      - name: marker\n        persistentVolumeClaim:\n          claimName: migrate-marker-pvc\n      restartPolicy: OnFailure\n      # probes would be defined on the migrator container\n```\n\nFollow-up Questions\n- How would you test idempotence across retries?\n- How would you handle partial CSV failures and ensure exactly-once semantics?","diagram":"flowchart TD\n  A[Start] --> B[InitContainer: install csvkit]\n  B --> C[Read /data/migrate]\n  C --> D[Batch to /api/v1/migrate]\n  D --> E[Write marker to PVC]\n  E --> F[Job completes]","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T11:32:11.040Z","createdAt":"2026-01-19T11:32:11.040Z"},{"id":"q-4418","question":"In CKAD terms, design a per-namespace Deployment that runs a rate-limited API proxy inside the cluster. It should read its rate limit from a ConfigMap named app-config, authenticate upstream calls with a Secret named api-credentials, and initialize dependencies in an InitContainer. Include a TLS cert-rotation sidecar using a Secret named tls-certs, readiness/liveness probes, and a Namespace NetworkPolicy restricting egress to upstream.internal:8080. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Design a per-namespace Deployment that runs a rate-limited API proxy. It uses a ConfigMap app-config with RPS, a Secret api-credentials for upstream API key, and an InitContainer to install deps. A TL","explanation":"## Why This Is Asked\nTests understanding of combining Kubernetes primitives for a practical, beginner-to-intermediate task: ConfigMaps and Secrets for config and credentials, InitContainers for bootstrapping, a TLS rotation sidecar for security, probes for reliability, and a Namespace NetworkPolicy to constrain egress.\n\n## Key Concepts\n- ConfigMap and Secret usage in a single pod\n- InitContainer for bootstrapping dependencies\n- Sidecar for TLS cert rotation\n- Readiness and Liveness probes\n- Namespace NetworkPolicy to constrain egress\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: proxy\n  namespace: default\nspec:\n  replicas: 1\n  template:\n    spec:\n      initContainers:\n      - name: install-deps\n        image: alpine:3.18\n        command: [\"/bin/sh\",\"-lc\",\"apk add --no-cache python3 py3-pip && pip3 install fastapi uvicorn httpx\"]\n        volumeMounts:\n        - name: config\n          mountPath: /config\n      containers:\n      - name: proxy\n        image: myorg/proxy:latest\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: app-config\n        - secretRef:\n            name: api-credentials\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n      volumes:\n      - name: config\n        configMap:\n          name: app-config\n      - name: certs\n        secret:\n          secretName: tls-certs\n```\n\n## Follow-up Questions\n- How would you test the rate limiter under burst traffic?\n- How would you rotate keys without downtime?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Uber","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T17:59:24.422Z","createdAt":"2026-01-19T17:59:24.422Z"},{"id":"q-4509","question":"CKAD intermediate: In namespace 'prod', a Deployment named 'image-processor' with 4 replicas experiences spikes under load. Propose and implement a minimal patch to: (a) add resource requests/limits and health probes; (b) configure an HPA targeting 75% CPU (min 3, max 10 replicas); (c) add a PodDisruptionBudget to keep at least 3 healthy pods; (d) tune rolling updates (maxUnavailable:1). Then describe how you'd validate under simulated load and rollback if needed?","answer":"Implement a comprehensive patch to the Deployment by adding resource requests/limits (cpu: 100m/500m, memory: 128Mi/512Mi), configuring readinessProbe and livenessProbe with /healthz endpoint, deploying an HPA with minReplicas: 3, maxReplicas: 10, targetCPUUtilizationPercent: 75, creating a PodDisruptionBudget with minAvailable: 3, and setting rollingUpdate strategy with maxUnavailable: 1. Validate the configuration using load testing tools like k6 or hey while monitoring HPA scaling behavior and pod health metrics, and rollback via kubectl rollout undo deployment/image-processor if issues arise.","explanation":"## Why This Is Asked\n\nAssesses practical CKAD mastery of production-ready deployments, autoscaling configuration, and operational safety measures in Kubernetes environments.\n\n## Key Concepts\n\n- Resource requests/limits for predictable performance and resource allocation\n- Readiness and liveness probes for health monitoring and traffic management\n- HorizontalPodAutoscaler for dynamic scaling based on CPU utilization\n- PodDisruptionBudget for availability guarantees during maintenance\n- RollingUpdate strategy for controlled deployments with zero downtime\n\n## Code Example\n\n```yaml\nresources:\n  requests:\n    cpu: \"100m\"\n    memory: \"128Mi\"\n  limits:\n    cpu: \"500m\"\n    memory: \"512Mi\"\nreadinessProbe:\n  httpGet:\n    path: /healthz\n    port: 8080\n  initialDelaySeconds: 5\n  periodSeconds: 10\nlivenessProbe:\n  httpGet:\n    path: /healthz\n    port: 8080\n  initialDelaySeconds: 15\n  periodSeconds: 20\n```\n\n```yaml\n# HPA Configuration\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: image-processor-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: image-processor\n  minReplicas: 3\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 75\n```","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","MongoDB","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T07:38:47.157Z","createdAt":"2026-01-19T21:49:54.992Z"},{"id":"q-4635","question":"In CKAD terms, design a per-namespace Deployment that serves a static site via nginx. Use a PVC-backed volume mounted at /usr/share/nginx/html, an InitContainer to fetch assets from an internal artifact registry based on a ConfigMap site-config (INDEX_FILE, THEME), and a Secret site-basic-auth for HTTP basic auth. Include readiness/liveness probes, and a Namespace NetworkPolicy that restricts egress to the asset registry and internal DNS. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Create a Deployment with nginx:alpine, mount a PVC at /usr/share/nginx/html, and an InitContainer that downloads assets from the internal registry based on site-config (INDEX_FILE, THEME) writing into","explanation":"## Why This Is Asked\nTests ability to compose CKAD primitives: ConfigMaps for runtime knobs, Secrets for credentials, InitContainers for bootstrapping assets, PVCs for content, and NetworkPolicy for egress control, all in a per-namespace scope.\n\n## Key Concepts\n- InitContainers, ConfigMaps, Secrets, PVCs\n- Liveness/Readiness probes\n- NetworkPolicy, egress controls\n- Static site hosting with nginx\n\n## Code Example\n```yaml\n# Minimal Deployment skeleton (conceptual)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: static-site\nspec:\n  template:\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:alpine\n        volumeMounts:\n        - mountPath: /usr/share/nginx/html\n          name: html\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n      initContainers:\n      - name: fetch-assets\n        image: appropriate/cull\n        # placeholder for fetch logic using site-config\n        volumeMounts:\n        - mountPath: /html\n          name: html\n      volumes:\n      - name: html\n        persistentVolumeClaim:\n          claimName: site-pvc\n      # ... Secrets/ConfigMaps mounted as envs or files\n```\n\n## Follow-up Questions\n- How would you handle asset updates without redeploying pods?\n- What are the security implications of mounting secrets vs. env vars in this setup?","diagram":"flowchart TD\n  A[ConfigMap site-config] --> B[InitContainer downloads assets]\n  B --> C[HTML content in PVC mounted at /usr/share/nginx/html]\n  C --> D[Nginx serves content]\n  A --> E[Secret site-basic-auth for HTTP auth]","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","DoorDash","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T05:50:44.714Z","createdAt":"2026-01-20T05:50:44.714Z"},{"id":"q-4791","question":"CKAD intermediate: In namespace prod, Deployment payments-api with 4 replicas uses a vendor-base image that runs as root. Provide a minimal patch to enforce a strict security posture: container securityContext (runAsNonRoot: true, runAsUser: 1000, readOnlyRootFilesystem: true, allowPrivilegeEscalation: false, capabilities: drop: ['ALL']); PodSecurityContext (fsGroup: 2000); and include imagePullSecrets if the registry is private. Explain how you would validate under load and rollback if issues arise?","answer":"Patch the Deployment payments-api in prod to add a pod securityContext and container securityContext enforcing non-root operation, read-only root FS, and dropped capabilities; e.g.: RunAsNonRoot: true","explanation":"## Why This Is Asked\nThis checks practical hardening and CKAD adherence under real workflows, not theory. It spans securityContext, PodSecurityContext, imagePullSecrets, and rollback strategies.\n\n## Key Concepts\n- PodSecurityContext and containerSecurityContext\n- runAsNonRoot, runAsUser, readOnlyRootFilesystem, capabilities\n- imagePullSecrets and rollout rollback\n- Load testing and safe rollback procedures\n\n## Code Example\n```yaml\n# patch example (conceptual)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: payments-api\n  namespace: prod\nspec:\n  template:\n    spec:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 1000\n        fsGroup: 2000\n        readOnlyRootFilesystem: true\n      containers:\n      - name: payments-api\n        image: vendor/payments-api:latest\n        securityContext:\n          allowPrivilegeEscalation: false\n          runAsNonRoot: true\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop: ['ALL']\n```\n\n## Follow-up Questions\n- How would you adapt if the vendor image requires a non-root user different from 1000?\n- What metrics and rollback threshold would you use to safely roll this out across clusters?","diagram":"flowchart TD\n  A[Identify risk] --> B[Apply patch]\n  B --> C[Validate in staging]\n  C --> D[Rollout to prod]\n  D --> E[Monitor & rollback if issues]","difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Scale Ai","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T13:13:20.529Z","createdAt":"2026-01-20T13:13:20.529Z"},{"id":"q-4932","question":"In namespace payments, Deployment 'invoice-processor' (3 replicas) uses a ConfigMap and Secret to call an external invoicing API. A rolling upgrade to invoice-processor:2.1 fails health checks under load due to slow startup. Provide a minimal patch to: (1) add a startupProbe and adjust the readinessProbe, (2) add an initContainer to pre-warm a local cache from the ConfigMap, (3) add a PodDisruptionBudget minAvailable: 2. Explain verification under load and rollback steps?","answer":"Patch: add startupProbe httpGet:/healthz:8080 with timeout 2, period 5, failureThreshold 12; set readinessProbe initialDelaySeconds 15, timeoutSeconds 2, periodSeconds 5; add initContainer that fetche","explanation":"## Why This Is Asked\n\nTests practical CKAD debugging: handling slow-start upgrades with probes, init containers for bootstrapping state, and PD&B safeguards to prevent downtime during maintenance.\n\n## Key Concepts\n\n- startupProbe and readinessProbe tuning\n- initContainer for pre-warming caches\n- PodDisruptionBudget for upgrade safety\n- RollingUpdate strategy and rollback procedures\n\n## Code Example\n\n```yaml\n# Minimal patch sketch (Deployment snippet)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: invoice-processor\n  namespace: payments\nspec:\n  replicas: 3\n  template:\n    spec:\n      initContainers:\n      - name: init-cache\n        image: invoice-processor:2.1\n        command: [\"sh\", \"-c\", \"cp /config/* /var/cache/ || true\"]\n        volumeMounts:\n        - name: config\n          mountPath: /config\n        - name: cache\n          mountPath: /var/cache\n      containers:\n      - name: app\n        image: invoice-processor:2.1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n      volumes:\n      - name: config\n        secret:\n          secretName: app-secret\n      - name: cache\n        emptyDir: {}\n```\n\n## Follow-up Questions\n\n- How would you adapt this for a cluster with limited CPU headroom?\n- What metrics would you monitor to confirm startup health and cache warmth?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["PayPal","Salesforce","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T20:22:29.820Z","createdAt":"2026-01-20T20:22:29.820Z"},{"id":"q-4982","question":"In CKAD terms, design a per-namespace caching API deployment that exposes a small REST service on port 8080 and serves cached responses to other pods. It reads CACHE_SIZE and TIMEOUT from a ConfigMap named service-config, and a backend API key from Secret named backend-credentials. Include InitContainer to install dependencies, a Sidecar for TLS cert rotation, readiness/liveness probes, and a Namespace NetworkPolicy restricting egress to backend.svc.cluster.local:8443. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Design a per-namespace Deployment named cache-api that serves on port 8080, utilizing a ConfigMap for CACHE_SIZE and TIMEOUT configuration and a Secret for API_KEY credentials. An InitContainer handles dependency installation, while a sidecar manages TLS certificate rotation. Include readiness and liveness probes for health monitoring, and implement a NetworkPolicy restricting egress traffic to backend.svc.cluster.local:8443.","explanation":"## Why This Is Asked\nTests CKAD fundamentals for multi-component deployments: ConfigMaps/Secrets, InitContainers, sidecars, probes, and namespace-scoped NetworkPolicies in a practical caching service scenario.\n\n## Key Concepts\n- ConfigMap/Secret integration for runtime configuration and credentials\n- InitContainer for bootstrapping dependencies\n- Sidecar pattern for TLS certificate rotation\n- Readiness/Liveness probes for service health monitoring\n- Namespace NetworkPolicy controlling egress traffic\n- Minimal YAML skeleton suitable for live CKAD exam tasks\n\n## Code Example\n```yaml\n<minimal Dep","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Snap","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-21T05:52:05.970Z","createdAt":"2026-01-20T22:35:13.844Z"},{"id":"q-5074","question":"Design a per-namespace CKAD-ready Deployment that acts as a synthetic feature-flag tester: it reads a list of feature endpoints from a Namespace ConfigMap named feature-endpoints, and an API_TOKEN from a Secret. InitContainer installs httpie. The Pod pings each endpoint at a configurable interval, writes per-endpoint results to /logs on a PVC, and updates a last-run summary in feat-test-status ConfigMap. Expose /metrics with per-feature latency; include readiness/liveness probes and a CPU-based HPA. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Design a per-namespace CKAD-ready Deployment that acts as a synthetic feature-flag tester: it reads a list of feature endpoints from a Namespace ConfigMap named feature-endpoints, and an API_TOKEN fro","explanation":"Why This Is Asked\n- Tests ability to wire ConfigMaps and Secrets across a namespace, plus InitContainer usage and lifecycle hooks.\n- Evaluates observability via a metrics endpoint and logs on a PVC, with a simple autoscale path via HPA.\n- Probes, resource requests, and minimal skeletons show readiness for real-world CKAD constraints.\n\nKey Concepts\n- ConfigMap/Secret integration, InitContainer tooling, PVC-backed logs, custom metrics exposure, readiness/liveness, HPA kinship, minimal YAML.\n\nCode Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: feature-tester\n  namespace: test-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: feature-tester\n  template:\n    metadata:\n      labels:\n        app: feature-tester\n    spec:\n      initContainers:\n      - name: init-tools\n        image: alpine:3.18\n        command: [\"sh\", \"-c\", \"apk add --no-cache httpie jq && mkdir -p /logs\"]\n        volumeMounts:\n        - name: logs\n          mountPath: /logs\n      containers:\n      - name: tester\n        image: alpine:3.18\n        command: [\"sh\", \"-c\", \"while true; do for url in $(cat /etc/feature-endpoints/endpoints); do resp=$(http --ignore-stdin GET \"$url\" Authorization:\"Bearer $(cat /secret/api_token)\"); echo \"$url -> $resp\" >> /logs/$(date +%s).log; done; sleep 60; done\"]\n        env:\n        - name: ENDPOINTS_CONFIG\n          value: \"/etc/feature-endpoints/endpoints\"\n        - name: TOKEN_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: api-token\n              key: token\n        volumeMounts:\n        - name: logs\n          mountPath: /logs\n        ports:\n        - containerPort: 8080\n          name: metrics\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n          limits:\n            cpu: \"500m\"\n            memory: \"256Mi\"\n        volumeMounts:\n        - name: logs\n          mountPath: /logs\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: feature-tester-logs\n      - name: feature-endpoints\n        configMap:\n          name: feature-endpoints\n      - name: secret\n        secret:\n          secretName: api-token\n      imagePullSecrets:\n      - name: regcred\n  \n# Note: This is a minimal skeleton; real manifests should separate concerns (InitContainer, metrics, status storage) and handle errors gracefully.\n```","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Robinhood","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T04:34:48.297Z","createdAt":"2026-01-21T04:34:48.297Z"},{"id":"q-5116","question":"CKAD intermediate: In namespace inventory, a Deployment stock-service with 3 replicas mounts an emptyDir at /data and relies on a ConfigMap stock-config and a Secret db-credentials. Implement an InitContainer that waits for a Postgres DB at postgresql.inventory.svc:5432 and seeds data from /seed/init.sql (mounted from a ConfigMap). Ensure InitContainer runs before the app, add readiness and liveness probes for /health, and provide a minimal manifest patch plus the steps to validate under load and rollback?","answer":"Add an InitContainer named seed-db to the stock-service Deployment that waits for postgresql.inventory.svc:5432 and runs psql -f /seed/init.sql using credentials from Secret db-credentials; mount /see","explanation":"## Why This Is Asked\nTests ability to coordinate InitContainers with main containers, use ConfigMap/Secret mounted data, and implement health checks for reliable startups.\n\n## Key Concepts\n- InitContainers sequencing\n- ConfigMap/Secret mounts\n- Probes for startup health\n- Rollback and load testing\n\n## Code Example\n```yaml\n# fragment patch adding initContainer and volumes\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: stock-service\n  namespace: inventory\nspec:\n  template:\n    spec:\n      initContainers:\n      - name: seed-db\n        image: postgres:15\n        command:\n        - bash\n        - -lc\n        - until pg_isready -h postgresql.inventory.svc -p 5432; do sleep 2; done; psql -h postgresql.inventory.svc -U dbuser -d inventory -f /seed/init.sql\n        env:\n        - name: PGPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-credentials\n              key: password\n        volumeMounts:\n        - name: seed-files\n          mountPath: /seed\n      containers:\n      - name: stock-service\n        image: my-reg/stock-service:latest\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n      volumes:\n      - name: seed-files\n        configMap:\n          name: stock-seed-config\n```\n\n## Follow-up Questions\n- How would you ensure idempotent seeds across restarts?\n- How would you scale this pattern for multiple environments?\n","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Airbnb","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T07:03:38.938Z","createdAt":"2026-01-21T07:03:38.938Z"},{"id":"q-5248","question":"CKAD intermediate: In namespace prod, a Deployment 'payments' with 3 replicas needs to talk to an external TLS endpoint using a Secret tls-secret (tls.crt, tls.key). Patch the Deployment to mount the secret as files at /tls, set TLS_CERT_FILE and TLS_KEY_FILE env vars, and add a readiness probe for /healthz. Provide a minimal manifest patch and verification approach?","answer":"Patch: add a secret volume from tls-secret mounted at /tls; set TLS_CERT_FILE=/tls/tls.crt and TLS_KEY_FILE=/tls/tls.key as envs; add a readinessProbe with httpGet /healthz, initialDelaySeconds:5, per","explanation":"## Why This Is Asked\nTests secret mounting, env wiring, and health checks for TLS in a CKAD context.\n\n## Key Concepts\n- Secret as files\n- Env vars from paths\n- Readiness checks for TLS paths\n- Rollout validation and rollback criteria\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: payments\n  namespace: prod\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: payments\n        image: your-image\n        env:\n        - name: TLS_CERT_FILE\n          value: /tls/tls.crt\n        - name: TLS_KEY_FILE\n          value: /tls/tls.key\n        volumeMounts:\n        - name: tls-volume\n          mountPath: /tls\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n      volumes:\n      - name: tls-volume\n        secret:\n          secretName: tls-secret\n```\n\n## Follow-up Questions\n- How would you validate zero-downtime during rollout with TLS cert changes?\n- How would you audit access to tls-secret in Pod logs?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T13:12:44.431Z","createdAt":"2026-01-21T13:12:44.431Z"},{"id":"q-5364","question":"CKAD intermediate: In namespace observability, a Deployment collector with 3 replicas throttles CPU under load. Provide a minimal patch to enable autoscaling and safe upgrades: (1) patch the Deployment with resource requests/limits and health probes; (2) add an HPA for min 2, max 6 with targetCPUUtilizationPercentage: 60; (3) add a PodDisruptionBudget to keep at least 2 healthy pods during maintenance. Include patches and describe validation under load?","answer":"Patch Deployment: set resources (requests cpu: 200m, limits cpu: 500m) and health probes for /health. Add HPA: minReplicas: 2, maxReplicas: 6, targetCPUUtilizationPercentage: 60. Add PodDisruptionBudg","explanation":"## Why This Is Asked\n\nTests practical CKAD skills: autoscaling, robustness, and upgrade safety in real workloads.\n\n## Key Concepts\n\n- Deployment resources and probes\n- HorizontalPodAutoscaler (autoscale on CPU)\n- PodDisruptionBudget for maintenance safety\n- Observability: kubectl top, metrics-server\n\n## Code Example\n\n```yaml\n# Deployment patch\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: collector\nspec:\n  template:\n    spec:\n      containers:\n      - name: collector\n        image: collector:latest\n        resources:\n          requests:\n            cpu: \"200m\"\n          limits:\n            cpu: \"500m\"\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          periodSeconds: 15\n```\n\n```yaml\n# HPA\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: collector-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: collector\n  minReplicas: 2\n  maxReplicas: 6\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 60\n```\n\n```yaml\n# PodDisruptionBudget\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: collector-pdb\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: collector\n```\n\n## Follow-up Questions\n\n- How would you verify HPA behavior under a synthetic load step?\n- What changes if workloads are bursty and you want per-pod CPU caps?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","LinkedIn","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T19:15:55.677Z","createdAt":"2026-01-21T19:15:55.677Z"},{"id":"q-5409","question":"Design a per-namespace canary-traffic tester that coordinates a controlled canary rollout for a single Deployment within the namespace. The test reads CANARY_TARGET_APP, CANARY_VERS, and TARGET_SVC from a Namespace ConfigMap and API_TOKEN from a Secret; an InitContainer installs a lightweight CLI (curl/hey). It should shift 5% of traffic every 2 minutes by updating a ConfigMap entry that tracks canary percentage, logs per-request latency to a PVC at /logs, and stops on full rollout or rollback. Include readiness/liveness probes, a CPU-based HPA, and provide a minimal YAML skeleton; discuss trade-offs?","answer":"Implement a per-namespace canary tester that increments canary_pct in a Namespace ConfigMap by 5% every 2 minutes, sourcing CANARY_TARGET_APP, CANARY_VERS, TARGET_SVC from the ConfigMap and API_TOKEN ","explanation":"## Why This Is Asked\n\nTests ability to design namespace-scoped control loops with observable metrics and safe rollout semantics.\n\n## Key Concepts\n\n- Canary rollout progression via ConfigMap-stored canary_pct\n- Namespace-scoped state in ConfigMap and Secret-backed credentials\n- InitContainer provisioning of a lightweight CLI\n- Readiness/Liveness probes and CPU-based HPA for constrained environments\n- PVC-backed logs for latency measurements and rollback decisions\n\n## Code Example\n\n```yaml\n# Minimal YAML skeleton for the canary tester (Deployment)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: canary-tester\n  namespace: my-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: canary-tester\n  template:\n    metadata:\n      labels:\n        app: canary-tester\n    spec:\n      initContainers:\n        - name: init-deps\n          image: alpine:3.18\n          command: [\"sh\",\"-c\",\"apk add --no-cache curl jq >/dev/null 2>&1\"]\n          volumeMounts:\n            - name: logs\n              mountPath: /logs\n      containers:\n        - name: tester\n          image: alpine:3.18\n          command: [\"sh\",\"-c\",\"sleep infinity\"]\n          volumeMounts:\n            - name: logs\n              mountPath: /logs\n      volumes:\n        - name: logs\n          persistentVolumeClaim:\n            claimName: canary-logs-pvc\n```\n\n## Follow-up Questions\n\n- How would you implement safe rollback if canary_pct exceeds 100% or metrics deteriorate?\n- How would you observe and alert on canary progress using in-cluster tools only?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T21:08:14.241Z","createdAt":"2026-01-21T21:08:14.241Z"},{"id":"q-5425","question":"CKAD intermediate: In namespace ckad, Deployment api-server with 3 replicas uses a seed workflow to populate a per-pod cache on first boot. Implement a minimal InitContainer that executes a seed script from a ConfigMap, uses an PVC-backed /cache to ensure idempotence, and creates a marker file to skip reseed on subsequent restarts. Add readiness and liveness probes for /healthz, and describe how you would verify under load and rollback if needed?","answer":"Add an InitContainer that executes a seed script from a ConfigMap on first pod boot, using a PVC-backed /cache directory with a marker file to ensure idempotence and avoid re-seeding on subsequent restarts. Patch the Deployment to mount the seed script from ConfigMap and configure readiness/liveness probes for /healthz.","explanation":"## Why This Is Asked\nTests understanding of InitContainer lifecycle, idempotent seeding patterns, and persistent cache management across pod restarts.\n\n## Key Concepts\n- InitContainers and startup ordering\n- ConfigMap-based seed script injection\n- PVC-backed shared cache for idempotence\n- Marker file pattern for preventing re-seeding\n- Readiness and liveness probe configuration\n- Load testing and rollback strategies\n\n## Code Example\n```yaml\n# Seed script ConfigMap\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: seed-scripts\ndata:\n  seed.sh: |\n    #!/bin/sh\n    set -e\n    if [ ! -f /cache/.seeded ]; then\n      # Execute seeding logic here\n      touch /cache/.seeded\n    fi\n---\n# Deployment with InitContainer\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-server\n  namespace: ckad\nspec:\n  replicas: 3\n  template:\n    spec:\n      initContainers:\n      - name: seed-cache\n        image: busybox:1.35\n        command: ['/bin/sh', '/scripts/seed.sh']\n        volumeMounts:\n        - name: seed-scripts\n          mountPath: /scripts\n        - name: cache-volume\n          mountPath: /cache\n      containers:\n      - name: api-server\n        image: api-server:latest\n        volumeMounts:\n        - name: cache-volume\n          mountPath: /cache\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n      volumes:\n      - name: seed-scripts\n        configMap:\n          name: seed-scripts\n      - name: cache-volume\n        persistentVolumeClaim:\n          claimName: cache-pvc\n```\n\n## Verification Strategy\n1. **Load Testing**: Use kubectl exec to simulate concurrent requests, monitor cache seeding behavior, and verify marker file prevents re-seeding\n2. **Rollback Approach**: Deploy with strategy type RollingUpdate, set maxUnavailable and maxSurge for controlled rollout, and use kubectl rollout undo if issues arise\n3. **Monitoring**: Check InitContainer logs, verify probe status, and confirm cache persistence across pod restarts","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Instacart","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T06:04:57.414Z","createdAt":"2026-01-21T21:53:09.519Z"},{"id":"q-5503","question":"In namespace data-ingest, a Deployment 'processor' with 2 replicas consumes daily CSV feeds from a mounted ConfigMap and writes to a PVC. Provide a minimal manifest patch to: mount the ConfigMap at /etc/processor, add a checksum/config annotation on the Pod template to trigger rolling restarts when the ConfigMap changes, set resource requests/limits, add readinessProbe on /livez and livenessProbe on /readyz, add a preStop sleep of 10s, and create a PodDisruptionBudget requiring at least 1 healthy pod during maintenance. Include how you'd verify under load and rollback?","answer":"Patch plan: mount the ConfigMap processor-config at /etc/processor in the processor Deployment; add annotation checksum/config: abc123 to force rollout on ConfigMap changes; set resources with requests cpu:100m, memory:128Mi and limits cpu:500m, memory:512Mi; add readinessProbe on /livez with initialDelaySeconds:10, periodSeconds:30 and livenessProbe on /readyz with initialDelaySeconds:30, periodSeconds:60; add lifecycle.preStop with sleep 10s; create PodDisruptionBudget with minAvailable: 1. Verify under load using kubectl rollout status while generating traffic with tools like hey or wrk, and rollback using kubectl rollout undo deployment/processor if issues arise.","explanation":"## Why This Is Asked\nTests practical CKAD knowledge: patching Deployments, ConfigMap-driven reloads, probes, lifecycle hooks, PDBs, and rollback under load.\n\n## Key Concepts\n- ConfigMap-driven configs with rollout triggers\n- Probes and lifecycle for reliability\n- Resource limits for predictability\n- PDB to protect during maintenance\n\n## Code Example\n```none\n```\n\n## Follow-up Questions\n- How would you automate the checksum recalculation in a CI pipeline?\n- How would you observe rollout status under burst load?","diagram":"flowchart TD\n  N[Namespace: data-ingest] --> D[Deployment: processor]\n  D --> P[Pod(s)]\n  P --> R[Readiness/Liveness checks: /livez, /readyz]\n  D --> C[ConfigMap changes trigger rollout via checksum]\n  D --> B[PDB: minAvailable: 1]\n  P --> L[Load test] --> R\n","difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T05:08:11.342Z","createdAt":"2026-01-22T02:45:36.599Z"},{"id":"q-5593","question":"Design a per-namespace backup CronJob that snapshots all PVCs using VolumeSnapshot, with Schedule from Namespace ConfigMap BACKUP_SCHEDULE, SnapshotClass from config, API_TOKEN from Secret to POST metadata to a REST endpoint, and LAST_BACKUP_STATUS written to a Namespace ConfigMap. Logs stored on a PVC. InitContainer installs kubectl and snapshotter CLI; include readiness/liveness probes and a minimal YAML skeleton; discuss trade-offs?","answer":"A per-namespace CronJob that snapshots all PVCs via VolumeSnapshot, driven by BACKUP_SCHEDULE in a Namespace ConfigMap. SnapshotClass from config, API_TOKEN Secret for POST to a backup REST endpoint, ","explanation":"## Why This Is Asked\nThis tests namespace-scoped backup design, VolumeSnapshot usage, and cross-service auth.\n\n## Key Concepts\n- VolumeSnapshot and SnapshotClass\n- Namespace ConfigMap and Secret usage\n- InitContainer tooling setup (kubectl)\n- Probes and TTL on CronJob\n- RBAC boundaries and blast radius\n\n## Code Example\n```yaml\n# minimal CronJob skeleton\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: ns-backup\nspec:\n  schedule: '*/15 * * * *'\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: backup\n            image: alpine:3.18\n            command: [/bin/sh, -c, echo backup]\n          restartPolicy: OnFailure\n```\n\n## Follow-up Questions\n- How would you ensure idempotent backups and avoid duplicate VolumeSnapshots?\n- How would you monitor and alert on failed backups?","diagram":"flowchart TD\n  N[Namespace] --> VS[VolumeSnapshot CRD]\n  VS --> REST[Backup REST Endpoint]\n  REST --> Logs[Logs PVC]\n  NConfig[Namespace ConfigMap: BACKUP_SCHEDULE, SNAPSHOT_CLASS] --> VS\n  NSecret[Secret: API_TOKEN] --> REST","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T07:45:03.904Z","createdAt":"2026-01-22T07:45:03.904Z"},{"id":"q-5638","question":"In namespace secure-app, a Deployment 'portal' runs nginx:1.25 as 2 replicas and must comply with a non-root security policy. Provide a minimal manifest patch to: run the container as a non-root user, drop all capabilities, enable readOnlyRootFilesystem, add a readinessProbe on /healthz and a livenessProbe on /healthz, and ensure zero-downtime upgrades with a rollingUpdate strategy. Include how you'd verify under load and rollback?","answer":"Patch should set container securityContext to run as non-root (runAsNonRoot: true, runAsUser: 10001), enable readOnlyRootFilesystem, drop ALL capabilities, add readiness/liveness probes on /healthz, a","explanation":"## Why This Is Asked\nSecurity hardening of container runtimes and correctness of patching deployments are critical in real-world CKAD tasks.\n\n## Key Concepts\n- SecurityContext: runAsNonRoot, runAsUser, readOnlyRootFilesystem, capabilities drop\n- Probes: readiness and liveness on /healthz\n- Deployment strategy: RollingUpdate for zero-downtime upgrades\n- Validation: load testing, rollout status, and rollback workflow\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: portal\n  namespace: secure-app\nspec:\n  replicas: 2\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  selector:\n    matchLabels:\n      app: portal\n  template:\n    metadata:\n      labels:\n        app: portal\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.25\n        ports:\n        - containerPort: 80\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 10001\n          readOnlyRootFilesystem: true\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 80\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 2\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 80\n          initialDelaySeconds: 15\n          periodSeconds: 20\n          timeoutSeconds: 2\n``` \n\n## Follow-up Questions\n- How would you adjust if the app requires a limited capability set?\n- How would you implement canary rollouts and monitor for regression?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T09:42:38.351Z","createdAt":"2026-01-22T09:42:38.351Z"},{"id":"q-5757","question":"In CKAD terms, design a per-namespace Deployment that runs a small data-collector UI/service. It reads ENDPOINT and MODE from a ConfigMap named app-config, uses a Secret api-credentials for basic auth, and mounts a PVC at /data. An InitContainer installs dependencies; a Sidecar rotates TLS certs from a Secret named tls-certs; include readiness/liveness probes, and a Namespace NetworkPolicy restricting egress to the internal endpoint at data-portal:8443. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Design a per-namespace Deployment that reads ENDPOINT and MODE via app-config ConfigMap, authenticates with api-credentials Secret, and stores work in a PVC at /data. InitContainer installs dependenci","explanation":"## Why This Is Asked\nTests ability to compose namespace-scoped resources and practical config/secret usage.\n\n## Key Concepts\n- ConfigMap and Secret usage in a Deployment\n- InitContainer for dependency install\n- Sidecar for TLS cert rotation\n- Readiness/Liveness probes\n- Namespace NetworkPolicy restricting egress\n- PVC mounting and per-namespace isolation\n\n## Code Example\n```yaml\n# YAML skeleton\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\n  namespace: example-ns\ndata:\n  ENDPOINT: \"http://data-portal:8443/api/v1/ingest\"\n  MODE: \"batch\"\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: api-credentials\n  namespace: example-ns\ntype: Opaque\nstringData:\n  username: collector\n  password: s3cr3t\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: data-collector\n  namespace: example-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: data-collector\n  template:\n    metadata:\n      labels:\n        app: data-collector\n    spec:\n      initContainers:\n        - name: init-deps\n          image: python:3.11\n          command: [\"bash\",\"-lc\",\"pip install -r requirements.txt\"]\n          volumeMounts:\n            - name: data\n              mountPath: /data\n      containers:\n        - name: collector\n          image: my-reg/collector:latest\n          ports:\n            - containerPort: 8080\n          envFrom:\n            - configMapRef:\n                name: app-config\n            - secretRef:\n                name: api-credentials\n          volumeMounts:\n            - name: data\n              mountPath: /data\n            - name: tls\n              mountPath: /tls\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 5\n            periodSeconds: 10\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 15\n            periodSeconds: 20\n      volumes:\n        - name: data\n          persistentVolumeClaim:\n            claimName: data-pvc\n        - name: tls\n          secret:\n            secretName: tls-certs\n```\n\n## Follow-up Questions\n- How would you test TLS rotation in a cluster?\n- What changes to support multiple namespaces with shared TLS secrets?","diagram":"flowchart TD\n  A[Per-namespace Deployment] --> B[InitContainer: install deps]\n  A --> C[Main app]\n  C --> D[Sidecar: TLS rotation]\n  A --> E[NetworkPolicy: restricts egress to data-portal:8443]","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","IBM","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T15:39:49.113Z","createdAt":"2026-01-22T15:39:49.115Z"},{"id":"q-5898","question":"In CKAD terms, design a per-namespace Deployment that runs a tiny Python API exposing /health and /metrics. It reads config from a ConfigMap named app-config (ENABLE_METRICS) and a Secret named api-credentials. Include an InitContainer to install dependencies, a Sidecar for TLS cert rotation, readiness/liveness probes, and a NamespaceNetworkPolicy restricting egress to the upstream metrics service at metrics.internal.svc:9000. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Configure a per-namespace Deployment with a Python API server exposing /health and /metrics endpoints. Load the ENABLE_METRICS flag from the app-config ConfigMap and API credentials from the api-credentials Secret. Use an InitContainer to install Python dependencies, a sidecar container for TLS certificate rotation, and implement both readiness and liveness probes. Apply a NamespaceNetworkPolicy to restrict egress traffic to only the upstream metrics service at metrics.internal.svc:9000.","explanation":"## Why This Is Asked\nTests the ability to compose CKAD primitives into a small, observable application with proper security and observability patterns.\n\n## Key Concepts\n- ConfigMaps and Secrets for configuration management\n- InitContainers and Sidecars for application lifecycle management\n- Probes (readiness and liveness) for health monitoring\n- Namespace NetworkPolicy for traffic security\n- RBAC basics for in-namespace metrics access\n- Observability and security trade-offs in containerized applications\n\n## Code Example\n```yaml\n# Minimal Deployment sketch\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: python-api\n  namespace: target-namespace\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: python-api\n  template:\n    metadata:\n      labels:\n        app: python-api\n    spec:\n      initContainers:\n      - name: install-deps\n        image: python:3.11-alpine\n        command: ['sh', '-c', 'pip install -r /app/requirements.txt']\n        volumeMounts:\n        - name: app-code\n          mountPath: /app\n      containers:\n      - name: api\n        image: python:3.11-alpine\n        command: ['python', '/app/main.py']\n        env:\n        - name: ENABLE_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: app-config\n              key: ENABLE_METRICS\n        - name: API_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: api-credentials\n              key: username\n        - name: API_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: api-credentials\n              key: password\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        volumeMounts:\n        - name: app-code\n          mountPath: /app\n      - name: cert-rotator\n        image: cert-manager/cert-manager-rotator:latest\n        volumeMounts:\n        - name: certs\n          mountPath: /etc/certs\n      volumes:\n      - name: app-code\n        configMap:\n          name: app-code\n      - name: certs\n        emptyDir: {}\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: restrict-egress\n  namespace: target-namespace\nspec:\n  podSelector: {}\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: monitoring\n    ports:\n    - protocol: TCP\n      port: 9000\n```\n\n## Trade-offs\n- **Security vs. Complexity**: NetworkPolicy adds security but increases deployment complexity\n- **Resource Overhead**: Sidecar containers double resource requirements for cert management\n- **Observability Cost**: Metrics endpoint requires additional monitoring infrastructure\n- **InitContainer Impact**: Dependency installation extends pod startup time\n- **Namespace Isolation**: Per-namespace design improves security boundaries but requires duplicate configuration management","diagram":"flowchart TD\n  N[Namespace] --> D[Deployment nm-api]\n  D --> I[InitContainer: install-deps]\n  D --> S[Sidecar: tls-rotation]\n  D --> P[Probes: readiness & liveness]\n  D --> Np[NetworkPolicy: egress to metrics.internal.svc:9000]\n  D --> R[RBAC: serviceAccount read pods status]\n","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:58:08.824Z","createdAt":"2026-01-22T21:11:59.248Z"},{"id":"q-6137","question":"In CKAD terms, design a per-namespace CronJob nightly validates CSV data stored in a PVC mounted at /data, using a ConfigMap app-config to supply SCHEMA_PATH and DATE_RANGE, a Secret qa-credentials to post results to the internal endpoint http://qa-portal:8090/api/v1/qa, an InitContainer to install pandas and jsonschema, a Sidecar for TLS cert rotation, readiness/liveness probes, and a Namespace NetworkPolicy restricting egress to the QA endpoint. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Use a per-namespace CronJob that nightly validates CSVs from a PVC (/data). Mount app-config ConfigMap with SCHEMA_PATH and DATE_RANGE, qa-credentials Secret for http://qa-portal:8090/api/v1/qa, an InitContainer for pandas/jsonschema installation, a TLS cert rotation sidecar, health probes, and restrictive NetworkPolicy allowing only QA endpoint egress.","explanation":"## Why This Is Asked\n\nTests CKAD fundamentals in a realistic nightly QA workflow: CronJobs, per-namespace isolation, PVC usage, ConfigMap/Secret mounts, InitContainer dependency management, TLS rotation sidecar, health probes, and restrictive NetworkPolicy. Also evaluates data validation patterns and architectural trade-offs.\n\n## Key Concepts\n\n- CronJob scheduling and namespace isolation\n- PVC data persistence and volume mounting\n- ConfigMap/Secret mounting and environment injection\n- InitContainer for runtime dependency installation\n- Sidecar pattern for TLS certificate rotation\n- Readiness/Liveness probe implementation\n- NetworkPolicy egress restrictions\n- Python data validation with pandas/jsonschema\n\n## Code Example\n\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: data-qa-cronjob\n  namespace: example\nspec:\n  schedule: \"0 2 * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          initContainers:\n          - name: install-deps\n            image: python:3.9-alpine\n            command:\n            - sh\n            - -c\n            - |\n              pip install pandas==1.3.3 jsonschema==3.2.0 --target /app/deps\n            volumeMounts:\n            - name: deps-volume\n              mountPath: /app/deps\n          containers:\n          - name: validator\n            image: python:3.9-alpine\n            command:\n            - python\n            - -c\n            - |\n              import sys\n              sys.path.append('/app/deps')\n              import os\n              import pandas as pd\n              import jsonschema\n              import requests\n              from datetime import datetime\n              \n              schema_path = os.getenv('SCHEMA_PATH', '/data/schema.json')\n              date_range = os.getenv('DATE_RANGE', '7')\n              \n              # Load schema\n              with open(schema_path) as f:\n                  schema = json.load(f)\n              \n              # Validate CSVs\n              for csv_file in os.listdir('/data'):\n                  if csv_file.endswith('.csv'):\n                      df = pd.read_csv(f'/data/{csv_file}')\n                      try:\n                          jsonschema.validate(df.to_dict('records'), schema)\n                          result = {'file': csv_file, 'status': 'pass', 'timestamp': datetime.utcnow().isoformat()}\n                      except Exception as e:\n                          result = {'file': csv_file, 'status': 'fail', 'error': str(e), 'timestamp': datetime.utcnow().isoformat()}\n                      \n                      # Send to QA portal\n                      requests.post('http://qa-portal:8090/api/v1/qa', \n                                   json=result,\n                                   headers={'Authorization': f\"Bearer {os.getenv('QA_TOKEN')}\"})\n            envFrom:\n            - configMapRef:\n                name: app-config\n            - secretRef:\n                name: qa-credentials\n            volumeMounts:\n            - name: data-volume\n              mountPath: /data\n            - name: deps-volume\n              mountPath: /app/deps\n              readOnly: true\n            readinessProbe:\n              exec:\n                command:\n                - python\n                - -c\n                - \"import pandas, jsonschema, requests\"\n              initialDelaySeconds: 10\n              periodSeconds: 5\n            livenessProbe:\n              exec:\n                command:\n                - python\n                - -c\n                - \"import sys; sys.exit(0)\"\n              initialDelaySeconds: 30\n              periodSeconds: 30\n          - name: cert-rotator\n            image: cert-manager/cert-manager-rotator:v1.8.0\n            args:\n            - --watch-cert-dir=/certs\n            - --cert-secret-name=qa-tls-cert\n            volumeMounts:\n            - name: cert-volume\n              mountPath: /certs\n          volumes:\n          - name: data-volume\n            persistentVolumeClaim:\n              claimName: data-pvc\n          - name: deps-volume\n            emptyDir: {}\n          - name: cert-volume\n            emptyDir: {}\n          restartPolicy: OnFailure\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: app-config\n  namespace: example\ndata:\n  SCHEMA_PATH: \"/data/schema.json\"\n  DATE_RANGE: \"7\"\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: qa-credentials\n  namespace: example\ntype: Opaque\ndata:\n  QA_TOKEN: <base64-encoded-token>\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: qa-egress-policy\n  namespace: example\nspec:\n  podSelector:\n    matchLabels:\n      job-name: data-qa-cronjob\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - namespaceSelector:\n        matchLabels:\n          name: qa-namespace\n    ports:\n    - protocol: TCP\n      port: 8090\n```\n\n## Trade-offs\n\n**CronJob vs Deployment**: CronJob provides scheduled execution but lacks continuous monitoring. A Deployment could run continuously but would require external coordination.\n\n**InitContainer Pattern**: Ensures dependencies are available but increases startup time and complexity. Using a pre-built image would be faster but reduces flexibility.\n\n**Sidecar for TLS**: Separates concerns and enables independent updates but adds resource overhead and coordination complexity.\n\n**NetworkPolicy Restriction**: Enhances security but requires careful rule management and monitoring to avoid blocking legitimate traffic.\n\n**PVC for Data**: Ensures persistence across runs but creates coupling between namespaces and requires careful capacity planning.","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Netflix","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":["cronjob scheduling","per-namespace isolation","pvc data persistence","configmap environment injection","secret mounting","initcontainer dependency management","tls certificate rotation","readiness liveness probes","networkpolicy egress restrictions","pandas jsonschema validation","sidecar pattern","qa endpoint authentication"],"voiceSuitable":true,"isNew":true,"lastUpdated":"2026-01-28T05:40:18.618Z","createdAt":"2026-01-23T10:35:35.666Z"},{"id":"q-6393","question":"In namespace telemetry, a StatefulSet named 'collector' with 2 replicas uses a hostPath volume at /var/lib/collector and mounts /etc/collector from a ConfigMap. Provide a minimal manifest patch to: (1) switch the storage to a PVC backed by StorageClass 'fast-ssd' mounted at /var/lib/collector, (2) mount the ConfigMap at /etc/collector with checksum/config annotation to trigger rolling restarts when the ConfigMap changes, (3) set resource requests/limits, (4) add a readinessProbe on /healthz and a livenessProbe on /livez, (5) run as non-root user 1000:1000, (6) create a PodDisruptionBudget to ensure at least 1 pod is available during disruptions. Include how you'd verify under load and rollback?","answer":"Apply a strategic merge patch to the StatefulSet to: replace the hostPath volume with a PVC using StorageClass 'fast-ssd' for /var/lib/collector, mount the ConfigMap at /etc/collector with checksum/config annotation for change detection, add resource requests/limits, configure readinessProbe on /healthz and livenessProbe on /livez endpoints, set securityContext to run as non-root user 1000:1000 with fsGroup 1000, and create a PodDisruptionBudget ensuring minimum availability of 1 pod during disruptions.","explanation":"## Why This Is Asked\nThis question evaluates practical Kubernetes operational skills including StatefulSet storage migration, configuration change propagation, health monitoring, security hardening, and availability guarantees during disruptions.\n\n## Key Concepts\n- PersistentVolumeClaim integration with StatefulSets and StorageClass selection\n- ConfigMap checksum annotations for automated rolling updates\n- Health probe configuration (readiness vs liveness)\n- Pod security context with non-root execution and file system permissions\n- PodDisruptionBudget for maintaining service availability\n- Load testing strategies and rollback procedures\n\n## Code Example\n```yaml\n# Strategic merge patch\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: collector\nspec:\n  template:\n    spec:\n      securityContext:\n        runAsUser: 1000\n        runAsGroup: 1000\n        fsGroup: 1000\n      containers:\n      - name: collector\n        resources:\n          requests:\n            memory: \"256Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /livez\n            port: 8080\n          initialDelaySeconds: 30\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/collector\n        - name: config\n          mountPath: /etc/collector\n      volumes:\n      - name: config\n        configMap:\n          name: collector-config\n          items:\n          - key: config\n            path: config\n        annotations:\n          checksum/config: \"{{ .Values.configChecksum }}\"\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      storageClassName: \"fast-ssd\"\n      resources:\n        requests:\n          storage: 10Gi\n---\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: collector-pdb\nspec:\n  minAvailable: 1\n  selector:\n    matchLabels:\n      app: collector\n```\n\n## Verification Strategy\nMonitor rollout progress: `kubectl rollout status statefulset/collector -n telemetry`\nValidate pod health: `kubectl get pods -n telemetry -l app=collector -w`\nCheck PVC binding: `kubectl get pvc -n telemetry`\nLoad test using k6 or hey targeting collector endpoints\nVerify ConfigMap changes trigger restarts by updating ConfigMap and watching pod recreation\n\n## Rollback Approach\nUse `kubectl rollout undo statefulset/collector -n telemetry` for immediate rollback\nFor complex issues, restore from previous StatefulSet manifest and scale down/up\nAlways validate PDB compliance during rollback to avoid availability violations","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","IBM","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T04:29:50.646Z","createdAt":"2026-01-23T21:58:50.314Z"},{"id":"q-6437","question":"In namespace analytics, a Deployment 'cache-writer' with 3 replicas uses a PVC for write-ahead logs and relies on a ConfigMap mounted at /config to initialize a cache. Provide a minimal manifest patch to add an InitContainer that copies /config/cache-base to /var/cache on first pod start, mount the ConfigMap at /config, and set a rollingUpdate strategy (maxUnavailable: 0), plus a PodDisruptionBudget requiring at least 2 healthy pods. Include how you'd validate under load and rollback?","answer":"Patch summary: add an InitContainer 'cache-init' using busybox to copy /config/cache-base to /var/cache, mount the ConfigMap at /config, and share /var/cache with the main container; set the Deployment strategy to RollingUpdate with maxUnavailable: 0, and create a PodDisruptionBudget requiring minAvailable: 2. For validation, test under load using kubectl apply with --dry-run=client, monitor rollout status with kubectl rollout status, and perform load testing while verifying cache initialization. Rollback using kubectl rollout undo if issues arise.","explanation":"## Why This Is Asked\nTests ability to add InitContainers and upgrade safeguards in Kubernetes CKAD tasks.\n\n## Key Concepts\n- InitContainers, ConfigMaps, Volumes\n- RollingUpdate strategy, maxUnavailable\n- PodDisruptionBudget, availability guarantees\n\n## Code Example\n\n```yaml\n# Patch sketch (high-level)\ninitContainers:\n  - name: cache-init\n    image: busybox\n    command: [\"sh\",\"-c\",\"cp -r /config/cache-base/* /var/cache/\"]\n    volumeMounts:\n      - name: cache\n        mountPath: /var/cache\nvolumes:\n  - name: config\n    configMap:\n      name: cache-config\n  - name: cache\n    emptyDir: {}\n```","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Snap","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T04:59:39.288Z","createdAt":"2026-01-23T23:53:53.542Z"},{"id":"q-6445","question":"In namespace secure, Deployment 'api-gateway' with 3 replicas mounts /srv from a PVC and reads config from a ConfigMap mounted at /etc/app. It currently runs as root. Provide a minimal manifest patch to harden security: set container securityContext (runAsNonRoot: true, runAsUser: 10001, readOnlyRootFilesystem: true, capabilities: drop: [\"ALL\"]), set PodSecurityContext (fsGroup: 1001), and add a PodDisruptionBudget requiring at least 2 healthy pods during maintenance. Include how you'd verify under load and rollback?","answer":"Apply a strategic patch to the Deployment that implements comprehensive security hardening: configure container.securityContext with runAsNonRoot: true, runAsUser: 10001, readOnlyRootFilesystem: true, and capabilities: drop: [\"ALL\"]; set pod securityContext with fsGroup: 1001 for proper PVC permissions; and create a PodDisruptionBudget ensuring minimum 2 healthy pods during maintenance operations. Validate under load using kubectl exec for connectivity testing and monitor key metrics, then rollback with kubectl rollout undo deployment/api-gateway if any issues arise.","explanation":"## Why This Is Asked\nTests practical security hardening and disruption management capabilities for CKAD-level Kubernetes deployments.\n\n## Key Concepts\n- Container SecurityContext: runAsNonRoot, runAsUser, readOnlyRootFilesystem for container isolation\n- PodSecurityContext: fsGroup for proper file permissions on PVC-mounted volumes\n- Kernel capabilities: drop ALL to minimize attack surface\n- PodDisruptionBudget: maintain minimum healthy pods during maintenance\n- Rollout and rollback procedures for safe deployments\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api-gateway\n  labels:\n    app: api-gateway\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: api-gateway\n    spec:\n      securityContext:\n        fsGroup: 1001\n      containers:\n      - name: api-gateway\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 10001\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - \"ALL\"\n        volumeMounts:\n        - name: data\n          mountPath: /srv\n        - name: config\n          mountPath: /etc/app\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: api-gateway-pvc\n      - name: config\n        configMap:\n          name: api-gateway-config\n---\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: api-gateway-pdb\nspec:\n  minAvailable: 2\n  selector:\n    matchLabels:\n      app: api-gateway\n```\n\n## Verification & Rollback\n```bash\n# Verify under load\nkubectl exec deployment/api-gateway -- curl http://localhost:8080/health\nkubectl top pods -l app=api-gateway\nkubectl get pods -l app=api-gateway --watch\n\n# Rollback if needed\nkubectl rollout undo deployment/api-gateway\nkubectl rollout status deployment/api-gateway\n```","diagram":"flowchart TD\n  A[Deployment: api-gateway] --> B[Service: api-svc]\n  B --> C[Pods: api-gateway-xyz]\n  C --> D[PVC mounted at /srv]\n  C --> E[ConfigMap mounted at /etc/app]","difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Meta","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T04:57:30.641Z","createdAt":"2026-01-24T02:15:11.087Z"},{"id":"q-6510","question":"In CKAD terms, design a per-namespace Deployment that watches a PVC-mounted /data for new JSON files, validates them against a JSON Schema provided in a ConfigMap named app-schema, and exposes a GET endpoint /count returning the number of valid files. Use an InitContainer to install jq and ajv-cli, a Secret for an API_KEY, a sidecar for TLS cert rotation, readiness/liveness probes, and a minimal YAML skeleton; discuss trade-offs?","answer":"Per-namespace Deployment monitors /data from a PVC, validates new JSON files against a JSON Schema in app-schema ConfigMap using ajv in InitContainer, and serves /count with the number of valid files.","explanation":"## Why This Is Asked\nAssesses practical CKAD pattern: file-watching via PVC, InitContainer deps, ConfigMap/Secret usage, and basic networking\n\n## Key Concepts\n- InitContainer dependency installation\n- ConfigMap usage\n- Secret usage\n- In-cluster networking with NetworkPolicy\n- Readiness/Liveness probes\n\n## Code Example\n```javascript\n# Minimal YAML skeleton (pseudo)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: json-processor\nspec:\n  replicas: 1\n  template:\n    spec:\n      volumes:\n        - name: data\n          persistentVolumeClaim:\n            claimName: data-pvc\n      initContainers:\n        - name: installer\n          image: alpine:3.17\n          command: [\"sh\", \"-c\", \"apk add --no-cache jq ajv-cli\"]\n      containers:\n        - name: processor\n          image: my-processor:latest\n          volumeMounts:\n            - name: data\n              mountPath: /data\n          ports:\n            - containerPort: 8080\n      # probes and sidecar omitted for brevity\n```\n\n## Follow-up Questions\n- How would you implement idempotency for repeated /count calls?\n- What changes for multi-replica consistency?\n","diagram":"flowchart TD\n  A[Namespace] --> B[Deployment]\n  B --> C[InitContainer]\n  B --> D[Main container]\n  D --> E[/count endpoint/]\n  D --> F[Optional POST to internal endpoint]","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Scale Ai","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T05:57:53.472Z","createdAt":"2026-01-24T05:57:53.473Z"},{"id":"q-6537","question":"In CKAD terms, design a per-namespace manifest-validation service: a Deployment that watches a PVC-mounted /manifests directory for YAML files, validates them using kubeval and yamllint, batches results according to BATCH_SIZE from a ConfigMap named validation-config, and POSTs a summary to http://validation-portal:8080/api/v1/validate. Include an InitContainer to install kubeval/yamllint, a Secret for API credentials, a Sidecar for TLS cert rotation, readiness/liveness probes, and a Namespace NetworkPolicy restricting egress to the internal endpoint. Provide a minimal YAML skeleton. Discuss trade-offs?","answer":"Implement a per-namespace Deployment that reads /manifests on a PVC, validates YAMLs in batches using kubeval and yamllint, batches controlled by BATCH_SIZE in validation-config, posts a summary to ht","explanation":"## Why This Is Asked\nTests practical CKAD skills: volume-mounted inputs, ConfigMap-driven behavior, InitContainer setup, and egress restrictions. It also touches idempotence and basic observability in a beginner-friendly way.\n\n## Key Concepts\n- PVC mounts; InitContainers; ConfigMaps; Secrets; sidecars; readiness/liveness; NetworkPolicy; HTTP post of results.\n\n## Code Example\n```yaml\n# minimal Deployment skeleton (placeholder)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: manifest-validator\n  namespace: <ns>\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n      - name: validator\n        image: validator:latest\n        volumeMounts:\n        - mountPath: /manifests\n          name: manifests\n      initContainers:\n      - name: install-tools\n        image: alpine:latest\n        command: [\"sh\", \"-c\", \"apk add --no-cache kubeval yamllint && sleep 1\"]\n      volumes:\n      - name: manifests\n        persistentVolumeClaim:\n          claimName: manifests-pvc\n```\n\n## Follow-up Questions\n- How would you implement idempotence and retry semantics for failed validations?\n- How would you scale this per namespace and handle large batches?","diagram":"flowchart TD\n  A[PVC mounted /manifests] --> B[InitContainer installs kubeval/yamllint]\n  B --> C[Main app validates in batches]\n  C --> D[POST summary to validation-portal]\n  D --> E[Store report on PVC]","difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T07:24:57.683Z","createdAt":"2026-01-24T07:24:57.683Z"},{"id":"q-6552","question":"Design a per-namespace canary controller in CKAD terms: a Deployment runs a reconciliation loop every 30s, reads a ConfigMap named canary-config in the same Namespace containing a JSON array [{name, replicas, canary}], and patches each target Deployment's replicas to the canary fraction. Persist last weights in canary-status ConfigMap. InitContainer installs jq, logs to /logs on a PVC, with readiness/liveness probes. Namespace NetworkPolicy allows egress only to kube-api:443. Include a minimal YAML skeleton and discuss trade-offs?","answer":"Build a per-namespace canary controller: a Deployment runs a loop every 30s, reads canary-config with JSON [{name, replicas, canary}], patches target Deployments' replicas to canary*replicas while pre","explanation":"## Why This Is Asked\nTests practical CKAD skills: in-namespace controller logic, ConfigMap/state handling, patching Deployments, RBAC, and logging.\n\n## Key Concepts\n- Namespace-scoped RBAC (Role/RoleBinding)\n- ConfigMap as config/state store\n- Patch strategy and idempotent reconciliation\n- InitContainer provisioning of tooling\n- NetworkPolicy restricting egress to the API server\n- Logging to PVC and per-cycle observability\n\n## Code Example\n```javascript\n// Pseudo-reconciliation sketch\n```\n\n## Follow-up Questions\n- How would you test this in a sandbox cluster?\n- How would you handle concurrent canary updates across multiple items?","diagram":"flowchart TD\n  A[Namespace Canary Controller] --> B[Read canary-config ConfigMap]\n  B --> C[Parse JSON items]\n  C --> D[Patch target Deployments replicas]\n  D --> E[Update canary-status ConfigMap]\n  E --> F[Write logs to /logs on PVC]","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","NVIDIA","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T07:54:53.025Z","createdAt":"2026-01-24T07:54:53.025Z"},{"id":"q-6604","question":"In namespace edge, a Deployment 'edge-processor' with 3 replicas must be security-hardened. Provide a minimal manifest patch to enforce pod/container securityContext (runAsNonRoot, readOnlyRootFilesystem, drop ALL capabilities, noPrivilegeEscalation), add a Namespace LimitRange and a ResourceQuota to bound resources, and describe verification steps under load and rollback?","answer":"Set container securityContext: runAsNonRoot: true, readOnlyRootFilesystem: true, allowPrivilegeEscalation: false, capabilities: { drop: ['ALL'] }. Also set pod securityContext accordingly. Add Namespa","explanation":"## Why This Is Asked\nTests practical hardening and namespace governance, not just patching.\n\n## Key Concepts\n- Pod securityContext, container securityContext\n- LimitRange and ResourceQuota\n- Verification and rollback\n\n## Code Example\n```yaml\n# example patch skeleton\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: edge-processor\nspec:\n  template:\n    spec:\n      securityContext:\n        runAsNonRoot: true\n      containers:\n      - name: edge\n        image: your-image\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: true\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop: [\"ALL\"]\n```\n\n## Follow-up Questions\n- How would you enforce these policies at admission without modifying each manifest?\n- What are pitfalls when using readOnlyRootFilesystem with writable paths?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","MongoDB","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T09:55:54.320Z","createdAt":"2026-01-24T09:55:54.320Z"},{"id":"q-6654","question":"CKAD advanced: design a per-namespace image-signature guard using a ValidatingWebhookConfiguration. In namespaces labeled guard=true, require all container images to be Cosign-signed with a public key stored in a ConfigMap. Provide a minimal YAML skeleton: Guard Deployment (InitContainer installs cosign), a ConfigMap with the key, a Secret for registry creds, a Service and ValidatingWebhookConfiguration, and a Namespace NetworkPolicy to restrict webhook traffic. Include trade-offs?","answer":"To implement, deploy a Guard service in the guarded namespace that runs as a Deployment with an InitContainer to install cosign, a ValidatingWebhookConfiguration that intercepts PodCreations, and a Co","explanation":"## Why This Is Asked\n\nTests understanding of admission control, per-namespace scoping, and Cosign-based image signing in CKAD-level design. It combines Deployments, InitContainers, ConfigMaps, Secrets, ValidatingWebhookConfiguration, and NetworkPolicy.\n\n## Key Concepts\n- ValidatingWebhookConfiguration scope by namespace label\n- Cosign-based image signing and public key storage in ConfigMap\n- InitContainer usage for tooling\n- Secrets for registry credentials\n- NetworkPolicy for webhook traffic control\n\n## Code Example\n```yaml\n# Minimal Guard Deployment (cosign setup) - placeholder\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: guard-service\n  namespace: guarded\nspec:\n  template:\n    spec:\n      initContainers:\n      - name: install-cosign\n        image: alpine:latest\n        command: [\"sh\", \"-c\", \"apk add --no-cache cosign && cosign version\"]\n      containers:\n      - name: guard\n        image: guard-service:latest\n        ports:\n        - containerPort: 443\n```\n\n```yaml\n# Minimal WebhookConfiguration skeleton\napiVersion: admissionregistration.k8s.io/v1\nkind: ValidatingWebhookConfiguration\nmetadata:\n  name: image-signature-validator\nwebhooks:\n- name: images.example.com\n  clientConfig:\n    service:\n      name: guard-service\n      namespace: guarded\n      port: 443\n    caBundle: <base64-ca>\n  rules:\n  - apiGroups: [\"\"]\n    apiVersions: [\"v1\"]\n    resources: [\"pods\"]\n    operations: [\"CREATE\"]\n  namespaceSelector:\n    matchLabels:\n      guard: \"true\"\n```\n\n## Follow-up Questions\n- How would you test for false positives/negatives and monitor webhook latency?\n- What risks are introduced by admission webhooks and how would you mitigate them?","diagram":"flowchart TD\n  A[Namespace: guard=true] --> B[Guard Service]\n  B --> C[ValidatingWebhookConfiguration]\n  C --> D[Pod Creation Request]\n  D --> E{Image is Cosign-signed?}\n  E -->|Yes| F[Allow]\n  E -->|No| G[Deny]\n","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Meta","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T11:46:30.559Z","createdAt":"2026-01-24T11:46:30.559Z"},{"id":"q-6763","question":"Design a CKAD per-namespace log-forwarder that tails a PVC-mounted /logs directory, batches lines using BATCH_SIZE from a ConfigMap named log-config, and POSTs to https://log-portal:9200/api/v1/logs. Include an InitContainer to install curl/jq, a Secret for API credentials, a TLS cert-rotation sidecar, readiness/liveness probes, and a Namespace NetworkPolicy restricting egress to the internal endpoint. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"I would implement a Deployment per namespace with an InitContainer to install curl/jq, a main log-forwarder that tails /logs from a PVC, batches lines using BATCH_SIZE from ConfigMap log-config, and P","explanation":"## Why This Is Asked\n\nTests practical CKAD skills: PVC-backed logs, ConfigMap-driven behavior, Secrets, InitContainers, sidecars for TLS rotation, health checks, and namespace-scoped NetworkPolicy, all in a minimal Deployment.\n\n## Key Concepts\n\n- PVC-backed log ingestion\n- ConfigMap-driven batching (BATCH_SIZE)\n- InitContainer for bootstrap deps\n- Secret for credentials\n- TLS rotation sidecar pattern\n- Readiness and liveness probes\n- Namespace NetworkPolicy for restricted egress\n\n## Code Example\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: log-forwarder\n  namespace: <namespace>\nspec:\n  replicas: 1\n  template:\n    spec:\n      volumes:\n        - name: logs\n          persistentVolumeClaim:\n            claimName: log-pvc\n        - name: tls\n          secret:\n            secretName: log-tls\n      initContainers:\n        - name: init-deps\n          image: alpine:3.18\n          command: [\"sh\",\"-c\",\"apk add --no-cache curl jq\"]\n          volumeMounts:\n            - name: logs\n              mountPath: /logs\n            - name: tls\n              mountPath: /tls\n      containers:\n        - name: forwarder\n          image: your-registry/log-forwarder:latest\n          volumeMounts:\n            - name: logs\n              mountPath: /logs\n            - name: tls\n              mountPath: /tls\n          env:\n            - name: DEST_URL\n              valueFrom:\n                configMapKeyRef:\n                  name: log-config\n                  key: DEST_URL\n            - name: BATCH_SIZE\n              valueFrom:\n                configMapKeyRef:\n                  name: log-config\n                  key: BATCH_SIZE\n            - name: API_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: log-creds\n                  key: token\n          readinessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 5\n            periodSeconds: 10\n          livenessProbe:\n            httpGet:\n              path: /health\n              port: 8080\n            initialDelaySeconds: 15\n            periodSeconds: 20\n      serviceAccountName: log-sa\n      restartPolicy: Always\n```\n\n## Follow-up Questions\n\n- How would you test TLS rotation in CI?\n- How would you scale this pattern across many namespaces while preserving isolation?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Cloudflare","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T16:48:01.581Z","createdAt":"2026-01-24T16:48:01.581Z"},{"id":"q-6832","question":"In namespace debug, a Deployment 'app' with 2 replicas needs on-demand debugging via EphemeralContainers. Provide a minimal manifest patch that adds an EphemeralContainer named 'debug-shell' (image busybox:1.36, command ['sh'], tty: true, stdin: true) to spec.template.spec.ephemeralContainers with resource limits; include a rollout-trace annotation on the Pod template to track the patch. Explain how to attach to the shell, how to rollback by removing the patch, and how you'd validate under load?","answer":"Add an EphemeralContainer to the pod template: spec.template.spec.ephemeralContainers: - name: debug-shell, image: busybox:1.36, command: ['sh'], tty: true, stdin: true, resources: limits: cpu: '50m',","explanation":"## Why This Is Asked\n- Tests live debugging of a running pod without redeploys. EphemeralContainers are ideal for incident response in CKAD.\n\n## Key Concepts\n- EphemeralContainers field in PodSpec; patching Deployments; patch lifecycles; attach methods; security posture.\n\n## Code Example\n```yaml\n# Minimal patch sketch (answer may vary by patch method)\n*** Patch: Update Deployment app\n*** End Patch\n```\n\n## Follow-up Questions\n- How would you ensure this doesn't leak during normal operation? \n- How would you automate cleanup after debugging?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T19:24:40.575Z","createdAt":"2026-01-24T19:24:40.575Z"},{"id":"q-6923","question":"In CKAD terms, design a per-namespace cache-warming service: a Deployment that on startup preloads an external API into a namespace-scoped in‑memory cache. TTL, ENDPOINT, and KEY_LIST live in a ConfigMap; API_KEY from a Secret authenticates requests. Use an InitContainer to install curl/jq, a sidecar to rotate credentials, and a Namespace NetworkPolicy restricting egress to the API host. Provide a minimal YAML skeleton and discuss trade‑offs?","answer":"Deploy a stateless cache-warm container that boots and preloads an external API into a per-namespace in‑memory cache. TTL, ENDPOINT, and KEY_LIST come from a ConfigMap; API_KEY authenticates requests ","explanation":"## Why This Is Asked\nTests CKAD depth in using ConfigMap/Secret, InitContainers, sidecars, and per-namespace scoping with network policies, plus practical cache warming patterns.\n\n## Key Concepts\n- InitContainers for dependency setup\n- ConfigMap/Secret for runtime config and credentials\n- Per-namespace isolation of cache\n- Sidecar for credential rotation\n- Readiness/Liveness probes for startup health\n- Namespace NetworkPolicy for restricted egress\n- Caching TTL and eviction trade-offs\n\n## Code Example\n```yaml\n# Minimal skeleton: Deployment with initContainers, envFrom from ConfigMap/Secret\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: namespace-cache-warm\n  namespace: example\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: namespace-cache-warm\n    spec:\n      initContainers:\n      - name: init-deps\n        image: busybox\n        command: [\"sh\", \"-c\", \"apk add --no-cache curl jq || true\"]\n      containers:\n      - name: warm\n        image: myorg/cache-warm:latest\n        envFrom:\n        - configMapRef:\n            name: cache-config\n        - secretRef:\n            name: cache-creds\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: cache-logs\n```\n\n## Follow-up Questions\n- How would you testTTL eviction across multiple replicas?\n- How would you rotate keys without pod restarts if the API_KEY changes?","diagram":"flowchart TD\n  Namespace[Namespace] --> Warm[Cache-Warmer Deployment]\n  Warm --> API[External API]\n  API --> TokenRot[Token Rotator Sidecar]\n  Warm --> Cache[In-Memory Cache]\n  Warm --> Logs[Logs PVC]","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T22:57:29.497Z","createdAt":"2026-01-24T22:57:29.497Z"},{"id":"q-7017","question":"In CKAD terms, design a per-namespace log-redaction service: a Deployment tails /logs from a PVC, redacts PII using patterns from a ConfigMap redaction-config, and writes sanitized logs back to the PVC. It POSTs a summary to http://logportal:8080/api/v1/sanitize. Include InitContainer to install sed/rg, a Secret for API credentials, a Sidecar for TLS rotation, readiness/liveness probes, and a Namespace NetworkPolicy restricting egress. Provide a minimal YAML skeleton. Discuss trade-offs?","answer":"Design a per-namespace log-redaction service: a Deployment tails /logs from a PVC, redacts PII using patterns from a ConfigMap redaction-config, and writes sanitized logs back to the PVC. It POSTs a s","explanation":"## Why This Is Asked\nCKAD fundamentals: volumes, InitContainers, ConfigMaps, Secrets, sidecars, network policies, and health checks in a per-namespace scope.\n\n## Key Concepts\n- Tail logs from a PVC-mounted path and write sanitized output back to the same volume\n- Use a ConfigMap for PII redaction patterns\n- Secure API credentials via a Secret and post summaries to an internal endpoint\n- InitContainer installs tooling; sidecar rotates TLS certs\n- NetworkPolicy confines egress to the internal log portal; readiness and liveness probes validate health\n\n## Code Example\n```yaml\n# Minimal skeleton for the Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: log-redact\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: log-redact\n  template:\n    metadata:\n      labels:\n        app: log-redact\n    spec:\n      initContainers:\n        - name: install-tools\n          image: alpine:3.18\n          command: [\"sh\", \"-c\", \"apk add --no-cache sed ripgrep\"]\n      containers:\n        - name: redactor\n          image: your-registry/log-redact:latest\n          volumeMounts:\n            - name: logs\n              mountPath: /logs\n            - name: patterns\n              mountPath: /patterns\n          env:\n            - name: POST_URL\n              value: \"http://logportal:8080/api/v1/sanitize\"\n      volumes:\n        - name: logs\n          persistentVolumeClaim:\n            claimName: logs-pvc\n        - name: patterns\n          configMap:\n            name: redaction-config\n```\n\n## Follow-up Questions\n- How would you test locally with minikube?\n- What failure modes require retries or idempotent postings?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Hugging Face","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:49:43.285Z","createdAt":"2026-01-25T05:49:43.286Z"},{"id":"q-7090","question":"In CKAD terms, design a per-namespace ephemeral-debug workflow: a CronJob that automatically attaches an ephemeral container to a running Pod labeled app=myservice in that namespace, executes diagnostics configured via a ConfigMap, and writes output to a PVC at /logs. Include InitContainer to install net-tools, a Secret for an upload endpoint, and provide a minimal YAML skeleton. Discuss RBAC/security trade-offs?","answer":"Outline: a CKAD CronJob uses a kubectl helper image to call kubectl debug on a Pod labeled app=myservice, injecting an ephemeral container that runs DEBUG_CMD (from a ConfigMap) and streams output to ","explanation":"## Why This Is Asked\n\nTests ability to design ephemeral debugging workflows, RBAC, ConfigMap/Secret usage, and per-namespace resource handling without impacting app pods.\n\n## Key Concepts\n\n- Ephemeral containers and kubectl debug flow\n- RBAC least privilege for patching pods\n- ConfigMap/Secret for commands and credentials\n- PVC-backed logs and InitContainer prerequisites\n\n## Code Example\n\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: ephemeral-debug\nspec:\n  schedule: \"*/5 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n            - name: debug\n              image: bitnami/kubectl:latest\n              command: [\"/bin/sh\",\"-c\",\"kubectl debug -it pod -l app=myservice --image=alpine -- /bin/sh -c '$DEBUG_CMD > /logs/diag.txt'\"]\n          restartPolicy: OnFailure\n```\n\n## Follow-up Questions\n\n- How would you ensure cleanup of ephemeral containers and logs after completion?\n- What if multiple pods match the selector?","diagram":"flowchart TD\n  CronJob --> PodSelector\n  PodSelector --> Ephemeral\n  Ephemeral --> Logs\n  Logs --> Upload\n  Upload --> Cleanup","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Instacart","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T08:52:47.066Z","createdAt":"2026-01-25T08:52:47.066Z"},{"id":"q-7245","question":"In namespace payments, update Deployment 'processor' (2 replicas) that reads daily CSV feeds from a mounted PVC and writes to a separate PVC. Add a sidecar 'log-shipper' that tails /var/log/processor/app.log and ships to http://log-collector:10514/ingest using a shared emptyDir volume mounted at /var/log/processor and /var/log/ship. Also add an InitContainer to fix permissions on the logs directory mounted from the PVC. Include resource requests/limits, readinessProbe for /healthz, livenessProbe for /readyz, and a clear rollback plan. Provide a minimal manifest patch that accomplishes this?","answer":"Patch plan: introduce an InitContainer to fix PVC log permissions, add an emptyDir volume logs shared by the processor and log-shipper, mount /var/log/processor and /var/log/ship, deploy a lightweight","explanation":"## Why This Is Asked\nTests real-world container orchestration patterns: InitContainers for bootstrap, sidecar for log shipping, shared volumes for cross-container data, and proper health checks plus rollback strategy.\n\n## Key Concepts\n- InitContainer for pre-boot config\n- Shared volumes and proper MountPaths\n- Sidecar pattern for log shipping\n- Readiness and Liveness probes tailored to production workloads\n- Rollback strategy and safe deploys\n\n## Code Example\n```yaml\n# Minimal patch sketch\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: processor\n  namespace: payments\nspec:\n  replicas: 2\n  template:\n    spec:\n      initContainers:\n      - name: fix-permissions\n        image: busybox\n        command: [\"sh\", \"-c\", \"chown -R 1000:1000 /mnt/logs\"]\n        volumeMounts:\n        - name: logs\n          mountPath: /mnt/logs\n      containers:\n      - name: processor\n        image: your-registry/processor:latest\n        volumeMounts:\n        - name: in-data\n          mountPath: /data/input\n        - name: logs\n          mountPath: /var/log/processor\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n          limits:\n            cpu: \"500m\"\n            memory: \"256Mi\"\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        livenessProbe:\n          httpGet:\n            path: /readyz\n            port: 8080\n      - name: log-shipper\n        image: busybox:stable\n        command: [\"sh\", \"-c\", \"tail -F /var/log/processor/app.log | nc log-collector 10514\"]\n        volumeMounts:\n        - name: logs\n          mountPath: /var/log/processor\n        - name: ship\n          mountPath: /var/log/ship\n      volumes:\n      - name: in-data\n        persistentVolumeClaim:\n          claimName: processor-input-pvc\n      - name: logs\n        emptyDir: {}\n      - name: ship\n        emptyDir: {}\n      restartPolicy: Always\n```\n\n## Follow-up Questions\n- How would you validate zero-downtime rollouts with this setup? \n- How would you monitor and retry failed log shipments without impacting processor throughput?","diagram":"flowchart TD\n  A[Processor Pod] --> B{Reads from PVC}\n  B --> C[Writes to PVC]\n  A --> D[Mounts /var/log/processor]\n  D --> E[Shared volume with log-shipper]\n  F[Log-shipper Sidecar] --> G[Ingest to log-collector:10514]\n  G --> H[Log collector]","difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T14:59:57.567Z","createdAt":"2026-01-25T14:59:57.568Z"},{"id":"q-7322","question":"In namespace prod, Deployment 'web' has 3 replicas. To debug a live pod under heavy load without rebuilding images, implement an Ephemeral Container approach. Provide a minimal manifest patch that adds an EphemeralContainer named 'trace' using image 'busybox:1.36', with command ['sh'], tty: true, and a volumeMount /debug mounted from an emptyDir volume 'trace'. Also add the corresponding volume. Outline how to attach to a running pod for debugging and how to rollback?","answer":"Patch the Pod template to include: spec.ephemeralContainers: - name: trace image: busybox:1.36 command: [\"sh\"] stdin: true tty: true volumeMounts: - name: trace mountPath: /debug volumes: - name: trac","explanation":"## Why This Is Asked\nCKAD often requires live debugging without rebuilding images. Ephemeral containers let you attach to a running pod for diagnosis while preserving production images.\n\n## Key Concepts\n- EphemeralContainers field in PodSpec\n- Patch/deployment rollouts and rollbacks\n- kubectl debug usage and verification\n\n## Code Example\n```yaml\n# Minimal patch snippet using a patch strategy\nspec:\n  template:\n    spec:\n      ephemeralContainers:\n      - name: trace\n        image: busybox:1.36\n        command: [\"sh\"]\n        stdin: true\n        tty: true\n        volumeMounts:\n        - name: trace\n          mountPath: /debug\n      volumes:\n      - name: trace\n        emptyDir: {}\n```\n\n## Follow-up Questions\n- How would you ensure security/compliance when using ephemeral containers? \n- How would you adapt this for a multi-namespace policy? ","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","NVIDIA","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T18:45:17.371Z","createdAt":"2026-01-25T18:45:17.372Z"},{"id":"q-7395","question":"In CKAD terms, design a per-namespace security-audit Deployment that scans all pods in the namespace for privileged or root user usage, writes a JSON report to a PVC at /audits/report.json, and POSTs a summary to http://audit-portal:8080/api/v1/audit; include InitContainer to install jq, an audit-config ConfigMap for patterns, a Secret for API creds, a TLS-rotation sidecar, readiness/liveness probes, and a Namespace NetworkPolicy restricting egress to the audit endpoint. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Design a per-namespace security-audit Deployment that scans all pods for privileged or root user usage, writes a JSON report to a PVC at /audits/report.json, and POSTs a summary to http://audit-portal:8080/api/v1/audit. The solution includes an InitContainer to install jq, an audit-config ConfigMap for patterns, a Secret for API credentials, a TLS-rotation sidecar, readiness/liveness probes, and a Namespace NetworkPolicy restricting egress to the audit endpoint.","explanation":"## Why This Is Asked\nThis question tests practical CKAD skills by requiring the integration of Pod security checks, PVC-backed persistence, and outbound reporting within namespace boundaries. It reinforces the use of InitContainers, ConfigMaps, Secrets, sidecars, probes, and namespace-scoped NetworkPolicies while evaluating trade-offs between in-cluster and external auditing approaches.\n\n## Key Concepts\n- Pod security checks: privileged containers, running as root, securityContext implications\n- PVC-backed reports and lightweight post-processing\n- InitContainer usage for tooling installation (jq)\n- ConfigMap-driven patterns and Secret-based credentials\n- TLS-rotation sidecar management\n- Health probe configuration\n- Namespace-scoped NetworkPolicy for egress restrictions","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Meta","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T06:52:27.187Z","createdAt":"2026-01-25T21:32:04.990Z"},{"id":"q-7481","question":"In CKAD terms, design a per-namespace baseline for resource governance: implement a Namespace containing a LimitRange that sets default CPU/memory requests and limits for pods, and a ResourceQuota that caps total CPU, memory, and pod count. Include a minimal Deployment manifest that uses these defaults. Provide minimal YAML skeleton and discuss trade-offs?","answer":"Create a Namespace with a LimitRange that sets default requests (cpu: 100m, memory: 128Mi) and default limits (cpu: 200m, memory: 256Mi) for pods, plus a ResourceQuota that caps total resources at pods: 10, requests.cpu: 1, requests.memory: 1Gi, and limits.cpu: 2. Include a minimal Deployment that inherits these defaults.","explanation":"## Why This Is Asked\nThis question tests knowledge of namespace-scoped governance primitives (LimitRange, ResourceQuota) and how defaults affect scheduling, QoS, and multi-tenant fairness in CKAD.\n\n## Key Concepts\n- Namespace scoping\n- LimitRange defaultRequests/defaultLimits\n- ResourceQuota hard limits\n- Pod scheduling under quotas\n- Impact on autoscaling and experimentation\n\n## Code Example\n```yaml\n# Namespace\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: test-ns\n---\n# LimitRange\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: default-limits\n  namespace: test-ns\nspec:\n  limits:\n  - default:\n      cpu: 200m\n      memory: 256Mi\n    defaultRequest:\n      cpu: 100m\n      memory: 128Mi\n    type: Container\n---\n# ResourceQuota\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: resource-quota\n  namespace: test-ns\nspec:\n  hard:\n    pods: \"10\"\n    requests.cpu: \"1\"\n    requests.memory: 1Gi\n    limits.cpu: \"2\"\n    limits.memory: 2Gi\n---\n# Deployment (inherits defaults)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-app\n  namespace: test-ns\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      labels:\n        app: sample-app\n    spec:\n      containers:\n      - name: app\n        image: nginx:latest\n```\n\n## Trade-offs\n**Advantages:**\n- Prevents resource starvation across namespaces\n- Enforces fair resource distribution\n- Automatic defaults reduce configuration errors\n- Provides cost control boundaries\n\n**Considerations:**\n- Too restrictive limits can hinder scaling\n- Default limits may be overprovisioned for small workloads\n- ResourceQuota refusals require manual quota increases\n- May impact autoscaling behavior if quotas are too tight\n\n**Best Practices:**\n- Set conservative defaults but allow overrides\n- Monitor quota utilization regularly\n- Use separate quotas for production vs development\n- Consider pod priority classes for critical workloads","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:22:26.331Z","createdAt":"2026-01-26T02:59:51.909Z"},{"id":"q-7647","question":"In CKAD terms, design a per-namespace secret-rotation workflow: a CronJob in the target namespace runs every 15 minutes to fetch rotated credentials from a Vault-like API, updates the app-credentials Secret in that namespace, and streams run logs to a PVC mounted at /logs. Use an InitContainer to install curl and jq; include readiness/liveness probes for the Job's Pod, and a Namespace NetworkPolicy restricting egress to the Vault endpoint. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"The CronJob runs every 15 minutes, fetches new credentials from Vault-like API, updates the app-credentials Secret in that namespace, and streams logs to a PVC at /logs. InitContainer installs curl an","explanation":"## Why This Is Asked\nAdvanced CKAD tasks include namespace-scoped automation, secure secret handling, and operational visibility. This question tests RBAC, init containers, log persistence, and network posture in a realistic rotation workflow.\n\n## Key Concepts\n- Namespace-scoped Secrets and Secrets:update permissions\n- CronJob scheduling and in-cluster API calls\n- InitContainers for dependency setup\n- PersistentVolumeClaim-backed logs for auditability\n- Namespace NetworkPolicy restricting egress to Vault\n\n## Code Example\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: secret-rotator\n  namespace: placeholder\nspec:\n  schedule: \"*/15 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          serviceAccountName: secret-rotator-sa\n          initContainers:\n            - name: deps\n              image: alpine:3.18\n              command: [\"sh\",\"-c\",\"apk add --no-cache curl jq\"]\n              volumeMounts:\n                - name: logs\n                  mountPath: /logs\n          containers:\n            - name: rotator\n              image: alpine:3.18\n              command: [\"sh\",\"-c\",\"/scripts/rotate.sh\"]\n              volumeMounts:\n                - name: logs\n                  mountPath: /logs\n          volumes:\n            - name: logs\n              persistentVolumeClaim:\n                claimName: secret-logs\n          restartPolicy: OnFailure\n```\n\n## Follow-up Questions\n- How would you handle rotation failures and reconcile drift across namespaces?\n- What audit/observability would you add to verify Secret updates without leaks?","diagram":"flowchart TD\n  CronJob[CronJob: secret-rotator] --> JobPod[Job Pod]\n  JobPod --> SecretUpdate[Secret: app-credentials (updated)]\n  JobPod --> Logs[Logs: /logs PVC]\n  Init[InitContainer: deps] --> Logs","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","PayPal","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T11:02:24.161Z","createdAt":"2026-01-26T11:02:24.161Z"},{"id":"q-7830","question":"In CKAD terms, design a per-namespace metrics-collector that tails a PVC-mounted /metrics, converts lines to Prometheus exposition format, and pushes to http://prometheus-pushgateway:9091/metrics/job/<namespace>. InitContainer installs awk/sed; ConfigMap sets METRIC_FORMAT and BATCH_INTERVAL; Secret holds Pushgateway creds; Sidecar rotates TLS certs; include readiness/liveness probes and a Namespace NetworkPolicy restricting egress to the Pushgateway. Provide a minimal YAML skeleton. Discuss trade-offs?","answer":"Design a per-namespace metrics-collector that tails a PVC-mounted /metrics, converts lines to Prometheus exposition format, and pushes to the cluster Pushgateway. InitContainer installs awk/sed; Confi","explanation":"## Why This Is Asked\nTests ability to design per-namespace tooling with InitContainers, sidecars, and NetworkPolicy, plus Prometheus integration.\n\n## Key Concepts\n- CKAD patterns: PVC as metric source, per-namespace isolation\n- InitContainer dependency management\n- Prometheus Pushgateway integration\n- TLS rotation sidecar, readiness/liveness probes\n- Namespace-scoped NetworkPolicy\n\n## Code Example\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: metrics-config\ndata:\n  METRIC_FORMAT: 'plain'\n  BATCH_INTERVAL: '5s'\n```\n\n## Follow-up Questions\n- How would you handle malformed metric lines gracefully?\n- What changes if Pushgateway is external vs internal to cluster?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Hugging Face","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T19:38:00.893Z","createdAt":"2026-01-26T19:38:00.893Z"},{"id":"q-7867","question":"In namespace secure, a Deployment 'api' with 2 replicas mounts a ConfigMap at /etc/api-config and a Secret at /etc/api-secret. Implement per-pod securityContext (runAsNonRoot: true, runAsUser: 10001, readOnlyRootFilesystem: true), drop ALL capabilities, set resource requests/limits, add a readinessProbe on /healthz and a livenessProbe on /readyz, include a preStop sleep 5s, and create a PodDisruptionBudget requiring at least 1 available during maintenance. Add a NetworkPolicy isolating the namespace but allowing egress to api.internal:443 and DNS, while denying all other egress. Provide a minimal manifest patch and describe how you'd verify under load and rollback?","answer":"Apply container securityContext: runAsNonRoot true, runAsUser 10001, readOnlyRootFilesystem true, drop ALL; set resources; add probes; preStop sleep; create PDB minAvailable 1; add NetworkPolicy with ","explanation":"## Why This Is Asked\nTests practical hardening: container securityContext, readiness/liveness, graceful shutdown, high-availability with PDB, and network isolation via NetworkPolicy.\n\n## Key Concepts\n- Pod securityContext and container securityContext\n- Probes and lifecycle hooks\n- PodDisruptionBudget\n- NetworkPolicy basics and egress constraints\n- Per-namespace isolation in production-grade CKAD\n\n## Code Example\n```javascript\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\n  namespace: secure\nspec:\n  replicas: 2\n  template:\n    spec:\n      securityContext:\n        runAsNonRoot: true\n      containers:\n      - name: api\n        image: api:latest\n        securityContext:\n          runAsUser: 10001\n          readOnlyRootFilesystem: true\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop: [\"ALL\"]\n        volumeMounts:\n        - name: config\n          mountPath: /etc/api-config\n        - name: secret\n          mountPath: /etc/api-secret\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n          limits:\n            cpu: \"500m\"\n            memory: \"512Mi\"\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        livenessProbe:\n          httpGet:\n            path: /readyz\n            port: 8080\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"/bin/sh\", \"-c\", \"sleep 5\"]\n      volumes:\n      - name: config\n        configMap:\n          name: api-config\n      - name: secret\n        secret:\n          secretName: api-secret\n---\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: api-pdb\n  namespace: secure\nspec:\n  minAvailable: 1\n  selector:\n    matchLabels:\n      app: api\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: api-egress\n  namespace: secure\nspec:\n  podSelector:\n    matchLabels:\n      app: api\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n      ipBlock:\n        cidr: 10.96.0.0/12\n    ports:\n    - protocol: TCP\n      port: 443\n  - to:\n      namespaceSelector: {}\n    ports:\n    - protocol: UDP\n      port: 53\n```\n\n## Follow-up Questions\n- How would you monitor and validate performance under load?\n- How would you rollback the namespace isolation changes safely?","diagram":"flowchart TD\n  A[Deployment api] --> B[SecurityContext & Resources]\n  B --> C[Probes & Lifecycle]\n  C --> D[PDB & NetworkPolicy]\n  D --> E[Validation under load]","difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Microsoft","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T20:55:57.028Z","createdAt":"2026-01-26T20:55:57.029Z"},{"id":"q-7961","question":"Design a Kubernetes CKAD CronJob in the data-warehouse namespace that runs 3 parallel pods daily. Each pod mounts a PVC at /data/reconcile and a ConfigMap at /etc/reconcile; DB creds are provided via a Secret as env vars. Include resource requests/limits, terminationGracePeriodSeconds 30, and a checksum/config annotation on the Job template to re-run when the ConfigMap changes. Provide a minimal RBAC binding?","answer":"Use a CronJob in data-warehouse with jobTemplate.spec.template.spec.parallelism: 3; volumes include reconcile-rules from a ConfigMap mounted at /etc/reconcile and reconcile-output from a PVC at /data/","explanation":"## Why This Is Asked\nTests ability to orchestrate time-based batch workloads, integrate ConfigMaps and Secrets with PVs, and enforce idempotence and rollback for Kubernetes workload automation.\n\n## Key Concepts\n- CronJob with Job template and parallelism\n- ConfigMap mounted as /etc/reconcile\n- PVC mounted at /data/reconcile\n- Secret-provided DB credentials via env vars\n- Resource requests/limits and graceful termination\n- checksum/config annotation to trigger updates\n- RBAC: RoleBinding for service account\n\n## Code Example\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: reconcile-cron\n  namespace: data-warehouse\nspec:\n  schedule: \"0 2 * * *\"\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          annotations:\n            checksum/config: \"abcdef1234\" # placeholder hash\n        spec:\n          serviceAccountName: reconcile-sa\n          containers:\n          - name: reconciler\n            image: my-registry/reconcile:latest\n            envFrom:\n            - secretRef:\n                name: db-creds\n            volumeMounts:\n            - name: reconcile-rules\n              mountPath: /etc/reconcile\n            - name: reconcile-output\n              mountPath: /data/reconcile\n            resources:\n              requests:\n                cpu: \"200m\"\n                memory: \"256Mi\"\n              limits:\n                cpu: \"500m\"\n                memory: \"512Mi\"\n          volumes:\n          - name: reconcile-rules\n            configMap:\n              name: reconcile-rules\n          - name: reconcile-output\n            persistentVolumeClaim:\n              claimName: reconcile-pvc\n          terminationGracePeriodSeconds: 30\n          restartPolicy: OnFailure\n```\n\n```yaml\n# RBAC skeleton (Role + RoleBinding)\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: data-warehouse\n  name: reconcile-cron-role\nrules:\n- apiGroups: [\"batch\"]\n  resources: [\"jobs\"]\n  verbs: [\"create\", \"get\", \"list\", \"watch\"]\n- apiGroups: [\"\"]\n  resources: [\"pods\"]\n  verbs: [\"get\", \"list\", \"watch\"]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: reconcile-cron-binding\n  namespace: data-warehouse\nsubjects:\n- kind: ServiceAccount\n  name: reconcile-sa\n  namespace: data-warehouse\nroleRef:\n  kind: Role\n  name: reconcile-cron-role\n  apiGroup: rbac.authorization.k8s.io\n```","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Google","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T02:44:30.701Z","createdAt":"2026-01-27T02:44:30.701Z"},{"id":"q-7998","question":"In namespace etl, design a 2-replica Deployment 'batcher' that reads YAML job specs from a ConfigMap-mounted directory /workloads, validates with kubeval in an InitContainer, generates and applies Job objects, and enforces a per-minute RATE_LIMIT from ConfigMap batch-config. Trigger rolling restarts on checksum/config annotation when specs change. Add resources, readinessProbe on /healthz, livenessProbe on /readyz, and a PodDisruptionBudget minAvailable: 1. Provide a minimal manifest patch and explain testing under load and rollback?","answer":"2 replicas of a batcher in etl. Mount ConfigMap-backed /workloads; InitContainer to install kubeval; a small controller translates specs.yaml to Jobs and applies them; RATE_LIMIT from batch-config, en","explanation":"## Why This Is Asked\nTests knowledge of per-namespace batch processing, using ConfigMap as data, InitContainer tooling, and a small controller to translate YAML into Kubernetes Jobs.\n\n## Key Concepts\n- ConfigMap-driven runtime data\n- InitContainer for kubeval\n- Lightweight controller to create Jobs\n- Rate limiting and idempotent applies\n- checksum/config annotation for rolling restarts\n- Probes and PDB for reliability\n- Load testing and rollback strategies\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: batcher\n  namespace: etl\nspec:\n  replicas: 2\n  template:\n    metadata:\n      annotations:\n        checksum/config: \"<computed>\"\n    spec:\n      containers:\n      - name: batcher\n        image: my/batcher:latest\n        resources: {requests: {cpu: \"100m\", memory: \"128Mi\"}, limits: {cpu: \"500m\", memory: \"256Mi\"}}\n        readinessProbe:\n          httpGet: {path: /healthz, port: 8080}\n        livenessProbe:\n          httpGet: {path: /readyz, port: 8080}\n      volumes:\n      - name: workloads\n        configMap:\n          name: batch-specs\n```\n\n## Follow-up Questions\n- How would you ensure idempotence of Job creation and avoid duplicates?\n- How would you observe and rollback a faulty batch update?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T04:40:51.648Z","createdAt":"2026-01-27T04:40:51.648Z"},{"id":"q-8153","question":"Design a per-namespace concurrent job orchestrator (CKAD): a Deployment per namespace with a worker and a sidecar that fetches batch-work from an external API using an API_TOKEN from a Secret, writes results to a per-namespace ConfigMap, and a CronJob to clean up old results. InitContainer installs curl; include readiness/liveness probes; a Namespace NetworkPolicy restricting egress to the API. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Design a per-namespace worker Deployment with a sidecar that polls the external API using the Secret API_TOKEN, writes results to a namespace ConfigMap, and a CronJob that prunes old data. InitContain","explanation":"## Why This Is Asked\nTests ability to compose several CKAD primitives with namespace isolation, secret handling, and network controls in a realistic workflow.\n\n## Key Concepts\n- Namespace-scoped resources (ConfigMap, Secret)\n- Deployment with sidecar and InitContainer\n- CronJob for cleanup\n- Readiness/Liveness probes\n- Namespace NetworkPolicy\n\n## Code Example\n```yaml\n# Minimal skeleton for Deployment and CronJob (per-namespace)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: worker\n  namespace: your-namespace\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: worker\n    spec:\n      containers:\n      - name: worker\n        image: your-image:latest\n        env:\n        - name: API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: api-secret\n              key: token\n      - name: sidecar\n        image: sidecar-image:latest\n        args: [\"--watch\"]\n```\n\n```yaml\n# Minimal CronJob skeleton\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: cleanup\n  namespace: your-namespace\nspec:\n  schedule: \"@daily\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: cleanup\n            image: ubuntu:jammy\n            args: [\"/bin/sh\", \"-c\", \"echo cleanup\"]\n          restartPolicy: OnFailure\n```\n\n## Follow-up Questions\n- How would you ensure idempotent cleanup across restarts?\n- What RBAC would you apply to restrict access to the Secret and ConfigMap?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Instacart","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T11:40:31.348Z","createdAt":"2026-01-27T11:40:31.348Z"},{"id":"q-8252","question":"In CKAD terms, design a per-namespace TLS certificate provisioning workflow: a CronJob in each namespace fetches short‑lived certs from a CA API using an API_TOKEN stored in a Secret, writes cert and key into a per-namespace Secret tls-cert-<svc>, and mounts them into the app via a Secret volume. Include a TLS‑reload sidecar to refresh on rotation and RBAC to permit Secret patching. Provide a minimal YAML skeleton and discuss trade‑offs?","answer":"Namespace CronJob fetches short‑lived certs from a CA API using API_TOKEN from a Secret, writes cert and key into tls-cert-<svc> in the same namespace, and mounts that Secret as a volume in the app. A","explanation":"## Why This Is Asked\nTests per‑namespace TLS provisioning, Secret management, and in‑cluster rotation with minimal impact. It also touches RBAC, service accounts, and how apps reload TLS without restarts.\n\n## Key Concepts\n- Per‑namespace CronJob scheduling\n- Secret data lifecycle (base64, patching)\n- External CA CSR/TLS issuance flow\n- TLS reload pattern via a watcher sidecar\n\n## Code Example\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: tls-cert-rotator\n  namespace: default\nspec:\n  schedule: \"*/5 * * * *\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          serviceAccountName: tls-rotator-sa\n          containers:\n          - name: ca-client\n            image: curlimages/curl:8.30\n            command: [\"sh\",\"-lc\",\"./rotate.sh\"]\n          restartPolicy: OnFailure\n```\n\n## Follow-up Questions\n- How would you test rotation in a staging namespace without affecting prod?\n- How would you revoke a cert and propagate the revocation to running pods?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T16:58:33.877Z","createdAt":"2026-01-27T16:58:33.877Z"},{"id":"q-8305","question":"In namespace frontend, implement a blue/green deployment behind a Service on port 80. Create two Deployments: web-blue and web-green with identical Pod specs; labels version=blue and version=green. The Service should route to version=blue by default. Provide a minimal manifest patch to switch traffic to green (via Service selector or Deployment label). Include readinessProbes, livenessProbes, and resource requests/limits. Describe load-test verification and rollback steps?","answer":"Set up a blue/green rollout behind a single Service in namespace frontend. Create two Deployments: web-blue and web-green with identical specs; labels version=blue and version=green. Service selects v","explanation":"## Why This Is Asked\nTests practical blue/green deployment in Kubernetes using native primitives, including patch-based traffic switch, health checks, and rollback.\n\n## Key Concepts\n- Deployments and labels/selectors\n- Services and traffic switching via selectors\n- Probes and resource constraints\n- Patching for traffic promotion and rollback\n\n## Code Example\n```yaml\n# Frontend blue Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-blue\n  labels:\n    app: web\n    version: blue\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n      version: blue\n  template:\n    metadata:\n      labels:\n        app: web\n        version: blue\n    spec:\n      containers:\n      - name: web\n        image: yourrepo/web:1.0\n        ports:\n        - containerPort: 80\n        resources:\n          requests: {\"cpu\": \"100m\", \"memory\": \"128Mi\"}\n          limits: {\"cpu\": \"500m\", \"memory\": \"256Mi\"}\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 80\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 80\n          initialDelaySeconds: 15\n```\n\n```yaml\n# Frontend Green Deployment (same spec with version=green)\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-green\n  labels:\n    app: web\n    version: green\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: web\n      version: green\n  template:\n    metadata:\n      labels:\n        app: web\n        version: green\n    spec:\n      containers:\n      - name: web\n        image: yourrepo/web:1.0\n        ports:\n        - containerPort: 80\n        resources:\n          requests: {\"cpu\": \"100m\", \"memory\": \"128Mi\"}\n          limits: {\"cpu\": \"500m\", \"memory\": \"256Mi\"}\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 80\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 80\n'''\n\n```yaml\n# Frontend Service routing blue by default\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-service\nspec:\n  selector:\n    app: web\n    version: blue\n  ports:\n  - port: 80\n    targetPort: 80\n```\n\n## Follow-up Questions\n- How would you automate promotion to green using a periodic job?\n- How would you monitor and rollback with a single kubectl patch versus an automated canary tool?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Microsoft","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T19:17:09.202Z","createdAt":"2026-01-27T19:17:09.203Z"},{"id":"q-8351","question":"In CKAD terms, design a per-namespace event-driven rollout tester: a CronJob in each namespace reads a desired version from a ConfigMap, authenticates to an internal image registry with a Secret, and patches a target Deployment's image tag via a small script installed by an InitContainer. Logs go to a PVC at /logs; include readiness/liveness probes; apply a Namespace NetworkPolicy restricting registry access. Provide a minimal YAML skeleton and discuss trade-offs?","answer":"Implement a namespace-scoped CronJob that reads version from ConfigMap, uses a Secret to authenticate to internal registry, and updates a Deployment image tag via a small InitContainer script. Logs to","explanation":"## Why This Is Asked\nTests ability to design namespace-scoped automation, leveraging InitContainers, Secrets, ConfigMaps, PVC-backed logs, and NetworkPolicy for secure registry access.\n\n## Key Concepts\n- CKAD: per-namespace resources; InitContainer bootstrap\n- Patch Deployment image tag safely in a controlled rollout\n- Logs persisted to PVC; readiness/liveness probes ensure health\n- NetworkPolicy to restrict registry egress\n\n## Code Example\n```yaml\n# Minimal skeleton: CronJob and patching script\n```\n\n## Follow-up Questions\n- How would you test the rollout in CI/CD? \n- What changes if the registry is intermittently unavailable?\n","diagram":"flowchart TD\n  CronJob[CronJob] --> ReadConfig[Read ConfigMap: version]\n  CronJob --> Init[InitContainer: install script]\n  Init --> Patch[Patch Deployment image tag]\n  Patch --> Logs[Logs to PVC /logs]\n  Logs --> Probes[Readiness/Liveness]\n  Probes --> Network[NetworkPolicy: registry egress restricted]\n","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Plaid","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T20:56:18.681Z","createdAt":"2026-01-27T20:56:18.681Z"},{"id":"q-858","question":"In a Kubernetes CKAD scenario, you have a Deployment named 'web-app' in namespace 'prod' with 3 replicas; pods frequently OOMKilled under load. Describe a practical debugging plan and provide a minimal manifest patch showing resource requests/limits, a readiness probe, and a liveness probe. Include scaling considerations and how you'd validate the fix under load?","answer":"First, inspect recent pod events and previous logs to confirm OOMKilled, then verify container resources and limits. Set requests/limits (e.g., 500m CPU, 512Mi memory; limit 1Gi). Add a readiness prob","explanation":"## Why This Is Asked\nCKAD candidates must diagnose real issues with limited tools. This tests debugging flow, resource tuning, and readiness/liveness strategies.\n\n## Key Concepts\n- OOMKilled diagnosis through pod events and logs\n- Resource requests/limits tuning and safety margins\n- Probes (readiness and liveness) to recover from bad states\n\n## Code Example\n````yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n  namespace: prod\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: app\n        image: myrepo/web-app:latest\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"1000m\"\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n````\n\n## Follow-up Questions\n- How would you validate changes in a staging environment before production?\n- What trade-offs exist between higher requests/limits and cluster density?","diagram":"flowchart TD\n  A[Pod Events] --> B{OOMKilled?}\n  B -->|Yes| C[Inspect Resources & Probes]\n  B -->|No| D[Monitor under load]\n  C --> E[Adjust requests/limits & Probes]\n  E --> F[Rollout restart]\n  F --> G[Validate under load]","difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","LinkedIn","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:40:43.301Z","createdAt":"2026-01-12T13:40:43.301Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":93,"beginner":28,"intermediate":37,"advanced":28,"newThisWeek":37}}