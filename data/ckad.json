{"questions":[{"id":"ckad-app-deployment-1768221857337-0","question":"You are updating a 3-replica stateless web app and require zero-downtime during rollouts. Which Deployment strategy configuration best achieves this?","answer":"[{\"id\":\"a\",\"text\":\"strategy: type: Recreate\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"strategy: type: RollingUpdate, rollingUpdate: maxUnavailable: 0, maxSurge: 1\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"strategy: type: RollingUpdate, rollingUpdate: maxUnavailable: 1, maxSurge: 0\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"strategy: type: RollingUpdate, rollingUpdate: maxUnavailable: 2, maxSurge: 0\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct option is B because RollingUpdate with maxUnavailable: 0 and maxSurge: 1 ensures at most one extra pod is created during the rollout and no pod becomes unavailable, achieving zero-downtime updates.\n\n## Why Other Options Are Wrong\n- Option A: Recreate terminates all old pods before new ones are started, causing downtime.\n- Option C: maxUnavailable: 1 can momentarily reduce available replicas during the rollout.\n- Option D: maxUnavailable: 2 allows two pods to be unavailable simultaneously, increasing risk of downtime.\n\n## Key Concepts\n- RollingUpdate strategy\n- maxUnavailable and maxSurge controls\n- Zero-downtime deployment practices\n\n## Real-World Application\n- Use this pattern in customer-facing services during CI/CD deployments to minimize user impact while rolling out updates.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","kubectl","Deployment","RollingUpdate","EKS","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T12:44:17.338Z","createdAt":"2026-01-12 12:44:17"},{"id":"ckad-app-deployment-1768221857337-1","question":"You have a Deployment that consumes configuration from a ConfigMap. You want updates to the ConfigMap to be automatically visible in the running pods without restarting them. Which approach achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Populate container environment variables directly from ConfigMap keys using envFrom\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Mount the ConfigMap as a volume and read values from the mounted files\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a Secret instead of ConfigMap to reflect changes automatically\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Inline the values in the Pod spec\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because mounting the ConfigMap as a volume exposes the config as files inside the container. When the ConfigMap changes, the volume contents update automatically in the running Pod, without requiring a restart. Environment variables sourced from a ConfigMap are fixed at container startup and do not refresh.\n\n## Why Other Options Are Wrong\n- Option A: EnvFrom creates environment variables at startup; updates to the ConfigMap do not propagate to running containers.\n- Option C: Secrets rotate with explicit restarts and are not automatically updated in mounted form like a ConfigMap volume.\n- Option D: Inlining values makes configuration immutable at pod creation time and cannot reflect dynamic changes.\n\n## Key Concepts\n- ConfigMap volumes vs environment variables\n- Live-updating configuration via mounted ConfigMap\n- Pod lifecycle considerations when config changes\n\n## Real-World Application\n- For dynamic configuration in microservices, prefer ConfigMap volumes so operators can update config without recreating pods; plan a rollout or automated restart if needed for drift guarantees.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","ConfigMap","Volumes","kubectl","EKS","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T12:44:17.833Z","createdAt":"2026-01-12 12:44:18"},{"id":"ckad-app-deployment-1768221857337-2","question":"Your app exposes /healthz on port 8080 and you want readiness to prevent traffic to unhealthy pods until ready. Which readinessProbe configuration matches the required settings: path /healthz, port 8080, initialDelaySeconds 15, periodSeconds 10, failureThreshold 3?","answer":"[{\"id\":\"a\",\"text\":\"readinessProbe: httpGet: path: /healthz, port: 8080, initialDelaySeconds: 15, periodSeconds: 10, failureThreshold: 3\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"readinessProbe: httpGet: path: /healthz, port: 8080, initialDelaySeconds: 15, periodSeconds: 5, failureThreshold: 3\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"readinessProbe: httpGet: path: /health, port: 8080, initialDelaySeconds: 15, periodSeconds: 10, failureThreshold: 3\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"readinessProbe: tcpSocket: port: 8080, initialDelaySeconds: 15, periodSeconds: 10, failureThreshold: 3\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because it precisely matches the specified readiness probe: an HTTP GET to /healthz on port 8080 with an initial delay of 15 seconds, a 10-second period, and a failure threshold of 3.\n\n## Why Other Options Are Wrong\n- Option B: Period is 5 seconds, which does not meet the 10-second requirement.\n- Option C: Path is /health, which is not the expected endpoint.\n- Option D: Uses a TCP probe instead of HTTP GET, which does not check the HTTP readiness endpoint.\n\n## Key Concepts\n- Readiness probes vs liveness probes\n- HTTP GET probes and endpoint selection\n- Timing controls: initialDelaySeconds, periodSeconds, failureThreshold\n\n## Real-World Application\n- Correct readiness probing prevents routing traffic to unready pods during startup or transient failures, improving user experience and stability during deployments.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","ReadinessProbe","HTTP","kubectl","Deployment","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T12:44:18.298Z","createdAt":"2026-01-12 12:44:18"},{"id":"ckad-app-deployment-1768279612845-0","question":"Your team deploys a stateless API server to Kubernetes and expects zero-downtime during updates. Which combination best achieves zero downtime while ensuring new pods are ready before receiving traffic?","answer":"[{\"id\":\"a\",\"text\":\"Deployment with RollingUpdate strategy and readinessProbe + livenessProbe, and maxUnavailable: 0\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Deployment with RollingUpdate but without readiness probes\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"StatefulSet with rolling updates\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"DaemonSet on all nodes\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is Option A. It ensures traffic is only sent to pods that have passed readiness checks and uses a RollingUpdate strategy configured with maxUnavailable: 0 to avoid taking down healthy pods mid-update.\n\n## Why Other Options Are Wrong\n- Option B: Lacks readiness probes, so traffic can be routed to pods not yet ready, causing failures.\n- Option C: StatefulSet is intended for stateful workloads; not ideal for stateless API servers.\n- Option D: DaemonSet runs a pod on every node; this is unnecessary and scales poorly for a single service.\n\n## Key Concepts\n- RollingUpdate strategy\n- readinessProbe\n- livenessProbe\n- maxUnavailable\n\n## Real-World Application\nUsed during production deployments to minimize downtime and ensure new pods are healthy before taking on traffic.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Deployment","RollingUpdate","readinessProbe","CKAD","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:46:52.846Z","createdAt":"2026-01-13 04:46:53"},{"id":"ckad-app-deployment-1768279612845-1","question":"You need to expose multiple services externally behind a single public IP and terminate TLS at the edge with host-based routing. Which Kubernetes resource/config achieves this most effectively?","answer":"[{\"id\":\"a\",\"text\":\"Create a separate LoadBalancer service for each app\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create an Ingress resource with TLS secret and an Ingress controller\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use NodePort services for each app\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use ExternalName services pointing to an external DNS\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because an Ingress resource with TLS terminates TLS at the edge and a single Ingress controller can route traffic to multiple services behind one IP.\n\n## Why Other Options Are Wrong\n- Option A: Exposes multiple external IPs; TLS termination isn’t centralized and adds management overhead.\n- Option C: NodePort exposes ports on each node and is not ideal for external routing or TLS termination.\n- Option D: ExternalName provides a DNS alias to an external endpoint and does not provide edge TLS termination or routing.\n\n## Key Concepts\n- Ingress\n- TLS termination\n- Ingress controller\n\n## Real-World Application\nSimplifies external access and TLS management for multiple services in production.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Ingress","TLS","Service","IngressController","CKAD","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:46:53.200Z","createdAt":"2026-01-13 04:46:53"},{"id":"ckad-app-deployment-1768279612845-2","question":"To store a database password securely and inject it as environment variables into pods, which approach is correct?","answer":"[{\"id\":\"a\",\"text\":\"Create a Secret with the password and reference it in the Pod spec using valueFrom: secretKeyRef\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Store password in a ConfigMap and reference as an environment variable\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Hardcode the password in the container image\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a HostPath volume to read the password from the node's filesystem\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct: use a Kubernetes Secret and reference it in the pod env, ensuring the secret is not exposed in plain text in the manifest.\n\n## Why Other Options Are Wrong\n- Option B: ConfigMaps are not designed for sensitive data like passwords.\n- Option C: Hardcoding passwords in images is insecure and violates best practices.\n- Option D: HostPath reads from node disks, which is insecure and not portable.\n\n## Key Concepts\n- Kubernetes Secret\n- secretKeyRef\n- Environment variable injection\n\n## Real-World Application\nKeeps credentials out of code and image layers, reducing exposure risk.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Secrets","Security","Environment","CKAD","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:46:53.559Z","createdAt":"2026-01-13 04:46:53"},{"id":"ckad-app-deployment-1768279612845-3","question":"Which combination ensures predictable resource usage and prevents OOMs while balancing scheduling on the cluster?","answer":"[{\"id\":\"a\",\"text\":\"Set both requests and limits for CPU and memory on the container\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Set only limits\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Do not set any resources\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Increase node size without changing pod specs\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct: specifying both requests and limits provides scheduling guidance and prevents OOMs by enforcing per-pod budgets.\n\n## Why Other Options Are Wrong\n- Option B: Limits alone can cause throttling without guarantees for scheduling and may lead to OOMs if requests are too low.\n- Option C: No resource constraints cause unpredictable scheduling and potential OOM conditions.\n- Option D: Scaling nodes without proper per-pod constraints does not address pod-level resource governance.\n\n## Key Concepts\n- Resource requests and limits\n- QoS classes\n- OOM killer prevention\n\n## Real-World Application\nHelps maintain cluster stability under load by enforcing per-pod budgets.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","ResourceManagement","QoS","CKAD","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:46:53.683Z","createdAt":"2026-01-13 04:46:53"},{"id":"ckad-app-deployment-1768279612845-4","question":"You want to perform a canary deployment without external tooling, gradually shifting traffic from the old version to the new version while keeping rollback options. What setup enables this in Kubernetes?","answer":"[{\"id\":\"a\",\"text\":\"Run two Deployments (old and canary) behind a single Service, and use an Ingress that supports weighted routing to shift traffic gradually\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a single Deployment with RollingUpdate and rely on image tags to shift traffic\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use a StatefulSet for both versions to manage upgrades\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a DaemonSet to replicate the canary across nodes\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct: two Deployments behind a single Service with a canary-capable Ingress enables weighted traffic routing and easy rollback without external tooling.\n\n## Why Other Options Are Wrong\n- Option B: A single Deployment cannot provide fine-grained traffic weighting between versions.\n- Option C: StatefulSet is intended for stateful workloads, not canary upgrades.\n- Option D: DaemonSet is for node-wide pod management and does not support canary traffic control.\n\n## Key Concepts\n- Canary deployment\n- Ingress weighted routing\n- Rollback strategy\n\n## Real-World Application\nEnables safer releases with measurable risk control before full rollout.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Canary","Ingress","Deployment","CKAD","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-deployment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:46:53.811Z","createdAt":"2026-01-13 04:46:53"},{"id":"ckad-app-design-1768193364551-0","question":"You are deploying a web app with a Deployment of 3 replicas. You want updates to roll out without any downtime. Which Deployment strategy configuration achieves zero downtime during rolling updates?","answer":"[{\"id\":\"a\",\"text\":\"maxUnavailable=0, maxSurge=1\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"maxUnavailable=1, maxSurge=1\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"maxUnavailable=0, maxSurge=0\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"strategy: Recreate\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption A is correct because rolling updates configured with maxUnavailable: 0 and maxSurge: 1 allow updating one extra pod while keeping all existing pods available. This avoids any downtime during the rollout.\n\n## Why Other Options Are Wrong\n\n- Option B may cause temporary unavailability because one pod is taken down while another starts up.\n- Option C with maxUnavailable: 0 and maxSurge: 0 cannot deploy new pods to replace old ones, leading to stalled updates and potential downtime.\n- Option D uses the Recreate strategy which stops all pods before starting new ones, causing downtime.\n\n## Key Concepts\n\n- Kubernetes Deployment\n- RollingUpdate strategy\n- maxUnavailable\n- maxSurge\n\n## Real-World Application\n\nUse this pattern when deploying user-facing web services that require continuous availability during deployments; pair with readiness probes to ensure traffic only reaches healthy pods.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Deployment","RollingUpdate","zero-downtime","CKAD","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-design","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:49:24.553Z","createdAt":"2026-01-12 04:49:24"},{"id":"ckad-app-design-1768193364551-1","question":"Your team wants to terminate TLS at the Ingress controller for a service exposed by a Kubernetes Ingress. Which combination of resources is required to achieve this in a typical setup?","answer":"[{\"id\":\"a\",\"text\":\"Deployment, Service, and Ingress with TLS secret in the same namespace\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Deployment, ConfigMap, and Ingress\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"StatefulSet, Ingress, and a TLS secret in a different namespace\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"DaemonSet with TLS secret only\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption A is correct because TLS termination at the Ingress requires a TLS secret in the same namespace and a Service exposing the pods you want to reach; the Ingress references the secret to terminate TLS and routes traffic to the backend Service.\n\n## Why Other Options Are Wrong\n\n- Option B lacks a TLS secret; simply using a Deployment and ConfigMap does not enable TLS termination.\n- Option C places the TLS secret in a different namespace; TLS secrets must be in the same namespace as the Ingress to be used.\n- Option D omits the necessary Ingress and Service wiring; only a TLS secret won't expose the workload.\n\n## Key Concepts\n\n- Kubernetes Ingress\n- TLS secrets\n- Ingress Controller\n- Service backends\n\n## Real-World Application\n\nIn production, create a TLS secret (e.g., tls-secret) in the app's namespace, configure Ingress with tls:\n  - hosts: [example.com]\n    secretName: tls-secret\nand expose the app via a ClusterIP Service; this enables end-to-end TLS termination at the Ingress layer.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Ingress","TLS","Secrets","Networking","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-design","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:49:24.966Z","createdAt":"2026-01-12 04:49:25"},{"id":"ckad-app-design-1768193364551-2","question":"You have a microservice that writes user-uploaded files to disk and you want the data to persist beyond the life of any single Pod and be accessible to all replicas. Which Kubernetes resource is appropriate to attach to the Pod to provide persistent storage?","answer":"[{\"id\":\"a\",\"text\":\"EmptyDir\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"ConfigMap\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"PersistentVolumeClaim\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"HostPath\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption C is correct because PersistentVolumeClaims provide durable storage that outlives individual Pods and can be attached to multiple Pods when using a matching StorageClass or access mode.\n\n## Why Other Options Are Wrong\n\n- Option A (EmptyDir) is ephemeral and vanishes when the Pod is deleted.\n- Option B (ConfigMap) is for configuration data, not for file storage persistence.\n- Option D (HostPath) ties storage to a specific node and is not portable across nodes.\n\n## Key Concepts\n\n- PersistentVolume\n- PersistentVolumeClaim\n- StorageClass and access modes\n\n## Real-World Application\n\nUse a PVC for user-upload directories to ensure data persists across Pod rescheduling and is accessible to all replicas behind a Service.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","PersistentVolume","PVC","Storage","CKAD","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-design","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T04:49:25.352Z","createdAt":"2026-01-12 04:49:25"},{"id":"ckad-app-design-1768260038996-0","question":"During a rolling update of a 4-replica Deployment, how can you ensure the update never reduces the number of available pods by more than one at a time?","answer":"[{\"id\":\"a\",\"text\":\"Configure the Deployment with RollingUpdate strategy and set maxUnavailable to 1\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Configure the Deployment with RollingUpdate strategy and set maxSurge to 1\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Set maxUnavailable to 2\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Increase replicas to 6 during the upgrade\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\na) Configure the Deployment with RollingUpdate strategy and set maxUnavailable to 1.\n\n## Why Other Options Are Wrong\n\n- b) Setting maxSurge to 1 controls extra pods during update, not the number of unavailable pods.\n- c) Setting maxUnavailable to 2 would allow up to two pods to be unavailable, violating the requirement.\n- d) Increasing replicas to 6 during the upgrade does not address the rollout constraint and could increase upgrade duration.\n\n## Key Concepts\n\n- RollingUpdate strategy\n- maxUnavailable\n- Pod availability during deployments\n\n## Real-World Application\n\nUsed in production deployments to minimize downtime during application updates.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Deployment","RollingUpdate","maxUnavailable","CKAD","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-design","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:20:38.997Z","createdAt":"2026-01-12 23:20:39"},{"id":"ckad-app-design-1768260038996-1","question":"Which statement about using a ConfigMap to configure a container is true?","answer":"[{\"id\":\"a\",\"text\":\"Use a ConfigMap and inject values as environment variables only\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use a ConfigMap and mount keys as files, but changes require pod restart\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use a ConfigMap to populate both environment variables and mounted files simultaneously\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Use a Secret for non-sensitive settings\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nc) Use a ConfigMap to populate both environment variables and mounted files simultaneously.\n\n## Why Other Options Are Wrong\n\n- a) ConfigMaps can populate environment variables, but you can also mount the same data as files, so stating environment variables only is incomplete.\n- b) ConfigMap data mounted as files can be updated in some cases without a pod restart; the statement claiming a restart is required is incorrect in general.\n- d) Secrets are intended for sensitive data; non-sensitive settings should use ConfigMaps.\n\n## Key Concepts\n\n- ConfigMap data can be consumed as environment variables and as mounted files\n- Separation of concerns between ConfigMap (non-sensitive) and Secret (sensitive)\n\n## Real-World Application\n\nAllows dynamic configuration of applications without rebuilding container images, supporting both runtime env vars and config files.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","ConfigMap","EnvironmentVariables","CKAD","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-design","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:20:39.490Z","createdAt":"2026-01-12 23:20:39"},{"id":"ckad-app-design-1768260038996-2","question":"You want to expose a service externally with TLS termination using Ingress. Which approach is most appropriate?","answer":"[{\"id\":\"a\",\"text\":\"Service of type LoadBalancer with TLS termination by external load balancer\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Ingress resource with TLS configuration and a cert-manager Certificate resource\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"NodePort service with manual TLS offloading\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"External API gateway outside cluster\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nb) Ingress resource with TLS configuration and a cert-manager Certificate resource.\n\n## Why Other Options Are Wrong\n\n- a) LoadBalancer with TLS offloading may work in some environments, but Ingress with TLS is the standard, scalable Kubernetes-native approach\n- c) NodePort with manual TLS offloading is less scalable and requires external handling\n- d) An external API gateway is outside the Kubernetes-native workflow and adds complexity\n\n## Key Concepts\n\n- Ingress with TLS termination\n- cert-manager for automated certificate management\n- TLS secrets referenced by Ingress\n\n## Real-World Application\n\nProvides a scalable, centralized TLS termination point for services inside a Kubernetes cluster.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Ingress","TLS","cert-manager","CKAD","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-design","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:20:39.902Z","createdAt":"2026-01-12 23:20:39"},{"id":"ckad-app-design-1768260038996-3","question":"You want to automatically scale a Deployment based on CPU utilization. Which of the following defines the HorizontalPodAutoscaler to scale from 2 to 10 replicas with a 50% CPU target?","answer":"[{\"id\":\"a\",\"text\":\"Create a HorizontalPodAutoscaler targeting the Deployment with minReplicas 2, maxReplicas 10, and targetCPUUtilizationPercentage 50\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Set container CPU requests and limits and rely on manual scaling\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use a StatefulSet with a manual scaling policy\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a Deployment with replicas 10 and manual scale\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\na) Create a HorizontalPodAutoscaler targeting the Deployment with minReplicas 2, maxReplicas 10, and targetCPUUtilizationPercentage 50.\n\n## Why Other Options Are Wrong\n\n- b) CPU requests/limits alone do not auto-scale the workload; manual scaling is required without HPA.\n- c) StatefulSet is not the typical object for stateless autoscaling based on CPU metrics.\n- d) A fixed replicas count prevents automatic scaling based on load.\n\n## Key Concepts\n\n- HorizontalPodAutoscaler (HPA)\n- minReplicas, maxReplicas, targetCPUUtilizationPercentage\n- scaleTargetRef\n\n## Real-World Application\n\nAutomatically adjusts pod count in response to load, improving efficiency and cost.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","HorizontalPodAutoscaler","Deployment","CPUUtilization","CKAD","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-design","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:20:40.044Z","createdAt":"2026-01-12 23:20:40"},{"id":"ckad-app-design-1768260038996-4","question":"To store TLS private key for an app, which approach ensures secret data is mounted as files and not exposed via environment variables?","answer":"[{\"id\":\"a\",\"text\":\"Create a Secret and mount it as a volume\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Expose secret via environment variables\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store secret in a ConfigMap\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a ServiceAccount to fetch secret from external store\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\na) Create a Secret and mount it as a volume.\n\n## Why Other Options Are Wrong\n\n- b) Secrets should generally not be exposed via environment variables for TLS private keys\n- c) ConfigMaps are not secure for sensitive data like private keys\n- d) ServiceAccounts do not provide a secret store by themselves\n\n## Key Concepts\n\n- Secrets for sensitive data\n- Mounting Secrets as files in pods\n- Access control and least privilege\n\n## Real-World Application\n\nKeeps private keys secure and access-controlled while enabling apps to read them as files.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Secret","TLS","CKAD","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"app-design","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:20:40.185Z","createdAt":"2026-01-12 23:20:40"},{"id":"ckad-app-environment-1768151848059-0","question":"Which approach protects sensitive data by avoiding passing the secret as an environment variable, while still allowing the application to read the secret?","answer":"[{\"id\":\"a\",\"text\":\"Mount the Secret as files in a volume and have the application read the password from the mounted file\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Continue to inject the Secret as an environment variable\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Copy the Secret into the container image\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store the Secret in a ConfigMap and read it from the application\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\n- a\n\n## Why Other Options Are Wrong\n\n- b: Exposing secrets in environment variables makes them visible in process lists and logs, violating best practices for secret handling.\n- c: Copying secrets into the image risks leakage across image layers and in registries.\n- d: ConfigMaps are intended for non-sensitive configuration data; secrets should use Secret objects, and storing them in a ConfigMap is not secure.\n\n## Key Concepts\n\n- Kubernetes Secrets vs ConfigMaps\n- Secrets mounted as files vs as environment variables\n- Secret-handling best practices in Pod specs\n\n## Real-World Application\n\n- In production, mount sensitive credentials as files from Secrets and configure apps to read from file paths, reducing exposure in the process environment.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Secrets","Security","CKAD","certification-mcq","domain-weight-25"],"channel":"ckad","subChannel":"app-environment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:17:28.060Z","createdAt":"2026-01-11 17:17:28"},{"id":"ckad-app-environment-1768151848059-1","question":"To securely supply a TLS certificate and its private key to a Pod, which approach is correct?","answer":"[{\"id\":\"a\",\"text\":\"Mount the TLS certificate and private key from a Secret as files into the container\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Pass the certificate and key as environment variables to the container\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store the certificate in a ConfigMap and mount as files\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Bake the certificate into the container image\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\n- a\n\n## Why Other Options Are Wrong\n\n- b: Exposing private keys via environment variables risks leakage through process listings and logs.\n- c: ConfigMaps are not intended for sensitive data; Secrets should be used for TLS material.\n- d: Including certs in the image makes rotation and revocation harder and risks distribution of secret material.\n\n## Key Concepts\n\n- Kubernetes Secrets for TLS material\n- Mounting Secrets as files vs exposing via env vars\n- Secret rotation considerations\n\n## Real-World Application\n\n- Use Secrets to store TLS certs/keys and mount them as files at standard paths for TLS-enabled services; enables safer rotation and access control.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","TLS","Secrets","CKAD","certification-mcq","domain-weight-25"],"channel":"ckad","subChannel":"app-environment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:17:28.472Z","createdAt":"2026-01-11 17:17:28"},{"id":"ckad-app-environment-1768151848059-2","question":"Which Pod spec snippet properly ensures a container runs as a non-root user and cannot escalate privileges, while also enforcing read-only root filesystem?","answer":"[{\"id\":\"a\",\"text\":\"securityContext: { runAsNonRoot: true, allowPrivilegeEscalation: false, readOnlyRootFilesystem: true }\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"securityContext: { runAsUser: 0 }\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"securityContext: { allowPrivilegeEscalation: true }\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"securityContext: { privileged: true }\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\n- a\n\n## Why Other Options Are Wrong\n\n- b: Running as UID 0 means the container is effectively root, defeating non-root enforcement.\n- c: AllowPrivilegeEscalation: true allows privilege escalation.\n- d: Privileged: true grants extensive host privileges and defeats isolation.\n\n## Key Concepts\n\n- Pod securityContext basics\n- runAsNonRoot, readOnlyRootFilesystem, and privilegeEscalation controls\n- Privilege separation in containers\n\n## Real-World Application\n\n- Applying these settings reduces attack surface by preventing root access and restricting file system writes, aligning with production-hardening practices.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Security","PodSecurity","CKAD","certification-mcq","domain-weight-25"],"channel":"ckad","subChannel":"app-environment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T17:17:28.886Z","createdAt":"2026-01-11 17:17:28"},{"id":"ckad-app-environment-1768293201089-0","question":"Which method is best to pass a non-sensitive configuration value to a container in Kubernetes?","answer":"[{\"id\":\"a\",\"text\":\"Mount the value as an environment variable from a ConfigMap\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Mount the value as a file in a volume and read it from the container\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store the value in a Secret and expose it as an environment variable\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Embed the value in the container image at build time\",\"isCorrect\":false}]","explanation":"## Correct Answer\nMount the value as an environment variable from a ConfigMap.\n\n## Why Other Options Are Wrong\n- Mounting as a file in a volume (Option B) is valid for file-based config but does not map to environment variables and can complicate access patterns for typical 12-factor apps.\n- Using a Secret for non-sensitive data (Option C) is unnecessary and blurs the security boundary between config and secrets.\n- Embedding config in the image (Option D) removes runtime configurability and hinders environment-specific customization.\n\n## Key Concepts\n- ConfigMaps for non-sensitive config\n- Environment variables from ConfigMaps\n- Separation of configuration from container images\n\n## Real-World Application\n- Use ConfigMaps to parameterize deployments across environments without rebuilding images; update config centrally and let pods read from environment at startup or via a reload mechanism.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","ConfigMaps","Environment-variables","CKAD","Pods","certification-mcq","domain-weight-25"],"channel":"ckad","subChannel":"app-environment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:33:21.090Z","createdAt":"2026-01-13 08:33:21"},{"id":"ckad-app-environment-1768293201089-1","question":"Which securityContext setting guarantees container does not run as root in most Kubernetes distributions, without requiring knowledge of image's default user?","answer":"[{\"id\":\"a\",\"text\":\"securityContext: privileged: true\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"securityContext: runAsNonRoot: true\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"securityContext: readOnlyRootFilesystem: true\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"securityContext: allowPrivilegeEscalation: true\",\"isCorrect\":false}]","explanation":"## Correct Answer\nsecurityContext: runAsNonRoot: true\nThis setting ensures the container is not allowed to run as root, aligning with common security hardening practices. It helps prevent accidental elevation of privileges regardless of the image's default user.\n\n## Why Other Options Are Wrong\n- Option A (privileged: true) elevates privileges, increasing risk.\n- Option C (readOnlyRootFilesystem: true) protects the filesystem but does not guarantee non-root execution.\n- Option D (allowPrivilegeEscalation: true) permits escalation, opposite of the intended hardening.\n\n## Key Concepts\n- Container securityContext basics\n- runAsNonRoot as a best-practice control\n- Privilege-related settings in Kubernetes\n\n## Real-World Application\n- Apply runAsNonRoot: true in PodSecurityContexts to enforce non-root execution across deployments, reducing risk of privilege abuse.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","SecurityContext","RBAC","CKAD","certification-mcq","domain-weight-25"],"channel":"ckad","subChannel":"app-environment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:33:21.541Z","createdAt":"2026-01-13 08:33:21"},{"id":"ckad-app-environment-1768293201089-2","question":"When a ConfigMap is mounted as a volume in a Pod, what happens when the ConfigMap is updated?","answer":"[{\"id\":\"a\",\"text\":\"Changes are immediately visible in the mounted files\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Changes are visible only after the Pod is restarted\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Changes are cached and never reflected in the volume\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Changes require recreating the Deployment to take effect\",\"isCorrect\":false}]","explanation":"## Correct Answer\nChanges to a ConfigMap mounted as a volume are reflected in the Pod's filesystem, making the updated files visible to the container. Applications may still need to reload configuration, but the files themselves update automatically.\n\n## Why Other Options Are Wrong\n- Option B implies a restart is required; valid for env-based config but not for volume-mounted ConfigMaps.\n- Option C is incorrect; volumes do reflect changes.\n- Option D is incorrect; deployment recreation is not required for volume-mounted updates.\n\n## Key Concepts\n- ConfigMaps mounted as volumes\n- Real-time update behavior\n- App reload considerations\n\n## Real-World Application\n- Use volume-mounted ConfigMaps for dynamic config changes that some apps can hot-reload; plan for graceful reloads where needed.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","ConfigMaps","Volumes","CKAD","certification-mcq","domain-weight-25"],"channel":"ckad","subChannel":"app-environment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:33:22.010Z","createdAt":"2026-01-13 08:33:22"},{"id":"ckad-app-environment-1768293201089-3","question":"Which statement about environment variables from ConfigMaps and Secrets is true?","answer":"[{\"id\":\"a\",\"text\":\"EnvFrom supports merging both ConfigMaps and Secrets into environment variables\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"EnvFrom can only reference ConfigMaps, not Secrets\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"EnvFrom automatically updates containers when the source ConfigMap changes without restart\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Secrets must always be mounted as files; they cannot be exposed as environment variables\",\"isCorrect\":false}]","explanation":"## Correct Answer\nEnvFrom supports merging both ConfigMaps and Secrets into environment variables, enabling applications to access non-sensitive and sensitive data via the environment at runtime.\n\n## Why Other Options Are Wrong\n- Option B incorrectly limits EnvFrom to ConfigMaps only.\n- Option C is false since containers may require a reload to pick up changes to sources; EnvFrom does not automatically restart containers.\n- Option D is false; Secrets can be exposed as environment variables with proper handling.\n\n## Key Concepts\n- EnvFrom usage with ConfigMapRef and SecretRef\n- Environment-based configuration\n- Secrets and ConfigMaps integration\n\n## Real-World Application\n- Use EnvFrom to inject configuration into application pods consistently across environments while preserving secret safety through Secrets.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","ConfigMaps","Secrets","Environment-variables","CKAD","certification-mcq","domain-weight-25"],"channel":"ckad","subChannel":"app-environment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:33:22.171Z","createdAt":"2026-01-13 08:33:22"},{"id":"ckad-app-environment-1768293201089-4","question":"A deployment's readinessProbe should be configured to reflect app startup delay; Given the app takes 15 seconds to start, which readinessProbe configuration would ensure the Pod is considered Ready only after startup?","answer":"[{\"id\":\"a\",\"text\":\"httpGet: path:/healthz, port: 8080; initialDelaySeconds: 20; periodSeconds: 10\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"httpGet: path:/healthz, port: 8080; initialDelaySeconds: 5; periodSeconds: 10\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"tcpSocket: port: 8080; initialDelaySeconds: 0; periodSeconds: 10\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"exec: command: [\\\"bash\\\",\\\"-c\\\",\\\"echo ok\\\"]; initialDelaySeconds: 0\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A correctly delays the readiness probe beyond the app's startup time (initialDelaySeconds: 20), allowing the container to become ready only after the app has had time to initialize.\n\n## Why Other Options Are Wrong\n- Option B starts checks after only 5 seconds, risking declaring readiness before startup completes.\n- Option C uses a TCP probe, which is less precise for HTTP apps and may pass before meaningful readiness.\n- Option D uses an exec probe that does not verify HTTP readiness and can be bypassed if the app is not yet ready.\n\n## Key Concepts\n- Readiness probes and startup timing\n- InitialDelaySeconds to accommodate startup latency\n- Choosing appropriate probe type for the app\n\n## Real-World Application\n- Configure readiness probes to reflect real startup times to avoid serving traffic to not-yet-ready containers.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Probes","Readiness-Probe","CKAD","certification-mcq","domain-weight-25"],"channel":"ckad","subChannel":"app-environment","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:33:22.336Z","createdAt":"2026-01-13 08:33:22"},{"id":"ckad-app-observability-1768245846198-0","question":"A Kubernetes Deployment runs a Java service that takes about 60 seconds to start. The readiness probe is causing restarts during startup. Which configuration best prevents restarts while the app warms up?","answer":"[{\"id\":\"a\",\"text\":\"Increase readinessProbe initialDelaySeconds to 120\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Add a startupProbe with longer startup timeout and keep the readiness and liveness checks configured as usual\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Remove the readinessProbe entirely\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Add a sidecar that warms up the app before the main container starts\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption B. This config uses a startupProbe to delay health checks until startup completes, preventing restarts during warm-up. After startup, standard readiness/liveness probes resume.\n\n## Why Other Options Are Wrong\n\n- Option A delays readiness but does not isolate startup; restart risk remains during startup.\n- Option C disables readiness checks and would route traffic to potentially unhealthy pods.\n- Option D is not a standard Kubernetes mechanism for startup sequencing.\n\n## Key Concepts\n\n- StartupProbe separates startup from readiness/liveness checks\n- Readiness vs Liveness vs StartupProbe semantics\n\n## Real-World Application\n\nUse a startupProbe with an appropriate timeout (e.g., 60–120s) in deployments for apps with lengthy startup, then observe with kubectl rollout status to verify health before traffic.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Observability","StartupProbe","kubectl","rollouts","certification-mcq","domain-weight-15"],"channel":"ckad","subChannel":"app-observability","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:24:06.199Z","createdAt":"2026-01-12 19:24:06"},{"id":"ckad-app-observability-1768245846198-1","question":"You want Prometheus to scrape metrics from pods running your application. Which annotation on the pod or service enables scraping by Prometheus by default?","answer":"[{\"id\":\"a\",\"text\":\"prometheus.io/scrape: true\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"metrics.k8s.io/scrape: true\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"prometheus.io/disabled: true\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"prometheus.io/path: /metrics\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption A. The annotation prometheus.io/scrape: true signals Prometheus to scrape metrics from the target endpoint (and you can optionally add prometheus.io/port for non-standard ports).\n\n## Why Other Options Are Wrong\n\n- Option B uses an annotation that Prometheus does not universally recognize for enabling scraping.\n- Option C disables scraping entirely.\n- Option D alone does not guarantee Prometheus will discover the target unless the scrape is already enabled.\n\n## Key Concepts\n\n- Prometheus service discovery via annotations\n- Optional port annotation for non-default metrics ports\n\n## Real-World Application\n\nAdd prometheus.io/scrape: true (and port if needed) to the deployment/service to enable scraping, then verify with Prometheus targets view.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Prometheus","kubectl","observability","certification-mcq","domain-weight-15"],"channel":"ckad","subChannel":"app-observability","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:24:06.833Z","createdAt":"2026-01-12 19:24:07"},{"id":"ckad-app-observability-1768245846198-2","question":"Your Kubernetes pod runs multiple containers, including a sidecar responsible for logging. You want to tail the sidecar logs in real time. Which command correctly targets the sidecar container?","answer":"[{\"id\":\"a\",\"text\":\"kubectl logs -f pod-name\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"kubectl logs -f pod-name -c sidecar-container\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"kubectl logs --previous pod-name\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"kubectl exec -it pod-name -- tail -f /var/log/app.log\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption B. When a pod contains multiple containers, you must specify the container with -c to fetch that container's logs; -f streams live logs.\n\n## Why Other Options Are Wrong\n\n- Option A would fetch logs from the first container, which may not be the sidecar.\n- Option C shows previous container logs, which is useful after a crash but not for live sidecar logging.\n- Option D uses exec to tail a file and does not target container logs directly and is less reliable for multi-container pods.\n\n## Key Concepts\n\n- Logs from multi-container pods require -c to select the container\n- -f streams live logs\n\n## Real-World Application\n\nWhen debugging a sidecar pattern (e.g., logging or proxy sidecar), target the sidecar container to correlate its output with the main application.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","kubectl","logs","sidecar","certification-mcq","domain-weight-15"],"channel":"ckad","subChannel":"app-observability","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:24:07.435Z","createdAt":"2026-01-12 19:24:07"},{"id":"ckad-app-observability-1768245846198-3","question":"During a rolling update, the new revision is not progressing to ready and pods remain in CrashLoopBackoff. Which command helps you monitor rollout progress and determine if a rollback is required?","answer":"[{\"id\":\"a\",\"text\":\"kubectl rollout status deployment/my-app\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"kubectl describe deployment my-app\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"kubectl get pods\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"kubectl apply -f deployment.yaml\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption A. kubectl rollout status deployment/my-app shows the progress of the update and indicates if any replica sets are failing, which informs whether a rollback is required.\n\n## Why Other Options Are Wrong\n\n- Option B provides detailed deployment information but does not actively show rollout progress or readiness status.\n- Option C lists pods but doesn’t summarize rollout health.\n- Option D applies a manifest and doesn’t help observe or revert a failing rollout.\n\n## Key Concepts\n\n- Rollout status for deployments\n- Rollback decisions based on rollout feedback\n\n## Real-World Application\n\nDuring a failed update, run kubectl rollout status to verify progress, then use kubectl rollout undo deployment/... if the new revision is unhealthy.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","kubectl","rollouts","deployments","certification-mcq","domain-weight-15"],"channel":"ckad","subChannel":"app-observability","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:24:07.610Z","createdAt":"2026-01-12 19:24:07"},{"id":"ckad-app-observability-1768245846198-4","question":"A Kubernetes readinessProbe is configured to httpGet port 8080 path /health, but the application requires authentication for all endpoints. The probe keeps failing. Which approach fixes readiness detection without exposing credentials?","answer":"[{\"id\":\"a\",\"text\":\"Create a dedicated /readiness endpoint that does not require authentication\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Switch to a TCP probe on port 8080\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use an Exec probe that runs curl with credentials\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Disable readiness probes entirely\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption A. A dedicated /readiness endpoint that does not require auth allows Kubernetes to determine readiness without exposing credentials or triggering auth flows. \n\n## Why Other Options Are Wrong\n\n- Option B may pass if the port is open, but it does not test app readiness and could be misleading.\n- Option C would require credentials in the probe, creating a security risk.\n- Option D removes readiness checks, risking traffic being sent to unready pods.\n\n## Key Concepts\n\n- Readiness probes must reflect actual app readiness without requiring authentication\n- Health endpoints should be accessible without auth or via a separate protected mechanism used only during deployment\n\n## Real-World Application\n\nImplement a lightweight, unauthenticated /readiness endpoint and annotate service to reflect healthy state, ensuring pods receive traffic only when ready.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","HTTP Probes","security","observability","certification-mcq","domain-weight-15"],"channel":"ckad","subChannel":"app-observability","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T19:24:07.798Z","createdAt":"2026-01-12 19:24:07"},{"id":"q-1066","question":"Inside namespace analytics, schedule a daily batch to process a CSV: use a CronJob (02:00 UTC) to start a Job that runs a Python script from a ConfigMap, reads input from a ConfigMap, uses a Secret for DB credentials to insert results into Postgres service, writes output to a PVC, runs as non-root with a readOnlyRootFilesystem, with resource limits, a backoffLimit of 3, and a 15-minute activeDeadlineSeconds; ensure proper probes?","answer":"Configure a CronJob in namespace analytics to launch a Job daily at 02:00 UTC. The Job runs a Python script sourced from a ConfigMap, reads input from another ConfigMap, uses a Secret for Postgres cre","explanation":"## Why This Is Asked\nThis problem tests advanced CKAD skills: combining CronJob and Job for batch work, wiring Script and data through ConfigMaps, securing credentials with Secrets, persisting output via PVCs, and enforcing pod security and lifecycle constraints. It also covers resource sizing, retries, timeouts, and basic health checks in a production-like scenario.\n\n## Key Concepts\n- CronJob, Job, PodSecurityContext\n- ConfigMap for scripts and input data\n- Secret for credentials\n- PersistentVolumeClaim for outputs\n- Probes, activeDeadlineSeconds, backoffLimit\n- runAsNonRoot, readOnlyRootFilesystem, resources\n\n## Code Example\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: daily-ingest\nspec:\n  schedule: '0 2 * * *'\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: ingest\n            image: myrepo/ingest:latest\n            volumeMounts:\n            - mountPath: /scripts\n              name: script\n          volumes:\n          - name: script\n            configMap:\n              name: ingest-scripts\n          restartPolicy: OnFailure\n```\n\n## Follow-up Questions\n- How would you test cron execution and idempotence?\n- How to rotate db credentials without redeploying the Job?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Microsoft","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:26:32.694Z","createdAt":"2026-01-12T21:26:32.694Z"},{"id":"q-858","question":"In a Kubernetes CKAD scenario, you have a Deployment named 'web-app' in namespace 'prod' with 3 replicas; pods frequently OOMKilled under load. Describe a practical debugging plan and provide a minimal manifest patch showing resource requests/limits, a readiness probe, and a liveness probe. Include scaling considerations and how you'd validate the fix under load?","answer":"First, inspect recent pod events and previous logs to confirm OOMKilled, then verify container resources and limits. Set requests/limits (e.g., 500m CPU, 512Mi memory; limit 1Gi). Add a readiness prob","explanation":"## Why This Is Asked\nCKAD candidates must diagnose real issues with limited tools. This tests debugging flow, resource tuning, and readiness/liveness strategies.\n\n## Key Concepts\n- OOMKilled diagnosis through pod events and logs\n- Resource requests/limits tuning and safety margins\n- Probes (readiness and liveness) to recover from bad states\n\n## Code Example\n````yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n  namespace: prod\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: app\n        image: myrepo/web-app:latest\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"1000m\"\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n````\n\n## Follow-up Questions\n- How would you validate changes in a staging environment before production?\n- What trade-offs exist between higher requests/limits and cluster density?","diagram":"flowchart TD\n  A[Pod Events] --> B{OOMKilled?}\n  B -->|Yes| C[Inspect Resources & Probes]\n  B -->|No| D[Monitor under load]\n  C --> E[Adjust requests/limits & Probes]\n  E --> F[Rollout restart]\n  F --> G[Validate under load]","difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","LinkedIn","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:40:43.301Z","createdAt":"2026-01-12T13:40:43.301Z"},{"id":"ckad-services-networking-1768231917982-0","question":"To provide stable DNS names per pod for a stateful application and enable direct addressing by name within the cluster, which combination would you use?","answer":"[{\"id\":\"a\",\"text\":\"StatefulSet with a headless service (clusterIP: None)\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Deployment with a ClusterIP service\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"DaemonSet with a NodePort service\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"ReplicaSet with a standard ClusterIP service\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is A because a StatefulSet combined with a headless service (clusterIP: None) yields stable network identities and DNS per pod (e.g., web-0, web-1). The headless service returns per-pod DNS A records, enabling direct addressing.\n\n## Why Other Options Are Wrong\n- Option B is incorrect because a standard ClusterIP service from a Deployment provides a single virtual IP, not per-pod DNS identities.\n- Option C is incorrect; a DaemonSet with NodePort yields disparate nodes and no stable per-pod DNS identity.\n- Option D is incorrect because a ReplicaSet with a standard ClusterIP service also uses a single service IP, not stable per-pod DNS names.\n\n## Key Concepts\n- StatefulSet and headless service (clusterIP: None)\n- Pod DNS naming and per-pod addressing\n- Service DNS resolution within a cluster\n\n## Real-World Application\n- Useful for apps requiring stable network identities across restarts, such as databases or clustered services that rely on per-pod DNS names for replication.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Networking","StatefulSet","DNS","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"services-networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:31:57.984Z","createdAt":"2026-01-12 15:31:58"},{"id":"ckad-services-networking-1768231917982-1","question":"Your frontend pods in namespace frontend must only talk to the backend API in namespace backend on port 8080, and not access other namespaces. Which resource enforces this rule?","answer":"[{\"id\":\"a\",\"text\":\"A NetworkPolicy in the frontend namespace allowing egress to pods in backend on port 8080\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"PodSecurityPolicy\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"ResourceQuota\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Service\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is A because a NetworkPolicy scoped to the frontend namespace can restrict egress to only the backend pods on port 8080 (assuming a default deny behavior is in place). This enforces cross-namespace communication policies.\n\n## Why Other Options Are Wrong\n- Option B is incorrect because PodSecurityPolicy controls pod security contexts, not network access between namespaces.\n- Option C is incorrect because ResourceQuota restricts resource usage, not network traffic.\n- Option D is incorrect because a Service abstracts access to a set of pods but does not enforce cross-namespace egress restrictions.\n\n## Key Concepts\n- NetworkPolicy scope per namespace\n- Egress rules and default-deny patterns\n- Namespace-based isolation\n\n## Real-World Application\n- Helps enforce microservice boundaries and reduce blast-radius in production environments.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Networking","NetworkPolicy","Namespace","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"services-networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:31:58.337Z","createdAt":"2026-01-12 15:31:58"},{"id":"ckad-services-networking-1768231917982-2","question":"To balance requests across pods and maintain client session affinity, which Service configuration is required?","answer":"[{\"id\":\"a\",\"text\":\"spec.sessionAffinity: ClientIP in the Service\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"StatefulSet with a Headless Service\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"DaemonSet with a NodePort service\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"ExternalName service\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is A because setting spec.sessionAffinity to ClientIP makes the Service route a client's requests to the same pod, providing simple session stickiness across requests.\n\n## Why Other Options Are Wrong\n- Option B offers stable per-pod identities but does not provide load-balanced session affinity.\n- Option C is unrelated to service-level load balancing.\n- Option D maps to an external name and does not provide internal session affinity.\n\n## Key Concepts\n- Service sessionAffinity (ClientIP)\n- Basic internal load balancing behavior\n- Pod distribution across a Service\n\n## Real-World Application\n- Maintains consistent user sessions for login or cart state without external session stores.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Networking","Service","LoadBalancer","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"services-networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:31:58.683Z","createdAt":"2026-01-12 15:31:58"},{"id":"ckad-services-networking-1768231917982-3","question":"You want to expose a service externally with TLS termination at the Ingress controller. Which Kubernetes object defines the routing rules and TLS configuration?","answer":"[{\"id\":\"a\",\"text\":\"Ingress\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Service\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Endpoint\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"ConfigMap\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is A because the Ingress resource defines host/path routing rules and TLS configurations, and TLS termination is typically performed by an Ingress Controller.\n\n## Why Other Options Are Wrong\n- Option B describes a basic Service, which does not define TLS termination or routing rules to external traffic.\n- Option C Endpoints represent the actual pods backing a Service, not routing rules.\n- Option D ConfigMap stores configuration data, not routing or TLS settings for external access.\n\n## Key Concepts\n- Ingress resource and TLS configuration\n- Ingress Controller role\n- External exposure of services\n\n## Real-World Application\n- Enables HTTPS termination and central routing for multiple services behind a single entry point.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","Networking","Ingress","TLS","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"services-networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:31:58.811Z","createdAt":"2026-01-12 15:31:58"},{"id":"ckad-services-networking-1768231917982-4","question":"From a pod in namespace frontend, you want to call the billing service in namespace billing; what is the fully qualified DNS name you should use for cross-namespace service discovery?","answer":"[{\"id\":\"a\",\"text\":\"billing.billing.svc.cluster.local\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"billing.svc.cluster.local\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"billing.frontend.svc.cluster.local\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"billing.cluster.local\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is A because cross-namespace service discovery uses the pattern <service>.<namespace>.svc.cluster.local, so billing.billing.svc.cluster.local resolves to the billing service in the billing namespace.\n\n## Why Other Options Are Wrong\n- Option B omits the namespace, which is required for cross-namespace resolution.\n- Option C incorrectly uses the frontend namespace in the DNS name.\n- Option D is not a valid cluster DNS name for a service.\n\n## Key Concepts\n- Kubernetes DNS hierarchy: <service>.<namespace>.svc.cluster.local\n- Service discovery across namespaces\n\n## Real-World Application\n- Enables clean service calls across multiple namespaces without hard-coding IPs.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","DNS","ServiceDiscovery","CrossNamespace","certification-mcq","domain-weight-20"],"channel":"ckad","subChannel":"services-networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:31:58.937Z","createdAt":"2026-01-12 15:31:58"}],"subChannels":["app-deployment","app-design","app-environment","app-observability","general","services-networking"],"companies":["Goldman Sachs","Hugging Face","LinkedIn","Microsoft","Snowflake"],"stats":{"total":36,"beginner":0,"intermediate":35,"advanced":1,"newThisWeek":36}}