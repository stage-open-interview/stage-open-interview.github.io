{"questions":[{"id":"q-1066","question":"Inside namespace analytics, schedule a daily batch to process a CSV: use a CronJob (02:00 UTC) to start a Job that runs a Python script from a ConfigMap, reads input from a ConfigMap, uses a Secret for DB credentials to insert results into Postgres service, writes output to a PVC, runs as non-root with a readOnlyRootFilesystem, with resource limits, a backoffLimit of 3, and a 15-minute activeDeadlineSeconds; ensure proper probes?","answer":"Configure a CronJob in namespace analytics to launch a Job daily at 02:00 UTC. The Job runs a Python script sourced from a ConfigMap, reads input from another ConfigMap, uses a Secret for Postgres cre","explanation":"## Why This Is Asked\nThis problem tests advanced CKAD skills: combining CronJob and Job for batch work, wiring Script and data through ConfigMaps, securing credentials with Secrets, persisting output via PVCs, and enforcing pod security and lifecycle constraints. It also covers resource sizing, retries, timeouts, and basic health checks in a production-like scenario.\n\n## Key Concepts\n- CronJob, Job, PodSecurityContext\n- ConfigMap for scripts and input data\n- Secret for credentials\n- PersistentVolumeClaim for outputs\n- Probes, activeDeadlineSeconds, backoffLimit\n- runAsNonRoot, readOnlyRootFilesystem, resources\n\n## Code Example\n```yaml\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: daily-ingest\nspec:\n  schedule: '0 2 * * *'\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: ingest\n            image: myrepo/ingest:latest\n            volumeMounts:\n            - mountPath: /scripts\n              name: script\n          volumes:\n          - name: script\n            configMap:\n              name: ingest-scripts\n          restartPolicy: OnFailure\n```\n\n## Follow-up Questions\n- How would you test cron execution and idempotence?\n- How to rotate db credentials without redeploying the Job?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Microsoft","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T21:26:32.694Z","createdAt":"2026-01-12T21:26:32.694Z"},{"id":"q-1312","question":"In a Kubernetes CKAD scenario, you have a Deployment named 'web-server' in namespace 'prod' running a Node.js app. During rolling updates, some pods terminate and restart slowly, causing request timeouts. Outline concrete changes to implement startupProbe, adjust readiness and liveness probes, and add a preStop hook to drain existing connections. Provide a minimal manifest patch showing startupProbe, a readinessProbe, a livenessProbe, and a preStop lifecycle hook that ensures graceful shutdown. How would you approach this?","answer":"Implement startupProbe to delay liveness checks until the app is ready, tune readiness and liveness probes for fast detection and avoid unnecessary restarts, and add a preStop hook to signal graceful ","explanation":"## Why This Is Asked\n\nTests practical lifecycle management and how probes interact with rolling updates to avoid downtime.\n\n## Key Concepts\n\n- StartupProbe, readinessProbe, livenessProbe\n- preStop and terminationGracePeriodSeconds\n- Graceful shutdown patterns in Node.js apps\n\n## Code Example\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-server\n  namespace: prod\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: web-server\n        image: node:18\n        ports:\n        - containerPort: 3000\n        startupProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n          failureThreshold: 30\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n          initialDelaySeconds: 30\n          periodSeconds: 15\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"bash\",\"-lc\",\"sleep 5\"]\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n          limits:\n            cpu: \"500m\"\n            memory: \"512Mi\"\n      terminationGracePeriodSeconds: 30\n```\n\n## Follow-up Questions\n\n- How would you adjust thresholds for busy traffic?\n- How would you verify graceful shutdown under load?\n- What pitfalls exist with preStop sleeps in container runtimes?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","MongoDB","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T10:37:57.488Z","createdAt":"2026-01-13T10:37:57.488Z"},{"id":"q-1352","question":"You have a small Python API app to run in Kubernetes. Write a minimal manifest that creates a Deployment with 3 replicas using image myregistry/api:1.0, a readinessProbe httpGet /health on port 8080, a livenessProbe with initialDelaySeconds 15 and periodSeconds 10, and a ClusterIP Service exposing port 8080. Inject config via ConfigMap app-config with LOG_LEVEL. How would you verify and how would you scale to 5 replicas?","answer":"Create Deployment with 3 replicas using image myregistry/api:1.0, add readinessProbe httpGet /health on 8080 and livenessProbe with initialDelaySeconds 15 and periodSeconds 10. Expose a ClusterIP Serv","explanation":"## Why This Is Asked\n\nThis tests converting a real-world requirement into Kubernetes manifests and basic operational steps, including probes and config injection.\n\n## Key Concepts\n\n- Deployment and ReplicaSet lifecycle\n- Readiness and liveness probes\n- ConfigMap-based configuration\n- Service exposure and ports\n\n## Code Example\n\n```javascript\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: api\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: api\n  template:\n    metadata:\n      labels:\n        app: api\n    spec:\n      containers:\n      - name: api\n        image: myregistry/api:1.0\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: app-config\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 10\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: api-svc\nspec:\n  type: ClusterIP\n  selector:\n    app: api\n  ports:\n  - port: 8080\n    targetPort: 8080\n```\n\n## Follow-up Questions\n\n- How would you extend configuration with Secrets for sensitive data?\n- How would you monitor readiness/liveness in a multi-container pod?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Meta","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T13:09:01.337Z","createdAt":"2026-01-13T13:09:01.337Z"},{"id":"q-1393","question":"Design a Kubernetes manifest that deploys a stateless app with 3 replicas, uses a ConfigMap and a Secret, attaches a 1Gi PVC for data, includes a HorizontalPodAutoscaler, exposes via ClusterIP, and enforces a NetworkPolicy restricting egress to api.internal.example.com:443. Provide YAML fragments and discuss trade-offs?","answer":"Deployment: 3 replicas; EnvFrom: ConfigMap and Secret; volume: 1Gi PVC mounted at /data; HPA: min 3, max 10, targetCPUUtilizationPercentage: 60; Service: ClusterIP exposing port 8080; NetworkPolicy: a","explanation":"## Why This Is Asked\nExplores practical CKAD competencies: deployment orchestration, config/secret management, volumes, autoscaling, service exposure, and network isolation.\n\n## Key Concepts\n- Deployment, PVC, ConfigMap/Secret, HPA\n- ClusterIP service semantics\n- NetworkPolicy gating and its limitations\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-app\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: app\n        image: my-app:latest\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: my-app-config\n        - secretRef:\n            name: my-app-creds\n        volumeMounts:\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: data-pvc\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: data-pvc\nspec:\n  accessModes: [\"ReadWriteOnce\"]\n  resources:\n    requests:\n      storage: 1Gi\n  storageClassName: standard\n---\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: my-app-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: my-app\n  minReplicas: 3\n  maxReplicas: 10\n  targetCPUUtilizationPercentage: 60\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: my-app\nspec:\n  selector:\n    app: my-app\n  ports:\n  - port: 8080\n    targetPort: 8080\n  type: ClusterIP\n---\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: allow-egress-api\nspec:\n  podSelector:\n    matchLabels:\n      app: my-app\n  policyTypes:\n  - Egress\n  egress:\n  - to:\n    - ipBlock:\n        cidr: 203.0.113.0/24\n    ports:\n    - protocol: TCP\n      port: 443\n```\n\n## Follow-up Questions\n- How would you test this in a cluster with a restricted CNI?\n- What changes if the app becomes stateful or requires dynamic PVC provisioning?","diagram":null,"difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["OpenAI","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T14:53:41.455Z","createdAt":"2026-01-13T14:53:41.455Z"},{"id":"q-1459","question":"Create Kubernetes manifests for a simple API: Deployment using image 'my-api:1.0' that reads PORT from a ConfigMap via env, a ConfigMap with PORT and APP_MODE, readinessProbe and livenessProbe for /healthz on that port, and resource requests/limits. Expose with a Service on port 80 targeting 3000. Describe a rolling update plan?","answer":"Provide YAML manifests showing a Deployment that uses image my-api:1.0, reads PORT from a ConfigMap, includes a ConfigMap named api-config with PORT and APP_MODE, readiness and liveness probes for /he","explanation":"## Why This Is Asked\\nPractical CKAD tasks using ConfigMaps, env injection, probes, resources, and Service exposure. Demonstrates understanding of update strategies.\\n\\n## Key Concepts\\n- ConfigMap usage via env/ENV\\n- Probes: readiness and liveness\\n- Resource requests/limits\\n- Service exposure and port mapping\\n- Rolling updates and rollout verification\\n\\n## Code Example\\n\\n```javascript\\napiVersion: v1\\nkind: ConfigMap\\nmetadata:\\n  name: api-config\\ndata:\\n  PORT: 3000\\n  APP_MODE: prod\\n\\n---\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: api\\nspec:\\n  replicas: 2\\n  selector:\\n    matchLabels:\\n      app: api\\ntemplate:\\n  metadata:\\n    labels:\\n      app: api\\n  spec:\\n    containers:\\n    - name: api\\n      image: my-api:1.0\\n      ports:\\n      - containerPort: 3000\\n      env:\\n      - name: PORT\\n        valueFrom:\\n          configMapKeyRef:\\n            name: api-config\\n            key: PORT\\n      readinessProbe:\\n        httpGet:\\n          path: /healthz\\n          port: 3000\\n        initialDelaySeconds: 5\\n        periodSeconds: 10\\n      livenessProbe:\\n        httpGet:\\n          path: /healthz\\n          port: 3000\\n        initialDelaySeconds: 15\\n        periodSeconds: 20\\n      resources:\\n        requests:\\n          cpu: 100m\\n          memory: 128Mi\\n        limits:\\n          cpu: 200m\\n          memory: 256Mi\\n\\n---\\napiVersion: v1\\nkind: Service\\nmetadata:\\n  name: api-service\\nspec:\\n  selector:\\n    app: api\\n  ports:\\n  - port: 80\\n    targetPort: 3000\\n```\\n\\n## Follow-up Questions\\n- How would you reload config without pod restart?\\n- How do you verify a rolling update complete with zero downtime?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Meta","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T17:55:04.986Z","createdAt":"2026-01-13T17:55:04.986Z"},{"id":"q-1488","question":"Given a Deployment named 'image-processor' in namespace 'prod' with 6 replicas processing images from a Redis queue, design a practical patch to ensure graceful shutdown of in-flight tasks during rollouts, prevent simultaneous pod terminations, and maintain availability during node drains; include a minimal manifest patch adding a preStop script, terminationGracePeriodSeconds, readiness and liveness probes, and a PodDisruptionBudget targeting the deployment. What steps would you take to validate under load?","answer":"Patch deployment to add lifecycle preStop draining, a 90s terminationGracePeriodSeconds, readinessProbe /ready and livenessProbe /healthz, plus resource requests/limits; create PodDisruptionBudget wit","explanation":"## Why This Is Asked\nTests practical mastery of graceful rollouts, pod eviction safety, and CKAD patterns beyond basics.\n\n## Key Concepts\n- PreStop draining\n- terminationGracePeriodSeconds\n- Probes\n- PodDisruptionBudget\n- Node drains under load\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: image-processor\n  namespace: prod\nspec:\n  replicas: 6\n  template:\n    spec:\n      containers:\n      - name: image-processor\n        image: myimage\n        ports:\n        - containerPort: 8080\n        lifecycle:\n          preStop:\n            exec:\n              command: [\"/bin/sh\",\"-c\",\"/usr/local/bin/drain-tasks.sh\"]\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"250m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"500m\"\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 2\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n          timeoutSeconds: 2\n      terminationGracePeriodSeconds: 90\n---\napiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: image-processor-pdb\n  namespace: prod\nspec:\n  minAvailable: 4\n  selector:\n    matchLabels:\n      app: image-processor\n      tier: backend\n```\n\n## Follow-up Questions\n- How would you adapt for multiple queues or different backoff strategies?\n- How would you monitor backlog during drains?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Airbnb","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:00:31.030Z","createdAt":"2026-01-13T19:00:31.030Z"},{"id":"q-1547","question":"Configure a NetworkPolicy to restrict egress from prod/checkout-service to only reach payments/payment-processor in the payments namespace on port 443, blocking all other egress. Provide a minimal manifest patch and a test strategy to validate allowed and blocked traffic inside the cluster?","answer":"Implement a NetworkPolicy in the prod namespace that allows egress from pods labeled app=checkout-service to payments/payment-processor:443 only, while denying all other egress. Apply the minimal YAML patch and validate using a test pod.","explanation":"## Why This Is Asked\n\nThis task evaluates practical NetworkPolicy design, cross-namespace scoping, and cluster service DNS testing under real workloads.\n\n## Key Concepts\n\n- Kubernetes NetworkPolicy\n- namespaceSelector and podSelector\n- DNS resolution for cluster services\n- Egress testing with a dedicated test pod\n- Labeling prerequisites for policy scope\n\n## Code Example\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: checkout-egress-restrict\n  namespace: prod\nspec:\n  podSelector:\n    matchLabels:\n      app: checkout-service\n  policyTypes:\n  - Egress\n  egress:\n```","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:30:52.600Z","createdAt":"2026-01-13T21:36:32.747Z"},{"id":"q-1609","question":"You're deploying inventory-service on Kubernetes. Write YAML to (1) deploy 3 replicas with a ConfigMap for API_ENDPOINT and a Secret for DB_PASSWORD, (2) an InitContainer that runs migrations, (3) readiness and liveness probes, resource requests/limits, and a RollingUpdate strategy with maxUnavailable: 1, (4) a Job to run migrations before the first pod starts, (5) a sidecar log-shipper. Include the key fragments and rationale?","answer":"Provide a compact YAML snippet for a Deployment creating 3 replicas of inventory-service, sourcing API_ENDPOINT from a ConfigMap and DB_PASSWORD from a Secret, with an InitContainer running migrations","explanation":"## Why This Is Asked\n\nTests ability to compose Kubernetes primitives for real-world apps: Deployment, ConfigMap, Secret, InitContainer, Probes, Resources, and Jobs. It also probes rollout safety and migration orchestration.\n\n## Key Concepts\n\n- ConfigMap and Secret integration into Pods\n- InitContainer for pre-start tasks (migrations)\n- Liveness/readiness probes and resource management\n- RollingUpdate strategy with maxUnavailable\n- Separate Job for one-off migrations to ensure DB schema readiness\n\n## Code Example\n\n```javascript\nconst deployment = {\n  apiVersion: 'apps/v1',\n  kind: 'Deployment',\n  metadata: { name: 'inventory-service' },\n  spec: { /* ... */ }\n};\n```\n\n## Follow-up Questions\n\n- How would you implement a canary rollout with traffic splitting?\n- How would you validate migrations and rollback on failure?","diagram":"flowchart TD\n  A[Deployment: inventory-service] --> B[ConfigMap: API_ENDPOINT]\n  A --> C[Secret: DB_PASSWORD]\n  A --> D[InitContainer: migrations]\n  A --> E[Readiness/Liveness probes]\n  A --> F[RollingUpdate: maxUnavailable: 1]\n  G[Job: migrations] --> A","difficulty":"advanced","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","NVIDIA","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T02:35:11.694Z","createdAt":"2026-01-14T02:35:11.694Z"},{"id":"q-1626","question":"Blue/Green rollout for a CKAD production web service: in namespace 'prod', a Deployment 'web-app' with 3 replicas serves traffic via the Service 'web-app'. Introduce a canary path with a second Deployment 'web-app-canary' and a canary route (via canary Ingress or a second Service) to validate with 10–20% traffic before full switch. Provide a minimal manifest patch showing resource requests/limits, readinessProbe, and a livenessProbe for both deployments, plus how to switch traffic and rollback. Include validation steps under load?","answer":"Use a blue/green pattern: keep web-app as blue; deploy web-app-canary with 10–20% traffic behind the same Service via a canary route (Ingress canary or a second Service) and implement identical resour","explanation":"## Why This Is Asked\n\nTests practical rollout skills: blue/green pattern, traffic routing, probes, and quick rollback under load.\n\n## Key Concepts\n- Blue/Green deployment\n- Canary routing with minimal impact\n- Resource requests/limits, readinessProbe, livenessProbe\n- Service selector switching for traffic control\n\n## Code Example\n\n```yaml\n# Deployment patch for canary\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app-canary\n  namespace: prod\nspec:\n  replicas: 1\n  template:\n    spec:\n      containers:\n      - name: app\n        image: myorg/web-app:canary\n        resources:\n          requests:\n            cpu: \"100m\"\n            memory: \"128Mi\"\n          limits:\n            cpu: \"300m\"\n            memory: \"256Mi\"\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 20\n```\n\n```yaml\n# Service patched for blue/green traffic routing\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-app\n  namespace: prod\nspec:\n  selector:\n    app: web-app-blue\n  ports:\n  - port: 80\n    targetPort: 8080\n```\n\n```yaml\n# Canary Service (optional, if using separate service for canary routing)\napiVersion: v1\nkind: Service\nmetadata:\n  name: web-app-canary-svc\n  namespace: prod\nspec:\n  selector:\n    app: web-app-canary\n  ports:\n  - port: 80\n    targetPort: 8080\n```\n\n## Follow-up Questions\n- How would you automate switching traffic and rollback if issues appear?\n- How would you validate stability under a 50k RPS load during the canary phase?","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","OpenAI","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T04:14:20.574Z","createdAt":"2026-01-14T04:14:20.576Z"},{"id":"q-1707","question":"Explain a progressive canary rollout for a 4-replica frontend behind an NGINX Ingress. Use a stable Deployment and a canary Deployment (image frontend:1.2-canary, replicas:1). Patch Ingress to route 10% to canary; later 50% and 100%. Include minimal YAML patches for deployments and a canary Ingress with canary-weight; how would you monitor health and rollback?","answer":"Implement a two-Deployment canary: stable frontend with 4 replicas and canary frontend-canary with 1 replica image frontend:1.2-canary. Create an Ingress canary rule with nginx.ingress.kubernetes.io/c","explanation":"## Why This Is Asked\nTests ability to design a safe, observable canary rollout using standard Kubernetes resources and NGINX Ingress without a service mesh.\n\n## Key Concepts\n- Canary deployment pattern with separate Deployments\n- Ingress annotations (nginx.ingress.kubernetes.io/canary, canary-weight)\n- Progressive traffic shifting and rollback strategy\n- Observability: latency, error rate, saturation\n\n## Code Example\n```yaml\n# Stable Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  namespace: prod\n  labels:\n    app: frontend\n    version: stable\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: frontend\n      version: stable\n  template:\n    metadata:\n      labels:\n        app: frontend\n        version: stable\n    spec:\n      containers:\n      - name: frontend\n        image: frontend:latest\n        ports:\n        - containerPort: 80\n```\n```yaml\n# Canary Deployment\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend-canary\n  namespace: prod\n  labels:\n    app: frontend\n    version: canary\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: frontend\n      version: canary\n  template:\n    metadata:\n      labels:\n        app: frontend\n        version: canary\n    spec:\n      containers:\n      - name: frontend\n        image: frontend:1.2-canary\n        ports:\n        - containerPort: 80\n```\n```yaml\n# Ingress (production path)\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: frontend-ingress\n  namespace: prod\n  annotations:\n    # enable canary routing for the canary backend\n    nginx.ingress.kubernetes.io/canary: \"true\"\n    nginx.ingress.kubernetes.io/canary-weight: \"10\"\nspec:\n  rules:\n  - host: \"frontend.example.com\"\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: frontend-canary\n            port:\n              number: 80\n```\n\n## Follow-up Questions\n- How would you promote canary to 50% and then 100% safely?\n- What metrics would you monitor to decide progression or rollback?\n- How would you rollback if the canary fails without affecting stable users?","diagram":"flowchart TD\n  A[User Request] --> B[Ingress]\n  B --> C[Stable Backend (frontend)]\n  B --> D[Canary Route (frontend-canary)]\n  D --> E[Canary Pods]\n  C --> F[User Response]\n  E --> F","difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T07:42:45.036Z","createdAt":"2026-01-14T07:42:45.036Z"},{"id":"q-1767","question":"Scenario: In a CKAD scenario, you must roll out a new image for a stateless API 'inventory-api' in namespace 'prod' with 2% traffic to the canary, using vanilla Kubernetes (no service mesh). Outline a practical canary strategy with two Deployments and two Services, explain how you split traffic without a mesh, and provide minimal manifests for the canary Deployment and a canary-facing Service, plus a rollback plan and how you'd validate under load?","answer":"Two Deployments: stable and canary with distinct labels; two Services: inventory-svc (stable) and inventory-svc-canary (canary). Vanilla Kubernetes lacks built-in weighted routing, so use an Ingress c","explanation":"## Why This Is Asked\n\nTests practical canary workflows in vanilla Kubernetes, a common CKAD constraint.\n\n## Key Concepts\n\n- Canary rollout without service mesh\n- Dual deployments and service selectors\n- Ingress-based weighted routing\n- Rollback and validation under load\n\n## Code Example\n\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inventory-api-canary\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: inventory-api\n      version: canary\n  template:\n    metadata:\n      labels:\n        app: inventory-api\n        version: canary\n    spec:\n      containers:\n      - name: inventory-api\n        image: repo/inventory-api:canary\n        ports:\n        - containerPort: 80\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 15\n          periodSeconds: 20\n```\n\n```yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: inventory-svc-canary\nspec:\n  selector:\n    app: inventory-api\n    version: canary\n  ports:\n  - port: 80\n    targetPort: 80\n```\n\n```yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: inventory-ingress\n  annotations:\n    nginx.ingress.kubernetes.io/canary: \"true\"\n    nginx.ingress.kubernetes.io/canary-weight: \"2\"\nspec:\n  rules:\n  - host: inventory.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: inventory-svc\n            port:\n              number: 80\n```\n\n## Follow-up Questions\n\n- How would you automate canary promotions or rollbacks?\n- What metrics would you monitor to detect canary issues? ","diagram":null,"difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:51:06.854Z","createdAt":"2026-01-14T09:51:06.854Z"},{"id":"q-1776","question":"Design and implement a CKAD deployment for a 3-replica web app: use a ConfigMap for env vars, define resource requests/limits, add readiness and liveness probes, and configure an HPA with min 3, max 10 and targetCPUUtilizationPercentage 50. Provide the YAML and show verification steps?","answer":"Define a Deployment with 3 replicas, envFrom: configMapRef: name: web-env, resources: requests: cpu: 100m, memory: 128Mi; limits: cpu: 250m, memory: 256Mi; readinessProbe and livenessProbe for /; HPA:","explanation":"## Why This Is Asked\nTests practical CKAD skills: Deployments, ConfigMaps, resources, probes, and autoscaling with verification.\n\n## Key Concepts\n- Deployment, ConfigMap, resources, probes, HPA\n- Rollout verification, kubectl, metrics-server\n- Safe rollout strategies\n\n## Code Example\n```yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: web-app\n    spec:\n      containers:\n      - name: web\n        image: nginx:latest\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: web-env\n        resources:\n          requests:\n            memory: \"128Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"256Mi\"\n            cpu: \"250m\"\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 80\n          initialDelaySeconds: 15\n          periodSeconds: 20\n```\n\n## Follow-up Questions\n- How would you adapt this for a multi-tenant cluster? \n- What changes if the app uses a sidecar log collector?","diagram":null,"difficulty":"beginner","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:39:53.916Z","createdAt":"2026-01-14T10:39:53.916Z"},{"id":"q-858","question":"In a Kubernetes CKAD scenario, you have a Deployment named 'web-app' in namespace 'prod' with 3 replicas; pods frequently OOMKilled under load. Describe a practical debugging plan and provide a minimal manifest patch showing resource requests/limits, a readiness probe, and a liveness probe. Include scaling considerations and how you'd validate the fix under load?","answer":"First, inspect recent pod events and previous logs to confirm OOMKilled, then verify container resources and limits. Set requests/limits (e.g., 500m CPU, 512Mi memory; limit 1Gi). Add a readiness prob","explanation":"## Why This Is Asked\nCKAD candidates must diagnose real issues with limited tools. This tests debugging flow, resource tuning, and readiness/liveness strategies.\n\n## Key Concepts\n- OOMKilled diagnosis through pod events and logs\n- Resource requests/limits tuning and safety margins\n- Probes (readiness and liveness) to recover from bad states\n\n## Code Example\n````yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-app\n  namespace: prod\nspec:\n  replicas: 3\n  template:\n    spec:\n      containers:\n      - name: app\n        image: myrepo/web-app:latest\n        resources:\n          requests:\n            memory: \"512Mi\"\n            cpu: \"500m\"\n          limits:\n            memory: \"1Gi\"\n            cpu: \"1000m\"\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 15\n          periodSeconds: 5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 30\n          periodSeconds: 10\n````\n\n## Follow-up Questions\n- How would you validate changes in a staging environment before production?\n- What trade-offs exist between higher requests/limits and cluster density?","diagram":"flowchart TD\n  A[Pod Events] --> B{OOMKilled?}\n  B -->|Yes| C[Inspect Resources & Probes]\n  B -->|No| D[Monitor under load]\n  C --> E[Adjust requests/limits & Probes]\n  E --> F[Rollout restart]\n  F --> G[Validate under load]","difficulty":"intermediate","tags":["ckad"],"channel":"ckad","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","LinkedIn","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:40:43.301Z","createdAt":"2026-01-12T13:40:43.301Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Coinbase","Discord","DoorDash","Goldman Sachs","Google","Hugging Face","IBM","LinkedIn","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","Slack","Snap","Snowflake","Two Sigma"],"stats":{"total":13,"beginner":3,"intermediate":7,"advanced":3,"newThisWeek":13}}