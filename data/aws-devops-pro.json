{"questions":[{"id":"aws-devops-pro-config-management-1768216764137-0","question":"You manage infrastructure with Terraform using a remote state stored in S3. An operator manually changes an EC2 instance type, causing drift from the declared configuration. What is the most reliable way to detect this drift and revert to the desired state in a production pipeline?","answer":"[{\"id\":\"a\",\"text\":\"Run terraform plan (which refreshes the state) to detect drift, then run terraform apply to revert to the desired state\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use AWS Config drift detection to identify drift and then apply the manual changes to the instance\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Run terraform refresh to update state and then manually patch the EC2 instance to match the config\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Destroy the instance and recreate it from scratch with terraform apply\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because Terraform plan (with state refresh) compares the real infrastructure against the desired state in code and the current state file, allowing you to identify drift. Running terraform apply then enacts the changes to revert to the declared configuration. \n\n## Why Other Options Are Wrong\n- Option B: AWS Config drift detection helps identify drift but does not automatically revert infrastructure to match Terraform configurations managed by IaC. It often requires additional remediation steps outside Terraform.\n- Option C: terraform refresh alone updates the state but does not enforce or apply changes to resources; manual patches break IaC discipline and drift remains.\n- Option D: Destroying and recreating can work but is disruptive and not idempotent; it’s not the recommended way to reconcile drift in a controlled IaC workflow.\n\n## Key Concepts\n- Terraform state and drift detection\n- Idempotent infrastructure with IaC and plan/apply workflow\n- Remote state management in S3\n\n## Real-World Application\n- In production, teams detect drift via planned state refresh checks and automatically re-apply configurations to maintain compliance with IaC definitions.","diagram":null,"difficulty":"intermediate","tags":["Terraform","AWS","drift-detection","certification-mcq","domain-weight-17"],"channel":"aws-devops-pro","subChannel":"config-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:19:24.139Z","createdAt":"2026-01-12 11:19:24"},{"id":"aws-devops-pro-config-management-1768216764137-1","question":"You are designing a Terraform-based deployment that provisions EC2 instances and uses a database password stored in AWS Secrets Manager. To avoid storing secrets in code or state, which approach is best for secret management in this scenario?","answer":"[{\"id\":\"a\",\"text\":\"Store the secret in AWS Secrets Manager and reference it in Terraform via a data source when rendering the instance's user-data\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Embed the secret directly in a Terraform variable and commit it to the repository\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store the secret in an S3 bucket in plaintext and fetch it at provisioning time\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Pass the secret through environment variables on the CI runner only\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because secrets are best managed by a dedicated secret store (Secrets Manager) with controlled access and rotation. The Terraform data source can fetch the secret securely at plan/apply time without storing it in code or state. \n\n## Why Other Options Are Wrong\n- Option B: Embedding secrets in code or variables defeats secret management and exposes sensitive data in version control.\n- Option C: Storing plaintext secrets in S3 is insecure and risks leakage.\n- Option D: Passing secrets only via CI runner environment variables risks exposure in logs and does not centralize secret management across environments.\n\n## Key Concepts\n- Secrets management with AWS Secrets Manager\n- Data sources in Terraform to fetch secrets securely\n- Avoiding secret leakage in IaC state\n\n## Real-World Application\n- Enables secure, auditable secret handling across deployments and simplifies rotation workflows.","diagram":null,"difficulty":"intermediate","tags":["Terraform","AWS","Secrets Manager","certification-mcq","domain-weight-17"],"channel":"aws-devops-pro","subChannel":"config-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:19:24.487Z","createdAt":"2026-01-12 11:19:24"},{"id":"aws-devops-pro-config-management-1768216764137-2","question":"In a multi-account AWS environment, you want to ensure a consistent tagging policy across all resources created by Terraform across accounts and environments. Which approach best achieves this with minimal drift and reusable code?","answer":"[{\"id\":\"a\",\"text\":\"Create a centralized Terraform module that applies a standard set of tags to all resources and enforce usage of the module across environments\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely on AWS Config to flag resources without required tags and manually remediate\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use CloudFormation StackSets to enforce tags on resources\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Tag resources manually after deployment to ensure coverage\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct because centralizing tagging logic in a reusable Terraform module guarantees consistent tags across all resources and environments, reducing drift and ensuring policy-compliant deployments. \n\n## Why Other Options Are Wrong\n- Option B: AWS Config helps detect non-compliant tags but remediation across many accounts is more error-prone and slower than a single source of truth in Terraform modules.\n- Option C: CloudFormation StackSets are not used to enforce Terraform tagging policies and would complicate a Terraform-centric workflow.\n- Option D: Manual tagging is error-prone and inconsistent across accounts.\n\n## Key Concepts\n- Terraform modules as policy of infrastructure\n- Tagging best practices in IaC\n- Multi-account governance\n\n## Real-World Application\n- Ensures consistent cost allocation and compliance across dozens of accounts with minimal manual intervention.","diagram":null,"difficulty":"intermediate","tags":["Terraform","AWS","Tagging","Policy as Code","certification-mcq","domain-weight-17"],"channel":"aws-devops-pro","subChannel":"config-management","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:19:24.835Z","createdAt":"2026-01-12 11:19:24"},{"id":"q-1183","question":"Design a cross-account AWS CI/CD flow for a real-time analytics platform with data plane in Account A, model training in Account B, and API endpoints in Account C. Implement Terraform-driven IaC, policy-driven approvals, drift detection, canary promotion with synthetic monitoring, and automated rollback. How do you enforce least privilege, secret rotation, and auditable artifact pipelines across accounts?","answer":"Central management account orchestrates Terraform deployments to three target accounts via cross‑account roles with per‑service least privilege. Each account uses S3 backend + DynamoDB locking, Secret","explanation":"## Why This Is Asked\n\nThis question probes cross-account IAM, IaC lifecycle, drift detection, canary testing, and automated rollback at scale—skills used in top shops.\n\n## Key Concepts\n\n- Cross-account roles and trust policies\n- Terraform with per-account backends\n- Secrets rotation with Secrets Manager\n- Canary testing with CloudWatch Synthetics and CodePipeline approvals\n- Drift detection with Config Rules and auditable artifact flows\n\n## Code Example\n\n```javascript\n// Example: assume-role to deploy to Account B from management account\nconst { STSClient, AssumeRoleCommand } = require(\"@aws-sdk/client-sts\");\nconst sts = new STSClient({ region: \"us-east-1\" });\nasync function assume(accountId) {\n  const cmd = new AssumeRoleCommand({ RoleArn: `arn:aws:iam::${accountId}:role/DeploymentRole`, RoleSessionName: \"deploy\" });\n  const res = await sts.send(cmd);\n  return res.Credentials;\n}\n```\n\n## Follow-up Questions\n\n- How would you handle secret rotation across accounts without coupling IAM roles tightly to rotation schedules?\n- What metrics would you expose for canary tests to detect regressions quickly and deterministically roll back?","diagram":"flowchart TD\n  A[GitHub Source] --> B[CodePipeline]\n  B --> C[Account A]\n  B --> D[Account B]\n  B --> E[Account C]\n  C --> F[Terraform Apply]\n  D --> G[Terraform Apply]\n  E --> H[Terraform Apply]","difficulty":"advanced","tags":["aws-devops-pro"],"channel":"aws-devops-pro","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Microsoft","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:43:50.447Z","createdAt":"2026-01-13T03:43:50.447Z"},{"id":"q-676","question":"You have a Node.js app deployed on EC2 instances behind an Application Load Balancer in a private VPC. You want a beginner-friendly, repeatable CI/CD pipeline that triggers on git pushes, runs tests, and safely deploys with rollback. Describe a practical setup using AWS CodePipeline, CodeBuild, and CodeDeploy (blue/green) to auto-build, test, and deploy with artifact flow in S3, and IAM roles with least privilege, including how to isolate staging vs production using separate target groups?","answer":"Describe a practical setup using AWS CodePipeline, CodeBuild, and CodeDeploy (blue/green) to auto-build, test, and deploy a Node.js app on EC2 behind an ALB, with artifacts in S3 and IAM roles with le","explanation":"## Why This Is Asked\n\nAssesses practical understanding of building an end-to-end CI/CD pipeline on AWS with concrete services, artifact handling, and rollback.\n\n## Key Concepts\n\n- CodePipeline source, build, deploy stages\n- Blue/green deployment with CodeDeploy\n- IAM least privilege roles and policy scoping\n- S3 artifacts and environment isolation\n\n## Code Example\n\n```bash\n# Example CLI invocation to create a pipeline (simplified)\naws codepipeline create-pipeline --cli-input-json file://pipeline.json\n```\n\n## Follow-up Questions\n\n- How would you validate the rollback path and automate it?\n- What are common pitfalls with blue/green workloads on EC2?","diagram":"flowchart TD\n  SourceGitHub[GitHub] --> Pipeline[CodePipeline]\n  Pipeline --> Build[CodeBuild]\n  Build --> Deploy[CodeDeploy (Blue/Green)]\n  Deploy --> ProdALB[ALB Target Group (Prod)]\n  ProdALB --> Health[Health Checks]\n  Health -->|OK| End[Done]\n  Health -->|Fail| Rollback[Rollback to Previous]","difficulty":"beginner","tags":["aws-devops-pro"],"channel":"aws-devops-pro","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Discord","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T14:47:30.424Z","createdAt":"2026-01-11T14:47:30.424Z"},{"id":"q-890","question":"Design an AWS-based CI/CD pipeline for a data platform (S3 data lake, Lambda ETL, ECS) that sources from Git, runs unit tests and data quality checks (Great Expectations) on staging data, then plans/applies Terraform in staging and promotes to production with canary deployment and traffic shift. Explain IAM least-privilege, S3 artifact flow, and staging vs prod isolation?","answer":"Propose a staged pipeline: source from Git to CodePipeline; CodeBuild runs unit tests and data quality checks against staging data in S3; Terraform plan/apply runs in a staging account; promote to pro","explanation":"## Why This Is Asked\n\nTests a real-world, multi-account CI/CD setup for a data platform, emphasizing data quality gates, safe promotion, and robust rollback.\n\n## Key Concepts\n\n- AWS CodePipeline/CodeBuild for CI\n- Great Expectations data quality checks\n- Terraform in staging vs prod with isolated state\n- Canary or blue/green deployment with traffic shift\n- Cross-account IAM and least privilege\n- S3-based artifact flow and versioning\n\n## Code Example\n\n```hcl\n# Terraform backend separation example\nterraform {\n  backend \"s3\" {\n    bucket = \"tf-artifacts-staging\"\n    key    = \"env/staging/terraform.tfstate\"\n    region = \"us-east-1\"\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you implement rollback if data quality gates fail mid-deploy?\n- What monitoring and alerting would you add for canary promotions?","diagram":"flowchart TD\n  A[Git] --> B[CodePipeline]\n  B --> C{Staging}\n  C --> D[Unit Tests & Data Quality]\n  D --> E[Terraform Plan - Staging]\n  E --> F[Terraform Apply - Staging]\n  F --> G[Promote to Prod Canary]\n  G --> H[Traffic Shift]\n  H --> I[Prod Alarms & Rollback]\n  I --> J[Failure Handling]","difficulty":"intermediate","tags":["aws-devops-pro"],"channel":"aws-devops-pro","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Citadel","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:30:19.001Z","createdAt":"2026-01-12T14:30:19.001Z"},{"id":"q-924","question":"Design a beginner-friendly AWS CI/CD pipeline for a serverless REST API deployed to AWS Lambda behind API Gateway. Source from GitHub; CodeBuild runs unit tests with pytest and a basic security scan with bandit; artifacts stored in S3; deployment uses SAM/CFN to Lambda with separate stages dev/stage/prod and a canary traffic shift via Lambda aliases. Explain IAM least-privilege and secure env vars (Secrets Manager or SSM)?","answer":"Implement a CodePipeline with CodeBuild that clones from GitHub, installs dependencies, runs pytest and bandit, builds a Lambda deployment package, stores artifacts in S3, and deploys with SAM/CFN to ","explanation":"## Why This Is Asked\nTests end-to-end pipeline for serverless workloads, covering CI/CD, security checks, and access control at beginner level.\n\n## Key Concepts\n- CodePipeline + CodeBuild for serverless workflows\n- Lambda versioning and traffic shifting (canary)\n- IAM least privilege per environment\n- Secrets management (Secrets Manager or SSM Parameter Store with KMS)\n\n## Code Example\n```yaml\n# Simplified SAM/CFN deployment snippet\nResources:\n  ApiFunction:\n    Type: AWS::Serverless::Function\n    Properties:\n      Handler: app.lambda_handler\n      Runtime: python3.9\n      Environment:\n        Variables:\n          DB_PASSWORD: {{resolve:ssm-secure:/prod/db/password:1}}\n```\n\n## Follow-up Questions\n- How would you roll back a failed canary step?\n- How do you rotate credentials without downtime?\n","diagram":"flowchart TD\nA[GitHub Repo] --> B[CodeBuild]\nB --> C[S3 Artifact]\nC --> D[CodePipeline]\nD --> E[Lambda Versioned Alias]\nE --> F[Canary Traffic Shift]","difficulty":"beginner","tags":["aws-devops-pro"],"channel":"aws-devops-pro","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Square","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:35:01.144Z","createdAt":"2026-01-12T15:35:01.144Z"},{"id":"q-998","question":"In a 3-account AWS setup (Dev, SecProd, Prod), you need a Git-driven CI/CD that builds a container image in Dev, promotes IaC and app config via Terraform, and signals canary tests in Prod before full rollout. Explain cross-account CodePipeline stages, IAM roles with least privilege, Secrets Manager rotation, and canary deployment with rollback triggers. Include artifact flow and auditing considerations?","answer":"Design a 3-account GitOps: Dev, SecProd, Prod. Build image in Dev, push to ECR; promote IaC/config through Terraform in SecProd then Prod using cross-account roles with least privilege. Rotate DB cred","explanation":"## Why This Is Asked\nTests cross-account IAM, secret rotation, and robust rollbacks in a real pipeline.\n\n## Key Concepts\n- Cross-account IAM roles and trust policies\n- Secrets Manager rotation with Lambda\n- Canary deployment via ALB/path-based routing + CloudWatch alarms\n- Terraform-based IaC promotion across accounts\n- Audit with CloudTrail and Config\n\n## Code Example\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\"Effect\": \"Allow\", \"Action\": [\"sts:AssumeRole\"], \"Resource\": \"arn:aws:iam::123456789012:role/CICD-Prod-Role\"}\n  ]\n}\n```\n\n## Follow-up Questions\n- How would you version and rollback Terraform state across accounts?\n- How to handle secret rotation failures and emergency stop signals?","diagram":"flowchart TD\n  A[Git Commit] --> B[Dev: CodePipeline builds image to ECR]\n  B --> C[Promote IaC to SecProd via cross-account roles]\n  C --> D[Promote to Prod]\n  D --> E[Canary traffic via ALB]\n  E --> F[Alarms trigger rollback]","difficulty":"intermediate","tags":["aws-devops-pro"],"channel":"aws-devops-pro","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:41:34.997Z","createdAt":"2026-01-12T18:41:34.997Z"},{"id":"aws-devops-pro-incident-response-1768279398224-0","question":"A production fleet of 8 EC2 instances behind an Application Load Balancer experiences a sudden CPU spike. You want to detect it quickly, unify alerts, and automatically initiate remediation via an Incident Manager playbook. Which combination is the most effective approach?","answer":"[{\"id\":\"a\",\"text\":\"Create a CloudWatch composite alarm that triggers an EventBridge rule to start an Incident Manager incident and run an SSM Automation document for remediation\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create separate CloudWatch alarms for each instance and notify the on-call via SNS; rely on manual triage\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use CloudTrail to monitor API activity and GuardDuty to trigger auto-remediation\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Configure Route 53 health checks to divert traffic away from overloaded instances\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because it combines a CloudWatch composite alarm to capture multi-instance pressure, a single EventBridge target to create one Incident Manager incident, and an SSM Automation document to execute remediation steps in a repeatable, automated fashion.\n\n## Why Other Options Are Wrong\n- Option b would generate separate incidents and notifications per instance, leading to alert storms and slower remediation.\n- Option c relies on security-centric services that are not designed for real-time incident detection or remediation orchestration in this scenario.\n- Option d only monitors external reachability and does not address in-application CPU pressure or automation.\n\n## Key Concepts\n- CloudWatch composite alarms\n- EventBridge routing to Incident Manager\n- Systems Manager (SSM) Automation for remediation\n- Incident-driven automation and runbooks\n\n## Real-World Application\nAdopt a single, automated incident creation path from a multi-metric alarm and attach an automated remediation runbook to reduce MTTR and ensure consistent response across the fleet.","diagram":null,"difficulty":"intermediate","tags":["AWS","EC2","CloudWatch","EventBridge","SSM","Incident-Manager","certification-mcq","domain-weight-14"],"channel":"aws-devops-pro","subChannel":"incident-response","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:43:18.225Z","createdAt":"2026-01-13 04:43:18"},{"id":"aws-devops-pro-incident-response-1768279398224-1","question":"Your organization operates across multiple AWS accounts and regions. You want centralized on-call management for incidents, with consistent playbooks and post-incident reporting. Which feature set best supports this capability?","answer":"[{\"id\":\"a\",\"text\":\"AWS Systems Manager Incident Manager with cross-account routing through a central EventBridge bus\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create alarms in each account and push notifications to a shared Slack channel via SNS\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Aggregate findings in Security Hub and rely on manual incident creation\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Manage incident workflows solely with Terraform across accounts\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because Incident Manager supports centralized incident handling, escalations, runbooks, and post-incident reporting, and can be connected across accounts/regions via a central EventBridge bus to route alerts consistently.\n\n## Why Other Options Are Wrong\n- Option b introduces manual, scattered notification paths and lacks centralized incident handling and post-incident analytics.\n- Option c relies on Security Hub without the structured incident lifecycle and remediation workflows.\n- Option d focuses on IaC provisioning and does not address real-time incident response orchestration.\n\n## Key Concepts\n- Incident Manager cross-account routing\n- EventBridge buses for central event ingestion\n- Standardized runbooks and post-incident reporting\n\n## Real-World Application\nImplement a multi-account incident management plane where regional alerts funnel into a central Incident Manager instance, enabling uniform playbooks, escalation, and RCA artifacts.","diagram":null,"difficulty":"intermediate","tags":["AWS","Incident-Manager","EventBridge","Cross-Account","On-Call","Playbooks","certification-mcq","domain-weight-14"],"channel":"aws-devops-pro","subChannel":"incident-response","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:43:18.593Z","createdAt":"2026-01-13 04:43:18"},{"id":"aws-devops-pro-incident-response-1768279398224-2","question":"After an incident, your team needs long-term RCA artifacts and audit trails. You want to store incident data (timeline, communications, and remediation steps) for 90 days with low cost and easy retrieval. Which approach is best?","answer":"[{\"id\":\"a\",\"text\":\"Export incident artifacts to an S3 bucket and apply a lifecycle policy to transition to Glacier after 90 days\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Keep all artifacts only in Incident Manager and rely on its built-in retention\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Archive data only in CloudWatch Logs with default retention\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store artifacts in DynamoDB for structured search\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because resilient RCA artifacts should be stored externally for long-term access; S3 provides durable storage with lifecycle policies to move data to cheaper cold storage like Glacier after 90 days, balancing cost and retrieval needs.\n\n## Why Other Options Are Wrong\n- Option b keeps data only in Incident Manager, which is not suitable for long-term archival or external analytics.\n- Option c CloudWatch Logs retention is suitable for logs, but incident timelines and runbooks may require richer structure beyond logs and is not optimized for policy-driven lifecycle.\n- Option d DynamoDB could work but is more costly and requires custom schema and indexing for search; S3 is simpler and cheaper for archival.\n\n## Key Concepts\n- Incident data archival\n- S3 with lifecycle policies\n- RCA artifact accessibility\n\n## Real-World Application\nSet up an automated post-incident export to S3 at incident creation and configure lifecycle rules to archive older artifacts, ensuring legal/compliance retention without overspending.","diagram":null,"difficulty":"intermediate","tags":["AWS","S3"," Glacier","Incidents","RCA","Incident-Manager","certification-mcq","domain-weight-14"],"channel":"aws-devops-pro","subChannel":"incident-response","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:43:18.937Z","createdAt":"2026-01-13 04:43:18"},{"id":"aws-devops-pro-incident-response-1768279398224-3","question":"During a period of high alert volume, you want to minimize duplicate incidents by grouping related alerts into a single incident. Which option best achieves this in a scalable way?","answer":"[{\"id\":\"a\",\"text\":\"Use a CloudWatch composite alarm that aggregates multiple alarms so a single incident is created\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create separate independent alarms with individual SNS topics that each start their own incident\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Disable all alarms during peak hours to avoid duplicates\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Manually merge incidents after they are created\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because composite alarms consolidate related signals into one alert, reducing duplicate incidents and providing a single entry point for incident management.\n\n## Why Other Options Are Wrong\n- Option b defeats the purpose by creating multiple incidents for related alerts.\n- Option c suppresses visibility and can hide real issues.\n- Option d is labor-intensive and error-prone, not scalable.\n\n## Key Concepts\n- CloudWatch composite alarms\n- Alert correlation for incidents\n\n## Real-World Application\nDesign alerting to trigger a single incident for outages affecting multiple services, helping on-call teams focus on remediation rather than noise.","diagram":null,"difficulty":"intermediate","tags":["AWS","CloudWatch","Alarms","Incident-Manager","Alerting","certification-mcq","domain-weight-14"],"channel":"aws-devops-pro","subChannel":"incident-response","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:43:19.075Z","createdAt":"2026-01-13 04:43:19"},{"id":"aws-devops-pro-incident-response-1768279398224-4","question":"To validate the effectiveness of your incident response readiness, which practice best aligns with mature DevOps and SRE principles?","answer":"[{\"id\":\"a\",\"text\":\"Regularly run simulated incidents (tabletop and/or live drills) using Incident Manager runbooks\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely only on monitoring dashboards without drills\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Wait for a real incident to occur to test runbooks\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Test runbooks only in a non-production environment\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because regular, structured drills (tabletop and live) validate end-to-end readiness, ensure runbooks are accurate, and reveal gaps in escalation, communication, and remediation timelines.\n\n## Why Other Options Are Wrong\n- Option b ignores the need to exercise and validate responses under realistic pressure.\n- Option c relies on destructive learning from failures rather than proactive drills.\n- Option d might miss production-specific constraints; real-world readiness requires production-like testing.\n\n## Key Concepts\n- Incident response drills\n- Runbooks validation\n- Escalation and communication readiness\n\n## Real-World Application\nIncorporate quarterly or monthly incident drills with predefined scenarios to continuously improve playbooks, tooling, and on-call readiness.","diagram":null,"difficulty":"intermediate","tags":["AWS","Incident-Manager","Runbooks","Tabletop","DR/DRP","SRE","certification-mcq","domain-weight-14"],"channel":"aws-devops-pro","subChannel":"incident-response","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T04:43:19.349Z","createdAt":"2026-01-13 04:43:19"},{"id":"aws-devops-pro-monitoring-logging-1768256701471-0","question":"You run an EKS cluster with many microservices. You want centralized, cost-effective logging with fast search across services. Which approach best meets this requirement?","answer":"[{\"id\":\"a\",\"text\":\"Ship all container logs to a single CloudWatch Logs log group using a structured JSON format and analyze with Logs Insights.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Create a separate CloudWatch Logs log group per service and namespace, and set long-term storage in S3, then query with Athena.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Push logs to a self-managed Elasticsearch/OpenSearch domain from every node to enable fast search.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Send only error logs to CloudWatch Logs and store everything else in S3.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n**Option a** is correct because it centralizes logs in a single CloudWatch Logs log group with a structured JSON format, enabling cross-service search via Logs Insights while keeping ingestion and management simple.\n\n## Why Other Options Are Wrong\n- **Option b** increases management overhead by splitting logs per service/namespace and adds complexity for cross-service queries; querying Athena across many log groups can be slower and more expensive.\n- **Option c** introduces operational overhead and cost of running a self-managed OpenSearch cluster, which is typically heavier for centralized application logs.\n- **Option d** hides the majority of logs, reducing visibility and hindering effective troubleshooting.\n\n## Key Concepts\n- Centralized logging with CloudWatch Logs\n- Logs Insights for ad-hoc queries\n- Cost considerations for log ingestion and retention\n\n## Real-World Application\n- Use a single CloudWatch Logs log group with a consistent JSON schema across services to enable fast, scalable search and correlation during incidents.","diagram":null,"difficulty":"intermediate","tags":["AWS","Amazon CloudWatch","Amazon EKS","Logs Insights","certification-mcq","domain-weight-15"],"channel":"aws-devops-pro","subChannel":"monitoring-logging","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:25:01.473Z","createdAt":"2026-01-12 22:25:01"},{"id":"aws-devops-pro-monitoring-logging-1768256701471-1","question":"You have a mix of Lambda and API Gateway endpoints and want to detect anomalies in latency with minimal alert noise. Which CloudWatch feature automatically adjusts thresholds to surface true anomalies?","answer":"[{\"id\":\"a\",\"text\":\"CloudWatch Composite Alarms\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"CloudWatch Anomaly Detection\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"CloudWatch Metrics Explorer\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"CloudWatch Alarm Actions\",\"isCorrect\":false}]","explanation":"## Correct Answer\n**Option b** is correct because CloudWatch Anomaly Detection uses ML-based dynamic thresholds to identify unusual patterns in metrics like latency, reducing noise compared to fixed thresholds.\n\n## Why Other Options Are Wrong\n- **Option a** (Composite Alarms) combines multiple alarms but does not dynamically adjust thresholds.\n- **Option c** (Metrics Explorer) is a visualization/consultation tool, not a dynamic thresholding mechanism.\n- **Option d** (Alarm Actions) governs notifications, not threshold behavior.\n\n## Key Concepts\n- Anomaly detection for dynamic thresholds\n- Latency monitoring for Lambda/API Gateway\n\n## Real-World Application\n- Apply anomaly detection to API latency metrics to alert on genuine deviations without alert fatigue.","diagram":null,"difficulty":"intermediate","tags":["AWS","CloudWatch","Lambda","API Gateway","AnomalyDetection","certification-mcq","domain-weight-15"],"channel":"aws-devops-pro","subChannel":"monitoring-logging","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:25:01.940Z","createdAt":"2026-01-12 22:25:02"},{"id":"aws-devops-pro-monitoring-logging-1768256701471-2","question":"You operate an EKS cluster and want to monitor Kubernetes workloads without instrumenting each application. Which approach best achieves this with minimal manual setup?","answer":"[{\"id\":\"a\",\"text\":\"Install Prometheus with the Prometheus Operator and scrape kubelets and pods.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable CloudWatch Container Insights for the cluster.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Install OpenTelemetry Collector as a sidecar in every namespace.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a third-party SaaS monitoring service exclusively.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n**Option b** is correct because CloudWatch Container Insights automatically collects Kubernetes and container metrics without instrumenting individual applications, offering out-of-the-box visibility.\n\n## Why Other Options Are Wrong\n- **Option a** requires deploying and maintaining Prometheus and exporters, adding operational overhead.\n- **Option c** adds per-namespace overhead and requires changes to apps to support OpenTelemetry data collection.\n- **Option d** may provide visibility but misses the AWS-native integration and potentially higher cost/less control.\n\n## Key Concepts\n- CloudWatch Container Insights\n- EKS monitoring without app instrumentation\n\n## Real-World Application\n- Enables rapid visibility across pods and nodes with minimal setup and integrates with CloudWatch dashboards.","diagram":null,"difficulty":"intermediate","tags":["AWS","Amazon EKS","CloudWatch","ContainerInsights","certification-mcq","domain-weight-15"],"channel":"aws-devops-pro","subChannel":"monitoring-logging","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:25:02.460Z","createdAt":"2026-01-12 22:25:02"},{"id":"aws-devops-pro-monitoring-logging-1768256701471-3","question":"To minimize alert fatigue in a multi-service environment, you want a single incident signal that reflects the status of multiple underlying alarms. Which CloudWatch feature provides this capability?","answer":"[{\"id\":\"a\",\"text\":\"Composite Alarms\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Metric Math\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Anomaly Detection\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"CloudWatch Synthetics\",\"isCorrect\":false}]","explanation":"## Correct Answer\n**Option a** is correct because Composite Alarms aggregate the state of multiple alarms into a single alarm state, reducing noise and providing a unified incident signal.\n\n## Why Other Options Are Wrong\n- **Option b** (Metric Math) computes derived metrics but does not consolidate alarm states.\n- **Option c** (Anomaly Detection) dynamically thresholds a single metric, not multiple alarms.\n- **Option d** (CloudWatch Synthetics) is for synthetic canaries, not alert consolidation.\n\n## Key Concepts\n- Composite alarms\n- Alert aggregation\n\n## Real-World Application\n- Use composite alarms to trigger on a critical incident when any of the underlying alarms fail, improving on-call efficiency.","diagram":null,"difficulty":"intermediate","tags":["AWS","CloudWatch","Alarms","Incident Response","certification-mcq","domain-weight-15"],"channel":"aws-devops-pro","subChannel":"monitoring-logging","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:25:02.646Z","createdAt":"2026-01-12 22:25:02"},{"id":"aws-devops-pro-monitoring-logging-1768256701471-4","question":"You need end-to-end tracing across multiple AWS services to identify latency hot spots and visualize service dependencies. Which AWS service is specifically designed for distributed tracing and service maps?","answer":"[{\"id\":\"a\",\"text\":\"AWS CloudWatch Logs\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"AWS X-Ray\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"AWS CloudTrail\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"AWS Config\",\"isCorrect\":false}]","explanation":"## Correct Answer\n**Option b** is correct because AWS X-Ray is a distributed tracing service that collects data about requests as they travel through services, providing service maps and latency analysis.\n\n## Why Other Options Are Wrong\n- **Option a** CloudWatch Logs stores logs but does not provide native distributed tracing visuals.\n- **Option c** CloudTrail records API activity, not tracing across service calls.\n- **Option d** Config tracks resource configuration changes, not traces.\n\n## Key Concepts\n- Distributed tracing\n- Service maps\n- Latency diagnostics\n\n## Real-World Application\n- Use X-Ray to trace requests across a microservices stack and identify slow components for optimization.","diagram":null,"difficulty":"intermediate","tags":["AWS","X-Ray","OpenTelemetry","CloudWatch","certification-mcq","domain-weight-15"],"channel":"aws-devops-pro","subChannel":"monitoring-logging","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:25:02.819Z","createdAt":"2026-01-12 22:25:02"},{"id":"aws-devops-pro-resilient-deployments-1768243322576-0","question":"You run a microservices app on ECS Fargate behind an Application Load Balancer. You want to deploy a new version with zero downtime and have a quick rollback option if failures occur. Which deployment approach best achieves this with minimal changes to code?","answer":"[{\"id\":\"a\",\"text\":\"In-place updates using ECS rolling updates with minHealthyPercent and desiredCount\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Blue/Green deployment using AWS CodeDeploy integration with ECS\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Update a single ECS service gradually with minHealthyPercent to 100% and wait for manual checks\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Replace tasks manually in the cluster behind the load balancer\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. Blue/Green deployments via CodeDeploy provide two parallel environments and allow traffic shifting, enabling zero-downtime deploys and quick rollback by redirecting traffic back to the previous version.\n\n## Why Other Options Are Wrong\n- A: In-place rolling updates can still cause brief downtime and rollback is more complex if failures occur after traffic is shifted.\n- C: Updating a single ECS service gradually may minimize downtime but lacks isolated staging/rollback environments.\n- D: Manual replacement is slow and error-prone during production P0/P1 incidents.\n\n## Key Concepts\n- Blue/Green deployments\n- ECS + CodeDeploy integration\n\n## Real-World Application\n- Use Blue/Green to deploy critical services with clear rollback paths and controlled traffic switching.","diagram":null,"difficulty":"intermediate","tags":["AWS","ECS","CodeDeploy","BlueGreen","HA","certification-mcq","domain-weight-15"],"channel":"aws-devops-pro","subChannel":"resilient-deployments","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:42:02.577Z","createdAt":"2026-01-12 18:42:03"},{"id":"aws-devops-pro-resilient-deployments-1768243322576-1","question":"In a multi-branch CI/CD pipeline using Terraform to manage AWS infrastructure, which backend setup best prevents concurrent state writes and ensures reproducible plans?","answer":"[{\"id\":\"a\",\"text\":\"Local backend with state file checked into VCS\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"S3 backend with DynamoDB table for state locking\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"EBS volume mounted to the build agent\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"RDS as the state store\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. S3 backend with a DynamoDB table provides state locking, preventing concurrent writes and enabling remote state storage for CI/CD pipelines. Local state is not safe for collaboration; EBS-based local workspaces and RDS as a store are not valid Terraform backends.\n\n## Why Other Options Are Wrong\n- A: Local backends don't coordinate across parallel runs.\n- C: EBS is not a Terraform backend and cannot provide distributed locking.\n- D: RDS is not a Terraform backend and does not support Terraform state management.\n\n## Key Concepts\n- Terraform backends\n- Remote state with S3\n- State locking with DynamoDB\n\n## Real-World Application\n- Ensures reproducible plans and safe concurrent runs across multiple CI jobs.","diagram":null,"difficulty":"intermediate","tags":["Terraform","S3","DynamoDB","Backend","StateManagement","AWS","certification-mcq","domain-weight-15"],"channel":"aws-devops-pro","subChannel":"resilient-deployments","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:42:03.129Z","createdAt":"2026-01-12 18:42:03"},{"id":"aws-devops-pro-resilient-deployments-1768243322576-2","question":"An application uses Amazon RDS in a Multi-AZ configuration. A failure occurs in the primary AZ. What is the expected failover behavior and typical recovery time?","answer":"[{\"id\":\"a\",\"text\":\"No automatic failover; you must promote a read replica\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Failover to the synchronous standby in another AZ happens automatically; recovery typically within 60–180 seconds\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Automatically switches to a cross-region standby without downtime\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Instance must be restarted manually\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. In a Multi-AZ RDS deployment, a failover to the synchronous standby in a different AZ occurs automatically, typically within 60–180 seconds, with minimal disruption. This is designed for high availability and rapid recovery. For cross-region DR, consider Aurora or cross-region replicas.\n\n## Why Other Options Are Wrong\n- A: Automatic failover exists in Multi-AZ; manual promotion is not required.\n- C: Cross-region failover is not automatic in standard Multi-AZ and incurs additional latency.\n- D: Failover does not require manual restart; it's automatic.\n\n## Key Concepts\n- RDS Multi-AZ HA\n- Automatic failover\n- Recovery time expectations\n\n## Real-World Application\n- Plan for rapid recovery in production DB using Multi-AZ and consider additional cross-region DR for regional outages.","diagram":null,"difficulty":"intermediate","tags":["AWS","RDS","Multi-AZ","HA","Failover","Database","certification-mcq","domain-weight-15"],"channel":"aws-devops-pro","subChannel":"resilient-deployments","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:42:03.582Z","createdAt":"2026-01-12 18:42:03"},{"id":"aws-devops-pro-resilient-deployments-1768243322576-3","question":"In an EKS cluster, during node maintenance you want to ensure that at least 80% of pods in a deployment remain available. Which Kubernetes construct enforces this policy?","answer":"[{\"id\":\"a\",\"text\":\"PodDisruptionBudget with minAvailable set to 8 for a 10-pod deployment\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Deployment strategy with maxUnavailable set to 2\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"ReadinessProbe alone ensures ongoing availability during maintenance\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"LivenessProbe alone prevents downtimes during maintenance\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. PodDisruptionBudget (PDB) can enforce minimum available pods during voluntary disruptions, such as node maintenance. For an 80% availability target on a 10-pod deployment, a PDB with minAvailable: 8 achieves this.\n\n## Why Other Options Are Wrong\n- B: Deployment maxUnavailable controls during updates but not during general voluntary disruptions like node maintenance.\n- C: Readiness probes help determine pod readiness but don’t enforce minimum availability across disruptions.\n- D: Liveness probes restart unhealthy pods but don't enforce minimum availability.\n\n## Key Concepts\n- PodDisruptionBudget (PDB)\n- High availability during disruptions\n\n## Real-World Application\n- Use PDBs to protect critical deployments during maintenance windows and autoscaling.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","EKS","PDB","HA","Deployment","certification-mcq","domain-weight-15"],"channel":"aws-devops-pro","subChannel":"resilient-deployments","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:42:03.775Z","createdAt":"2026-01-12 18:42:03"},{"id":"aws-devops-pro-resilient-deployments-1768243322576-4","question":"You have a CloudFront distribution serving a static site with two S3 origins in different regions: a primary and a secondary for failover. You want automatic failover if the primary origin becomes unhealthy. Which CloudFront feature enables this?","answer":"[{\"id\":\"a\",\"text\":\"Origin Groups with Failover\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Route 53 DNS failover\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"CloudFront signed URLs\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Global Accelerator\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Origin Groups with Failover allows CloudFront to automatically switch to a secondary origin when the primary origin is unhealthy, providing automatic failover for high availability.\n\n## Why Other Options Are Wrong\n- B: DNS failover can provide regional failover but not as fast as origin failover within CloudFront; this is a different mechanism.\n- C: Signed URLs control access, not failover behavior.\n- D: Global Accelerator is a different service for accelerating traffic but isn’t the native CloudFront origin failover mechanism.\n\n## Key Concepts\n- CloudFront origin groups\n- Automatic origin failover\n\n## Real-World Application\n- Use origin groups for resilient static sites serving from multi-region S3 buckets.","diagram":null,"difficulty":"intermediate","tags":["CloudFront","OriginFailover","WAF","Shield","AWS","certification-mcq","domain-weight-15"],"channel":"aws-devops-pro","subChannel":"resilient-deployments","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:42:03.938Z","createdAt":"2026-01-12 18:42:04"},{"id":"aws-devops-pro-sdlc-automation-1768181394902-0","question":"You have a microservices app deployed to Amazon ECS Fargate behind an Application Load Balancer. Your CI/CD pipeline uses CodePipeline with CodeBuild and a separate CodeDeploy stage to deploy service revisions. To minimize downtime during a deployment, which deployment strategy should you adopt?","answer":"[{\"id\":\"a\",\"text\":\"Use ECS rolling updates with CodePipeline deploy stage\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use CodeDeploy blue/green deployments for ECS\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Perform an in-place update and swap target groups manually\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Update the task definition and redeploy without traffic shifting\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct: CodeDeploy blue/green deployments for ECS enable traffic shifting between the original and new task sets, allowing instant rollback and minimal downtime. \n\n## Why Other Options Are Wrong\n- Option A: ECS rolling updates may still cause brief downtime if not tuned for min healthy percent and desired count; it doesn’t guarantee seamless traffic shift like blue/green.\n- Option C: Manual traffic switching is error-prone and adds operational overhead, defeating automation goals.\n- Option D: Updating the task definition in place without traffic shifting can cause service interruptions during new task deployment.\n\n## Key Concepts\n- ECS blue/green deployments\n- Traffic shifting and minimal-downtime releases\n- CodePipeline integration with CodeDeploy for ECS\n\n## Real-World Application\n- Configure CodeDeploy deployment group for your ECS service.\n- Set up a load balancer listener rule to shift traffic from old to new task sets gradually.\n- Implement rollback triggers to revert if metrics exceed thresholds.\n","diagram":null,"difficulty":"intermediate","tags":["AWS CodePipeline","AWS CodeDeploy","Amazon ECS","Fargate","certification-mcq","domain-weight-22"],"channel":"aws-devops-pro","subChannel":"sdlc-automation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:29:54.904Z","createdAt":"2026-01-12 01:29:55"},{"id":"aws-devops-pro-sdlc-automation-1768181394902-1","question":"You are building a multi-account CI/CD pipeline that deploys resources via Terraform. You want to ensure secrets are not exposed in code or logs and are rotated securely. Which approach provides the best balance of security and automation?","answer":"[{\"id\":\"a\",\"text\":\"Store secrets in AWS Secrets Manager and fetch them at runtime in the pipeline with an IAM role\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Commit secrets to the repository as encrypted files with the key checked in alongside code\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Pass secrets as plain environment variables in the build project\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Store secrets in an S3 bucket with public read access and fetch them via URL\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct: AWS Secrets Manager (or Secrets Manager-backed retrieval) with an IAM role scoped to the pipeline enables automatic rotation and secure, runtime access without embedding secrets in code or logs. \n\n## Why Other Options Are Wrong\n- Option B: Storing secrets in the repository—even if encrypted—risks exposure if encryption is compromised or keys are mishandled.\n- Option C: Plaintext environment variables risk leakage in logs and process listings; they also don’t support automatic rotation.\n- Option D: Public S3 access exposes secrets to anyone and violates least-privilege security practices.\n\n## Key Concepts\n- Secrets management in CI/CD\n- Rotation and least-privilege access with IAM roles\n- Runtime retrieval of secrets during builds\n\n## Real-World Application\n- Create Secrets Manager entries for each secret and map them to CodeBuild/CodePipeline tasks via IAM roles.\n- Enable automatic rotation and audit access via CloudTrail.\n- Remove any secret references from buildspec and logs.\n","diagram":null,"difficulty":"intermediate","tags":["AWS Secrets Manager","AWS CodeBuild","IAM","CI/CD","certification-mcq","domain-weight-22"],"channel":"aws-devops-pro","subChannel":"sdlc-automation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:29:55.390Z","createdAt":"2026-01-12 01:29:55"},{"id":"aws-devops-pro-sdlc-automation-1768181394902-2","question":"In a Terraform-based IaC workflow used across multiple environments, you want to enforce policy compliance before applying changes and catch drift automatically. Which approach best achieves this in a scalable, collaborative setup?","answer":"[{\"id\":\"a\",\"text\":\"Use Terraform Cloud with Sentinel policies to gate apply plans\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Run terraform apply directly in CI without plan or policy checks\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Migrate to CloudFormation to avoid Terraform policy concerns\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely on manual approvals in the pipeline without automated policy checks\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A is correct: Terraform Cloud (or Terraform Enterprise) with Sentinel enables policy as code, enforcing compliance before plans are applied and enabling drift detection across teams. \n\n## Why Other Options Are Wrong\n- Option B: Skipping policy checks undermines governance and increases risk of non-compliant deployments.\n- Option C: CloudFormation is a different IaC tool; migrating does not address Terraform-specific policy needs and may introduce workflow disruption.\n- Option D: Manual approvals alone lack automated policy enforcement and drift detection, reducing scalability.\n\n## Key Concepts\n- Policy as code with Sentinel\n- Gatekeeping plan/apply in Terraform workflows\n- Drift detection and multi-account governance\n\n## Real-World Application\n- Integrate Terraform Cloud/Enterprise with your VCS; define policies (e.g., tagging, encryption, no public access).\n- Require policy-compliant plans before apply; implement drift checks as part of the pipeline.\n- Audit policy hits and provide actionable remediation guidance to teams.\n","diagram":null,"difficulty":"intermediate","tags":["Terraform","Terraform Cloud","Sentinel","IaC","CI/CD","certification-mcq","domain-weight-22"],"channel":"aws-devops-pro","subChannel":"sdlc-automation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T01:29:55.857Z","createdAt":"2026-01-12 01:29:55"},{"id":"aws-devops-pro-sdlc-automation-1768293007245-0","question":"Scenario: Your organization hosts a SaaS platform across multiple AWS accounts (dev, test, prod). You use GitHub for source control, Terraform for IaC, and CodePipeline for CI/CD. You want to ensure each account maintains isolated Terraform state and that plan/apply operations run with least-privilege roles. Which approach best achieves proper state isolation and secure deployments?","answer":"[{\"id\":\"a\",\"text\":\"Create a separate CodePipeline and CodeBuild per target account, each running Terraform with a role in that account and storing remote state in an account-specific S3 bucket using DynamoDB for locking.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a single central pipeline that runs Terraform against all accounts using a single service account and a shared remote state bucket in the management account.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store all Terraform state in a single S3 bucket in the management account and assume a cross-account role during apply, with no per-account locking.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Replace Terraform with CloudFormation stacks deployed via CodePipeline.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption A (per-target account pipelines with per-account remote state) is correct because it isolates state per account, uses dedicated roles with least privilege, and avoids cross-account state sharing; this aligns with best practices for multi-account deployments.\n\n## Why Other Options Are Wrong\n- Option B is incorrect because a single central pipeline across accounts increases blast radius and complicates credentials management, violating least privilege and state isolation.\n- Option C is incorrect because a single shared state bucket across accounts can lead to cross-account access and race conditions, compromising isolation.\n- Option D is incorrect because CloudFormation stacks cannot natively manage Terraform state or support per-account/multi-account Terraform workflows without additional tooling.\n\n## Key Concepts\n- Terraform remote state per environment\n- DynamoDB state locking\n- Cross-account IAM roles and assume-role patterns\n- CI/CD orchestration across AWS accounts\n\n## Real-World Application\nThis pattern enables strict account isolation, safer state management, and scalable deployment automation across multiple AWS accounts in production environments.","diagram":null,"difficulty":"intermediate","tags":["Terraform","AWS","CI/CD","Multi-Account","certification-mcq","domain-weight-22"],"channel":"aws-devops-pro","subChannel":"sdlc-automation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:30:07.246Z","createdAt":"2026-01-13 08:30:07"},{"id":"aws-devops-pro-sdlc-automation-1768293007245-1","question":"Scenario: You deploy a microservice to an EKS cluster managed by a GitOps workflow using Argo CD. You need canary deployments with traffic shifting based on metrics and automatic rollback if the service health degrades. Which pattern best implements this?","answer":"[{\"id\":\"a\",\"text\":\"Use a simple Kubernetes Deployment with a basic readinessProbe and rely on the Ingress rule to split traffic.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Argo Rollouts with AnalysisRuns to progressively shift traffic and automatically rollback if metrics degrade.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use CodePipeline to apply manifests in a GitOps fashion with manual approvals before any traffic shift.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a separate CloudFormation pipeline to manage canary via traffic splitting.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. Argo Rollouts with AnalysisRuns gating traffic based on metrics and performing automatic rollbacks when health degrades.\n\n## Why Other Options Are Wrong\n- A is wrong because a basic Deployment with Ingress does not provide canary deployments or metrics-driven rollback.\n- C is wrong because CodePipeline alone is not designed for progressive delivery patterns within Kubernetes and lacks in-cluster analysis-driven rollback.\n- D is wrong because CloudFormation-based deployment cannot express in-cluster progressive delivery as cleanly as Argo Rollouts.\n\n## Key Concepts\n- Canary deployments in Kubernetes\n- Argo Rollouts and AnalysisRuns\n- Metrics-based progressive delivery and automatic rollback\n\n## Real-World Application\nTeams can safely roll new versions by gradually increasing traffic to the canary, verifying service metrics, and automatically rolling back if predefined SLOs are violated.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","EKS","GitOps","Argo Rollouts","certification-mcq","domain-weight-22"],"channel":"aws-devops-pro","subChannel":"sdlc-automation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:30:07.727Z","createdAt":"2026-01-13 08:30:08"},{"id":"aws-devops-pro-sdlc-automation-1768293007245-2","question":"Scenario: Your Terraform code uses separate workspaces or environments (dev, stage, prod). A CI pipeline attempts to apply all environments in parallel and sometimes fails due to state locking. What approach best resolves this issue while preserving isolation and consistency?","answer":"[{\"id\":\"a\",\"text\":\"Configure a separate remote backend per environment (S3 + DynamoDB) and orchestrate environment deployments so that applies are serialized per environment to avoid state lock contention.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Disable state locking in Terraform to allow parallel applies.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store state locally on CI runners.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Terraform workspace feature to apply squads in parallel.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Use a separate remote backend per environment (S3 + DynamoDB) and orchestrate environment deployments so that applies are serialized per environment to avoid state lock contention.\n\n## Why Other Options Are Wrong\n- B is wrong because disabling state locking undermines state integrity and can cause corruption.\n- C is wrong because local state is not shared across runners and breaks reproducibility.\n- D is wrong because workspace isolation alone does not guarantee safe parallel applies across multiple environments.\n\n## Key Concepts\n- Terraform remote state backends\n- DynamoDB as a state lock\n- Environment isolation in CI/CD\n\n## Real-World Application\nAdopting per-environment backends ensures safe, auditable deployments with proper locking and isolation across dev, stage, and prod pipelines.","diagram":null,"difficulty":"intermediate","tags":["Terraform","State Management","Backends","CI/CD","certification-mcq","domain-weight-22"],"channel":"aws-devops-pro","subChannel":"sdlc-automation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:30:08.234Z","createdAt":"2026-01-13 08:30:08"},{"id":"aws-devops-pro-sdlc-automation-1768293007245-3","question":"Scenario: You want to integrate image vulnerability scanning into your CI/CD pipeline for containerized workloads on AWS. You need to surface findings but avoid blocking deployments by default, while still enabling a gated release for high-severity issues. Which approach is best?","answer":"[{\"id\":\"a\",\"text\":\"Fail the build immediately if vulnerabilities are detected by a container scanner integrated in CI.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable ECR image scanning on push and treat findings as information, with a separate security gate (manual review) to block production deployments only when high-severity findings exist.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rely solely on static code analysis and skip image scanning entirely.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a third-party scanner outside CI and manually review results after deployment.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. Enable ECR image scanning on push and treat findings as information, with a separate security gate (manual review) to block production deployments only when high-severity findings exist.\n\n## Why Other Options Are Wrong\n- A would block velocity by failing builds on any vulnerability.\n- C ignores image scanning, missing proactive risk visibility.\n- D delays gating to post-deployment and relies on manual steps outside the CI flow.\n\n## Key Concepts\n- ECR image scanning on push\n- Non-blocking security gates\n- Separate governance checks in CI/CD\n\n## Real-World Application\nTeams surface vulnerabilities early but keep deployment flow fast, while maintaining a governance gate for high-risk findings before production.","diagram":null,"difficulty":"intermediate","tags":["Container Security","ECR","CI/CD","Vulnerability Scanning","certification-mcq","domain-weight-22"],"channel":"aws-devops-pro","subChannel":"sdlc-automation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:30:08.402Z","createdAt":"2026-01-13 08:30:08"},{"id":"aws-devops-pro-sdlc-automation-1768293007245-4","question":"Scenario: You manage a Kubernetes-based production platform on AWS EKS using GitOps. Any production changes must pass a PR-based governance review before Argo CD applies them. Which pattern ensures auditable, governance-driven deployments?","answer":"[{\"id\":\"a\",\"text\":\"Enable automated SyncPolicy in Argo CD so changes deploy immediately after PR merge.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Configure Argo CD with Manual Sync (Sync Policy Manual) and enforce PR-based approvals, using pre-sync or post-sync hooks to validate changes before deployments.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Replace Argo CD with CloudFormation to apply manifests.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a separate CI tool to push manifests to S3 and have Argo CD poll for changes.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. Configure Argo CD with Manual Sync (Sync Policy Manual) and enforce PR-based approvals, using pre-sync or post-sync hooks to validate changes before deployments.\n\n## Why Other Options Are Wrong\n- A would bypass governance by auto-syncing after PRs, removing review control.\n- C abandons GitOps principles in favor of CloudFormation, losing in-cluster Git-driven deployment benefits.\n- D introduces an unnecessary detour and does not guarantee governance in the Argo CD flow.\n\n## Key Concepts\n- GitOps with Argo CD\n- SyncPolicy Manual\n- PR-based governance and hooks\n\n## Real-World Application\nThis pattern provides auditable change control for production, ensuring approvals are in place before any deployment, while keeping the GitOps loop intact for traceability.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","ArgoCD","GitOps","Governance","certification-mcq","domain-weight-22"],"channel":"aws-devops-pro","subChannel":"sdlc-automation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:30:08.580Z","createdAt":"2026-01-13 08:30:08"},{"id":"aws-devops-pro-security-compliance-1768228273614-0","question":"An AWS Organization has an OU with an SCP that restricts most IAM actions by default. A CI/CD pipeline in a member account needs to create EC2 instances, S3 buckets, and IAM roles for deployments. Which approach best achieves least privilege while allowing the pipeline to perform only the required actions?","answer":"[{\"id\":\"a\",\"text\":\"Attach a dedicated pipeline IAM role with precisely scoped permissions and apply a permission boundary to cap allowed actions.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Temporarily remove the SCP restrictions so the pipeline can operate.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Grant the pipeline role full admin privileges and monitor using audits.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Move the pipeline to a separate, less-restrictive OU.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- A. Attach a dedicated pipeline IAM role with precisely scoped permissions and apply a permission boundary to cap allowed actions.\n\n## Why Other Options Are Wrong\n- B: Removing SCP restrictions defeats the purpose of centralized governance and undermines least privilege.\n- C: Full admin privileges contradict the principle of least privilege and increase risk.\n- D: Moving the pipeline to another OU does not resolve the governance constraints and reduces centralized control.\n\n## Key Concepts\n- IAM permission boundaries\n- Service Control Policies (SCPs)\n- Least privilege principle in multi-account environments\n\n## Real-World Application\n- Use this pattern for CI/CD pipelines that deploy across accounts while keeping governance intact and minimizing blast radius.","diagram":null,"difficulty":"intermediate","tags":["AWS","IAM","SCP","Organizations","certification-mcq","domain-weight-17"],"channel":"aws-devops-pro","subChannel":"security-compliance","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:31:13.615Z","createdAt":"2026-01-12 14:31:14"},{"id":"aws-devops-pro-security-compliance-1768228273614-1","question":"To enforce encryption at rest across multi-account S3 buckets and EBS volumes and ensure CMK rotation, which approach provides ongoing enforcement and visibility?","answer":"[{\"id\":\"a\",\"text\":\"Enable AWS Config with managed rules for kms-key-rotation-enabled and s3-bucket-encrypted-with-cmk and configure remediation.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely on periodic manual audits of encryption settings.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use bucket policies to require SSE-KMS but do not enforce rotation.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use SCPs to prevent use of SSE-S3.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- A. Enable AWS Config with managed rules kms-key-rotation-enabled and s3-bucket-encrypted-with-cmk and configure remediation.\n\n## Why Other Options Are Wrong\n- B: Manual audits are not proactive or scalable for multi-account environments.\n- C: Requiring SSE-KMS in policies enforces encryption but does not guarantee rotation of CMKs across accounts.\n- D: SCPs cannot reliably enforce encryption mode and rotation at scale.\n\n## Key Concepts\n- AWS Config managed rules (kms-key-rotation-enabled, s3-bucket-encrypted-with-cmk)\n- Centralized compliance and remediation\n- CMK rotation governance\n\n## Real-World Application\n- Ideal for org-wide encryption governance with automated detection and remediation.","diagram":null,"difficulty":"intermediate","tags":["AWS","KMS","S3","Config","Compliance","certification-mcq","domain-weight-17"],"channel":"aws-devops-pro","subChannel":"security-compliance","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:31:14.397Z","createdAt":"2026-01-12 14:31:14"},{"id":"aws-devops-pro-security-compliance-1768228273614-2","question":"A DevOps pipeline stores credentials in AWS Secrets Manager and must rotate credentials automatically for a Postgres RDS instance every 30 days. Which option ensures automatic rotation with minimal downtime?","answer":"[{\"id\":\"a\",\"text\":\"Enable Secrets Manager automatic rotation for the secret using the provided rotation function for RDS.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rotate credentials manually in Secrets Manager every 30 days.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store credentials in SSM Parameter Store and rotate on a schedule.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Embed credentials in the application code and rotate manually.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- A. Enable Secrets Manager automatic rotation for the secret using the provided rotation function for RDS.\n\n## Why Other Options Are Wrong\n- B: Manual rotation introduces delay and human error risks.\n- C: SSM Parameter Store rotation is not natively connected to RDS rotation workflows.\n- D: Embedding credentials in code is insecure and brittle.\n\n## Key Concepts\n- Secrets Manager rotation templates for RDS\n- Automated credential rotation\n- Minimal downtime during rotation\n\n## Real-World Application\n- Maintains database access credentials securely with predictable rotation cycles, reducing exposure.","diagram":null,"difficulty":"intermediate","tags":["AWS","SecretsManager","RDS","CI/CD","certification-mcq","domain-weight-17"],"channel":"aws-devops-pro","subChannel":"security-compliance","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:31:14.914Z","createdAt":"2026-01-12 14:31:14"},{"id":"aws-devops-pro-security-compliance-1768228273614-3","question":"In an EKS cluster, you need a pod to access a specific S3 bucket with least privilege. Which configuration achieves this?","answer":"[{\"id\":\"a\",\"text\":\"Create an IAM OIDC provider for the cluster, bind an IAM role with a policy scoped to the bucket to a Kubernetes ServiceAccount used by the pod.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Attach the bucket policy to all pods in the cluster.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store credentials in Kubernetes secrets and mount as environment variables.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Grant the node instance role broad S3 access.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- A. Create an IAM OIDC provider for the cluster, bind an IAM role with a policy scoped to the bucket to a Kubernetes ServiceAccount used by the pod.\n\n## Why Other Options Are Wrong\n- B: Bucket policies applied to all pods are overly permissive and violate least privilege.\n- C: Storing credentials in Kubernetes secrets and env vars is insecure and hard to rotate.\n- D: Granting the node instance role broad S3 access gives excessive privileges to all pods on the node.\n\n## Key Concepts\n- IRSA (IAM Roles for Service Accounts)\n- OIDC federation for EKS\n- Least privilege for pod access\n\n## Real-World Application\n- Enables fine-grained AWS permissions for specific workloads without leaking credentials.","diagram":null,"difficulty":"intermediate","tags":["Kubernetes","EKS","IRSA","IAM","certification-mcq","domain-weight-17"],"channel":"aws-devops-pro","subChannel":"security-compliance","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:31:15.076Z","createdAt":"2026-01-12 14:31:15"},{"id":"aws-devops-pro-security-compliance-1768228273614-4","question":"You want to detect and prevent unencrypted data in S3 across an organization. Which approach provides continuous visibility and automatic remediation?","answer":"[{\"id\":\"a\",\"text\":\"Enable AWS Config with managed rules s3-bucket-server-side-encryption-enabled and kms-key-rotation-enabled, with automated remediation.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Rely on CloudTrail daily audits of bucket configurations.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enforce encryption using IAM policies on users.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Enable S3 access logs and inspect them to identify non-encrypted buckets.\",\"isCorrect\":false}]","explanation":"## Correct Answer\n- A. Enable AWS Config with managed rules s3-bucket-server-side-encryption-enabled and kms-key-rotation-enabled, with automated remediation.\n\n## Why Other Options Are Wrong\n- B: CloudTrail audits alone do not provide continuous enforcement or remediation.\n- C: IAM policies cannot reliably enforce encryption at rest across buckets; policies don’t automatically remediate non-compliant resources.\n- D: Access logs reveal activity but do not prevent or remediate non-encrypted data.\n\n## Key Concepts\n- AWS Config managed rules for S3 encryption\n- Automated remediation in Config\n- Continuous enforcement and visibility\n\n## Real-World Application\n- Ensures org-wide data protection by automatically identifying and remediating non-encrypted S3 buckets.","diagram":null,"difficulty":"intermediate","tags":["AWS","Config","S3","Compliance","certification-mcq","domain-weight-17"],"channel":"aws-devops-pro","subChannel":"security-compliance","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:31:15.240Z","createdAt":"2026-01-12 14:31:15"}],"subChannels":["config-management","general","incident-response","monitoring-logging","resilient-deployments","sdlc-automation","security-compliance"],"companies":["Anthropic","Apple","Citadel","Cloudflare","Coinbase","Databricks","Discord","Google","Microsoft","MongoDB","Square","Uber"],"stats":{"total":36,"beginner":2,"intermediate":33,"advanced":1,"newThisWeek":36}}