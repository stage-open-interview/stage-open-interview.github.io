{"questions":[{"id":"aws-database-specialty-workload-requirements-1768148665414-0","question":"An ecommerce retailer runs an OLTP workload on Amazon RDS for PostgreSQL. During business hours, analytics queries run on the same database, causing CPU spikes and higher latency for transactions. What is the most appropriate architecture to allow analytics without impacting OLTP performance?","answer":"[{\"id\":\"a\",\"text\":\"Create RDS Read Replicas and route analytics queries to the replicas\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable Multi-AZ with synchronous replication to the primary for better write durability\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Move analytics to a separate Amazon Redshift cluster and keep OLTP unchanged\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Enable Amazon Aurora Global Database with cross-region writes to support analytics\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct choice is a) Create RDS Read Replicas and route analytics queries to the replicas. This offloads read-heavy analytics to replicas, reducing contention on the primary and preserving OLTP latency.\n\n## Why Other Options Are Wrong\n- b) While Multi-AZ improves availability, it does not offload analytics or reduce OLTP contention, so it doesn't address the primary issue.\n- c) Moving analytics to Redshift is viable but involves ETL and separate data stores; it adds latency and complexity for integrated OLTP analytics and may not preserve transactional latency.\n- d) Aurora Global Database focuses on cross-region disaster recovery and global writes; it isn't the most suitable path to offload analytics from a single primary region.\n\n## Key Concepts\n- OLTP vs OLAP separation\n- Read replicas for read-heavy workloads\n- Asynchronous replication latency considerations\n\n## Real-World Application\n- Implement read replicas for analytics workloads and direct BI queries to replicas; monitor replication lag and ensure ETL pipelines keep data synced to reporting schemas.","diagram":null,"difficulty":"intermediate","tags":["RDS","Aurora","Redshift","OLTP","OLAP","AWS","certification-mcq","domain-weight-26"],"channel":"aws-database-specialty","subChannel":"workload-requirements","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:24:25.415Z","createdAt":"2026-01-11 16:24:25"},{"id":"aws-database-specialty-workload-requirements-1768148665414-1","question":"Which AWS database option best supports a global, low-latency relational workload with cross-region disaster recovery and minimal downtime?","answer":"[{\"id\":\"a\",\"text\":\"DynamoDB Global Tables\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"RDS with cross-region read replicas\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Amazon Aurora Global Database\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Redshift\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct choice is c) Amazon Aurora Global Database. It provides a relational, ACID-compliant engine with a primary region and read-only replicas in other regions for low-latency reads and DR support across regions. It minimizes downtime by enabling rapid disaster recovery and continuity.\n\n## Why Other Options Are Wrong\n- a) DynamoDB Global Tables are multi-region and fast for non-relational workloads but not ideal for traditional relational OLTP schemas.\n- b) RDS with cross-region read replicas can provide some DR but lacks the seamless global read-localization and scale of Aurora Global Database.\n- d) Redshift is analytic-oriented and not suitable for OLTP workload requirements.\n\n## Key Concepts\n- Aurora Global Database for cross-region DR\n- Relational OLTP with global reads\n- Recovery time objectives in multi-region setups\n\n## Real-World Application\n- Use Aurora Global Database to serve global customers with near-local reads; plan failover strategies and DR testing across regions.","diagram":null,"difficulty":"intermediate","tags":["Aurora","RDS","DynamoDB","Global Tables","DR","AWS","certification-mcq","domain-weight-26"],"channel":"aws-database-specialty","subChannel":"workload-requirements","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:24:25.761Z","createdAt":"2026-01-11 16:24:26"},{"id":"aws-database-specialty-workload-requirements-1768148665414-2","question":"A DynamoDB table stores user session events with partition key userId and sort key eventTime. A small subset of users generate events at very high rates, creating hot partitions that throttle throughput. Which design change best distributes load and avoids hot partitions?","answer":"[{\"id\":\"a\",\"text\":\"Add a random prefix (shard) to the partition key to distribute traffic across partitions\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Increase provisioned throughput on the existing partition key to handle bursts\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Migrate to a relational database to manage hot partitions more effectively\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Introduce DynamoDB Streams to throttle write throughput\",\"isCorrect\":false}]","explanation":"## Correct Answer\na) Add a random prefix (shard) to the partition key to distribute traffic across partitions. This mitigates hot partitions by spreading writes across multiple partition keys.\n\n## Why Other Options Are Wrong\n- b) Simply increasing throughput on the same partition key does not solve uneven distribution and keeps the hot-partition risk.\n- c) Migrating to a relational database does not inherently fix partition-level throttling in DynamoDB and adds unnecessary complexity.\n- d) DynamoDB Streams does not throttle writes and is intended for change data capture, not load distribution.\n\n## Key Concepts\n- DynamoDB partition keys and hot partitions\n- Horizontal sharding strategies\n- Throughput management in DynamoDB\n\n## Real-World Application\n- Implement sharding at the partition key layer when encountering hot keys; monitor partition-level utilization and adjust shard count as needed.","diagram":null,"difficulty":"intermediate","tags":["DynamoDB","PartitionKey","Sharding","Throughput","AWS","certification-mcq","domain-weight-26"],"channel":"aws-database-specialty","subChannel":"workload-requirements","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:24:26.103Z","createdAt":"2026-01-11 16:24:26"}],"subChannels":["workload-requirements"],"companies":[],"stats":{"total":3,"beginner":0,"intermediate":3,"advanced":0,"newThisWeek":3}}