{"questions":[{"id":"q-1074","question":"Scenario: A global time-series platform ingests 1M events/hour in us-west-2; dashboards in eu-central-1 and ap-southeast-2 need sub-200ms reads on the latest window. Data must be immutable for 90 days for compliance. Compare DynamoDB Global Tables with DAX vs Aurora PostgreSQL Global Database with cross-region backups. Provide topology, replication, PITR/backup plans, and RPO/RTO targets?","answer":"Choose DynamoDB Global Tables in three regions (us-west-2, eu-central-1, ap-southeast-2) with DAX caching per region and multi-region writes. Enable PITR for 35 days and S3 immutable archives for 90 d","explanation":"## Why This Is Asked\n\nTests cross-region replication, latency trade-offs, and DR design between NoSQL and relational engines.\n\n## Key Concepts\n- Global Tables vs Global Database replication\n- Read latency and consistency models\n- PITR and cross-region backups\n- Archival and compliance (S3 Object Lock)\n\n## Code Example\n\n```javascript\n// Example: pseudo-endpoint selection logic for region failover\n```\n\n## Follow-up Questions\n- How would you validate RPO/RTO in a simulated outage?\n- What monitoring would you implement to detect replication lag across regions?","diagram":"flowchart TD\nA[Ingest] --> B[Global Tables: us-west-2, eu-central-1, ap-southeast-2]\nB --> C[DAX per region]\nA --> D[S3 immutable archive (90d)]\nC --> E[Dashboards in region]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T21:33:34.955Z","createdAt":"2026-01-12T21:33:34.955Z"},{"id":"q-1131","question":"**Hybrid Analytics Path for Multiregion Aurora**\n\nYou're running an Aurora PostgreSQL OLTP cluster with tenant isolation via RLS in us-east-1. A regulatory BI team in eu-west-1 requires near real-time analytics with masked PII. Design a hybrid analytics path using Aurora Global Database for OLTP replicas and a CDC-based analytic store (Redshift or DynamoDB+Lambda) in eu-west-1. Describe data flow, masking strategy, encryption, failover, and how to meet RPO 5s and RTO 60s, including cost considerations?","answer":"Run OLTP in Aurora PostgreSQL with RLS isolation in us-east-1. Replicate via Aurora Global Database to eu-west-1. Ingest CDC to a masked analytics store (Redshift or DynamoDB) in eu-west-1; apply per-","explanation":"## Why This Is Asked\nTests cross-region replication, hybrid OLTP/OLAP, security, and DR.\n\n## Key Concepts\n- Aurora Global Database, RLS, CDC, masking, cross-region KMS, hot failover\n- DR: RPO 5s, RTO 60s\n- Cost considerations: hot standby vs on-demand\n\n## Code Example\n```javascript\n// AWS CLI example (conceptual)\naws dms create-replication-task --replication-task-identifier cbd-task --source-endpoint-arn <src> --target-endpoint-arn <tgt> --migration-type full-load-and-cdc\n```\n\n## Follow-up Questions\n- How would you validate masking correctness without exposing PII?\n- What metrics indicate replication lag violations and how to remediate?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Goldman Sachs","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T01:22:46.568Z","createdAt":"2026-01-13T01:22:46.568Z"},{"id":"q-1279","question":"In a multi-tenant SaaS on AWS, run a single Aurora PostgreSQL cluster with per-tenant schemas and RLS to isolate data. An analytics team in eu-west-1 requires cross-tenant BI with masked PII in near real-time dashboards. Design a cost-aware architecture that delivers masking, auditing, and SLA, comparing per-tenant schemas in a single cluster vs separate clusters per tenant. Include data flow, backup, and failover?","answer":"Adopt a hybrid: maintain one Aurora PostgreSQL cluster with per-tenant schemas and RLS for isolation; expose masked analytic views for BI from a dedicated eu-west-1 read replica. Use a CDC pipeline to","explanation":"## Why This Is Asked\nThis question probes practical multi-tenant isolation, cross-region analytics, and governance trade-offs in AWS databases.\n\n## Key Concepts\n- Row-level security and per-tenant schemas\n- Cross-region BI with masked analytics\n- CDC pipelines to analytics stores\n- Audit, backup (PITR), and cost governance\n- Single-cluster vs multi-cluster trade-offs\n\n## Code Example\n```sql\n-- Enable RLS on a tenant table\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_rls ON orders\n  USING (tenant_id = current_setting('my.tenant_id')::int);\n```\n\n## Follow-up Questions\n- How would you implement masking for PII in the analytics store without leaking through cached results?\n- How would you monitor latency, replication lag, and cost to meet SLA?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Hugging Face","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T07:44:04.097Z","createdAt":"2026-01-13T07:44:04.097Z"},{"id":"q-1303","question":"In a multi-tenant SaaS using Aurora PostgreSQL with Global Database spanning us-west-2 and us-east-1, tenants must have isolated data access and BI dashboards must mask PII in real time. Propose an end-to-end design using per-tenant RLS, dynamic masking for BI, and a separate analytics store fed by CDC (DMS/Debezium). Include cross-region DR with RPO <5s and RTO <60s, data flow, encryption, backups, and a concrete sizing plan (replicas, window, network)?","answer":"Leverage Aurora PostgreSQL with per-tenant RLS and dynamic BI masking, plus a CDC pipeline (DMS/Debezium) feeding a dedicated analytics store (Redshift or DynamoDB+ Lambda) in the secondary region. Us","explanation":"## Why This Is Asked\nTests ability to architect multi-tenant isolation, real-time masking, and cross-region DR.\n\n## Key Concepts\n- Aurora PostgreSQL with Global Database\n- Row-Level Security and dynamic masking\n- CDC pipelines (DMS/Debezium)\n- Analytics stores (Redshift, DynamoDB)\n- DR targets (RPO/RTO), encryption, backups\n\n## Code Example\n```javascript\n-- SQL for RLS policy (illustrative)\nALTER TABLE events ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_isolation ON events\n  USING (tenant_id = current_setting('app.tenant_id')::int)\n  WITH CHECK (tenant_id = current_setting('app.tenant_id')::int);\n```\n\n## Follow-up Questions\n- How would you test tenant isolation in the analytics path?\n- What changes if BI dashboards must also support cross-tenant rollups?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Lyft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T08:50:50.803Z","createdAt":"2026-01-13T08:50:50.803Z"},{"id":"q-1314","question":"Design a GDPR-compliant data deletion strategy for a multi-region Aurora PostgreSQL Global Database that uses us-east-1 as the writer and replicas in multiple regions. How would you implement Right-to-Erasure for tenant data, propagate deletions with minimal latency, handle referential integrity, and maintain an auditable trail while meeting RPO/RTO targets? Include practical steps and trade-offs?","answer":"Adopt soft deletes with a tenant-scoped deleted_at flag and propagate deletions via logical replication or DMS CDC to all regions. Enforce FK cascades in the origin region only; keep a purge window (e","explanation":"## Why This Is Asked\nData privacy and cross-region deletion are common but tricky with global databases. The answer tests practical strategies for timing, integrity, and audit.\n\n## Key Concepts\n- GDPR deletion rights and retention policies\n- Aurora Global Database cross-region replication\n- Soft vs hard deletes and foreign-key considerations\n- Auditability and compliance tracking\n\n## Code Example\n```\nALTER TABLE tenants ADD COLUMN deleted_at TIMESTAMPTZ;\nCREATE VIEW active_items AS SELECT * FROM items WHERE deleted_at IS NULL;\nUPDATE items SET deleted_at = NOW() WHERE id = ?;\n```\n\n## Follow-up Questions\n- How would you test cross-region deletion latency and audit integrity?\n- How to handle legal-hold scenarios and purge timing?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hashicorp","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T11:28:34.774Z","createdAt":"2026-01-13T11:28:34.776Z"},{"id":"q-1382","question":"In an Aurora PostgreSQL Global Database with a writer in us-east-1 and replicas in eu-west-1, a financial balance update must be atomic across regions. Explain why cross-region distributed transactions are not supported and propose a practical pattern to achieve atomic-ish behavior with low latency, including data flow, failover handling, and cost/latency trade-offs?","answer":"Cross-region atomicity isn’t supported: Aurora Global Database uses asynchronous cross-region replication, so a two-stage commit across regions can’t be guaranteed. A practical pattern is a single-wri","explanation":"## Why This Is Asked\n\nTests understanding of cross-region replication limits in Aurora Global Database and practical patterns to achieve atomic-like behavior without distributed transactions.\n\n## Key Concepts\n\n- Aurora Global Database replication model and its eventual cross-region consistency\n- ACID vs eventual consistency in multi-region setups\n- Single-writer boundary, event-driven replication, compensating actions, and idempotent processing\n\n## Code Example\n\n```javascript\n// Pseudo-implementation: single-writer boundary with event emission\nasync function updateBalance(accountId, delta) {\n  // Begin in writer region\n  await beginLocalTx();\n  await updateLocalBalance(accountId, delta);\n  await commitLocalTx();\n  // Publish event to eu-west-1 for downstream update\n  await publishEvent({ accountId, delta, txnId: generateId() });\n}\n```\n\n## Follow-up Questions\n\n- How would you test this pattern under network partitions?\n- What latency and cost implications arise from cross-region event streams and reconciliation?","diagram":"flowchart TD\n  A[Client Request] --> B[Coordinator (us-east-1)]\n  B --> C[Prepare: Update primary balance]\n  C --> D[Commit: Emit event to eu-west-1]\n  D --> E[Apply delta in eu-west-1]\n  E --> F[Response to client]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T14:45:15.419Z","createdAt":"2026-01-13T14:45:15.419Z"},{"id":"q-1441","question":"In Aurora PostgreSQL (us-east-1) with tenant isolation via RLS, design a near real-time analytics path for a eu-west-1 consumer needing masked data and <5s lag. Use Aurora Global Database for OLTP and a CDC store in eu-west-1 (Redshift or DynamoDB+Lambda). Explain data flow, masking/encryption, consistency, failover, and cost with concrete config sketches?","answer":"Use Aurora Global Database to keep a writer in us-east-1 with cross-region replicas; route analytic reads to eu-west-1 via DMS CDC to Redshift or DynamoDB+Lambda. Implement per-tenant masking in the p","explanation":"## Why This Is Asked\nTests ability to design cross-region analytics paths, balancing data masking, security, latency, failover, and cost for a regulated multi-tenant SaaS using AWS DB services.\n\n## Key Concepts\n- Aurora Global Database cross-region replication\n- CDC options: DMS vs Debezium vs native\n- Data masking: per-tenant with RLS and column-level masking\n- Encryption: KMS, TLS, envelope encryption\n- Failure scenarios: failover/failback and RPO/RTO\n- Cost trade-offs: Redshift vs DynamoDB, compute/storage\n\n## Code Example\n```javascript\n// Example: publication for CDC (PostgreSQL)\nALTER SYSTEM SET wal_level = logical;\nCREATE PUBLICATION analytics_pub FOR TABLE orders, customers;\n```\n\n## Follow-up Questions\n- How would you validate lag and data completeness end-to-end?\n- How would you enforce per-tenant masking in the analytics store?","diagram":"flowchart TD\n  A[OLTP writer us-east-1] --> B[Global DB replica us-east-1]\n  A --> C[DMS CDC in eu-west-1? (for analytics store)]\n  B --> D[Analytics store in eu-west-1]\n  D --> E[Analytics consumer in eu-west-1]","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T17:03:20.591Z","createdAt":"2026-01-13T17:03:20.592Z"},{"id":"q-1447","question":"You run a high-volume ecommerce on Aurora PostgreSQL Global Database with a single writer in us-east-1 and read replicas in eu-west-1. An outage in us-east-1 requires routing writes to eu-west-1 within 60s while ensuring RPO<5s, idempotent writes, and no double billing. Design the architecture and concrete configuration (replica counts, failover procedures, analytics CDC path, masking, PITR) to meet these goals?","answer":"Use Aurora Global Database with a hot-standby writable clone in eu-west-1 for DR so writes can continue within 60s of a us-east outage. Route writes to the EU writer via Route 53 health checks; keep a","explanation":"## Why This Is Asked\n\nAssesses cross-region DR planning for a mission-critical OLTP with minimal data loss and downtime, plus robust write-idempotency and analytics integration.\n\n## Key Concepts\n\n- Aurora Global Database architecture and cross-region failover considerations\n- DR targets: RPO < 5s, RTO < 60s with hot-standby writable clone\n- Idempotent write patterns: transaction IDs, upserts to prevent double billing\n- CDC to analytics (DMS) and data masking at read time\n- PITR retention planning and backup strategies\n\n## Code Example\n\n```sql\n-- idempotent upsert pattern for an order\nINSERT INTO orders (order_id, customer_id, amount, txn_id)\nVALUES (:order_id, :customer_id, :amount, :txn_id)\nON CONFLICT (order_id) DO UPDATE\n  SET amount = EXCLUDED.amount, updated_at = NOW(), txn_id = EXCLUDED.txn_id;\n```\n\n## Follow-up Questions\n\n- How would you validate RPO/RTO in production and what telemetry would you collect?\n- How would you test and verify the cross-region failover without impacting live traffic?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T17:43:10.564Z","createdAt":"2026-01-13T17:43:10.564Z"},{"id":"q-1550","question":"In a two-region deployment with a single writer in us-east-1 and analytic reads in eu-west-1, design a CDC pipeline to keep a near real-time analytic store updated within 5 seconds of commits, while masking per-tenant data and enforcing encryption at rest and in transit. Compare AWS DMS, Debezium/Kafka, and native Aurora logical replication, and provide concrete configuration (engine, instance types, replica counts, PITR, KMS keys, VPC endpoints, and network topology) to meet RPO 5s and RTO 60s?","answer":"Deploy an Aurora Global Database with the primary writer in us-east-1 and implement a CDC pipeline using AWS DMS to maintain a near real-time analytic store in eu-west-1. Configure DMS with change data capture to stream committed changes within 5 seconds, applying per-tenant data masking and enforcing encryption both in transit and at rest. AWS DMS provides operational simplicity with managed infrastructure, meeting the RPO of 5 seconds and RTO of 60 seconds while ensuring data security and compliance requirements.","explanation":"## Why This Is Asked\nTests ability to design cross-region CDC pipelines with strict latency targets while implementing tenant-level data masking and comprehensive encryption. It also evaluates knowledge of live replication technologies and their operational trade-offs.\n\n## Key Concepts\n- Change data capture latency across AWS regions\n- Cross-region data movement options (AWS DMS vs Debezium/Kafka vs native Aurora logical replication)\n- Per-tenant data masking and encryption strategies\n- Network topology and VPC endpoint configuration\n- Disaster recovery objectives: RPO/RTO, PITR, and failover procedures\n\n## Code Example\n```sql\n-- On writer: publish all tables for CDC\nCREATE PUBLICATION cdc_pub FOR ALL TABLES;\n```\n\n## Follow-up Questions\n- How would you handle failover scenarios and ensure data consistency?\n- What monitoring and alerting would you implement for the CDC pipeline?\n- How would you optimize costs while maintaining the 5-second latency requirement?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","DoorDash","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T06:30:05.991Z","createdAt":"2026-01-13T21:38:49.169Z"},{"id":"q-1747","question":"You run a multi-region SaaS with Aurora PostgreSQL as the OLTP in us-east-1 and read replicas in eu-west-1. A new requirement enforces strict per-tenant data isolation via Row-Level Security and data residency controls for backups. Design a concrete approach: RLS policy skeletons for all tables, session-based tenant_id from authentication, per-tenant restore strategy, cross-region backup copy schedule, and a testing/validation plan that proves no cross-tenant leakage under burden. Include concrete config knobs and a sample policy?","answer":"Implement per-tenant isolation with Postgres Row-Level Security on all tables, driven by a session tenant_id set from the user’s JWT. Use a separate role per tenant and policies like: USING (tenant_id","explanation":"## Why This Is Asked\nTests ability to implement robust data isolation with RLS, ensure data residency through cross-region snapshots, and design test plans for high-concurrency workloads.\n\n## Key Concepts\n- PostgreSQL Row-Level Security (RLS) on all tables\n- session context via SET myapp.tenant_id from authentication\n- cross-region Aurora backups and snapshot copies\n- PITR windows and data residency/compliance\n- tenant onboarding/offboarding and data retention controls\n\n## Code Example\n```sql\n-- Example RLS skeleton\nCREATE POLICY tenant_rls ON users\nFOR ALL USING (tenant_id = current_setting('myapp.tenant_id')::int)\nWITH CHECK (tenant_id = current_setting('myapp.tenant_id')::int);\n\nALTER TABLE users ENABLE ROW LEVEL SECURITY;\nALTER TABLE users FORCE ROW LEVEL SECURITY;\n```\n\n## Follow-up Questions\n- How would you automate per-tenant onboarding to ensure tenant_id is set on every DB connection?\n- What metrics would you monitor to detect leakage or policy misconfigurations?","diagram":"flowchart TD\n  C[Client Request] --> APP[App Layer]\n  APP --> U[Aurora US-East OLTP]\n  U --> EU[EU-West Replica]\n  APP --> S[Snapshot Copy to EU-West]\n  S --> D[Data Residency & Compliance]\n  D --> M[Monitoring & Alerts]","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:37:30.426Z","createdAt":"2026-01-14T09:37:30.428Z"},{"id":"q-1775","question":"In a multi-region Aurora PostgreSQL Global Database setup (writer in us-east-1; readers in eu-west-1 and ap-south-1) with strict tenant-level data residency, design a scalable architecture that provides sub-50ms reads for hot paths in each region while ensuring RPO <= 5s and RTO <= 60s, using row-level security and a CDC-based analytic store; explain data partitioning, access controls, and failover strategy, plus cost trade-offs?","answer":"Use region-scoped tenant sharding with Postgres RLS to enforce residency; writer in us-east-1 with Aurora Global Database and replicas in eu-west-1/ap-south-1 for sub-50ms regional reads. Use CDC from","explanation":"## Why This Is Asked\nAssesses ability to architect cross-region, residency-bound databases with real-time analytics, balancing latency, consistency, and cost.\n\n## Key Concepts\n- Aurora Global Database and cross-region replication\n- Row-Level Security (RLS) for tenant isolation\n- CDC-based analytic store (Redshift/S3+Glue)\n- RPO/RTO design and failover strategy\n- Cost trade-offs: replication, egress, storage, and analytics\n\n## Code Example\n```sql\n-- Example RLS policy\nCREATE POLICY tenant_rls ON orders\n  USING (tenant_id = current_setting('myapp.tenant_id')::int);\n```\n\n## Follow-up Questions\n- How would you validate RPO/RTO in a disaster scenario?\n- What strategies minimize cross-region CDC costs while preserving freshness?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:39:05.726Z","createdAt":"2026-01-14T10:39:05.727Z"},{"id":"q-1882","question":"Design a cross-region analytic path for a SaaS app with an Aurora PostgreSQL OLTP cluster in us-east-1 as the single writer and regional read replicas in us-west-2. The goal: near real-time analytics with masked PII in the analytics store. Propose a CDC-based pipeline (Aurora CDC, DMS, Debezium, or Kinesis) to load into Redshift or DynamoDB in us-west-2, choose masking strategy, encryption, data freshness target (RPO), failover plan, and cost considerations. Include concrete config choices (instance types, retention, network, and security)?","answer":"Use Aurora Global Database to replicate OLTP across regions, and implement CDC from the writer to a masked analytics store in the remote region (Redshift via DMS or DynamoDB via Kinesis). Mask PII at ","explanation":"## Why This Is Asked\nAssess cross-region data flow, masking, and cost-aware analytics separation. It tests practical CDC choices, DR timing, and security implications.\n\n## Key Concepts\n- Aurora Global Database, CDC, DMS/Debezium, Redshift Spectrum, DynamoDB Streams\n- Data masking, encryption at rest/in transit, RPO/RTO targets\n- Cross-region networking and cost optimization\n\n## Code Example\n```sql\n-- Example masking policy sketch (pseudo)\nCREATE POLICY mask_ssn ON customers\nAS (SELECT mask_ssn(ssn) AS ssn_masked);\n```\n\n## Follow-up Questions\n- How would you test RPO/RTO guarantees in this pipeline?\n- What are failure modes if CDC lag increases beyond threshold?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T15:42:33.696Z","createdAt":"2026-01-14T15:42:33.696Z"},{"id":"q-2139","question":"You run OLTP in Aurora PostgreSQL us-east-1 and need near real-time BI in us-west-2. Design a CDC pipeline: enable a dedicated logical replication slot in Aurora; use DMS in CDC mode to stream changes to Redshift in us-west-2 via a staging S3 bucket; apply PII masking at BI layer; enable PITR and cross-region backups; target end-to-end latency ~2s and RTO <60s. Include data flow, failover, and cost trade-offs?","answer":"Enable a dedicated Aurora logical replication slot in us-east-1; route changes through DMS in CDC mode to Redshift in us-west-2 (via S3 staging). Use MERGE-based upserts for idempotence. Apply PII mas","explanation":"## Why This Is Asked\nTests real-time cross-region CDC design with security, masking, and failover.\n\n## Key Concepts\n- Aurora logical replication\n- AWS DMS CDC\n- Redshift ingestion from S3\n- Data masking and RLS\n- PITR and cross-region backups\n- Cost trade-offs\n\n## Code Example\n```json\n{\n  \"DMSTaskSettings\": {\"TargetTablePrepMode\":\"DO_NOTHING\",\"FullLoadTask\": false,\"CdcInsertsOnly\": false}\n}\n```\n\n## Follow-up Questions\n- How to handle schema changes downstream?\n- What metrics validate 2s latency under load?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:19:48.906Z","createdAt":"2026-01-15T04:19:48.906Z"},{"id":"q-2163","question":"A multi-region SaaS app needs sub-20ms reads for hot tenants across three continents, writes allowed in any region, and PCI-DSS data residency constraints. Design a data-layer using AWS: compare Aurora PostgreSQL Global Database with DynamoDB Global Tables plus CDC to an analytics store, including consistency, DR, backups, and concrete configurations to meet RPO 5s and RTO 60s?","answer":"Option A: Aurora PostgreSQL Global Database with a single writer in us-east-1 and regional read replicas in eu-west-1 and ap-south-1; 7-day PITR, KMS encryption, TLS, and Route 53 latency routing to m","explanation":"## Why This Is Asked\nTests cross-region DR, read latency, and data residency decisions for AWS database services.\n\n## Key Concepts\n- Aurora Global Database constraints: single writer, cross-region replication\n- DynamoDB Global Tables: multi-region writes with replication and eventual consistency trade-offs\n- Data residency and PCI-DSS: encryption (at rest/in transit), KMS keys, IAM access controls\n- DR planning: RPO/RTO targets, PITR, backups, failover orchestration\n\n## Code Example\n```javascript\n// CDK sketch for core Aurora Global DB settings (conceptual)\nconst glb = new aurora.GlobalDatabase(this, 'GlobalDb', {\n  writerRegion: 'us-east-1',\n  regions: ['us-east-1', 'eu-west-1', 'ap-south-1'],\n  engine: aurora.PostgresEngineVersion.VER_13_6,\n});\n```\n\n## Follow-up Questions\n- If latency budgets tighten, how would you restructure reads?\n- How would you monitor cross-region replication lag and auto-tune write routing?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:41:48.135Z","createdAt":"2026-01-15T05:41:48.135Z"},{"id":"q-2211","question":"Two-region, multi-tenant SaaS with strict data residency: EU tenants' data must stay in EU, US tenants' data in US. Needs sub-15ms reads for hot tenants, writes in any region, and cross-region analytics. Compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables with analytics options; provide a concrete topology, replication, backups, and DR plan to meet RPO 5s and RTO 60s, including per-tenant routing and residency enforcement?","answer":"Use Aurora PostgreSQL Global Database across EU and US with per-tenant residency enforced by Row-Level Security; hot reads served from a regional cache (DynamoDB or MemoryDB) and analytics from a data","explanation":"## Why This Is Asked\nTests ability to design multi-region, tenancy-aware architectures with DR and residency constraints; assesses knowledge of Aurora Global Database, RLS, read caching, analytics integration, and cost/latency trade-offs.\n\n## Key Concepts\n- Data residency with row-level security (RLS) in PostgreSQL\n- Aurora Global Database topology across regions\n- Cross-region replication and WAL shipping mechanics\n- Read caching and analytics integration (DynamoDB/MemoryDB, S3/Glue)\n\n## Code Example\n```sql\n-- Enable per-tenant isolation via RLS\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_isolation ON orders\n  USING (tenant_id = current_setting('app.tenant_id')::int);\n```\n\n## Follow-up Questions\n- How would you validate DR failover latency and RPO under peak load?\n- What monitoring and cost controls would you implement to maintain SLAs across regions?","diagram":"flowchart TD\n  EU_Tenant[EU region data store] --> EU_Reads[EU read replicas]\n  US_Tenant[US region data store] --> US_Reads[US read replicas]\n  EU_Reads --> AnalyticsEU[Analytics]\n  US_Reads --> AnalyticsUS[Analytics]\n  AnalyticsEU --> Lake[Analytics Lake]\n  AnalyticsUS --> Lake","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Salesforce","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T07:40:12.465Z","createdAt":"2026-01-15T07:40:12.466Z"},{"id":"q-2248","question":"A SaaS news site logs user events at ~1000 writes/s, with dashboards needing sub-200 ms reads. Data is append-only; hot data kept 30 days, archived after. Compare DynamoDB (on-demand, TTL, GSI) vs Aurora PostgreSQL (partitioned tables, read replicas) for this workload. Provide concrete configs (primary key design, indexes, TTL window, backup/retention, RPO/RTO) and justify choice?","answer":"Recommended: DynamoDB on-demand with a TTL attribute of 90 days, primary key (user_id HASH, event_ts RANGE), optional GSI on event_type for analytics, enable Streams for CDC, and PITR. Data is append-","explanation":"## Why This Is Asked\nTests ability to pick between a serverless NoSQL and a relational option for high-velocity, append-only data with TTL and analytics needs. Assesses data modeling, retention strategy, and DR considerations.\n\n## Key Concepts\n- Append-only data modeling for time-series-like logs\n- Throughput modes: on-demand vs provisioned capacity\n- TTL data pruning and retention windows\n- Read latency strategies (indexes, caching, Streams)\n- Backups, PITR, and DR differences between DynamoDB and Aurora\n\n## Code Example\n```sql\nCREATE TABLE event_logs (\n  user_id VARCHAR(36) NOT NULL,\n  event_ts BIGINT NOT NULL,\n  payload JSON,\n  PRIMARY KEY (user_id, event_ts)\n) PARTITION BY RANGE (event_ts);\n```\n\n```bash\naws dynamodb create-table --table-name UserEventLog \\\n  --attribute-definitions AttributeName=user_id,AttributeType=S AttributeName=event_ts,AttributeType=N \\\n  --key-schema AttributeName=user_id,KeyType=HASH AttributeName=event_ts,KeyType=RANGE \\\n  --billing-mode PAY_PER_REQUEST \\\n  --stream-specification StreamEnabled=true,StreamViewType=NEW_IMAGE\n```\n\n```bash\naws dynamodb update-time-to-live --table-name UserEventLog --time-to-live-specification Enabled=true,AttributeName=ttl\n```\n\n## Follow-up Questions\n- What are the trade-offs of not using TTL on DynamoDB for hot data?\n- How would you validate data durability and latency under bursty traffic?","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Square","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T09:04:51.962Z","createdAt":"2026-01-15T09:04:51.962Z"},{"id":"q-2268","question":"Your app uses AWS Lambda functions that connect to an RDS PostgreSQL instance; during bursts, you see many connections causing failures. How would you leverage Amazon RDS Proxy to manage connections, configure auth, and ensure stable performance? Include what to monitor, any pricing considerations, and a basic setup outline?","answer":"Use RDS Proxy to pool connections and decouple Lambda bursts from DB connection limits. Create a private RDS Proxy in the same VPC, target the RDS instance, store credentials in Secrets Manager, attac","explanation":"This question tests practical understanding of connection management for serverless apps. Candidates should cite: where to place the proxy, how credentials are supplied, how to route Lambda traffic, what to monitor (proxy health, connection count, latency), and cost trade-offs. They should mention security group rules and multi-AZ considerations.","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Goldman Sachs","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T09:50:32.865Z","createdAt":"2026-01-15T09:50:32.865Z"},{"id":"q-2320","question":"Scenario: A SaaS app stores event data in DynamoDB and must retain 90 days in DynamoDB and archive older events to S3 for analytics. Design a pragmatic lifecycle: data model, TTL, export to S3, per-tenant/year/month partitioning, storage class selection, and validation plan to ensure data integrity and queryability via Athena. Include concrete steps and considerations for cost?","answer":"Two-tier approach: keep hot data for 90 days in DynamoDB with TTL on expireAt; archive older events to S3 per tenant/year/month using DynamoDB export-to-S3 (or Glue), compress to Parquet, and apply li","explanation":"## Why This Is Asked\nValidates practical data lifecycle design, cost awareness, and AWS integration.\n\n## Key Concepts\n- DynamoDB TTL on expireAt\n- DynamoDB export-to-S3 or Glue-based export\n- S3 Lifecycle and storage classes (Standard/IA/Glacier)\n- Per-tenant partitioning in S3 (tenant/year/month)\n- Athena/Glue for analytics on archived data\n\n## Code Example\n```javascript\n// TTL example snippet (not production-ready)\nconst item = { tenantId:'T1', eventId:'E123', ts:Date.now(), expireAt: Math.floor(Date.now()/1000) + 90*24*3600 };\n```\n\n## Follow-up Questions\n- How would you adjust for cross-region access to archived data?\n- What are trade-offs of online TTL vs scheduled archive windows?","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T11:49:47.720Z","createdAt":"2026-01-15T11:49:47.720Z"},{"id":"q-2331","question":"For a multi-tenant SaaS app storing per-tenant PII with EU/US data residency, compare Aurora PostgreSQL with Row-Level Security (RLS) vs DynamoDB with per-tenant access patterns. Explain schema design, replication strategy, consistency, DR, and cost. Provide a concrete configuration to meet RPO < 5s and RTO < 60s, including region placement, replica counts, backup windows, and key management?","answer":"Recommended approach: Aurora PostgreSQL Global Database with per-tenant RLS and cross-region replicas. Writer in us-east-1; replicas in eu-west-1 and ap-south-1; enable PITR for 35 days and automated ","explanation":"## Why This Is Asked\nEvaluates RBAC at the database layer, cross-region data residency, and trade-offs between relational and NoSQL models in multi-tenant SaaS.\n\n## Key Concepts\n- Row-Level Security (RLS) in PostgreSQL\n- DynamoDB conditional writes and per-tenant design\n- Cross-region replication, PITR, and RTO/RPO targets\n\n## Code Example\n\n```javascript\nCREATE POLICY tenant_access ON users\nFOR ALL USING (tenant_id = current_setting('tenants.current')::int);\n```\n\n## Follow-up Questions\n- How would you audit per-tenant data access across regions?\n- What metrics would you monitor to detect RBAC misconfigurations?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Plaid","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T13:07:36.036Z","createdAt":"2026-01-15T13:07:36.036Z"},{"id":"q-2411","question":"In an IoT platform, 100k writes/sec of time-series data arrive from devices worldwide. You must ingest region-locally with sub-20ms latency, perform near real-time analytics in a separate region, retain 30 days of data, and ensure PCI-DSS residency. Compare AWS Timestream, DynamoDB Global Tables with a CDC pipeline, and an Aurora-based time-series schema. Propose architecture, data model, retention, DR, and concrete config to meet RPO 5s and RTO 60s?","answer":"DynamoDB Global Tables with regional writes, plus DynamoDB Streams feeding a near-real-time analytics path (Kinesis→S3/Redshift). For PCI residency, encrypt at rest with a restricted KMS CMK and stric","explanation":"## Why This Is Asked\nTests multi-region data ingestion, analytics separation, retention, and compliance trade-offs with real workloads.\n\n## Key Concepts\n- DynamoDB Global Tables, DynamoDB Streams, KMS\n- Data residency for PCI-DSS, PITR, TTL\n- Time-series patterns, retention strategies, and analytics integration\n\n## Code Example\n```bash\n# Create a Global Table (example flag values for illustration)\naws dynamodb create-global-table --table-name IoTTimeSeries \\\n  --region us-east-1 us-west-2 eu-west-1\n```\n\n## Follow-up Questions\n- How would you validate RPO/RTO in disaster scenarios?\n- What monitoring/alerting ensures latency stays within targets?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Twitter","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T16:59:48.253Z","createdAt":"2026-01-15T16:59:48.253Z"},{"id":"q-2465","question":"An Aurora PostgreSQL cluster in us-east-1 serves 10k+ tenants via IAM database authentication. Each tenant must only access its own rows using Row-Level Security. A separate analytics workload must run against a eu-west-1 replica with data within 5 seconds of writes. Design the architecture: RLS policy and session management for per-tenant isolation, how analytics access is granted without leaking data, replication strategy (global DB vs separate KV store), backup/PITR, and a practical test plan to verify isolation and latency?","answer":"Implement RLS with per-session tenant_id set by the application, using policy USING (tenant_id = current_setting('app.tenant_id')::BIGINT) and WITH CHECK the same. Middleware sets app.tenant_id on con","explanation":"## Why This Is Asked\nTests practical RLS design, session-scoped tenant isolation, and cross-region analytics with minimal leakage.\n\n## Key Concepts\n- Row-Level Security (RLS) in Aurora PostgreSQL\n- Per-session context via current_setting()\n- IAM DB authentication integration\n- Aurora Global Database cross-region replication\n- PITR, KMS encryption, and audit logging\n\n## Code Example\n```sql\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_isolation ON orders\n  USING (tenant_id = current_setting('app.tenant_id')::BIGINT)\n  WITH CHECK (tenant_id = current_setting('app.tenant_id')::BIGINT);\n```\n```sql\nSET app.tenant_id = '12345';\n```\n```sql\nSET row_security = OFF; -- on analytics connection if needed for cross-tenant analytics\n```\n\n## Follow-up Questions\n- How would you validate tenant isolation under peak write latency?\n- What monitoring would you add to detect RLS bypass attempts or replication lag spikes?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Oracle","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T19:05:57.687Z","createdAt":"2026-01-15T19:05:57.687Z"},{"id":"q-2511","question":"In a globally distributed SaaS, Aurora PostgreSQL Global Database writer in us-east-1 and read replicas in eu-west-1 and ap-southeast-2. To deliver sub-50ms reads for hot tenants during peak while writes can occur anywhere, design a hybrid path using ElastiCache Redis in each region plus a cache-aside strategy. Include concrete config: DB instance classes, replica counts, Redis node types, TTL, invalidation mechanism, and a failover plan that meets RPO 5s and RTO 60s?","answer":"Use Aurora PostgreSQL Global Database with a writer in us-east-1 and cross-region replicas in eu-west-1 and ap-southeast-2. Layer in ElastiCache Redis in each region for the hot keys, cache-aside, TTL","explanation":"## Why This Is Asked\nTests ability to design multi-region data paths with both OLTP and caching to meet latency SLAs.\n\n## Key Concepts\n- Aurora Global Database cross-region replication.\n- Cache-aside strategy with ElastiCache Redis.\n- Per-region caching and invalidation messaging.\n- SLOs, RPO/RTO alignment.\n\n## Code Example\n```javascript\nasync function getUserRow(id){\n  const cached = await cache.get(`user:${id}`)\n  if(cached) return JSON.parse(cached)\n  const row = await db.query('SELECT * FROM users WHERE id=$1', [id])\n  await cache.set(`user:${id}`, JSON.stringify(row), 60)\n  return row\n}\n```\n\n## Follow-up Questions\n- How would you validate cache stampede risk and implement backpressure?\n- What are the DR implications if the writer region fails?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","MongoDB","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T20:54:14.192Z","createdAt":"2026-01-15T20:54:14.192Z"},{"id":"q-2533","question":"In a multi-region SaaS app using Aurora PostgreSQL with a single writer in us-east-1 and reads in eu-west-1 and ap-south-1, implement row-level security to restrict each tenant's data. Explain how you would design the RLS policy, index usage, and the impact on cross-region CDC via DMS or logical replication, and outline monitoring for unauthorized access. Include testing steps?","answer":"Implement PostgreSQL Row-Level Security (RLS) with per-tenant session context. Create the policy: `USING (tenant_id = current_setting('app.tenant_id')::int)` and enable RLS on the tenant table. The application must set `set_config('app.tenant_id', tenant_id, true)` at connection initialization. For cross-region replication, RLS policies propagate automatically via DMS or logical replication, but session context does not—each region must independently enforce tenant isolation. Monitor through CloudWatch metrics for RLS policy violations and audit logs using pg_audit. Validate with multi-tenant scenarios, cross-region consistency checks, and unauthorized access attempt testing.","explanation":"## Why This Is Asked\nTests ability to design secure data access patterns in multi-region Aurora environments. It combines RBAC-like row-level security with cross-region replication and monitoring, requiring understanding of policy propagation and comprehensive testing strategies.\n\n## Key Concepts\n- Row Level Security (RLS) and policy implementation in PostgreSQL/Aurora\n- Session context management via current_setting/set_config\n- Cross-region replication implications with DMS or logical replication\n- Performance optimization, audit capabilities, and failover considerations\n\n## Code Example","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Netflix","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:35:46.313Z","createdAt":"2026-01-15T21:41:50.605Z"},{"id":"q-2581","question":"Global SaaS with EU data residency: Writes in us-east-1; reads in EU must be sub-20ms; PCI-DSS data residency constraints; design a cross-region OLTP data layer and compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables with CDC. Provide concrete configurations (engine/edition, instance types, replica counts, PITR window, backup cadence, KMS keys, VPC design, inter-region networking), DR plan, and how you meet RPO <5s and RTO <60s?","answer":"Design a cross-region OLTP architecture with writes in us-east-1 and EU reads under 20ms, adhering to PCI-DSS data residency requirements. Compare Aurora PostgreSQL Global Database against DynamoDB Global Tables with CDC, providing specific configurations including engine/edition, instance types, replica counts, PITR window, backup cadence, KMS keys, VPC design, and inter-region networking. Deliver a disaster recovery plan achieving RPO <5s and RTO <60s.","explanation":"## Why This Is Asked\nTests the ability to design cross-region OLTP systems under strict residency and security constraints while comparing relational global databases with NoSQL global tables, including concrete disaster recovery configurations.\n\n## Key Concepts\n- Cross-region OLTP architectures\n- Data residency and PCI-DSS compliance\n- Aurora Global Database vs DynamoDB Global Tables with CDC\n- DR metrics: RPO, RTO, PITR, backup strategies, KMS key policies\n\n## Code Example\n```javascript\n// Pseudo-CDK: define a DynamoDB global table across two regions\nconst table = new dynamodb.Table(this, 'OrderTable', {\n  partitionKey: { name: 'orderId', type: dynamodb.AttributeType.STRING },\n  billingMode: dynamodb.BillingMode.PROVISIONED,\n  readCapacity: 100,\n  writeCapacity: 50,\n  streams: dynamodb.StreamType.NEW_AND_OLD_IMAGES,\n  encryption: dynamodb.TableEncryption.AWS_MANAGED,\n  pointInTimeRecovery: true,\n  globalTables: [\n    { region: 'us-east-1' },\n    { region: 'eu-west-1' }\n  ]\n});\n```","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:14:31.132Z","createdAt":"2026-01-15T23:41:13.159Z"},{"id":"q-2701","question":"In Aurora PostgreSQL design per-tenant data isolation using Row-Level Security and partitioned tables for a global setup: writer in us-east-1; regional reads in eu-west-1 and ap-south-1. Propose a concrete data path and DR strategy to meet RPO 5s and RTO 60s, including instance types, backup windows, PITR, KMS, cross-region replication, and a real failover plan?","answer":"Use a single writer in us-east-1 with Aurora Global Database, partitioned tables by tenant_id, and enable RLS policies bound to a session context. Route reads to regional readers; writes hit the write","explanation":"## Why This Is Asked\n\nAssesses tenant isolation, cross-region replication, and DR under compliance constraints.\n\n## Key Concepts\n\n- Row-Level Security and partitioning for multi-tenant isolation\n- Aurora Global Database cross-region replication and write routing constraints\n- PCI-DSS and data residency, encryption with KMS, and backup strategies\n- RPO/RTO goals and failover planning\n\n## Code Example\n\n```sql\nCREATE TABLE tenants (\n  tenant_id TEXT NOT NULL,\n  data JSONB,\n  region TEXT\n);\nALTER TABLE tenants ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_isolation ON tenants\n  USING (tenant_id = current_setting('app.tenant_id')::TEXT);\n```\n\n## Follow-up Questions\n\n- How would you test RLS policies at scale across regions?\n- What monitoring would you put in place to detect cross-region replication lag?","diagram":"flowchart TD\n  W[Writer in us-east-1] --> R1[Replica eu-west-1]\n  W --> R2[Replica ap-south-1]\n  R1 --> C1[(Cache layer)]\n  R2 --> C2[(Cache layer)]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T07:37:43.229Z","createdAt":"2026-01-16T07:37:43.230Z"},{"id":"q-2907","question":"A multi-region fintech app stores trades in Aurora PostgreSQL us-west-2 with sub-5ms writes and analytics in eu-central-1. Design an AWS-native architecture to meet RPO 5s and RTO 60s, ensure immutable audit logs in S3 with Object Lock, and maintain data residency. Compare Aurora Global Database writer+region replica vs DynamoDB+DMS analytics path, with concrete config (instance classes, replica counts, backup windows, KMS keys, IAM roles) and failover plan?","answer":"Use Aurora PostgreSQL Global Database with writer in us-west-2 and a read replica in eu-central-1 for sub-5ms writes and near-real reads; enable multi-AZ, cross-region backups, and PITR with 5s RPO. S","explanation":"Why This Is Asked\nTests cross-region DR, data residency, and immutable auditing in a realistic fintech context where latency targets are strict and analytics must co-exist with OLTP.\n\nKey Concepts\n- Aurora Global Database topology and cross-region replication\n- Immutable audit logging with S3 Object Lock and KMS\n- Data residency controls and per-tenant access (RLS)\n\nCode Example\n```typescript\n// CDK-like sketch for Aurora Global Database setup (illustrative)\nimport * as cdk from 'aws-cdk-lib';\nimport * as rds from 'aws-cdk-lib/aws-rds';\nimport * as ec2 from 'aws-cdk-lib/aws-ec2';\n\nconst vpc = new ec2.Vpc(this, 'VPC');\nnew rds.CfnDBCluster(this, 'WriterCluster', {\n  engine: 'aurora-postgresql',\n  databaseName: 'trades',\n  // replicas and cross-region and other config would be here\n});\n```\n\nFollow-up Questions\n- How would you handle schema changes with zero downtime across regions?\n- What monitoring and alerting would you implement to ensure RPO/RTOs are met in practice?","diagram":"flowchart TD\nA[Trade Write] --> B[(Aurora Global Writer in us-west-2)]\nB --> C[(Replica in eu-central-1)]\nA --> D[(Audit to S3 with Object Lock)]\nE[Analytics via DMS] --> F[Redshift/Analytics]\n","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","MongoDB","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T16:57:36.602Z","createdAt":"2026-01-16T16:57:36.602Z"},{"id":"q-2935","question":"Design a cross-region fintech data path where writes land in DynamoDB Global Tables (multi-master) in us-east-1 and eu-west-1, while analytics are powered by an Aurora PostgreSQL cluster in eu-west-1. Explain conflict handling, data residency for PCI-DSS, and how you meet RPO <5s and RTO <60s. Include concrete configurations: DynamoDB table keys and streams, Lambda, DMS or Debezium, Aurora size, backups, and network topology?","answer":"Use DynamoDB Global Tables (us-east-1 and eu-west-1) for multi-master writes; analytics replicated to Aurora PostgreSQL (eu-west-1) via a DynamoDB Streams → Lambda → DMS CDC pipeline. Resolve conflict","explanation":"## Why This Is Asked\nTests ability to design cross-region, multi-master transactional paths with real-time analytics, while honoring security and compliance constraints.\n\n## Key Concepts\n- DynamoDB Global Tables for low-latency multi-region writes\n- CDC pipeline (Streams → Lambda → DMS/Debezium) to Aurora\n- Conflict resolution and data governance for cross-region writes\n- PCI-DSS residency and EU-centric encryption/backups\n\n## Code Example\n```javascript\n// Lambda sample: process DynamoDB stream and push to Aurora via CDC\nexports.handler = async (event) => {\n  for (const rec of event.Records) {\n    if (rec.eventName === 'INSERT' || rec.eventName === 'MODIFY') {\n      const item = AWS.DynamoDB.Converter.unmarshall(rec.dynamodb.NewImage);\n      await upsertAurora(item);\n    }\n  }\n};\n```\n\n## Follow-up Questions\n- How would you handle a write-conflict when two regions update the same item simultaneously?\n- What monitoring and alerting would you implement to guarantee RPO and RTO targets under failover conditions?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T17:55:49.159Z","createdAt":"2026-01-16T17:55:49.159Z"},{"id":"q-2979","question":"Beginner scenario: An online storefront runs an AWS RDS MySQL instance in a single region with 1–2 read replicas. Compliance requires RPO 15 minutes and RTO 60 seconds during disaster recovery. Propose a practical backup and recovery plan using automated backups, PITR, Multi-AZ, and read replicas, with concrete values for backup retention, PITR window, and snapshot cadence, plus a reproducible failover procedure?","answer":"Enable automated backups with PITR window 7 days and backup retention 7 days; use Multi-AZ for automatic failover; create 1–2 read replicas for reads and potential DR primaries; schedule daily automat","explanation":"## Why This Is Asked\nTests practical understanding of RDS backup/DR features and DR testing steps in a beginner-friendly way.\n\n## Key Concepts\n- Automated backups and PITR\n- Multi-AZ vs Read Replicas\n- DR testing for RTO/RPO\n- Snapshot cadence and retention\n\n## Code Example\n```bash\n# Create a manual snapshot example\naws rds create-db-snapshot --db-instance-identifier storefront-db --db-snapshot-identifier storefront-sn-20260116\n```\n\n## Follow-up Questions\n- How would you monitor backup success and alert on failures?\n- How would you perform a low-traffic failover test with minimal user disruption?","diagram":"flowchart TD\nA[Start] --> B[Automated backups enabled]\nB --> C[Multi-AZ]\nC --> D[Read replicas configured]\nD --> E[Failover by promoting replica]\nE --> F[Switch endpoint (Route 53)]\nF --> G[Validate RPO/RTO]","difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T19:43:48.467Z","createdAt":"2026-01-16T19:43:48.467Z"},{"id":"q-3137","question":"Aurora PostgreSQL is deployed for a new product with highly variable traffic but strict latency targets. Compare Aurora Serverless v2 vs provisioned Aurora with read replicas for cost, latency, and maintenance. Provide a concrete setup example: (A) Serverless v2 with min 2 ACU, max 32 ACU, pause after idle, and RDS Proxy for connection pooling; (B) provisioned Aurora with 2 writer instances and 3 read replicas in the same region, plus a PgBouncer pool. Conclude when to choose each?","answer":"Serverless v2 excels with highly variable traffic, scaling from ~2 to 32 ACU and reducing idle costs; use RDS Proxy to stabilize connection churn. Provisioned Aurora works best for steady bursts with ","explanation":"## Why This Is Asked\n\nTests the candidate's understanding of when to use Aurora Serverless v2 versus provisioned Aurora, focusing on cost, latency stability, and maintenance practices for variable workloads.\n\n## Key Concepts\n\n- Aurora Serverless v2 scaling behavior and pause settings\n- RDS Proxy vs PgBouncer for connection pooling\n- Read replicas and failover implications\n- Cost versus latency trade-offs in serverless vs provisioned models\n\n## Code Example\n\n```sql\n-- example query to illustrate typical workload\nSELECT * FROM orders WHERE created_at >= now() - interval '1 day' LIMIT 100;\n```\n\n## Follow-up Questions\n\n- How would you measure and compare P95 latency and cost between the two setups over a 2-week window?\n- What failure modes differ between Serverless v2 and provisioned with read replicas, and how would you test them?","diagram":"flowchart TD\n  ATraffic[Traffic load] --> BOption{Aurora option}\n  BOption --> CServerless[Serverless v2]\n  BOption --> DProvisioned[Provisioned Aurora + Read Replicas]\n  CServerless --> EProxy[RDS Proxy for connections]\n  DProvisioned --> FPgBouncer[PgBouncer pool]\n  CServerless --> GCost[Cost optimization on idle]\n  DProvisioned --> HLatency[Lower controlled latency during peaks]","difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T04:16:21.862Z","createdAt":"2026-01-17T04:16:21.862Z"},{"id":"q-3289","question":"Two-region SaaS with strict tenant data residency: tenants assigned to a region cannot write in the other region, but hot tenants require sub-20ms reads globally. OLTP store is Aurora PostgreSQL and needs to support a single writer region with read replicas in the other region. Design a data layer that enforces per-tenant residency, delivers fast reads, and meets RPO 5s and RTO 60s. Include concrete configurations, data partitioning, routing rules, and a rollback plan for residency violations?","answer":"Route writes by tenant to its designated writer region; use Aurora PostgreSQL Global Database with writer in us-east-1 and a read replica in eu-west-1. Enforce per-tenant residency in the app via a re","explanation":"## Why This Is Asked\nThe question probes practical enforcement of data residency DevOps patterns while maintaining low-latency reads and robust DR in a two-region setup. It tests architectural judgment on cross-region replication, routing, and tenant isolation.\n\n## Key Concepts\n- Aurora PostgreSQL Global Database with a single writer region and cross-region replicas\n- Per-tenant data residency policies and region-aware schema partitioning\n- Read routing to nearest replica to minimize latency\n- RPO/RTO targets and PITR backups in multi-region setups\n\n## Code Example\n```javascript\n// Pseudo routing logic for tenant requests\nfunction routeRequest(tenantId, operation){\n  const region = residencyMap[tenantId];\n  if(operation === 'WRITE' && currentRegion !== region){\n    throw new Error('Tenant not writable in this region');\n  }\n  // route to appropriate endpoint based on operation\n  return regionEndpoints[region][operation];\n}\n```\n\n## Follow-up Questions\n- How would you handle a tenant migrating regions without data loss or RPO impact?\n- How would you validate RPO/RTO during a simulated regional outage and during maintenance windows?","diagram":"flowchart TD\nA[Tenant Residency Policy] --> B[Policy Enforcement]\nB --> C[Writes go to regional writer]\nB --> D[Reads directed to local replica]\nC --> E[Aurora Global DB: writer us-east-1; replica eu-west-1]\nD --> F[Sub-20ms reads for hot tenants]","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T10:30:32.064Z","createdAt":"2026-01-17T10:30:32.064Z"},{"id":"q-3367","question":"In a three-region deployment (us-east-1, eu-west-1, ap-southeast-2) with a single Aurora PostgreSQL Global Database writer in us-east-1, design a 2-tier data path that delivers sub-20 ms reads for hot tenants region-locally while writes can occur anywhere. Propose a cross-region caching strategy using Redis Global Datastore, plus a per-region analytics store. Include concrete configurations (replica counts, Redis node types, TTLs, backup windows, cross-region data sharing, and data residency controls) to meet RPO 5s and RTO 60s?","answer":"To meet RPO 5s and RTO 60s, run Aurora PostgreSQL Global Database with 1 writer in us-east-1 and read replicas in eu-west-1 and ap-southeast-2. Add a Redis Global Datastore: 2 nodes per region (memory","explanation":"## Why This Is Asked\\nTests cross-region data residency and consistency trade-offs under advanced requirements; ensures candidate can architect cross-region caching, RPO/RTO, and data governance.\\n\\n## Key Concepts\\n- Aurora Global Database characteristics (writer region, replication lag)\\n- Redis Global Datastore and cross-region caching\\n- Cache-aside pattern and TTL tuning\\n- Cross-region backups and KMS keys and data residency\\n- CDC to analytics stores and data governance\\n\\n## Code Example\\n```json\n{\n  \\\"redisGlobalDatastore\\\": {\n    \\\"regions\\\": [\\\"us-east-1\\\",\\\"eu-west-1\\\",\\\"ap-southeast-2\\\"],\n    \\\"nodesPerRegion\\\": 2,\n    \\\"ttlSeconds\\\": 300\n  }\n}\n```\n\\n## Follow-up Questions\\n- How would you monitor cross-region replication lag and SLA adherence?\\n- How would you handle a region outage and the impact on RPO/RTO?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T13:43:12.604Z","createdAt":"2026-01-17T13:43:12.604Z"},{"id":"q-3440","question":"Design and compare two data-architecture options for a PCI-DSS-compliant, multi-tenant SaaS using AWS databases: option A uses a single Aurora PostgreSQL cluster with Row-Level Security (RLS) to isolate tenants and KMS-encrypted backups plus cross-region snapshot replication; option B uses separate clusters per tenant across regions. Which approach would you pick and why?","answer":"Recommend a single Aurora PostgreSQL cluster with Row-Level Security (RLS) to isolate tenants, backed by KMS-encrypted backups and cross-region snapshot replication for DR. Compare with per-tenant clu","explanation":"## Why This Is Asked\n\nAssess ability to trade off isolation, cost, and DR in a PCI-DSS context; tests understanding of RLS, encryption, backups, cross-region replication, and migration paths.\n\n## Key Concepts\n\n- Row-Level Security (RLS) in PostgreSQL/Aurora\n- AWS KMS encryption for backups and at-rest data\n- Cross-region snapshot replication and RPO/RTO targets\n- Tenancy isolation trade-offs: single-cluster with shared schema vs per-tenant clusters\n- Migration and operational overhead\n\n## Code Example\n\n```sql\nCREATE POLICY tenant_rls ON public.users\nUSING (tenant_id = current_setting('app.tenant_id')::int);\nALTER ROLE app_user SET app.tenant_id = '1';\n```\n\n```\nNote: This snippet illustrates an RLS policy bound to a per-session tenant_id setting.\n```\n\n## Follow-up Questions\n\n- How would you monitor per-tenant access and detect policy misuse?\n- How would you onboard/offboard tenants with minimal downtime?","diagram":"flowchart TD\n  A[Option A: Single cluster with RLS] --> B[KMS encryption on backups]\n  A --> C[Cross-region snapshot replication]\n  D[Option B: Per-tenant clusters] --> E[Stronger isolation]\n  D --> F[Higher operational overhead]\n  B --> G[DR readiness]\n  C --> H[RPO/RTO targets]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","LinkedIn","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T16:38:56.423Z","createdAt":"2026-01-17T16:38:56.423Z"},{"id":"q-3465","question":"Scenario: A real-time fraud graph workload spans three regions. Primary data sits in Neptune with cross-region replication. Need sub-50ms neighbor lookups on random 1k-node subgraphs; writes across regions; DR targets: RPO 5s, RTO 60s. Design a concrete architecture comparing Neptune Global Database alone vs a hybrid approach using Neptune in each region plus ElastiCache Redis caches and a streaming path to OpenSearch for analytics. Provide concrete config: engine versions, instance counts, TTLs, and explicit failover steps?","answer":"Recommendation: use Neptune Global Database with a single writer in us-east-1 and regional readers in eu-west-1 and ap-south-1. Add ElastiCache Redis per region (2 shards, 3–4 nodes each) caching 1k-n","explanation":"## Why This Is Asked\nTests multi-region graph workloads, consistency, and DR trade-offs between a pure managed graph DB vs a hybrid approach with in-region caching and analytics streaming. It probes Neptune Global Database, Streams, ElastiCache, and OpenSearch integration for real-time surfaces.\n\n## Key Concepts\n- Neptune Global Database cross-region replication and single-writer model\n- Neptune Streams for change data capture\n- ElastiCache Redis caching and per-region invalidation\n- OpenSearch as a downstream analytics sink\n- DR: RPO/RTO targets and failover orchestration\n\n## Code Example\n```javascript\n// Example: Lambda handler for Neptune Stream events to invalidate regional caches\nexports.handler = async (event) => {\n  for (const rec of event.Records) {\n    // parse Neptune stream record\n    // identify affected neighborhood keys and regional caches\n    // publish invalidation to corresponding Redis clusters\n  }\n}\n```\n\n## Follow-up Questions\n- How would you validate RPO/RTO in this architecture? \n- What metrics would you monitor to detect cache invalidation lag and cross-region replication delay?","diagram":"flowchart TD\n  N[Neptune Global DB] --> R[Region Replicas]\n  R --> C[ElastiCache Redis (per region)]\n  C --> O[OpenSearch (Analytics)]\n  N --> S[Neptune Streams]","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Citadel","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T17:30:52.953Z","createdAt":"2026-01-17T17:30:52.953Z"},{"id":"q-3587","question":"Compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables for a read-heavy user profile store spanning us-east-1 and us-west-2. Provide concrete configurations to meet RPO 5s and RTO 60s; detail data model fit, backup plans, and failover steps. Assume a SQL workload with FK constraints and occasional transactional updates; keep latency targets <50ms in primary region?","answer":"Choose Aurora PostgreSQL Global Database for this use case. It maintains relational integrity and supports the complex SQL operations required for user profiles with foreign key constraints. Configure the deployment as follows: primary region us-east-1 with 1 writer instance (db.r5.large) and 2 reader instances, secondary region us-west-2 with 2 reader instances (db.r5.large). Enable cross-region replication with sub-second lag to meet the RPO 5s requirement. Configure automated backups for 35 days with point-in-time recovery enabled. For RTO 60s: implement Aurora's built-in fast failover mechanism combined with Route 53 health checks and automated failover scripts. The relational data model should include normalized tables for user profiles, preferences, and activity logs with proper foreign key constraints. Backup strategy combines daily snapshots, continuous backups, and cross-region backup copies. Failover procedure: 1) Monitor primary region health via Route 53 checks, 2) Automatically promote secondary region to writer upon failure detection, 3) Update DNS records to redirect traffic, 4) Validate connectivity and data consistency. Performance targets: <50ms latency for reads in us-east-1, <100ms for cross-region reads from us-west-2.","explanation":"## Why This Is Asked\n\nThis question evaluates cross-region disaster recovery design for AWS databases and tests the ability to choose between relational and NoSQL solutions based on workload requirements.\n\n## Key Concepts\n\n- Aurora Global Database architecture and replication mechanisms\n- DynamoDB Global Tables and conflict resolution strategies\n- RPO/RTO requirements and corresponding recovery approaches\n- Point-in-time recovery (PITR) and comprehensive backup strategies\n- Cross-region failover automation and DNS management\n- Relational data modeling with foreign key constraints\n- SQL workload optimization and transactional consistency guarantees\n\n## Code Example\n\n```sql\n-- Example: Query user profile with related data\nSELECT u.user_id, u.email, u.created_at,\n       p.theme, p.language, p.notifications,\n       a.last_login, a.activity_count\nFROM users u\nLEFT JOIN user_preferences p ON u.user_id = p.user_id\nLEFT JOIN user_activity a ON u.user_id = a.user_id\nWHERE u.user_id = :user_id;\n```\n\nThis query demonstrates the relational nature of user profile data and the need for JOIN operations that Aurora PostgreSQL handles efficiently, while DynamoDB would require multiple queries or data denormalization.","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:33:42.352Z","createdAt":"2026-01-17T22:36:20.795Z"},{"id":"q-3673","question":"Scenario: You manage an Aurora PostgreSQL Global Database with writer in us-east-1 and read replicas in eu-west-1 and ap-southeast-2. A new tenant requires strict data isolation via Row-Level Security and an auditable analytics path. Design a plan that (a) enforces per-tenant isolation in the global cluster (RLS + SECURITY DEFINER wrappers), (b) maintains cross-region replication with RPO <= 10s while writes land in writer only, (c) captures tenant-access events to S3 with a near-real-time pipeline (DMS) across regions, and (d) specifies concrete DB settings (instance class, replica count, parameter group, backup window, and cross-region KMS keys) to meet an RTO <= 60s. Include concrete configuration choices?","answer":"Implement per-tenant RLS on all tables using a session parameter (app.tenant_id) and a SECURITY DEFINER wrapper to enforce access; use a dedicated tenant role and policy: USING (tenant_id = current_se","explanation":"## Why This Is Asked\nTests ability to design multi-region OLTP with data isolation, auditability, and DR under real-world constraints.\n\n## Key Concepts\n- Aurora PostgreSQL Global Database architecture and cross-region replication timing\n- Row-Level Security in PostgreSQL and security wrappers\n- Change Data Capture with AWS DMS to S3 and analytics lake\n- Data residency and encryption with region-specific KMS keys\n- DR planning: RPO/RTO targets with backups and writes to writer only\n\n## Code Example\n```sql\n-- Enable RLS on a table and enforce tenant access\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_access ON orders\n  FOR ALL USING (tenant_id = current_setting('app.tenant_id')::int);\n\n-- Set tenant for the session\nSET app.tenant_id = '123';\n```\n\n## Follow-up Questions\n- How would you monitor replication lag and auto-tune?\n- How would you validate RLS coverage across regional replicas during failover?","diagram":"flowchart TD\n  A[Writer us-east-1] --> B[Global DB]\n  B --> C[EU-West read replica]\n  B --> D[AP-Southeast read replica]\n  E[Audit pipeline (DMS to S3)] --> F[(Audit data lake)]\n  C --> F\n  D --> F","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Google","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T04:22:02.471Z","createdAt":"2026-01-18T04:22:02.471Z"},{"id":"q-3711","question":"Scenario: Build a mobile analytics backend in us-east-1 handling 1.5k–2k writes/sec and per-user reads for dashboards. Compare DynamoDB with Global Tables vs Aurora MySQL Serverless v2 for this workload. Provide a concrete setup (schema, indexes, backups, DR, failover steps) to meet RPO 5s and RTO 60s, and note consistency and cost trade-offs?","answer":"Leverage DynamoDB Global Tables in us-east-1 and eu-west-1 to meet 5s RPO and 60s RTO. Data model: partition key user_id, sort key event_ts; GSI on event_type; enable on-demand backups and PITR to S3;","explanation":"## Why This Is Asked\n\nTests ability to select between managed AWS databases for a high-ingest, per-user read workload, and to translate SLAs (RPO/RTO) into concrete configurations using familiar features such as Global Tables, PITR, TTL, and streams. It also probes cost considerations and failover testing strategies.\n\n## Key Concepts\n\n- DynamoDB Global Tables for cross-region DR\n- Point-in-Time Restore (PITR) and on-demand backups\n- TTL (time-to-live) and event lifecycle planning\n- DynamoDB Streams as CDC to data lake\n- Aurora Serverless v2 trade-offs (read scaling, cold starts)\n\n## Code Example\n\n```javascript\n// DynamoDB put item example (AWS SDK v3)\nconst item = { user_id: {S: 'u123'}, event_ts: {N: '1700000000'}, payload: {S: '{}'} };\nconst params = { TableName: 'Events', Item: item };\n```\n\n## Follow-up Questions\n\n- How would you validate RPO/RTO in a staged failover test?\n- How would you handle hot partitions or skewed write throughput in DynamoDB?","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","DoorDash","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T06:48:28.893Z","createdAt":"2026-01-18T06:48:28.893Z"},{"id":"q-3772","question":"In a multi-region deployment, need strong OLTP in us-east-1 and real-time analytics in eu-west-1. The dashboards must reflect writes within ~5 seconds with writes allowed in both regions. Design a practical CDC path using AWS tools (DMS vs Debezium), specify data flow, latency targets, conflict handling, backups, and monitoring to meet RPO 5s and RTO 60s?","answer":"Use DMS CDC from Aurora PostgreSQL in us-east-1 to Redshift in eu-west-1, with an initial full load followed by ongoing CDC. Capture via WAL/txn logs, throttle to 5–10s batches, filter to analytic-tab","explanation":"## Why This Is Asked\nAssesses cross-region data pipelines, latency budgets, and DR planning across OLTP and analytics.\n\n## Key Concepts\n- CDC pipelines (DMS vs Debezium)\n- Cross-region latency, RPO/RTO targets\n- Data modeling for analytics vs transactional data\n\n## Code Example\n```javascript\n{\n  \"ReplicationTaskSettings\": \"{\\\"ParallelLoadThreads\\\":4,\\\"BatchApplyEnabled\\\":true}\",\n  \"TableMappings\": \"{\\\"rules\\\":[{\\\"rule-type\\\":\\\"selection\\\",\\\"object-locator\\\":{\\\"schema-name\\\":\\\"public\\\",\\\"table-name\\\":\\\"*\\\"},\\\"rule-action\\\":\\\"include\\\"}] }\"\n}\n```\n\n## Follow-up Questions\n- How would you handle write conflicts if writes occur in both regions simultaneously?\n- What monitoring dashboards and alerts would you configure to ensure RPO/RTO adherence?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Meta","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T08:51:49.792Z","createdAt":"2026-01-18T08:51:49.792Z"},{"id":"q-3816","question":"Two-region SaaS using Aurora PostgreSQL with writes allowed in both regions. Propose a concrete, auditable design to meet RPO 5s and RTO 60s while ensuring tenant data isolation. Compare Aurora PostgreSQL Global Database (single writer) vs independent clusters with bidirectional logical replication (DMS or Debezium), including conflict resolution, auditing, backups, and monitoring. Provide a recommended concrete configuration and rationale?","answer":"Recommendation: two independent Aurora PostgreSQL clusters (us-east-1 and eu-west-1) with bidirectional logical replication (DMS or Debezium) and a tenant-aware conflict policy (per-tenant last-writer","explanation":"## Why This Is Asked\nAuditors and engineers must balance latency, consistency, and auditability in multi-region writes.\n\n## Key Concepts\n- Aurora Global Database vs multi-region writes\n- Conflict resolution strategies in bidirectional replication\n- Tamper-evident auditing with S3 Object Lock and KMS\n- PITR, cross-region backups, and monitoring replication lag\n\n## Code Example\n```bash\n# Example DMS task setup (illustrative)\naws dms create-replication-task --replication-task-identifier bidir-task \\\n  --source-endpoint-arn <src> --target-endpoint-arn <tgt> \\\n  --replication-instance-arn <ri> --migration-type full-load-and-ccdc \\\n  --table-mappings file://mappings.json\n```\n\n## Follow-up Questions\n- How would you handle tenant schema changes mid-flight?\n- What are your observability defaults for replication lag and data freshness?","diagram":"flowchart TD\n  US[US-East Writer] --> REB[Bidirectional WAL Replication]\n  REB --> EU[EU-West Writer]\n  EU --> AUD[Audit Trail to S3 (immutable)]\n  AUD --> MON[Monitoring & Backups]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Databricks","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T10:41:08.899Z","createdAt":"2026-01-18T10:41:08.899Z"},{"id":"q-851","question":"Two-region OLTP SaaS with a single writer in us-east-1 and read replicas in eu-west-1. Compare Aurora Global Database (PostgreSQL) vs DynamoDB Global Tables for this workload: latency targets, consistency model, failover behavior, and cost. Which approach would you pick and why, and what concrete configuration (replica count, failover window, write routing) would you implement to meet RTO < 60s and RPO < 5s?","answer":"Choose Aurora Global Database (PostgreSQL) with a single writer in us-east-1 and one or more read replicas in eu-west-1. It preserves transactional invariants with asynchronous cross-region replicatio","explanation":"## Why This Is Asked\nThis question probes understanding of cross-region OLTP replication choices, consistency, failover, and cost. It contrasts HTAP-like needs with strict ACID guarantees in real-world deployments.\n\n## Key Concepts\n- Aurora Global Database vs DynamoDB Global Tables trade-offs\n- Single-writer constraint and cross-region replication lag\n- RTO/RPO targets, failover orchestration, PITR\n\n## Code Example\n```javascript\naws rds create-global-database --global-database-name my-globaldb --source-db-cluster-identifier mydbcluster\n```\n```\n\n## Follow-up Questions\n- How would you monitor replication lag and what alerts would you set?\n- How would you handle schema migrations with zero downtime across regions?\n","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:32:32.498Z","createdAt":"2026-01-12T13:32:32.498Z"},{"id":"q-871","question":"Migration plan: An OLTP app runs on Aurora PostgreSQL provisioned; traffic is bursty; you want to evaluate Aurora Serverless v2. Provide a concrete plan to migrate, including: (1) start/stop criteria and scaling configuration; (2) handling of long-running transactions and prepared statements; (3) how to keep reads consistent during scaling; (4) testing approach for failover/RTO targets; (5) cost considerations and potential pitfalls with Serverless v2?","answer":"Plan a phased migration to Aurora Serverless v2 from provisioned instances. Use min_capacity 0.5 and max_capacity 16, disable auto_pause initially, route traffic through RDS Proxy, and validate long-r","explanation":"## Why This Is Asked\nServerless migrations are common; evaluate trade-offs.\n\n## Key Concepts\n- Aurora Serverless v2\n- scaling policies\n- transaction semantics\n- prepared statements\n- RDS Proxy\n- failover testing\n\n## Code Example\n```javascript\n// Aurora Serverless v2 config (example)\nconst auroraConfig = {\n  engine: 'aurora-postgresql',\n  scale: { min_capacity: 0.5, max_capacity: 16, auto_pause: false }\n};\n```\n\n## Follow-up Questions\n- How would you monitor connection pool usage during scaling?\n- What are Serverless v2 limitations with prepared statements or long-running queries?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","IBM","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:53:18.115Z","createdAt":"2026-01-12T13:53:18.115Z"},{"id":"q-895","question":"Your multi-region SaaS needs an audit-friendly cross-tenant analytics store with writes transactional in us-east-1 and analytics queries in eu-west-1 under GDPR. Compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables for this workload, focusing on transactional integrity, analytics capability, PITR/retention, cross-region latency, and cost. Recommend a concrete configuration (writer region, replica counts, PITR window, tenant isolation, ETL approach) to meet RPO 15 minutes and RTO 1 hour?","answer":"Aurora PostgreSQL Global Database best meets cross-region transactional integrity with SQL analytics, in a GDPR context. Put writer in us-east-1, two read replicas in eu-west-1; enable PITR 30 days; r","explanation":"## Why This Is Asked\nTests a candidate's ability to balance transactional integrity, cross-region DR, and analytics in a regulated multi-tenant environment.\n\n## Key Concepts\n- Aurora Global Database vs DynamoDB Global Tables\n- PITR, RPO/RTO targets, GDPR/tenant isolation\n- ETL paths to analytics stores (Redshift/Data Lake)\n\n## Code Example\n```javascript\n// Pseudo: configure a DMS task to replicate from us-east-1 to eu-west-1\nconst task = await dms.createReplicationTask({ ... });\n```\n\n## Follow-up Questions\n- How would you handle schema changes across regions without downtime?\n- What telemetry would you collect to validate RPO/RTO in production?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Slack","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T14:36:01.445Z","createdAt":"2026-01-12T14:36:01.445Z"},{"id":"q-956","question":"For a real-time fraud graph application needing sub-100ms neighbor lookups across two AWS regions, compare Amazon Neptune Global Database with DynamoDB (using graph patterns and DAX) for this workload. Writer region us-east-1; readers in eu-west-1; assess graph traversal latency, consistency guarantees, failover behavior, and total cost. Provide a concrete setup (cluster engine and size, replica counts, PITR window, backup schedule, and network/config) to meet an RPO of 5s and an RTO of 60s?","answer":"Choose Neptune Global Database for graph-centric queries. It provides cross-region replication with near real-time reads; DynamoDB+DAX is weaker for complex traversals. Recommend a primary cluster in ","explanation":"## Why This Is Asked\nDiscusses cross-region graph DB choices and DR readiness in practice.\n\n## Key Concepts\n- Neptune Global Database vs DynamoDB/DAX trade-offs\n- Graph traversals, latency budgets, consistency models\n- Cross-region failover, PITR, backups, and cost\n\n## Code Example\n```javascript\n// Illustrative AWS CLI usage (not executed here)\naws neptune create-global-cluster --global-cluster-name FraudGraphGlobal --engine neptune\n```\n\n## Follow-up Questions\n- How would you monitor replication lag and graph query latency?\n- What tests validate RPO/RTO under regional failure?\n","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","NVIDIA","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T16:42:36.355Z","createdAt":"2026-01-12T16:42:36.355Z"},{"id":"q-964","question":"You run an Amazon RDS PostgreSQL in **us-east-1** with automated backups. A regional outage blocks access from that region. How would you achieve **RPO ≤ 60s** and **RTO ≤ 15 minutes** by restoring to **eu-west-1**? Compare cross-region read replicas, backup copy, and Aurora Global Database, and outline concrete steps, knobs, and caveats?","answer":"Prefer Aurora Global Database for true cross-region DR with <60s RPO and <15m RTO. If constrained to RDS PostgreSQL, copy automated backups to eu-west-1 and create a read replica there; promote replic","explanation":"## Why This Is Asked\nDR planning across AWS regions is a core skill. This question tests understanding of RPO/RTO, replication guarantees, and the trade-offs between RDS Cross-Region Replicas, snapshot copying, and Aurora Global Database.\n\n## Key Concepts\n- RPO vs RTO and replication lag\n- Cross-region DR options: RDS read replicas, snapshot copy, Aurora Global DB\n- Failover orchestration and DNS routing\n\n## Code Example\n```javascript\n// AWS CLI example: copy a snapshot to another region\naws rds copy-db-snapshot --source-db-snapshot-identifier arn:aws:rds:us-east-1:123456789012:snapshot:mydb-2026-01-01 --target-db-snapshot-identifier eu-west-1-mydb-2026-01-01 --source-region us-east-1 --region eu-west-1\n\n// Restore a DB instance from the cross-region snapshot\naws rds restore-db-instance-from-db-snapshot --db-instance-identifier eu-west-1-mydb-restored --db-snapshot-identifier eu-west-1-mydb-2026-01-01\n```\n\n## Follow-up Questions\n- How would you automate the failover and DNS switch?\n- What monitoring would you put in place to detect lag and test RPO/RTO?","diagram":"flowchart TD\n A[Source: RDS us-east-1] --> B{Strategy}\n B --> C[Aurora Global Database]\n B --> D[Cross-region backups + EU replica]\n C --> E[Fast RTO, low RPO]\n D --> F[Longer RPO, slower failover]","difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Slack","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T17:26:08.106Z","createdAt":"2026-01-12T17:26:08.106Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Citadel","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":43,"beginner":8,"intermediate":19,"advanced":16,"newThisWeek":43}}