{"questions":[{"id":"aws-database-specialty-deployment-migration-1768189741649-0","question":"You're migrating a 5 TB on-prem MySQL database to Amazon RDS for MySQL with minimal downtime. Which approach best achieves this while preserving data consistency and minimizing downtime?","answer":"[{\"id\":\"a\",\"text\":\"Perform a one-time full backup restore to RDS and switch endpoints during a maintenance window.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use AWS SCT to convert the schema, then run a DMS task with full load plus CDC to migrate data while keeping the sources in sync, and cut over during a brief maintenance window.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Migrate by exporting data to CSV and importing into RDS after downtime.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use AWS DataSync to move data files and then replay changes after cutover.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B provides near-zero downtime by performing a full data load via DMS's CDC-based replication and using SCT to convert the schema, allowing cutover to occur during a brief maintenance window while keeping source and target in sync.\n\n## Why Other Options Are Wrong\n- A: A one-time backup restore requires a longer maintenance window and does not keep ongoing changes synchronized.\n- C: CSV export/import is manual, error-prone, and incurs downtime during data load and reconciliation.\n- D: DataSync moves files, not the database state or incremental changes, so it cannot capture ongoing transactions.\n\n## Key Concepts\n- AWS DMS for continuous data replication during migrations\n- AWS Schema Conversion Tool for schema compatibility\n- Cutover planning to minimize downtime\n- Zero-downtime migration strategies in relational databases\n\n## Real-World Application\nIn production migrations, combine SCT and DMS to minimize downtime, ensure schema compatibility, and perform a controlled cutover during a maintenance window.","diagram":null,"difficulty":"intermediate","tags":["AWS DMS","AWS SCT","Amazon RDS","MySQL","Migration","Terraform","certification-mcq","domain-weight-20"],"channel":"aws-database-specialty","subChannel":"deployment-migration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:49:01.650Z","createdAt":"2026-01-12 03:49:01"},{"id":"aws-database-specialty-deployment-migration-1768189741649-1","question":"For a globally distributed relational database requiring low-latency reads in two regions with fast cross-region failover and a single writer, which AWS solution best meets this requirement?","answer":"[{\"id\":\"a\",\"text\":\"RDS Cross-Region Read Replicas\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Aurora Global Database with cross-region replication\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"DynamoDB Global Tables\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"RDS Multi-AZ with cross-region replication\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because Aurora Global Database provides low-latency reads across regions with a single writer per database and supports fast cross-region failover.\n\n## Why Other Options Are Wrong\n- A: RDS cross-region read replicas are read-only in the secondary region and do not provide a fast global failover solution with a single writer.\n- C: DynamoDB is a NoSQL service and does not provide a PostgreSQL-relational experience; not suitable for this workload.\n- D: RDS Multi-AZ is an in-region HA feature and does not span regions for cross-region failover.\n\n## Key Concepts\n- Aurora Global Database\n- Cross-region replication\n- Single writer architecture\n- Disaster recovery planning\n\n## Real-World Application\nUse Aurora Global Database to serve low-latency reads globally while keeping a single primary writer and enabling regional failover when needed.","diagram":null,"difficulty":"intermediate","tags":["Aurora Global Database","Amazon RDS","PostgreSQL","Cross-Region Replication","Disaster Recovery","certification-mcq","domain-weight-20"],"channel":"aws-database-specialty","subChannel":"deployment-migration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:49:02.038Z","createdAt":"2026-01-12 03:49:02"},{"id":"aws-database-specialty-deployment-migration-1768189741649-2","question":"An organization plans to migrate a transactional PostgreSQL workload from on-premises to AWS and expects highly variable traffic. Which AWS service provides automatic compute scaling with PostgreSQL compatibility while minimizing cost?","answer":"[{\"id\":\"a\",\"text\":\"RDS for PostgreSQL with provisioned instances\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Aurora PostgreSQL with Serverless\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"RDS for PostgreSQL with read replicas in multiple regions\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"DynamoDB\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption B is correct because Aurora PostgreSQL with Serverless automatically scales compute capacity to match variable workloads and is compatible with PostgreSQL.\n\n## Why Other Options Are Wrong\n- A: Provisioned RDS requires manual sizing and does not automatically scale with traffic.\n- C: Read replicas do not handle compute scaling for the primary write workload and are not a scaling solution.\n- D: DynamoDB is a NoSQL service and not PostgreSQL-compatible.\n\n## Key Concepts\n- Aurora Serverless for PostgreSQL\n- Automatic compute scaling\n- PostgreSQL compatibility in Aurora\n\n## Real-World Application\nFor workloads with unpredictable spikes, use Aurora Serverless to minimize costs while maintaining compatibility with PostgreSQL applications.","diagram":null,"difficulty":"intermediate","tags":["Aurora Serverless","Amazon RDS","PostgreSQL","Terraform","certification-mcq","domain-weight-20"],"channel":"aws-database-specialty","subChannel":"deployment-migration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T03:49:02.463Z","createdAt":"2026-01-12 03:49:02"},{"id":"q-851","question":"Two-region OLTP SaaS with a single writer in us-east-1 and read replicas in eu-west-1. Compare Aurora Global Database (PostgreSQL) vs DynamoDB Global Tables for this workload: latency targets, consistency model, failover behavior, and cost. Which approach would you pick and why, and what concrete configuration (replica count, failover window, write routing) would you implement to meet RTO < 60s and RPO < 5s?","answer":"Choose Aurora Global Database (PostgreSQL) with a single writer in us-east-1 and one or more read replicas in eu-west-1. It preserves transactional invariants with asynchronous cross-region replicatio","explanation":"## Why This Is Asked\nThis question probes understanding of cross-region OLTP replication choices, consistency, failover, and cost. It contrasts HTAP-like needs with strict ACID guarantees in real-world deployments.\n\n## Key Concepts\n- Aurora Global Database vs DynamoDB Global Tables trade-offs\n- Single-writer constraint and cross-region replication lag\n- RTO/RPO targets, failover orchestration, PITR\n\n## Code Example\n```javascript\naws rds create-global-database --global-database-name my-globaldb --source-db-cluster-identifier mydbcluster\n```\n```\n\n## Follow-up Questions\n- How would you monitor replication lag and what alerts would you set?\n- How would you handle schema migrations with zero downtime across regions?\n","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:32:32.498Z","createdAt":"2026-01-12T13:32:32.498Z"},{"id":"q-871","question":"Migration plan: An OLTP app runs on Aurora PostgreSQL provisioned; traffic is bursty; you want to evaluate Aurora Serverless v2. Provide a concrete plan to migrate, including: (1) start/stop criteria and scaling configuration; (2) handling of long-running transactions and prepared statements; (3) how to keep reads consistent during scaling; (4) testing approach for failover/RTO targets; (5) cost considerations and potential pitfalls with Serverless v2?","answer":"Plan a phased migration to Aurora Serverless v2 from provisioned instances. Use min_capacity 0.5 and max_capacity 16, disable auto_pause initially, route traffic through RDS Proxy, and validate long-r","explanation":"## Why This Is Asked\nServerless migrations are common; evaluate trade-offs.\n\n## Key Concepts\n- Aurora Serverless v2\n- scaling policies\n- transaction semantics\n- prepared statements\n- RDS Proxy\n- failover testing\n\n## Code Example\n```javascript\n// Aurora Serverless v2 config (example)\nconst auroraConfig = {\n  engine: 'aurora-postgresql',\n  scale: { min_capacity: 0.5, max_capacity: 16, auto_pause: false }\n};\n```\n\n## Follow-up Questions\n- How would you monitor connection pool usage during scaling?\n- What are Serverless v2 limitations with prepared statements or long-running queries?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","IBM","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:53:18.115Z","createdAt":"2026-01-12T13:53:18.115Z"},{"id":"q-895","question":"Your multi-region SaaS needs an audit-friendly cross-tenant analytics store with writes transactional in us-east-1 and analytics queries in eu-west-1 under GDPR. Compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables for this workload, focusing on transactional integrity, analytics capability, PITR/retention, cross-region latency, and cost. Recommend a concrete configuration (writer region, replica counts, PITR window, tenant isolation, ETL approach) to meet RPO 15 minutes and RTO 1 hour?","answer":"Aurora PostgreSQL Global Database best meets cross-region transactional integrity with SQL analytics, in a GDPR context. Put writer in us-east-1, two read replicas in eu-west-1; enable PITR 30 days; r","explanation":"## Why This Is Asked\nTests a candidate's ability to balance transactional integrity, cross-region DR, and analytics in a regulated multi-tenant environment.\n\n## Key Concepts\n- Aurora Global Database vs DynamoDB Global Tables\n- PITR, RPO/RTO targets, GDPR/tenant isolation\n- ETL paths to analytics stores (Redshift/Data Lake)\n\n## Code Example\n```javascript\n// Pseudo: configure a DMS task to replicate from us-east-1 to eu-west-1\nconst task = await dms.createReplicationTask({ ... });\n```\n\n## Follow-up Questions\n- How would you handle schema changes across regions without downtime?\n- What telemetry would you collect to validate RPO/RTO in production?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Slack","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:36:01.445Z","createdAt":"2026-01-12T14:36:01.445Z"},{"id":"aws-database-specialty-management-operations-1768217010401-0","question":"A multinational e-commerce application runs a production RDS MySQL instance in us-east-1 with Multi-AZ and automated backups. A regional disaster takes us-east-1 offline and you must meet an RTO of 15 minutes and an RPO under 5 minutes by running your DR workload in us-west-2. Which approach provides the best cross-region disaster recovery with the stated RPO?","answer":"[{\"id\":\"a\",\"text\":\"Enable Multi-AZ deployment in the primary region\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a cross-region read replica in us-west-2 and promote it on failover\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Rely on automated backups and perform a manual restore in the secondary region\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use DynamoDB Global Tables for this workload\",\"isCorrect\":false}]","explanation":"## Correct Answer\nCreate a cross-region read replica in the DR region and promote it on failover. This provides a dedicated DR copy in another region with asynchronous replication, enabling a controlled cutover within the target RTO and meeting the required RPO if replication lag is within acceptable limits.\n\n## Why Other Options Are Wrong\n- A: Multi-AZ improves high availability within a single region, not cross-region disaster recovery.\n- C: Automated backups in the primary region do not automatically provide a ready-to-use DR instance in another region within the required RPO.\n- D: DynamoDB Global Tables apply to NoSQL workloads, not a relational RDS MySQL workload.\n\n## Key Concepts\n- Cross-region read replicas\n- Disaster recovery (DR) planning\n- RPO and RTO considerations\n\n## Real-World Application\n- Enable cross-region replica in us-west-2 for the primary RDS instance in us-east-1.\n- Regularly test failover by promoting the replica and routing traffic to the DR region.\n- Monitor replication lag and adjust instance sizes to meet latency targets.","diagram":null,"difficulty":"intermediate","tags":["RDS","CrossRegion","HA","DR","MySQL","certification-mcq","domain-weight-18"],"channel":"aws-database-specialty","subChannel":"management-operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:23:30.402Z","createdAt":"2026-01-12 11:23:30"},{"id":"aws-database-specialty-management-operations-1768217010401-1","question":"An RDS PostgreSQL instance in us-east-1 experiences a user deleting a row at 12:03 UTC. You have 7 days of automated backups and point-in-time recovery (PITR) enabled. To recover only the deleted row without affecting other data, which approach is most appropriate?","answer":"[{\"id\":\"a\",\"text\":\"Restore the most recent backup to a new instance and export the deleted row, then import into production\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Perform a PITR to 12:03 UTC on a new temporary instance and copy the row back to production\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Perform a PITR to 12:00 UTC on a new temporary instance and copy the row back to production\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Use a read replica to retrieve the deleted row and apply it to production\",\"isCorrect\":false}]","explanation":"## Correct Answer\nPerform a PITR to 12:00 UTC on a new temporary instance and copy the row back to production. Restoring to a moment before the deletion allows you to recover the specific row in isolation and then apply it to the production database without rolling forward transactions past the deletion.\n\n## Why Other Options Are Wrong\n- A: Restoring the most recent backup may miss the interval before deletion and would require broader data reconciliation.\n- B: Restoring exactly to 12:03 UTC would reintroduce the deleted row at the moment of deletion, not recover it.\n- D: A read replica is typically read-only; it cannot be used to selectively recover a single deleted row without additional steps.\n\n## Key Concepts\n- Point-in-time recovery (PITR)\n- Time-window-based recovery strategy\n- Row-level recovery methods in relational DBs\n\n## Real-World Application\n- Initiate PITR to a timestamp before the incident on a temporary instance.\n- Extract the missing row (e.g., via a bounded SELECT) and apply an INSERT/UPDATE to the production database within a controlled window.\n- Validate data integrity after the patch and monitor for any replication lag or conflicts.","diagram":null,"difficulty":"intermediate","tags":["RDS","PITR","PostgreSQL","Backups","certification-mcq","domain-weight-18"],"channel":"aws-database-specialty","subChannel":"management-operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:23:30.780Z","createdAt":"2026-01-12 11:23:31"},{"id":"aws-database-specialty-management-operations-1768217010401-2","question":"You run an RDS instance encrypted with a customer-managed CMK in AWS KMS. You need to rotate the encryption keys with minimal downtime. Which approach is recommended?","answer":"[{\"id\":\"a\",\"text\":\"Create a new encrypted copy of the DB using a new CMK and switch over\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable automatic rotation of the CMK used to encrypt the RDS instance\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Disable encryption, re-create the DB with a new key, and re-import data\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Take a cross-region encrypted snapshot and restore in the target region\",\"isCorrect\":false}]","explanation":"## Correct Answer\nEnable automatic rotation of the CMK used to encrypt the RDS instance. AWS KMS key rotation for CMKs rotates the cryptographic material without requiring downtime, and RDS-encrypted storage will transparently use the new data keys as keys are rotated.\n\n## Why Other Options Are Wrong\n- A: Re-creating a new encrypted copy can be done, but it incurs downtime and effort; automatic CMK rotation avoids this.\n- C: Decrypting and re-encrypting would require downtime and data movement.\n- D: Cross-region restoration does not address in-place key rotation and adds unnecessary complexity.\n\n## Key Concepts\n- KMS CMK rotation\n- In-place encryption key management\n- Transparent re-encryption with CMK rotation\n\n## Real-World Application\n- Enable automatic CMK rotation on the CMK used by the RDS encryption.\n- Validate that disk encryption and backup/restore paths continue to work post-rotation.\n- Plan a verification run to confirm no performance impact during rotation.","diagram":null,"difficulty":"intermediate","tags":["RDS","KMS","CMK","Encryption","certification-mcq","domain-weight-18"],"channel":"aws-database-specialty","subChannel":"management-operations","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T11:23:31.140Z","createdAt":"2026-01-12 11:23:31"},{"id":"aws-database-specialty-workload-requirements-1768148665414-0","question":"An ecommerce retailer runs an OLTP workload on Amazon RDS for PostgreSQL. During business hours, analytics queries run on the same database, causing CPU spikes and higher latency for transactions. What is the most appropriate architecture to allow analytics without impacting OLTP performance?","answer":"[{\"id\":\"a\",\"text\":\"Create RDS Read Replicas and route analytics queries to the replicas\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Enable Multi-AZ with synchronous replication to the primary for better write durability\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Move analytics to a separate Amazon Redshift cluster and keep OLTP unchanged\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Enable Amazon Aurora Global Database with cross-region writes to support analytics\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct choice is a) Create RDS Read Replicas and route analytics queries to the replicas. This offloads read-heavy analytics to replicas, reducing contention on the primary and preserving OLTP latency.\n\n## Why Other Options Are Wrong\n- b) While Multi-AZ improves availability, it does not offload analytics or reduce OLTP contention, so it doesn't address the primary issue.\n- c) Moving analytics to Redshift is viable but involves ETL and separate data stores; it adds latency and complexity for integrated OLTP analytics and may not preserve transactional latency.\n- d) Aurora Global Database focuses on cross-region disaster recovery and global writes; it isn't the most suitable path to offload analytics from a single primary region.\n\n## Key Concepts\n- OLTP vs OLAP separation\n- Read replicas for read-heavy workloads\n- Asynchronous replication latency considerations\n\n## Real-World Application\n- Implement read replicas for analytics workloads and direct BI queries to replicas; monitor replication lag and ensure ETL pipelines keep data synced to reporting schemas.","diagram":null,"difficulty":"intermediate","tags":["RDS","Aurora","Redshift","OLTP","OLAP","AWS","certification-mcq","domain-weight-26"],"channel":"aws-database-specialty","subChannel":"workload-requirements","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:24:25.415Z","createdAt":"2026-01-11 16:24:25"},{"id":"aws-database-specialty-workload-requirements-1768148665414-1","question":"Which AWS database option best supports a global, low-latency relational workload with cross-region disaster recovery and minimal downtime?","answer":"[{\"id\":\"a\",\"text\":\"DynamoDB Global Tables\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"RDS with cross-region read replicas\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Amazon Aurora Global Database\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Redshift\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct choice is c) Amazon Aurora Global Database. It provides a relational, ACID-compliant engine with a primary region and read-only replicas in other regions for low-latency reads and DR support across regions. It minimizes downtime by enabling rapid disaster recovery and continuity.\n\n## Why Other Options Are Wrong\n- a) DynamoDB Global Tables are multi-region and fast for non-relational workloads but not ideal for traditional relational OLTP schemas.\n- b) RDS with cross-region read replicas can provide some DR but lacks the seamless global read-localization and scale of Aurora Global Database.\n- d) Redshift is analytic-oriented and not suitable for OLTP workload requirements.\n\n## Key Concepts\n- Aurora Global Database for cross-region DR\n- Relational OLTP with global reads\n- Recovery time objectives in multi-region setups\n\n## Real-World Application\n- Use Aurora Global Database to serve global customers with near-local reads; plan failover strategies and DR testing across regions.","diagram":null,"difficulty":"intermediate","tags":["Aurora","RDS","DynamoDB","Global Tables","DR","AWS","certification-mcq","domain-weight-26"],"channel":"aws-database-specialty","subChannel":"workload-requirements","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:24:25.761Z","createdAt":"2026-01-11 16:24:26"},{"id":"aws-database-specialty-workload-requirements-1768148665414-2","question":"A DynamoDB table stores user session events with partition key userId and sort key eventTime. A small subset of users generate events at very high rates, creating hot partitions that throttle throughput. Which design change best distributes load and avoids hot partitions?","answer":"[{\"id\":\"a\",\"text\":\"Add a random prefix (shard) to the partition key to distribute traffic across partitions\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Increase provisioned throughput on the existing partition key to handle bursts\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Migrate to a relational database to manage hot partitions more effectively\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Introduce DynamoDB Streams to throttle write throughput\",\"isCorrect\":false}]","explanation":"## Correct Answer\na) Add a random prefix (shard) to the partition key to distribute traffic across partitions. This mitigates hot partitions by spreading writes across multiple partition keys.\n\n## Why Other Options Are Wrong\n- b) Simply increasing throughput on the same partition key does not solve uneven distribution and keeps the hot-partition risk.\n- c) Migrating to a relational database does not inherently fix partition-level throttling in DynamoDB and adds unnecessary complexity.\n- d) DynamoDB Streams does not throttle writes and is intended for change data capture, not load distribution.\n\n## Key Concepts\n- DynamoDB partition keys and hot partitions\n- Horizontal sharding strategies\n- Throughput management in DynamoDB\n\n## Real-World Application\n- Implement sharding at the partition key layer when encountering hot keys; monitor partition-level utilization and adjust shard count as needed.","diagram":null,"difficulty":"intermediate","tags":["DynamoDB","PartitionKey","Sharding","Throughput","AWS","certification-mcq","domain-weight-26"],"channel":"aws-database-specialty","subChannel":"workload-requirements","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T16:24:26.103Z","createdAt":"2026-01-11 16:24:26"}],"subChannels":["deployment-migration","general","management-operations","workload-requirements"],"companies":["Google","Hugging Face","IBM","Robinhood","Slack","Snowflake","Square","Uber"],"stats":{"total":12,"beginner":0,"intermediate":12,"advanced":0,"newThisWeek":12}}