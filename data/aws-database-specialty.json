{"questions":[{"id":"q-1074","question":"Scenario: A global time-series platform ingests 1M events/hour in us-west-2; dashboards in eu-central-1 and ap-southeast-2 need sub-200ms reads on the latest window. Data must be immutable for 90 days for compliance. Compare DynamoDB Global Tables with DAX vs Aurora PostgreSQL Global Database with cross-region backups. Provide topology, replication, PITR/backup plans, and RPO/RTO targets?","answer":"Choose DynamoDB Global Tables in three regions (us-west-2, eu-central-1, ap-southeast-2) with DAX caching per region and multi-region writes. Enable PITR for 35 days and S3 immutable archives for 90 d","explanation":"## Why This Is Asked\n\nTests cross-region replication, latency trade-offs, and DR design between NoSQL and relational engines.\n\n## Key Concepts\n- Global Tables vs Global Database replication\n- Read latency and consistency models\n- PITR and cross-region backups\n- Archival and compliance (S3 Object Lock)\n\n## Code Example\n\n```javascript\n// Example: pseudo-endpoint selection logic for region failover\n```\n\n## Follow-up Questions\n- How would you validate RPO/RTO in a simulated outage?\n- What monitoring would you implement to detect replication lag across regions?","diagram":"flowchart TD\nA[Ingest] --> B[Global Tables: us-west-2, eu-central-1, ap-southeast-2]\nB --> C[DAX per region]\nA --> D[S3 immutable archive (90d)]\nC --> E[Dashboards in region]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:33:34.955Z","createdAt":"2026-01-12T21:33:34.955Z"},{"id":"q-1131","question":"**Hybrid Analytics Path for Multiregion Aurora**\n\nYou're running an Aurora PostgreSQL OLTP cluster with tenant isolation via RLS in us-east-1. A regulatory BI team in eu-west-1 requires near real-time analytics with masked PII. Design a hybrid analytics path using Aurora Global Database for OLTP replicas and a CDC-based analytic store (Redshift or DynamoDB+Lambda) in eu-west-1. Describe data flow, masking strategy, encryption, failover, and how to meet RPO 5s and RTO 60s, including cost considerations?","answer":"Run OLTP in Aurora PostgreSQL with RLS isolation in us-east-1. Replicate via Aurora Global Database to eu-west-1. Ingest CDC to a masked analytics store (Redshift or DynamoDB) in eu-west-1; apply per-","explanation":"## Why This Is Asked\nTests cross-region replication, hybrid OLTP/OLAP, security, and DR.\n\n## Key Concepts\n- Aurora Global Database, RLS, CDC, masking, cross-region KMS, hot failover\n- DR: RPO 5s, RTO 60s\n- Cost considerations: hot standby vs on-demand\n\n## Code Example\n```javascript\n// AWS CLI example (conceptual)\naws dms create-replication-task --replication-task-identifier cbd-task --source-endpoint-arn <src> --target-endpoint-arn <tgt> --migration-type full-load-and-cdc\n```\n\n## Follow-up Questions\n- How would you validate masking correctness without exposing PII?\n- What metrics indicate replication lag violations and how to remediate?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Goldman Sachs","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:22:46.568Z","createdAt":"2026-01-13T01:22:46.568Z"},{"id":"q-1279","question":"In a multi-tenant SaaS on AWS, run a single Aurora PostgreSQL cluster with per-tenant schemas and RLS to isolate data. An analytics team in eu-west-1 requires cross-tenant BI with masked PII in near real-time dashboards. Design a cost-aware architecture that delivers masking, auditing, and SLA, comparing per-tenant schemas in a single cluster vs separate clusters per tenant. Include data flow, backup, and failover?","answer":"Adopt a hybrid: maintain one Aurora PostgreSQL cluster with per-tenant schemas and RLS for isolation; expose masked analytic views for BI from a dedicated eu-west-1 read replica. Use a CDC pipeline to","explanation":"## Why This Is Asked\nThis question probes practical multi-tenant isolation, cross-region analytics, and governance trade-offs in AWS databases.\n\n## Key Concepts\n- Row-level security and per-tenant schemas\n- Cross-region BI with masked analytics\n- CDC pipelines to analytics stores\n- Audit, backup (PITR), and cost governance\n- Single-cluster vs multi-cluster trade-offs\n\n## Code Example\n```sql\n-- Enable RLS on a tenant table\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_rls ON orders\n  USING (tenant_id = current_setting('my.tenant_id')::int);\n```\n\n## Follow-up Questions\n- How would you implement masking for PII in the analytics store without leaking through cached results?\n- How would you monitor latency, replication lag, and cost to meet SLA?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Hugging Face","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:44:04.097Z","createdAt":"2026-01-13T07:44:04.097Z"},{"id":"q-1303","question":"In a multi-tenant SaaS using Aurora PostgreSQL with Global Database spanning us-west-2 and us-east-1, tenants must have isolated data access and BI dashboards must mask PII in real time. Propose an end-to-end design using per-tenant RLS, dynamic masking for BI, and a separate analytics store fed by CDC (DMS/Debezium). Include cross-region DR with RPO <5s and RTO <60s, data flow, encryption, backups, and a concrete sizing plan (replicas, window, network)?","answer":"Leverage Aurora PostgreSQL with per-tenant RLS and dynamic BI masking, plus a CDC pipeline (DMS/Debezium) feeding a dedicated analytics store (Redshift or DynamoDB+ Lambda) in the secondary region. Us","explanation":"## Why This Is Asked\nTests ability to architect multi-tenant isolation, real-time masking, and cross-region DR.\n\n## Key Concepts\n- Aurora PostgreSQL with Global Database\n- Row-Level Security and dynamic masking\n- CDC pipelines (DMS/Debezium)\n- Analytics stores (Redshift, DynamoDB)\n- DR targets (RPO/RTO), encryption, backups\n\n## Code Example\n```javascript\n-- SQL for RLS policy (illustrative)\nALTER TABLE events ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_isolation ON events\n  USING (tenant_id = current_setting('app.tenant_id')::int)\n  WITH CHECK (tenant_id = current_setting('app.tenant_id')::int);\n```\n\n## Follow-up Questions\n- How would you test tenant isolation in the analytics path?\n- What changes if BI dashboards must also support cross-tenant rollups?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Lyft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:50:50.803Z","createdAt":"2026-01-13T08:50:50.803Z"},{"id":"q-1314","question":"Design a GDPR-compliant data deletion strategy for a multi-region Aurora PostgreSQL Global Database that uses us-east-1 as the writer and replicas in multiple regions. How would you implement Right-to-Erasure for tenant data, propagate deletions with minimal latency, handle referential integrity, and maintain an auditable trail while meeting RPO/RTO targets? Include practical steps and trade-offs?","answer":"Adopt soft deletes with a tenant-scoped deleted_at flag and propagate deletions via logical replication or DMS CDC to all regions. Enforce FK cascades in the origin region only; keep a purge window (e","explanation":"## Why This Is Asked\nData privacy and cross-region deletion are common but tricky with global databases. The answer tests practical strategies for timing, integrity, and audit.\n\n## Key Concepts\n- GDPR deletion rights and retention policies\n- Aurora Global Database cross-region replication\n- Soft vs hard deletes and foreign-key considerations\n- Auditability and compliance tracking\n\n## Code Example\n```\nALTER TABLE tenants ADD COLUMN deleted_at TIMESTAMPTZ;\nCREATE VIEW active_items AS SELECT * FROM items WHERE deleted_at IS NULL;\nUPDATE items SET deleted_at = NOW() WHERE id = ?;\n```\n\n## Follow-up Questions\n- How would you test cross-region deletion latency and audit integrity?\n- How to handle legal-hold scenarios and purge timing?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hashicorp","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T11:28:34.774Z","createdAt":"2026-01-13T11:28:34.776Z"},{"id":"q-1382","question":"In an Aurora PostgreSQL Global Database with a writer in us-east-1 and replicas in eu-west-1, a financial balance update must be atomic across regions. Explain why cross-region distributed transactions are not supported and propose a practical pattern to achieve atomic-ish behavior with low latency, including data flow, failover handling, and cost/latency trade-offs?","answer":"Cross-region atomicity isn’t supported: Aurora Global Database uses asynchronous cross-region replication, so a two-stage commit across regions can’t be guaranteed. A practical pattern is a single-wri","explanation":"## Why This Is Asked\n\nTests understanding of cross-region replication limits in Aurora Global Database and practical patterns to achieve atomic-like behavior without distributed transactions.\n\n## Key Concepts\n\n- Aurora Global Database replication model and its eventual cross-region consistency\n- ACID vs eventual consistency in multi-region setups\n- Single-writer boundary, event-driven replication, compensating actions, and idempotent processing\n\n## Code Example\n\n```javascript\n// Pseudo-implementation: single-writer boundary with event emission\nasync function updateBalance(accountId, delta) {\n  // Begin in writer region\n  await beginLocalTx();\n  await updateLocalBalance(accountId, delta);\n  await commitLocalTx();\n  // Publish event to eu-west-1 for downstream update\n  await publishEvent({ accountId, delta, txnId: generateId() });\n}\n```\n\n## Follow-up Questions\n\n- How would you test this pattern under network partitions?\n- What latency and cost implications arise from cross-region event streams and reconciliation?","diagram":"flowchart TD\n  A[Client Request] --> B[Coordinator (us-east-1)]\n  B --> C[Prepare: Update primary balance]\n  C --> D[Commit: Emit event to eu-west-1]\n  D --> E[Apply delta in eu-west-1]\n  E --> F[Response to client]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T14:45:15.419Z","createdAt":"2026-01-13T14:45:15.419Z"},{"id":"q-1441","question":"In Aurora PostgreSQL (us-east-1) with tenant isolation via RLS, design a near real-time analytics path for a eu-west-1 consumer needing masked data and <5s lag. Use Aurora Global Database for OLTP and a CDC store in eu-west-1 (Redshift or DynamoDB+Lambda). Explain data flow, masking/encryption, consistency, failover, and cost with concrete config sketches?","answer":"Use Aurora Global Database to keep a writer in us-east-1 with cross-region replicas; route analytic reads to eu-west-1 via DMS CDC to Redshift or DynamoDB+Lambda. Implement per-tenant masking in the p","explanation":"## Why This Is Asked\nTests ability to design cross-region analytics paths, balancing data masking, security, latency, failover, and cost for a regulated multi-tenant SaaS using AWS DB services.\n\n## Key Concepts\n- Aurora Global Database cross-region replication\n- CDC options: DMS vs Debezium vs native\n- Data masking: per-tenant with RLS and column-level masking\n- Encryption: KMS, TLS, envelope encryption\n- Failure scenarios: failover/failback and RPO/RTO\n- Cost trade-offs: Redshift vs DynamoDB, compute/storage\n\n## Code Example\n```javascript\n// Example: publication for CDC (PostgreSQL)\nALTER SYSTEM SET wal_level = logical;\nCREATE PUBLICATION analytics_pub FOR TABLE orders, customers;\n```\n\n## Follow-up Questions\n- How would you validate lag and data completeness end-to-end?\n- How would you enforce per-tenant masking in the analytics store?","diagram":"flowchart TD\n  A[OLTP writer us-east-1] --> B[Global DB replica us-east-1]\n  A --> C[DMS CDC in eu-west-1? (for analytics store)]\n  B --> D[Analytics store in eu-west-1]\n  D --> E[Analytics consumer in eu-west-1]","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T17:03:20.591Z","createdAt":"2026-01-13T17:03:20.592Z"},{"id":"q-1447","question":"You run a high-volume ecommerce on Aurora PostgreSQL Global Database with a single writer in us-east-1 and read replicas in eu-west-1. An outage in us-east-1 requires routing writes to eu-west-1 within 60s while ensuring RPO<5s, idempotent writes, and no double billing. Design the architecture and concrete configuration (replica counts, failover procedures, analytics CDC path, masking, PITR) to meet these goals?","answer":"Use Aurora Global Database with a hot-standby writable clone in eu-west-1 for DR so writes can continue within 60s of a us-east outage. Route writes to the EU writer via Route 53 health checks; keep a","explanation":"## Why This Is Asked\n\nAssesses cross-region DR planning for a mission-critical OLTP with minimal data loss and downtime, plus robust write-idempotency and analytics integration.\n\n## Key Concepts\n\n- Aurora Global Database architecture and cross-region failover considerations\n- DR targets: RPO < 5s, RTO < 60s with hot-standby writable clone\n- Idempotent write patterns: transaction IDs, upserts to prevent double billing\n- CDC to analytics (DMS) and data masking at read time\n- PITR retention planning and backup strategies\n\n## Code Example\n\n```sql\n-- idempotent upsert pattern for an order\nINSERT INTO orders (order_id, customer_id, amount, txn_id)\nVALUES (:order_id, :customer_id, :amount, :txn_id)\nON CONFLICT (order_id) DO UPDATE\n  SET amount = EXCLUDED.amount, updated_at = NOW(), txn_id = EXCLUDED.txn_id;\n```\n\n## Follow-up Questions\n\n- How would you validate RPO/RTO in production and what telemetry would you collect?\n- How would you test and verify the cross-region failover without impacting live traffic?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T17:43:10.564Z","createdAt":"2026-01-13T17:43:10.564Z"},{"id":"q-1550","question":"In a two-region deployment with a single writer in us-east-1 and analytic reads in eu-west-1, design a CDC pipeline to keep a near real-time analytic store updated within 5 seconds of commits, while masking per-tenant data and enforcing encryption at rest and in transit. Compare AWS DMS, Debezium/Kafka, and native Aurora logical replication, and provide concrete configuration (engine, instance types, replica counts, PITR, KMS keys, VPC endpoints, and network topology) to meet RPO 5s and RTO 60s?","answer":"Deploy an Aurora Global Database with the primary writer in us-east-1 and implement a CDC pipeline using AWS DMS to maintain a near real-time analytic store in eu-west-1. Configure DMS with change data capture to stream committed changes within 5 seconds, applying per-tenant data masking and enforcing encryption both in transit and at rest. AWS DMS provides operational simplicity with managed infrastructure, meeting the RPO of 5 seconds and RTO of 60 seconds while ensuring data security and compliance requirements.","explanation":"## Why This Is Asked\nTests ability to design cross-region CDC pipelines with strict latency targets while implementing tenant-level data masking and comprehensive encryption. It also evaluates knowledge of live replication technologies and their operational trade-offs.\n\n## Key Concepts\n- Change data capture latency across AWS regions\n- Cross-region data movement options (AWS DMS vs Debezium/Kafka vs native Aurora logical replication)\n- Per-tenant data masking and encryption strategies\n- Network topology and VPC endpoint configuration\n- Disaster recovery objectives: RPO/RTO, PITR, and failover procedures\n\n## Code Example\n```sql\n-- On writer: publish all tables for CDC\nCREATE PUBLICATION cdc_pub FOR ALL TABLES;\n```\n\n## Follow-up Questions\n- How would you handle failover scenarios and ensure data consistency?\n- What monitoring and alerting would you implement for the CDC pipeline?\n- How would you optimize costs while maintaining the 5-second latency requirement?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","DoorDash","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:30:05.991Z","createdAt":"2026-01-13T21:38:49.169Z"},{"id":"q-1747","question":"You run a multi-region SaaS with Aurora PostgreSQL as the OLTP in us-east-1 and read replicas in eu-west-1. A new requirement enforces strict per-tenant data isolation via Row-Level Security and data residency controls for backups. Design a concrete approach: RLS policy skeletons for all tables, session-based tenant_id from authentication, per-tenant restore strategy, cross-region backup copy schedule, and a testing/validation plan that proves no cross-tenant leakage under burden. Include concrete config knobs and a sample policy?","answer":"Implement per-tenant isolation with Postgres Row-Level Security on all tables, driven by a session tenant_id set from the user’s JWT. Use a separate role per tenant and policies like: USING (tenant_id","explanation":"## Why This Is Asked\nTests ability to implement robust data isolation with RLS, ensure data residency through cross-region snapshots, and design test plans for high-concurrency workloads.\n\n## Key Concepts\n- PostgreSQL Row-Level Security (RLS) on all tables\n- session context via SET myapp.tenant_id from authentication\n- cross-region Aurora backups and snapshot copies\n- PITR windows and data residency/compliance\n- tenant onboarding/offboarding and data retention controls\n\n## Code Example\n```sql\n-- Example RLS skeleton\nCREATE POLICY tenant_rls ON users\nFOR ALL USING (tenant_id = current_setting('myapp.tenant_id')::int)\nWITH CHECK (tenant_id = current_setting('myapp.tenant_id')::int);\n\nALTER TABLE users ENABLE ROW LEVEL SECURITY;\nALTER TABLE users FORCE ROW LEVEL SECURITY;\n```\n\n## Follow-up Questions\n- How would you automate per-tenant onboarding to ensure tenant_id is set on every DB connection?\n- What metrics would you monitor to detect leakage or policy misconfigurations?","diagram":"flowchart TD\n  C[Client Request] --> APP[App Layer]\n  APP --> U[Aurora US-East OLTP]\n  U --> EU[EU-West Replica]\n  APP --> S[Snapshot Copy to EU-West]\n  S --> D[Data Residency & Compliance]\n  D --> M[Monitoring & Alerts]","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T09:37:30.426Z","createdAt":"2026-01-14T09:37:30.428Z"},{"id":"q-1775","question":"In a multi-region Aurora PostgreSQL Global Database setup (writer in us-east-1; readers in eu-west-1 and ap-south-1) with strict tenant-level data residency, design a scalable architecture that provides sub-50ms reads for hot paths in each region while ensuring RPO <= 5s and RTO <= 60s, using row-level security and a CDC-based analytic store; explain data partitioning, access controls, and failover strategy, plus cost trade-offs?","answer":"Use region-scoped tenant sharding with Postgres RLS to enforce residency; writer in us-east-1 with Aurora Global Database and replicas in eu-west-1/ap-south-1 for sub-50ms regional reads. Use CDC from","explanation":"## Why This Is Asked\nAssesses ability to architect cross-region, residency-bound databases with real-time analytics, balancing latency, consistency, and cost.\n\n## Key Concepts\n- Aurora Global Database and cross-region replication\n- Row-Level Security (RLS) for tenant isolation\n- CDC-based analytic store (Redshift/S3+Glue)\n- RPO/RTO design and failover strategy\n- Cost trade-offs: replication, egress, storage, and analytics\n\n## Code Example\n```sql\n-- Example RLS policy\nCREATE POLICY tenant_rls ON orders\n  USING (tenant_id = current_setting('myapp.tenant_id')::int);\n```\n\n## Follow-up Questions\n- How would you validate RPO/RTO in a disaster scenario?\n- What strategies minimize cross-region CDC costs while preserving freshness?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T10:39:05.726Z","createdAt":"2026-01-14T10:39:05.727Z"},{"id":"q-1882","question":"Design a cross-region analytic path for a SaaS app with an Aurora PostgreSQL OLTP cluster in us-east-1 as the single writer and regional read replicas in us-west-2. The goal: near real-time analytics with masked PII in the analytics store. Propose a CDC-based pipeline (Aurora CDC, DMS, Debezium, or Kinesis) to load into Redshift or DynamoDB in us-west-2, choose masking strategy, encryption, data freshness target (RPO), failover plan, and cost considerations. Include concrete config choices (instance types, retention, network, and security)?","answer":"Use Aurora Global Database to replicate OLTP across regions, and implement CDC from the writer to a masked analytics store in the remote region (Redshift via DMS or DynamoDB via Kinesis). Mask PII at ","explanation":"## Why This Is Asked\nAssess cross-region data flow, masking, and cost-aware analytics separation. It tests practical CDC choices, DR timing, and security implications.\n\n## Key Concepts\n- Aurora Global Database, CDC, DMS/Debezium, Redshift Spectrum, DynamoDB Streams\n- Data masking, encryption at rest/in transit, RPO/RTO targets\n- Cross-region networking and cost optimization\n\n## Code Example\n```sql\n-- Example masking policy sketch (pseudo)\nCREATE POLICY mask_ssn ON customers\nAS (SELECT mask_ssn(ssn) AS ssn_masked);\n```\n\n## Follow-up Questions\n- How would you test RPO/RTO guarantees in this pipeline?\n- What are failure modes if CDC lag increases beyond threshold?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T15:42:33.696Z","createdAt":"2026-01-14T15:42:33.696Z"},{"id":"q-2139","question":"You run OLTP in Aurora PostgreSQL us-east-1 and need near real-time BI in us-west-2. Design a CDC pipeline: enable a dedicated logical replication slot in Aurora; use DMS in CDC mode to stream changes to Redshift in us-west-2 via a staging S3 bucket; apply PII masking at BI layer; enable PITR and cross-region backups; target end-to-end latency ~2s and RTO <60s. Include data flow, failover, and cost trade-offs?","answer":"Enable a dedicated Aurora logical replication slot in us-east-1; route changes through DMS in CDC mode to Redshift in us-west-2 (via S3 staging). Use MERGE-based upserts for idempotence. Apply PII mas","explanation":"## Why This Is Asked\nTests real-time cross-region CDC design with security, masking, and failover.\n\n## Key Concepts\n- Aurora logical replication\n- AWS DMS CDC\n- Redshift ingestion from S3\n- Data masking and RLS\n- PITR and cross-region backups\n- Cost trade-offs\n\n## Code Example\n```json\n{\n  \"DMSTaskSettings\": {\"TargetTablePrepMode\":\"DO_NOTHING\",\"FullLoadTask\": false,\"CdcInsertsOnly\": false}\n}\n```\n\n## Follow-up Questions\n- How to handle schema changes downstream?\n- What metrics validate 2s latency under load?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:19:48.906Z","createdAt":"2026-01-15T04:19:48.906Z"},{"id":"q-2163","question":"A multi-region SaaS app needs sub-20ms reads for hot tenants across three continents, writes allowed in any region, and PCI-DSS data residency constraints. Design a data-layer using AWS: compare Aurora PostgreSQL Global Database with DynamoDB Global Tables plus CDC to an analytics store, including consistency, DR, backups, and concrete configurations to meet RPO 5s and RTO 60s?","answer":"Option A: Aurora PostgreSQL Global Database with a single writer in us-east-1 and regional read replicas in eu-west-1 and ap-south-1; 7-day PITR, KMS encryption, TLS, and Route 53 latency routing to m","explanation":"## Why This Is Asked\nTests cross-region DR, read latency, and data residency decisions for AWS database services.\n\n## Key Concepts\n- Aurora Global Database constraints: single writer, cross-region replication\n- DynamoDB Global Tables: multi-region writes with replication and eventual consistency trade-offs\n- Data residency and PCI-DSS: encryption (at rest/in transit), KMS keys, IAM access controls\n- DR planning: RPO/RTO targets, PITR, backups, failover orchestration\n\n## Code Example\n```javascript\n// CDK sketch for core Aurora Global DB settings (conceptual)\nconst glb = new aurora.GlobalDatabase(this, 'GlobalDb', {\n  writerRegion: 'us-east-1',\n  regions: ['us-east-1', 'eu-west-1', 'ap-south-1'],\n  engine: aurora.PostgresEngineVersion.VER_13_6,\n});\n```\n\n## Follow-up Questions\n- If latency budgets tighten, how would you restructure reads?\n- How would you monitor cross-region replication lag and auto-tune write routing?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:41:48.135Z","createdAt":"2026-01-15T05:41:48.135Z"},{"id":"q-2211","question":"Two-region, multi-tenant SaaS with strict data residency: EU tenants' data must stay in EU, US tenants' data in US. Needs sub-15ms reads for hot tenants, writes in any region, and cross-region analytics. Compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables with analytics options; provide a concrete topology, replication, backups, and DR plan to meet RPO 5s and RTO 60s, including per-tenant routing and residency enforcement?","answer":"Use Aurora PostgreSQL Global Database across EU and US with per-tenant residency enforced by Row-Level Security; hot reads served from a regional cache (DynamoDB or MemoryDB) and analytics from a data","explanation":"## Why This Is Asked\nTests ability to design multi-region, tenancy-aware architectures with DR and residency constraints; assesses knowledge of Aurora Global Database, RLS, read caching, analytics integration, and cost/latency trade-offs.\n\n## Key Concepts\n- Data residency with row-level security (RLS) in PostgreSQL\n- Aurora Global Database topology across regions\n- Cross-region replication and WAL shipping mechanics\n- Read caching and analytics integration (DynamoDB/MemoryDB, S3/Glue)\n\n## Code Example\n```sql\n-- Enable per-tenant isolation via RLS\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_isolation ON orders\n  USING (tenant_id = current_setting('app.tenant_id')::int);\n```\n\n## Follow-up Questions\n- How would you validate DR failover latency and RPO under peak load?\n- What monitoring and cost controls would you implement to maintain SLAs across regions?","diagram":"flowchart TD\n  EU_Tenant[EU region data store] --> EU_Reads[EU read replicas]\n  US_Tenant[US region data store] --> US_Reads[US read replicas]\n  EU_Reads --> AnalyticsEU[Analytics]\n  US_Reads --> AnalyticsUS[Analytics]\n  AnalyticsEU --> Lake[Analytics Lake]\n  AnalyticsUS --> Lake","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Salesforce","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T07:40:12.465Z","createdAt":"2026-01-15T07:40:12.466Z"},{"id":"q-2248","question":"A SaaS news site logs user events at ~1000 writes/s, with dashboards needing sub-200 ms reads. Data is append-only; hot data kept 30 days, archived after. Compare DynamoDB (on-demand, TTL, GSI) vs Aurora PostgreSQL (partitioned tables, read replicas) for this workload. Provide concrete configs (primary key design, indexes, TTL window, backup/retention, RPO/RTO) and justify choice?","answer":"Recommended: DynamoDB on-demand with a TTL attribute of 90 days, primary key (user_id HASH, event_ts RANGE), optional GSI on event_type for analytics, enable Streams for CDC, and PITR. Data is append-","explanation":"## Why This Is Asked\nTests ability to pick between a serverless NoSQL and a relational option for high-velocity, append-only data with TTL and analytics needs. Assesses data modeling, retention strategy, and DR considerations.\n\n## Key Concepts\n- Append-only data modeling for time-series-like logs\n- Throughput modes: on-demand vs provisioned capacity\n- TTL data pruning and retention windows\n- Read latency strategies (indexes, caching, Streams)\n- Backups, PITR, and DR differences between DynamoDB and Aurora\n\n## Code Example\n```sql\nCREATE TABLE event_logs (\n  user_id VARCHAR(36) NOT NULL,\n  event_ts BIGINT NOT NULL,\n  payload JSON,\n  PRIMARY KEY (user_id, event_ts)\n) PARTITION BY RANGE (event_ts);\n```\n\n```bash\naws dynamodb create-table --table-name UserEventLog \\\n  --attribute-definitions AttributeName=user_id,AttributeType=S AttributeName=event_ts,AttributeType=N \\\n  --key-schema AttributeName=user_id,KeyType=HASH AttributeName=event_ts,KeyType=RANGE \\\n  --billing-mode PAY_PER_REQUEST \\\n  --stream-specification StreamEnabled=true,StreamViewType=NEW_IMAGE\n```\n\n```bash\naws dynamodb update-time-to-live --table-name UserEventLog --time-to-live-specification Enabled=true,AttributeName=ttl\n```\n\n## Follow-up Questions\n- What are the trade-offs of not using TTL on DynamoDB for hot data?\n- How would you validate data durability and latency under bursty traffic?","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Square","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T09:04:51.962Z","createdAt":"2026-01-15T09:04:51.962Z"},{"id":"q-2268","question":"Your app uses AWS Lambda functions that connect to an RDS PostgreSQL instance; during bursts, you see many connections causing failures. How would you leverage Amazon RDS Proxy to manage connections, configure auth, and ensure stable performance? Include what to monitor, any pricing considerations, and a basic setup outline?","answer":"Use RDS Proxy to pool connections and decouple Lambda bursts from DB connection limits. Create a private RDS Proxy in the same VPC, target the RDS instance, store credentials in Secrets Manager, attac","explanation":"This question tests practical understanding of connection management for serverless apps. Candidates should cite: where to place the proxy, how credentials are supplied, how to route Lambda traffic, what to monitor (proxy health, connection count, latency), and cost trade-offs. They should mention security group rules and multi-AZ considerations.","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Goldman Sachs","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T09:50:32.865Z","createdAt":"2026-01-15T09:50:32.865Z"},{"id":"q-2320","question":"Scenario: A SaaS app stores event data in DynamoDB and must retain 90 days in DynamoDB and archive older events to S3 for analytics. Design a pragmatic lifecycle: data model, TTL, export to S3, per-tenant/year/month partitioning, storage class selection, and validation plan to ensure data integrity and queryability via Athena. Include concrete steps and considerations for cost?","answer":"Two-tier approach: keep hot data for 90 days in DynamoDB with TTL on expireAt; archive older events to S3 per tenant/year/month using DynamoDB export-to-S3 (or Glue), compress to Parquet, and apply li","explanation":"## Why This Is Asked\nValidates practical data lifecycle design, cost awareness, and AWS integration.\n\n## Key Concepts\n- DynamoDB TTL on expireAt\n- DynamoDB export-to-S3 or Glue-based export\n- S3 Lifecycle and storage classes (Standard/IA/Glacier)\n- Per-tenant partitioning in S3 (tenant/year/month)\n- Athena/Glue for analytics on archived data\n\n## Code Example\n```javascript\n// TTL example snippet (not production-ready)\nconst item = { tenantId:'T1', eventId:'E123', ts:Date.now(), expireAt: Math.floor(Date.now()/1000) + 90*24*3600 };\n```\n\n## Follow-up Questions\n- How would you adjust for cross-region access to archived data?\n- What are trade-offs of online TTL vs scheduled archive windows?","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T11:49:47.720Z","createdAt":"2026-01-15T11:49:47.720Z"},{"id":"q-2331","question":"For a multi-tenant SaaS app storing per-tenant PII with EU/US data residency, compare Aurora PostgreSQL with Row-Level Security (RLS) vs DynamoDB with per-tenant access patterns. Explain schema design, replication strategy, consistency, DR, and cost. Provide a concrete configuration to meet RPO < 5s and RTO < 60s, including region placement, replica counts, backup windows, and key management?","answer":"Recommended approach: Aurora PostgreSQL Global Database with per-tenant RLS and cross-region replicas. Writer in us-east-1; replicas in eu-west-1 and ap-south-1; enable PITR for 35 days and automated ","explanation":"## Why This Is Asked\nEvaluates RBAC at the database layer, cross-region data residency, and trade-offs between relational and NoSQL models in multi-tenant SaaS.\n\n## Key Concepts\n- Row-Level Security (RLS) in PostgreSQL\n- DynamoDB conditional writes and per-tenant design\n- Cross-region replication, PITR, and RTO/RPO targets\n\n## Code Example\n\n```javascript\nCREATE POLICY tenant_access ON users\nFOR ALL USING (tenant_id = current_setting('tenants.current')::int);\n```\n\n## Follow-up Questions\n- How would you audit per-tenant data access across regions?\n- What metrics would you monitor to detect RBAC misconfigurations?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Plaid","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T13:07:36.036Z","createdAt":"2026-01-15T13:07:36.036Z"},{"id":"q-2411","question":"In an IoT platform, 100k writes/sec of time-series data arrive from devices worldwide. You must ingest region-locally with sub-20ms latency, perform near real-time analytics in a separate region, retain 30 days of data, and ensure PCI-DSS residency. Compare AWS Timestream, DynamoDB Global Tables with a CDC pipeline, and an Aurora-based time-series schema. Propose architecture, data model, retention, DR, and concrete config to meet RPO 5s and RTO 60s?","answer":"DynamoDB Global Tables with regional writes, plus DynamoDB Streams feeding a near-real-time analytics path (Kinesis→S3/Redshift). For PCI residency, encrypt at rest with a restricted KMS CMK and stric","explanation":"## Why This Is Asked\nTests multi-region data ingestion, analytics separation, retention, and compliance trade-offs with real workloads.\n\n## Key Concepts\n- DynamoDB Global Tables, DynamoDB Streams, KMS\n- Data residency for PCI-DSS, PITR, TTL\n- Time-series patterns, retention strategies, and analytics integration\n\n## Code Example\n```bash\n# Create a Global Table (example flag values for illustration)\naws dynamodb create-global-table --table-name IoTTimeSeries \\\n  --region us-east-1 us-west-2 eu-west-1\n```\n\n## Follow-up Questions\n- How would you validate RPO/RTO in disaster scenarios?\n- What monitoring/alerting ensures latency stays within targets?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Twitter","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T16:59:48.253Z","createdAt":"2026-01-15T16:59:48.253Z"},{"id":"q-2465","question":"An Aurora PostgreSQL cluster in us-east-1 serves 10k+ tenants via IAM database authentication. Each tenant must only access its own rows using Row-Level Security. A separate analytics workload must run against a eu-west-1 replica with data within 5 seconds of writes. Design the architecture: RLS policy and session management for per-tenant isolation, how analytics access is granted without leaking data, replication strategy (global DB vs separate KV store), backup/PITR, and a practical test plan to verify isolation and latency?","answer":"Implement RLS with per-session tenant_id set by the application, using policy USING (tenant_id = current_setting('app.tenant_id')::BIGINT) and WITH CHECK the same. Middleware sets app.tenant_id on con","explanation":"## Why This Is Asked\nTests practical RLS design, session-scoped tenant isolation, and cross-region analytics with minimal leakage.\n\n## Key Concepts\n- Row-Level Security (RLS) in Aurora PostgreSQL\n- Per-session context via current_setting()\n- IAM DB authentication integration\n- Aurora Global Database cross-region replication\n- PITR, KMS encryption, and audit logging\n\n## Code Example\n```sql\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_isolation ON orders\n  USING (tenant_id = current_setting('app.tenant_id')::BIGINT)\n  WITH CHECK (tenant_id = current_setting('app.tenant_id')::BIGINT);\n```\n```sql\nSET app.tenant_id = '12345';\n```\n```sql\nSET row_security = OFF; -- on analytics connection if needed for cross-tenant analytics\n```\n\n## Follow-up Questions\n- How would you validate tenant isolation under peak write latency?\n- What monitoring would you add to detect RLS bypass attempts or replication lag spikes?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Oracle","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T19:05:57.687Z","createdAt":"2026-01-15T19:05:57.687Z"},{"id":"q-2511","question":"In a globally distributed SaaS, Aurora PostgreSQL Global Database writer in us-east-1 and read replicas in eu-west-1 and ap-southeast-2. To deliver sub-50ms reads for hot tenants during peak while writes can occur anywhere, design a hybrid path using ElastiCache Redis in each region plus a cache-aside strategy. Include concrete config: DB instance classes, replica counts, Redis node types, TTL, invalidation mechanism, and a failover plan that meets RPO 5s and RTO 60s?","answer":"Use Aurora PostgreSQL Global Database with a writer in us-east-1 and cross-region replicas in eu-west-1 and ap-southeast-2. Layer in ElastiCache Redis in each region for the hot keys, cache-aside, TTL","explanation":"## Why This Is Asked\nTests ability to design multi-region data paths with both OLTP and caching to meet latency SLAs.\n\n## Key Concepts\n- Aurora Global Database cross-region replication.\n- Cache-aside strategy with ElastiCache Redis.\n- Per-region caching and invalidation messaging.\n- SLOs, RPO/RTO alignment.\n\n## Code Example\n```javascript\nasync function getUserRow(id){\n  const cached = await cache.get(`user:${id}`)\n  if(cached) return JSON.parse(cached)\n  const row = await db.query('SELECT * FROM users WHERE id=$1', [id])\n  await cache.set(`user:${id}`, JSON.stringify(row), 60)\n  return row\n}\n```\n\n## Follow-up Questions\n- How would you validate cache stampede risk and implement backpressure?\n- What are the DR implications if the writer region fails?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","MongoDB","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T20:54:14.192Z","createdAt":"2026-01-15T20:54:14.192Z"},{"id":"q-2533","question":"In a multi-region SaaS app using Aurora PostgreSQL with a single writer in us-east-1 and reads in eu-west-1 and ap-south-1, implement row-level security to restrict each tenant's data. Explain how you would design the RLS policy, index usage, and the impact on cross-region CDC via DMS or logical replication, and outline monitoring for unauthorized access. Include testing steps?","answer":"Implement PostgreSQL Row-Level Security (RLS) with per-tenant session context. Create the policy: `USING (tenant_id = current_setting('app.tenant_id')::int)` and enable RLS on the tenant table. The application must set `set_config('app.tenant_id', tenant_id, true)` at connection initialization. For cross-region replication, RLS policies propagate automatically via DMS or logical replication, but session context does not—each region must independently enforce tenant isolation. Monitor through CloudWatch metrics for RLS policy violations and audit logs using pg_audit. Validate with multi-tenant scenarios, cross-region consistency checks, and unauthorized access attempt testing.","explanation":"## Why This Is Asked\nTests ability to design secure data access patterns in multi-region Aurora environments. It combines RBAC-like row-level security with cross-region replication and monitoring, requiring understanding of policy propagation and comprehensive testing strategies.\n\n## Key Concepts\n- Row Level Security (RLS) and policy implementation in PostgreSQL/Aurora\n- Session context management via current_setting/set_config\n- Cross-region replication implications with DMS or logical replication\n- Performance optimization, audit capabilities, and failover considerations\n\n## Code Example","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Netflix","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:35:46.313Z","createdAt":"2026-01-15T21:41:50.605Z"},{"id":"q-2581","question":"Global SaaS with EU data residency: Writes in us-east-1; reads in EU must be sub-20ms; PCI-DSS data residency constraints; design a cross-region OLTP data layer and compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables with CDC. Provide concrete configurations (engine/edition, instance types, replica counts, PITR window, backup cadence, KMS keys, VPC design, inter-region networking), DR plan, and how you meet RPO <5s and RTO <60s?","answer":"Design a cross-region OLTP architecture with writes in us-east-1 and EU reads under 20ms, adhering to PCI-DSS data residency requirements. Compare Aurora PostgreSQL Global Database against DynamoDB Global Tables with CDC, providing specific configurations including engine/edition, instance types, replica counts, PITR window, backup cadence, KMS keys, VPC design, and inter-region networking. Deliver a disaster recovery plan achieving RPO <5s and RTO <60s.","explanation":"## Why This Is Asked\nTests the ability to design cross-region OLTP systems under strict residency and security constraints while comparing relational global databases with NoSQL global tables, including concrete disaster recovery configurations.\n\n## Key Concepts\n- Cross-region OLTP architectures\n- Data residency and PCI-DSS compliance\n- Aurora Global Database vs DynamoDB Global Tables with CDC\n- DR metrics: RPO, RTO, PITR, backup strategies, KMS key policies\n\n## Code Example\n```javascript\n// Pseudo-CDK: define a DynamoDB global table across two regions\nconst table = new dynamodb.Table(this, 'OrderTable', {\n  partitionKey: { name: 'orderId', type: dynamodb.AttributeType.STRING },\n  billingMode: dynamodb.BillingMode.PROVISIONED,\n  readCapacity: 100,\n  writeCapacity: 50,\n  streams: dynamodb.StreamType.NEW_AND_OLD_IMAGES,\n  encryption: dynamodb.TableEncryption.AWS_MANAGED,\n  pointInTimeRecovery: true,\n  globalTables: [\n    { region: 'us-east-1' },\n    { region: 'eu-west-1' }\n  ]\n});\n```","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T05:14:31.132Z","createdAt":"2026-01-15T23:41:13.159Z"},{"id":"q-2701","question":"In Aurora PostgreSQL design per-tenant data isolation using Row-Level Security and partitioned tables for a global setup: writer in us-east-1; regional reads in eu-west-1 and ap-south-1. Propose a concrete data path and DR strategy to meet RPO 5s and RTO 60s, including instance types, backup windows, PITR, KMS, cross-region replication, and a real failover plan?","answer":"Use a single writer in us-east-1 with Aurora Global Database, partitioned tables by tenant_id, and enable RLS policies bound to a session context. Route reads to regional readers; writes hit the write","explanation":"## Why This Is Asked\n\nAssesses tenant isolation, cross-region replication, and DR under compliance constraints.\n\n## Key Concepts\n\n- Row-Level Security and partitioning for multi-tenant isolation\n- Aurora Global Database cross-region replication and write routing constraints\n- PCI-DSS and data residency, encryption with KMS, and backup strategies\n- RPO/RTO goals and failover planning\n\n## Code Example\n\n```sql\nCREATE TABLE tenants (\n  tenant_id TEXT NOT NULL,\n  data JSONB,\n  region TEXT\n);\nALTER TABLE tenants ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_isolation ON tenants\n  USING (tenant_id = current_setting('app.tenant_id')::TEXT);\n```\n\n## Follow-up Questions\n\n- How would you test RLS policies at scale across regions?\n- What monitoring would you put in place to detect cross-region replication lag?","diagram":"flowchart TD\n  W[Writer in us-east-1] --> R1[Replica eu-west-1]\n  W --> R2[Replica ap-south-1]\n  R1 --> C1[(Cache layer)]\n  R2 --> C2[(Cache layer)]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T07:37:43.229Z","createdAt":"2026-01-16T07:37:43.230Z"},{"id":"q-2907","question":"A multi-region fintech app stores trades in Aurora PostgreSQL us-west-2 with sub-5ms writes and analytics in eu-central-1. Design an AWS-native architecture to meet RPO 5s and RTO 60s, ensure immutable audit logs in S3 with Object Lock, and maintain data residency. Compare Aurora Global Database writer+region replica vs DynamoDB+DMS analytics path, with concrete config (instance classes, replica counts, backup windows, KMS keys, IAM roles) and failover plan?","answer":"Use Aurora PostgreSQL Global Database with writer in us-west-2 and a read replica in eu-central-1 for sub-5ms writes and near-real reads; enable multi-AZ, cross-region backups, and PITR with 5s RPO. S","explanation":"Why This Is Asked\nTests cross-region DR, data residency, and immutable auditing in a realistic fintech context where latency targets are strict and analytics must co-exist with OLTP.\n\nKey Concepts\n- Aurora Global Database topology and cross-region replication\n- Immutable audit logging with S3 Object Lock and KMS\n- Data residency controls and per-tenant access (RLS)\n\nCode Example\n```typescript\n// CDK-like sketch for Aurora Global Database setup (illustrative)\nimport * as cdk from 'aws-cdk-lib';\nimport * as rds from 'aws-cdk-lib/aws-rds';\nimport * as ec2 from 'aws-cdk-lib/aws-ec2';\n\nconst vpc = new ec2.Vpc(this, 'VPC');\nnew rds.CfnDBCluster(this, 'WriterCluster', {\n  engine: 'aurora-postgresql',\n  databaseName: 'trades',\n  // replicas and cross-region and other config would be here\n});\n```\n\nFollow-up Questions\n- How would you handle schema changes with zero downtime across regions?\n- What monitoring and alerting would you implement to ensure RPO/RTOs are met in practice?","diagram":"flowchart TD\nA[Trade Write] --> B[(Aurora Global Writer in us-west-2)]\nB --> C[(Replica in eu-central-1)]\nA --> D[(Audit to S3 with Object Lock)]\nE[Analytics via DMS] --> F[Redshift/Analytics]\n","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","MongoDB","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T16:57:36.602Z","createdAt":"2026-01-16T16:57:36.602Z"},{"id":"q-2935","question":"Design a cross-region fintech data path where writes land in DynamoDB Global Tables (multi-master) in us-east-1 and eu-west-1, while analytics are powered by an Aurora PostgreSQL cluster in eu-west-1. Explain conflict handling, data residency for PCI-DSS, and how you meet RPO <5s and RTO <60s. Include concrete configurations: DynamoDB table keys and streams, Lambda, DMS or Debezium, Aurora size, backups, and network topology?","answer":"Use DynamoDB Global Tables (us-east-1 and eu-west-1) for multi-master writes; analytics replicated to Aurora PostgreSQL (eu-west-1) via a DynamoDB Streams → Lambda → DMS CDC pipeline. Resolve conflict","explanation":"## Why This Is Asked\nTests ability to design cross-region, multi-master transactional paths with real-time analytics, while honoring security and compliance constraints.\n\n## Key Concepts\n- DynamoDB Global Tables for low-latency multi-region writes\n- CDC pipeline (Streams → Lambda → DMS/Debezium) to Aurora\n- Conflict resolution and data governance for cross-region writes\n- PCI-DSS residency and EU-centric encryption/backups\n\n## Code Example\n```javascript\n// Lambda sample: process DynamoDB stream and push to Aurora via CDC\nexports.handler = async (event) => {\n  for (const rec of event.Records) {\n    if (rec.eventName === 'INSERT' || rec.eventName === 'MODIFY') {\n      const item = AWS.DynamoDB.Converter.unmarshall(rec.dynamodb.NewImage);\n      await upsertAurora(item);\n    }\n  }\n};\n```\n\n## Follow-up Questions\n- How would you handle a write-conflict when two regions update the same item simultaneously?\n- What monitoring and alerting would you implement to guarantee RPO and RTO targets under failover conditions?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T17:55:49.159Z","createdAt":"2026-01-16T17:55:49.159Z"},{"id":"q-2979","question":"Beginner scenario: An online storefront runs an AWS RDS MySQL instance in a single region with 1–2 read replicas. Compliance requires RPO 15 minutes and RTO 60 seconds during disaster recovery. Propose a practical backup and recovery plan using automated backups, PITR, Multi-AZ, and read replicas, with concrete values for backup retention, PITR window, and snapshot cadence, plus a reproducible failover procedure?","answer":"Enable automated backups with PITR window 7 days and backup retention 7 days; use Multi-AZ for automatic failover; create 1–2 read replicas for reads and potential DR primaries; schedule daily automat","explanation":"## Why This Is Asked\nTests practical understanding of RDS backup/DR features and DR testing steps in a beginner-friendly way.\n\n## Key Concepts\n- Automated backups and PITR\n- Multi-AZ vs Read Replicas\n- DR testing for RTO/RPO\n- Snapshot cadence and retention\n\n## Code Example\n```bash\n# Create a manual snapshot example\naws rds create-db-snapshot --db-instance-identifier storefront-db --db-snapshot-identifier storefront-sn-20260116\n```\n\n## Follow-up Questions\n- How would you monitor backup success and alert on failures?\n- How would you perform a low-traffic failover test with minimal user disruption?","diagram":"flowchart TD\nA[Start] --> B[Automated backups enabled]\nB --> C[Multi-AZ]\nC --> D[Read replicas configured]\nD --> E[Failover by promoting replica]\nE --> F[Switch endpoint (Route 53)]\nF --> G[Validate RPO/RTO]","difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-16T19:43:48.467Z","createdAt":"2026-01-16T19:43:48.467Z"},{"id":"q-3137","question":"Aurora PostgreSQL is deployed for a new product with highly variable traffic but strict latency targets. Compare Aurora Serverless v2 vs provisioned Aurora with read replicas for cost, latency, and maintenance. Provide a concrete setup example: (A) Serverless v2 with min 2 ACU, max 32 ACU, pause after idle, and RDS Proxy for connection pooling; (B) provisioned Aurora with 2 writer instances and 3 read replicas in the same region, plus a PgBouncer pool. Conclude when to choose each?","answer":"Serverless v2 excels with highly variable traffic, scaling from ~2 to 32 ACU and reducing idle costs; use RDS Proxy to stabilize connection churn. Provisioned Aurora works best for steady bursts with ","explanation":"## Why This Is Asked\n\nTests the candidate's understanding of when to use Aurora Serverless v2 versus provisioned Aurora, focusing on cost, latency stability, and maintenance practices for variable workloads.\n\n## Key Concepts\n\n- Aurora Serverless v2 scaling behavior and pause settings\n- RDS Proxy vs PgBouncer for connection pooling\n- Read replicas and failover implications\n- Cost versus latency trade-offs in serverless vs provisioned models\n\n## Code Example\n\n```sql\n-- example query to illustrate typical workload\nSELECT * FROM orders WHERE created_at >= now() - interval '1 day' LIMIT 100;\n```\n\n## Follow-up Questions\n\n- How would you measure and compare P95 latency and cost between the two setups over a 2-week window?\n- What failure modes differ between Serverless v2 and provisioned with read replicas, and how would you test them?","diagram":"flowchart TD\n  ATraffic[Traffic load] --> BOption{Aurora option}\n  BOption --> CServerless[Serverless v2]\n  BOption --> DProvisioned[Provisioned Aurora + Read Replicas]\n  CServerless --> EProxy[RDS Proxy for connections]\n  DProvisioned --> FPgBouncer[PgBouncer pool]\n  CServerless --> GCost[Cost optimization on idle]\n  DProvisioned --> HLatency[Lower controlled latency during peaks]","difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T04:16:21.862Z","createdAt":"2026-01-17T04:16:21.862Z"},{"id":"q-3289","question":"Two-region SaaS with strict tenant data residency: tenants assigned to a region cannot write in the other region, but hot tenants require sub-20ms reads globally. OLTP store is Aurora PostgreSQL and needs to support a single writer region with read replicas in the other region. Design a data layer that enforces per-tenant residency, delivers fast reads, and meets RPO 5s and RTO 60s. Include concrete configurations, data partitioning, routing rules, and a rollback plan for residency violations?","answer":"Route writes by tenant to its designated writer region; use Aurora PostgreSQL Global Database with writer in us-east-1 and a read replica in eu-west-1. Enforce per-tenant residency in the app via a re","explanation":"## Why This Is Asked\nThe question probes practical enforcement of data residency DevOps patterns while maintaining low-latency reads and robust DR in a two-region setup. It tests architectural judgment on cross-region replication, routing, and tenant isolation.\n\n## Key Concepts\n- Aurora PostgreSQL Global Database with a single writer region and cross-region replicas\n- Per-tenant data residency policies and region-aware schema partitioning\n- Read routing to nearest replica to minimize latency\n- RPO/RTO targets and PITR backups in multi-region setups\n\n## Code Example\n```javascript\n// Pseudo routing logic for tenant requests\nfunction routeRequest(tenantId, operation){\n  const region = residencyMap[tenantId];\n  if(operation === 'WRITE' && currentRegion !== region){\n    throw new Error('Tenant not writable in this region');\n  }\n  // route to appropriate endpoint based on operation\n  return regionEndpoints[region][operation];\n}\n```\n\n## Follow-up Questions\n- How would you handle a tenant migrating regions without data loss or RPO impact?\n- How would you validate RPO/RTO during a simulated regional outage and during maintenance windows?","diagram":"flowchart TD\nA[Tenant Residency Policy] --> B[Policy Enforcement]\nB --> C[Writes go to regional writer]\nB --> D[Reads directed to local replica]\nC --> E[Aurora Global DB: writer us-east-1; replica eu-west-1]\nD --> F[Sub-20ms reads for hot tenants]","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T10:30:32.064Z","createdAt":"2026-01-17T10:30:32.064Z"},{"id":"q-3367","question":"In a three-region deployment (us-east-1, eu-west-1, ap-southeast-2) with a single Aurora PostgreSQL Global Database writer in us-east-1, design a 2-tier data path that delivers sub-20 ms reads for hot tenants region-locally while writes can occur anywhere. Propose a cross-region caching strategy using Redis Global Datastore, plus a per-region analytics store. Include concrete configurations (replica counts, Redis node types, TTLs, backup windows, cross-region data sharing, and data residency controls) to meet RPO 5s and RTO 60s?","answer":"To meet RPO 5s and RTO 60s, run Aurora PostgreSQL Global Database with 1 writer in us-east-1 and read replicas in eu-west-1 and ap-southeast-2. Add a Redis Global Datastore: 2 nodes per region (memory","explanation":"## Why This Is Asked\\nTests cross-region data residency and consistency trade-offs under advanced requirements; ensures candidate can architect cross-region caching, RPO/RTO, and data governance.\\n\\n## Key Concepts\\n- Aurora Global Database characteristics (writer region, replication lag)\\n- Redis Global Datastore and cross-region caching\\n- Cache-aside pattern and TTL tuning\\n- Cross-region backups and KMS keys and data residency\\n- CDC to analytics stores and data governance\\n\\n## Code Example\\n```json\n{\n  \\\"redisGlobalDatastore\\\": {\n    \\\"regions\\\": [\\\"us-east-1\\\",\\\"eu-west-1\\\",\\\"ap-southeast-2\\\"],\n    \\\"nodesPerRegion\\\": 2,\n    \\\"ttlSeconds\\\": 300\n  }\n}\n```\n\\n## Follow-up Questions\\n- How would you monitor cross-region replication lag and SLA adherence?\\n- How would you handle a region outage and the impact on RPO/RTO?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T13:43:12.604Z","createdAt":"2026-01-17T13:43:12.604Z"},{"id":"q-3440","question":"Design and compare two data-architecture options for a PCI-DSS-compliant, multi-tenant SaaS using AWS databases: option A uses a single Aurora PostgreSQL cluster with Row-Level Security (RLS) to isolate tenants and KMS-encrypted backups plus cross-region snapshot replication; option B uses separate clusters per tenant across regions. Which approach would you pick and why?","answer":"Recommend a single Aurora PostgreSQL cluster with Row-Level Security (RLS) to isolate tenants, backed by KMS-encrypted backups and cross-region snapshot replication for DR. Compare with per-tenant clu","explanation":"## Why This Is Asked\n\nAssess ability to trade off isolation, cost, and DR in a PCI-DSS context; tests understanding of RLS, encryption, backups, cross-region replication, and migration paths.\n\n## Key Concepts\n\n- Row-Level Security (RLS) in PostgreSQL/Aurora\n- AWS KMS encryption for backups and at-rest data\n- Cross-region snapshot replication and RPO/RTO targets\n- Tenancy isolation trade-offs: single-cluster with shared schema vs per-tenant clusters\n- Migration and operational overhead\n\n## Code Example\n\n```sql\nCREATE POLICY tenant_rls ON public.users\nUSING (tenant_id = current_setting('app.tenant_id')::int);\nALTER ROLE app_user SET app.tenant_id = '1';\n```\n\n```\nNote: This snippet illustrates an RLS policy bound to a per-session tenant_id setting.\n```\n\n## Follow-up Questions\n\n- How would you monitor per-tenant access and detect policy misuse?\n- How would you onboard/offboard tenants with minimal downtime?","diagram":"flowchart TD\n  A[Option A: Single cluster with RLS] --> B[KMS encryption on backups]\n  A --> C[Cross-region snapshot replication]\n  D[Option B: Per-tenant clusters] --> E[Stronger isolation]\n  D --> F[Higher operational overhead]\n  B --> G[DR readiness]\n  C --> H[RPO/RTO targets]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","LinkedIn","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T16:38:56.423Z","createdAt":"2026-01-17T16:38:56.423Z"},{"id":"q-3465","question":"Scenario: A real-time fraud graph workload spans three regions. Primary data sits in Neptune with cross-region replication. Need sub-50ms neighbor lookups on random 1k-node subgraphs; writes across regions; DR targets: RPO 5s, RTO 60s. Design a concrete architecture comparing Neptune Global Database alone vs a hybrid approach using Neptune in each region plus ElastiCache Redis caches and a streaming path to OpenSearch for analytics. Provide concrete config: engine versions, instance counts, TTLs, and explicit failover steps?","answer":"Recommendation: use Neptune Global Database with a single writer in us-east-1 and regional readers in eu-west-1 and ap-south-1. Add ElastiCache Redis per region (2 shards, 3–4 nodes each) caching 1k-n","explanation":"## Why This Is Asked\nTests multi-region graph workloads, consistency, and DR trade-offs between a pure managed graph DB vs a hybrid approach with in-region caching and analytics streaming. It probes Neptune Global Database, Streams, ElastiCache, and OpenSearch integration for real-time surfaces.\n\n## Key Concepts\n- Neptune Global Database cross-region replication and single-writer model\n- Neptune Streams for change data capture\n- ElastiCache Redis caching and per-region invalidation\n- OpenSearch as a downstream analytics sink\n- DR: RPO/RTO targets and failover orchestration\n\n## Code Example\n```javascript\n// Example: Lambda handler for Neptune Stream events to invalidate regional caches\nexports.handler = async (event) => {\n  for (const rec of event.Records) {\n    // parse Neptune stream record\n    // identify affected neighborhood keys and regional caches\n    // publish invalidation to corresponding Redis clusters\n  }\n}\n```\n\n## Follow-up Questions\n- How would you validate RPO/RTO in this architecture? \n- What metrics would you monitor to detect cache invalidation lag and cross-region replication delay?","diagram":"flowchart TD\n  N[Neptune Global DB] --> R[Region Replicas]\n  R --> C[ElastiCache Redis (per region)]\n  C --> O[OpenSearch (Analytics)]\n  N --> S[Neptune Streams]","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Citadel","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-17T17:30:52.953Z","createdAt":"2026-01-17T17:30:52.953Z"},{"id":"q-3587","question":"Compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables for a read-heavy user profile store spanning us-east-1 and us-west-2. Provide concrete configurations to meet RPO 5s and RTO 60s; detail data model fit, backup plans, and failover steps. Assume a SQL workload with FK constraints and occasional transactional updates; keep latency targets <50ms in primary region?","answer":"Choose Aurora PostgreSQL Global Database for this use case. It maintains relational integrity and supports the complex SQL operations required for user profiles with foreign key constraints. Configure the deployment as follows: primary region us-east-1 with 1 writer instance (db.r5.large) and 2 reader instances, secondary region us-west-2 with 2 reader instances (db.r5.large). Enable cross-region replication with sub-second lag to meet the RPO 5s requirement. Configure automated backups for 35 days with point-in-time recovery enabled. For RTO 60s: implement Aurora's built-in fast failover mechanism combined with Route 53 health checks and automated failover scripts. The relational data model should include normalized tables for user profiles, preferences, and activity logs with proper foreign key constraints. Backup strategy combines daily snapshots, continuous backups, and cross-region backup copies. Failover procedure: 1) Monitor primary region health via Route 53 checks, 2) Automatically promote secondary region to writer upon failure detection, 3) Update DNS records to redirect traffic, 4) Validate connectivity and data consistency. Performance targets: <50ms latency for reads in us-east-1, <100ms for cross-region reads from us-west-2.","explanation":"## Why This Is Asked\n\nThis question evaluates cross-region disaster recovery design for AWS databases and tests the ability to choose between relational and NoSQL solutions based on workload requirements.\n\n## Key Concepts\n\n- Aurora Global Database architecture and replication mechanisms\n- DynamoDB Global Tables and conflict resolution strategies\n- RPO/RTO requirements and corresponding recovery approaches\n- Point-in-time recovery (PITR) and comprehensive backup strategies\n- Cross-region failover automation and DNS management\n- Relational data modeling with foreign key constraints\n- SQL workload optimization and transactional consistency guarantees\n\n## Code Example\n\n```sql\n-- Example: Query user profile with related data\nSELECT u.user_id, u.email, u.created_at,\n       p.theme, p.language, p.notifications,\n       a.last_login, a.activity_count\nFROM users u\nLEFT JOIN user_preferences p ON u.user_id = p.user_id\nLEFT JOIN user_activity a ON u.user_id = a.user_id\nWHERE u.user_id = :user_id;\n```\n\nThis query demonstrates the relational nature of user profile data and the need for JOIN operations that Aurora PostgreSQL handles efficiently, while DynamoDB would require multiple queries or data denormalization.","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T05:33:42.352Z","createdAt":"2026-01-17T22:36:20.795Z"},{"id":"q-3673","question":"Scenario: You manage an Aurora PostgreSQL Global Database with writer in us-east-1 and read replicas in eu-west-1 and ap-southeast-2. A new tenant requires strict data isolation via Row-Level Security and an auditable analytics path. Design a plan that (a) enforces per-tenant isolation in the global cluster (RLS + SECURITY DEFINER wrappers), (b) maintains cross-region replication with RPO <= 10s while writes land in writer only, (c) captures tenant-access events to S3 with a near-real-time pipeline (DMS) across regions, and (d) specifies concrete DB settings (instance class, replica count, parameter group, backup window, and cross-region KMS keys) to meet an RTO <= 60s. Include concrete configuration choices?","answer":"Implement per-tenant RLS on all tables using a session parameter (app.tenant_id) and a SECURITY DEFINER wrapper to enforce access; use a dedicated tenant role and policy: USING (tenant_id = current_se","explanation":"## Why This Is Asked\nTests ability to design multi-region OLTP with data isolation, auditability, and DR under real-world constraints.\n\n## Key Concepts\n- Aurora PostgreSQL Global Database architecture and cross-region replication timing\n- Row-Level Security in PostgreSQL and security wrappers\n- Change Data Capture with AWS DMS to S3 and analytics lake\n- Data residency and encryption with region-specific KMS keys\n- DR planning: RPO/RTO targets with backups and writes to writer only\n\n## Code Example\n```sql\n-- Enable RLS on a table and enforce tenant access\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_access ON orders\n  FOR ALL USING (tenant_id = current_setting('app.tenant_id')::int);\n\n-- Set tenant for the session\nSET app.tenant_id = '123';\n```\n\n## Follow-up Questions\n- How would you monitor replication lag and auto-tune?\n- How would you validate RLS coverage across regional replicas during failover?","diagram":"flowchart TD\n  A[Writer us-east-1] --> B[Global DB]\n  B --> C[EU-West read replica]\n  B --> D[AP-Southeast read replica]\n  E[Audit pipeline (DMS to S3)] --> F[(Audit data lake)]\n  C --> F\n  D --> F","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Google","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T04:22:02.471Z","createdAt":"2026-01-18T04:22:02.471Z"},{"id":"q-3711","question":"Scenario: Build a mobile analytics backend in us-east-1 handling 1.5k–2k writes/sec and per-user reads for dashboards. Compare DynamoDB with Global Tables vs Aurora MySQL Serverless v2 for this workload. Provide a concrete setup (schema, indexes, backups, DR, failover steps) to meet RPO 5s and RTO 60s, and note consistency and cost trade-offs?","answer":"Leverage DynamoDB Global Tables in us-east-1 and eu-west-1 to meet 5s RPO and 60s RTO. Data model: partition key user_id, sort key event_ts; GSI on event_type; enable on-demand backups and PITR to S3;","explanation":"## Why This Is Asked\n\nTests ability to select between managed AWS databases for a high-ingest, per-user read workload, and to translate SLAs (RPO/RTO) into concrete configurations using familiar features such as Global Tables, PITR, TTL, and streams. It also probes cost considerations and failover testing strategies.\n\n## Key Concepts\n\n- DynamoDB Global Tables for cross-region DR\n- Point-in-Time Restore (PITR) and on-demand backups\n- TTL (time-to-live) and event lifecycle planning\n- DynamoDB Streams as CDC to data lake\n- Aurora Serverless v2 trade-offs (read scaling, cold starts)\n\n## Code Example\n\n```javascript\n// DynamoDB put item example (AWS SDK v3)\nconst item = { user_id: {S: 'u123'}, event_ts: {N: '1700000000'}, payload: {S: '{}'} };\nconst params = { TableName: 'Events', Item: item };\n```\n\n## Follow-up Questions\n\n- How would you validate RPO/RTO in a staged failover test?\n- How would you handle hot partitions or skewed write throughput in DynamoDB?","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","DoorDash","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T06:48:28.893Z","createdAt":"2026-01-18T06:48:28.893Z"},{"id":"q-3772","question":"In a multi-region deployment, need strong OLTP in us-east-1 and real-time analytics in eu-west-1. The dashboards must reflect writes within ~5 seconds with writes allowed in both regions. Design a practical CDC path using AWS tools (DMS vs Debezium), specify data flow, latency targets, conflict handling, backups, and monitoring to meet RPO 5s and RTO 60s?","answer":"Use DMS CDC from Aurora PostgreSQL in us-east-1 to Redshift in eu-west-1, with an initial full load followed by ongoing CDC. Capture via WAL/txn logs, throttle to 5–10s batches, filter to analytic-tab","explanation":"## Why This Is Asked\nAssesses cross-region data pipelines, latency budgets, and DR planning across OLTP and analytics.\n\n## Key Concepts\n- CDC pipelines (DMS vs Debezium)\n- Cross-region latency, RPO/RTO targets\n- Data modeling for analytics vs transactional data\n\n## Code Example\n```javascript\n{\n  \"ReplicationTaskSettings\": \"{\\\"ParallelLoadThreads\\\":4,\\\"BatchApplyEnabled\\\":true}\",\n  \"TableMappings\": \"{\\\"rules\\\":[{\\\"rule-type\\\":\\\"selection\\\",\\\"object-locator\\\":{\\\"schema-name\\\":\\\"public\\\",\\\"table-name\\\":\\\"*\\\"},\\\"rule-action\\\":\\\"include\\\"}] }\"\n}\n```\n\n## Follow-up Questions\n- How would you handle write conflicts if writes occur in both regions simultaneously?\n- What monitoring dashboards and alerts would you configure to ensure RPO/RTO adherence?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Meta","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T08:51:49.792Z","createdAt":"2026-01-18T08:51:49.792Z"},{"id":"q-3816","question":"Two-region SaaS using Aurora PostgreSQL with writes allowed in both regions. Propose a concrete, auditable design to meet RPO 5s and RTO 60s while ensuring tenant data isolation. Compare Aurora PostgreSQL Global Database (single writer) vs independent clusters with bidirectional logical replication (DMS or Debezium), including conflict resolution, auditing, backups, and monitoring. Provide a recommended concrete configuration and rationale?","answer":"Recommendation: two independent Aurora PostgreSQL clusters (us-east-1 and eu-west-1) with bidirectional logical replication (DMS or Debezium) and a tenant-aware conflict policy (per-tenant last-writer","explanation":"## Why This Is Asked\nAuditors and engineers must balance latency, consistency, and auditability in multi-region writes.\n\n## Key Concepts\n- Aurora Global Database vs multi-region writes\n- Conflict resolution strategies in bidirectional replication\n- Tamper-evident auditing with S3 Object Lock and KMS\n- PITR, cross-region backups, and monitoring replication lag\n\n## Code Example\n```bash\n# Example DMS task setup (illustrative)\naws dms create-replication-task --replication-task-identifier bidir-task \\\n  --source-endpoint-arn <src> --target-endpoint-arn <tgt> \\\n  --replication-instance-arn <ri> --migration-type full-load-and-ccdc \\\n  --table-mappings file://mappings.json\n```\n\n## Follow-up Questions\n- How would you handle tenant schema changes mid-flight?\n- What are your observability defaults for replication lag and data freshness?","diagram":"flowchart TD\n  US[US-East Writer] --> REB[Bidirectional WAL Replication]\n  REB --> EU[EU-West Writer]\n  EU --> AUD[Audit Trail to S3 (immutable)]\n  AUD --> MON[Monitoring & Backups]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Databricks","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T10:41:08.899Z","createdAt":"2026-01-18T10:41:08.899Z"},{"id":"q-3977","question":"Scenario: A SaaS app needs per-tenant data isolation and fast reads in two AWS regions, with ~100 tenants and light writes. Compare DynamoDB Global Tables vs Aurora PostgreSQL cross-region replicas, focusing on data model, transactional guarantees, backups/DR, latency, and cost. Propose a concrete starter config to meet an RPO of 5 seconds and an RTO of 60 seconds (include region pair and basic sizing)?","answer":"Choose DynamoDB Global Tables if cost and simple organization trump strict cross-region transactions. DynamoDB favors low maintenance and fast regional reads; Global Tables replicate quickly but are n","explanation":"## Why This Is Asked\n\nTests understanding of cross-region data replication, consistency models, and operational trade-offs for multi-region SaaS workloads. The beginner level focuses on practical choices rather than exotic configurations.\n\n## Key Concepts\n\n- DynamoDB Global Tables: multi-region replication, eventual/strong reads, transactional support via DynamoDB transactions, cost implications.\n- Aurora PostgreSQL cross-region replication: true ACID across regions, read replicas, promotion/failover considerations, backup strategies.\n- Data isolation patterns: per-tenant keys vs schemas, access controls, and backups.\n- DR planning: RPO/RTO targets, PITR windows, cross-region backups.\n\n## Code Example\n\n```text\n# DynamoDB (pseudo) - PutItem with conditional write to ensure tenant isolation\naws dynamodb put-item --table-name TenantEvents \\\n  --item '{\"TenantId\":{\"S\":\"t1\"},\"EventId\":{\"S\":\"e123\"},\"Payload\":{\"S\":\"...\"}}' \\\n  --condition-expression 'attribute_not_exists(EventId)'\n```\n\n## Follow-up Questions\n\n- If cross-region reads must be <5 ms at 99th percentile, how would you adjust the architecture?\n- How would you implement per-tenant access controls and schema migrations in each approach?","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-18T18:38:41.046Z","createdAt":"2026-01-18T18:38:41.047Z"},{"id":"q-4194","question":"A startup SaaS app experiences highly variable traffic with 2-5x daily peaks and a need for both relational queries and fast session lookups. Design a cost-conscious AWS data layer comparing Aurora Serverless v2 (PostgreSQL) versus DynamoDB (on-demand) for this workload. Include data model, indexing, consistency, backups, failover, latency targets, and concrete configs to meet an RPO of 15s and an RTO of 60s?","answer":"Recommendation: DynamoDB on-demand for hot keys (sessions, user profiles) and Aurora Serverless v2 for relational queries. Use DynamoDB Global Tables across two regions with PITR and Streams for CDC t","explanation":"## Why This Is Asked\nAssess practical thinking on choosing between serverless relational vs NoSQL with cross-region DR. It tests data modeling and when to split workloads.\n\n## Key Concepts\n- Serverless data layers\n- Global Databases and Global Tables\n- PITR, CDC, cross-region replication\n- Latency, RPO, RTO trade-offs\n\n## Code Example\n```javascript\n// Config sketch (conceptual)\nconst config = {\n  dynamo: { mode: 'on-demand', globalTables: true, regions: ['us-east-1','eu-west-1'] },\n  aurora: { mode: 'Serverless v2', regions: ['us-east-1','eu-west-1'], replicas: 2 }\n}\n```\n\n## Follow-up Questions\n- How would you test the RPO/RTO targets in a staging environment?\n- What monitoring alerts would you set for cross-region replication lag?","diagram":"flowchart TD\n  A[User action] --> B[DynamoDB on-demand]\n  B --> C[Global Tables across us-east-1,eu-west-1]\n  A --> D[Aurora Serverless v2]\n  D --> E[Aurora Global DB with cross-region replicas]\n","difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Oracle","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T07:08:06.564Z","createdAt":"2026-01-19T07:08:06.564Z"},{"id":"q-4210","question":"Beginner scenario: An Aurora PostgreSQL cluster stores PCI data in **us-east-1**. To meet RPO < 15s and RTO < 2m after accidental data deletion, propose a practical backup/DR plan using automated backups, PITR, cross-region replication (Global Database), and cross-account DR drills. Include concrete settings (backup retention days, PITR window, replica count, failover priority) and the exact steps to run a DR drill?","answer":"Enable Aurora Global Database with a DR region; set automated backups retention to 14 days; configure PITR window to 30 minutes; create one read replica in the DR region as a member; run quarterly DR ","explanation":"## Why This Is Asked\nTests practical DR planning in AWS DB, including backups, PITR, and cross-region failover for PCI data.\n\n## Key Concepts\n- Aurora Global Database, PITR, automated backups\n- Cross-region DR, retention windows, failover priorities\n- Validation steps and drill frequency\n\n## Code Example\n```bash\n# illustrative AWS CLI commands (non-executable in this snippet)\naws rds create-global-database --global-database-name MyAuroraGlobal --source-db-cluster-identifier my-cluster --engine aurora-postgresql\naws rds create-global-database-member --global-database-name MyAuroraGlobal --region us-west-2\n```\n\n## Follow-up Questions\n- How would you monitor RPO/RTO during a drill and alert on misses?\n- What cost controls would you apply for cross-region DR?","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T08:52:47.500Z","createdAt":"2026-01-19T08:52:47.500Z"},{"id":"q-4310","question":"You run an Aurora PostgreSQL Global Database with a writer in us-east-1 and read replicas in eu-west-1 and ap-south-1. A new requirement demands sub-50ms per-tenant OLTP reads and near-real-time analytics per tenant using a separate analytics store. Propose a concrete hybrid architecture (OLTP + analytics) using AWS services (e.g., DMS, DynamoDB, Redshift/Glue), detail data flows, replication settings, consistency, failover, and a plan to validate SLAs before production?","answer":"Use Aurora PostgreSQL Global Database with a single writer in us-east-1 and cross-region replicas in eu-west-1 and ap-south-1. For analytics, stream CDC with AWS DMS to a DynamoDB per-tenant analytics","explanation":"## Why This Is Asked\nTests hybrid OLTP+Analytics design across regions, cross-region replication, and practical SLA enforcement.\n\n## Key Concepts\n- Aurora Global Database: single writer, cross-region replicas\n- DMS CDC: live change data capture to analytics store\n- Latency routing: per-tenant read paths\n- SLA validation: load tests and DR drills\n\n## Code Example\n```bash\n# Example DMS task (illustrative)\naws dms create-replication-task --replication-task-identifier OLTP-Analytics-Task \\\n  --source-endpoint-arn <source> --target-endpoint-arn <target> \\\n  --replication-task-settings '{\"TargetMetadata\": {\"ParallelApply\": true}}'\n```\n\n## Follow-up Questions\n- How would you monitor and tune cross-region replication lag for <50ms OLTP reads?\n- What failure scenarios would you test in DR drills to satisfy RTO < 60s?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T13:12:58.151Z","createdAt":"2026-01-19T13:12:58.152Z"},{"id":"q-4355","question":"You maintain an Aurora PostgreSQL cluster serving 100+ tenants in a SaaS product. Data for all tenants lives in the same database but must be strictly isolated in reads; tenants have varying access patterns and analytics needs. Design a solution using PostgreSQL Row-Level Security and tenant_id partitioning, plus AWS tools (Global Database, PITR, cross-region replicas) to meet isolation, sub-second hot-tenant reads, and near real-time analytics for aggregates. Include credential rotation and audit considerations. What is your approach?","answer":"Implement per-tenant isolation with PostgreSQL Row-Level Security (RLS) and tenant_id partitioning; enforce policy USING (tenant_id = current_setting('app.tenant')::int). Route hot tenants to optimize","explanation":"## Why This Is Asked\n\nTests advanced skills in multi-tenant DB design, RLS, and AWS DR/analytics integration. Requires concrete decisions on isolation, latency, and operational controls beyond generic design.\n\n## Key Concepts\n\n- PostgreSQL Row-Level Security (RLS) and policy design\n- Table partitioning by tenant_id and hot-tenant indexing\n- Aurora Global Database for cross-region analytics\n- PITR configuration and cross-region replica topology\n- Secrets Manager for credential rotation and CloudWatch/CloudTrail for auditing\n\n## Code Example\n\n```javascript\n// Example: JS snippet showing RLS policy application via client\nasync function setupRLS(client, tenant) {\n  await client.query(\"ALTER TABLE orders ENABLE ROW LEVEL SECURITY;\");\n  await client.query(\"CREATE POLICY tenant_rls ON orders USING (tenant_id = current_setting('app.tenant')::int);\");\n}\n```\n\n## Follow-up Questions\n\n- How would you test tenant isolation guarantees in DR drills?\n- Describe monitoring and alerting for RLS misconfigurations and partition bloat.","diagram":"flowchart TD\n  A[Tenant Isolation] --> B[Partitioning by tenant_id]\n  B --> C[RLS Policies]\n  C --> D[Analytics via Global DB]\n  D --> E[Auditing & Compliance]","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Hugging Face","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T15:40:51.696Z","createdAt":"2026-01-19T15:40:51.696Z"},{"id":"q-4372","question":"Cross-account analytics mirror: A SaaS app writes to an RDS PostgreSQL in us-east-1. A separate analytics account needs a read-only mirror for dashboards with low lag. Design a beginner-friendly pattern using cross-account RDS snapshot sharing and a refreshed read replica, specify backup retention, sharing steps, and the exact process to refresh daily?","answer":"Enable automated backups on the source RDS and create a manual snapshot. Share that snapshot with the analytics account; in the analytics account, copy the shared snapshot and restore it as a new, rea","explanation":"## Why This Is Asked\nTests practical cross‑account data sharing, a common beginner pattern for analytics pipelines, including permissions, backup cadence, and refresh automation.\n\n## Key Concepts\n- Cross‑account snapshot sharing\n- Read-only replica restoration in analytics account\n- IAM roles for access control\n- Backup retention and CloudWatch monitoring\n\n## Code Example\n```bash\n# In source account: share the snapshot\naws rds modify-db-snapshot-attribute --db-snapshot-identifier my-snap --attribute-name restore --values 111122223333\n```\n\n## Follow-up Questions\n- How would you handle encryption keys across accounts?\n- What would you monitor to detect drift between source and mirror?","diagram":"flowchart TD\n  A(Source RDS us-east-1) --> B(Share snapshot with Analytics acct)\n  B --> C(Copy snapshot in Analytics acct)\n  C --> D(Restore as read-only RDS in Analytics VPC)\n  D --> E(Grant read-only DB role to dashboards)\n  E --> F(Automate daily refresh and monitor lag)\n","difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T16:39:12.075Z","createdAt":"2026-01-19T16:39:12.077Z"},{"id":"q-4403","question":"Scenario: A fintech SaaS stores PCI-DSS data for some tenants in us-east-1 and non-PCI data globally. Design an architecture using Aurora PostgreSQL Global Database to minimize latency, ensure strict data residency, and provide auditable backups and DR. Include topology, data isolation (RLS), backup strategy, and DR test steps?","answer":"Use Aurora PostgreSQL Global Database with a PCI-primary in us-east-1 and a regional replica in eu-west-1 for non-PCI tenants. Enforce per-tenant isolation via Row-Level Security and per-tenant schema","explanation":"## Why This Is Asked\n\nThis tests architecture for data residency, PCI compliance, multi-region DR, and per-tenant isolation at scale.\n\n## Key Concepts\n\n- Aurora Global Database topology with cross-region replicas\n- Row-Level Security and per-tenant schema isolation\n- Cross-account AWS Backup vaults and encrypted PITR in S3\n- DR validation, audit logging, and latency targets\n\n## Code Example\n\n```sql\n-- Example RLS policy for tenant isolation\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_isolation ON orders USING (tenant_id = current_setting('myapp.tenant_id')::int);\n```\n\n## Follow-up Questions\n\n- How would you monitor replication lag and alert on threshold breaches?\n- How would you test residency constraints and audit logs during DR drills?","diagram":"flowchart TD\n  A[PCI tenants: primary us-east-1]\n  B[Replica: eu-west-1 for non-PCI tenants]\n  C[Cross-account AWS Backup vaults]\n  D[DR test: cross-region restore]\n  A --> C\n  C --> B\n  B --> D","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Instacart","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T17:35:21.236Z","createdAt":"2026-01-19T17:35:21.236Z"},{"id":"q-4467","question":"In a two-region Aurora PostgreSQL Global Database with a primary cluster in us-east-1 and an analytics sink in eu-west-1 (Redshift), design a practical CDC pipeline to feed near-real-time analytics. Choose between AWS DMS and Debezium, justify the choice, and specify data flow, latency targets (RPO <5s, RTO <60s), conflict handling for mirrored updates, backups, and monitoring. Include concrete settings (PITR window, replication task settings, lag budgets) and DR testing cadence?","answer":"Adopt DMS CDC from Aurora primary in us-east-1 to Redshift in eu-west-1, with a 1k-row CDC batch, 1s max lag, and a medium-replication instance. Enable 35-day PITR, CloudWatch alarms on replicationLag","explanation":"## Why This Is Asked\nTests practical CDC design, latency budgeting, cross-region data freshness, and operational controls.\n\n## Key Concepts\n- Aurora Global Database cross-region replication\n- CDC latency and backfill\n- DMS vs Debezium trade-offs\n- PITR and DR drills\n\n## Code Example\n```javascript\n// Pseudo DMS task config (illustrative)\nconst dmsTask = {\n  MigrationType: \"cdc\",\n  SourceEndpointArn: \"arn:aws:dms:...\",\n  TargetEndpointArn: \"arn:aws:dms:...\",\n  TableMappings: \"{\\\"rules\\\":[]}\"\n}\n```\n\n## Follow-up Questions\n- How would you test RPO/RTO in this setup?\n- What would change if analytics sinks to Athena instead?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-19T19:53:38.210Z","createdAt":"2026-01-19T19:53:38.210Z"},{"id":"q-4619","question":"A PCI-compliant SaaS stores customer data in Aurora PostgreSQL in us-west-2. An analytics sink in eu-central-1 must receive masked PII with near real-time updates. Design a streaming path using DMS or Debezium (or AWS Glue) that masks PII before replication, specify RPO <5s, RTO <60s, encryption (TLS at transit, KMS at rest), PITR windows, failover steps, and monitoring. Include data masking rules and DR testing cadence?","answer":"Design a DMS CDC path with a masking stage before the sink in eu-central-1 (e.g., redact SSNs and emails). Enforce TLS for all transfers; use CMKs in both regions for at-rest encryption. Set PITR to 3","explanation":"Why This Is Asked\nTests ability to design a cross-region, PCI-compliant streaming path with masking and robust DR. It probes data masking strategy, replication choice, and operational readiness.\n\nKey Concepts\n- Data masking in streaming CDC\n- Cross-region replication and latency targets\n- Encryption: TLS in transit, KMS at rest\n- PITR window and RPO/RTO goals\n- DR testing cadence and monitoring\n\nCode Example\n```javascript\n// Simple masking example used in testing\nfunction maskPII(value, field){\n  if(!value) return null;\n  switch(field){\n    case 'ssn': return 'XXX-XX-' + value.slice(-4);\n    case 'email': return '***@example.com';\n    default: return 'REDACTED';\n  }\n}\n```\n\nFollow-up Questions\n- How would you validate masking at scale across schema changes?\n- What metrics and alarms would you configure for DR drills and replication lag?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Twitter","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T04:37:45.859Z","createdAt":"2026-01-20T04:37:45.859Z"},{"id":"q-4634","question":"In a production Aurora PostgreSQL cluster in us-east-1 handling PCI data, you need a cost-effective staging clone for weekly integration tests that does not impact production performance. Explain how you would use Aurora Fast Database Clones (and/or Snapshots), setup, automation, and rollback strategy, including backup retention, PITR window, and permissions. Include concrete settings and steps to refresh weekly?","answer":"Use Aurora PostgreSQL Fast Database Clones to create a weekly staging clone from production in us-east-1. Take a snapshot, clone it to a dedicated staging cluster, mask PCI data, and run tests. Automa","explanation":"## Why This Is Asked\nIsolation of testing from production is critical for PCI workloads; clones enable realistic testing without affecting live performance or compliance.\n\n## Key Concepts\n- Aurora Fast Database Clones\n- Snapshot-based staging\n- PCI data masking\n- Access controls and least privilege\n\n## Code Example\n```javascript\n// Example: AWS SDK pseudocode for cloning workflow\n// This is illustrative; implement with your infra\nconst cloneWorkflow = async () => {\n  // 1) create a snapshot from prod\n  // 2) restore DB cluster from snapshot to staging\n  // 3) apply data masking\n  // 4) trigger tests\n  // 5) delete staging clone\n}\n``` \n\n## Follow-up Questions\n- How would you enforce data masking in the staging clone? \n- How would you validate that the PITR window settings do not expose production data in staging? \n- What monitoring alerts would you add for clone lifecycle failures?","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","NVIDIA","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T05:50:08.045Z","createdAt":"2026-01-20T05:50:08.045Z"},{"id":"q-4799","question":"Real-time bidding platform uses Aurora PostgreSQL Global Database with a primary in us-east-1 ingesting ~200k inserts/sec from a streaming pipeline; read replicas in eu-west-1 run analytics. Peak load spikes WAL generation and replica lag. Propose concrete optimizations to reduce WAL pressure and lag: batch commit sizing, WAL-related parameters (wal_level, max_wal_senders, wal_buffers), autovacuum tuning, and a connection-pooling strategy. Include exact parameter ranges, testing steps, and validation metrics?","answer":"Batch inserts of 1000–5000 rows per transaction; set wal_level = replica; max_wal_senders = 4–8; wal_buffers = 32MB; wal_keep_size = 512MB; autovacuum_vacuum_cost_limit = 2000; enable a connection poo","explanation":"## Why This Is Asked\nThis question probes practical WAL/replication tuning under multi-region Aurora PG, a common production pain point.\n\n## Key Concepts\n- WAL management: wal_level, max_wal_senders, wal_buffers\n- Replication lag and throughput metrics\n- Autovacuum tuning and connection pooling\n- Parameter groups and DR testing\n\n## Code Example\n```json\n{\n  \"wal_level\": \"replica\",\n  \"max_wal_senders\": \"6\",\n  \"wal_buffers\": \"32MB\",\n  \"wal_keep_size\": \"512MB\",\n  \"autovacuum_vacuum_cost_limit\": 2000\n}\n```\n\n## Follow-up Questions\n- How would you stage and test a DR drill for this tuning?\n- What monitoring and rollback plan would you implement if latency degrades?\n","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Microsoft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T13:20:39.657Z","createdAt":"2026-01-20T13:20:39.657Z"},{"id":"q-4861","question":"In Aurora PostgreSQL Global Database, cross-region replication lag causes dashboards to show stale per-tenant metrics. Propose a pragmatic solution that keeps writes in one region but serves near-real-time analytics in a separate analytics sink. Include data flow (primary -> CDC -> analytics store), consistency guarantees, concrete settings (PITR window, replication slot, DMS task settings or Debezium), and DR testing cadence?","answer":"Implement an analytics sink fed by CDC from the primary Aurora PostgreSQL. Use logical replication slots and DMS (or Debezium) with an initial full load, then CDC in micro-batches; batch size 2000, pa","explanation":"## Why This Is Asked\nThis question tests practical dataflow design for cross-region analytics, balancing latency, consistency, and DR in a real product. It probes knowledge of Aurora Global Database limitations, CDC tooling, and how to validate freshness.\n\n## Key Concepts\n- Aurora PostgreSQL Global Database\n- CDC pipelines (DMS, Debezium)\n- Logical replication slots and WAL\n- DR testing and RPO/RTO validation\n- Upserts and sink consistency\n\n## Code Example\n```javascript\n-- sample upsert used in analytics sink\nINSERT INTO analytics.tenants_metric (tenant_id, ts, value)\nVALUES (?, NOW(), ?)\nON CONFLICT (tenant_id, ts) DO UPDATE SET value = EXCLUDED.value;\n```\n\n## Follow-up Questions\n- How would you adapt this approach for tenant-aware SLAs and varying data volumes?\n- What monitoring would you add to detect increasing lag or drift?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T16:51:00.211Z","createdAt":"2026-01-20T16:51:00.211Z"},{"id":"q-5022","question":"Design a beginner-friendly Aurora PostgreSQL Serverless v2 setup for a mobile app with bursty traffic in us-west-2. Specify minCapacity, maxCapacity, and auto-pause window, and justify RDS Proxy usage for persistent connections. Include concrete values and a simple test plan to validate cold-start latency under 1s after idle, and burst latency under 2s during a spike?","answer":"Configure minCapacity to 2 ACUs and maxCapacity to 32 ACUs with an auto-pause delay of 15 minutes. Implement RDS Proxy to manage connection pooling (target approximately 300 max connections with pool size at ~80% of maximum capacity). Validate the setup by allowing 15 minutes of idle time to trigger auto-pause, then measure cold-start latency to ensure it remains under 1 second, and simulate burst traffic to confirm latency stays under 2 seconds during traffic spikes.","explanation":"## Why This Is Asked\n\nThis question evaluates practical knowledge of Aurora Serverless v2 scaling behavior, idle pause mechanisms, and RDS Proxy's role in handling connection bursts for applications with variable traffic patterns.\n\n## Key Concepts\n\n- Aurora Serverless v2 ACU scaling and billing granularity\n- Auto-pause delay configuration and its latency implications\n- RDS Proxy connection pooling for traffic burst management\n- Validation methodologies for cold-start and burst latency testing\n\n## Code Example\n\n```javascript\n// Pseudo-test harness for latency validation\nfunction isAcceptable(latencyMs) { \n  return latencyMs < 1000; \n}\n```\n\n## Follow-up Questions\n\n- How would you adjust these parameters for a production environment with higher traffic volumes?\n- What monitoring metrics would you track to optimize this configuration over time?\n- How would you handle connection pooling for multiple application instances?","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Cloudflare","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:20:22.118Z","createdAt":"2026-01-21T00:00:57.183Z"},{"id":"q-5143","question":"You manage an Aurora PostgreSQL cluster used by three microservices across two AWS accounts. You need per-service, least-privilege access via IAM database authentication and RDS Proxy, with dedicated DB users mapped to IAM roles. Outline the concrete design: object names, table-level grants, IAM role ARNs, and how to test permissions, latency, and failover. Include sample grants (e.g., SELECT on sales.*; INSERT/UPDATE on orders.*) and a quick validation plan?","answer":"Design three DB users: sales_read, orders_read, orders_rw. Grants: sales_read -> SELECT on sales.*; orders_read -> SELECT on orders.*; orders_rw -> INSERT, UPDATE, DELETE on orders.* (plus SELECT). Ma","explanation":"## Why This Is Asked\nTests practical use of IAM DB authentication, RDS Proxy, and fine-grained access controls across accounts, plus a concrete testing plan.\n\n## Key Concepts\n- IAM database authentication with Aurora PostgreSQL\n- RDS Proxy for serverless/large-concurrency access\n- Fine-grained grants and per-service DB users\n- Cross-account role mappings and network isolation\n- Validation: latency, authorization, and failover tests\n\n## Code Example\n```sql\nCREATE USER sales_read LOGIN;\nGRANT USAGE ON SCHEMA sales TO sales_read;\nGRANT SELECT ON ALL TABLES IN SCHEMA sales TO sales_read;\n\nCREATE USER orders_rw LOGIN;\nGRANT INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA orders TO orders_rw;\nGRANT SELECT ON ALL TABLES IN SCHEMA orders TO orders_rw;\nALTER DEFAULT PRIVILEGES IN SCHEMA orders GRANT INSERT, UPDATE, DELETE ON TABLES TO orders_rw;\n```\n\n## Follow-up Questions\n- How would you verify IAM role to DB user mapping using Lambda? \n- What monitoring would you set to detect privilege drift or misconfiguration? ","diagram":"flowchart TD\n  L1[Lambda: role SalesRead] --> Proxy[RDS Proxy: IAM DB auth]\n  Proxy --> DB1[Aurora: sales_read]\n  L2[Lambda: role OrdersRead] --> Proxy\n  Proxy --> DB2[Aurora: orders_read]\n  L3[Lambda: role OrdersRW] --> Proxy\n  Proxy --> DB3[Aurora: orders_rw]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","MongoDB","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T08:02:04.852Z","createdAt":"2026-01-21T08:02:04.852Z"},{"id":"q-851","question":"Two-region OLTP SaaS with a single writer in us-east-1 and read replicas in eu-west-1. Compare Aurora Global Database (PostgreSQL) vs DynamoDB Global Tables for this workload: latency targets, consistency model, failover behavior, and cost. Which approach would you pick and why, and what concrete configuration (replica count, failover window, write routing) would you implement to meet RTO < 60s and RPO < 5s?","answer":"Choose Aurora Global Database (PostgreSQL) with a single writer in us-east-1 and one or more read replicas in eu-west-1. It preserves transactional invariants with asynchronous cross-region replicatio","explanation":"## Why This Is Asked\nThis question probes understanding of cross-region OLTP replication choices, consistency, failover, and cost. It contrasts HTAP-like needs with strict ACID guarantees in real-world deployments.\n\n## Key Concepts\n- Aurora Global Database vs DynamoDB Global Tables trade-offs\n- Single-writer constraint and cross-region replication lag\n- RTO/RPO targets, failover orchestration, PITR\n\n## Code Example\n```javascript\naws rds create-global-database --global-database-name my-globaldb --source-db-cluster-identifier mydbcluster\n```\n```\n\n## Follow-up Questions\n- How would you monitor replication lag and what alerts would you set?\n- How would you handle schema migrations with zero downtime across regions?\n","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:32:32.498Z","createdAt":"2026-01-12T13:32:32.498Z"},{"id":"q-871","question":"Migration plan: An OLTP app runs on Aurora PostgreSQL provisioned; traffic is bursty; you want to evaluate Aurora Serverless v2. Provide a concrete plan to migrate, including: (1) start/stop criteria and scaling configuration; (2) handling of long-running transactions and prepared statements; (3) how to keep reads consistent during scaling; (4) testing approach for failover/RTO targets; (5) cost considerations and potential pitfalls with Serverless v2?","answer":"Plan a phased migration to Aurora Serverless v2 from provisioned instances. Use min_capacity 0.5 and max_capacity 16, disable auto_pause initially, route traffic through RDS Proxy, and validate long-r","explanation":"## Why This Is Asked\nServerless migrations are common; evaluate trade-offs.\n\n## Key Concepts\n- Aurora Serverless v2\n- scaling policies\n- transaction semantics\n- prepared statements\n- RDS Proxy\n- failover testing\n\n## Code Example\n```javascript\n// Aurora Serverless v2 config (example)\nconst auroraConfig = {\n  engine: 'aurora-postgresql',\n  scale: { min_capacity: 0.5, max_capacity: 16, auto_pause: false }\n};\n```\n\n## Follow-up Questions\n- How would you monitor connection pool usage during scaling?\n- What are Serverless v2 limitations with prepared statements or long-running queries?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","IBM","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:53:18.115Z","createdAt":"2026-01-12T13:53:18.115Z"},{"id":"q-895","question":"Your multi-region SaaS needs an audit-friendly cross-tenant analytics store with writes transactional in us-east-1 and analytics queries in eu-west-1 under GDPR. Compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables for this workload, focusing on transactional integrity, analytics capability, PITR/retention, cross-region latency, and cost. Recommend a concrete configuration (writer region, replica counts, PITR window, tenant isolation, ETL approach) to meet RPO 15 minutes and RTO 1 hour?","answer":"Aurora PostgreSQL Global Database best meets cross-region transactional integrity with SQL analytics, in a GDPR context. Put writer in us-east-1, two read replicas in eu-west-1; enable PITR 30 days; r","explanation":"## Why This Is Asked\nTests a candidate's ability to balance transactional integrity, cross-region DR, and analytics in a regulated multi-tenant environment.\n\n## Key Concepts\n- Aurora Global Database vs DynamoDB Global Tables\n- PITR, RPO/RTO targets, GDPR/tenant isolation\n- ETL paths to analytics stores (Redshift/Data Lake)\n\n## Code Example\n```javascript\n// Pseudo: configure a DMS task to replicate from us-east-1 to eu-west-1\nconst task = await dms.createReplicationTask({ ... });\n```\n\n## Follow-up Questions\n- How would you handle schema changes across regions without downtime?\n- What telemetry would you collect to validate RPO/RTO in production?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Slack","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:36:01.445Z","createdAt":"2026-01-12T14:36:01.445Z"},{"id":"q-956","question":"For a real-time fraud graph application needing sub-100ms neighbor lookups across two AWS regions, compare Amazon Neptune Global Database with DynamoDB (using graph patterns and DAX) for this workload. Writer region us-east-1; readers in eu-west-1; assess graph traversal latency, consistency guarantees, failover behavior, and total cost. Provide a concrete setup (cluster engine and size, replica counts, PITR window, backup schedule, and network/config) to meet an RPO of 5s and an RTO of 60s?","answer":"Choose Neptune Global Database for graph-centric queries. It provides cross-region replication with near real-time reads; DynamoDB+DAX is weaker for complex traversals. Recommend a primary cluster in ","explanation":"## Why This Is Asked\nDiscusses cross-region graph DB choices and DR readiness in practice.\n\n## Key Concepts\n- Neptune Global Database vs DynamoDB/DAX trade-offs\n- Graph traversals, latency budgets, consistency models\n- Cross-region failover, PITR, backups, and cost\n\n## Code Example\n```javascript\n// Illustrative AWS CLI usage (not executed here)\naws neptune create-global-cluster --global-cluster-name FraudGraphGlobal --engine neptune\n```\n\n## Follow-up Questions\n- How would you monitor replication lag and graph query latency?\n- What tests validate RPO/RTO under regional failure?\n","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","NVIDIA","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:42:36.355Z","createdAt":"2026-01-12T16:42:36.355Z"},{"id":"q-964","question":"You run an Amazon RDS PostgreSQL in **us-east-1** with automated backups. A regional outage blocks access from that region. How would you achieve **RPO ≤ 60s** and **RTO ≤ 15 minutes** by restoring to **eu-west-1**? Compare cross-region read replicas, backup copy, and Aurora Global Database, and outline concrete steps, knobs, and caveats?","answer":"Prefer Aurora Global Database for true cross-region DR with <60s RPO and <15m RTO. If constrained to RDS PostgreSQL, copy automated backups to eu-west-1 and create a read replica there; promote replic","explanation":"## Why This Is Asked\nDR planning across AWS regions is a core skill. This question tests understanding of RPO/RTO, replication guarantees, and the trade-offs between RDS Cross-Region Replicas, snapshot copying, and Aurora Global Database.\n\n## Key Concepts\n- RPO vs RTO and replication lag\n- Cross-region DR options: RDS read replicas, snapshot copy, Aurora Global DB\n- Failover orchestration and DNS routing\n\n## Code Example\n```javascript\n// AWS CLI example: copy a snapshot to another region\naws rds copy-db-snapshot --source-db-snapshot-identifier arn:aws:rds:us-east-1:123456789012:snapshot:mydb-2026-01-01 --target-db-snapshot-identifier eu-west-1-mydb-2026-01-01 --source-region us-east-1 --region eu-west-1\n\n// Restore a DB instance from the cross-region snapshot\naws rds restore-db-instance-from-db-snapshot --db-instance-identifier eu-west-1-mydb-restored --db-snapshot-identifier eu-west-1-mydb-2026-01-01\n```\n\n## Follow-up Questions\n- How would you automate the failover and DNS switch?\n- What monitoring would you put in place to detect lag and test RPO/RTO?","diagram":"flowchart TD\n A[Source: RDS us-east-1] --> B{Strategy}\n B --> C[Aurora Global Database]\n B --> D[Cross-region backups + EU replica]\n C --> E[Fast RTO, low RPO]\n D --> F[Longer RPO, slower failover]","difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Slack","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:26:08.106Z","createdAt":"2026-01-12T17:26:08.106Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":57,"beginner":14,"intermediate":23,"advanced":20,"newThisWeek":40}}