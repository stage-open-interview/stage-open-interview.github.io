{"questions":[{"id":"q-1074","question":"Scenario: A global time-series platform ingests 1M events/hour in us-west-2; dashboards in eu-central-1 and ap-southeast-2 need sub-200ms reads on the latest window. Data must be immutable for 90 days for compliance. Compare DynamoDB Global Tables with DAX vs Aurora PostgreSQL Global Database with cross-region backups. Provide topology, replication, PITR/backup plans, and RPO/RTO targets?","answer":"Choose DynamoDB Global Tables in three regions (us-west-2, eu-central-1, ap-southeast-2) with DAX caching per region and multi-region writes. Enable PITR for 35 days and S3 immutable archives for 90 d","explanation":"## Why This Is Asked\n\nTests cross-region replication, latency trade-offs, and DR design between NoSQL and relational engines.\n\n## Key Concepts\n- Global Tables vs Global Database replication\n- Read latency and consistency models\n- PITR and cross-region backups\n- Archival and compliance (S3 Object Lock)\n\n## Code Example\n\n```javascript\n// Example: pseudo-endpoint selection logic for region failover\n```\n\n## Follow-up Questions\n- How would you validate RPO/RTO in a simulated outage?\n- What monitoring would you implement to detect replication lag across regions?","diagram":"flowchart TD\nA[Ingest] --> B[Global Tables: us-west-2, eu-central-1, ap-southeast-2]\nB --> C[DAX per region]\nA --> D[S3 immutable archive (90d)]\nC --> E[Dashboards in region]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:33:34.955Z","createdAt":"2026-01-12T21:33:34.955Z"},{"id":"q-1131","question":"**Hybrid Analytics Path for Multiregion Aurora**\n\nYou're running an Aurora PostgreSQL OLTP cluster with tenant isolation via RLS in us-east-1. A regulatory BI team in eu-west-1 requires near real-time analytics with masked PII. Design a hybrid analytics path using Aurora Global Database for OLTP replicas and a CDC-based analytic store (Redshift or DynamoDB+Lambda) in eu-west-1. Describe data flow, masking strategy, encryption, failover, and how to meet RPO 5s and RTO 60s, including cost considerations?","answer":"Run OLTP in Aurora PostgreSQL with RLS isolation in us-east-1. Replicate via Aurora Global Database to eu-west-1. Ingest CDC to a masked analytics store (Redshift or DynamoDB) in eu-west-1; apply per-","explanation":"## Why This Is Asked\nTests cross-region replication, hybrid OLTP/OLAP, security, and DR.\n\n## Key Concepts\n- Aurora Global Database, RLS, CDC, masking, cross-region KMS, hot failover\n- DR: RPO 5s, RTO 60s\n- Cost considerations: hot standby vs on-demand\n\n## Code Example\n```javascript\n// AWS CLI example (conceptual)\naws dms create-replication-task --replication-task-identifier cbd-task --source-endpoint-arn <src> --target-endpoint-arn <tgt> --migration-type full-load-and-cdc\n```\n\n## Follow-up Questions\n- How would you validate masking correctness without exposing PII?\n- What metrics indicate replication lag violations and how to remediate?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Goldman Sachs","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T01:22:46.568Z","createdAt":"2026-01-13T01:22:46.568Z"},{"id":"q-1279","question":"In a multi-tenant SaaS on AWS, run a single Aurora PostgreSQL cluster with per-tenant schemas and RLS to isolate data. An analytics team in eu-west-1 requires cross-tenant BI with masked PII in near real-time dashboards. Design a cost-aware architecture that delivers masking, auditing, and SLA, comparing per-tenant schemas in a single cluster vs separate clusters per tenant. Include data flow, backup, and failover?","answer":"Adopt a hybrid: maintain one Aurora PostgreSQL cluster with per-tenant schemas and RLS for isolation; expose masked analytic views for BI from a dedicated eu-west-1 read replica. Use a CDC pipeline to","explanation":"## Why This Is Asked\nThis question probes practical multi-tenant isolation, cross-region analytics, and governance trade-offs in AWS databases.\n\n## Key Concepts\n- Row-level security and per-tenant schemas\n- Cross-region BI with masked analytics\n- CDC pipelines to analytics stores\n- Audit, backup (PITR), and cost governance\n- Single-cluster vs multi-cluster trade-offs\n\n## Code Example\n```sql\n-- Enable RLS on a tenant table\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_rls ON orders\n  USING (tenant_id = current_setting('my.tenant_id')::int);\n```\n\n## Follow-up Questions\n- How would you implement masking for PII in the analytics store without leaking through cached results?\n- How would you monitor latency, replication lag, and cost to meet SLA?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Hugging Face","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:44:04.097Z","createdAt":"2026-01-13T07:44:04.097Z"},{"id":"q-1303","question":"In a multi-tenant SaaS using Aurora PostgreSQL with Global Database spanning us-west-2 and us-east-1, tenants must have isolated data access and BI dashboards must mask PII in real time. Propose an end-to-end design using per-tenant RLS, dynamic masking for BI, and a separate analytics store fed by CDC (DMS/Debezium). Include cross-region DR with RPO <5s and RTO <60s, data flow, encryption, backups, and a concrete sizing plan (replicas, window, network)?","answer":"Leverage Aurora PostgreSQL with per-tenant RLS and dynamic BI masking, plus a CDC pipeline (DMS/Debezium) feeding a dedicated analytics store (Redshift or DynamoDB+ Lambda) in the secondary region. Us","explanation":"## Why This Is Asked\nTests ability to architect multi-tenant isolation, real-time masking, and cross-region DR.\n\n## Key Concepts\n- Aurora PostgreSQL with Global Database\n- Row-Level Security and dynamic masking\n- CDC pipelines (DMS/Debezium)\n- Analytics stores (Redshift, DynamoDB)\n- DR targets (RPO/RTO), encryption, backups\n\n## Code Example\n```javascript\n-- SQL for RLS policy (illustrative)\nALTER TABLE events ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_isolation ON events\n  USING (tenant_id = current_setting('app.tenant_id')::int)\n  WITH CHECK (tenant_id = current_setting('app.tenant_id')::int);\n```\n\n## Follow-up Questions\n- How would you test tenant isolation in the analytics path?\n- What changes if BI dashboards must also support cross-tenant rollups?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Lyft","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:50:50.803Z","createdAt":"2026-01-13T08:50:50.803Z"},{"id":"q-1314","question":"Design a GDPR-compliant data deletion strategy for a multi-region Aurora PostgreSQL Global Database that uses us-east-1 as the writer and replicas in multiple regions. How would you implement Right-to-Erasure for tenant data, propagate deletions with minimal latency, handle referential integrity, and maintain an auditable trail while meeting RPO/RTO targets? Include practical steps and trade-offs?","answer":"Adopt soft deletes with a tenant-scoped deleted_at flag and propagate deletions via logical replication or DMS CDC to all regions. Enforce FK cascades in the origin region only; keep a purge window (e","explanation":"## Why This Is Asked\nData privacy and cross-region deletion are common but tricky with global databases. The answer tests practical strategies for timing, integrity, and audit.\n\n## Key Concepts\n- GDPR deletion rights and retention policies\n- Aurora Global Database cross-region replication\n- Soft vs hard deletes and foreign-key considerations\n- Auditability and compliance tracking\n\n## Code Example\n```\nALTER TABLE tenants ADD COLUMN deleted_at TIMESTAMPTZ;\nCREATE VIEW active_items AS SELECT * FROM items WHERE deleted_at IS NULL;\nUPDATE items SET deleted_at = NOW() WHERE id = ?;\n```\n\n## Follow-up Questions\n- How would you test cross-region deletion latency and audit integrity?\n- How to handle legal-hold scenarios and purge timing?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hashicorp","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T11:28:34.774Z","createdAt":"2026-01-13T11:28:34.776Z"},{"id":"q-1382","question":"In an Aurora PostgreSQL Global Database with a writer in us-east-1 and replicas in eu-west-1, a financial balance update must be atomic across regions. Explain why cross-region distributed transactions are not supported and propose a practical pattern to achieve atomic-ish behavior with low latency, including data flow, failover handling, and cost/latency trade-offs?","answer":"Cross-region atomicity isn’t supported: Aurora Global Database uses asynchronous cross-region replication, so a two-stage commit across regions can’t be guaranteed. A practical pattern is a single-wri","explanation":"## Why This Is Asked\n\nTests understanding of cross-region replication limits in Aurora Global Database and practical patterns to achieve atomic-like behavior without distributed transactions.\n\n## Key Concepts\n\n- Aurora Global Database replication model and its eventual cross-region consistency\n- ACID vs eventual consistency in multi-region setups\n- Single-writer boundary, event-driven replication, compensating actions, and idempotent processing\n\n## Code Example\n\n```javascript\n// Pseudo-implementation: single-writer boundary with event emission\nasync function updateBalance(accountId, delta) {\n  // Begin in writer region\n  await beginLocalTx();\n  await updateLocalBalance(accountId, delta);\n  await commitLocalTx();\n  // Publish event to eu-west-1 for downstream update\n  await publishEvent({ accountId, delta, txnId: generateId() });\n}\n```\n\n## Follow-up Questions\n\n- How would you test this pattern under network partitions?\n- What latency and cost implications arise from cross-region event streams and reconciliation?","diagram":"flowchart TD\n  A[Client Request] --> B[Coordinator (us-east-1)]\n  B --> C[Prepare: Update primary balance]\n  C --> D[Commit: Emit event to eu-west-1]\n  D --> E[Apply delta in eu-west-1]\n  E --> F[Response to client]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T14:45:15.419Z","createdAt":"2026-01-13T14:45:15.419Z"},{"id":"q-1441","question":"In Aurora PostgreSQL (us-east-1) with tenant isolation via RLS, design a near real-time analytics path for a eu-west-1 consumer needing masked data and <5s lag. Use Aurora Global Database for OLTP and a CDC store in eu-west-1 (Redshift or DynamoDB+Lambda). Explain data flow, masking/encryption, consistency, failover, and cost with concrete config sketches?","answer":"Use Aurora Global Database to keep a writer in us-east-1 with cross-region replicas; route analytic reads to eu-west-1 via DMS CDC to Redshift or DynamoDB+Lambda. Implement per-tenant masking in the p","explanation":"## Why This Is Asked\nTests ability to design cross-region analytics paths, balancing data masking, security, latency, failover, and cost for a regulated multi-tenant SaaS using AWS DB services.\n\n## Key Concepts\n- Aurora Global Database cross-region replication\n- CDC options: DMS vs Debezium vs native\n- Data masking: per-tenant with RLS and column-level masking\n- Encryption: KMS, TLS, envelope encryption\n- Failure scenarios: failover/failback and RPO/RTO\n- Cost trade-offs: Redshift vs DynamoDB, compute/storage\n\n## Code Example\n```javascript\n// Example: publication for CDC (PostgreSQL)\nALTER SYSTEM SET wal_level = logical;\nCREATE PUBLICATION analytics_pub FOR TABLE orders, customers;\n```\n\n## Follow-up Questions\n- How would you validate lag and data completeness end-to-end?\n- How would you enforce per-tenant masking in the analytics store?","diagram":"flowchart TD\n  A[OLTP writer us-east-1] --> B[Global DB replica us-east-1]\n  A --> C[DMS CDC in eu-west-1? (for analytics store)]\n  B --> D[Analytics store in eu-west-1]\n  D --> E[Analytics consumer in eu-west-1]","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T17:03:20.591Z","createdAt":"2026-01-13T17:03:20.592Z"},{"id":"q-1447","question":"You run a high-volume ecommerce on Aurora PostgreSQL Global Database with a single writer in us-east-1 and read replicas in eu-west-1. An outage in us-east-1 requires routing writes to eu-west-1 within 60s while ensuring RPO<5s, idempotent writes, and no double billing. Design the architecture and concrete configuration (replica counts, failover procedures, analytics CDC path, masking, PITR) to meet these goals?","answer":"Use Aurora Global Database with a hot-standby writable clone in eu-west-1 for DR so writes can continue within 60s of a us-east outage. Route writes to the EU writer via Route 53 health checks; keep a","explanation":"## Why This Is Asked\n\nAssesses cross-region DR planning for a mission-critical OLTP with minimal data loss and downtime, plus robust write-idempotency and analytics integration.\n\n## Key Concepts\n\n- Aurora Global Database architecture and cross-region failover considerations\n- DR targets: RPO < 5s, RTO < 60s with hot-standby writable clone\n- Idempotent write patterns: transaction IDs, upserts to prevent double billing\n- CDC to analytics (DMS) and data masking at read time\n- PITR retention planning and backup strategies\n\n## Code Example\n\n```sql\n-- idempotent upsert pattern for an order\nINSERT INTO orders (order_id, customer_id, amount, txn_id)\nVALUES (:order_id, :customer_id, :amount, :txn_id)\nON CONFLICT (order_id) DO UPDATE\n  SET amount = EXCLUDED.amount, updated_at = NOW(), txn_id = EXCLUDED.txn_id;\n```\n\n## Follow-up Questions\n\n- How would you validate RPO/RTO in production and what telemetry would you collect?\n- How would you test and verify the cross-region failover without impacting live traffic?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T17:43:10.564Z","createdAt":"2026-01-13T17:43:10.564Z"},{"id":"q-1550","question":"In a two-region deployment with a single writer in us-east-1 and analytic reads in eu-west-1, design a CDC pipeline to keep a near real-time analytic store updated within 5 seconds of commits, while masking per-tenant data and enforcing encryption at rest and in transit. Compare AWS DMS, Debezium/Kafka, and native Aurora logical replication, and provide concrete configuration (engine, instance types, replica counts, PITR, KMS keys, VPC endpoints, and network topology) to meet RPO 5s and RTO 60s?","answer":"Deploy an Aurora Global Database with the primary writer in us-east-1 and implement a CDC pipeline using AWS DMS to maintain a near real-time analytic store in eu-west-1. Configure DMS with change data capture to stream committed changes within 5 seconds, applying per-tenant data masking and enforcing encryption both in transit and at rest. AWS DMS provides operational simplicity with managed infrastructure, meeting the RPO of 5 seconds and RTO of 60 seconds while ensuring data security and compliance requirements.","explanation":"## Why This Is Asked\nTests ability to design cross-region CDC pipelines with strict latency targets while implementing tenant-level data masking and comprehensive encryption. It also evaluates knowledge of live replication technologies and their operational trade-offs.\n\n## Key Concepts\n- Change data capture latency across AWS regions\n- Cross-region data movement options (AWS DMS vs Debezium/Kafka vs native Aurora logical replication)\n- Per-tenant data masking and encryption strategies\n- Network topology and VPC endpoint configuration\n- Disaster recovery objectives: RPO/RTO, PITR, and failover procedures\n\n## Code Example\n```sql\n-- On writer: publish all tables for CDC\nCREATE PUBLICATION cdc_pub FOR ALL TABLES;\n```\n\n## Follow-up Questions\n- How would you handle failover scenarios and ensure data consistency?\n- What monitoring and alerting would you implement for the CDC pipeline?\n- How would you optimize costs while maintaining the 5-second latency requirement?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","DoorDash","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:30:05.991Z","createdAt":"2026-01-13T21:38:49.169Z"},{"id":"q-1747","question":"You run a multi-region SaaS with Aurora PostgreSQL as the OLTP in us-east-1 and read replicas in eu-west-1. A new requirement enforces strict per-tenant data isolation via Row-Level Security and data residency controls for backups. Design a concrete approach: RLS policy skeletons for all tables, session-based tenant_id from authentication, per-tenant restore strategy, cross-region backup copy schedule, and a testing/validation plan that proves no cross-tenant leakage under burden. Include concrete config knobs and a sample policy?","answer":"Implement per-tenant isolation with Postgres Row-Level Security on all tables, driven by a session tenant_id set from the user’s JWT. Use a separate role per tenant and policies like: USING (tenant_id","explanation":"## Why This Is Asked\nTests ability to implement robust data isolation with RLS, ensure data residency through cross-region snapshots, and design test plans for high-concurrency workloads.\n\n## Key Concepts\n- PostgreSQL Row-Level Security (RLS) on all tables\n- session context via SET myapp.tenant_id from authentication\n- cross-region Aurora backups and snapshot copies\n- PITR windows and data residency/compliance\n- tenant onboarding/offboarding and data retention controls\n\n## Code Example\n```sql\n-- Example RLS skeleton\nCREATE POLICY tenant_rls ON users\nFOR ALL USING (tenant_id = current_setting('myapp.tenant_id')::int)\nWITH CHECK (tenant_id = current_setting('myapp.tenant_id')::int);\n\nALTER TABLE users ENABLE ROW LEVEL SECURITY;\nALTER TABLE users FORCE ROW LEVEL SECURITY;\n```\n\n## Follow-up Questions\n- How would you automate per-tenant onboarding to ensure tenant_id is set on every DB connection?\n- What metrics would you monitor to detect leakage or policy misconfigurations?","diagram":"flowchart TD\n  C[Client Request] --> APP[App Layer]\n  APP --> U[Aurora US-East OLTP]\n  U --> EU[EU-West Replica]\n  APP --> S[Snapshot Copy to EU-West]\n  S --> D[Data Residency & Compliance]\n  D --> M[Monitoring & Alerts]","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T09:37:30.426Z","createdAt":"2026-01-14T09:37:30.428Z"},{"id":"q-1775","question":"In a multi-region Aurora PostgreSQL Global Database setup (writer in us-east-1; readers in eu-west-1 and ap-south-1) with strict tenant-level data residency, design a scalable architecture that provides sub-50ms reads for hot paths in each region while ensuring RPO <= 5s and RTO <= 60s, using row-level security and a CDC-based analytic store; explain data partitioning, access controls, and failover strategy, plus cost trade-offs?","answer":"Use region-scoped tenant sharding with Postgres RLS to enforce residency; writer in us-east-1 with Aurora Global Database and replicas in eu-west-1/ap-south-1 for sub-50ms regional reads. Use CDC from","explanation":"## Why This Is Asked\nAssesses ability to architect cross-region, residency-bound databases with real-time analytics, balancing latency, consistency, and cost.\n\n## Key Concepts\n- Aurora Global Database and cross-region replication\n- Row-Level Security (RLS) for tenant isolation\n- CDC-based analytic store (Redshift/S3+Glue)\n- RPO/RTO design and failover strategy\n- Cost trade-offs: replication, egress, storage, and analytics\n\n## Code Example\n```sql\n-- Example RLS policy\nCREATE POLICY tenant_rls ON orders\n  USING (tenant_id = current_setting('myapp.tenant_id')::int);\n```\n\n## Follow-up Questions\n- How would you validate RPO/RTO in a disaster scenario?\n- What strategies minimize cross-region CDC costs while preserving freshness?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T10:39:05.726Z","createdAt":"2026-01-14T10:39:05.727Z"},{"id":"q-1882","question":"Design a cross-region analytic path for a SaaS app with an Aurora PostgreSQL OLTP cluster in us-east-1 as the single writer and regional read replicas in us-west-2. The goal: near real-time analytics with masked PII in the analytics store. Propose a CDC-based pipeline (Aurora CDC, DMS, Debezium, or Kinesis) to load into Redshift or DynamoDB in us-west-2, choose masking strategy, encryption, data freshness target (RPO), failover plan, and cost considerations. Include concrete config choices (instance types, retention, network, and security)?","answer":"Use Aurora Global Database to replicate OLTP across regions, and implement CDC from the writer to a masked analytics store in the remote region (Redshift via DMS or DynamoDB via Kinesis). Mask PII at ","explanation":"## Why This Is Asked\nAssess cross-region data flow, masking, and cost-aware analytics separation. It tests practical CDC choices, DR timing, and security implications.\n\n## Key Concepts\n- Aurora Global Database, CDC, DMS/Debezium, Redshift Spectrum, DynamoDB Streams\n- Data masking, encryption at rest/in transit, RPO/RTO targets\n- Cross-region networking and cost optimization\n\n## Code Example\n```sql\n-- Example masking policy sketch (pseudo)\nCREATE POLICY mask_ssn ON customers\nAS (SELECT mask_ssn(ssn) AS ssn_masked);\n```\n\n## Follow-up Questions\n- How would you test RPO/RTO guarantees in this pipeline?\n- What are failure modes if CDC lag increases beyond threshold?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T15:42:33.696Z","createdAt":"2026-01-14T15:42:33.696Z"},{"id":"q-2139","question":"You run OLTP in Aurora PostgreSQL us-east-1 and need near real-time BI in us-west-2. Design a CDC pipeline: enable a dedicated logical replication slot in Aurora; use DMS in CDC mode to stream changes to Redshift in us-west-2 via a staging S3 bucket; apply PII masking at BI layer; enable PITR and cross-region backups; target end-to-end latency ~2s and RTO <60s. Include data flow, failover, and cost trade-offs?","answer":"Enable a dedicated Aurora logical replication slot in us-east-1; route changes through DMS in CDC mode to Redshift in us-west-2 (via S3 staging). Use MERGE-based upserts for idempotence. Apply PII mas","explanation":"## Why This Is Asked\nTests real-time cross-region CDC design with security, masking, and failover.\n\n## Key Concepts\n- Aurora logical replication\n- AWS DMS CDC\n- Redshift ingestion from S3\n- Data masking and RLS\n- PITR and cross-region backups\n- Cost trade-offs\n\n## Code Example\n```json\n{\n  \"DMSTaskSettings\": {\"TargetTablePrepMode\":\"DO_NOTHING\",\"FullLoadTask\": false,\"CdcInsertsOnly\": false}\n}\n```\n\n## Follow-up Questions\n- How to handle schema changes downstream?\n- What metrics validate 2s latency under load?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T04:19:48.906Z","createdAt":"2026-01-15T04:19:48.906Z"},{"id":"q-2163","question":"A multi-region SaaS app needs sub-20ms reads for hot tenants across three continents, writes allowed in any region, and PCI-DSS data residency constraints. Design a data-layer using AWS: compare Aurora PostgreSQL Global Database with DynamoDB Global Tables plus CDC to an analytics store, including consistency, DR, backups, and concrete configurations to meet RPO 5s and RTO 60s?","answer":"Option A: Aurora PostgreSQL Global Database with a single writer in us-east-1 and regional read replicas in eu-west-1 and ap-south-1; 7-day PITR, KMS encryption, TLS, and Route 53 latency routing to m","explanation":"## Why This Is Asked\nTests cross-region DR, read latency, and data residency decisions for AWS database services.\n\n## Key Concepts\n- Aurora Global Database constraints: single writer, cross-region replication\n- DynamoDB Global Tables: multi-region writes with replication and eventual consistency trade-offs\n- Data residency and PCI-DSS: encryption (at rest/in transit), KMS keys, IAM access controls\n- DR planning: RPO/RTO targets, PITR, backups, failover orchestration\n\n## Code Example\n```javascript\n// CDK sketch for core Aurora Global DB settings (conceptual)\nconst glb = new aurora.GlobalDatabase(this, 'GlobalDb', {\n  writerRegion: 'us-east-1',\n  regions: ['us-east-1', 'eu-west-1', 'ap-south-1'],\n  engine: aurora.PostgresEngineVersion.VER_13_6,\n});\n```\n\n## Follow-up Questions\n- If latency budgets tighten, how would you restructure reads?\n- How would you monitor cross-region replication lag and auto-tune write routing?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T05:41:48.135Z","createdAt":"2026-01-15T05:41:48.135Z"},{"id":"q-2211","question":"Two-region, multi-tenant SaaS with strict data residency: EU tenants' data must stay in EU, US tenants' data in US. Needs sub-15ms reads for hot tenants, writes in any region, and cross-region analytics. Compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables with analytics options; provide a concrete topology, replication, backups, and DR plan to meet RPO 5s and RTO 60s, including per-tenant routing and residency enforcement?","answer":"Use Aurora PostgreSQL Global Database across EU and US with per-tenant residency enforced by Row-Level Security; hot reads served from a regional cache (DynamoDB or MemoryDB) and analytics from a data","explanation":"## Why This Is Asked\nTests ability to design multi-region, tenancy-aware architectures with DR and residency constraints; assesses knowledge of Aurora Global Database, RLS, read caching, analytics integration, and cost/latency trade-offs.\n\n## Key Concepts\n- Data residency with row-level security (RLS) in PostgreSQL\n- Aurora Global Database topology across regions\n- Cross-region replication and WAL shipping mechanics\n- Read caching and analytics integration (DynamoDB/MemoryDB, S3/Glue)\n\n## Code Example\n```sql\n-- Enable per-tenant isolation via RLS\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_isolation ON orders\n  USING (tenant_id = current_setting('app.tenant_id')::int);\n```\n\n## Follow-up Questions\n- How would you validate DR failover latency and RPO under peak load?\n- What monitoring and cost controls would you implement to maintain SLAs across regions?","diagram":"flowchart TD\n  EU_Tenant[EU region data store] --> EU_Reads[EU read replicas]\n  US_Tenant[US region data store] --> US_Reads[US read replicas]\n  EU_Reads --> AnalyticsEU[Analytics]\n  US_Reads --> AnalyticsUS[Analytics]\n  AnalyticsEU --> Lake[Analytics Lake]\n  AnalyticsUS --> Lake","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Salesforce","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T07:40:12.465Z","createdAt":"2026-01-15T07:40:12.466Z"},{"id":"q-2248","question":"A SaaS news site logs user events at ~1000 writes/s, with dashboards needing sub-200 ms reads. Data is append-only; hot data kept 30 days, archived after. Compare DynamoDB (on-demand, TTL, GSI) vs Aurora PostgreSQL (partitioned tables, read replicas) for this workload. Provide concrete configs (primary key design, indexes, TTL window, backup/retention, RPO/RTO) and justify choice?","answer":"Recommended: DynamoDB on-demand with a TTL attribute of 90 days, primary key (user_id HASH, event_ts RANGE), optional GSI on event_type for analytics, enable Streams for CDC, and PITR. Data is append-","explanation":"## Why This Is Asked\nTests ability to pick between a serverless NoSQL and a relational option for high-velocity, append-only data with TTL and analytics needs. Assesses data modeling, retention strategy, and DR considerations.\n\n## Key Concepts\n- Append-only data modeling for time-series-like logs\n- Throughput modes: on-demand vs provisioned capacity\n- TTL data pruning and retention windows\n- Read latency strategies (indexes, caching, Streams)\n- Backups, PITR, and DR differences between DynamoDB and Aurora\n\n## Code Example\n```sql\nCREATE TABLE event_logs (\n  user_id VARCHAR(36) NOT NULL,\n  event_ts BIGINT NOT NULL,\n  payload JSON,\n  PRIMARY KEY (user_id, event_ts)\n) PARTITION BY RANGE (event_ts);\n```\n\n```bash\naws dynamodb create-table --table-name UserEventLog \\\n  --attribute-definitions AttributeName=user_id,AttributeType=S AttributeName=event_ts,AttributeType=N \\\n  --key-schema AttributeName=user_id,KeyType=HASH AttributeName=event_ts,KeyType=RANGE \\\n  --billing-mode PAY_PER_REQUEST \\\n  --stream-specification StreamEnabled=true,StreamViewType=NEW_IMAGE\n```\n\n```bash\naws dynamodb update-time-to-live --table-name UserEventLog --time-to-live-specification Enabled=true,AttributeName=ttl\n```\n\n## Follow-up Questions\n- What are the trade-offs of not using TTL on DynamoDB for hot data?\n- How would you validate data durability and latency under bursty traffic?","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Square","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T09:04:51.962Z","createdAt":"2026-01-15T09:04:51.962Z"},{"id":"q-2268","question":"Your app uses AWS Lambda functions that connect to an RDS PostgreSQL instance; during bursts, you see many connections causing failures. How would you leverage Amazon RDS Proxy to manage connections, configure auth, and ensure stable performance? Include what to monitor, any pricing considerations, and a basic setup outline?","answer":"Use RDS Proxy to pool connections and decouple Lambda bursts from DB connection limits. Create a private RDS Proxy in the same VPC, target the RDS instance, store credentials in Secrets Manager, attac","explanation":"This question tests practical understanding of connection management for serverless apps. Candidates should cite: where to place the proxy, how credentials are supplied, how to route Lambda traffic, what to monitor (proxy health, connection count, latency), and cost trade-offs. They should mention security group rules and multi-AZ considerations.","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Goldman Sachs","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T09:50:32.865Z","createdAt":"2026-01-15T09:50:32.865Z"},{"id":"q-2320","question":"Scenario: A SaaS app stores event data in DynamoDB and must retain 90 days in DynamoDB and archive older events to S3 for analytics. Design a pragmatic lifecycle: data model, TTL, export to S3, per-tenant/year/month partitioning, storage class selection, and validation plan to ensure data integrity and queryability via Athena. Include concrete steps and considerations for cost?","answer":"Two-tier approach: keep hot data for 90 days in DynamoDB with TTL on expireAt; archive older events to S3 per tenant/year/month using DynamoDB export-to-S3 (or Glue), compress to Parquet, and apply li","explanation":"## Why This Is Asked\nValidates practical data lifecycle design, cost awareness, and AWS integration.\n\n## Key Concepts\n- DynamoDB TTL on expireAt\n- DynamoDB export-to-S3 or Glue-based export\n- S3 Lifecycle and storage classes (Standard/IA/Glacier)\n- Per-tenant partitioning in S3 (tenant/year/month)\n- Athena/Glue for analytics on archived data\n\n## Code Example\n```javascript\n// TTL example snippet (not production-ready)\nconst item = { tenantId:'T1', eventId:'E123', ts:Date.now(), expireAt: Math.floor(Date.now()/1000) + 90*24*3600 };\n```\n\n## Follow-up Questions\n- How would you adjust for cross-region access to archived data?\n- What are trade-offs of online TTL vs scheduled archive windows?","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T11:49:47.720Z","createdAt":"2026-01-15T11:49:47.720Z"},{"id":"q-2331","question":"For a multi-tenant SaaS app storing per-tenant PII with EU/US data residency, compare Aurora PostgreSQL with Row-Level Security (RLS) vs DynamoDB with per-tenant access patterns. Explain schema design, replication strategy, consistency, DR, and cost. Provide a concrete configuration to meet RPO < 5s and RTO < 60s, including region placement, replica counts, backup windows, and key management?","answer":"Recommended approach: Aurora PostgreSQL Global Database with per-tenant RLS and cross-region replicas. Writer in us-east-1; replicas in eu-west-1 and ap-south-1; enable PITR for 35 days and automated ","explanation":"## Why This Is Asked\nEvaluates RBAC at the database layer, cross-region data residency, and trade-offs between relational and NoSQL models in multi-tenant SaaS.\n\n## Key Concepts\n- Row-Level Security (RLS) in PostgreSQL\n- DynamoDB conditional writes and per-tenant design\n- Cross-region replication, PITR, and RTO/RPO targets\n\n## Code Example\n\n```javascript\nCREATE POLICY tenant_access ON users\nFOR ALL USING (tenant_id = current_setting('tenants.current')::int);\n```\n\n## Follow-up Questions\n- How would you audit per-tenant data access across regions?\n- What metrics would you monitor to detect RBAC misconfigurations?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Plaid","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T13:07:36.036Z","createdAt":"2026-01-15T13:07:36.036Z"},{"id":"q-2411","question":"In an IoT platform, 100k writes/sec of time-series data arrive from devices worldwide. You must ingest region-locally with sub-20ms latency, perform near real-time analytics in a separate region, retain 30 days of data, and ensure PCI-DSS residency. Compare AWS Timestream, DynamoDB Global Tables with a CDC pipeline, and an Aurora-based time-series schema. Propose architecture, data model, retention, DR, and concrete config to meet RPO 5s and RTO 60s?","answer":"DynamoDB Global Tables with regional writes, plus DynamoDB Streams feeding a near-real-time analytics path (Kinesis→S3/Redshift). For PCI residency, encrypt at rest with a restricted KMS CMK and stric","explanation":"## Why This Is Asked\nTests multi-region data ingestion, analytics separation, retention, and compliance trade-offs with real workloads.\n\n## Key Concepts\n- DynamoDB Global Tables, DynamoDB Streams, KMS\n- Data residency for PCI-DSS, PITR, TTL\n- Time-series patterns, retention strategies, and analytics integration\n\n## Code Example\n```bash\n# Create a Global Table (example flag values for illustration)\naws dynamodb create-global-table --table-name IoTTimeSeries \\\n  --region us-east-1 us-west-2 eu-west-1\n```\n\n## Follow-up Questions\n- How would you validate RPO/RTO in disaster scenarios?\n- What monitoring/alerting ensures latency stays within targets?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Twitter","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T16:59:48.253Z","createdAt":"2026-01-15T16:59:48.253Z"},{"id":"q-2465","question":"An Aurora PostgreSQL cluster in us-east-1 serves 10k+ tenants via IAM database authentication. Each tenant must only access its own rows using Row-Level Security. A separate analytics workload must run against a eu-west-1 replica with data within 5 seconds of writes. Design the architecture: RLS policy and session management for per-tenant isolation, how analytics access is granted without leaking data, replication strategy (global DB vs separate KV store), backup/PITR, and a practical test plan to verify isolation and latency?","answer":"Implement RLS with per-session tenant_id set by the application, using policy USING (tenant_id = current_setting('app.tenant_id')::BIGINT) and WITH CHECK the same. Middleware sets app.tenant_id on con","explanation":"## Why This Is Asked\nTests practical RLS design, session-scoped tenant isolation, and cross-region analytics with minimal leakage.\n\n## Key Concepts\n- Row-Level Security (RLS) in Aurora PostgreSQL\n- Per-session context via current_setting()\n- IAM DB authentication integration\n- Aurora Global Database cross-region replication\n- PITR, KMS encryption, and audit logging\n\n## Code Example\n```sql\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_isolation ON orders\n  USING (tenant_id = current_setting('app.tenant_id')::BIGINT)\n  WITH CHECK (tenant_id = current_setting('app.tenant_id')::BIGINT);\n```\n```sql\nSET app.tenant_id = '12345';\n```\n```sql\nSET row_security = OFF; -- on analytics connection if needed for cross-tenant analytics\n```\n\n## Follow-up Questions\n- How would you validate tenant isolation under peak write latency?\n- What monitoring would you add to detect RLS bypass attempts or replication lag spikes?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Oracle","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T19:05:57.687Z","createdAt":"2026-01-15T19:05:57.687Z"},{"id":"q-2511","question":"In a globally distributed SaaS, Aurora PostgreSQL Global Database writer in us-east-1 and read replicas in eu-west-1 and ap-southeast-2. To deliver sub-50ms reads for hot tenants during peak while writes can occur anywhere, design a hybrid path using ElastiCache Redis in each region plus a cache-aside strategy. Include concrete config: DB instance classes, replica counts, Redis node types, TTL, invalidation mechanism, and a failover plan that meets RPO 5s and RTO 60s?","answer":"Use Aurora PostgreSQL Global Database with a writer in us-east-1 and cross-region replicas in eu-west-1 and ap-southeast-2. Layer in ElastiCache Redis in each region for the hot keys, cache-aside, TTL","explanation":"## Why This Is Asked\nTests ability to design multi-region data paths with both OLTP and caching to meet latency SLAs.\n\n## Key Concepts\n- Aurora Global Database cross-region replication.\n- Cache-aside strategy with ElastiCache Redis.\n- Per-region caching and invalidation messaging.\n- SLOs, RPO/RTO alignment.\n\n## Code Example\n```javascript\nasync function getUserRow(id){\n  const cached = await cache.get(`user:${id}`)\n  if(cached) return JSON.parse(cached)\n  const row = await db.query('SELECT * FROM users WHERE id=$1', [id])\n  await cache.set(`user:${id}`, JSON.stringify(row), 60)\n  return row\n}\n```\n\n## Follow-up Questions\n- How would you validate cache stampede risk and implement backpressure?\n- What are the DR implications if the writer region fails?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","MongoDB","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T20:54:14.192Z","createdAt":"2026-01-15T20:54:14.192Z"},{"id":"q-2533","question":"In a multi-region SaaS app using Aurora PostgreSQL with a single writer in us-east-1 and reads in eu-west-1 and ap-south-1, implement row-level security to restrict each tenant's data. Explain how you would design the RLS policy, index usage, and the impact on cross-region CDC via DMS or logical replication, and outline monitoring for unauthorized access. Include testing steps?","answer":"Implement PostgreSQL Row-Level Security (RLS) with per-tenant session context. Create the policy: `USING (tenant_id = current_setting('app.tenant_id')::int)` and enable RLS on the tenant table. The application must set `set_config('app.tenant_id', tenant_id, true)` at connection initialization. For cross-region replication, RLS policies propagate automatically via DMS or logical replication, but session context does not—each region must independently enforce tenant isolation. Monitor through CloudWatch metrics for RLS policy violations and audit logs using pg_audit. Validate with multi-tenant scenarios, cross-region consistency checks, and unauthorized access attempt testing.","explanation":"## Why This Is Asked\nTests ability to design secure data access patterns in multi-region Aurora environments. It combines RBAC-like row-level security with cross-region replication and monitoring, requiring understanding of policy propagation and comprehensive testing strategies.\n\n## Key Concepts\n- Row Level Security (RLS) and policy implementation in PostgreSQL/Aurora\n- Session context management via current_setting/set_config\n- Cross-region replication implications with DMS or logical replication\n- Performance optimization, audit capabilities, and failover considerations\n\n## Code Example","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Netflix","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:35:46.313Z","createdAt":"2026-01-15T21:41:50.605Z"},{"id":"q-2581","question":"Global SaaS with EU data residency: Writes in us-east-1; reads in EU must be sub-20ms; PCI-DSS data residency constraints; design a cross-region OLTP data layer and compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables with CDC. Provide concrete configurations (engine/edition, instance types, replica counts, PITR window, backup cadence, KMS keys, VPC design, inter-region networking), DR plan, and how you meet RPO <5s and RTO <60s?","answer":"Design a cross-region OLTP architecture with writes in us-east-1 and EU reads under 20ms, adhering to PCI-DSS data residency requirements. Compare Aurora PostgreSQL Global Database against DynamoDB Global Tables with CDC, providing specific configurations including engine/edition, instance types, replica counts, PITR window, backup cadence, KMS keys, VPC design, and inter-region networking. Deliver a disaster recovery plan achieving RPO <5s and RTO <60s.","explanation":"## Why This Is Asked\nTests the ability to design cross-region OLTP systems under strict residency and security constraints while comparing relational global databases with NoSQL global tables, including concrete disaster recovery configurations.\n\n## Key Concepts\n- Cross-region OLTP architectures\n- Data residency and PCI-DSS compliance\n- Aurora Global Database vs DynamoDB Global Tables with CDC\n- DR metrics: RPO, RTO, PITR, backup strategies, KMS key policies\n\n## Code Example\n```javascript\n// Pseudo-CDK: define a DynamoDB global table across two regions\nconst table = new dynamodb.Table(this, 'OrderTable', {\n  partitionKey: { name: 'orderId', type: dynamodb.AttributeType.STRING },\n  billingMode: dynamodb.BillingMode.PROVISIONED,\n  readCapacity: 100,\n  writeCapacity: 50,\n  streams: dynamodb.StreamType.NEW_AND_OLD_IMAGES,\n  encryption: dynamodb.TableEncryption.AWS_MANAGED,\n  pointInTimeRecovery: true,\n  globalTables: [\n    { region: 'us-east-1' },\n    { region: 'eu-west-1' }\n  ]\n});\n```","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:14:31.132Z","createdAt":"2026-01-15T23:41:13.159Z"},{"id":"q-2701","question":"In Aurora PostgreSQL design per-tenant data isolation using Row-Level Security and partitioned tables for a global setup: writer in us-east-1; regional reads in eu-west-1 and ap-south-1. Propose a concrete data path and DR strategy to meet RPO 5s and RTO 60s, including instance types, backup windows, PITR, KMS, cross-region replication, and a real failover plan?","answer":"Use a single writer in us-east-1 with Aurora Global Database, partitioned tables by tenant_id, and enable RLS policies bound to a session context. Route reads to regional readers; writes hit the write","explanation":"## Why This Is Asked\n\nAssesses tenant isolation, cross-region replication, and DR under compliance constraints.\n\n## Key Concepts\n\n- Row-Level Security and partitioning for multi-tenant isolation\n- Aurora Global Database cross-region replication and write routing constraints\n- PCI-DSS and data residency, encryption with KMS, and backup strategies\n- RPO/RTO goals and failover planning\n\n## Code Example\n\n```sql\nCREATE TABLE tenants (\n  tenant_id TEXT NOT NULL,\n  data JSONB,\n  region TEXT\n);\nALTER TABLE tenants ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_isolation ON tenants\n  USING (tenant_id = current_setting('app.tenant_id')::TEXT);\n```\n\n## Follow-up Questions\n\n- How would you test RLS policies at scale across regions?\n- What monitoring would you put in place to detect cross-region replication lag?","diagram":"flowchart TD\n  W[Writer in us-east-1] --> R1[Replica eu-west-1]\n  W --> R2[Replica ap-south-1]\n  R1 --> C1[(Cache layer)]\n  R2 --> C2[(Cache layer)]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T07:37:43.229Z","createdAt":"2026-01-16T07:37:43.230Z"},{"id":"q-2907","question":"A multi-region fintech app stores trades in Aurora PostgreSQL us-west-2 with sub-5ms writes and analytics in eu-central-1. Design an AWS-native architecture to meet RPO 5s and RTO 60s, ensure immutable audit logs in S3 with Object Lock, and maintain data residency. Compare Aurora Global Database writer+region replica vs DynamoDB+DMS analytics path, with concrete config (instance classes, replica counts, backup windows, KMS keys, IAM roles) and failover plan?","answer":"Use Aurora PostgreSQL Global Database with writer in us-west-2 and a read replica in eu-central-1 for sub-5ms writes and near-real reads; enable multi-AZ, cross-region backups, and PITR with 5s RPO. S","explanation":"Why This Is Asked\nTests cross-region DR, data residency, and immutable auditing in a realistic fintech context where latency targets are strict and analytics must co-exist with OLTP.\n\nKey Concepts\n- Aurora Global Database topology and cross-region replication\n- Immutable audit logging with S3 Object Lock and KMS\n- Data residency controls and per-tenant access (RLS)\n\nCode Example\n```typescript\n// CDK-like sketch for Aurora Global Database setup (illustrative)\nimport * as cdk from 'aws-cdk-lib';\nimport * as rds from 'aws-cdk-lib/aws-rds';\nimport * as ec2 from 'aws-cdk-lib/aws-ec2';\n\nconst vpc = new ec2.Vpc(this, 'VPC');\nnew rds.CfnDBCluster(this, 'WriterCluster', {\n  engine: 'aurora-postgresql',\n  databaseName: 'trades',\n  // replicas and cross-region and other config would be here\n});\n```\n\nFollow-up Questions\n- How would you handle schema changes with zero downtime across regions?\n- What monitoring and alerting would you implement to ensure RPO/RTOs are met in practice?","diagram":"flowchart TD\nA[Trade Write] --> B[(Aurora Global Writer in us-west-2)]\nB --> C[(Replica in eu-central-1)]\nA --> D[(Audit to S3 with Object Lock)]\nE[Analytics via DMS] --> F[Redshift/Analytics]\n","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","MongoDB","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T16:57:36.602Z","createdAt":"2026-01-16T16:57:36.602Z"},{"id":"q-2935","question":"Design a cross-region fintech data path where writes land in DynamoDB Global Tables (multi-master) in us-east-1 and eu-west-1, while analytics are powered by an Aurora PostgreSQL cluster in eu-west-1. Explain conflict handling, data residency for PCI-DSS, and how you meet RPO <5s and RTO <60s. Include concrete configurations: DynamoDB table keys and streams, Lambda, DMS or Debezium, Aurora size, backups, and network topology?","answer":"Use DynamoDB Global Tables (us-east-1 and eu-west-1) for multi-master writes; analytics replicated to Aurora PostgreSQL (eu-west-1) via a DynamoDB Streams → Lambda → DMS CDC pipeline. Resolve conflict","explanation":"## Why This Is Asked\nTests ability to design cross-region, multi-master transactional paths with real-time analytics, while honoring security and compliance constraints.\n\n## Key Concepts\n- DynamoDB Global Tables for low-latency multi-region writes\n- CDC pipeline (Streams → Lambda → DMS/Debezium) to Aurora\n- Conflict resolution and data governance for cross-region writes\n- PCI-DSS residency and EU-centric encryption/backups\n\n## Code Example\n```javascript\n// Lambda sample: process DynamoDB stream and push to Aurora via CDC\nexports.handler = async (event) => {\n  for (const rec of event.Records) {\n    if (rec.eventName === 'INSERT' || rec.eventName === 'MODIFY') {\n      const item = AWS.DynamoDB.Converter.unmarshall(rec.dynamodb.NewImage);\n      await upsertAurora(item);\n    }\n  }\n};\n```\n\n## Follow-up Questions\n- How would you handle a write-conflict when two regions update the same item simultaneously?\n- What monitoring and alerting would you implement to guarantee RPO and RTO targets under failover conditions?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T17:55:49.159Z","createdAt":"2026-01-16T17:55:49.159Z"},{"id":"q-2979","question":"Beginner scenario: An online storefront runs an AWS RDS MySQL instance in a single region with 1–2 read replicas. Compliance requires RPO 15 minutes and RTO 60 seconds during disaster recovery. Propose a practical backup and recovery plan using automated backups, PITR, Multi-AZ, and read replicas, with concrete values for backup retention, PITR window, and snapshot cadence, plus a reproducible failover procedure?","answer":"Enable automated backups with PITR window 7 days and backup retention 7 days; use Multi-AZ for automatic failover; create 1–2 read replicas for reads and potential DR primaries; schedule daily automat","explanation":"## Why This Is Asked\nTests practical understanding of RDS backup/DR features and DR testing steps in a beginner-friendly way.\n\n## Key Concepts\n- Automated backups and PITR\n- Multi-AZ vs Read Replicas\n- DR testing for RTO/RPO\n- Snapshot cadence and retention\n\n## Code Example\n```bash\n# Create a manual snapshot example\naws rds create-db-snapshot --db-instance-identifier storefront-db --db-snapshot-identifier storefront-sn-20260116\n```\n\n## Follow-up Questions\n- How would you monitor backup success and alert on failures?\n- How would you perform a low-traffic failover test with minimal user disruption?","diagram":"flowchart TD\nA[Start] --> B[Automated backups enabled]\nB --> C[Multi-AZ]\nC --> D[Read replicas configured]\nD --> E[Failover by promoting replica]\nE --> F[Switch endpoint (Route 53)]\nF --> G[Validate RPO/RTO]","difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T19:43:48.467Z","createdAt":"2026-01-16T19:43:48.467Z"},{"id":"q-3137","question":"Aurora PostgreSQL is deployed for a new product with highly variable traffic but strict latency targets. Compare Aurora Serverless v2 vs provisioned Aurora with read replicas for cost, latency, and maintenance. Provide a concrete setup example: (A) Serverless v2 with min 2 ACU, max 32 ACU, pause after idle, and RDS Proxy for connection pooling; (B) provisioned Aurora with 2 writer instances and 3 read replicas in the same region, plus a PgBouncer pool. Conclude when to choose each?","answer":"Serverless v2 excels with highly variable traffic, scaling from ~2 to 32 ACU and reducing idle costs; use RDS Proxy to stabilize connection churn. Provisioned Aurora works best for steady bursts with ","explanation":"## Why This Is Asked\n\nTests the candidate's understanding of when to use Aurora Serverless v2 versus provisioned Aurora, focusing on cost, latency stability, and maintenance practices for variable workloads.\n\n## Key Concepts\n\n- Aurora Serverless v2 scaling behavior and pause settings\n- RDS Proxy vs PgBouncer for connection pooling\n- Read replicas and failover implications\n- Cost versus latency trade-offs in serverless vs provisioned models\n\n## Code Example\n\n```sql\n-- example query to illustrate typical workload\nSELECT * FROM orders WHERE created_at >= now() - interval '1 day' LIMIT 100;\n```\n\n## Follow-up Questions\n\n- How would you measure and compare P95 latency and cost between the two setups over a 2-week window?\n- What failure modes differ between Serverless v2 and provisioned with read replicas, and how would you test them?","diagram":"flowchart TD\n  ATraffic[Traffic load] --> BOption{Aurora option}\n  BOption --> CServerless[Serverless v2]\n  BOption --> DProvisioned[Provisioned Aurora + Read Replicas]\n  CServerless --> EProxy[RDS Proxy for connections]\n  DProvisioned --> FPgBouncer[PgBouncer pool]\n  CServerless --> GCost[Cost optimization on idle]\n  DProvisioned --> HLatency[Lower controlled latency during peaks]","difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:16:21.862Z","createdAt":"2026-01-17T04:16:21.862Z"},{"id":"q-3289","question":"Two-region SaaS with strict tenant data residency: tenants assigned to a region cannot write in the other region, but hot tenants require sub-20ms reads globally. OLTP store is Aurora PostgreSQL and needs to support a single writer region with read replicas in the other region. Design a data layer that enforces per-tenant residency, delivers fast reads, and meets RPO 5s and RTO 60s. Include concrete configurations, data partitioning, routing rules, and a rollback plan for residency violations?","answer":"Route writes by tenant to its designated writer region; use Aurora PostgreSQL Global Database with writer in us-east-1 and a read replica in eu-west-1. Enforce per-tenant residency in the app via a re","explanation":"## Why This Is Asked\nThe question probes practical enforcement of data residency DevOps patterns while maintaining low-latency reads and robust DR in a two-region setup. It tests architectural judgment on cross-region replication, routing, and tenant isolation.\n\n## Key Concepts\n- Aurora PostgreSQL Global Database with a single writer region and cross-region replicas\n- Per-tenant data residency policies and region-aware schema partitioning\n- Read routing to nearest replica to minimize latency\n- RPO/RTO targets and PITR backups in multi-region setups\n\n## Code Example\n```javascript\n// Pseudo routing logic for tenant requests\nfunction routeRequest(tenantId, operation){\n  const region = residencyMap[tenantId];\n  if(operation === 'WRITE' && currentRegion !== region){\n    throw new Error('Tenant not writable in this region');\n  }\n  // route to appropriate endpoint based on operation\n  return regionEndpoints[region][operation];\n}\n```\n\n## Follow-up Questions\n- How would you handle a tenant migrating regions without data loss or RPO impact?\n- How would you validate RPO/RTO during a simulated regional outage and during maintenance windows?","diagram":"flowchart TD\nA[Tenant Residency Policy] --> B[Policy Enforcement]\nB --> C[Writes go to regional writer]\nB --> D[Reads directed to local replica]\nC --> E[Aurora Global DB: writer us-east-1; replica eu-west-1]\nD --> F[Sub-20ms reads for hot tenants]","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Instacart"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T10:30:32.064Z","createdAt":"2026-01-17T10:30:32.064Z"},{"id":"q-3367","question":"In a three-region deployment (us-east-1, eu-west-1, ap-southeast-2) with a single Aurora PostgreSQL Global Database writer in us-east-1, design a 2-tier data path that delivers sub-20 ms reads for hot tenants region-locally while writes can occur anywhere. Propose a cross-region caching strategy using Redis Global Datastore, plus a per-region analytics store. Include concrete configurations (replica counts, Redis node types, TTLs, backup windows, cross-region data sharing, and data residency controls) to meet RPO 5s and RTO 60s?","answer":"To meet RPO 5s and RTO 60s, run Aurora PostgreSQL Global Database with 1 writer in us-east-1 and read replicas in eu-west-1 and ap-southeast-2. Add a Redis Global Datastore: 2 nodes per region (memory","explanation":"## Why This Is Asked\\nTests cross-region data residency and consistency trade-offs under advanced requirements; ensures candidate can architect cross-region caching, RPO/RTO, and data governance.\\n\\n## Key Concepts\\n- Aurora Global Database characteristics (writer region, replication lag)\\n- Redis Global Datastore and cross-region caching\\n- Cache-aside pattern and TTL tuning\\n- Cross-region backups and KMS keys and data residency\\n- CDC to analytics stores and data governance\\n\\n## Code Example\\n```json\n{\n  \\\"redisGlobalDatastore\\\": {\n    \\\"regions\\\": [\\\"us-east-1\\\",\\\"eu-west-1\\\",\\\"ap-southeast-2\\\"],\n    \\\"nodesPerRegion\\\": 2,\n    \\\"ttlSeconds\\\": 300\n  }\n}\n```\n\\n## Follow-up Questions\\n- How would you monitor cross-region replication lag and SLA adherence?\\n- How would you handle a region outage and the impact on RPO/RTO?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T13:43:12.604Z","createdAt":"2026-01-17T13:43:12.604Z"},{"id":"q-3440","question":"Design and compare two data-architecture options for a PCI-DSS-compliant, multi-tenant SaaS using AWS databases: option A uses a single Aurora PostgreSQL cluster with Row-Level Security (RLS) to isolate tenants and KMS-encrypted backups plus cross-region snapshot replication; option B uses separate clusters per tenant across regions. Which approach would you pick and why?","answer":"Recommend a single Aurora PostgreSQL cluster with Row-Level Security (RLS) to isolate tenants, backed by KMS-encrypted backups and cross-region snapshot replication for DR. Compare with per-tenant clu","explanation":"## Why This Is Asked\n\nAssess ability to trade off isolation, cost, and DR in a PCI-DSS context; tests understanding of RLS, encryption, backups, cross-region replication, and migration paths.\n\n## Key Concepts\n\n- Row-Level Security (RLS) in PostgreSQL/Aurora\n- AWS KMS encryption for backups and at-rest data\n- Cross-region snapshot replication and RPO/RTO targets\n- Tenancy isolation trade-offs: single-cluster with shared schema vs per-tenant clusters\n- Migration and operational overhead\n\n## Code Example\n\n```sql\nCREATE POLICY tenant_rls ON public.users\nUSING (tenant_id = current_setting('app.tenant_id')::int);\nALTER ROLE app_user SET app.tenant_id = '1';\n```\n\n```\nNote: This snippet illustrates an RLS policy bound to a per-session tenant_id setting.\n```\n\n## Follow-up Questions\n\n- How would you monitor per-tenant access and detect policy misuse?\n- How would you onboard/offboard tenants with minimal downtime?","diagram":"flowchart TD\n  A[Option A: Single cluster with RLS] --> B[KMS encryption on backups]\n  A --> C[Cross-region snapshot replication]\n  D[Option B: Per-tenant clusters] --> E[Stronger isolation]\n  D --> F[Higher operational overhead]\n  B --> G[DR readiness]\n  C --> H[RPO/RTO targets]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","LinkedIn","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T16:38:56.423Z","createdAt":"2026-01-17T16:38:56.423Z"},{"id":"q-3465","question":"Scenario: A real-time fraud graph workload spans three regions. Primary data sits in Neptune with cross-region replication. Need sub-50ms neighbor lookups on random 1k-node subgraphs; writes across regions; DR targets: RPO 5s, RTO 60s. Design a concrete architecture comparing Neptune Global Database alone vs a hybrid approach using Neptune in each region plus ElastiCache Redis caches and a streaming path to OpenSearch for analytics. Provide concrete config: engine versions, instance counts, TTLs, and explicit failover steps?","answer":"Recommendation: use Neptune Global Database with a single writer in us-east-1 and regional readers in eu-west-1 and ap-south-1. Add ElastiCache Redis per region (2 shards, 3–4 nodes each) caching 1k-n","explanation":"## Why This Is Asked\nTests multi-region graph workloads, consistency, and DR trade-offs between a pure managed graph DB vs a hybrid approach with in-region caching and analytics streaming. It probes Neptune Global Database, Streams, ElastiCache, and OpenSearch integration for real-time surfaces.\n\n## Key Concepts\n- Neptune Global Database cross-region replication and single-writer model\n- Neptune Streams for change data capture\n- ElastiCache Redis caching and per-region invalidation\n- OpenSearch as a downstream analytics sink\n- DR: RPO/RTO targets and failover orchestration\n\n## Code Example\n```javascript\n// Example: Lambda handler for Neptune Stream events to invalidate regional caches\nexports.handler = async (event) => {\n  for (const rec of event.Records) {\n    // parse Neptune stream record\n    // identify affected neighborhood keys and regional caches\n    // publish invalidation to corresponding Redis clusters\n  }\n}\n```\n\n## Follow-up Questions\n- How would you validate RPO/RTO in this architecture? \n- What metrics would you monitor to detect cache invalidation lag and cross-region replication delay?","diagram":"flowchart TD\n  N[Neptune Global DB] --> R[Region Replicas]\n  R --> C[ElastiCache Redis (per region)]\n  C --> O[OpenSearch (Analytics)]\n  N --> S[Neptune Streams]","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Citadel","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T17:30:52.953Z","createdAt":"2026-01-17T17:30:52.953Z"},{"id":"q-3587","question":"Compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables for a read-heavy user profile store spanning us-east-1 and us-west-2. Provide concrete configurations to meet RPO 5s and RTO 60s; detail data model fit, backup plans, and failover steps. Assume a SQL workload with FK constraints and occasional transactional updates; keep latency targets <50ms in primary region?","answer":"Choose Aurora PostgreSQL Global Database for this use case. It maintains relational integrity and supports the complex SQL operations required for user profiles with foreign key constraints. Configure the deployment as follows: primary region us-east-1 with 1 writer instance (db.r5.large) and 2 reader instances, secondary region us-west-2 with 2 reader instances (db.r5.large). Enable cross-region replication with sub-second lag to meet the RPO 5s requirement. Configure automated backups for 35 days with point-in-time recovery enabled. For RTO 60s: implement Aurora's built-in fast failover mechanism combined with Route 53 health checks and automated failover scripts. The relational data model should include normalized tables for user profiles, preferences, and activity logs with proper foreign key constraints. Backup strategy combines daily snapshots, continuous backups, and cross-region backup copies. Failover procedure: 1) Monitor primary region health via Route 53 checks, 2) Automatically promote secondary region to writer upon failure detection, 3) Update DNS records to redirect traffic, 4) Validate connectivity and data consistency. Performance targets: <50ms latency for reads in us-east-1, <100ms for cross-region reads from us-west-2.","explanation":"## Why This Is Asked\n\nThis question evaluates cross-region disaster recovery design for AWS databases and tests the ability to choose between relational and NoSQL solutions based on workload requirements.\n\n## Key Concepts\n\n- Aurora Global Database architecture and replication mechanisms\n- DynamoDB Global Tables and conflict resolution strategies\n- RPO/RTO requirements and corresponding recovery approaches\n- Point-in-time recovery (PITR) and comprehensive backup strategies\n- Cross-region failover automation and DNS management\n- Relational data modeling with foreign key constraints\n- SQL workload optimization and transactional consistency guarantees\n\n## Code Example\n\n```sql\n-- Example: Query user profile with related data\nSELECT u.user_id, u.email, u.created_at,\n       p.theme, p.language, p.notifications,\n       a.last_login, a.activity_count\nFROM users u\nLEFT JOIN user_preferences p ON u.user_id = p.user_id\nLEFT JOIN user_activity a ON u.user_id = a.user_id\nWHERE u.user_id = :user_id;\n```\n\nThis query demonstrates the relational nature of user profile data and the need for JOIN operations that Aurora PostgreSQL handles efficiently, while DynamoDB would require multiple queries or data denormalization.","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:33:42.352Z","createdAt":"2026-01-17T22:36:20.795Z"},{"id":"q-3673","question":"Scenario: You manage an Aurora PostgreSQL Global Database with writer in us-east-1 and read replicas in eu-west-1 and ap-southeast-2. A new tenant requires strict data isolation via Row-Level Security and an auditable analytics path. Design a plan that (a) enforces per-tenant isolation in the global cluster (RLS + SECURITY DEFINER wrappers), (b) maintains cross-region replication with RPO <= 10s while writes land in writer only, (c) captures tenant-access events to S3 with a near-real-time pipeline (DMS) across regions, and (d) specifies concrete DB settings (instance class, replica count, parameter group, backup window, and cross-region KMS keys) to meet an RTO <= 60s. Include concrete configuration choices?","answer":"Implement per-tenant RLS on all tables using a session parameter (app.tenant_id) and a SECURITY DEFINER wrapper to enforce access; use a dedicated tenant role and policy: USING (tenant_id = current_se","explanation":"## Why This Is Asked\nTests ability to design multi-region OLTP with data isolation, auditability, and DR under real-world constraints.\n\n## Key Concepts\n- Aurora PostgreSQL Global Database architecture and cross-region replication timing\n- Row-Level Security in PostgreSQL and security wrappers\n- Change Data Capture with AWS DMS to S3 and analytics lake\n- Data residency and encryption with region-specific KMS keys\n- DR planning: RPO/RTO targets with backups and writes to writer only\n\n## Code Example\n```sql\n-- Enable RLS on a table and enforce tenant access\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_access ON orders\n  FOR ALL USING (tenant_id = current_setting('app.tenant_id')::int);\n\n-- Set tenant for the session\nSET app.tenant_id = '123';\n```\n\n## Follow-up Questions\n- How would you monitor replication lag and auto-tune?\n- How would you validate RLS coverage across regional replicas during failover?","diagram":"flowchart TD\n  A[Writer us-east-1] --> B[Global DB]\n  B --> C[EU-West read replica]\n  B --> D[AP-Southeast read replica]\n  E[Audit pipeline (DMS to S3)] --> F[(Audit data lake)]\n  C --> F\n  D --> F","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Google","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T04:22:02.471Z","createdAt":"2026-01-18T04:22:02.471Z"},{"id":"q-3711","question":"Scenario: Build a mobile analytics backend in us-east-1 handling 1.5k–2k writes/sec and per-user reads for dashboards. Compare DynamoDB with Global Tables vs Aurora MySQL Serverless v2 for this workload. Provide a concrete setup (schema, indexes, backups, DR, failover steps) to meet RPO 5s and RTO 60s, and note consistency and cost trade-offs?","answer":"Leverage DynamoDB Global Tables in us-east-1 and eu-west-1 to meet 5s RPO and 60s RTO. Data model: partition key user_id, sort key event_ts; GSI on event_type; enable on-demand backups and PITR to S3;","explanation":"## Why This Is Asked\n\nTests ability to select between managed AWS databases for a high-ingest, per-user read workload, and to translate SLAs (RPO/RTO) into concrete configurations using familiar features such as Global Tables, PITR, TTL, and streams. It also probes cost considerations and failover testing strategies.\n\n## Key Concepts\n\n- DynamoDB Global Tables for cross-region DR\n- Point-in-Time Restore (PITR) and on-demand backups\n- TTL (time-to-live) and event lifecycle planning\n- DynamoDB Streams as CDC to data lake\n- Aurora Serverless v2 trade-offs (read scaling, cold starts)\n\n## Code Example\n\n```javascript\n// DynamoDB put item example (AWS SDK v3)\nconst item = { user_id: {S: 'u123'}, event_ts: {N: '1700000000'}, payload: {S: '{}'} };\nconst params = { TableName: 'Events', Item: item };\n```\n\n## Follow-up Questions\n\n- How would you validate RPO/RTO in a staged failover test?\n- How would you handle hot partitions or skewed write throughput in DynamoDB?","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","DoorDash","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T06:48:28.893Z","createdAt":"2026-01-18T06:48:28.893Z"},{"id":"q-3772","question":"In a multi-region deployment, need strong OLTP in us-east-1 and real-time analytics in eu-west-1. The dashboards must reflect writes within ~5 seconds with writes allowed in both regions. Design a practical CDC path using AWS tools (DMS vs Debezium), specify data flow, latency targets, conflict handling, backups, and monitoring to meet RPO 5s and RTO 60s?","answer":"Use DMS CDC from Aurora PostgreSQL in us-east-1 to Redshift in eu-west-1, with an initial full load followed by ongoing CDC. Capture via WAL/txn logs, throttle to 5–10s batches, filter to analytic-tab","explanation":"## Why This Is Asked\nAssesses cross-region data pipelines, latency budgets, and DR planning across OLTP and analytics.\n\n## Key Concepts\n- CDC pipelines (DMS vs Debezium)\n- Cross-region latency, RPO/RTO targets\n- Data modeling for analytics vs transactional data\n\n## Code Example\n```javascript\n{\n  \"ReplicationTaskSettings\": \"{\\\"ParallelLoadThreads\\\":4,\\\"BatchApplyEnabled\\\":true}\",\n  \"TableMappings\": \"{\\\"rules\\\":[{\\\"rule-type\\\":\\\"selection\\\",\\\"object-locator\\\":{\\\"schema-name\\\":\\\"public\\\",\\\"table-name\\\":\\\"*\\\"},\\\"rule-action\\\":\\\"include\\\"}] }\"\n}\n```\n\n## Follow-up Questions\n- How would you handle write conflicts if writes occur in both regions simultaneously?\n- What monitoring dashboards and alerts would you configure to ensure RPO/RTO adherence?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Meta","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T08:51:49.792Z","createdAt":"2026-01-18T08:51:49.792Z"},{"id":"q-3816","question":"Two-region SaaS using Aurora PostgreSQL with writes allowed in both regions. Propose a concrete, auditable design to meet RPO 5s and RTO 60s while ensuring tenant data isolation. Compare Aurora PostgreSQL Global Database (single writer) vs independent clusters with bidirectional logical replication (DMS or Debezium), including conflict resolution, auditing, backups, and monitoring. Provide a recommended concrete configuration and rationale?","answer":"Recommendation: two independent Aurora PostgreSQL clusters (us-east-1 and eu-west-1) with bidirectional logical replication (DMS or Debezium) and a tenant-aware conflict policy (per-tenant last-writer","explanation":"## Why This Is Asked\nAuditors and engineers must balance latency, consistency, and auditability in multi-region writes.\n\n## Key Concepts\n- Aurora Global Database vs multi-region writes\n- Conflict resolution strategies in bidirectional replication\n- Tamper-evident auditing with S3 Object Lock and KMS\n- PITR, cross-region backups, and monitoring replication lag\n\n## Code Example\n```bash\n# Example DMS task setup (illustrative)\naws dms create-replication-task --replication-task-identifier bidir-task \\\n  --source-endpoint-arn <src> --target-endpoint-arn <tgt> \\\n  --replication-instance-arn <ri> --migration-type full-load-and-ccdc \\\n  --table-mappings file://mappings.json\n```\n\n## Follow-up Questions\n- How would you handle tenant schema changes mid-flight?\n- What are your observability defaults for replication lag and data freshness?","diagram":"flowchart TD\n  US[US-East Writer] --> REB[Bidirectional WAL Replication]\n  REB --> EU[EU-West Writer]\n  EU --> AUD[Audit Trail to S3 (immutable)]\n  AUD --> MON[Monitoring & Backups]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Databricks","DoorDash"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T10:41:08.899Z","createdAt":"2026-01-18T10:41:08.899Z"},{"id":"q-3977","question":"Scenario: A SaaS app needs per-tenant data isolation and fast reads in two AWS regions, with ~100 tenants and light writes. Compare DynamoDB Global Tables vs Aurora PostgreSQL cross-region replicas, focusing on data model, transactional guarantees, backups/DR, latency, and cost. Propose a concrete starter config to meet an RPO of 5 seconds and an RTO of 60 seconds (include region pair and basic sizing)?","answer":"Choose DynamoDB Global Tables if cost and simple organization trump strict cross-region transactions. DynamoDB favors low maintenance and fast regional reads; Global Tables replicate quickly but are n","explanation":"## Why This Is Asked\n\nTests understanding of cross-region data replication, consistency models, and operational trade-offs for multi-region SaaS workloads. The beginner level focuses on practical choices rather than exotic configurations.\n\n## Key Concepts\n\n- DynamoDB Global Tables: multi-region replication, eventual/strong reads, transactional support via DynamoDB transactions, cost implications.\n- Aurora PostgreSQL cross-region replication: true ACID across regions, read replicas, promotion/failover considerations, backup strategies.\n- Data isolation patterns: per-tenant keys vs schemas, access controls, and backups.\n- DR planning: RPO/RTO targets, PITR windows, cross-region backups.\n\n## Code Example\n\n```text\n# DynamoDB (pseudo) - PutItem with conditional write to ensure tenant isolation\naws dynamodb put-item --table-name TenantEvents \\\n  --item '{\"TenantId\":{\"S\":\"t1\"},\"EventId\":{\"S\":\"e123\"},\"Payload\":{\"S\":\"...\"}}' \\\n  --condition-expression 'attribute_not_exists(EventId)'\n```\n\n## Follow-up Questions\n\n- If cross-region reads must be <5 ms at 99th percentile, how would you adjust the architecture?\n- How would you implement per-tenant access controls and schema migrations in each approach?","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T18:38:41.046Z","createdAt":"2026-01-18T18:38:41.047Z"},{"id":"q-4194","question":"A startup SaaS app experiences highly variable traffic with 2-5x daily peaks and a need for both relational queries and fast session lookups. Design a cost-conscious AWS data layer comparing Aurora Serverless v2 (PostgreSQL) versus DynamoDB (on-demand) for this workload. Include data model, indexing, consistency, backups, failover, latency targets, and concrete configs to meet an RPO of 15s and an RTO of 60s?","answer":"Recommendation: DynamoDB on-demand for hot keys (sessions, user profiles) and Aurora Serverless v2 for relational queries. Use DynamoDB Global Tables across two regions with PITR and Streams for CDC t","explanation":"## Why This Is Asked\nAssess practical thinking on choosing between serverless relational vs NoSQL with cross-region DR. It tests data modeling and when to split workloads.\n\n## Key Concepts\n- Serverless data layers\n- Global Databases and Global Tables\n- PITR, CDC, cross-region replication\n- Latency, RPO, RTO trade-offs\n\n## Code Example\n```javascript\n// Config sketch (conceptual)\nconst config = {\n  dynamo: { mode: 'on-demand', globalTables: true, regions: ['us-east-1','eu-west-1'] },\n  aurora: { mode: 'Serverless v2', regions: ['us-east-1','eu-west-1'], replicas: 2 }\n}\n```\n\n## Follow-up Questions\n- How would you test the RPO/RTO targets in a staging environment?\n- What monitoring alerts would you set for cross-region replication lag?","diagram":"flowchart TD\n  A[User action] --> B[DynamoDB on-demand]\n  B --> C[Global Tables across us-east-1,eu-west-1]\n  A --> D[Aurora Serverless v2]\n  D --> E[Aurora Global DB with cross-region replicas]\n","difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Oracle","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T07:08:06.564Z","createdAt":"2026-01-19T07:08:06.564Z"},{"id":"q-4210","question":"Beginner scenario: An Aurora PostgreSQL cluster stores PCI data in **us-east-1**. To meet RPO < 15s and RTO < 2m after accidental data deletion, propose a practical backup/DR plan using automated backups, PITR, cross-region replication (Global Database), and cross-account DR drills. Include concrete settings (backup retention days, PITR window, replica count, failover priority) and the exact steps to run a DR drill?","answer":"Enable Aurora Global Database with a DR region; set automated backups retention to 14 days; configure PITR window to 30 minutes; create one read replica in the DR region as a member; run quarterly DR ","explanation":"## Why This Is Asked\nTests practical DR planning in AWS DB, including backups, PITR, and cross-region failover for PCI data.\n\n## Key Concepts\n- Aurora Global Database, PITR, automated backups\n- Cross-region DR, retention windows, failover priorities\n- Validation steps and drill frequency\n\n## Code Example\n```bash\n# illustrative AWS CLI commands (non-executable in this snippet)\naws rds create-global-database --global-database-name MyAuroraGlobal --source-db-cluster-identifier my-cluster --engine aurora-postgresql\naws rds create-global-database-member --global-database-name MyAuroraGlobal --region us-west-2\n```\n\n## Follow-up Questions\n- How would you monitor RPO/RTO during a drill and alert on misses?\n- What cost controls would you apply for cross-region DR?","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T08:52:47.500Z","createdAt":"2026-01-19T08:52:47.500Z"},{"id":"q-4310","question":"You run an Aurora PostgreSQL Global Database with a writer in us-east-1 and read replicas in eu-west-1 and ap-south-1. A new requirement demands sub-50ms per-tenant OLTP reads and near-real-time analytics per tenant using a separate analytics store. Propose a concrete hybrid architecture (OLTP + analytics) using AWS services (e.g., DMS, DynamoDB, Redshift/Glue), detail data flows, replication settings, consistency, failover, and a plan to validate SLAs before production?","answer":"Use Aurora PostgreSQL Global Database with a single writer in us-east-1 and cross-region replicas in eu-west-1 and ap-south-1. For analytics, stream CDC with AWS DMS to a DynamoDB per-tenant analytics","explanation":"## Why This Is Asked\nTests hybrid OLTP+Analytics design across regions, cross-region replication, and practical SLA enforcement.\n\n## Key Concepts\n- Aurora Global Database: single writer, cross-region replicas\n- DMS CDC: live change data capture to analytics store\n- Latency routing: per-tenant read paths\n- SLA validation: load tests and DR drills\n\n## Code Example\n```bash\n# Example DMS task (illustrative)\naws dms create-replication-task --replication-task-identifier OLTP-Analytics-Task \\\n  --source-endpoint-arn <source> --target-endpoint-arn <target> \\\n  --replication-task-settings '{\"TargetMetadata\": {\"ParallelApply\": true}}'\n```\n\n## Follow-up Questions\n- How would you monitor and tune cross-region replication lag for <50ms OLTP reads?\n- What failure scenarios would you test in DR drills to satisfy RTO < 60s?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Google","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T13:12:58.151Z","createdAt":"2026-01-19T13:12:58.152Z"},{"id":"q-4355","question":"You maintain an Aurora PostgreSQL cluster serving 100+ tenants in a SaaS product. Data for all tenants lives in the same database but must be strictly isolated in reads; tenants have varying access patterns and analytics needs. Design a solution using PostgreSQL Row-Level Security and tenant_id partitioning, plus AWS tools (Global Database, PITR, cross-region replicas) to meet isolation, sub-second hot-tenant reads, and near real-time analytics for aggregates. Include credential rotation and audit considerations. What is your approach?","answer":"Implement per-tenant isolation with PostgreSQL Row-Level Security (RLS) and tenant_id partitioning; enforce policy USING (tenant_id = current_setting('app.tenant')::int). Route hot tenants to optimize","explanation":"## Why This Is Asked\n\nTests advanced skills in multi-tenant DB design, RLS, and AWS DR/analytics integration. Requires concrete decisions on isolation, latency, and operational controls beyond generic design.\n\n## Key Concepts\n\n- PostgreSQL Row-Level Security (RLS) and policy design\n- Table partitioning by tenant_id and hot-tenant indexing\n- Aurora Global Database for cross-region analytics\n- PITR configuration and cross-region replica topology\n- Secrets Manager for credential rotation and CloudWatch/CloudTrail for auditing\n\n## Code Example\n\n```javascript\n// Example: JS snippet showing RLS policy application via client\nasync function setupRLS(client, tenant) {\n  await client.query(\"ALTER TABLE orders ENABLE ROW LEVEL SECURITY;\");\n  await client.query(\"CREATE POLICY tenant_rls ON orders USING (tenant_id = current_setting('app.tenant')::int);\");\n}\n```\n\n## Follow-up Questions\n\n- How would you test tenant isolation guarantees in DR drills?\n- Describe monitoring and alerting for RLS misconfigurations and partition bloat.","diagram":"flowchart TD\n  A[Tenant Isolation] --> B[Partitioning by tenant_id]\n  B --> C[RLS Policies]\n  C --> D[Analytics via Global DB]\n  D --> E[Auditing & Compliance]","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Hugging Face","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T15:40:51.696Z","createdAt":"2026-01-19T15:40:51.696Z"},{"id":"q-4372","question":"Cross-account analytics mirror: A SaaS app writes to an RDS PostgreSQL in us-east-1. A separate analytics account needs a read-only mirror for dashboards with low lag. Design a beginner-friendly pattern using cross-account RDS snapshot sharing and a refreshed read replica, specify backup retention, sharing steps, and the exact process to refresh daily?","answer":"Enable automated backups on the source RDS and create a manual snapshot. Share that snapshot with the analytics account; in the analytics account, copy the shared snapshot and restore it as a new, rea","explanation":"## Why This Is Asked\nTests practical cross‑account data sharing, a common beginner pattern for analytics pipelines, including permissions, backup cadence, and refresh automation.\n\n## Key Concepts\n- Cross‑account snapshot sharing\n- Read-only replica restoration in analytics account\n- IAM roles for access control\n- Backup retention and CloudWatch monitoring\n\n## Code Example\n```bash\n# In source account: share the snapshot\naws rds modify-db-snapshot-attribute --db-snapshot-identifier my-snap --attribute-name restore --values 111122223333\n```\n\n## Follow-up Questions\n- How would you handle encryption keys across accounts?\n- What would you monitor to detect drift between source and mirror?","diagram":"flowchart TD\n  A(Source RDS us-east-1) --> B(Share snapshot with Analytics acct)\n  B --> C(Copy snapshot in Analytics acct)\n  C --> D(Restore as read-only RDS in Analytics VPC)\n  D --> E(Grant read-only DB role to dashboards)\n  E --> F(Automate daily refresh and monitor lag)\n","difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T16:39:12.075Z","createdAt":"2026-01-19T16:39:12.077Z"},{"id":"q-4403","question":"Scenario: A fintech SaaS stores PCI-DSS data for some tenants in us-east-1 and non-PCI data globally. Design an architecture using Aurora PostgreSQL Global Database to minimize latency, ensure strict data residency, and provide auditable backups and DR. Include topology, data isolation (RLS), backup strategy, and DR test steps?","answer":"Use Aurora PostgreSQL Global Database with a PCI-primary in us-east-1 and a regional replica in eu-west-1 for non-PCI tenants. Enforce per-tenant isolation via Row-Level Security and per-tenant schema","explanation":"## Why This Is Asked\n\nThis tests architecture for data residency, PCI compliance, multi-region DR, and per-tenant isolation at scale.\n\n## Key Concepts\n\n- Aurora Global Database topology with cross-region replicas\n- Row-Level Security and per-tenant schema isolation\n- Cross-account AWS Backup vaults and encrypted PITR in S3\n- DR validation, audit logging, and latency targets\n\n## Code Example\n\n```sql\n-- Example RLS policy for tenant isolation\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_isolation ON orders USING (tenant_id = current_setting('myapp.tenant_id')::int);\n```\n\n## Follow-up Questions\n\n- How would you monitor replication lag and alert on threshold breaches?\n- How would you test residency constraints and audit logs during DR drills?","diagram":"flowchart TD\n  A[PCI tenants: primary us-east-1]\n  B[Replica: eu-west-1 for non-PCI tenants]\n  C[Cross-account AWS Backup vaults]\n  D[DR test: cross-region restore]\n  A --> C\n  C --> B\n  B --> D","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Instacart","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T17:35:21.236Z","createdAt":"2026-01-19T17:35:21.236Z"},{"id":"q-4467","question":"In a two-region Aurora PostgreSQL Global Database with a primary cluster in us-east-1 and an analytics sink in eu-west-1 (Redshift), design a practical CDC pipeline to feed near-real-time analytics. Choose between AWS DMS and Debezium, justify the choice, and specify data flow, latency targets (RPO <5s, RTO <60s), conflict handling for mirrored updates, backups, and monitoring. Include concrete settings (PITR window, replication task settings, lag budgets) and DR testing cadence?","answer":"Adopt DMS CDC from Aurora primary in us-east-1 to Redshift in eu-west-1, with a 1k-row CDC batch, 1s max lag, and a medium-replication instance. Enable 35-day PITR, CloudWatch alarms on replicationLag","explanation":"## Why This Is Asked\nTests practical CDC design, latency budgeting, cross-region data freshness, and operational controls.\n\n## Key Concepts\n- Aurora Global Database cross-region replication\n- CDC latency and backfill\n- DMS vs Debezium trade-offs\n- PITR and DR drills\n\n## Code Example\n```javascript\n// Pseudo DMS task config (illustrative)\nconst dmsTask = {\n  MigrationType: \"cdc\",\n  SourceEndpointArn: \"arn:aws:dms:...\",\n  TargetEndpointArn: \"arn:aws:dms:...\",\n  TableMappings: \"{\\\"rules\\\":[]}\"\n}\n```\n\n## Follow-up Questions\n- How would you test RPO/RTO in this setup?\n- What would change if analytics sinks to Athena instead?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Apple","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T19:53:38.210Z","createdAt":"2026-01-19T19:53:38.210Z"},{"id":"q-4619","question":"A PCI-compliant SaaS stores customer data in Aurora PostgreSQL in us-west-2. An analytics sink in eu-central-1 must receive masked PII with near real-time updates. Design a streaming path using DMS or Debezium (or AWS Glue) that masks PII before replication, specify RPO <5s, RTO <60s, encryption (TLS at transit, KMS at rest), PITR windows, failover steps, and monitoring. Include data masking rules and DR testing cadence?","answer":"Design a DMS CDC path with a masking stage before the sink in eu-central-1 (e.g., redact SSNs and emails). Enforce TLS for all transfers; use CMKs in both regions for at-rest encryption. Set PITR to 3","explanation":"Why This Is Asked\nTests ability to design a cross-region, PCI-compliant streaming path with masking and robust DR. It probes data masking strategy, replication choice, and operational readiness.\n\nKey Concepts\n- Data masking in streaming CDC\n- Cross-region replication and latency targets\n- Encryption: TLS in transit, KMS at rest\n- PITR window and RPO/RTO goals\n- DR testing cadence and monitoring\n\nCode Example\n```javascript\n// Simple masking example used in testing\nfunction maskPII(value, field){\n  if(!value) return null;\n  switch(field){\n    case 'ssn': return 'XXX-XX-' + value.slice(-4);\n    case 'email': return '***@example.com';\n    default: return 'REDACTED';\n  }\n}\n```\n\nFollow-up Questions\n- How would you validate masking at scale across schema changes?\n- What metrics and alarms would you configure for DR drills and replication lag?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Twitter","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T04:37:45.859Z","createdAt":"2026-01-20T04:37:45.859Z"},{"id":"q-4634","question":"In a production Aurora PostgreSQL cluster in us-east-1 handling PCI data, you need a cost-effective staging clone for weekly integration tests that does not impact production performance. Explain how you would use Aurora Fast Database Clones (and/or Snapshots), setup, automation, and rollback strategy, including backup retention, PITR window, and permissions. Include concrete settings and steps to refresh weekly?","answer":"Use Aurora PostgreSQL Fast Database Clones to create a weekly staging clone from production in us-east-1. Take a snapshot, clone it to a dedicated staging cluster, mask PCI data, and run tests. Automa","explanation":"## Why This Is Asked\nIsolation of testing from production is critical for PCI workloads; clones enable realistic testing without affecting live performance or compliance.\n\n## Key Concepts\n- Aurora Fast Database Clones\n- Snapshot-based staging\n- PCI data masking\n- Access controls and least privilege\n\n## Code Example\n```javascript\n// Example: AWS SDK pseudocode for cloning workflow\n// This is illustrative; implement with your infra\nconst cloneWorkflow = async () => {\n  // 1) create a snapshot from prod\n  // 2) restore DB cluster from snapshot to staging\n  // 3) apply data masking\n  // 4) trigger tests\n  // 5) delete staging clone\n}\n``` \n\n## Follow-up Questions\n- How would you enforce data masking in the staging clone? \n- How would you validate that the PITR window settings do not expose production data in staging? \n- What monitoring alerts would you add for clone lifecycle failures?","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","NVIDIA","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T05:50:08.045Z","createdAt":"2026-01-20T05:50:08.045Z"},{"id":"q-4799","question":"Real-time bidding platform uses Aurora PostgreSQL Global Database with a primary in us-east-1 ingesting ~200k inserts/sec from a streaming pipeline; read replicas in eu-west-1 run analytics. Peak load spikes WAL generation and replica lag. Propose concrete optimizations to reduce WAL pressure and lag: batch commit sizing, WAL-related parameters (wal_level, max_wal_senders, wal_buffers), autovacuum tuning, and a connection-pooling strategy. Include exact parameter ranges, testing steps, and validation metrics?","answer":"Batch inserts of 1000–5000 rows per transaction; set wal_level = replica; max_wal_senders = 4–8; wal_buffers = 32MB; wal_keep_size = 512MB; autovacuum_vacuum_cost_limit = 2000; enable a connection poo","explanation":"## Why This Is Asked\nThis question probes practical WAL/replication tuning under multi-region Aurora PG, a common production pain point.\n\n## Key Concepts\n- WAL management: wal_level, max_wal_senders, wal_buffers\n- Replication lag and throughput metrics\n- Autovacuum tuning and connection pooling\n- Parameter groups and DR testing\n\n## Code Example\n```json\n{\n  \"wal_level\": \"replica\",\n  \"max_wal_senders\": \"6\",\n  \"wal_buffers\": \"32MB\",\n  \"wal_keep_size\": \"512MB\",\n  \"autovacuum_vacuum_cost_limit\": 2000\n}\n```\n\n## Follow-up Questions\n- How would you stage and test a DR drill for this tuning?\n- What monitoring and rollback plan would you implement if latency degrades?\n","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Microsoft","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T13:20:39.657Z","createdAt":"2026-01-20T13:20:39.657Z"},{"id":"q-4861","question":"In Aurora PostgreSQL Global Database, cross-region replication lag causes dashboards to show stale per-tenant metrics. Propose a pragmatic solution that keeps writes in one region but serves near-real-time analytics in a separate analytics sink. Include data flow (primary -> CDC -> analytics store), consistency guarantees, concrete settings (PITR window, replication slot, DMS task settings or Debezium), and DR testing cadence?","answer":"Implement an analytics sink fed by CDC from the primary Aurora PostgreSQL. Use logical replication slots and DMS (or Debezium) with an initial full load, then CDC in micro-batches; batch size 2000, pa","explanation":"## Why This Is Asked\nThis question tests practical dataflow design for cross-region analytics, balancing latency, consistency, and DR in a real product. It probes knowledge of Aurora Global Database limitations, CDC tooling, and how to validate freshness.\n\n## Key Concepts\n- Aurora PostgreSQL Global Database\n- CDC pipelines (DMS, Debezium)\n- Logical replication slots and WAL\n- DR testing and RPO/RTO validation\n- Upserts and sink consistency\n\n## Code Example\n```javascript\n-- sample upsert used in analytics sink\nINSERT INTO analytics.tenants_metric (tenant_id, ts, value)\nVALUES (?, NOW(), ?)\nON CONFLICT (tenant_id, ts) DO UPDATE SET value = EXCLUDED.value;\n```\n\n## Follow-up Questions\n- How would you adapt this approach for tenant-aware SLAs and varying data volumes?\n- What monitoring would you add to detect increasing lag or drift?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-20T16:51:00.211Z","createdAt":"2026-01-20T16:51:00.211Z"},{"id":"q-5022","question":"Design a beginner-friendly Aurora PostgreSQL Serverless v2 setup for a mobile app with bursty traffic in us-west-2. Specify minCapacity, maxCapacity, and auto-pause window, and justify RDS Proxy usage for persistent connections. Include concrete values and a simple test plan to validate cold-start latency under 1s after idle, and burst latency under 2s during a spike?","answer":"Configure minCapacity to 2 ACUs and maxCapacity to 32 ACUs with an auto-pause delay of 15 minutes. Implement RDS Proxy to manage connection pooling (target approximately 300 max connections with pool size at ~80% of maximum capacity). Validate the setup by allowing 15 minutes of idle time to trigger auto-pause, then measure cold-start latency to ensure it remains under 1 second, and simulate burst traffic to confirm latency stays under 2 seconds during traffic spikes.","explanation":"## Why This Is Asked\n\nThis question evaluates practical knowledge of Aurora Serverless v2 scaling behavior, idle pause mechanisms, and RDS Proxy's role in handling connection bursts for applications with variable traffic patterns.\n\n## Key Concepts\n\n- Aurora Serverless v2 ACU scaling and billing granularity\n- Auto-pause delay configuration and its latency implications\n- RDS Proxy connection pooling for traffic burst management\n- Validation methodologies for cold-start and burst latency testing\n\n## Code Example\n\n```javascript\n// Pseudo-test harness for latency validation\nfunction isAcceptable(latencyMs) { \n  return latencyMs < 1000; \n}\n```\n\n## Follow-up Questions\n\n- How would you adjust these parameters for a production environment with higher traffic volumes?\n- What monitoring metrics would you track to optimize this configuration over time?\n- How would you handle connection pooling for multiple application instances?","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Citadel","Cloudflare","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T05:20:22.118Z","createdAt":"2026-01-21T00:00:57.183Z"},{"id":"q-5143","question":"You manage an Aurora PostgreSQL cluster used by three microservices across two AWS accounts. You need per-service, least-privilege access via IAM database authentication and RDS Proxy, with dedicated DB users mapped to IAM roles. Outline the concrete design: object names, table-level grants, IAM role ARNs, and how to test permissions, latency, and failover. Include sample grants (e.g., SELECT on sales.*; INSERT/UPDATE on orders.*) and a quick validation plan?","answer":"Design three DB users: sales_read, orders_read, orders_rw. Grants: sales_read -> SELECT on sales.*; orders_read -> SELECT on orders.*; orders_rw -> INSERT, UPDATE, DELETE on orders.* (plus SELECT). Ma","explanation":"## Why This Is Asked\nTests practical use of IAM DB authentication, RDS Proxy, and fine-grained access controls across accounts, plus a concrete testing plan.\n\n## Key Concepts\n- IAM database authentication with Aurora PostgreSQL\n- RDS Proxy for serverless/large-concurrency access\n- Fine-grained grants and per-service DB users\n- Cross-account role mappings and network isolation\n- Validation: latency, authorization, and failover tests\n\n## Code Example\n```sql\nCREATE USER sales_read LOGIN;\nGRANT USAGE ON SCHEMA sales TO sales_read;\nGRANT SELECT ON ALL TABLES IN SCHEMA sales TO sales_read;\n\nCREATE USER orders_rw LOGIN;\nGRANT INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA orders TO orders_rw;\nGRANT SELECT ON ALL TABLES IN SCHEMA orders TO orders_rw;\nALTER DEFAULT PRIVILEGES IN SCHEMA orders GRANT INSERT, UPDATE, DELETE ON TABLES TO orders_rw;\n```\n\n## Follow-up Questions\n- How would you verify IAM role to DB user mapping using Lambda? \n- What monitoring would you set to detect privilege drift or misconfiguration? ","diagram":"flowchart TD\n  L1[Lambda: role SalesRead] --> Proxy[RDS Proxy: IAM DB auth]\n  Proxy --> DB1[Aurora: sales_read]\n  L2[Lambda: role OrdersRead] --> Proxy\n  Proxy --> DB2[Aurora: orders_read]\n  L3[Lambda: role OrdersRW] --> Proxy\n  Proxy --> DB3[Aurora: orders_rw]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","MongoDB","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T08:02:04.852Z","createdAt":"2026-01-21T08:02:04.852Z"},{"id":"q-5363","question":"Scenario: An industrial IoT platform streams 20k telemetry events/sec (sensor_id, timestamp, temperature, vibration) into AWS. Real-time anomaly alerts must respond within 200ms; 12-month analytics needed. Compare Amazon Timestream, DynamoDB time-series design, and S3+Athena for this workload. Provide data models, retention tiers, indexing/partitioning, cross-region DR options, and concrete configurations to meet latency and cost targets?","answer":"Amazon Timestream is ideal for hot telemetry; for analytics, mirror to S3 for long-term queries. Model: single table with dimensions sensor_id, location; measures temperature, vibration; time column t","explanation":"## Why This Is Asked\nTests ability to compare time-series storage options across hot and cold paths, with concrete retention and DR strategies.\n\n## Key Concepts\n- Time-series data modeling (dimensions, measures, time)\n- Tiered storage and retention policies\n- Ingestion patterns and cross-region DR\n- Cost trade-offs and query patterns\n\n## Code Example\n```json\n{\n  \\\"Dimensions\\\": [\\\"sensor_id\\\",\\\"location\\\"],\n  \\\"Measures\\\": [\\\"temperature\\\",\\\"vibration\\\"],\n  \\\"TimeColumn\\\": \\\"timestamp\\\"\n}\n```\n\n## Follow-up Questions\n- How would you validate latency under a burst load?\n- What changes if ingestion doubles or retention extends to 24 months?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Bloomberg","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T19:15:02.141Z","createdAt":"2026-01-21T19:15:02.141Z"},{"id":"q-5372","question":"How would you implement per-tenant data isolation in Aurora PostgreSQL for a multi-tenant SaaS app using Row-Level Security, including how to pass tenant context and test for cross-tenant leakage?","answer":"Use Row-Level Security on the tenants table. Enable RLS, create POLICY USING (tenant_id = current_setting('myapp.tenant_id')::int), and ensure the application sets SET myapp.tenant_id = '<tenant>' on ","explanation":"## Why This Is Asked\nReal-world SaaS apps need strict tenant isolation; RLS is a core PostgreSQL pattern at scale.\n\n## Key Concepts\n- Row-Level Security policies tied to per-connection settings\n- Per-session context propagation from app to database\n- Risk areas: leakage through prepared statements and plan cache\n\n## Code Example\n```sql\n-- Enable RLS on the table\nALTER TABLE tenants ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_isolation ON tenants\n  USING (tenant_id = current_setting('myapp.tenant_id')::int);\n```\n\n## Follow-up Questions\n- How would you migrate existing data and tests when introducing RLS? \n- How would you monitor for cross-tenant leakage in production?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Plaid","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T19:48:27.950Z","createdAt":"2026-01-21T19:48:27.950Z"},{"id":"q-5453","question":"Scenario: An RDS Oracle 19c instance in us-east-1 hosts 60 tenants in a single shared schema. Design a row-level data isolation using Oracle Virtual Private Database (VPD) with a dynamic security context and per-tenant predicates; specify the policy function, how you attach it, and how you audit access. Then outline a cross-region DR to eu-west-1 using AWS DMS with full load + CDC to meet RPO ≤ 15s and RTO ≤ 20m, including PITR window, DMS task settings, and Route 53-based failover orchestration?","answer":"Implement Oracle Virtual Private Database (VPD) with dynamic row-level security: create a policy function that returns the predicate 'TENANT_ID = SYS_CONTEXT('USERENV','TENANT_ID')' and attach it to all shared tables using DBMS_RLS.ADD_POLICY; enable Unified Audit Trail to capture all access attempts with tenant context for compliance monitoring.","explanation":"## Why This Is Asked\nTests advanced data isolation using Oracle VPD and robust cross-region DR strategy with AWS DMS CDC, reflecting real production multi-tenant architectures.\n\n## Key Concepts\n- Oracle Virtual Private Database (VPD) for row-level security\n- Dynamic security context with per-tenant predicates\n- Unified Audit Trail for comprehensive access monitoring\n- AWS DMS full load + CDC for minimal data loss\n- Cross-region DR with PITR and Route 53 failover\n\n## Code Example\n```sql\n-- Policy function for tenant isolation\nCREATE OR REPLACE FUNCTION vpd_tenant_pred(p_schema IN VARCHAR2, p_ta","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T05:57:22.995Z","createdAt":"2026-01-21T22:54:20.462Z"},{"id":"q-5475","question":"In a starter Aurora PostgreSQL cluster in us-west-2 serving a SaaS app with mixed workload (approximately 70% reads, 30% writes), design a beginner-friendly observability plan using Performance Insights, Enhanced Monitoring, and CloudWatch. List 6-8 concrete metrics, propose sensible baselines and alert thresholds, and provide a one-page runbook for triage and recovery when a metric breaches its threshold?","answer":"Enable Performance Insights and Enhanced Monitoring on the Aurora PostgreSQL cluster with comprehensive CloudWatch alarm configuration for complete operational visibility. Monitor eight key metrics: CPUUtilization, FreeableMemory, DiskQueueDepth, ReadIOPS, WriteIOPS, AuroraReplicaLag, DatabaseConnections, and ActiveTransactions. Establish operational baselines at 70% CPU utilization, 20% freeable memory, disk queue depth below 10, IOPS within provisioned limits, replica lag under 100ms, database connections below 80% of maximum, and active transactions under 1000. Configure CloudWatch alarms with alert thresholds set at 85% CPU, 15% freeable memory, disk queue depth of 20, IOPS at 90% of provisioned capacity, replica lag of 500ms, database connections at 90% of maximum, and active transactions at 1500. Develop a comprehensive one-page runbook covering immediate assessment procedures, scaling workflows, query optimization techniques, and clear escalation protocols for consistent incident response.","explanation":"## Why This Is Asked\nThis question evaluates practical ability to design real-world observability solutions for AWS databases, requiring candidates to balance comprehensive visibility with cost-effectiveness while demonstrating expertise in metric selection, baseline establishment, and actionable incident response procedures.\n\n## Key Concepts\n- Performance Insights provides deep visibility into database query performance and resource utilization patterns\n- Enhanced Monitoring enables OS-level metrics collection for comprehensive infrastructure monitoring\n- CloudWatch alarms transform raw metrics into actionable alerts with configurable thresholds and notification workflows","diagram":"flowchart TD\n  A[User action] --> B[Query arrives to Aurora]\n  B --> C{Observability enabled?}\n  C -->|Yes| D[Data emitted to CloudWatch & Performance Insights]\n  D --> E[Triage steps from runbook]\n","difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Netflix","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T05:40:06.015Z","createdAt":"2026-01-21T23:47:43.829Z"},{"id":"q-5669","question":"Scenario: A SaaS app serves tenants with different residency requirements across AWS regions. Some tenants require writes strictly in their home region; others need fast reads in both regions but can tolerate eventual cross-region writes via replication. Design an AWS-based data plane using Aurora PostgreSQL Global Database and per-tenant resident clusters. Outline data partitioning, routing, DR steps, and concrete configurations (backup retention, PITR, replication lag, failover). End with a question mark?","answer":"Architect a tenant-aware data plane: allocate resident-region Aurora PostgreSQL clusters per tenant group with writes restricted to the home region; deploy Aurora Global Database for cross-region read","explanation":"## Why This Is Asked\nTests ability to architect per-tenant residency using AWS DB services, balancing write locality with cross-region reads and DR.\n\n## Key Concepts\n- Aurora PostgreSQL Global Database for cross-region reads\n- Per-tenant data partitioning and residency enforcement\n- Routing layer to direct writes locally and reads from nearest replica\n- Backups and PITR budgets; DR RTO/RPO targets\n\n## Code Example\n```javascript\n// Pseudo routing logic for tenancy\nfunction routeTenant(tenantId, policy) {\n  if (policy === 'resident') return 'us-east-1';\n  return 'us-east-1'; // multi-region reads served from global cluster\n}\n```\n\n## Follow-up Questions\n- How would you test regional failover for tenants with different residency policies?\n- What are the latency and consistency trade-offs when serving reads from a global read cluster vs local replicas?","diagram":"flowchart TD\n  A[Tenant Router] --> B[Resident Region Cluster]\n  A --> C[Global Read Cluster]\n  B --> D[Aurora Primary]\n  C --> E[Read Replicas]\n  F[DR Failover] --> G[Target Region]\n  D --> H[Writes in Home Region]\n  E --> I[Reads in Any Region]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Microsoft","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T10:53:12.546Z","createdAt":"2026-01-22T10:53:12.546Z"},{"id":"q-5722","question":"In a compliant multi-region setup, Aurora PostgreSQL in us-east-1 must have cross-region backups to a KMS-encrypted S3 bucket in eu-central-1 under a dedicated security account. Describe the exact steps, IAM roles, KMS key policy, PITR window, DR test cadence, and how you would validate restores and cost implications?","answer":"Use cross-region snapshot export from Aurora us-east-1 to a KMS-encrypted S3 bucket in eu-central-1, with a dedicated KMS key in a security/account. Create a cross-account IAM role trusted by RDS to e","explanation":"## Why This Is Asked\n\nTests ability to design cross-account, cross-region DR using AWS-native features, focusing on data security, compliance, and reliable restores. Challenges include cross-account KMS governance, IAM trust policies, and consistent PITR/DR testing cadences.\n\n## Key Concepts\n\n- Cross-region snapshot exports in Aurora\n- Cross-account KMS key policy and IAM roles\n- PITR window configuration\n- DR test cadence and restore validation\n- Cost and latency considerations\n\n## Code Example\n\n```bash\n# AWS CLI example (conceptual)\naws rds modify-db-cluster --db-cluster-identifier aurora-cluster --backup-retention-period 35\n```\n\n## Follow-up Questions\n\n- How would you automate key rotation across accounts?\n- How would you validate a restore in a separate AWS account without data leakage?","diagram":"flowchart TD\n  A[Aurora us-east-1] --> B[Snapshot export to S3 in eu-central-1]\n  B --> C[KMS key in Security acct]\n  C --> D[Cross-account restore path]\n  D --> E[DR test cadence]","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T13:17:33.953Z","createdAt":"2026-01-22T13:17:33.953Z"},{"id":"q-5796","question":"You operate an Aurora PostgreSQL cluster in us-east-1 and a separate analytics stack in Redshift. You need near real-time analytics via Change Data Capture. Describe a practical CDC pipeline using logical replication (publication/slot) and a sink (DMS/Glue/Debezium). Include how to handle schema evolution and idempotent upserts. Provide concrete settings: wal_level, max_replication_slots, max_wal_senders, slot_name, publication tables, and a sample Debezium connector JSON?","answer":"Enable logical replication (wal_level=logical) and raise max_wal_senders and max_replication_slots to match consumers; create a replication slot and a publication for the tables; route CDC to Redshift","explanation":"## Why This Is Asked\n\nAssess practical CDC pipeline design between Aurora PostgreSQL and analytics sink, focusing on replication settings, schema changes, and idempotent loads.\n\n## Key Concepts\n\n- Logical replication in Postgres\n- Publications, slots, pgoutput\n- Debezium/DMS/ Glue as sink\n- Schema evolution handling and upserts\n- Lag monitoring and fault tolerance\n\n## Code Example\n\n```javascript\n{\n  \"name\": \"aurora-postgres-cdc\",\n  \"config\": {\n    \"connector.class\": \"io.debezium.connector.postgresql.PostgresqlConnector\",\n    \"database.hostname\": \"aurora-cluster.cluster-xyz.us-east-1.rds.amazonaws.com\",\n    \"database.port\": \"5432\",\n    \"database.user\": \"cdc_user\",\n    \"database.password\": \"REDACTED\",\n    \"database.dbname\": \"postgres\",\n    \"plugin.name\": \"pgoutput\",\n    \"publication.autocreate.mode\": \"filtered\",\n    \"publication.include.list\": \"public.orders,public.payments\",\n    \"slot.name\": \"aurora_pubslot\",\n    \"offset.flush.interval.ms\": \"10000\",\n    \"database.server.name\": \"aurora-us-east-1\"\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you test lag and resilience?\n- How would you handle schema change events?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Salesforce","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T17:03:56.282Z","createdAt":"2026-01-22T17:03:56.282Z"},{"id":"q-5821","question":"Beginner-level: You operate a PCI-compliant Aurora PostgreSQL cluster in us-west-2. To meet RPO <5m and RTO <20m with a minimal, cross-account DR, design a DR plan using automated backups, cross-account manual snapshots, and cross-region restore to a DR account in us-east-2. Provide exact settings (backup retention days, PITR range), snapshot-sharing cadence and IAM permissions, cross-region copy settings, DR drill steps (7-8 steps) and success criteria. Include a concise runbook outline and a test cadence?","answer":"Enable automated backups for the prod cluster with 7 days retention and PITR to 7 days. Weekly manual snapshots shared with DR account; in DR region copy to us-east-2 and restore a new cluster. Promot","explanation":"## Why This Is Asked\nTests practical DR planning across accounts and regions with concrete steps. \n\n## Key Concepts\n- Automated backups, manual snapshot sharing, cross-region restore, runbook, validation. \n\n## Code Example\n```javascript\n// AWS SDK snippet illustrating cross-account snapshot share and restore (illustrative)\nconst rds = new AWS.RDS();\nrds.createDBClusterSnapshot({DBClusterIdentifier:'prod-cluster',DBClusterSnapshotIdentifier:'prod-snap-001'});\n```\n\n## Follow-up Questions\n- How would you automate the runbook using EventBridge and Step Functions? \n- How would you verify restoration integrity beyond a checksum?","diagram":"flowchart TD\n  A[Prod Aurora in us-west-2] --> B[Automated backups 7d]\n  B --> C[Weekly manual snapshot shared with DR acct]\n  C --> D[DR acct copies snapshot to us-east-2]\n  D --> E[Restore new cluster in DR]\n  E --> F[Promote Route53 alias]\n  F --> G[DR validation and drill cadence]","difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Google","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T18:01:30.090Z","createdAt":"2026-01-22T18:01:30.090Z"},{"id":"q-6000","question":"In Aurora PostgreSQL, implement tenant isolation with **Row-Level Security (RLS)** using a per-request session context (no per-tenant schemas). The app uses IAM database authentication and a pooled connection. Provide concrete steps, DDL, example policies, how to set the session var, and an audit/testing plan?","answer":"Set a per-request session variable app.tenant_id via SET LOCAL app.tenant_id = 'TENANT1' and enforce an RLS policy: CREATE POLICY tenant_rls ON orders USING (tenant_id = current_setting('app.tenant_id","explanation":"## Why This Is Asked\nTests practical mastery of per-tenant isolation, RLS, and session context in Aurora PostgreSQL. Requires concrete DDL, policy syntax, and how to wire the per-request value from the app through DB config and auditing.\n\n## Key Concepts\n- Row-Level Security (RLS)\n- current_setting() and per-request session context\n- IAM database authentication integration\n- DB parameter groups and auditing\n\n## Code Example\n```sql\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_rls ON orders\nFOR ALL USING (tenant_id = current_setting('app.tenant_id')::int);\n```\n\n```sql\nSET LOCAL app.tenant_id = '1001';\nSELECT * FROM orders WHERE tenant_id = 1001;\n```\n```\n## Follow-up Questions\n- How would you prevent leakage if a query forgets to set the session context?\n- How would you monitor and alert on RLS violations and misconfigurations?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T04:16:53.075Z","createdAt":"2026-01-23T04:16:53.075Z"},{"id":"q-6054","question":"Scenario: An Aurora PostgreSQL multi-tenant app stores per-tenant data in a single cluster. You must stream CDC to a data lake for near-real-time analytics with 5s end-to-end latency, while preserving tenant isolation and enabling schema evolution. Design an end-to-end CDC stack using logical replication (or DMS/Debezium), decide on sink (S3+Glue catalog, Redshift), handling idempotent upserts, late-arriving data, and cross-region replication. Include concrete config hints (replication slots, bucket naming, backup, IAM roles)?","answer":"Use PostgreSQL logical replication with a CDC tool (Debezium/AWS DMS) to stream changes to a sink (Kinesis or Firehose → S3 with Glue catalog, or Redshift). Implement per-tenant keys and upserts in th","explanation":"## Why This Is Asked\n\nTests ability to design a real-world CDC pipeline from OLTP to analytics with cross-region considerations, latency, and strict isolation. Evaluates trade-offs between Debezium and AWS-native DMS, sink choices, and how to handle schema changes.\n\n## Key Concepts\n\n- PostgreSQL logical replication and replication slots\n- CDC tooling: Debezium vs AWS DMS\n- Sinks: Kinesis Firehose, S3+Glue catalog, Redshift\n- Idempotent upserts and late-arriving data handling\n- Schema evolution strategies and backward-compatible migrations\n- Tenant isolation via row-level security (RLS)\n\n## Code Example\n\n```sql\n-- create logical replication slot for streaming\nSELECT * FROM pg_create_logical_replication_slot('tenant_slot','pgoutput');\n```\n\n## Follow-up Questions\n\n- How would you monitor latency and lag, and handle backpressure?\n- How would you test failure modes (slot rewind, sink outage, replica lag) and recover?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T07:00:08.697Z","createdAt":"2026-01-23T07:00:08.697Z"},{"id":"q-6072","question":"In a multi-region SaaS app, design a DR plan using Aurora PostgreSQL Global Database with a primary in us-east-1 and a read replica in eu-west-1 to serve tenants with strict isolation and low-latency reads. Propose topology, concrete backup/PITR settings, data isolation strategy, cross-account IAM roles for DR drills, and a step-by-step failover/testing plan that achieves RPO ~5s and RTO <=60s?","answer":"Primary in us-east-1; one eu-west-1 replica; per-tenant isolation via schema + row-level security on tenant_id; small cache for hot data. backups 30 days; PITR window 7 days; RPO ~5s; RTO <=60s via fa","explanation":"## Why This Is Asked\nThis question probes Aurora Global Database topology, DR readiness, multitenant isolation, and cross-account drills.\n\n## Key Concepts\n- Aurora Global Database topology and failover\n- Row-level security and per-tenant isolation\n- PITR, backup windows, retention\n- Route 53 DNS failover and cross-account roles\n- DR testing practices\n\n## Code Example\n```javascript\n// Pseudo-SQL for enabling RLS (interview-example)\nCREATE POLICY tenant_rls ON orders\nFOR ALL USING (tenant_id = current_setting('myapp.tenant_id')::int);\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\n```\n\n## Follow-up Questions\n- How would you monitor replication lag and detect anomalies?\n- How would you validate a DR switch without impacting customers?","diagram":"flowchart TD\n  A[Writes in us-east-1] --> B[Aurora Global Database replication to eu-west-1]\n  B --> C[Reads served from eu-west-1 replicas]\n  D[Route53 DNS failover to DR region]\n  E[DR drill: cross-account role assume and validation]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T07:43:03.667Z","createdAt":"2026-01-23T07:43:03.667Z"},{"id":"q-6189","question":"Design a multi-tenant SaaS data design on Aurora PostgreSQL (us-east-1) that isolates each tenant's data via Row-Level Security, while running analytics on a separate replica without impacting OLTP. Describe topology (primary, read replicas, cross-region considerations), data export path to an analytics store (Redshift or S3), backup/DR settings, and a testing plan. Provide concrete policy examples and replication settings?","answer":"Implement per-tenant isolation with PostgreSQL Row-Level Security on the OLTP primary; direct analytics workloads to dedicated read replicas. Use DMS or logical replication to continuously export to R","explanation":"## Why This Is Asked\nTests ability to architect data isolation, analytics offload, and DR in AWS DB services. Requires practical choices (RLS config, replica topology, DMS vs native replication, PITR). Focus on trade-offs like latency, complexity, cost, data residency, and testing.\n\n## Key Concepts\n- PostgreSQL Row-Level Security\n- Aurora primary vs read replicas\n- DMS and CDC\n- Redshift and S3 data lake\n- PITR, backups, DR testing\n- Replication lag monitoring\n\n## Code Example\n```sql\n-- Enable RLS and policy for tenant isolation\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_isolation ON orders\n  USING (tenant_id = current_setting('app.tenant_id')::int)\n  WITH CHECK (tenant_id = current_setting('app.tenant_id')::int);\n```\n\n## Follow-up Questions\n- How would you test RLS permissions at scale?\n- How would you monitor and alert for replication lag and data freshness?","diagram":"flowchart TD\n  A[OLTP Primary - Aurora PostgreSQL] --> B[Dedicated OLTP Read Replica(s)]\n  A --> C[Analytics Pipeline (DMS/CDC)]\n  C --> D[Analytics Store (Redshift/S3)]\n  D --> E[BI/ML Workloads]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hashicorp","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T13:08:56.981Z","createdAt":"2026-01-23T13:08:56.981Z"},{"id":"q-6211","question":"In a multi-account setup, an Aurora PostgreSQL secret lives in Account A and an app in Account B reads credentials via Secrets Manager. Describe a practical, low-downtime rotation design using Secrets Manager rotation for PostgreSQL, including (1) IAM roles and trust policies needed, (2) resource policy to allow cross-account secret access, (3) rotation lambda configuration and timing, and (4) how to validate zero-downtime rotation in production?","answer":"Enable Secrets Manager rotation for the RDS PostgreSQL secret in Account A. Use the AWS rotation function; set AutomaticallyAfterDays: 30. Add a resource policy to allow the app in Account B to retrie","explanation":"## Why This Is Asked\nTests understanding of cross-account secrets, rotation workflows, and zero-downtime updates in production.\n\n## Key Concepts\n- Secrets Manager rotation for RDS PostgreSQL\n- Cross-account IAM access and resource policies\n- Rotation Lambda configuration and scheduling\n- Validation for seamless reconnect without downtime\n\n## Code Example\n```bash\naws secretsmanager rotate-secret --secret-id arn:aws:secretsmanager:region:acct:secret:mydb --rotation-lambda-arn arn:aws:lambda:region:acct:function:SecretsRotatePostgres --rotation-rules AutomaticallyAfterDays=30\n```\n\n## Follow-up Questions\n- How would you monitor rotation health and alert on failures?\n- What happens if rotation fails and the app attempts reconnects?\n","diagram":"flowchart TD\nA[Secret in Account A] --> B[Rotation Lambda]\nB --> C[New credentials stored in Secrets Manager]\nC --> D[App in Account B fetches secret via cross-account policy]\nD --> E[App reconnects with new creds]","difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Meta","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T14:44:20.495Z","createdAt":"2026-01-23T14:44:20.495Z"},{"id":"q-6297","question":"In an Aurora PostgreSQL multi-region real-time messaging app, with a primary cluster in us-east-1 and a secondary via Aurora Global Database in us-west-2, design a system that guarantees exactly-once message processing while keeping low latency. Include data model, upsert strategy, conflict handling, replication behavior, monitoring, and DR testing steps?","answer":"Use a single source of truth in the primary region with idempotent writes. Model messages with message_id as PK and use INSERT ... ON CONFLICT DO UPDATE to deduplicate. Maintain a lightweight dedup ta","explanation":"## Why This Is Asked\n\nTests end-to-end design: data modeling, idempotent processing, cross-region replication behavior, observability, and DR readiness in a real-time, multi-region AWS setup.\n\n## Key Concepts\n\n- Idempotent writes at the API layer to guarantee exact-once semantics\n- Upsert pattern using message_id as primary key\n- Deduplication table with TTL to filter replayed messages\n- Aurora Global Database replication characteristics and eventual consistency implications\n- Observability, lag budgets, and DR testing cadence\n\n## Code Example\n\n```sql\nCREATE TABLE messages (\n  message_id VARCHAR(255) PRIMARY KEY,\n  payload JSONB,\n  created_at TIMESTAMPTZ DEFAULT now()\n);\n\n-- Idempotent write\nINSERT INTO messages (message_id, payload) VALUES ('msg-1', '{\"text\":\"hello\"}')\n  ON CONFLICT (message_id) DO UPDATE SET payload = EXCLUDED.payload;\n```\n\n## Follow-up Questions\n\n- How would you handle a case where a broker sends the same message_id twice with different payloads?\n- How would you validate DR readiness across regions and prevent data loss during failover?","diagram":"flowchart TD\n  A[Publish message] --> B[API writes to us-east-1]\n  B --> C[Upsert into messages (message_id PK)]\n  B --> D[Dedup table with TTL]\n  C --> E[Global Database replication to us-west-2]\n  E --> F[Consumers/Analytics read from both regions]","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T18:03:06.652Z","createdAt":"2026-01-23T18:03:06.653Z"},{"id":"q-6465","question":"In a production Aurora PostgreSQL deployment, design a sandbox DR workflow to test restores without impacting prod. Outline how to copy backups to a sandbox environment, perform PITR to a defined point, spin up a fresh test cluster in a separate VPC, apply a sandbox parameter group, grant time-limited IAM access, and validate with health/read/write tests before teardown. Include concrete timing and steps?","answer":"Implement a sandbox disaster recovery workflow by establishing nightly cross-account backup replication from production to a dedicated sandbox environment. When triggered, perform Point-in-Time Recovery to restore the database to a specific point-in-time, provision a new Aurora cluster within an isolated VPC, apply sandbox-specific parameter group configurations, grant time-bound IAM credentials for access validation, execute comprehensive health checks including read/write operations, and automate teardown of the test infrastructure upon completion.","explanation":"## Why This Is Asked\nThis question evaluates your understanding of disaster recovery testing methodologies, cross-account AWS operations, and environment isolation practices—critical skills for maintaining production database reliability.\n\n## Key Concepts\n- Point-in-Time Recovery (PITR) implementation and timing\n- Cross-account snapshot copying mechanisms\n- Sandbox environment security and network isolation\n- Temporary IAM access management\n- Automated validation and cleanup procedures\n\n## Code Example\n```bash\n# AWS CLI example for cross-account snapshot replication\naws rds copy-db-snapshot \\\n  --source-db-snapshot-identifier prod-snapshot-2024-01-15 \\\n  --target-db-snapshot-identifier sandbox-test-snapshot \\\n  --kms-key-id alias/sandbox-kms-key \\\n  --copy-tags\n```\n\n## Follow-up Questions\n- How would you optimize the frequency of backup replication?\n- What monitoring metrics would you track during DR validation?\n- How do you handle sandbox environment scaling for different database sizes?","diagram":"flowchart TD\n  A[Prod Aurora] --> B[Snapshot Created]\n  B --> C[Copy Snapshot to Sandbox Account]\n  C --> D[PITR Restore in Sandbox]\n  D --> E[Launch Sandbox Cluster in separate VPC]\n  E --> F[Apply Sandbox Parameter Group]\n  F --> G[Grant Time-Limited IAM Access]\n  G --> H[Run Validation Tests]\n  H --> I[Tear Down Sandbox]","difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Salesforce","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T04:37:49.861Z","createdAt":"2026-01-24T02:41:13.583Z"},{"id":"q-6586","question":"In an Aurora PostgreSQL Global Database with a primary in us-east-1 and a read replica in eu-west-1, design an access-control strategy that enforces per-tenant isolation so each tenant can access only their own schema, supports on-demand provisioning, and keeps analytics fast on the replica. Include concrete policy examples, credential rotation, and a validation plan?","answer":"Leverage PostgreSQL RLS with per-tenant roles. Each session runs set_config('app.tenant_id', 'TENANT123'), and policies enforce USING (tenant_id = current_setting('app.tenant_id')). Use a shared table","explanation":"## Why This Is Asked\nTests practical use of RLS, provisioning, and multi-region constraints in Aurora PostgreSQL Global Database, including security, ops, and testing.\n\n## Key Concepts\n- PostgreSQL Row-Level Security (RLS) with current_setting and set_config\n- Credential rotation with AWS Secrets Manager\n- Multi-region replication lag in Global Database and read replica usage\n- Automated tenant provisioning scripts and audit trails\n\n## Code Example\n```sql\n-- Example RLS policy\nCREATE POLICY tenant_isolation ON public.orders\nUSING (tenant_id = current_setting('app.tenant_id')::int)\nWITH CHECK (tenant_id = current_setting('app.tenant_id')::int);\n```\n\n## Follow-up Questions\n- How would you test for privilege escalation bypassing RLS?\n- How would you handle analytics that require cross-tenant joins or data sharing?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T08:58:15.090Z","createdAt":"2026-01-24T08:58:15.091Z"},{"id":"q-6694","question":"A fleet telemetry pipeline ingests JSON events (vehicleId, ts, metrics) at up to 400k reqs/sec per region (us-east-1, eu-west-1). You need sub-10ms per-vehicle lookups for last minute aggregations, TTL archive to 90 days, and cross-region resilience. Design a DynamoDB-based hot path plus S3-based cold path, with schema, keys, TTL, and a Global Tables replication strategy. Address hot partitions and cost?","answer":"Use a sharded DynamoDB hot path per vehicle to avoid hotspots, with partition keys like vehicle#<id>#shard<N> and a timestamp sort key. Enable on-demand capacity and TTL (expireAt = ts + 90d). Enable ","explanation":"## Why This Is Asked\nTests data modeling for ultra high-throughput time-series in DynamoDB, shard strategy, TTL, cross-region replication, and archival.\n\n## Key Concepts\n- DynamoDB on-demand, hot-partition avoidance, sharding\n- TTL and retention policies\n- Global Tables cross-region replication\n- DynamoDB Streams + Lambda/Firehose to S3\n- Parallel reads across shards for low latency\n\n## Code Example\n```javascript\nfunction shardForVehicle(vehicleId){\n  const hash = [...vehicleId].reduce((a,c)=> a*31 + c.charCodeAt(0), 7);\n  return Math.abs(hash) % 8;\n}\n```\n\n## Follow-up Questions\n- How would you monitor shard skew and rebalance shards?\n- What are potential consistency pitfalls with cross-region reads?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T13:44:33.694Z","createdAt":"2026-01-24T13:44:33.694Z"},{"id":"q-6738","question":"An Aurora PostgreSQL cluster under heavy transactional load begins to experience increasing latency during peak hours due to autovacuum thrash causing table bloat and stale visibility maps. Propose a concrete, production-safe plan to remediate without app downtime, covering: enabling parallel vacuum workers, per-table autovacuum_vacuum_scale_factor overrides, adjusting maintenance_work_mem and WAL settings, and scheduling automated vacuum maintenance with monitoring and automatic alerts. Include concrete parameter ranges and steps to validate the improvement?","answer":"8 autovacuum workers; maintenance_work_mem = 1.5GB; autovacuum_vacuum_scale_factor per table: hot tables 0.05, others 0.2; autovacuum_naptime 30s; autovacuum_cost_limit = -1; targeted VACUUM ANALYZE o","explanation":"## Why This Is Asked\nLatency spikes from autovacuum thrash are a real production pain; this asks for a concrete remediation plan that minimizes downtime.\n\n## Key Concepts\n- Autovacuum tuning and per-table overrides\n- WAL/maintenance_work_mem trade-offs\n- Observability of dead tuples, bloat, and VT latency\n\n## Code Example\n````sql\nALTER TABLE huge_table SET (autovacuum_vacuum_scale_factor = 0.05);\nVACUUM ANALYZE huge_table;\n````\n\n## Follow-up Questions\n- How would you validate improvement under a synthetic peak load?\n- What risks come with per-table overrides and how mitigate them?","diagram":null,"difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Meta","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T15:32:04.158Z","createdAt":"2026-01-24T15:32:04.159Z"},{"id":"q-6788","question":"Architect a multi-tenant SaaS workload on a single Aurora PostgreSQL cluster. Each tenant's data must be isolated via **Row Level Security** and tenants must have quotas on reads, writes, and storage. Describe the schema/RLS design, how to enforce quotas (db vs app), how to meter usage for billing, and your monitoring/alerting strategy plus a practical test plan to validate isolation and quota enforcement?","answer":"Use a single Aurora PostgreSQL with per-tenant RLS: all tables include tenant_id; policies ensure tenant_id = current_setting('app.tenant_id', true). App sets the tenant context on each connection. En","explanation":"## Why This Is Asked\nChecks ability to design robust multi-tenant isolation with RLS and practical quota/metering mechanisms in AWS.\n\n## Key Concepts\n- Aurora PostgreSQL Row Level Security\n- Per-tenant metering and billing integration\n- Schema design for multi-tenancy with tenant_id\n- Observability and testing strategies\n\n## Code Example\n```sql\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_isolator ON orders USING (tenant_id = current_setting('app.tenant_id', true)) WITH CHECK (tenant_id = current_setting('app.tenant_id', true));\n```\n\n```\n-- Pseudo: set per-session tenant for a connection\nSELECT set_config('app.tenant_id', 'tenant_123', true);\n```\n\n## Follow-up Questions\n- How would you handle quota drift with batch jobs?\n- What security checks would you add if tenants share read replicas?","diagram":"flowchart TD\n  A[App] --> B[API Gateway]\n  B --> C[Aurora PostgreSQL (RLS)]\n  C --> D[Metering: DynamoDB]\n  D --> E[Alerts & Billing Portal]","difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","LinkedIn","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T17:34:02.811Z","createdAt":"2026-01-24T17:34:02.811Z"},{"id":"q-6926","question":"Design an archival strategy for historical rows from Aurora PostgreSQL to Amazon S3 that preserves referential integrity and enables analytics on archived data without impacting OLTP performance. Use partitioning, incremental CDC (logical replication or DMS), and AWS services; specify retention policy (7 years), data format (Parquet), encryption, and cost controls. Include validation steps, rollback plan, and cross-region considerations for a multi-account setup?","answer":"Partition historical data in a dedicated archive table using append-only patterns, then stream changes via logical replication or DMS to an S3 Parquet data lake. Retain data for 7 years with S3 lifecycle policies transitioning to Glacier, encrypt with KMS, validate integrity using checksums, and implement rollback through point-in-time recovery.","explanation":"## Why This Is Asked\n\nThis question evaluates practical archival design in a real AWS multi-account, multi-region environment, focusing on data lifecycle management, performance impact mitigation, and compliance requirements.\n\n## Key Concepts\n\n- OLTP archival partitioning and append-only design patterns\n- CDC options: logical replication versus DMS trade-offs\n- S3 Parquet data lake architecture, Glacier lifecycle policies, and optimal data formats\n- Cross-account IAM roles, KMS encryption, and access control mechanisms\n- Data integrity validation, rollback strategies, and disaster recovery considerations\n\n## Code Example\n\n```javascript\n// Pseudo archival hook\nasync function archiveOldRows() {\n  // extract older rows, convert t\n```","diagram":"flowchart TD\n  A[Aurora OLTP] --> B[Archive Table (partitioned)]\n  B --> C[DMS/Logical Replication] \n  C --> D[S3 Parquet Lake] \n  D --> E[Analytics Data Lake]","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:51:39.718Z","createdAt":"2026-01-24T23:28:38.574Z"},{"id":"q-6950","question":"You operate an Aurora PostgreSQL Global Database with a writable primary in us-east-1 and a synchronous secondary in eu-west-1 for DR. The business requires RPO <= 5 seconds and RTO <= 60 seconds, with cross-account access to a secure DR data lake in S3 for audit logs. Describe the exact configuration (backup retention, PITR window, Global Database settings, replica counts) and a concrete DR drill plan including steps to test failover and verify data integrity?","answer":"Implement an Aurora PostgreSQL Global Database with a writable primary in us-east-1 and a synchronous secondary replica in eu-west-1. Configure backup retention to 7 days with a 5-minute Point-In-Time Recovery (PITR) window. Set up the Global Database with exactly 1 secondary replica and configure appropriate failover priority. Establish cross-account IAM roles and KMS key policies to enable secure access to the DR data lake in S3 for audit log storage.","explanation":"## Why This Is Asked\n\nThis question evaluates your practical expertise in designing and implementing disaster recovery solutions for Aurora Global Database environments, including cross-account data lake integration and measurable DR metrics compliance.\n\n## Key Concepts\n\n- Aurora Global Database architecture and configuration\n- Point-In-Time Recovery (PITR) and backup retention strategies\n- Cross-account IAM and KMS integration for S3 data lake access\n- Disaster recovery drill planning and data integrity verification\n- RPO (Recovery Point Objective) and RTO (Recovery Time Objective) implementation\n\n## Code Example\n\n```bash\n# Configure Aurora backup settings and PITR window\naws rds modify-db-cluster \\\n  --db-cluster-identifier prod-aurora \\\n  --backup-retention-period 7 \\\n  --preferred-backup-window 02:00-03:00\n\n# Create Global Database with secondary replica\naws rds create-global-cluster \\\n  --global-cluster-identifier prod-global \\\n  --source-db-cluster-identifier prod-aurora\n\n# Add secondary replica in eu-west-1\naws rds create-db-cluster \\\n  --db-cluster-identifier prod-aurora-eu \\\n  --global-cluster-identifier prod-global \\\n  --engine aurora-postgresql \\\n  --region eu-west-1\n```\n\n## Follow-up Questions\n\n- How would you measure and validate RPO/RTO metrics during disaster recovery drills?\n- What specific failure scenarios would you test in your DR validation plan?\n- How would you automate cross-account audit log replication to the S3 data lake?\n- What monitoring and alerting would you implement for Global Database health and replication lag?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","PayPal","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T04:25:45.547Z","createdAt":"2026-01-25T02:32:29.612Z"},{"id":"q-7113","question":"Aurora PostgreSQL in us-east-1 serves 3 tenants via IAM database authentication. Outline a beginner-friendly pattern to grant per-tenant read-only access using IAM auth without passwords, mapping IAM roles to database users, and rotating credentials with zero downtime. Include example IAM role/trust, DB user creation, and a minimal test plan?","answer":"Enable IAM DB authentication on the cluster. Create per-tenant DB users (tenant1, tenant2, tenant3) with SELECT rights on their data and map each to an IAM role. Use RDS Signer for token-based connect","explanation":"## Why This Is Asked\nTests IAM DB authentication, per-tenant access, token-based connections, and zero-downtime rotation.\n\n## Key Concepts\n- IAM DB authentication in Aurora PostgreSQL\n- Per-tenant DB users and role mappings\n- Trust relationships and policy rotations\n- Token-based connections (RDS Signer)\n- Non-disruptive credential rotation and testing\n\n## Code Example\n```sql\nCREATE USER tenant_a LOGIN;\nGRANT SELECT ON ALL TABLES IN SCHEMA public TO tenant_a;\n```\n\n```bash\n# Example role creation (illustrative)\naws iam create-role --role-name TenantARole --assume-role-policy-document file://trust.json\n```\n\n## Follow-up Questions\n- How would you audit cross-tenant access?\n- How would you on-board a new tenant without downtime?","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T09:43:13.380Z","createdAt":"2026-01-25T09:43:13.380Z"},{"id":"q-7193","question":"In a prod Aurora PostgreSQL cluster in Account A (us-east-1), design a cross-account analytics replica in Account B (us-west-2) without Global Database. Propose a practical approach using automated backups, cross-account snapshot sharing, and a read-only analytics replica. Include steps for sharing/restoring the snapshot, provisioning the analytics cluster, access control, and a lightweight monitoring plan?","answer":"Share the prod Aurora snapshot from Account A to Account B, restore as a new us-west-2 cluster, create a read-only role and grant SELECT on required schemas, and set up a refresh path via DMS or logic","explanation":"## Why This Is Asked\nTests cross-account data sharing, practical use of automated backups, and a non-prod analytics replica without Global Database.\n\n## Key Concepts\n- Cross-account snapshot sharing and restore\n- IAM roles and KMS for cross-account encryption\n- DMS vs logical replication for near-real-time analytics\n- Network paths (VPC peering/privateLink) and access controls\n- Monitoring (CloudWatch, Performance Insights)\n\n## Code Example\n```bash\n# Share snapshot to Account B\naws rds modify-db-snapshot-attribute \\\n  --db-snapshot-identifier prod-snapshot \\\n  --attribute-name restore \\\n  --values-to-add 123456789012\n\n# Restore as analytics cluster in Account B\naws rds restore-db-cluster-from-snapshot \\\n  --db-cluster-identifier analytics-cluster \\\n  --snapshot-identifier arn:aws:rds:us-west-2:123456789012:snapshot:prod-snapshot \\\n  --engine aurora-postgresql\n```\n\n## Follow-up Questions\n- How would you handle schema changes on the analytics replica?\n- What are the cost implications of frequent refresh vs on-demand refresh?","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Goldman Sachs","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T13:25:16.622Z","createdAt":"2026-01-25T13:25:16.622Z"},{"id":"q-7203","question":"You're migrating a 1.2 TB PostgreSQL on-premises database with 20k+ updates per minute to Amazon Aurora PostgreSQL in AWS. You must minimize downtime and preserve ongoing writes. Design a concrete migration plan using AWS DMS and/or native PITR, including replication topology, DMS settings (Full Load + CDC, LOB handling), target instance sizing, network considerations (VPC, endpoints, security groups), validation steps, cutover procedure, and rollback plan. Provide concrete values?","answer":"Plan uses DMS with Full Load + CDC and LOBs enabled. Replication instance: dms.r5.4xlarge in a private VPC; target Aurora PostgreSQL cluster in the same VPC. Task settings: FullLoad+CDC, LOBMode=TRANS","explanation":"## Why This Is Asked\n\nTests real-world migration planning, including minimal-downtime strategies, DMS capability, LOB handling, validation, and rollback.\n\n## Key Concepts\n\n- AWS DMS full-load + CDC with LOBs\n- Aurora PostgreSQL target considerations\n- Cutover timing and rollback planning\n- Data validation and monitoring\n\n## Code Example\n\n```javascript\naws dms create-replication-task \\\n  --replication-task-identifier aurora-migrate \\\n  --source-endpoint-arn <src-arn> \\\n  --target-endpoint-arn <tgt-arn> \\\n  --migration-type \"full-load-and-cdc\" \\\n  --replication-task-settings '{\"FullLoadSettings\": {\"TargetTablePrepMode\":\"DO_NOTHING\",\"LobMode\":\"TRANSACTIONAL\",\"LobChunkSize\":65536},\"CDCSettings\": {\"BatchApplyEnabled\":true}}' \\\n  --table-mappings '{\"rules\":[{\"rule-type\":\"selection\",\"object-locator\":{\"schema-name\":\"public\",\"table-name\":\"%\"},\"rule-action\":\"include\"}]}'\n```\n\n## Follow-up Questions\n\n- How would you monitor replication lag and how would you react to spikes?\n- What changes if the source network becomes intermittent during cutover?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","MongoDB","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T13:45:07.618Z","createdAt":"2026-01-25T13:45:07.618Z"},{"id":"q-7309","question":"In a single-region AWS deployment, design a beginner-friendly data model for storing user profiles and event analytics in Aurora PostgreSQL. Choose between a normalized schema vs JSONB-friendly denormalization, specify table definitions, indices, and a scalable approach to analytics data (partitioning or separate table). Outline a simple automated backup plan with retention and PITR, plus a basic DR drill outline with concrete steps and success criteria?","answer":"Proposed: use Aurora PostgreSQL in us-east-1 with two tables: users(id PK, name, email, prefs JSONB); events(id PK, user_id FK, action, ts, details JSONB). Prefer normalized users + analytics events w","explanation":"## Why This Is Asked\nTests practical data modeling choices, including when to use JSONB vs normalization, and how to structure analytics data for growth. Also evaluates operational discipline around backups, PITR, partitioning, and a basic DR drill.\n\n## Key Concepts\n- Aurora PostgreSQL design choices (normalized vs JSONB)\n- Partitioning strategy for analytics data\n- Automated backups and PITR windows\n- Simple DR drill planning and success criteria\n\n## Code Example\n```sql\nCREATE TABLE users (\n  id UUID PRIMARY KEY,\n  name VARCHAR(100),\n  email VARCHAR(255) UNIQUE,\n  prefs JSONB\n);\n\nCREATE TABLE events (\n  id UUID PRIMARY KEY,\n  user_id UUID REFERENCES users(id),\n  action VARCHAR(50),\n  ts TIMESTAMPTZ NOT NULL,\n  details JSONB\n) PARTITION BY RANGE (ts);\n```\n\n## Follow-up Questions\n- How would you validate PITR restores in AWS Console vs CLI?\n- How would you evolve the schema with zero-downtime migrations for new analytics fields?","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T17:39:53.441Z","createdAt":"2026-01-25T17:39:53.441Z"},{"id":"q-7426","question":"A Aurora PostgreSQL cluster in us-east-1 shows intermittent latency during peak hours. Propose a beginner-friendly plan to diagnose and reduce latency using Performance Insights and autovacuum tuning. Include concrete settings (autovacuum_vacuum_cost_delay, autovacuum_vacuum_cost_limit), a sample SQL to find heavy statements, and how you’d validate a 20% latency drop in two weeks?","answer":"Enable Performance Insights on the Aurora PostgreSQL cluster to establish baseline latency metrics during peak hours. Use pg_stat_statements to identify resource-intensive queries: SELECT query, total_exec_time, calls FROM pg_stat_statements ORDER BY total_exec_time DESC LIMIT 10. Configure autovacuum settings for optimal performance: SET autovacuum_vacuum_cost_delay = '10ms'; SET autovacuum_vacuum_cost_limit = 200; Monitor VACUUM activity and adjust parameters based on observed workload patterns. Validate improvements by comparing P95 latency metrics before and after optimization over a two-week period to confirm the targeted 20% latency reduction.","explanation":"## Why This Is Asked\nTests practical, beginner-friendly performance diagnostics using native PostgreSQL and AWS RDS features. Emphasizes real-world tooling and measurable improvements.\n\n## Key Concepts\n- Performance Insights visualization\n- pg_stat_statements query analysis\n- autovacuum cost-based tuning\n- VACUUM/ANALYZE optimization\n- latency measurement (percentiles)\n\n## Code Example\n```javascript\nconst res = await client.query(`\n  SELECT query, total_exec_time, calls\n  FROM pg_stat_statements\n  ORDER BY total_exec_time DESC\n  LIMIT 10;\n`);\nconsole.log(res.rows);\n```\n\n## Follow-up Questions","diagram":"flowchart TD\n  A[Peak latency observed] --> B[Enable Performance Insights]\n  B --> C[Query pg_stat_statements for hotspots]\n  C --> D[Tune autovacuum settings]\n  D --> E[Run VACUUM ANALYZE nightly]\n  E --> F[Measure 95th percentile latency]\n  F --> G[Validate 20% latency drop]","difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T06:06:04.264Z","createdAt":"2026-01-25T22:42:25.574Z"},{"id":"q-7542","question":"In a single Aurora PostgreSQL cluster used by a multi-tenant SaaS, design a near-real-time immutable audit log that captures every DML with before/after images and stores it in S3 with 7-year retention. Evaluate whether to use AWS DMS CDC vs Debezium, how to enforce immutability (S3 Object Lock, per-tenant isolation), and how to validate DR and retention?","answer":"Use AWS DMS with a CDC target to S3 in Parquet, with S3 Object Lock in Governance mode for 7-year retention. Capture before/after images, table, primary key, txn timestamp, and user; partition by tena","explanation":"## Why This Is Asked\nAuditors require a tamper-evident trail with low latency. This tests knowledge of CDC options (DMS vs Debezium), immutable storage (S3 Object Lock, per-tenant isolation), and DR planning for long-term retention.\n\n## Key Concepts\n- CDC design choices: DMS vs Debezium\n- Immutable storage: S3 Object Lock, bucket/tenant isolation\n- Data modeling: before/after, txn metadata\n- DR and testing: cross-region replication, failover validation\n\n## Code Example\n```json\n{\n  \"cdcHandler\": \"DMS\",\n  \"s3TargetFormat\": \"PARQUET\",\n  \"objectLock\": \"ENABLED\",\n  \"retentionYears\": 7\n}\n```\n\n## Follow-up Questions\n- What are the trade-offs of DMS vs Debezium regarding schema evolution and latency?\n- How would you implement tenant-level access controls on the audit data in S3?","diagram":"flowchart TD\n  Aurora[Aurora PostgreSQL] --> CDC[CDC pipeline (DMS or Debezium)]\n  CDC --> S3[S3 Audit Bucket]\n  S3 --> DR[Cross-region replication]\n  Tenant[Tenant isolation] --> S3","difficulty":"advanced","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T07:04:16.263Z","createdAt":"2026-01-26T07:04:16.263Z"},{"id":"q-7600","question":"You operate an Aurora PostgreSQL Global Database with primary in us-east-1 and a cross-region replica in eu-west-2. A schema change adds a new NOT NULL column with a safe default, but you cannot lock the table. Describe a practical, zero-downtime migration plan across regions, including: (1) how to implement a non-blocking DDL, (2) how to backfill and switch writes, (3) how to verify cross-region consistency and recovery, (4) concrete DR-related settings (PITR window, backup retention, lag SLA), and (5) rollback steps if issues arise?","answer":"Zero-downtime online DDL across Aurora PostgreSQL Global Database: 1) on us-east-1 primary, ALTER TABLE t ADD COLUMN new_col bigint NULL; 2) batched backfill of new_col while writes continue to old sc","explanation":"## Why This Is Asked\nTests practical, real-world schema migrations in a global Aurora setup, emphasizing online DDL, cross-region replication, and rollback readiness.\n\n## Key Concepts\n- Online DDL patterns for PostgreSQL on Aurora Global Database\n- Batched backfill and dual-write strategy\n- Cross-region replication lag and validation\n- PITR, backup retention, and rollback readiness\n\n## Code Example\n```sql\n-- phase 1: add nullable column\nALTER TABLE t ADD COLUMN new_col BIGINT NULL;\n-- phase 2: backfill in application-controlled batches\nUPDATE t SET new_col = COALESCE(new_col, old_col)\nWHERE new_col IS NULL AND <batch_condition>;\n```\n\n## Follow-up Questions\n- How would you test failover after the migration?\n- What metrics signal readiness to enforce NOT NULL and drop the old column?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T09:08:11.933Z","createdAt":"2026-01-26T09:08:11.933Z"},{"id":"q-7691","question":"In a multi-region Aurora PostgreSQL Global Database with a writer in us-east-1 and a secondary in eu-west-1, two tenants share the same logical schema but require strict per-tenant isolation. Design a solution to give TenantB near real-time dashboards in eu-west-1 while writes occur in us-east-1. Use RLS for tenant isolation, IAM database authentication, and cross-region replication settings. Include (1) per-tenant access controls, (2) replication topology and read latency targets, (3) backup/DR plan with RPO/RTO, (4) validation steps to verify isolation and latency?","answer":"Leverage Aurora Global Database with writer in us-east-1 and a read replica in eu-west-1 for TenantB. Implement per-tenant isolation with RLS on a tenant_id column; map IAM database authentication to ","explanation":"## Why This Is Asked\nReal-world cross-region multi-tenant workloads require secure isolation and low-latency analytics. This question probes practical design choices for data isolation, cross-region performance, and robust DR in a unified database.\n\n## Key Concepts\n- Aurora Global Database, cross-region replication latency\n- Row-level security (RLS) for per-tenant isolation\n- IAM database authentication for tenant-specific access\n- Secrets Manager per-tenant credentials with rotation\n- PITR and DR testing across regions\n\n## Code Example\n```sql\n-- Enable RLS and policy for tenant isolation\nALTER TABLE orders ENABLE ROW LEVEL SECURITY;\nCREATE POLICY tenant_isolation ON orders FOR ALL USING (tenant_id = current_setting('my.tenant_id')::int);\n```\n\n## Follow-up Questions\n- How would you monitor cross-region replication lag and alert on SLA breaches?\n- How would you handle schema migrations affecting tenants without downtime?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","NVIDIA","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T13:33:22.242Z","createdAt":"2026-01-26T13:33:22.242Z"},{"id":"q-7875","question":"Scenario: A SaaS app uses Aurora PostgreSQL in us-east-1. Build a beginner plan to give BI workloads access to a dedicated read replica with a restricted read-only role and a separate BI schema copy. Outline steps to create the replica, configure the restricted role, copy schema, enable Performance Insights, and set CloudWatch alarms for lag and CPU. What checks ensure a safe switch to BI workloads?","answer":"Create a dedicated Aurora PostgreSQL read replica in us-east-1; grant a dedicated read-only user with limited privileges on the BI schema; export/copy only BI schema via pg_dump/pg_restore or DDL and ","explanation":"## Why This Is Asked\nTests a junior understanding of workload isolation: creating a read replica, restricting access, and monitoring impact with built-in AWS tools.\n\n## Key Concepts\n- Aurora PostgreSQL read replicas and BI isolation\n- Schema copy strategies and restricted privileges\n- Performance Insights and CloudWatch alarms\n- Safe validation and minimal OLTP impact\n\n## Code Example\n```sql\nCREATE USER bi_viewer WITH PASSWORD 'REDACTED';\nGRANT USAGE ON SCHEMA bi_bi TO bi_viewer;\nGRANT SELECT ON ALL TABLES IN SCHEMA bi_bi TO bi_viewer;\nALTER DEFAULT PRIVILEGES IN SCHEMA bi_bi GRANT SELECT ON TABLES TO bi_viewer;\n```\n\n```bash\n# Example (illustrative): create BI read replica and attach to BI cluster\naws rds create-db-instance --db-instance-identifier bi-read-replica --db-instance-class db.t3.medium --engine aurora-postgresql --db-cluster-identifier bi-cluster\n```\n\n## Follow-up Questions\n- How would you test a safe switch to BI workloads during peak traffic?\n- How would you handle schema drift or upgrades on the BI schema without impacting OLTP?\n","diagram":null,"difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T21:32:53.209Z","createdAt":"2026-01-26T21:32:53.209Z"},{"id":"q-851","question":"Two-region OLTP SaaS with a single writer in us-east-1 and read replicas in eu-west-1. Compare Aurora Global Database (PostgreSQL) vs DynamoDB Global Tables for this workload: latency targets, consistency model, failover behavior, and cost. Which approach would you pick and why, and what concrete configuration (replica count, failover window, write routing) would you implement to meet RTO < 60s and RPO < 5s?","answer":"Choose Aurora Global Database (PostgreSQL) with a single writer in us-east-1 and one or more read replicas in eu-west-1. It preserves transactional invariants with asynchronous cross-region replicatio","explanation":"## Why This Is Asked\nThis question probes understanding of cross-region OLTP replication choices, consistency, failover, and cost. It contrasts HTAP-like needs with strict ACID guarantees in real-world deployments.\n\n## Key Concepts\n- Aurora Global Database vs DynamoDB Global Tables trade-offs\n- Single-writer constraint and cross-region replication lag\n- RTO/RPO targets, failover orchestration, PITR\n\n## Code Example\n```javascript\naws rds create-global-database --global-database-name my-globaldb --source-db-cluster-identifier mydbcluster\n```\n```\n\n## Follow-up Questions\n- How would you monitor replication lag and what alerts would you set?\n- How would you handle schema migrations with zero downtime across regions?\n","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Robinhood","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:32:32.498Z","createdAt":"2026-01-12T13:32:32.498Z"},{"id":"q-871","question":"Migration plan: An OLTP app runs on Aurora PostgreSQL provisioned; traffic is bursty; you want to evaluate Aurora Serverless v2. Provide a concrete plan to migrate, including: (1) start/stop criteria and scaling configuration; (2) handling of long-running transactions and prepared statements; (3) how to keep reads consistent during scaling; (4) testing approach for failover/RTO targets; (5) cost considerations and potential pitfalls with Serverless v2?","answer":"Plan a phased migration to Aurora Serverless v2 from provisioned instances. Use min_capacity 0.5 and max_capacity 16, disable auto_pause initially, route traffic through RDS Proxy, and validate long-r","explanation":"## Why This Is Asked\nServerless migrations are common; evaluate trade-offs.\n\n## Key Concepts\n- Aurora Serverless v2\n- scaling policies\n- transaction semantics\n- prepared statements\n- RDS Proxy\n- failover testing\n\n## Code Example\n```javascript\n// Aurora Serverless v2 config (example)\nconst auroraConfig = {\n  engine: 'aurora-postgresql',\n  scale: { min_capacity: 0.5, max_capacity: 16, auto_pause: false }\n};\n```\n\n## Follow-up Questions\n- How would you monitor connection pool usage during scaling?\n- What are Serverless v2 limitations with prepared statements or long-running queries?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","IBM","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:53:18.115Z","createdAt":"2026-01-12T13:53:18.115Z"},{"id":"q-895","question":"Your multi-region SaaS needs an audit-friendly cross-tenant analytics store with writes transactional in us-east-1 and analytics queries in eu-west-1 under GDPR. Compare Aurora PostgreSQL Global Database vs DynamoDB Global Tables for this workload, focusing on transactional integrity, analytics capability, PITR/retention, cross-region latency, and cost. Recommend a concrete configuration (writer region, replica counts, PITR window, tenant isolation, ETL approach) to meet RPO 15 minutes and RTO 1 hour?","answer":"Aurora PostgreSQL Global Database best meets cross-region transactional integrity with SQL analytics, in a GDPR context. Put writer in us-east-1, two read replicas in eu-west-1; enable PITR 30 days; r","explanation":"## Why This Is Asked\nTests a candidate's ability to balance transactional integrity, cross-region DR, and analytics in a regulated multi-tenant environment.\n\n## Key Concepts\n- Aurora Global Database vs DynamoDB Global Tables\n- PITR, RPO/RTO targets, GDPR/tenant isolation\n- ETL paths to analytics stores (Redshift/Data Lake)\n\n## Code Example\n```javascript\n// Pseudo: configure a DMS task to replicate from us-east-1 to eu-west-1\nconst task = await dms.createReplicationTask({ ... });\n```\n\n## Follow-up Questions\n- How would you handle schema changes across regions without downtime?\n- What telemetry would you collect to validate RPO/RTO in production?","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Slack","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:36:01.445Z","createdAt":"2026-01-12T14:36:01.445Z"},{"id":"q-956","question":"For a real-time fraud graph application needing sub-100ms neighbor lookups across two AWS regions, compare Amazon Neptune Global Database with DynamoDB (using graph patterns and DAX) for this workload. Writer region us-east-1; readers in eu-west-1; assess graph traversal latency, consistency guarantees, failover behavior, and total cost. Provide a concrete setup (cluster engine and size, replica counts, PITR window, backup schedule, and network/config) to meet an RPO of 5s and an RTO of 60s?","answer":"Choose Neptune Global Database for graph-centric queries. It provides cross-region replication with near real-time reads; DynamoDB+DAX is weaker for complex traversals. Recommend a primary cluster in ","explanation":"## Why This Is Asked\nDiscusses cross-region graph DB choices and DR readiness in practice.\n\n## Key Concepts\n- Neptune Global Database vs DynamoDB/DAX trade-offs\n- Graph traversals, latency budgets, consistency models\n- Cross-region failover, PITR, backups, and cost\n\n## Code Example\n```javascript\n// Illustrative AWS CLI usage (not executed here)\naws neptune create-global-cluster --global-cluster-name FraudGraphGlobal --engine neptune\n```\n\n## Follow-up Questions\n- How would you monitor replication lag and graph query latency?\n- What tests validate RPO/RTO under regional failure?\n","diagram":null,"difficulty":"intermediate","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","NVIDIA","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T16:42:36.355Z","createdAt":"2026-01-12T16:42:36.355Z"},{"id":"q-964","question":"You run an Amazon RDS PostgreSQL in **us-east-1** with automated backups. A regional outage blocks access from that region. How would you achieve **RPO ≤ 60s** and **RTO ≤ 15 minutes** by restoring to **eu-west-1**? Compare cross-region read replicas, backup copy, and Aurora Global Database, and outline concrete steps, knobs, and caveats?","answer":"Prefer Aurora Global Database for true cross-region DR with <60s RPO and <15m RTO. If constrained to RDS PostgreSQL, copy automated backups to eu-west-1 and create a read replica there; promote replic","explanation":"## Why This Is Asked\nDR planning across AWS regions is a core skill. This question tests understanding of RPO/RTO, replication guarantees, and the trade-offs between RDS Cross-Region Replicas, snapshot copying, and Aurora Global Database.\n\n## Key Concepts\n- RPO vs RTO and replication lag\n- Cross-region DR options: RDS read replicas, snapshot copy, Aurora Global DB\n- Failover orchestration and DNS routing\n\n## Code Example\n```javascript\n// AWS CLI example: copy a snapshot to another region\naws rds copy-db-snapshot --source-db-snapshot-identifier arn:aws:rds:us-east-1:123456789012:snapshot:mydb-2026-01-01 --target-db-snapshot-identifier eu-west-1-mydb-2026-01-01 --source-region us-east-1 --region eu-west-1\n\n// Restore a DB instance from the cross-region snapshot\naws rds restore-db-instance-from-db-snapshot --db-instance-identifier eu-west-1-mydb-restored --db-snapshot-identifier eu-west-1-mydb-2026-01-01\n```\n\n## Follow-up Questions\n- How would you automate the failover and DNS switch?\n- What monitoring would you put in place to detect lag and test RPO/RTO?","diagram":"flowchart TD\n A[Source: RDS us-east-1] --> B{Strategy}\n B --> C[Aurora Global Database]\n B --> D[Cross-region backups + EU replica]\n C --> E[Fast RTO, low RPO]\n D --> F[Longer RPO, slower failover]","difficulty":"beginner","tags":["aws-database-specialty"],"channel":"aws-database-specialty","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Slack","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T17:26:08.106Z","createdAt":"2026-01-12T17:26:08.106Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":87,"beginner":23,"intermediate":38,"advanced":26,"newThisWeek":36}}