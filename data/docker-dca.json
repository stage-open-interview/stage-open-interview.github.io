{"questions":[{"id":"q-856","question":"You're running a Docker Swarm with services frontend, api, and worker. A feature-flag config is provided via Docker Config mounted at /etc/flags.json in all containers. You must rotate this config weekly with zero downtime. Describe the exact sequence of commands to create a new config version, rotate the services to use it, and implement a graceful reload inside apps so the new flags are picked up without losing requests. Include any Swarm update options you would tune?","answer":"Create a new config file (e.g., /tmp/flags.v2.json) and docker config create flags.v2 /tmp/flags.v2.json. Then update each service: docker service update --config-rm flags.v1 --config-add flags.v2 --u","explanation":"## Why This Is Asked\nTo assess practical config rotation in Swarm and application reload behavior.\n\n## Key Concepts\n- Docker Config rotation\n- Rolling updates with update-parallelism and update-delay\n- Graceful reload semantics (SIGHUP or /reload)\n\n## Code Example\n```javascript\n// Node.js reload handler\nconst fs = require('fs');\nlet flags = JSON.parse(fs.readFileSync('/etc/flags.json','utf8'));\nprocess.on('SIGHUP', () => {\n  flags = JSON.parse(fs.readFileSync('/etc/flags.json','utf8'));\n  // apply flags to runtime features\n});\n```\n\n## Follow-up Questions\n- How would you test the zero-downtime rollout in a CI pipeline?\n- What failure scenarios would prompt you to revert the config rotation?","diagram":null,"difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:38:40.484Z","createdAt":"2026-01-12T13:38:40.484Z"},{"id":"q-862","question":"In a Docker Swarm with a stateful web app that uses Postgres, you must roll out version 3.2 with a DB schema migration and zero downtime. Propose a concrete upgrade plan that uses a start-first, one-task-at-a-time update, a separate migration container, and post-migration validation. Include exact Swarm commands and a minimal docker-compose snippet showing update_config?","answer":"Plan: run DB migration first in a dedicated one-off container, then perform a start-first, one-task-at-a-time web upgrade. Commands: docker run --rm --network swarm_net migrate:3.2; docker service upd","explanation":"## Why This Is Asked\nAssesses ability to coordinate schema migrations with zero downtime, manage Swarm update strategies, and handle rollback safely.\n\n## Key Concepts\n- Start-first updates and parallelism control\n- Separate migration container orchestration\n- Post-migration validation and rollback\n- Health checks and observability during upgrade\n\n## Code Example\n```yaml\nversion: '3.8'\nservices:\n  web:\n    image: myapp:3.2\n    deploy:\n      update_config:\n        parallelism: 1\n        delay: 15s\n        order: start-first\n```\n\n## Follow-up Questions\n- How would you handle long-running migrations?\n- How ensure data consistency with read replicas during upgrade?","diagram":null,"difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:43:29.032Z","createdAt":"2026-01-12T13:43:29.032Z"},{"id":"docker-dca-image-creation-1768206789900-0","question":"When building a production Docker image for a microservice, which approach minimizes final image size while preserving build cache and readability?","answer":"[{\"id\":\"a\",\"text\":\"Use a multi-stage build and copy only the final binary to a minimal base image\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a single-stage build with the full toolchain to simplify the Dockerfile\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Install build tools in the final image to ensure reproducibility\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Pin the base image tag to latest to receive security updates automatically\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nThe correct option is a because using a multi-stage build reduces the final image size by excluding the build tools and intermediate artifacts from the runtime image.\n\n## Why Other Options Are Wrong\n\n- Option B: Using a single-stage build with the full toolchain would bloat the final image and hamper caching.\n- Option C: Installing build tools in the final image defeats the purpose of a minimal runtime image.\n- Option D: Pinning the base image tag to latest leads to non-deterministic builds and potential drift.\n\n## Key Concepts\n\n- Multi-stage builds\n- Minimal runtime images\n- Build cache optimization\n\n## Real-World Application\n\nEmploy multi-stage builds in CI pipelines to produce lean images, speeding deployments and reducing attack surface.","diagram":null,"difficulty":"intermediate","tags":["Docker","ECR","EKS","Terraform","certification-mcq","domain-weight-20"],"channel":"docker-dca","subChannel":"image-creation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T08:33:09.901Z","createdAt":"2026-01-12 08:33:10"},{"id":"docker-dca-image-creation-1768206789900-1","question":"To ensure that only signed images are deployed to a private registry, which practice should you implement?","answer":"[{\"id\":\"a\",\"text\":\"Disable content trust and rely on registry access controls\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable Docker Content Trust and sign images with Notary before pushing\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Sign images only after deployment using runtime attestation\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use an external script to verify checksums after pulling images\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption B is correct because Docker Content Trust signs images using Notary, ensuring only signed images are deployed.\n\n## Why Other Options Are Wrong\n\n- Option A: Disabling content trust removes integrity guarantees and undermines deployment policies.\n- Option C: Runtime attestation is not the standard mechanism for enforcing signed images at push time.\n- Option D: Verifying checksums after pulling does not enforce signing and provenance at build/push time.\n\n## Key Concepts\n\n- Docker Content Trust\n- Notary\n- Image signing\n\n## Real-World Application\n\nImplements policy enforcement in CI/CD workflows, preventing untrusted images from reaching production registries.","diagram":null,"difficulty":"intermediate","tags":["Docker","ECR","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"docker-dca","subChannel":"image-creation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T08:33:10.341Z","createdAt":"2026-01-12 08:33:10"},{"id":"docker-dca-image-creation-1768206789900-2","question":"Your private registry is growing and stale images consume storage. Which approach is recommended to reclaim space in a Docker Registry V2 when artifacts are unreferenced by any tag or digest?","answer":"[{\"id\":\"a\",\"text\":\"Run registry garbage collection with the garbage-collect tool to reclaim unreferenced blobs\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Manually delete blobs from the registry's filesystem\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use docker image prune on the registry host\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Set a hard limit on repository size and purge via cron job\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption A is correct because registry garbage collection reclaims unreferenced blobs and cleans up space; manual deletion can corrupt blob storage; docker image prune operates on the host, not the registry storage; hard limits with cron pose data loss risks.\n\n## Why Other Options Are Wrong\n\n- Option B: Manually deleting blobs is error-prone and not scalable.\n- Option C: Docker image prune affects only the host daemon, not the registry storage.\n- Option D: Hard limits risk data loss and are not a robust cleanup strategy.\n\n## Key Concepts\n\n- Registry garbage collection\n- Unreferenced blobs\n- Blob storage lifecycle\n\n## Real-World Application\n\nKeeps private registries lean by reclaiming storage as images become unreferenced, reducing storage costs and ensuring cleaner artifact management.","diagram":null,"difficulty":"intermediate","tags":["Docker","ECR","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"docker-dca","subChannel":"image-creation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T08:33:10.747Z","createdAt":"2026-01-12 08:33:10"},{"id":"docker-dca-installation-config-1768231621334-0","question":"After adding a user to the docker group on an Ubuntu host, which action is most likely required for the user to run docker commands without sudo?","answer":"[{\"id\":\"a\",\"text\":\"Reboot the system\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Start the docker daemon\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Log out and log back in to refresh group membership\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Remove the user from the docker group\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption c is correct because group membership changes require a new login session to take effect.\n\n## Why Other Options Are Wrong\n- A: Reboot is not required to apply group membership changes; a re-login suffices.\n- B: Starting the Docker daemon is unrelated to refreshing group membership.\n- D: Removing the user from the docker group defeats the purpose of granting access.\n\n## Key Concepts\n- Linux user groups and permission management\n- Session refresh and login reauthentication\n\n## Real-World Application\n- Onboarding new developers to a host and ensuring proper Docker access after group changes.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-15"],"channel":"docker-dca","subChannel":"installation-config","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:27:01.335Z","createdAt":"2026-01-12 15:27:01"},{"id":"docker-dca-installation-config-1768231621334-1","question":"To ensure Docker uses systemd as the cgroup driver for Kubernetes workloads on a node, which action should you take?","answer":"[{\"id\":\"a\",\"text\":\"Set the docker daemon to use systemd as the cgroup driver via daemon.json and restart Docker\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Configure the kubelet with the flag --cgroup-driver=systemd only\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enable cgroup v2 in the kernel and keep the default Docker driver\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Start Docker with the flag --cgroup-driver=cgroupfs\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because aligning Docker's cgroup driver to systemd is a prerequisite for stable Kubernetes integration; this is achieved by configuring daemon.json or the daemon startup options and then restarting Docker.\n\n## Why Other Options Are Wrong\n- B: Configuring the kubelet alone does not guarantee Docker uses systemd as the driver.\n- C: Enabling cgroup v2 does not specifically set the Docker driver to systemd.\n- D: cgroupfs is the alternative and is not aligned with typical Kubernetes expectations.\n\n## Key Concepts\n- Cgroup driver compatibility between Docker and Kubernetes\n- Docker daemon configuration\n\n## Real-World Application\n- Ensures consistent resource management and avoids daemon/kubelet driver mismatches in a Kubernetes cluster.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-15"],"channel":"docker-dca","subChannel":"installation-config","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:27:01.846Z","createdAt":"2026-01-12 15:27:02"},{"id":"docker-dca-installation-config-1768231621334-2","question":"You want Docker to pull from an internal registry mirror at https://docker-registry.company.local. Which daemon.json entry enables this?","answer":"[{\"id\":\"a\",\"text\":\"registry-mirrors set to the internal mirror\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"insecure-registries set to docker-registry.company.local\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"registry set to https://docker-registry.company.local\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"mirror-registry set to https://docker-registry.company.local\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because registry-mirrors is the daemon setting used to specify image mirrors for pulls.\n\n## Why Other Options Are Wrong\n- B: insecure-registries refers to registries that do not use TLS, not to mirrors.\n- C: registry is not a valid daemon.json key for mirror configuration.\n- D: mirror-registry is not a recognized daemon setting.\n\n## Key Concepts\n- Docker daemon configuration for image sources\n- Distinction between registry mirrors and insecure registries\n\n## Real-World Application\n- Accelerates image pulls in network-restricted environments by using internal mirrors.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-15"],"channel":"docker-dca","subChannel":"installation-config","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:27:02.209Z","createdAt":"2026-01-12 15:27:02"},{"id":"docker-dca-installation-config-1768231621334-3","question":"In a two-node environment, you want to run a simple Docker Swarm that shares an overlay network across both nodes. Which sequence correctly initializes the swarm, creates the overlay network, and deploys a service attached to that network?","answer":"[{\"id\":\"a\",\"text\":\"docker swarm init on manager; docker network create -d overlay webnet; docker service create --name web --network webnet nginx\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"docker network create -d overlay webnet; docker swarm init; docker service create --name web --network webnet nginx\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"docker swarm join on worker1; docker swarm join on worker2; docker network create overlay webnet; docker service create --name web --network webnet nginx\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"docker swarm init; docker service create --name web --network host nginx\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because the proper flow is to initialize the swarm on a manager, create an overlay network, and then deploy a service attached to that overlay.\n\n## Why Other Options Are Wrong\n- B: Overlay networks belong to an active swarm; creating the network before initializing the swarm is invalid.\n- C: Joining workers before the manager is ready can fail to form the swarm properly.\n- D: Attaching the service to the host network bypasses the overlay concept and is not suitable for swarm overlay use.\n\n## Key Concepts\n- Docker Swarm workflow\n- Overlay networks across multiple nodes\n- Service deployment in Swarm\n\n## Real-World Application\n- Setting up a simple multi-node Swarm for scalable microservices with isolated networks.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-15"],"channel":"docker-dca","subChannel":"installation-config","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:27:02.336Z","createdAt":"2026-01-12 15:27:02"},{"id":"docker-dca-installation-config-1768231621334-4","question":"You want to harden a Docker daemon before production deployment by enabling user namespace remapping. Which daemon.json setting enables this by mapping container root to a non-root host user?","answer":"[{\"id\":\"a\",\"text\":\"userns-remap set to default\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"exec-opts for systemd cgroup driver\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"security-opt no-new-privileges\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"live-restore enabled\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because enabling user namespaces with the remapping feature maps container root to a non-root host user, mitigating privilege escalation risks.\n\n## Why Other Options Are Wrong\n- B: This configures the cgroup driver, not user namespaces.\n- C: no-new-privileges is a runtime security option, not a namespace remapping setting.\n- D: live-restore preserves containers across Docker restarts, not related to user namespaces.\n\n## Key Concepts\n- Docker daemon security hardening\n- User namespaces and mappings\n- Privilege isolation in containers\n\n## Real-World Application\n- Strengthens defense-in-depth for production Docker hosts against container escape.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-15"],"channel":"docker-dca","subChannel":"installation-config","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:27:02.463Z","createdAt":"2026-01-12 15:27:02"},{"id":"docker-dca-orchestration-1768169949801-0","question":"In Docker Swarm, which approach best achieves a rolling update with at most two updated tasks at a time and a ten-second delay between updates, while minimizing downtime?","answer":"[{\"id\":\"a\",\"text\":\"docker service update --update-parallelism 2 --update-delay 10s <service>\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"docker stack deploy --compose-file docker-compose.yaml <stack>\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"docker service update --update-parallelism 3 --update-delay 10s <service>\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"docker service update --update-order start-first --update-parallelism 2 --update-delay 10s <service>\",\"isCorrect\":true}]","explanation":"## Correct Answer\n**Option D**\n\nThe best approach explicitly sets the update order to start-first, ensuring new tasks come up before old ones are stopped, which minimizes downtime. It also enforces a maximum of two simultaneous updates and a 10s delay between updates via --update-parallelism 2 and --update-delay 10s.\n\n```javascript\n# Example (illustrative only):\ndocker service update --update-order start-first --update-parallelism 2 --update-delay 10s my_web_service\n```\n\n## Why Other Options Are Wrong\n- A: Lacks an explicit update-order, so it may follow the default stop-first behavior which can increase downtime.\n- B: Deploys a new stack rather than performing an in-place update of an existing service.\n- C: Uses a higher parallelism (3), violating the requirement of at most two concurrent updates.\n\n## Key Concepts\n- docker service update options: --update-parallelism, --update-delay, --update-order\n- Update order semantics: start-first minimizes downtime; stop-first is safer but can cause longer downtime\n\n## Real-World Application\n- Precise rollout controls in production Swarm clusters reduce risk during deployments.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","Terraform","Orchestration","certification-mcq","domain-weight-25"],"channel":"docker-dca","subChannel":"orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:19:09.803Z","createdAt":"2026-01-11 22:19:10"},{"id":"docker-dca-orchestration-1768169949801-1","question":"You want to securely provide a database password to all replicas of a service in a Swarm cluster. Which approach is correct?","answer":"[{\"id\":\"a\",\"text\":\"Pass the password as an environment variable in the service spec\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a Docker secret named db_password and attach it to the service; read from /run/secrets/db_password\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Store the password in a mounted file on the host and read it from the container\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a Kubernetes ConfigMap to inject the password\",\"isCorrect\":false}]","explanation":"## Correct Answer\n**Option B**\n\nDocker secrets provide encrypted storage and are mounted into containers at /run/secrets/<secret_name>, and only to the services that are granted the secret. This avoids plaintext environment variables and keeps credentials out of image layers.\n\n```bash\n# Example conceptually (secret creation and attach):\ndocker secret create db_password password.txt\ndocker service update --secret-add db_password my_db_service\n```\n\n## Why Other Options Are Wrong\n- A: Environment variables can leak via process listing and are stored in container metadata and image layers.\n- C: Host-mounted files are not managed by Swarm and can be less secure and harder to rotate.\n- D: Configs are for non-secret data; they are not ideal for sensitive credentials.\n\n## Key Concepts\n- Docker Secrets\n- /run/secrets/<name> access path\n- Service-level access control for secrets\n\n## Real-World Application\n- Centralized secret management for database credentials in a Swarm cluster, enabling secure rotation and restricted access.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","Terraform","Orchestration","certification-mcq","domain-weight-25"],"channel":"docker-dca","subChannel":"orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:19:10.273Z","createdAt":"2026-01-11 22:19:10"},{"id":"docker-dca-orchestration-1768169949801-2","question":"Which statement best reflects autoscaling capabilities in Docker Swarm for a service based on CPU utilization?","answer":"[{\"id\":\"a\",\"text\":\"Docker Swarm provides built-in autoscaling based on CPU via an HPA-like feature\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Autoscaling can be achieved automatically by Swarm when CPU usage spikes\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"There is no built-in autoscaling in Swarm; external tooling or custom scripts are required to adjust replicas based on metrics\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Swarm automatically scales across nodes when there is a single failed node\",\"isCorrect\":false}]","explanation":"## Correct Answer\n**Option C**\n\nDocker Swarm does not include built-in autoscaling based on metrics. To auto-scale, you typically integrate external monitoring/automation (e.g., custom scripts, external schedulers) that adjust the replica count via the Docker API or CLI.\n\n## Why Other Options Are Wrong\n- A: Swarm lacks an HPA-like autoscaler comparable to Kubernetes.\n- B: Swarm does not automatically scale purely on CPU without external tooling.\n- D: Swarm does not auto-scale on node failures by default.\n\n## Key Concepts\n- Lack of native autoscaling in Swarm\n- External autoscaling approaches (external monitors, scripts, API calls)\n\n## Real-World Application\n- Implementing a monitoring-based scaler to maintain desired replica counts in response to load in a Swarm cluster.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","Terraform","Orchestration","certification-mcq","domain-weight-25"],"channel":"docker-dca","subChannel":"orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:19:10.736Z","createdAt":"2026-01-11 22:19:10"}],"subChannels":["general","image-creation","installation-config","orchestration"],"companies":["Hashicorp","IBM","PayPal","Snowflake"],"stats":{"total":13,"beginner":1,"intermediate":11,"advanced":1,"newThisWeek":13}}