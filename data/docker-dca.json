{"questions":[{"id":"q-856","question":"You're running a Docker Swarm with services frontend, api, and worker. A feature-flag config is provided via Docker Config mounted at /etc/flags.json in all containers. You must rotate this config weekly with zero downtime. Describe the exact sequence of commands to create a new config version, rotate the services to use it, and implement a graceful reload inside apps so the new flags are picked up without losing requests. Include any Swarm update options you would tune?","answer":"Create a new config file (e.g., /tmp/flags.v2.json) and docker config create flags.v2 /tmp/flags.v2.json. Then update each service: docker service update --config-rm flags.v1 --config-add flags.v2 --u","explanation":"## Why This Is Asked\nTo assess practical config rotation in Swarm and application reload behavior.\n\n## Key Concepts\n- Docker Config rotation\n- Rolling updates with update-parallelism and update-delay\n- Graceful reload semantics (SIGHUP or /reload)\n\n## Code Example\n```javascript\n// Node.js reload handler\nconst fs = require('fs');\nlet flags = JSON.parse(fs.readFileSync('/etc/flags.json','utf8'));\nprocess.on('SIGHUP', () => {\n  flags = JSON.parse(fs.readFileSync('/etc/flags.json','utf8'));\n  // apply flags to runtime features\n});\n```\n\n## Follow-up Questions\n- How would you test the zero-downtime rollout in a CI pipeline?\n- What failure scenarios would prompt you to revert the config rotation?","diagram":null,"difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:38:40.484Z","createdAt":"2026-01-12T13:38:40.484Z"},{"id":"q-862","question":"In a Docker Swarm with a stateful web app that uses Postgres, you must roll out version 3.2 with a DB schema migration and zero downtime. Propose a concrete upgrade plan that uses a start-first, one-task-at-a-time update, a separate migration container, and post-migration validation. Include exact Swarm commands and a minimal docker-compose snippet showing update_config?","answer":"Plan: run DB migration first in a dedicated one-off container, then perform a start-first, one-task-at-a-time web upgrade. Commands: docker run --rm --network swarm_net migrate:3.2; docker service upd","explanation":"## Why This Is Asked\nAssesses ability to coordinate schema migrations with zero downtime, manage Swarm update strategies, and handle rollback safely.\n\n## Key Concepts\n- Start-first updates and parallelism control\n- Separate migration container orchestration\n- Post-migration validation and rollback\n- Health checks and observability during upgrade\n\n## Code Example\n```yaml\nversion: '3.8'\nservices:\n  web:\n    image: myapp:3.2\n    deploy:\n      update_config:\n        parallelism: 1\n        delay: 15s\n        order: start-first\n```\n\n## Follow-up Questions\n- How would you handle long-running migrations?\n- How ensure data consistency with read replicas during upgrade?","diagram":null,"difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:43:29.032Z","createdAt":"2026-01-12T13:43:29.032Z"},{"id":"docker-dca-image-creation-1768206789900-0","question":"When building a production Docker image for a microservice, which approach minimizes final image size while preserving build cache and readability?","answer":"[{\"id\":\"a\",\"text\":\"Use a multi-stage build and copy only the final binary to a minimal base image\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a single-stage build with the full toolchain to simplify the Dockerfile\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Install build tools in the final image to ensure reproducibility\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Pin the base image tag to latest to receive security updates automatically\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nThe correct option is a because using a multi-stage build reduces the final image size by excluding the build tools and intermediate artifacts from the runtime image.\n\n## Why Other Options Are Wrong\n\n- Option B: Using a single-stage build with the full toolchain would bloat the final image and hamper caching.\n- Option C: Installing build tools in the final image defeats the purpose of a minimal runtime image.\n- Option D: Pinning the base image tag to latest leads to non-deterministic builds and potential drift.\n\n## Key Concepts\n\n- Multi-stage builds\n- Minimal runtime images\n- Build cache optimization\n\n## Real-World Application\n\nEmploy multi-stage builds in CI pipelines to produce lean images, speeding deployments and reducing attack surface.","diagram":null,"difficulty":"intermediate","tags":["Docker","ECR","EKS","Terraform","certification-mcq","domain-weight-20"],"channel":"docker-dca","subChannel":"image-creation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T08:33:09.901Z","createdAt":"2026-01-12 08:33:10"},{"id":"docker-dca-image-creation-1768206789900-1","question":"To ensure that only signed images are deployed to a private registry, which practice should you implement?","answer":"[{\"id\":\"a\",\"text\":\"Disable content trust and rely on registry access controls\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable Docker Content Trust and sign images with Notary before pushing\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Sign images only after deployment using runtime attestation\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use an external script to verify checksums after pulling images\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption B is correct because Docker Content Trust signs images using Notary, ensuring only signed images are deployed.\n\n## Why Other Options Are Wrong\n\n- Option A: Disabling content trust removes integrity guarantees and undermines deployment policies.\n- Option C: Runtime attestation is not the standard mechanism for enforcing signed images at push time.\n- Option D: Verifying checksums after pulling does not enforce signing and provenance at build/push time.\n\n## Key Concepts\n\n- Docker Content Trust\n- Notary\n- Image signing\n\n## Real-World Application\n\nImplements policy enforcement in CI/CD workflows, preventing untrusted images from reaching production registries.","diagram":null,"difficulty":"intermediate","tags":["Docker","ECR","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"docker-dca","subChannel":"image-creation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T08:33:10.341Z","createdAt":"2026-01-12 08:33:10"},{"id":"docker-dca-image-creation-1768206789900-2","question":"Your private registry is growing and stale images consume storage. Which approach is recommended to reclaim space in a Docker Registry V2 when artifacts are unreferenced by any tag or digest?","answer":"[{\"id\":\"a\",\"text\":\"Run registry garbage collection with the garbage-collect tool to reclaim unreferenced blobs\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Manually delete blobs from the registry's filesystem\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Use docker image prune on the registry host\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Set a hard limit on repository size and purge via cron job\",\"isCorrect\":false}]","explanation":"## Correct Answer\n\nOption A is correct because registry garbage collection reclaims unreferenced blobs and cleans up space; manual deletion can corrupt blob storage; docker image prune operates on the host, not the registry storage; hard limits with cron pose data loss risks.\n\n## Why Other Options Are Wrong\n\n- Option B: Manually deleting blobs is error-prone and not scalable.\n- Option C: Docker image prune affects only the host daemon, not the registry storage.\n- Option D: Hard limits risk data loss and are not a robust cleanup strategy.\n\n## Key Concepts\n\n- Registry garbage collection\n- Unreferenced blobs\n- Blob storage lifecycle\n\n## Real-World Application\n\nKeeps private registries lean by reclaiming storage as images become unreferenced, reducing storage costs and ensuring cleaner artifact management.","diagram":null,"difficulty":"intermediate","tags":["Docker","ECR","Kubernetes","Terraform","certification-mcq","domain-weight-20"],"channel":"docker-dca","subChannel":"image-creation","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T08:33:10.747Z","createdAt":"2026-01-12 08:33:10"},{"id":"docker-dca-installation-config-1768231621334-0","question":"After adding a user to the docker group on an Ubuntu host, which action is most likely required for the user to run docker commands without sudo?","answer":"[{\"id\":\"a\",\"text\":\"Reboot the system\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Start the docker daemon\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Log out and log back in to refresh group membership\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Remove the user from the docker group\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption c is correct because group membership changes require a new login session to take effect.\n\n## Why Other Options Are Wrong\n- A: Reboot is not required to apply group membership changes; a re-login suffices.\n- B: Starting the Docker daemon is unrelated to refreshing group membership.\n- D: Removing the user from the docker group defeats the purpose of granting access.\n\n## Key Concepts\n- Linux user groups and permission management\n- Session refresh and login reauthentication\n\n## Real-World Application\n- Onboarding new developers to a host and ensuring proper Docker access after group changes.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-15"],"channel":"docker-dca","subChannel":"installation-config","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:27:01.335Z","createdAt":"2026-01-12 15:27:01"},{"id":"docker-dca-installation-config-1768231621334-1","question":"To ensure Docker uses systemd as the cgroup driver for Kubernetes workloads on a node, which action should you take?","answer":"[{\"id\":\"a\",\"text\":\"Set the docker daemon to use systemd as the cgroup driver via daemon.json and restart Docker\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Configure the kubelet with the flag --cgroup-driver=systemd only\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Enable cgroup v2 in the kernel and keep the default Docker driver\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Start Docker with the flag --cgroup-driver=cgroupfs\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because aligning Docker's cgroup driver to systemd is a prerequisite for stable Kubernetes integration; this is achieved by configuring daemon.json or the daemon startup options and then restarting Docker.\n\n## Why Other Options Are Wrong\n- B: Configuring the kubelet alone does not guarantee Docker uses systemd as the driver.\n- C: Enabling cgroup v2 does not specifically set the Docker driver to systemd.\n- D: cgroupfs is the alternative and is not aligned with typical Kubernetes expectations.\n\n## Key Concepts\n- Cgroup driver compatibility between Docker and Kubernetes\n- Docker daemon configuration\n\n## Real-World Application\n- Ensures consistent resource management and avoids daemon/kubelet driver mismatches in a Kubernetes cluster.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-15"],"channel":"docker-dca","subChannel":"installation-config","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:27:01.846Z","createdAt":"2026-01-12 15:27:02"},{"id":"docker-dca-installation-config-1768231621334-2","question":"You want Docker to pull from an internal registry mirror at https://docker-registry.company.local. Which daemon.json entry enables this?","answer":"[{\"id\":\"a\",\"text\":\"registry-mirrors set to the internal mirror\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"insecure-registries set to docker-registry.company.local\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"registry set to https://docker-registry.company.local\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"mirror-registry set to https://docker-registry.company.local\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because registry-mirrors is the daemon setting used to specify image mirrors for pulls.\n\n## Why Other Options Are Wrong\n- B: insecure-registries refers to registries that do not use TLS, not to mirrors.\n- C: registry is not a valid daemon.json key for mirror configuration.\n- D: mirror-registry is not a recognized daemon setting.\n\n## Key Concepts\n- Docker daemon configuration for image sources\n- Distinction between registry mirrors and insecure registries\n\n## Real-World Application\n- Accelerates image pulls in network-restricted environments by using internal mirrors.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-15"],"channel":"docker-dca","subChannel":"installation-config","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:27:02.209Z","createdAt":"2026-01-12 15:27:02"},{"id":"docker-dca-installation-config-1768231621334-3","question":"In a two-node environment, you want to run a simple Docker Swarm that shares an overlay network across both nodes. Which sequence correctly initializes the swarm, creates the overlay network, and deploys a service attached to that network?","answer":"[{\"id\":\"a\",\"text\":\"docker swarm init on manager; docker network create -d overlay webnet; docker service create --name web --network webnet nginx\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"docker network create -d overlay webnet; docker swarm init; docker service create --name web --network webnet nginx\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"docker swarm join on worker1; docker swarm join on worker2; docker network create overlay webnet; docker service create --name web --network webnet nginx\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"docker swarm init; docker service create --name web --network host nginx\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because the proper flow is to initialize the swarm on a manager, create an overlay network, and then deploy a service attached to that overlay.\n\n## Why Other Options Are Wrong\n- B: Overlay networks belong to an active swarm; creating the network before initializing the swarm is invalid.\n- C: Joining workers before the manager is ready can fail to form the swarm properly.\n- D: Attaching the service to the host network bypasses the overlay concept and is not suitable for swarm overlay use.\n\n## Key Concepts\n- Docker Swarm workflow\n- Overlay networks across multiple nodes\n- Service deployment in Swarm\n\n## Real-World Application\n- Setting up a simple multi-node Swarm for scalable microservices with isolated networks.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-15"],"channel":"docker-dca","subChannel":"installation-config","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:27:02.336Z","createdAt":"2026-01-12 15:27:02"},{"id":"docker-dca-installation-config-1768231621334-4","question":"You want to harden a Docker daemon before production deployment by enabling user namespace remapping. Which daemon.json setting enables this by mapping container root to a non-root host user?","answer":"[{\"id\":\"a\",\"text\":\"userns-remap set to default\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"exec-opts for systemd cgroup driver\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"security-opt no-new-privileges\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"live-restore enabled\",\"isCorrect\":false}]","explanation":"## Correct Answer\nOption a is correct because enabling user namespaces with the remapping feature maps container root to a non-root host user, mitigating privilege escalation risks.\n\n## Why Other Options Are Wrong\n- B: This configures the cgroup driver, not user namespaces.\n- C: no-new-privileges is a runtime security option, not a namespace remapping setting.\n- D: live-restore preserves containers across Docker restarts, not related to user namespaces.\n\n## Key Concepts\n- Docker daemon security hardening\n- User namespaces and mappings\n- Privilege isolation in containers\n\n## Real-World Application\n- Strengthens defense-in-depth for production Docker hosts against container escape.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-15"],"channel":"docker-dca","subChannel":"installation-config","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T15:27:02.463Z","createdAt":"2026-01-12 15:27:02"},{"id":"docker-dca-networking-1768243086318-0","question":"In a Docker Swarm with three nodes, you deploy a service 'web' attached to an overlay network shared by other services. The web service needs to communicate with a 'db' service by name. Which mechanism enables resolving service names to container IPs across the cluster?","answer":"[{\"id\":\"a\",\"text\":\"Use the host's /etc/hosts file to map service name to IP\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"The embedded DNS server in Docker Swarm on the overlay network\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use an external DNS service like Route 53\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely on the bridge network's built-in DNS\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe embedded DNS server in Docker Swarm on the overlay network resolves service names to VIPs across the cluster.\n\n## Why Other Options Are Wrong\n- External DNS like Route 53 is not used for internal Swarm service discovery.\n- /etc/hosts is not dynamic and cannot scale with container lifecycle or multiple nodes.\n- Bridge networks don't provide cross-host service discovery across nodes.\n\n## Key Concepts\n- Docker Swarm service discovery\n- Overlay network DNS resolution\n\n## Real-World Application\n- When deploying multi-service apps in Swarm, services can refer to each other by their service names across nodes without manual IP management.","diagram":null,"difficulty":"intermediate","tags":["Docker","Networking","Swarm","OverlayNetwork","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-15"],"channel":"docker-dca","subChannel":"networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:38:06.319Z","createdAt":"2026-01-12 18:38:06"},{"id":"docker-dca-networking-1768243086318-1","question":"You deploy a swarm service with replicas and publish port 8080 on the service. You want the port to be accessible on every node in the swarm and load balanced across replicas. Which publishing mode provides this behavior?","answer":"[{\"id\":\"a\",\"text\":\"host mode\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"ingress mode\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"local mode\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"bridge mode\",\"isCorrect\":false}]","explanation":"## Correct Answer\nIngress mode enables the Docker routing mesh, making the published port available on every node and balancing requests across task replicas.\n\n## Why Other Options Are Wrong\n- Host mode binds the port only on the specific node where the task runs.\n- Local mode is not a valid Swarm publishing mode.\n- Bridge mode does not apply to Swarm service publishing across nodes.\n\n## Key Concepts\n- Swarm published ports\n- Routing mesh (ingress)\n\n## Real-World Application\n- Ensures a single service port is consistently reachable from any node, simplifying client access patterns in a multi-node swarm.","diagram":null,"difficulty":"intermediate","tags":["Docker","Networking","Swarm","Ingress","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-15"],"channel":"docker-dca","subChannel":"networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:38:06.822Z","createdAt":"2026-01-12 18:38:07"},{"id":"docker-dca-networking-1768243086318-2","question":"To ensure web and backend services only communicate over a dedicated overlay network and are not reachable from containers on other networks, which approach should you take?","answer":"[{\"id\":\"a\",\"text\":\"Use host networking on all services\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a dedicated overlay network and attach only those services to it\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Create a separate bridge network on each host\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Expose ports publicly and rely on firewall rules\",\"isCorrect\":false}]","explanation":"## Correct Answer\nAttach the services to a dedicated overlay network so traffic between them remains isolated from other containers on different networks.\n\n## Why Other Options Are Wrong\n- Host networking bypasses Dockerâ€™s network isolation.\n- Separate bridge networks on each host do not provide cross-host isolation for these services.\n- Exposing ports publicly undermines container-level isolation and requires external firewall management.\n\n## Key Concepts\n- Overlay network isolation\n- Service-to-service communication scope\n\n## Real-World Application\n- isolates microservices in a swarm to minimize blast radius and reduce unintended cross-network access.","diagram":null,"difficulty":"intermediate","tags":["Docker","Networking","Swarm","OverlayNetwork","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-15"],"channel":"docker-dca","subChannel":"networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:38:07.310Z","createdAt":"2026-01-12 18:38:07"},{"id":"docker-dca-networking-1768243086318-3","question":"If containers on one swarm node cannot reach containers on another node over the overlay network, what is the most likely cause?","answer":"[{\"id\":\"a\",\"text\":\"The overlay network is not created on the second node\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"The swarm is not using an overlay network for inter-node communication\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"The containers are attached to the wrong Docker network type on the host\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"The inter-node routing mesh has been disabled globally\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe overlay network must be created and available on all swarm nodes; missing its creation on the second node prevents cross-node communication.\n\n## Why Other Options Are Wrong\n- Swarm can use an overlay for inter-node comms, so B is not the root cause.\n- Attaching containers to the wrong network type is less likely if the overlay was intended; it would more typically fail at service creation time.\n- The routing mesh is enabled by default when using overlays; global disable is not a common, supported setting.\n\n## Key Concepts\n- Overlay network scope across nodes\n- Swarm network consistency across agents\n\n## Real-World Application\n- When expanding a swarm cluster, ensure all nodes have identical overlay networks created and joined.","diagram":null,"difficulty":"intermediate","tags":["Docker","Networking","Swarm","OverlayNetwork","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-15"],"channel":"docker-dca","subChannel":"networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:38:07.470Z","createdAt":"2026-01-12 18:38:07"},{"id":"docker-dca-networking-1768243086318-4","question":"To secure inter-node traffic on a Docker Swarm overlay network, which feature should you enable?","answer":"[{\"id\":\"a\",\"text\":\"Use TLS certificates on the container apps\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Enable overlay network encryption by setting --opt encrypted on the overlay network\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use a VPN between nodes\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use iptables rules to block inbound traffic\",\"isCorrect\":false}]","explanation":"## Correct Answer\nEnable overlay network encryption (IPsec) by setting --opt encrypted on the overlay network to protect inter-node traffic.\n\n## Why Other Options Are Wrong\n- TLS for container apps does not automatically secure inter-node overlay traffic.\n- A VPN between nodes is possible but not the built-in overlay encryption feature.\n- General iptables restrictions do not selectively encrypt overlay traffic.\n\n## Key Concepts\n- Overlay network encryption (IPsec)\n- Swarm security features\n\n## Real-World Application\n- Helps meet security requirements for multi-host container communications without additional tunnel infraestructura.","diagram":null,"difficulty":"intermediate","tags":["Docker","Networking","Swarm","OverlayNetwork","Kubernetes","AWS","Terraform","certification-mcq","domain-weight-15"],"channel":"docker-dca","subChannel":"networking","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T18:38:07.632Z","createdAt":"2026-01-12 18:38:07"},{"id":"docker-dca-orchestration-1768169949801-0","question":"In Docker Swarm, which approach best achieves a rolling update with at most two updated tasks at a time and a ten-second delay between updates, while minimizing downtime?","answer":"[{\"id\":\"a\",\"text\":\"docker service update --update-parallelism 2 --update-delay 10s <service>\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"docker stack deploy --compose-file docker-compose.yaml <stack>\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"docker service update --update-parallelism 3 --update-delay 10s <service>\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"docker service update --update-order start-first --update-parallelism 2 --update-delay 10s <service>\",\"isCorrect\":true}]","explanation":"## Correct Answer\n**Option D**\n\nThe best approach explicitly sets the update order to start-first, ensuring new tasks come up before old ones are stopped, which minimizes downtime. It also enforces a maximum of two simultaneous updates and a 10s delay between updates via --update-parallelism 2 and --update-delay 10s.\n\n```javascript\n# Example (illustrative only):\ndocker service update --update-order start-first --update-parallelism 2 --update-delay 10s my_web_service\n```\n\n## Why Other Options Are Wrong\n- A: Lacks an explicit update-order, so it may follow the default stop-first behavior which can increase downtime.\n- B: Deploys a new stack rather than performing an in-place update of an existing service.\n- C: Uses a higher parallelism (3), violating the requirement of at most two concurrent updates.\n\n## Key Concepts\n- docker service update options: --update-parallelism, --update-delay, --update-order\n- Update order semantics: start-first minimizes downtime; stop-first is safer but can cause longer downtime\n\n## Real-World Application\n- Precise rollout controls in production Swarm clusters reduce risk during deployments.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","Terraform","Orchestration","certification-mcq","domain-weight-25"],"channel":"docker-dca","subChannel":"orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:19:09.803Z","createdAt":"2026-01-11 22:19:10"},{"id":"docker-dca-orchestration-1768169949801-1","question":"You want to securely provide a database password to all replicas of a service in a Swarm cluster. Which approach is correct?","answer":"[{\"id\":\"a\",\"text\":\"Pass the password as an environment variable in the service spec\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Create a Docker secret named db_password and attach it to the service; read from /run/secrets/db_password\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Store the password in a mounted file on the host and read it from the container\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a Kubernetes ConfigMap to inject the password\",\"isCorrect\":false}]","explanation":"## Correct Answer\n**Option B**\n\nDocker secrets provide encrypted storage and are mounted into containers at /run/secrets/<secret_name>, and only to the services that are granted the secret. This avoids plaintext environment variables and keeps credentials out of image layers.\n\n```bash\n# Example conceptually (secret creation and attach):\ndocker secret create db_password password.txt\ndocker service update --secret-add db_password my_db_service\n```\n\n## Why Other Options Are Wrong\n- A: Environment variables can leak via process listing and are stored in container metadata and image layers.\n- C: Host-mounted files are not managed by Swarm and can be less secure and harder to rotate.\n- D: Configs are for non-secret data; they are not ideal for sensitive credentials.\n\n## Key Concepts\n- Docker Secrets\n- /run/secrets/<name> access path\n- Service-level access control for secrets\n\n## Real-World Application\n- Centralized secret management for database credentials in a Swarm cluster, enabling secure rotation and restricted access.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","Terraform","Orchestration","certification-mcq","domain-weight-25"],"channel":"docker-dca","subChannel":"orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:19:10.273Z","createdAt":"2026-01-11 22:19:10"},{"id":"docker-dca-orchestration-1768169949801-2","question":"Which statement best reflects autoscaling capabilities in Docker Swarm for a service based on CPU utilization?","answer":"[{\"id\":\"a\",\"text\":\"Docker Swarm provides built-in autoscaling based on CPU via an HPA-like feature\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Autoscaling can be achieved automatically by Swarm when CPU usage spikes\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"There is no built-in autoscaling in Swarm; external tooling or custom scripts are required to adjust replicas based on metrics\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Swarm automatically scales across nodes when there is a single failed node\",\"isCorrect\":false}]","explanation":"## Correct Answer\n**Option C**\n\nDocker Swarm does not include built-in autoscaling based on metrics. To auto-scale, you typically integrate external monitoring/automation (e.g., custom scripts, external schedulers) that adjust the replica count via the Docker API or CLI.\n\n## Why Other Options Are Wrong\n- A: Swarm lacks an HPA-like autoscaler comparable to Kubernetes.\n- B: Swarm does not automatically scale purely on CPU without external tooling.\n- D: Swarm does not auto-scale on node failures by default.\n\n## Key Concepts\n- Lack of native autoscaling in Swarm\n- External autoscaling approaches (external monitors, scripts, API calls)\n\n## Real-World Application\n- Implementing a monitoring-based scaler to maintain desired replica counts in response to load in a Swarm cluster.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","Terraform","Orchestration","certification-mcq","domain-weight-25"],"channel":"docker-dca","subChannel":"orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-11T22:19:10.736Z","createdAt":"2026-01-11 22:19:10"},{"id":"docker-dca-orchestration-1768289041919-0","question":"In a Docker Swarm stack, you want to roll out a new image with a limited number of tasks updated at a time and ensure the updated tasks are healthy before proceeding to update more?","answer":"[{\"id\":\"a\",\"text\":\"update_config with order stop-first\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"update_config with parallelism and delay\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"placement constraints on running replicas\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"restart_policy with condition on failure\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B because update_config with parallelism and delay staggers updates and allows health monitoring to validate before proceeding.\n\n## Why Other Options Are Wrong\n- Option A: update_config with order stop-first would stop old tasks first and does not provide staged, health-verified updates.\n- Option C: Placement constraints do not influence how service updates are rolled out.\n- Option D: Restart policy is about container restarts on failure, not update choreography.\n\n## Key Concepts\n- Docker Swarm service update\n- update_config parallelism\n- health monitoring during updates\n- rolling updates\n\n## Real-World Application\nIn production, configure parallelism and delay to limit blast radius during updates and tie in health checks to automatically roll back if new tasks fail.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","Docker Swarm","AWS","EKS","Terraform","certification-mcq","domain-weight-25"],"channel":"docker-dca","subChannel":"orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:24:01.922Z","createdAt":"2026-01-13 07:24:02"},{"id":"docker-dca-orchestration-1768289041919-1","question":"In a Kubernetes cluster, you need to guarantee that the exact container image used in deployments remains immutable across environments. Which approach ensures this immutability?","answer":"[{\"id\":\"a\",\"text\":\"Use image tags like latest\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use image digest in PodSpec image field\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Use imagePullPolicy Always\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use Helm chart version numbers as tag\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B because using the image digest pins the exact image hash, ensuring immutability across environments.\n\n## Why Other Options Are Wrong\n- Option A: latest is mutable and can resolve to different images over time.\n- Option C: imagePullPolicy Always controls when to pull, not immutability.\n- Option D: Helm version numbers as a tag do not guarantee the underlying image hash remains the same.\n\n## Key Concepts\n- Image digest immutability\n- Kubernetes PodSpec\n- reproducible deployments\n\n## Real-World Application\nWhen promoting builds across dev/stage/prod, pin images by digest to guarantee identical runtimes and reproducible environments.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","Docker Swarm","AWS","EKS","Terraform","certification-mcq","domain-weight-25"],"channel":"docker-dca","subChannel":"orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:24:02.504Z","createdAt":"2026-01-13 07:24:02"},{"id":"docker-dca-orchestration-1768289041919-2","question":"Your Docker Swarm service requires database credentials to be kept secure. Which method provides secret management that the service can access as a mounted file or environment variable?","answer":"[{\"id\":\"a\",\"text\":\"Put credentials in environment variables in the service spec\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Docker Secrets and reference them in the service spec\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Store credentials on a ConfigMap in Kubernetes\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a host-mounted file with plaintext credentials\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B because Docker Secrets provide a secure, encrypted store mounted into containers at runtime without embedding them in environment variables or plaintext files.\n\n## Why Other Options Are Wrong\n- Option A: Environment variables expose secrets in process lists and environment dumps.\n- Option C: Kubernetes ConfigMap is not Docker Secrets and is not appropriate for Swarm secrets.\n- Option D: Host-mounted plaintext credentials are insecure and bypass secret management.\n\n## Key Concepts\n- Docker Secrets\n- secret isolation\n- secure runtime injection\n\n## Real-World Application\nUse Docker Secrets to store DB credentials in Swarm and mount them into containers, granting least-privilege access without exposure.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","Docker Swarm","AWS","EKS","Terraform","certification-mcq","domain-weight-25"],"channel":"docker-dca","subChannel":"orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:24:03.025Z","createdAt":"2026-01-13 07:24:03"},{"id":"docker-dca-orchestration-1768289041919-3","question":"To ensure an even distribution of five replicas across a set of swarm nodes, which Swarm feature should you configure?","answer":"[{\"id\":\"a\",\"text\":\"resources with limits and reservations\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"placement preferences to spread by node.hostname\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"update_config with parallelism\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"constraints to limit to a single node\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is B because placement preferences to spread by node.hostname distribute tasks evenly across nodes.\n\n## Why Other Options Are Wrong\n- Option A: Resource reservations control resource quotas but not distribution across nodes.\n- Option C: Update configurations affect rolling updates, not task placement.\n- Option D: Limiting to a single node defeats distribution and fault tolerance.\n\n## Key Concepts\n- Swarm placement preferences\n- spread strategy\n- replica distribution\n\n## Real-World Application\nConfigure spread placement in production to balance load and improve resilience across the cluster.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","Docker Swarm","AWS","EKS","Terraform","certification-mcq","domain-weight-25"],"channel":"docker-dca","subChannel":"orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:24:03.208Z","createdAt":"2026-01-13 07:24:03"},{"id":"docker-dca-orchestration-1768289041919-4","question":"Which tool is used to convert a Docker Compose file into Kubernetes manifests to facilitate migrating from Swarm to Kubernetes?","answer":"[{\"id\":\"a\",\"text\":\"Kompose\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"kubectl\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"docker stack deploy\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Helm\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct answer is A because Kompose translates docker-compose files into Kubernetes manifests to aid migration.\n\n## Why Other Options Are Wrong\n- Option B: kubectl applies manifests but does not perform format conversion from Compose.\n- Option C: docker stack deploy deploys Swarm stacks, not conversion.\n- Option D: Helm installs charts, not convert docker-compose to Kubernetes.\n\n## Key Concepts\n- Docker Compose to Kubernetes conversion\n- Kompose tool\n- migration strategy\n\n## Real-World Application\nUse Kompose during migration planning to accelerate moving from Swarm to Kubernetes with minimal re-implementation.","diagram":null,"difficulty":"intermediate","tags":["Docker","Kubernetes","Docker Swarm","AWS","EKS","Terraform","certification-mcq","domain-weight-25"],"channel":"docker-dca","subChannel":"orchestration","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T07:24:03.390Z","createdAt":"2026-01-13 07:24:03"},{"id":"docker-dca-security-1768253080331-0","question":"A development team wants to ensure that only signed container images are deployed to a Docker Swarm cluster. Which action will enforce image signing by default on the client side?","answer":"[{\"id\":\"a\",\"text\":\"Enable Docker Content Trust by setting DOCKER_CONTENT_TRUST=1 on all clients and ensure the registry supports Notary signing.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Sign images with docker trust sign and configure clients to skip verification using DOCKER_CONTENT_TRUST=0.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Require TLS on the registry and sign images, but do not enable image signature verification on clients.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely on tagging images with latest and assume registry will enforce trust automatically.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Enable Docker Content Trust by setting DOCKER_CONTENT_TRUST=1 on all clients and ensure the registry supports Notary signing.\n\n## Why Other Options Are Wrong\n- B: Verification must not be disabled; DOCKER_CONTENT_TRUST=0 disables trust checks.\n- C: TLS on the registry is good practice but does not enforce client-side signature verification.\n- D: Using the latest tag provides no guarantee of signing or trust.\n\n## Key Concepts\n- Docker Content Trust\n- Image signing with Notary\n- Registry signing support\n\n## Real-World Application\nUsed in CI/CD pipelines to prevent deployment of unsigned images, reducing supply-chain risk.\n","diagram":null,"difficulty":"intermediate","tags":["Docker","Notary","AWS_ECR","Kubernetes","Security","certification-mcq","domain-weight-15"],"channel":"docker-dca","subChannel":"security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:24:40.332Z","createdAt":"2026-01-12 21:24:40"},{"id":"docker-dca-security-1768253080331-1","question":"In a Linux host running multiple Docker containers, an attacker could potentially break out from a container to the host. Which measure is most effective at reducing this risk without impacting application functionality?","answer":"[{\"id\":\"a\",\"text\":\"Enable user namespaces so that container root maps to a non-root host UID/GID.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Run containers with the --privileged flag to simplify operations.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Disable seccomp profiles entirely to maximize compatibility.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Mount /proc with read-write permissions to allow deeper host access.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Enable user namespaces so that container root maps to a non-root host UID/GID.\n\n## Why Other Options Are Wrong\n- B: --privileged elevates privileges and increases the risk of host compromise.\n- C: Disabling seccomp weakens syscall filtering and reduces defense in depth.\n- D: Read-write /proc access broadens host visibility and can enable host-level attacks.\n\n## Key Concepts\n- User namespaces isolation\n- Privilege minimization\n- Container security posture\n\n## Real-World Application\nHelps isolate containers in multi-tenant environments and limits blast radius if a container is compromised.\n","diagram":null,"difficulty":"intermediate","tags":["Docker","UserNamespaces","Kubernetes","Security","certification-mcq","domain-weight-15"],"channel":"docker-dca","subChannel":"security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:24:40.856Z","createdAt":"2026-01-12 21:24:41"},{"id":"docker-dca-security-1768253080331-2","question":"A CI/CD pipeline builds a Docker image from a base image with multiple critical CVEs detected by your scanner. What is the most reliable remediation before promoting the image to production?","answer":"[{\"id\":\"a\",\"text\":\"Rebuild the image using an updated base image, re-scan, and push to registry only if no critical CVEs remain.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Ignore the CVEs if they are not exploitable in your environment and proceed.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Assume the CVEs will be patched automatically in production and proceed.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Patch running containers in-place without rebuilding the image.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Rebuild the image using an updated base image, re-scan, and push to registry only if no critical CVEs remain.\n\n## Why Other Options Are Wrong\n- B: Ignoring CVEs risks exploitation; not acceptable for production security.\n- C: Automatic patching in production is not guaranteed and can introduce instability.\n- D: Patching running containers does not fix the underlying vulnerable image layers.\n\n## Key Concepts\n- Vulnerability scanning\n- Base image updates\n- Immutable images\n\n## Real-World Application\nEnsures baseline security posture before deployment by validating clean images through the build pipeline.\n","diagram":null,"difficulty":"intermediate","tags":["Docker","Vulnerability_Scanning","AWS_ECR","Kubernetes","certification-mcq","domain-weight-15"],"channel":"docker-dca","subChannel":"security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:24:41.361Z","createdAt":"2026-01-12 21:24:41"},{"id":"docker-dca-security-1768253080331-3","question":"In a Docker Swarm environment, secrets management is critical. Which approach provides the most secure and scalable method for distributing sensitive data like database passwords to services?","answer":"[{\"id\":\"a\",\"text\":\"Use environment variables injected at runtime by the orchestrator.\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Use Docker secrets to mount sensitive data as restricted files inside containers.\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Store secrets in files baked into the image at build time.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Fetch secrets from a public cloud bucket at startup.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nB. Use Docker secrets to mount sensitive data as restricted files inside containers.\n\n## Why Other Options Are Wrong\n- A: Environment variables can be exposed in process listings and logs.\n- C: Storing secrets in image layers risks leakage and requires rebuilding to rotate.\n- D: Public cloud buckets introduce access controls and rotation gaps; not inherently secret-management friendly.\n\n## Key Concepts\n- Docker secrets (Swarm)\n- Least privilege data exposure\n- Secrets rotation\n\n## Real-World Application\nAllows secure distribution and automatic rotation of credentials without leaking them in images or environment variables.\n","diagram":null,"difficulty":"intermediate","tags":["Docker","Secrets","Kubernetes","HashiCorp_Vault","certification-mcq","domain-weight-15"],"channel":"docker-dca","subChannel":"security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:24:41.540Z","createdAt":"2026-01-12 21:24:41"},{"id":"docker-dca-security-1768253080331-4","question":"A workload runs containers that mount the host's Docker daemon socket for orchestration tasks. Which practice best reduces the risk of a host compromise while preserving required functionality?","answer":"[{\"id\":\"a\",\"text\":\"Avoid mounting the host's docker.sock into containers; instead, use a restricted API or orchestration pattern with tightly-scoped permissions.\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Always run containers with the --privileged flag to ensure operations succeed.\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Disable AppArmor and SELinux to improve compatibility.\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Expose the daemon over HTTP without TLS for speed.\",\"isCorrect\":false}]","explanation":"## Correct Answer\nA. Avoid mounting the host's docker.sock into containers; instead, use a restricted API or orchestration pattern with tightly-scoped permissions.\n\n## Why Other Options Are Wrong\n- B: --privileged grants broad host access and greatly increases risk.\n- C: Disabling AppArmor/SELinux removes important host protections.\n- D: Unencrypted daemon exposure creates a critical remote attack surface.\n\n## Key Concepts\n- Docker.sock danger\n- Least-privilege API access\n- Secure orchestration patterns\n\n## Real-World Application\nPrevents lateral movement from compromised containers to the host while enabling necessary orchestration tasks.\n","diagram":null,"difficulty":"intermediate","tags":["Docker","Container_Security","Kubernetes","AWS_ECS","certification-mcq","domain-weight-15"],"channel":"docker-dca","subChannel":"security","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:24:41.720Z","createdAt":"2026-01-12 21:24:41"},{"id":"docker-dca-storage-volumes-1768274851631-0","question":"You run a database inside a container and need data to persist beyond the container lifecycle and be easy to back up and move between hosts. Which storage approach should you use?","answer":"[{\"id\":\"a\",\"text\":\"Bind mount to a host directory, e.g. /var/lib/dbdata\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Anonymous Docker volume created implicitly by Docker\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Named Docker volume managed by Docker\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Tmpfs mounted in memory\",\"isCorrect\":false}]","explanation":"## Correct Answer\nThe correct approach is a named Docker volume managed by Docker.\n\n## Why Other Options Are Wrong\n- Bind mount to a host directory ties data to the host path, reducing portability and standard backup practices across multiple hosts.\n- An anonymous Docker volume exists but lacks a stable, referenceable name for consistent backup/migration workflows.\n- Tmpfs stores data in memory and is not persistent across container restarts or host reboots.\n\n## Key Concepts\n- Named volumes provide durable storage decoupled from containers.\n- Named volumes are easier to reference in backup and migration processes.\n\n## Real-World Application\nFor stateful services like databases, use named volumes to enable reliable backups and seamless data migration between environments.","diagram":null,"difficulty":"intermediate","tags":["Docker","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-10"],"channel":"docker-dca","subChannel":"storage-volumes","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:27:31.633Z","createdAt":"2026-01-13 03:27:32"},{"id":"docker-dca-storage-volumes-1768274851631-1","question":"To share persistent data between containers running on different Docker hosts in a cluster, which storage approach is appropriate?","answer":"[{\"id\":\"a\",\"text\":\"Bind mounts on each host pointing to the same path\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"A local Docker volume on each node\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"A volume driver plugin that supports multi-host sharing (e.g., NFS or GlusterFS)\",\"isCorrect\":true},{\"id\":\"d\",\"text\":\"Tmpfs volumes mounted on all containers\",\"isCorrect\":false}]","explanation":"## Correct Answer\nUse a volume driver plugin that supports multi-host sharing (such as NFS or GlusterFS) to enable data access across nodes.\n\n## Why Other Options Are Wrong\n- Bind mounts rely on host path consistency across hosts, which is not practical for multi-host sharing.\n- Local volumes are node-local and not designed for cross-node sharing.\n- Tmpfs volumes are in-memory and do not persist across reboots or across hosts.\n\n## Key Concepts\n- Multi-host shared storage requires a driver that supports remote access.\n- External storage like NFS/GlusterFS can provide consistent data access across nodes.\n\n## Real-World Application\nIn a Docker Swarm or Kubernetes cluster, configure a shared volume via an appropriate driver so services on different nodes can read/write the same data.","diagram":null,"difficulty":"intermediate","tags":["Docker","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-10"],"channel":"docker-dca","subChannel":"storage-volumes","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:27:32.286Z","createdAt":"2026-01-13 03:27:32"},{"id":"docker-dca-storage-volumes-1768274851631-2","question":"You want to back up data from a Docker volume to AWS S3 regularly. Which approach provides a robust, container-managed backup workflow?","answer":"[{\"id\":\"a\",\"text\":\"Use docker cp from the container to S3 manually\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Run a backup container that mounts the target volume and streams data to S3\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Back up the host directory with a cron job on the host only\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Install a S3-based volume plugin that handles snapshots automatically\",\"isCorrect\":false}]","explanation":"## Correct Answer\nRunning a backup container that mounts the target volume and streams data to S3 provides a repeatable, portable backup process.\n\n## Why Other Options Are Wrong\n- docker cp is ad hoc and not ideal for automated, scheduled backups.\n- Host-based cron backup ties the process to a single host and bypasses container boundaries.\n- An S3-based volume plugin may introduce complexity and may not be universally available or supported.\n\n## Key Concepts\n- Containerized backup workflows enable consistent, automated backups.\n- Mounting the same volume in a backup container ensures data consistency during backups.\n\n## Real-World Application\nDeploy a scheduled backup container that mounts the service data volume and streams the archive to AWS S3 for point-in-time restore capabilities.","diagram":null,"difficulty":"intermediate","tags":["Docker","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-10"],"channel":"docker-dca","subChannel":"storage-volumes","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:27:32.806Z","createdAt":"2026-01-13 03:27:32"},{"id":"docker-dca-storage-volumes-1768274851631-3","question":"You need to allow two containers to access the same volume but ensure that only one can write while the other can read. How would you implement this?","answer":"[{\"id\":\"a\",\"text\":\"Create two separate volumes and copy data as needed\",\"isCorrect\":false},{\"id\":\"b\",\"text\":\"Mount the volume in the reader container as read-only while the writer container uses a writable mount\",\"isCorrect\":true},{\"id\":\"c\",\"text\":\"Configure Docker to enforce per-container access controls on volumes\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Rely on host filesystem permissions alone to restrict writes\",\"isCorrect\":false}]","explanation":"## Correct Answer\nMounting the volume into the reader container as read-only while the writer container uses a writable mount enforces the desired access pattern at runtime.\n\n## Why Other Options Are Wrong\n- Creating two separate volumes defeats the requirement to share data.\n- Docker does not provide per-container volume access controls beyond mount options.\n- Host permissions alone are insufficient and can be bypassed by container processes with sufficient privileges.\n\n## Key Concepts\n- Read-only mounts restrict write access at runtime.\n- Proper mount configuration is essential for controlled data sharing.\n\n## Real-World Application\nIn data sharing scenarios where one service reads reference data while another updates it, use ro mounts for readers and rw mounts for writers to prevent data corruption.","diagram":null,"difficulty":"intermediate","tags":["Docker","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-10"],"channel":"docker-dca","subChannel":"storage-volumes","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:27:32.987Z","createdAt":"2026-01-13 03:27:33"},{"id":"docker-dca-storage-volumes-1768274851631-4","question":"Which approach yields the best I/O performance for a database container on a Linux host when you must choose between bind mounts and Docker named volumes with the default local driver?","answer":"[{\"id\":\"a\",\"text\":\"Bind mount the database directory to a high-performance SSD path on the host\",\"isCorrect\":true},{\"id\":\"b\",\"text\":\"Use a named volume with the default local driver; there is no difference in performance\",\"isCorrect\":false},{\"id\":\"c\",\"text\":\"Store data in a tmpfs volume for faster writes\",\"isCorrect\":false},{\"id\":\"d\",\"text\":\"Use a network storage volume to maximize I/O throughput\",\"isCorrect\":false}]","explanation":"## Correct Answer\nBind mounting the database directory to a high-performance SSD path on the host typically yields the best I/O performance for database workloads on Linux due to reduced abstraction and direct host filesystem access.\n\n## Why Other Options Are Wrong\n- Named volumes with the default local driver can introduce a small overhead and may not outperform direct bind mounts for local I/O.\n- Tmpfs uses RAM and is volatile with size limitations.\n- Network storage adds latency and can reduce I/O throughput for latency-sensitive databases.\n\n## Key Concepts\n- Bind mounts give direct, lower-overhead access to host storage.\n- Volume abstractions can incur small performance penalties for high-IO workloads.\n\n## Real-World Application\nFor latency-sensitive databases running on a single Linux host, prefer a bind mount to an SSD-backed path to maximize throughput and reduce latency.","diagram":null,"difficulty":"intermediate","tags":["Docker","AWS","Kubernetes","Terraform","certification-mcq","domain-weight-10"],"channel":"docker-dca","subChannel":"storage-volumes","sourceUrl":null,"videos":null,"companies":null,"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:27:33.169Z","createdAt":"2026-01-13 03:27:33"}],"subChannels":["general","image-creation","installation-config","networking","orchestration","security","storage-volumes"],"companies":["Hashicorp","IBM","PayPal","Snowflake"],"stats":{"total":33,"beginner":1,"intermediate":31,"advanced":1,"newThisWeek":33}}