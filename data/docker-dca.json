{"questions":[{"id":"q-1392","question":"In a three-node Docker Swarm spanning two data centers, deploy a stateless API across an overlay network and ensure zero-downtime upgrades via canary, using update_config (start-first, parallelism 1). Outline the exact sequence of commands to init/join the swarm, create the overlay, deploy the service with update settings, and perform a canary upgrade with health checks. Include a minimal docker-compose snippet showing update_config?","answer":"Init swarm on node A: docker swarm init --advertise-addr <A>; join B/C: docker swarm join --token <tok> <A>:2377; overlay: docker network create -d overlay --attachable app-net; deploy: docker service","explanation":"## Why This Is Asked\nTests practical mastery of Swarm lifecycle, overlay networking, and zero-downtime upgrades in multi-datacenter setups.\n\n## Key Concepts\n- Docker Swarm init/join and multi-datacenter networking\n- Overlay networks and service deployment in Swarm\n- update_config for canary upgrades and health checks\n\n## Code Example\n```docker-compose\nversion: '3.8'\nservices:\n  api:\n    image: nginx:stable\n    networks:\n      - app-net\n    deploy:\n      replicas: 3\n      update_config:\n        parallelism: 1\n        delay: 10s\n        order: start-first\nnetworks:\n  app-net:\n    driver: overlay\n```\n\n## Follow-up Questions\n- How would you handle service traffic shifting for canary without external LB?\n- What are failure_action and monitor thresholds used for in update_config?","diagram":null,"difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Cloudflare","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T14:52:07.307Z","createdAt":"2026-01-13T14:52:07.307Z"},{"id":"q-1412","question":"Scenario: you’re building a beginner-friendly Docker Compose setup for a FastAPI microservice with PostgreSQL on a single host. Create Dockerfiles for the app and an init script, plus a docker-compose.yml with a named volume for Postgres data, healthchecks, and a startup script that waits for PostgreSQL on port 5432 before starting the app. Explain the exact files, commands, and deployment sequence?","answer":"I would dockerize the FastAPI app with Python 3.11, add a Postgres container, and provide a docker-compose.yml with services app and db, a named volume pgdata, environment vars, and healthchecks. Incl","explanation":"## Why This Is Asked\nTests practical Docker Compose skills: multi-service setup, data persistence, healthchecks, startup dependencies.\n\n## Key Concepts\n- Dockerfile basics, Python 3.11\n- Postgres container with pgdata volume\n- docker-compose networks/depends_on healthchecks\n- wait-for-it pattern for DB readiness\n\n## Code Example\n```yaml\n# docker-compose.yaml (excerpt)\nversion: '3.9'\nservices:\n  app:\n    build: ./app\n    ports:\n      - '8000:8000'\n    depends_on:\n      - db\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8000/health\"]\n      interval: 10s\n      retries: 5\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_PASSWORD: example\n    volumes:\n      - pgdata:/var/lib/postgresql/data\nvolumes:\n  pgdata:\n    name: pgdata\n```\n\n## Follow-up Questions\n- How would you add a migration step to initialize schemas?\n- How would you scale this with separate networks or secrets management?","diagram":"flowchart TD\nA[Developer writes app] --> B[Build Dockerfile]\nB --> C[Create docker-compose.yaml]\nC --> D[Start stack with docker compose up -d]\nD --> E[Healthchecks verify readiness]","difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","IBM","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T15:51:50.736Z","createdAt":"2026-01-13T15:51:50.737Z"},{"id":"q-1443","question":"In a 3-node Swarm across DC-A and DC-B, deploy a stateless API via overlay api-net with DC-affinity (2 replicas in DC-A and 1 in DC-B). Outline the exact CLI steps to init/join, create the overlay, and deploy two services with update_config (order: start-first, parallelism: 1). Describe a canary upgrade process that gradually updates replicas across DCs and validates with health checks. Include a minimal docker-compose snippet showing the two services, the overlay network, and update_config?","answer":"Init Swarm on DC-A; join DC-B; create overlay api-net; deploy two services: api-dcA with 2 replicas constrained to DC-A and api-dcB with 1 replica constrained to DC-B, both on api-net and update_confi","explanation":"## Why This Is Asked\nTests cross-DC orchestration, DC-aware placement, and canary upgrades using Docker Swarm update_config.\n\n## Key Concepts\n- Overlay networks spanning DCs\n- Placement constraints and spread/preference for DC distribution\n- update_config for staged upgrades\n- Canary rollout and health-check driven promotion\n\n## Code Example\n```javascript\nversion: \"3.8\"\nservices:\n  api-dcA:\n    image: myapi:latest\n    deploy:\n      replicas: 2\n      update_config:\n        order: start-first\n        parallelism: 1\n      placement:\n        constraints:\n          - node.labels.dc == DC-A\n      networks:\n        - api-net\n  api-dcB:\n    image: myapi:latest\n    deploy:\n      replicas: 1\n      update_config:\n        order: start-first\n        parallelism: 1\n      placement:\n        constraints:\n          - node.labels.dc == DC-B\n      networks:\n        - api-net\nnetworks:\n  api-net:\n```\n\n## Follow-up Questions\n- How would you monitor canary success across DCs and automate rollback if latency spikes? \n- How would you adapt this for a rolling upgrade with multiple versions concurrently?","diagram":"flowchart TD\n  A[Init Swarm in DC-A] --> B[Join DC-B]\n  B --> C[Create overlay api-net]\n  C --> D[Deploy two services with update_config]\n  D --> E[Canary rollout across DCs]","difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T17:34:28.800Z","createdAt":"2026-01-13T17:34:28.803Z"},{"id":"q-1530","question":"In a three-node Docker Swarm spanning two data centers, deploy a stateless API with TLS termination that uses Vault for dynamic TLS certificate rotation. Use Docker secrets to distribute certs, and implement a lightweight sidecar that refreshes certificates without dropping connections. Configure a canary-style rolling upgrade ensuring zero-downtime during cert rotations. Outline the exact steps: swarm init/join, overlay network creation, stack/deploy with secret handling, and the certificate rotation workflow with health checks?","answer":"Plan: initialize a 3-node Docker Swarm across two data centers, create an overlay network, deploy a stack with API and sidecar containers, mount Vault-issued certificates as Docker secrets and rotate them via the sidecar without container restarts, and perform a canary-style rolling upgrade ensuring zero-downtime during certificate rotations.","explanation":"## Why This Is Asked\nTests multi-datacenter Docker Swarm setup, TLS automation with HashiCorp Vault, Docker secrets management, and zero-downtime deployment strategies.\n\n## Key Concepts\n- Docker Swarm spanning multiple data centers\n- Docker secrets with external secret management (Vault)\n- Sidecar pattern for live certificate rotation\n- Update configuration with start-first for zero-downtime upgrades\n- Health checks and canary rollout strategies\n\n## Code Example\n```yaml\nversion: \"3.9\"\nservices:\n  api:\n    image: myorg/api:latest\n    ports:\n      - \"80:80\"\n    secrets:\n      - source: tls_cert\n        target: tls.crt\n    deploy:\n      update_config:\n        parallelism: 1\n        delay: 10s\n        failure_action: rollback\n        order: start-first\n      healthcheck:\n        test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:80/health\"]\n        interval: 30s\n        timeout: 10s\n        retries: 3\n  sidecar:\n    image: myorg/cert-rotator:latest\n    secrets:\n      - source: tls_cert\n        target: tls.crt\n      - source: vault_token\n        target: vault.token\n    deploy:\n      replicas: 1\nsecrets:\n  tls_cert:\n    external: true\n  vault_token:\n    external: true\n```","diagram":null,"difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T05:10:39.873Z","createdAt":"2026-01-13T20:46:17.744Z"},{"id":"q-1579","question":"Design a secret-rotation workflow for a Docker Swarm stack that uses Vault to rotate a TLS cert and a database credential with zero downtime. Use Swarm secrets and a rolling update (start-first). Outline exact steps: swarm init/join, overlay network, create secrets, deploy stack, Vault rotate trigger, service update commands. Include a minimal docker-compose snippet showing secret usage and update_config?","answer":"Plan: Vault issues new TLS certificate and database credential; create new Swarm secrets (e.g., api-tls-v2, db-cred-v2); perform service updates with `docker service update --secret-add` then `--secret-rm`, using update_config with start-first strategy for zero downtime.\n\n## Implementation Steps\n\n1. **Initialize Swarm cluster**\n   ```bash\n   docker swarm init --advertise-addr <MANAGER-IP>\n   docker swarm join --token <TOKEN> <MANAGER-IP>:2377\n   ```\n\n2. **Create overlay network**\n   ```bash\n   docker network create --driver overlay --attachable app-network\n   ```\n\n3. **Create initial secrets from Vault**\n   ```bash\n   vault kv get -field=cert secret/api/tls | docker secret create api-tls-v1 -\n   vault kv get -field=cred secret/api/db | docker secret create db-cred-v1 -\n   ```\n\n4. **Deploy stack with rolling update configuration**\n   ```yaml\n   version: '3.8'\n   services:\n     api:\n       image: myapp:latest\n       secrets:\n         - source: api-tls-v1\n           target: /app/tls.crt\n         - source: db-cred-v1\n           target: /app/db-cred\n       networks:\n         - app-network\n       update_config:\n         parallelism: 1\n         delay: 10s\n         order: start-first\n       healthcheck:\n           test: [\"CMD\", \"curl\", \"-f\", \"https://localhost:8443/health\"]\n           interval: 30s\n           timeout: 10s\n           retries: 3\n   ```\n\n5. **Vault rotation trigger**\n   - Vault generates new TLS certificate and database credential\n   - Automation script detects rotation event\n\n6. **Service update with new secrets**\n   ```bash\n   # Create new secret versions\n   vault kv get -field=cert secret/api/tls | docker secret create api-tls-v2 -\n   vault kv get -field=cred secret/api/db | docker secret create db-cred-v2 -\n   \n   # Update service to add new secrets\n   docker service update api \\\n     --secret-add source=api-tls-v2,target=/app/tls.crt \\\n     --secret-add source=db-cred-v2,target=/app/db-cred\n   \n   # Remove old secrets after health checks pass\n   docker service update api \\\n     --secret-rm api-tls-v1 \\\n     --secret-rm db-cred-v1\n   ```","explanation":"## Why This Is Asked\nDemonstrates practical secret lifecycle management in Docker Swarm, integrating Vault for automated rotation with zero-downtime upgrades and proper secret versioning.\n\n## Key Concepts\n- Swarm secrets versioning and dynamic reattachment\n- Vault-based rotation triggers and secret provisioning\n- Rolling updates with start-first strategy to avoid downtime\n- Health checks to confirm new secrets are loaded before removing old ones\n- Atomic secret updates using add-then-remove pattern\n\n## Code Example\n```javascript\n// Pseudo-automation: rotate Vault-derived Swarm secrets\nasync function rotateSecrets(serviceName, vaultPath) {\n  // Fetch new credentials from Vault\n  const newCreds = await vault.read(vaultPath);\n  \n  // Create new Swarm secrets\n  const newSecrets = await createSwarmSecrets(newCreds);\n  \n  // Update service with new secrets\n  await updateService(serviceName, newSecrets);\n  \n  // Verify health before cleanup\n  await verifyServiceHealth(serviceName);\n  \n  // Remove old secrets\n  await cleanupOldSecrets(serviceName);\n}\n```","diagram":null,"difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Databricks","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T05:56:17.721Z","createdAt":"2026-01-13T22:45:30.688Z"},{"id":"q-1628","question":"Scenario: In a 3-node Swarm across two DCs, deploy an API service behind Traefik with image signing enforcement (cosign/DOCKER_CONTENT_TRUST) and implement a canary upgrade to v2 with a 10% traffic split via a separate api-v2-canary service. Outline exact commands, Swarm update_config usage, and docker-compose/service definitions to achieve zero-downtime upgrade and safe rollback?","answer":"Enable Docker Content Trust and cosign signing on all nodes (DOCKER_CONTENT_TRUST=1; cosign sign). Deploy api-v1 and a canary api-v2-canary behind Traefik with a 10% canary route. Use update_config { ","explanation":"## Why This Is Asked\nEvaluates image provenance, canary sequencing, and Swarm upgrade semantics in a multi-datacenter setup.\n\n## Key Concepts\n- Image signing with cosign and DOCKER_CONTENT_TRUST\n- Swarm update_config for controlled upgrades\n- Traffic splitting via an in-cluster reverse proxy (Traefik)\n- Canary pattern and safe rollback in production\n\n## Code Example\n```javascript\n// illustrative docker-compose-like snippet (Traefik routing hints)\nservices:\n  api-v1:\n    image: registry.example.com/api:v1\n    deploy:\n      replicas: 4\n      labels:\n        - \"traefik.enable=true\"\n        - \"traefik.http.routers.api-v1.rule=Host(`api.example.com` )\"\n  api-v2-canary:\n    image: registry.example.com/api:v2-canary\n    deploy:\n      replicas: 1\n      labels:\n        - \"traefik.enable=true\"\n        - \"traefik.http.routers.api-v2-canary.rule=Host(`canary.api.example.com` )\"\n```\n\n## Follow-up Questions\n- How would key rotation and automatic signing verification be automated?\n- How would you measure canary health and decide promotion/rollback automatically?","diagram":null,"difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","IBM","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T04:16:19.227Z","createdAt":"2026-01-14T04:16:19.227Z"},{"id":"q-1653","question":"Scenario: On a single host, implement a beginner-friendly Docker Compose stack: a Node.js API that talks to Redis and a Fluent Bit logging service that reads logs from the API via a shared volume and forwards them to stdout. Provide a Dockerfile for the API, a docker-compose.yml with api, redis, fluent-bit, a logs volume, and healthchecks; explain the run steps and verification?","answer":"Create a docker-compose.yml with api, redis, and fluent-bit services on one host, plus a shared logs volume. The API writes to /var/log/app/app.log; Fluent Bit tails that file and forwards to stdout. ","explanation":"## Why This Is Asked\nTests practical docker-compose discipline: multi-service coordination, logging discipline, and health checks.\n\n## Key Concepts\n- docker-compose multi-service orchestration\n- log aggregation with Fluent Bit\n- log file sharing via volumes\n- minimal health checks for API service\n\n## Code Example\n```dockerfile\n# Dockerfile for API\nFROM node:18\nWORKDIR /app\nCOPY package*.json ./\nRUN npm install\nCOPY . .\nEXPOSE 3000\nCMD node server.js\n```\n```yaml\n# docker-compose.yml\nversion: '3.8'\nservices:\n  api:\n    build: ./api\n    depends_on:\n      - redis\n    environment:\n      - REDIS_URL=redis://redis:6379\n    healthcheck:\n      test: curl -f http://localhost:3000/health || exit 1\n      interval: 10s\n      timeout: 5s\n      retries: 5\n    volumes:\n      - ./logs:/var/log/app\n  redis:\n    image: redis:7\n  fluent-bit:\n    image: fluent/fluent-bit:1.9\n    volumes:\n      - ./logs:/var/log/app\n      - ./fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf\n    depends_on:\n      - api\n```\n\n## Follow-up Questions\n- How would you extend this to ship logs to an external system\n- How would you ensure log retention and rotate the log files","diagram":null,"difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Instacart","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T05:35:42.804Z","createdAt":"2026-01-14T05:35:42.804Z"},{"id":"q-1688","question":"Beginner-level: design a local docker-compose stack for a Node.js API that uses Redis as a cache. Provide a Dockerfile for the API, a startup script that waits for Redis to be reachable on 6379 before starting, and a docker-compose.yml with healthchecks for both services on a shared network. Include exact file contents or minimal snippets, the commands to build and run, and how to validate a cache-hit endpoint?","answer":"Build a Node.js API image with a Dockerfile, and a start script wait-for-redis.sh that pings redis:6379 until ready, then runs node index.js. In docker-compose.yml, define services api and redis on a ","explanation":"Why This Is Asked\nTests ability to coordinate container startup and readiness in Compose using a startup gate and healthchecks.\n\nKey Concepts\n- Dockerfile for Node.js\n- startup gating with a wait script\n- docker-compose healthchecks\n- Docker networking and service discovery\n\nCode Example\n```javascript\n# Dockerfile\nFROM node:18-alpine\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci --only=production\nCOPY . .\nEXPOSE 3000\n# Start directly with node (no quotes) for simplicity\nCMD node index.js\n```\n```javascript\n# wait-for-redis.sh (simplified)\nuntil nc -z  redis 6379; do\n  sleep 0.2\ndone\nexec \"$@\"\n```\n```javascript\n# docker-compose.yml\nversion: '3.9'\nservices:\n  api:\n    build: .\n    ports:\n      - 3000:3000\n    depends_on:\n      - redis\n    healthcheck:\n      test: [\"CMD-SHELL\", \"curl -f http://localhost:3000/health\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n    command: [\"sh\",\"/wait-for-redis.sh\",\"redis\",\"node\",\"index.js\"]\n  redis:\n    image: redis:7-alpine\n    ports:\n      - 6379:6379\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"ping\"]\n      interval: 10s\n      timeout: 5s\n      retries: 3\nnetworks:\n  default:\n    name: appnet\n```\n\nFollow-up Questions\n- How would you adapt for an optional Redis cache with a fallback?\n- How would you test failure of Redis and verify API behavior?","diagram":null,"difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","MongoDB","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T06:58:39.175Z","createdAt":"2026-01-14T06:58:39.175Z"},{"id":"q-1760","question":"In a three-node Docker Swarm spanning two data centers, introduce a new internal auth service that all APIs depend on. Roll it out with zero downtime and a canary, using update_config (start-first, parallelism 1), Docker Secrets, and a routing shim. Provide exact commands to init/join the swarm, create the overlay, deploy the stack, add the secret, and perform the canary upgrade with health checks?","answer":"Two-stage canary: deploy auth-v2 with 1 replica and route 5% of traffic to it; ensure health checks pass; then roll the main stack with update_config start-first, parallelism 1. Commands: docker swarm","explanation":"## Why This Is Asked\nRealistic multi-datacenter upgrades require controlled rollouts with canary, health checks, and secrets. This tests operational discipline and deep Docker Swarm knowledge.\n\n## Key Concepts\n- Swarm upgrades with update_config; - Canaries across regions; - Secrets management; - Overlay networking; - Health checks and rollback.\n\n## Code Example\n```javascript\nversion: '3.8'\nservices:\n  auth:\n    image: myrepo/auth:canary\n    secrets:\n      - source: auth-secret\n        target: /etc/auth/secret.json\n    deploy:\n      update_config:\n        parallelism: 1\n        order: start-first\n        delay: 10s\n      restart_policy:\n        condition: on-failure\nsecrets:\n  auth-secret:\n    external: true\n```\n\n## Follow-up Questions\n- How would you automate canary traffic routing without a reverse proxy? \n- How would you verify no active sessions are dropped during promotion?","diagram":null,"difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T09:46:53.036Z","createdAt":"2026-01-14T09:46:53.036Z"},{"id":"q-1786","question":"In a 3-node Docker Swarm spanning two data centers, deploy a GPU-accelerated model-serving API (TorchServe) using the NVIDIA runtime. Expose it behind an internal overlay network and a simple LB. Ensure zero-downtime upgrades with canary traffic shifts and a pre-warm sidecar to warm the new replica without serving traffic. Outline the steps for swarm init/join, overlay creation, service spec with GPU constraints, secret handling, and a separate migration container if needed. Include a minimal docker-compose snippet showing update_config (start-first, parallelism 1)?","answer":"Outline a GPU-enabled TorchServe deployment on a 3-node Swarm across 2 DCs. Use NVIDIA runtime and a service with GPU constraint (nvidia.com/gpu=1). Implement canary upgrades: deploy 1 new replica, ru","explanation":"## Why This Is Asked\nTests ability to orchestrate GPU-enabled deployments across DCs, implement safe upgrades, and coordinate sidecar pre-warming with a migration task.\n\n## Key Concepts\n- GPU scheduling in Swarm with NVIDIA runtime\n- Overlay networks across data centers\n- Canary rollout using update_config (start-first, single-threaded)\n- Sidecar pre-warm pattern for zero-downtime\n- Migration container for schema/model updates\n\n## Code Example\n```yaml\nversion: '3.8'\nservices:\n  model:\n    image: myorg/torchserve:latest\n    deploy:\n      replicas: 3\n      update_config:\n        parallelism: 1\n        order: start-first\n    environment:\n      - NVIDIA_VISIBLE_DEVICES=all\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - capabilities:\n                -gpu\n                  count: 1\n```\n\n## Follow-up Questions\n- How would you monitor GPU utilization across nodes?\n- How would you handle model/version rollback in a canary rollout?","diagram":"flowchart TD\n  A[Init Swarm] --> B[Create Overlay Network]\n  B --> C[Deploy TorchServe with NVIDIA runtime]\n  C --> D[Canary Upgrade: 1 new replica]\n  D --> E[Health Checks & Traffic Shift]\n  E --> F[Promote & Cleanup]","difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T10:48:28.317Z","createdAt":"2026-01-14T10:48:28.317Z"},{"id":"q-1832","question":"Design and implement a zero-downtime upgrade for a 4-node Docker Swarm across two data centers hosting a stateful Redis queue and a stateless worker service. The worker consumes messages encoded in a new proto format; the cluster uses TLS mutual authentication with Vault for cert rotation and Docker Secrets for credentials. Outline exact upgrade steps, a 10% canary rollout, health checks, and a rollback plan, and include a minimal docker-compose snippet showing update_config(order: start-first, parallelism: 1)?","answer":"Use a 4-node Swarm across two DCs, with a 10% canary of the worker upgraded to the new proto-format, while 90% stay on the legacy image. Deploy update_config: order: start-first; parallelism: 1. Route","explanation":"## Why This Is Asked\nTests real-world upgrade workflows in a distributed Swarm with cross-DC considerations, TLS cert rotation, and in-flight data.\n\n## Key Concepts\n- Swarm update_config with start-first and parallelism\n- Canary rollouts and rollback triggers\n- TLS mutual authentication, Vault cert rotation, Docker secrets\n- In-flight data migration and zero-downtime guarantees\n\n## Code Example\n```yaml\nversion: '3.8'\nservices:\n  worker:\n    image: myorg/worker:upgrade\n    deploy:\n      replicas: 4\n      update_config:\n        order: start-first\n        parallelism: 1\n    secrets:\n      - tls_cert\n      - db_creds\n    environment:\n      - PROTO_VERSION=v2\n```\n\n## Follow-up Questions\n- How would you validate canary health beyond a simple Liveness probe?\n- What rollback automation would you add to rapidly revert if metrics deteriorate?","diagram":null,"difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Two Sigma","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T13:15:02.309Z","createdAt":"2026-01-14T13:15:02.310Z"},{"id":"q-1967","question":"In a production Swarm across two data centers with four nodes, implement end-to-end image provenance using Cosign. Sign CI artifacts, publish signatures to a registry, and enforce verification before deploys. Outline the exact steps: key management, signing workflow in CI, signature verification at pull-time, updating services with image digests, and a rollback plan if verification fails. Provide concrete Cosign commands and how to incorporate into a CI/CD pipeline?","answer":"CI builds, signs, and publishes; Swarm deploy uses digest; if cosign verify fails, rollback to previous digest. Commands: cosign generate-key-pair, cosign sign --key cosign.key registry.example.com/ap","explanation":"## Why This Is Asked\nAssesses practical image provenance, CI/CD integration, and rollback readiness in distributed Swarm.\n\n## Key Concepts\n- Image signing with Cosign; key management; digest-based deployments; runtime verification; rollback strategy.\n\n## Code Example\n```\n// Implementation code here\n```\n\n## Follow-up Questions\n- How would you automate key rotation and revocation?\n- How do you test the rollback in a canary deployment?","diagram":null,"difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","DoorDash","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T19:00:51.280Z","createdAt":"2026-01-14T19:00:51.281Z"},{"id":"q-2016","question":"In a three-node Docker Swarm spanning two data centers, deploy a stateless API behind a Traefik ingress with a Redis-backed cache layer and mutual TLS between services. Use Docker secrets/configs for TLS certs and cache creds. Apply zero-downtime upgrades via update_config (start-first, parallelism 1) and implement a canary upgrade path with health checks and automatic rollback. Outline the exact init/join commands, overlay creation, stack file, and upgrade sequence?","answer":"Initialize a three-node Swarm across two DCs, join the remaining nodes with generated tokens, create an overlay network, and deploy a Traefik-based API with a Redis cache layer. Use Docker secrets for","explanation":"## Why This Is Asked\nTests orchestration across DCs, zero-downtime upgrades, and secure service-to-service traffic with mTLS. It also probes secret/config management, canary rollout, and rollback handling.\n\n## Key Concepts\n- Swarm multi-DC deployment and node join tokens\n- Overlay networks and Traefik ingress with mTLS\n- Docker secrets/configs for TLS and credentials\n- update_config with start_first and parallelism, canary deployments\n\n## Code Example\n```yaml\n# docker-stack.yml (excerpt)\nversion: '3.8'\nservices:\n  api:\n    image: myapi:v2\n    deploy:\n      replicas: 3\n      update_config:\n        parallelism: 1\n        order: start-first\n    secrets:\n      - redis-creds\n      - tls-cert\n    networks:\n      - app-net\n  cache:\n    image: redis:6\n    deploy:\n      replicas: 3\n    secrets:\n      - redis-creds\n    networks:\n      - app-net\nnetworks:\n  app-net:\n    external: true\nsecrets:\n  tls-cert:\n    external: true\n  redis-creds:\n    external: true\n```\n\n## Follow-up Questions\n- How would you implement automatic rollback on canary failure?\n- How do you measure canary success beyond HTTP 200s (latency, error rate)?","diagram":null,"difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T20:53:37.375Z","createdAt":"2026-01-14T20:53:37.375Z"},{"id":"q-2022","question":"Scenario: On a single host, implement a blue-green deployment for a stateless API behind an Nginx reverse proxy using docker-compose. Start with green (v1) receiving all traffic; deploy blue (v2) and switch traffic to blue only after a 30-second health-check window confirms readiness, ensuring zero downtime. Provide: a) docker-compose.yml with two API services (api-green and api-blue) and Nginx, b) nginx.conf with a switchable upstream, c) a Bash script upgrade.sh that promotes blue by reconfiguring upstream and reloading Nginx, d) exact commands to bring up green, deploy blue, run the upgrade, and verify with curl?","answer":"Bootstrap green (v1) behind a single Nginx proxy. Deploy blue (v2) as a separate container and run healthchecks. When blue is healthy for 30s, swap nginx upstream to point to api-blue and reload Nginx","explanation":"## Why This Is Asked\nTests practical blue-green deployment on a single host, emphasizing health-driven promotion and service-rotations with minimal downtime.\n\n## Key Concepts\n- Blue-green deployment on a standalone host\n- Docker Compose for multi-service apps\n- Nginx upstream switching and live reload\n- Health checks and rollback safety\n\n## Code Example\n```yaml\nversion: \"3.9\"\nservices:\n  nginx:\n    container_name: nginx\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n  api-green:\n    image: myapi:v1\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/health\"]\n      interval: 5s\n      timeout: 2s\n      retries: 3\n  api-blue:\n    image: myapi:v2\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/health\"]\n      interval: 5s\n      timeout: 2s\n      retries: 3\nnetworks:\n  default:\n    driver: bridge\n```\n\n```nginx\nevents { worker_connections 1024; }\nhttp {\n  upstream api_green { server api-green:8080; }\n  server { listen 80; location / { proxy_pass http://api_green; } }\n}\n```\n\n```bash\n#!/usr/bin/env bash\nset -e\n# Start green (v1)\ndocker-compose up -d api-green nginx\n# Deploy blue (v2)\ndocker-compose up -d api-blue\n# Wait for blue health (simplified placeholder)\n# ... poll health until healthy for 30s ...\n# Promote blue\nsed -i 's/server api-green:8080;/server api-blue:8080;/' nginx.conf\ndocker exec -i nginx nginx -s reload\n```\n\n## Follow-up Questions\n- How would you automate the 30s health window and rollback if any check fails?\n- How would you extend this to three or more canaries and track traffic ratios over time?","diagram":null,"difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Hashicorp","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-14T21:31:46.162Z","createdAt":"2026-01-14T21:31:46.162Z"},{"id":"q-2053","question":"In a four-node **Docker Swarm** spanning two data centers, deploy a stateless API with a Redis-backed rate limiter and a shared cache. Implement a canary upgrade of the API using update_config (start-first, parallelism 1), with a front-door that routes the canary subset using label-based routing. Outline exact commands: swarm init/join, overlay creation, stack deploy, and the health-check-driven promotion. Include a minimal docker-compose snippet showing update_config?","answer":"Initialize a 4-node Docker Swarm across two data centers and create an overlay network for cross-DC communication. Deploy the API stack with 3 production replicas and 1 canary replica, both configured with update_config using parallelism: 1 and order: start-first. Implement label-based routing through a front-door proxy that directs traffic to the canary based on service labels. Monitor health checks and promote the canary by updating the production service image after validation.","explanation":"## Why This Is Asked\nTests practical canary deployment strategies across distributed infrastructure with stateful dependencies (Redis rate limiter, shared cache) and intelligent traffic routing. Evaluates understanding of Swarm's update_config mechanics and health-driven promotion workflows.\n\n## Key Concepts\n- Docker Swarm update_config with start-first ordering\n- Canary deployment patterns and traffic splitting\n- Multi-datacenter overlay networking\n- Health check-based promotion strategies\n- Label-based service routing\n\n## Code Example\n```yaml\nversion: \"3.8\"\nservices:\n api:\n   image: myapi:2.0\n   deploy:\n     replicas: 3\n     update_config:\n       parallelism: 1\n       order: start-first\n     labels:\n       - \"version=production\"\n   networks:\n     - front\n   healthcheck:\n     test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/health\"]\n     interval: 30s\n     timeout: 10s\n     retries: 3\n canary:\n   image: myapi:2.0-canary\n   deploy:\n     replicas: 1\n     update_config:\n       parallelism: 1\n       order: start-first\n     labels:\n       - \"version=canary\"\n   networks:\n     - front\n redis:\n   image: redis:alpine\n   deploy:\n     replicas: 1\n   networks:\n     - front\nnetworks:\n front:\n   driver: overlay\n   attachable: true\n```","diagram":null,"difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Instacart","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T04:40:42.873Z","createdAt":"2026-01-14T22:40:23.213Z"},{"id":"q-2120","question":"In a two-node Swarm, deploy a TLS-secured stateless API behind an Nginx proxy. Use Docker secrets for TLS certs, a Docker config for API settings, and a small sidecar that ships logs without affecting requests. Attach both services to a single overlay network and perform a canary upgrade with a rolling update (start-first, parallelism 1). Provide exact commands and a docker-compose.yml skeleton?","answer":"Initialize the Swarm cluster on node A with `docker swarm init --advertise-addr <IP1>`, then join node B using `docker swarm join --token <SWMTKN...> <IP1>:2377`. Create the overlay network with `docker network create -d overlay prod-net`.","explanation":"## Why This Is Asked\nThis question assesses practical mastery of Docker Swarm, including secrets management, overlay networking, and zero-downtime deployments through rolling updates.\n\n## Key Concepts\n- Swarm initialization and multi-node clustering\n- Overlay network creation and service attachment\n- Docker secrets and configs for secure configuration management\n- Rolling updates with health checks and canary-style deployments\n- Sidecar pattern for non-blocking log shipping\n\n## Code Example\n```yaml\nversion: '3.8'\nservices:\n  proxy:\n    image: nginx:alpine\n    ports:\n      - '443:443'\n    secrets: [ tls_cert, tls_key ]\n    configs: [ app_config ]\n    networks: [ prod-net ]\n  api:\n    image: my-api:latest\n    secrets: [ tls_cert, tls_key ]\n    configs: [ app_config ]\n    networks: [ prod-net ]\n    deploy:\n      update_config:\n        parallelism: 1\n        delay: 10s\n        failure_action: rollback\n        order: start-first\n  log-shipper:\n    image: log-shipper:latest\n    networks: [ prod-net ]\n    depends_on: [ api ]\n```\n\n## Additional Commands\n```bash\n# Create secrets\necho \"cert-content\" | docker secret create tls_cert -\necho \"key-content\" | docker secret create tls_key -\n\n# Create config\necho \"app-settings\" | docker config create app_config -\n\n# Deploy stack\ndocker stack deploy -c docker-compose.yml myapp\n```","diagram":"flowchart TD\n  A[Init Swarm] --> B[Create overlay net]\n  B --> C[Create secrets/configs]\n  C --> D[Stack deploy]\n  D --> E[Canary upgrade]\n  E --> F[Promote canary]","difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T04:53:30.558Z","createdAt":"2026-01-15T02:28:35.499Z"},{"id":"q-2140","question":"In a two-datacenter Docker Swarm (2 nodes per DC), deploy a stateless API behind Traefik with TLS termination on an overlay network across DCs. Implement a canary rollout for a feature-flag change using a Swarm Config and route 10% of traffic to canary via Traefik labels. Outline exact swarm init/join commands, overlay creation, stack deploys, config creation, and the canary upgrade with health checks and rollback criteria?","answer":"Plan: Bring up both DCs as a single swarm, create an attachable overlay, deploy Traefik with TLS via Docker secrets, create a Swarm Config with the feature flag, launch two API stacks (prod and canary","explanation":"## Why This Is Asked\nTests cross-datacenter orchestration, canary traffic, and secret/config handling with Traefik in Swarm across DCs.\n\n## Key Concepts\n- Docker Swarm multi-DC overlay networking\n- Traefik dynamic routing and weights for canaries\n- Swarm Config vs Secret for feature flags\n- Health checks and safe rollback strategies\n\n## Code Example\n```bash\n# Swarm init (DC1)\ndocker swarm init --advertise-addr <dc1-ip>\n# Join DC2\n# docker swarm join ...\n\n\ndocker network create -d overlay --attachable traefik-net\n# Deploy Traefik with TLS secret and config-driven routes\n# ...\n```\n\n## Follow-up Questions\n- How would you test the rollover window and rollback boundaries?\n- How would you monitor traffic split and auto-promote canary on success?","diagram":"flowchart TD\nA[Two-DC Swarm] --> B[Overlay Network]\nB --> C[Traefik TLS]\nC --> D[Prod API]\nC --> E[Canary API]","difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T04:20:30.901Z","createdAt":"2026-01-15T04:20:30.901Z"},{"id":"q-2231","question":"In a four-node docker-dca cluster, deploy a TLS-secured stateless API behind an Nginx edge proxy. Use Docker secrets for TLS certs and a Docker config for API endpoints, and add an Envoy sidecar per app to drive traffic-splitting controlled by a central Consul KV flag. Implement a progressive canary: 10% steps every 30s up to 100%, with health checks and automatic rollback on failure. Provide explicit bootstrap commands and a docker-compose.yml skeleton?","answer":"Use a four-node docker-dca canary with Envoy sidecars, edge TLS via Docker secrets, and a Consul KV flag to drive 10%→100% traffic steps every 30s. Health checks gate progress; rollback triggers on tw","explanation":"## Why This Is Asked\nTests practical orchestration of traffic-shifting, secret/config management, and automated rollback in a docker-dca setup.\n\n## Key Concepts\n- Envoy sidecars for per-service traffic-splitting\n- Consul KV as a dynamic control flag\n- Docker secrets/configs for TLS and endpoints\n- Canary rollout with health checks and rollback\n\n## Code Example\n```yaml\nversion: '3.8'\nservices:\n  edge-nginx:\n    image: nginx:alpine\n    secrets:\n      - tls_cert\n      - tls_key\n    configs:\n      - source: api_endpoints\n        target: /etc/api_endpoints.conf\n  app:\n    image: myapi:latest\n    deploy:\n      replicas: 3\n      update_config:\n        parallelism: 1\n        order: start-first\n    depends_on:\n      - envoy\n  envoy:\n    image: envoyproxy/envoy:v1.18.3\n    depends_on:\n      - app\n    volumes:\n      - ./envoy.yaml:/etc/envoy/envoy.yaml:ro\nsecrets:\n  tls_cert:\n  tls_key:\nconfigs:\n  api_endpoints:\n```\n\n## Follow-up Questions\n- How would you test rollback safety during regional failover?\n- How would you extend the flow to autoscale canary windows based on real-time metrics?","diagram":"flowchart TD\n  A[Edge TLS Termination (Nginx)] --> B[App Replica Set 1]\n  A --> C[App Replica Set 2]\n  B --> D[Envoy Sidecar]\n  C --> E[Envoy Sidecar]\n  D --> F[Central Router (Consul Map)]\n  E --> F","difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Microsoft","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T08:47:25.253Z","createdAt":"2026-01-15T08:47:25.253Z"},{"id":"q-2254","question":"Create a local Docker Compose setup with two Python services: web (Flask API) and cache (Redis). Duplicate web as web_canary with CANARY=true. Add an Nginx container as a reverse proxy routing /api to web and /canary to web_canary. Use a named volume for Redis data. Add healthchecks for all three containers and provide a docker-compose.yml skeleton plus exact bootstrap commands to seed data and verify endpoints?","answer":"Four services: web, web_canary, cache, and nginx. web and web_canary expose ports and use CANARY to differentiate behavior. Redis uses a named volume redis-data for persistence. Nginx routes /api to w","explanation":"## Why This Is Asked\n\nTests ability to design a practical local deployment with a stable/canary pattern, data persistence, and a proxy layer.\n\n## Key Concepts\n\n- Docker Compose service composition\n- Health checks and readiness\n- Canary routing via a dedicated proxy\n- Volume for data persistence\n\n## Code Example\n\n```javascript\n// Placeholder illustrating structure for health checks\n```\n\n## Follow-up Questions\n\n- How would you promote canary to stable with zero downtime?\n- How would you extend to multiple canaries and metrics-based routing?","diagram":"flowchart TD\n  Client --> NginxProxy[Nginx Proxy]\n  NginxProxy --> Web[web]\n  NginxProxy --> Canary[web_canary]\n  NginxProxy --> Redis[cache]","difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Robinhood","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T09:39:38.182Z","createdAt":"2026-01-15T09:39:38.182Z"},{"id":"q-2295","question":"In a 5-node Docker Swarm spanning two data centers, deploy a TLS-secured stateless API behind an Nginx edge proxy. Use Docker secrets for TLS certs and a Docker config for per-tenant routing templates, consuming routing directives from a central HTTP API. Attach a per-service Envoy sidecar to drive traffic-splitting with a progressive canary (25% steps every 20s). Ship logs via a separate sidecar to a Loki stack without blocking requests. Implement a zero-downtime upgrade using update_config (start-first, parallelism 1). Provide exact commands and a docker-compose.yml skeleton?","answer":"Architect a 5-node Swarm across DCs, TLS via Docker secrets, Nginx edge proxy, and a per-service Envoy sidecar for canary routing. Routing rules pulled from a central HTTP API rendered into a Docker c","explanation":"## Why This Is Asked\n\nTests ability to design multi-DC Swarm, TLS secrets, dynamic routing, and Canary with sidecars and observability. Checks how routing rules are distributed and how upgrades remain zero-downtime.\n\n## Key Concepts\n\n- Docker Swarm across data centers\n- Secrets and Configs for TLS and routing\n- Nginx edge proxy with per-service Envoy sidecar\n- Canary rollout with health checks and rollback\n- Sidecar logging to Loki without blocking requests\n\n## Code Example\n\n```yaml\nversion: \"3.8\"\nservices:\n  edge-nginx:\n    image: nginx:alpine\n    secrets: [ tls_cert ]\n    configs: [ routing_tpl ]\n  api:\n    image: my/api:latest\n    deploy:\n      replicas: 3\n      update_config:\n        parallelism: 1\n        order: start-first\n```\n\n## Follow-up Questions\n\n- How would you validate canary health signals and rollback criteria?\n- How would you handle TLS certificate rotation without downtime?\n","diagram":"flowchart TD\n  DC1[Data Center 1] --> SwarmMgr[Swarm Manager]\n  DC2[Data Center 2] --> SwarmMgr\n  SwarmMgr --> Edge[Edge Proxy: Nginx]\n  Edge --> API[API Service]\n  API --> Logger[Log-Sidecar]\n  API --> Canary[Envoy Sidecar]","difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Slack","Tesla","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T10:56:17.946Z","createdAt":"2026-01-15T10:56:17.946Z"},{"id":"q-2346","question":"In a 3-node docker-dca cluster spanning two data centers, deploy a TLS-secured stateless API behind an Nginx proxy. Use Docker secrets for TLS certs and a Docker config for API endpoints. Add a shadow sidecar that mirrors 5% of production traffic to a canary version. Implement a canary rollout by adjusting weights in 10% increments using update_config, with health checks and automatic rollback on failure. Provide exact commands and a docker-compose.yml skeleton?","answer":"Implement dynamic traffic shadow: deploy a main API and a canary, with a shadow Envoy sidecar mirroring ~5% of traffic to the canary; TLS via Docker secrets; Nginx routes 95% to main and 5% to shadow;","explanation":"## Why This Is Asked\nTests ability to orchestrate multi-DC deployments, secret-driven TLS, and traffic shaping with live canary upgrades. It also probes rollback discipline and the interaction between edge proxies, sidecars, and the orchestrator.\n\n## Key Concepts\n- Docker secrets and configs for TLS and endpoints\n- Nginx/Envoy traffic routing and shadow/canary pattern\n- Canary rollout via update_config with health checks\n- Multi-DC networking and rollback strategies\n\n## Code Example\n```yaml\nversion: '3.8'\nservices:\n  nginx:\n    image: nginx:stable\n    secrets:\n      - tls_cert\n    configs:\n      - source: api_endpoints\n        target: /etc/api/endpoints.conf\n  api-main:\n    image: myapi:latest\n    secrets:\n      - tls_cert\n  api-canary:\n    image: myapi:canary\n    secrets:\n      - tls_cert\n    deploy:\n      update_config:\n        parallelism: 1\n        order: start-first\n        failure_action: rollback\nsecrets:\n  tls_cert:\n    file: ./certs/tls_cert.pem\nconfigs:\n  api_endpoints:\n    file: ./configs/endpoints.conf\n```\n\n## Follow-up Questions\n- How would you monitor canary health and automate rollback decisions?\n- How would you secure inter-service communication and ensure TLS mutual authentication across DCs?","diagram":null,"difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Google","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T14:35:40.425Z","createdAt":"2026-01-15T14:35:40.425Z"},{"id":"q-2386","question":"In a two-datacenter Docker Swarm with a TLS-enabled REST API behind an Nginx edge proxy, TLS certs are stored as Docker secrets. Design a zero-downtime rotation workflow: publish a new cert secret api_tls_new without restarting Nginx, upgrade the service with the new secret using a canary approach (update_config: order start-first, parallelism 1), and provide the exact shell commands plus a minimal docker-compose.yml skeleton showing secrets, config, and a small log-shipper sidecar that does not delay requests?","answer":"Rotate TLS certs by creating api_tls_new, attach it via docker service update with --secret-add api_tls_new and --secret-rm api_tls_old using update-order start-first and update-parallelism 1 for a ca","explanation":"Why This Is Asked\nTests ability to design zero-downtime secret rotation in Swarm, including secret lifecycle, canary upgrade discipline, and edge-reload strategy.\n\nKey Concepts\n- Docker secrets lifecycle and updates\n- Swarm update_config for canary-like upgrades\n- Edge proxy reload without downtime\n- Non-blocking sidecar logging\n\nCode Example\n```yaml\nversion: '3.8'\nservices:\n  api:\n    image: myapi:latest\n    secrets:\n      - api_tls_old\n      - api_tls_new\n    deploy:\n      update_config:\n        parallelism: 1\n        order: start-first\n  log_shipper:\n    image: log-shipper:latest\n    secrets:\n      - api_tls_old\n      - api_tls_new\nsecrets:\n  api_tls_old:\n    external: true\n  api_tls_new:\n    external: true\n```\n\nFollow-up Questions\n- How would you automate rotation cadence and auditing of secret changes?\n- How would you test the zero-downtime guarantee in CI/CD before production rollout?","diagram":null,"difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Meta","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T15:50:28.068Z","createdAt":"2026-01-15T15:50:28.068Z"},{"id":"q-2432","question":"In a 4-node docker-dca Swarm, deploy a TLS-secured event-processing pipeline behind an Nginx edge proxy. Use Docker secrets for TLS certs, a Docker config for routing rules, and implement a 5% canary upgrade for the Transform service controlled by a Consul KV flag. Provide exact commands and a docker-compose.yml skeleton that demonstrates canary traffic, health checks, and zero-downtime upgrades?","answer":"Set up a 4-node Swarm with an overlay network, create TLS secrets tls.crt and tls.key, and a routing config. Deploy ingest, transform-prod, transform-canary, and sink behind edge-nginx. Route 95% to p","explanation":"## Why This Is Asked\n\nTests ability to orchestrate secrets/configs with a canary using a proxy. Requires Swarm networking, overlay, health checks, and rollback semantics.\n\n## Key Concepts\n\n- Docker secrets/configs\n- Swarm services and update_config\n- Overlay networks and edge proxy traffic-splitting\n- Consul KV flag-based traffic routing\n\n## Code Example\n\n```yaml\nversion: '3.8'\nservices:\n  ingest:\n    image: myreg/ingest:latest\n    deploy:\n      replicas: 1\n      restart_policy:\n        condition: on-failure\n  transform-prod:\n    image: myreg/transform-prod:latest\n    deploy:\n      replicas: 1\n      update_config:\n        parallelism: 1\n        order: start-first\n  transform-canary:\n    image: myreg/transform-canary:latest\n    deploy:\n      replicas: 1\n  sink:\n    image: myreg/sink:latest\n    deploy:\n      replicas: 1\n  edge-nginx:\n    image: nginx:alpine\n    secrets:\n      - tls_crt\n      - tls_key\n    configs:\n      - source: routing_conf\n        target: /etc/nginx/conf.d/routing.conf\nnetworks:\n  pipeline:\n    driver: overlay\nsecrets:\n  tls_crt:\n    file: ./certs/tls.crt\n  tls_key:\n    file: ./certs/tls.key\nconfigs:\n  routing_conf:\n    file: ./routing/nginx.conf\n```\n\n## Follow-up Questions\n\n- How would you test the canary flag in production-like load?\n- How would you roll back if the canary shows degradation?","diagram":"flowchart TD\n  A[Edge Proxy] --> B[Ingest]\n  B --> C[Transform Prod]\n  C --> D[Sink]\n  D --> E[Storage]","difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","Robinhood","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T17:50:37.495Z","createdAt":"2026-01-15T17:50:37.495Z"},{"id":"q-2485","question":"In a three-node Docker Swarm spanning two data centers, deploy a TLS-secured stateless event API behind an Nginx edge proxy. Use Docker secrets for TLS certs and a Docker config for API endpoints. Introduce an SPIRE-based mTLS mesh by wiring an SPIRE agent as a sidecar to each app container to issue short-lived mTLS certs and enforce SPIFFE IDs. Implement automatic certificate rotation every 60 minutes and a progressive canary upgrade (start-first, parallelism 1). Provide exact docker-compose.yml skeleton and bootstrap commands?","answer":"A possible answer would outline: init a swarm across both data centers, create an overlay network, load TLS secrets and API configs, deploy the app with a sidecar SPIRE agent container, configure SPIR","explanation":"## Why This Is Asked\nTests knowledge of multi-datacenter Swarm, TLS secrets, config usage, and a real-world mTLS service mesh with SPIRE. It also probes canary upgrades and security in production.\n\n## Key Concepts\n- Docker Swarm orchestration across DCs\n- TLS with Docker secrets and configs\n- SPIRE-based mTLS workload identity\n- Certificate rotation and health-driven canaries\n- Update strategy: start-first, parallelism 1\n\n## Code Example\n```yaml\n# docker-compose.yml skeleton\nversion: '3.8'\nservices:\n  api:\n    image: my/api:latest\n    deploy:\n      replicas: 2\n      update_config:\n        order: start-first\n        parallelism: 1\n    configs:\n      - source: api_config\n        target: /etc/api/config.yaml\n    secrets:\n      - tls_cert\n      - tls_key\n    networks:\n      - ov\n    depends_on: []\n    # SPIRE sidecar will be defined in a separate service or as a sidecar container\n  spire-agent:\n    image: quay.io/spiffe/spire-agent:latest\n    volumes:\n      - /var/lib/spire:/run/spire\n    command: run\n    deploy:\n      replicas: 2\n      labels:\n        SPIRE: agent\nnetworks:\n  ov:\n    external: true\nsecrets:\n  tls_cert:\n    file: ./certs/server.crt\n  tls_key:\n    file: ./certs/server.key\nconfigs:\n  api_config:\n    file: ./config/api.yaml\n```\n\n### Follow-up Steps\n- Show exact bootstrap commands for swarm init/join and SPIRE setup.\n- Describe monitoring and rotation verification.","diagram":null,"difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Citadel","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-15T19:46:58.046Z","createdAt":"2026-01-15T19:46:58.046Z"},{"id":"q-2558","question":"In a two-node docker-dca Swarm, deploy a TLS-enabled stateless API behind an Nginx proxy. Use Docker secrets for TLS certs, a Docker config for API endpoints, and a lightweight log-shipper sidecar that adds no latency. Attach both services to a single overlay network. Implement a 5% canary rollout with health checks and automatic rollback on failure. Provide exact commands and a docker-compose.yml skeleton, plus how to measure and enforce performance during the rollout?","answer":"Initialize a two-node Docker Swarm cluster, create an overlay network, store TLS certificates as Docker secrets, API endpoints as a Docker config, deploy Nginx as an edge proxy with the stateless API backend, include a lightweight log-shipper sidecar, implement a 5% canary rollout with health checks and automatic rollback on failure, and measure performance throughout the deployment process.","explanation":"## Why This Is Asked\nTests practical experience with Docker Swarm fundamentals, secrets and configs management, edge proxy orchestration, controlled deployment strategies, and performance monitoring during rollouts.\n\n## Key Concepts\n- Docker Swarm initialization and overlay networking\n- Docker secrets for sensitive data (TLS certificates)\n- Docker configs for application configuration\n- Edge proxy pattern with Nginx in Swarm\n- Canary deployments with health-check-driven rollbacks\n- Sidecar containers for log shipping\n- Performance measurement and enforcement during rollouts\n\n## Code Example\n```\n","diagram":null,"difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","DoorDash","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:26:12.739Z","createdAt":"2026-01-15T22:46:54.516Z"},{"id":"q-2588","question":"On a two-node Docker Swarm, deploy a TLS-secured stateless API behind an Nginx edge proxy. Use a Docker secret for TLS certs and a Docker config for API endpoints. Ensure the app runs as a non-root user and add a tiny sidecar that tails logs to a shared volume without affecting requests. Attach both services to an overlay network and implement a rolling update with start_first, parallelism 1. Include exact commands to initialize swarm, create secret/config, build/deploy, and a docker-compose.yml skeleton?","answer":"Run: `docker swarm init`; `docker network create -d overlay appnet`; `docker secret create tls.pem cert.pem`; `docker secret create tls.key key.pem`; `docker config create api-endpoints /path/api.json`; `docker stack deploy -c docker-compose.yml mystack`","explanation":"## Why This Is Asked\nTests wiring of secrets/config, non-root containers, and safe rolling updates in a two-node swarm.\n\n## Key Concepts\n- Docker secrets/config\n- Non-root user in containers\n- Sidecar log shipping\n- Overlay networking and rolling updates\n\n## Code Example\n```dockerfile\nFROM node:18\nRUN groupadd -r app && useradd -r -g app app\nUSER app\nWORKDIR /app\nCOPY --chmod=644 . .\nCMD [\"node\",\"server.js\"]\n```\n\n```yaml\nversion: \"3.8\"\nservices:\n  api:\n    image: mystack/api:latest\n    networks: [ appnet ]\n    secrets: [ tls.pem, tls.key ]\n    configs: [ api-endpoints ]\n    deploy:\n      update_config:\n        parallelism: 1\n        order: start-first\n      restart_policy:\n        condition: on-failure\n  nginx:\n    image: nginx:alpine\n    networks: [ appnet ]\n    secrets: [ tls.pem, tls.key ]\n    ports:\n      - \"443:443\"\n    deploy:\n      replicas: 1\n  logtailer:\n    image: alpine:latest\n    command: tail -f /logs/app.log\n    volumes:\n      - shared-logs:/logs\n    networks: [ appnet ]\n    deploy:\n      replicas: 1\n\nnetworks:\n  appnet:\n    driver: overlay\n    external: true\n\nvolumes:\n  shared-logs:\n```\n\n## Implementation Steps\n1. Initialize swarm on both nodes and join them\n2. Create overlay network for service communication\n3. Generate TLS certificates and create secrets\n4. Create API endpoint configuration\n5. Build and push API image with non-root user\n6. Deploy stack with rolling update configuration\n7. Verify TLS termination and log shipping functionality","diagram":"flowchart TD\n  A[Swarm Init] --> B[Create Overlay Network]\n  B --> C[Create Secrets & Configs]\n  C --> D[Deploy Stack]\n  D --> E[Canary Upgrade / Health Checks]","difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Salesforce","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T05:07:32.152Z","createdAt":"2026-01-15T23:46:32.556Z"},{"id":"q-2829","question":"In a two-node docker-dca Swarm, deploy a TLS-enabled stateless API behind an Nginx edge proxy. Add a shadow version of the API and use traffic mirroring to send 5% of live traffic to the shadow for black-box testing without affecting live latency. Use Docker secrets for TLS certs and a Docker config for API endpoints. Ensure health checks and automatic rollback if the shadow underperforms. Provide exact commands and a docker-compose.yml skeleton?","answer":"Deploy TLS-enabled API behind Nginx edge proxy with a second shadow API. Mirror 5% of requests to the shadow via Nginx mirror, keeping live path on the primary. Use Docker secrets for TLS certs and a ","explanation":"## Why This Is Asked\nTests ability to implement traffic mirroring for safe shadow testing, validate latency impact, and design robust rollback.\n\n## Key Concepts\n- traffic mirroring with Nginx\n- docker secrets/configs\n- overlay networking in Swarm\n- health checks and rollback policies\n- non-disruptive canary/shadow deployments\n\n## Code Example\n```yaml\nversion: '3.8'\nservices:\n  live-api:\n    image: myorg/live\n    secrets: [ tls_cert, tls_key ]\n  shadow-api:\n    image: myorg/shadow\n    secrets: [ tls_cert, tls_key ]\n  edge-nginx:\n    image: nginx:latest\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n    depends_on: [live-api, shadow-api]\n    configs: [ api-endpoints_config ]\nconfigs:\n  api-endpoints_config:\n    external: true\nsecrets:\n  tls_cert:\n    external: true\n  tls_key:\n    external: true\nnetworks:\n  proxy:\n    driver: overlay\n    attachable: true\n```\n\n```nginx\n# nginx.conf (simplified)\nserver {\n  listen 443 ssl;\n  ssl_certificate /run/secrets/tls_cert;\n  ssl_certificate_key /run/secrets/tls_key;\n  location / {\n    proxy_pass http://live-api:8080;\n    mirror /mirror_shadow;\n  }\n  location = /mirror_shadow {\n    proxy_pass http://shadow-api:8081$request_uri;\n    internal;\n  }\n}\n```\n\n## Follow-up Questions\n- How would you calculate and enforce a 95th percentile SLA during shadow testing?\n- How would you automate rollback if shadow latency exceeds threshold for three consecutive minutes?","diagram":null,"difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Plaid","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T14:03:42.560Z","createdAt":"2026-01-16T14:03:42.560Z"},{"id":"q-2918","question":"In a two-node docker-dca Swarm, deploy a TLS-enabled stateless API behind an Nginx edge proxy. Use Docker secrets for TLS certs and a Docker config to store per-tenant ACLs. Add an Open Policy Agent (OPA) sidecar to enforce per-request authorization, with policy loaded from a Docker config. Include a separate log-collector sidecar that writes audit logs to a shared volume. Implement a 20% canary rollout every 15 seconds with health checks and automatic rollback on policy violation. Provide exact commands and a minimal docker-compose.yml skeleton?","answer":"Deploy a TLS-enabled API behind Nginx with OPA policy enforcement and canary rollouts. Use Docker secrets for certs, configs for ACLs, and sidecar containers for authorization and logging. Implement 20% canary deployment with health checks and automatic rollback on policy violations.\n\n## Commands\n```bash\n# Create TLS secrets\necho \"-----BEGIN CERTIFICATE-----\" | docker secret create tls-cert -\necho \"-----BEGIN PRIVATE KEY-----\" | docker secret create tls-key -\n\n# Create OPA policy config\ncat > policy.rego << 'EOF'\npackage example.api.auth\ndefault allow = false\nallow {\n  input.method == \"GET\"\n  input.path = [\"api\", \"v1\", \"data\"]\n  input.tenants[_] = input.user.tenant\n}\nEOF\ndocker config create opa-policy policy.rego\n\n# Create ACL config\ncat > acls.json << 'EOF'{\"tenants\": {\"tenant1\": [\"read\", \"write\"], \"tenant2\": [\"read\"]}}\nEOF\ndocker config create tenant-acls acls.json\n\n# Deploy stack\ndocker stack deploy -c docker-compose.yml api-stack\n\n# Monitor canary rollout\nwatch docker service ps api-stack_api\n```","explanation":"## Why This Is Asked\nTests ability to implement zero-trust security, multi-tenant isolation, and safe deployment patterns in Docker Swarm with policy-as-code enforcement.\n\n## Key Concepts\n- Swarm secrets/configs for secure data management\n- Sidecar pattern for policy enforcement and logging\n- Canary deployments with automatic rollback\n- TLS termination at edge proxy\n- OPA for real-time authorization decisions\n\n## Code Example\n```yaml\nversion: '3.8'\nservices:\n  nginx:\n    image: nginx:alpine\n    ports: ['443:443']\n    secrets:\n      - source: tls-cert\n        target: /etc/nginx/ssl/tls.crt\n      - source: tls-key\n        target: /etc/nginx/ssl/tls.key\n    configs:\n      - source: nginx-conf\n        target: /etc/nginx/nginx.conf\n    deploy:\n      replicas: 1\n      restart_policy:\n        condition: on-failure\n        delay: 5s\n        max_attempts: 3\n    networks:\n      - frontend\n\n  api:\n    image: my-api:latest\n    environment:\n      - TENANT_CONFIGS=/configs/acls.json\n    configs:\n      - source: tenant-acls\n        target: /configs/acls.json\n    deploy:\n      replicas: 5\n      update_config:\n        parallelism: 1\n        delay: 15s\n        failure_action: rollback\n        monitor: 30s\n        max_failure_ratio: 0.2\n      rollback_config:\n        parallelism: 1\n        delay: 10s\n      healthcheck:\n        test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8080/health\"]\n        interval: 10s\n        timeout: 5s\n        retries: 3\n        start_period: 30s\n    networks:\n      - frontend\n      - backend\n    volumes:\n      - audit-logs:/var/log/audit\n\n  opa:\n    image: openpolicyagent/opa:latest\n    command: [\"run\", \"--server\", \"/policies/policy.rego\"]\n    configs:\n      - source: opa-policy\n        target: /policies/policy.rego\n    environment:\n      - OPA_LOG_LEVEL=info\n    networks:\n      - backend\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:8181/v1/data\"]\n      interval: 30s\n      timeout: 5s\n      retries: 3\n\n  log-collector:\n    image: fluent/fluent-bit:latest\n    command: [\"/fluent-bit/bin/fluent-bit\", \"--config\", \"/fluent-bit/etc/fluent-bit.conf\"]\n    volumes:\n      - audit-logs:/var/log/audit\n      - ./fluent-bit.conf:/fluent-bit/etc/fluent-bit.conf:ro\n    networks:\n      - backend\n    deploy:\n      replicas: 1\n      restart_policy:\n        condition: on-failure\n\nsecrets:\n  tls-cert:\n    external: true\n  tls-key:\n    external: true\n\nconfigs:\n  nginx-conf:\n    external: true\n  opa-policy:\n    external: true\n  tenant-acls:\n    external: true\n\nvolumes:\n  audit-logs:\n    driver: local\n\nnetworks:\n  frontend:\n    driver: overlay\n  backend:\n    driver: overlay\n    internal: true\n```\n\n## fluent-bit.conf\n```ini\n[SERVICE]\n    Flush         5\n    Log_Level     info\n    Daemon        off\n\n[INPUT]\n    Name          tail\n    Path          /var/log/audit/*.log\n    Tag           audit.*\n    Refresh_Interval 10\n\n[OUTPUT]\n    Name          file\n    Match         audit.*\n    Path          /var/log/processed\n    File          audit.log\n```\n\n## Implementation Details\n\n**OPA Integration**: OPA sidecar loads policies from Docker config and exposes authorization endpoint at `http://localhost:8181/v1/data`. API checks authorization before processing requests.\n\n**Canary Strategy**: 20% rollout (1 out of 5 replicas) every 15 seconds with 30-second health monitoring. Automatic rollback on >20% failures.\n\n**Security**: TLS termination at Nginx edge, secrets management for certificates, internal backend network isolation, per-tenant ACL enforcement.\n\n**Logging**: Audit logs written to shared volume, processed by fluent-bit sidecar for aggregation and forwarding.","diagram":"flowchart TD\n  Edge[Nginx edge proxy] --> API[API service]\n  API --> OPA[OPA sidecar (policy)]\n  OPA --> API\n  API --> Log[Audit logs]","difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T03:50:28.764Z","createdAt":"2026-01-16T17:39:15.047Z"},{"id":"q-2951","question":"On a three-node docker-dca Swarm, deploy a TLS-enabled API behind an Nginx TLS-terminating proxy with mutual TLS to the API, using Vault PKI to issue short-lived certs and a sidecar that auto-refreshes certs into a shared volume; implement a 25% canary rollout every 20s with health checks and auto-rollback. Include exact swarm bootstrap commands and a docker-compose.yml skeleton?","answer":"Bootstrap a 3-node Swarm; initialize Vault PKI, create roles for API and clients with short TTLs; store Vault token in a Docker secret; define services: nginx (TLS, mTLS, certs from a writable volume)","explanation":"## Why This Is Asked\nTests secure, rotating certs in Swarm with Vault, handling cert storage (secret vs volume), and canary semantics.\n\n## Key Concepts\n- Swarm multi-node with TLS and mTLS\n- Vault PKI for dynamic certs\n- Certs via writable volume + sidecar refresh\n- Canary rollout with health checks and rollback\n- Nginx TLS termination and service-to-service mTLS\n\n## Code Example\n```javascript\n// Simple cert refresh watcher from Vault to a mounted volume\nconst fetch = require('node-fetch');\nconst fs = require('fs');\nasync function refresh() {\n  const res = await fetch('https://vault.example/v1/pki/issue/api', { headers: { 'X-Vault-Token': process.env.VAULT_TOKEN } });\n  const cert = await res.text();\n  fs.writeFileSync('/certs/api.crt', cert);\n}\nsetInterval(refresh, 5 * 60 * 1000);\n```\n\n## Follow-up Questions\n- How would you validate cert rotation does not break existing connections?\n- How would you monitor Vault token renewal and automatic revocation?","diagram":"flowchart TD\n  Init[Swarm Init] --> Vault[Vault PKI setup]\n  Vault --> Nginx[Nginx TLS termination]\n  Nginx --> API[API with mTLS]\n  API --> Canary[Canary rollout with Health Checks]","difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Oracle","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-16T18:55:28.921Z","createdAt":"2026-01-16T18:55:28.921Z"},{"id":"q-3052","question":"In a two-node docker-dca Swarm, deploy a TLS-enabled stateless API behind Nginx. Use Docker secrets for TLS certs and a Docker Config named flag.json mounted at /config/flag.json. Implement a simple feature toggle in the API that reads the flag from /config/flag.json and supports hot reload via SIGHUP without restarting. Add a progressive canary rollout: enable the feature in 0%, 25%, 50%, 75%, 100% in 20-second steps with health checks and automatic rollback on failure. Provide exact docker commands and a docker-compose.yml skeleton that demonstrates the rollout. Also show how to verify latency impact during each step?","answer":"Deploy a Flask API that reads /config/flag.json to toggle GET /v1/feature, with SIGUSR1 handling for hot reload. TLS termination via Nginx using Docker Secrets, running 2 replicas on an overlay network. Implement canary rollout from 0% to 100% in 25% increments every 20 seconds with health checks and automatic rollback on failure.","explanation":"## Why This Is Asked\nTests dynamic configuration reload, feature toggling, and safe canary rollout capabilities in a Docker Swarm environment.\n\n## Key Concepts\n- Docker Configs and Secrets management\n- TLS termination with Nginx reverse proxy\n- Signal-based hot reload in Python applications\n- Progressive canary deployment with health monitoring\n- Automated rollback mechanisms\n\n## Code Example\n```python\nfrom flask import Flask, jsonify\nimport json, signal\napp = Flask(__name__)\nflag = False\n\ndef load_flag():\n    global flag\n    try:\n        with open('/config/flag.json') as f:\n            data = json.load(f)\n            flag = data.get('feature_x', False)\n    except Exception:\n        flag = False\n\ndef reload_handler(signum, frame):\n    load_flag()\n    \nsignal.signal(signal.SIGUSR1, reload_handler)\nload_flag()\n\n@app.route('/v1/feature')\ndef feature():\n    return jsonify({'enabled': flag})\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=8080)\n```\n\n## Implementation\n- Docker Config mounts flag.json at /config/flag.json\n- Docker Secrets store TLS certificates for Nginx\n- SIGUSR1 triggers config reload without container restart\n- Canary rollout uses service update with rollback on health check failure","diagram":"flowchart TD\n  Swarm[Two-Node Swarm] --> Nginx[Nginx TLS on Secrets]\n  Nginx --> API[API replicas on overlay]\n  API --> Config[Docker Config /config/flag.json]\n  Config -->|Reload| API\n  Canary[Canary rollout] --> Health[Health checks]","difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:15:13.387Z","createdAt":"2026-01-16T22:45:29.570Z"},{"id":"q-3119","question":"In a two-node docker-dca Swarm spanning two data centers, deploy a TLS-enabled stateless API behind an Nginx edge proxy. Use Docker secrets for TLS certs and a Docker config for API endpoints. Attach both services to a single overlay network. Add an Envoy sidecar to the API service to perform region-aware traffic-splitting driven by a global feature flag stored in Consul KV at /features/new-api. Implement a progressive canary: 20% traffic to v2 until the flag is 1, with health checks and automatic rollback on failure. Provide exact commands and a docker-compose.yml skeleton?","answer":"Leverage Envoy as a per-service sidecar to implement traffic-splitting, controlled by Consul KV flag /features/new-api. Nginx edge handles TLS via Docker secrets; services join one overlay network. St","explanation":"Why this is asked\n- Tests practical canary via a sidecar proxy and a feature flag, without a full service mesh\n- Validates multi-site deployment, TLS handling, and dynamic traffic control\n\nKey Concepts\n- Envoy as a sidecar for per-service traffic shaping\n- Consul KV feature flags for cross-service rollout control\n- Docker secrets/config for TLS and API metadata\n- Canary progression and rollback based on health/latency metrics\n\nCode Example\n```yaml\nversion: '3.8'\nservices:\n  edge-nginx:\n    image: nginx:alpine\n    secrets:\n      - tls_cert\n      - tls_key\n    networks:\n      - overlay\n  api:\n    image: myapi:latest\n    depends_on:\n      - envoy\n    networks:\n      - overlay\n  envoy:\n    image: envoyproxy/envoy:v1.25.0\n    volumes:\n      - ./envoy.yaml:/etc/envoy/envoy.yaml:ro\n    networks:\n      - overlay\n\ndevices: []\nsecrets:\n  tls_cert:\n    file: ./certs/server.crt\n  tls_key:\n    file: ./certs/server.key\nnetworks:\n  overlay:\n    driver: overlay\n```\n\nFollow-up Questions\n- How would you monitor and alert on canary drift vs. full traffic?\n- How would you handle certificate rotation without downtime in Swarm?","diagram":"flowchart TD\n  A[Client] --> B[Edge: Nginx TLS]\n  B --> C[Envoy Sidecar]\n  C --> D[API v1]\n  C --> E[API v2]\n  D --> F[Backend]\n  E --> F","difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Oracle","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:02:19.096Z","createdAt":"2026-01-17T04:02:19.096Z"},{"id":"q-3162","question":"In a two-node docker-dca Swarm, deploy a TLS-enabled stateless API behind an Nginx edge proxy. Use Docker secrets for TLS certs, a Docker config for API endpoints, and a lightweight traffic-controller container that adjusts canary weights by updating a shared Nginx config mounted as a Docker config. Route 10% traffic to v2 initially, then scale to 100% per 15-second steps based on p95 latency measured via a small health-check endpoint; auto rollback to previous weight if latency exceeds 250ms for two consecutive checks. Provide exact docker commands and a docker-compose.yml skeleton, plus how to measure latency and trigger rollback?","answer":"Implement a Python controller in Swarm that reads p95 latency from a Prometheus-compatible endpoint and gradually shifts Nginx upstream weights from 10/90 to 0/100 in 15s steps. Use Docker secrets for","explanation":"## Why This Is Asked\nTests ability to design dynamic traffic routing with strict latency guardrails in a small Swarm, plus integration of Docker secrets/configs and live config reloads.\n\n## Key Concepts\n- Swarm-based Canary with external controller\n- Nginx upstream weight management via Docker Configs\n- TLS secrets for mTLS-like security\n- Latency-driven rollback and health checks\n\n## Code Example\n```bash\n# example commands showing secret/config creation and stack deploy\ndocker swarm init\ndocker secret create tls_api path/to/tls.pem\ndocker secret create tls_key path/to/key.pem\ndocker config create upstreams.yaml /dev/null\n# deploy stack with nginx, api-v1, api-v2, traffic-controller\n```\n\n## Follow-up Questions\n- How would you ensure idempotent config updates?\n- How to validate rollback safety under burst traffic?","diagram":null,"difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Goldman Sachs","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T04:56:12.240Z","createdAt":"2026-01-17T04:56:12.240Z"},{"id":"q-3283","question":"In a three-node docker-dca cluster, deploy a TLS-enabled API behind an Nginx gateway using mutual TLS. Secrets: CA and API certs as Docker secrets; a Vault-backed secret-provider sidecar rotates short-lived MTLS creds with zero latency. Add a lightweight tracing sidecar to ship spans non-blockingly. All services on one overlay network. Implement zero-downtime rotation with health checks and automatic rollback on failure. Provide docker-compose skeleton and bootstrap steps?","answer":"Mutual TLS between Nginx and the API using CA and service certs as Docker secrets (ca.pem, api.crt/api.key). Add a Vault-backed secret-provider sidecar to rotate short-lived MTLS creds with zero laten","explanation":"## Why This Is Asked\nTests mastery of secure service-to-service communication, secret rotation, and non-disruptive deployments in docker-dca. Emphasizes Safer-by-design patterns and observable behavior under rotation.\n\n## Key Concepts\n- Mutual TLS between gateway and API with Docker secrets\n- Vault-based secret provisioning for dynamic cert rotation\n- Sidecar patterns: tracing and credential rotation with no request latency\n- Overlay networking, zero-downtime updates, health-check driven rollback\n- Bootstrap steps: swarm init/join, secret/config creation, compose deploy\n\n## Code Example\n```yaml\nversion: \"3.8\"\nservices:\n  nginx:\n    image: nginx:stable-alpine\n    ports:\n      - \"443:443\"\n    secrets:\n      - source: ca.pem\n        target: ca.pem\n    configs:\n      - source: api-endpoints\n        target: /etc/nginx/conf.d/api-endpoints.conf\n  api:\n    image: myorg/api:latest\n    secrets:\n      - source: ca.pem\n      - source: api.crt\n      - source: api.key\n    deploy:\n      replicas: 2\n  secret-provider:\n    image: vault-bridge:latest\n    environment:\n      - VAULT_ADDR=http://vault:8200\n    depends_on:\n      - vault\n  tracing:\n    image: tracing-collector:latest\n    depends_on:\n      - api\n```\n\n## Follow-up Questions\n- How would you verify rotation safety under peak traffic?\n- What metrics would you collect to detect failed rotations early?\n- How would you handle secret revocation if a node goes offline?","diagram":null,"difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T09:45:46.300Z","createdAt":"2026-01-17T09:45:46.301Z"},{"id":"q-3297","question":"In a two-node docker-dca Swarm, deploy a TLS-enabled stateless API behind an Nginx proxy. Instrument the API with OpenTelemetry and run an OpenTelemetry Collector as a sidecar in each service, exporting traces to a Jaeger backend running on the swarm. Use Docker secrets for TLS certs and a Docker config for API endpoints. Attach both services to a single overlay network. Set 50% sampling; verify traces in Jaeger; ensure per-request latency overhead stays under 2 ms. Provide exact docker-compose.yml skeleton and minimal OTEL/Jaeger config?","answer":"Instrument the API with OpenTelemetry, run a per-task OTEL Collector sidecar, and export traces to Jaeger via OTLP. TLS via Docker secrets; TLS termination at Nginx. All services on a single overlay n","explanation":"## Why This Is Asked\n\nAssessing observability discipline in Docker Swarm: instrumenting code, sidecar collectors, and end-to-end tracing with Jaeger, while validating latency overhead.\n\n## Key Concepts\n\n- OpenTelemetry instrumentation and OTLP exporters\n- Sidecar collector pattern in Swarm\n- Jaeger backend and sampling configuration\n- Docker secrets for TLS and Docker configs for routing\n- Overlay network communication and minimal overhead\n\n## Code Example\n\n```yaml\n# docker-compose skeleton (draft)\nversion: '3.8'\nservices:\n  api:\n    image: myapi:latest\n    secrets:\n      - tls_cert\n      - tls_key\n    configs:\n      - source: endpoints\n        target: /config/endpoints.json\n    networks:\n      - overlay\n    deploy:\n      replicas: 1\n  otel-collector:\n    image: otel/opentelemetry-collector-contrib:0.63.0\n    volumes:\n      - ./otel-config.yaml:/conf/otel-config.yaml\n    command: [\"--config\", \"/conf/otel-config.yaml\"]\n    networks:\n      - overlay\n    deploy:\n      replicas: 1\nnetworks:\n  overlay:\n    driver: overlay\nsecrets:\n  tls_cert:\n    file: ./certs/api.crt\n  tls_key:\n    file: ./certs/api.key\nconfigs:\n  endpoints:\n    file: ./config/endpoints.json\n```\n\n## Follow-up Questions\n\n- How would you scale the OTEL collectors in swarm?\n- How to adjust sampling dynamically without redeploying?\n","diagram":null,"difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T10:35:51.882Z","createdAt":"2026-01-17T10:35:51.882Z"},{"id":"q-3395","question":"**Advanced Docker DCA Challenge**: In a three-node docker-dca cluster across two data centers, deploy a TLS-enabled, stateful API behind an Nginx edge proxy. Use Docker secrets for TLS, a Docker config for API endpoints, and a sidecar that ships logs to a central ELK stack with zero latency. Implement blue-green deploy with health checks and automatic rollback; include exact commands and a minimal docker-compose.yml, plus how to measure performance during rollout?","answer":"Deploy plan: create TLS secret and API endpoints config, run a two-variant stack (blue/green) behind Nginx, wire a sidecar log-shipper, attach to overlay net, and use health checks with automatic roll","explanation":"## Why This Is Asked\nTests ability to design cross-datacenter, TLS-terminated, stateful deployment with zero-downtime, plus observability-driven rollbacks.\n\n## Key Concepts\n- docker-dca, TLS secrets, Docker configs, overlay networks\n- blue-green rollout, health checks, automatic rollback\n- sidecar logging without request latency, stateful volume handling\n\n## Code Example\n```yaml\nversion: '3.8'\nservices:\n  api:\n    image: myorg/api:latest\n    deploy:\n      replicas: 2\n    configs:\n      - source: api-endpoints\n        target: /etc/api/endpoints.json\n    secrets:\n      - source: tls-cert\n        target: tls.crt\n    networks:\n      - overlay\n    volumes:\n      - api-data:/var/lib/api\n  sidecar:\n    image: log-shipper:latest\n    deploy:\n      replicas: 1\n    volumes:\n      - api-logs:/var/log/api\n    networks:\n      - overlay\nnetworks:\n  overlay:\n    driver: overlay\nvolumes:\n  api-data:\n  api-logs:\nsecrets:\n  tls-cert:\n    file: tls/cert.pem\nconfigs:\n  api-endpoints:\n    file: configs/endpoints.json\n```\n\n## Follow-up Questions\n- How would you test rollback behavior under latency spikes?\n- What observability signals and thresholds ensure safe rollout?","diagram":null,"difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","NVIDIA","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T14:35:44.736Z","createdAt":"2026-01-17T14:35:44.736Z"},{"id":"q-3425","question":"In a three-node docker-dca cluster spanning on-prem and cloud, implement a TLS-enabled, multi-tenant API gateway behind an Nginx edge. Use one TLS secret per tenant, per-tenant rate limits via a small config mounted as a Docker config, and a Redis-backed counter. Add a canary rollout for a new per-tenant routing rule driven by a Consul KV flag, with health checks and automatic rollback on failure. Provide exact commands and a docker-compose.yml skeleton?","answer":"Use a per-tenant TLS secret and an Nginx map keyed by host to select the certs, plus a Redis-backed rate limiter per tenant read via a small gateway helper. Apply a canary rollout controlled by a Cons","explanation":"## Why This Is Asked\nTests ability to design multi-tenant TLS handling, dynamic routing, and safe deployments across hybrid clusters, tying together secrets, configs, and feature flags.\n\n## Key Concepts\n- Docker secrets/configs per tenant\n- Nginx TLS SNI-based cert selection\n- Redis-backed per-tenant rate limiting\n- Consul KV feature flags for canary routing\n- Swarm update_config with rollback on failure\n\n## Code Example\n```yaml\nversion: '3.8'\nservices:\n  edge:\n    image: nginx:1.25\n    ports:\n      - '443:443'\n    secrets:\n      - tenantA.crt\n      - tenantA.key\n      - tenantB.crt\n      - tenantB.key\n    configs:\n      - source: nginx_tenant_map\n        target: /etc/nginx/conf.d/tenant_map.conf\n    deploy:\n      replicas: 3\n      update_config:\n        parallelism: 1\n        delay: 5s\n        order: start-first\n        failure_action: rollback\nconfigs:\n  nginx_tenant_map:\n    file: ./nginx/tenant_map.conf\n```\n\n## Follow-up Questions\n- How would you test canary rollout without affecting production traffic?\n- How would you audit tenant isolation and key rotation?\n- What are the failure modes if Redis is unreachable during rollout?","diagram":"flowchart TD\n  A[Client] --> B[Nginx Edge]\n  B --> C[Tenant API]\n  B --> D[Canary Route]\n  D --> E[Health Checks]\n  E --> F[Rollback on Failure]","difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Snowflake","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-17T15:39:44.805Z","createdAt":"2026-01-17T15:39:44.805Z"},{"id":"q-3592","question":"In a docker-dca Swarm across two data centers, add end-to-end distributed tracing to a TLS-enabled API behind an Nginx proxy. Deploy an OpenTelemetry Collector as a sidecar on the API service and a central collector, propagate W3C trace context, configure sampling via Docker config, and export via OTLP to Jaeger. Measure and keep overhead under 1 ms per request; provide commands and a docker-compose skeleton?","answer":"Instrument the API with OpenTelemetry, deploy a sidecar collector per replica, propagate W3C trace context across services, and export traces via OTLP to Jaeger. Configure sampling through Docker configs, ensure cross-datacenter trace delivery, and maintain sub-1ms overhead through optimized collector configuration and proper resource allocation.","explanation":"## Why This Is Asked\n\nThis question evaluates your ability to design and implement distributed tracing across a multi-datacenter Docker Swarm environment while meeting strict performance requirements and handling TLS/Nginx integration complexities.\n\n## Key Concepts\n\n- OpenTelemetry instrumentation and configuration\n- W3C trace context propagation across service boundaries\n- OTLP (OpenTelemetry Protocol) exporters and data flow\n- Jaeger backend integration for trace storage and visualization\n- Docker secrets and configs for secure configuration management\n- Multi-datacenter deployment considerations and latency optimization\n- Sidecar patterns for collector deployment\n- Performance monitoring and overhead control strategies\n\n## Code Example\n\n```yaml\n# docker-compose.yml skeleton for distributed tracing\nversion: '3.8'\nservices:\n  api:\n    image: my-api:latest\n    deploy:\n      replicas: 2\n    environment:\n      - OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317\n      - OTEL_SERVICE_NAME=my-api\n      - OTEL_RESOURCE_ATTRIBUTES=service.name=my-api\n    depends_on:\n      - otel-collector\n  otel-collector:\n    image: otel/opentelemetry-collector-contrib:latest\n    deploy:\n      replicas: 1\n    volumes:\n      - ./otel-collector-config.yml:/etc/otelcol-contrib/config.yaml\n    command: [\"--config=/etc/otelcol-contrib/config.yaml\"]","diagram":"flowchart TD\n  API[API service]\n  Nginx[Nginx proxy]\n  OTEL[OpenTelemetry Collector]\n  Jaeger[Jaeger backend]\n  API --> Nginx\n  Nginx --> API\n  API --> OTEL\n  OTEL --> Jaeger","difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","Two Sigma","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T05:31:47.090Z","createdAt":"2026-01-17T22:39:07.222Z"},{"id":"q-3762","question":"In a three-node docker-dca Swarm, deploy a TLS-enabled stateless API behind an Nginx proxy. Use Docker secrets for TLS certs, a Docker config for API endpoints, and add a small compliance sidecar that streams headers and audit data to a central collector via UDP without affecting request latency. Attach services to a single overlay network and implement a progressive canary that also validates a policy flag (X-Policy: strict) in 5% increments every 20s with health checks and automatic rollback on failure. Provide exact commands and a docker-compose.yml skeleton?","answer":"Outline a three-node Swarm deployment using TLS secrets, a config for endpoints, an Nginx proxy, an API service, and a lightweight compliance sidecar that UDPs audit data to a collector with no measur","explanation":"Why This Is Asked\n- Tests ability to design a tight, auditable, policy-driven deployment with sidecars that don’t impact latency.\n\nKey Concepts\n- Docker secrets for TLS, Docker configs for runtime endpoints\n- Sidecar pattern for compliance/audit without altering app logic\n- UDP-based, zero-latency streaming to a centralized collector\n- Canary rollout with policy validation and automatic rollback\n- Proper health checks and overlay network wiring\n\nCode Example\n```yaml\nversion: '3.8'\nservices:\n  nginx:\n    image: nginx:stable\n    ports:\n      - 443:443\n    deploy:\n      replicas: 1\n      update_config:\n        parallelism: 1\n        order: start-first\n    secrets:\n      - tls_cert\n      - tls_key\n    configs:\n      - source: endpoints\n        target: /etc/endpoints.json\n    networks:\n      - overlay-net\n  api:\n    image: my/api:latest\n    deploy:\n      replicas: 1\n      update_config:\n        parallelism: 1\n        order: start-first\n        failure_action: rollback\n    secrets:\n      - tls_cert\n      - tls_key\n    configs:\n      - source: endpoints\n        target: /etc/endpoints.json\n    networks:\n      - overlay-net\n  compliance:\n    image: compliance-sidecar:latest\n    depends_on:\n      - api\n    networks:\n      - overlay-net\n    deploy:\n      resources:\n        limits:\n          cpus: '0.1'\n          memory: 128M\nsecrets:\n  tls_cert:\n    file: tls/api.crt\n  tls_key:\n    file: tls/api.key\nconfigs:\n  endpoints:\n    file: endpoints.json\nnetworks:\n  overlay-net:\n    external: true\n```\n\nFollow-up Questions\n- How would you handle cert rotation without restarting services?\n- How would you simulate and verify rollback behavior under load?","diagram":null,"difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T08:43:24.663Z","createdAt":"2026-01-18T08:43:24.664Z"},{"id":"q-3803","question":"In a three-node docker-dca cluster across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy. Use Vault to issue SPIFFE SVIDs, and a sidecar that validates them and enforces an OPA policy loaded via a Docker Config; include a lightweight log-shipper. Attach to a single overlay network. Implement a 20% canary rollout with latency/errors thresholds and automatic rollback. Provide exact commands and a docker-compose.yml skeleton?","answer":"Outline a zero-trust path: in a three-node docker-dca cluster across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy. Use Vault to issue SPIFFE SVIDs, and a per-app sidecar that ","explanation":"## Why This Is Asked\nAssesses ability to design zero-trust with SPIFFE, Vault-based certs, policy enforcement, and safe rollouts in Docker DCA.\n\n## Key Concepts\n- Zero-trust networking\n- SPIFFE/SVIDs and Vault integration\n- Sidecar pattern for mTLS validation\n- OPA policies via Docker Config\n- Canary rollout safety checks\n\n## Code Example\n```yaml\nversion: '3.8'\nservices:\n  edge-nginx:\n    image: nginx:alpine\n    ports:\n      - \"443:443\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n      - secrets:/secrets:ro\n  api:\n    image: myapi:latest\n    secrets:\n      - spa-cert\nvolumes:\n  secrets:\n    external: true\n```\n\n## Follow-up Questions\n- How would you rotate SPIFFE certs without downtime?\n- How would you test policy changes safely?","diagram":null,"difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T10:31:32.924Z","createdAt":"2026-01-18T10:31:32.924Z"},{"id":"q-3933","question":"In a two-node docker-dca Swarm, deploy a TLS-enabled REST API behind Traefik as the edge proxy. Configure Traefik's ACME to obtain TLS certificates from Let's Encrypt in staging. Attach all services to a single overlay network. Implement a 15% canary rollout of a new /status/v2 endpoint (v1 remains) using Traefik's weighted routing with health checks, automatic rollback on failures. Add a minimal OpenTelemetry tracing sidecar exporting to stdout; ensure trace headers propagate?","answer":"Set up a two-node docker-dca Swarm with Traefik as the edge proxy, TLS via Let’s Encrypt staging, and a single overlay network. Deploy api-v1 and a canary api-v2; steer 15% traffic to v2 using Traefik","explanation":"## Why This Is Asked\n\nTests ability to configure a lightweight canary in a real swarm, using Traefik as a modern edge proxy, and integrate observability with OpenTelemetry. It checks practical networking, TLS setup, and traffic shaping without requiring complex multi-cluster setups.\n\n## Key Concepts\n\n- Docker Swarm overlay networks and service deployment\n- Traefik ACME TLS with staging\n- Weighted canary routing and health-check-based rollback\n- Lightweight OpenTelemetry tracing (export to stdout) and header propagation\n\n## Code Example\n\n```javascript\n// Example: simple trace header propagation (conceptual)\nconst http = require('http');\nhttp.get({ hostname: 'api.example', path: '/status', headers: { 'Trace-Id': 'abc123' } }, (res) => {\n  // handle response\n});\n```\n\n## Follow-up Questions\n\n- How would you monitor the canary rollout with metrics and logs?\n- How to handle TLS certificate rotation during canary without downtime?","diagram":null,"difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Scale Ai","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T15:47:17.632Z","createdAt":"2026-01-18T15:47:17.632Z"},{"id":"q-3944","question":"In a three-node docker-dca Swarm within a single data center, deploy a TLS-enabled API behind an Nginx edge proxy. Use Docker Secrets for TLS certificates and a Docker Config to supply a canary routing rule. Add a lightweight log-shipper as a sidecar and attach both services to a single overlay network. Implement a 15% canary rollout of a new API image with automatic rollback on failed health checks. Provide exact commands and a docker-compose.yml skeleton?","answer":"Proposed approach: three-node Swarm, TLS secrets mounted by nginx, a config with canary routing, and a log-shipper container. Deploy api and nginx on an overlay network; implement 15% canary by runnin","explanation":"## Why This Is Asked\nTests understanding of basic Swarm deployment, secret/config handling, and safe canary rollouts with health checks.\n\n## Key Concepts\n- Docker Secrets and Configs\n- Overlay networks and Swarm deploy\n- Canary rollouts and rollback on health failure\n- Basic log-shipper sidecar\n\n## Code Example\n```yaml\nversion: '3.8'\nservices:\n  api:\n    image: myorg/api:latest\n    secrets:\n      - tls.crt\n      - tls.key\n    configs:\n      - source: nginx_canary_conf\n        target: /etc/nginx/conf.d/canary.conf\n    deploy:\n      replicas: 3\n      update_config:\n        parallelism: 1\n        order: start-first\n      health_check:\n        test: [\"CMD-SHELL\", \"curl -f https://localhost/health || exit 1\"]\n        interval: 10s\n        timeout: 5s\n        retries: 3\n    networks:\n      - overlay_net\n  nginx:\n    image: nginx:alpine\n    secrets:\n      - tls.crt\n      - tls.key\n    configs:\n      - source: nginx_canary_conf\n        target: /etc/nginx/conf.d/canary.conf\n    deploy:\n      replicas: 2\n      update_config:\n        parallelism: 1\n        order: start-first\n    networks:\n      - overlay_net\n  log-shipper:\n    image: myorg/log-shipper:latest\n    volumes:\n      - logs:/var/log/app\n    deploy:\n      replicas: 1\n    networks:\n      - overlay_net\nvolumes:\n  logs:\nnetworks:\n  overlay_net:\n    driver: overlay\n```\n\n## Follow-up Questions\n- How would you verify the 15% canary routing meets latency SLA?\n- How would you automate secret rotation without downtime?","diagram":"flowchart TD\n  A[User Request] --> B[Swarm Manager]\n  B --> C[Nginx Edge Proxy]\n  B --> D[API Replica Set]\n  C --> E[Canary Routing Config]","difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T16:42:23.443Z","createdAt":"2026-01-18T16:42:23.444Z"},{"id":"q-3981","question":"In a four-node docker-dca cluster across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy. Use Vault to issue SPIFFE SVIDs; attach an Envoy sidecar per app to validate them and enforce tenant RBAC via OPA policies stored in Consul KV (refreshed via webhook). Attach all to one overlay network. Implement a progressive canary rollout of 15% steps every 30s with latency >200ms or error rate >2% triggering rollback. Provide exact commands and a docker-compose.yml skeleton?","answer":"Design multi-DC deployment with Vault SPIFFE SVIDs, Envoy sidecar enforcing per-tenant RBAC via OPA in Consul KV, and Nginx edge. Canary 15% every 30s, rollback on latency >200ms or error rate >2%. In","explanation":"## Why This Is Asked\nTests capability for complex, real-world docker-dca setups: SPIFFE SVIDs from Vault, Envoy as a policy-enforcing proxy, Consul KV-driven OPA policies, and geo-distributed canary rollouts with strict SLA boundaries.\n\n## Key Concepts\n- Multi-DC docker-dca deployment\n- Vault SPIFFE SVID provisioning\n- Envoy sidecar with mTLS and OPA RBAC via Consul KV\n- Webhook-driven policy refresh\n- Canary rollout discipline with latency/error budgets and automatic rollback\n\n## Code Example\n```javascript\n# docker-compose.yml skeleton (YAML content displayed in a JS code block for formatting)\nversion: '3.8'\nservices:\n  edge:\n    image: nginx:stable\n    ports:\n      - \"443:443\"\n    networks:\n      - web\n  api:\n    image: myapi:latest\n    networks:\n      - web\n    depends_on:\n      - edge\n  envoy:\n    image: envoyproxy/envoy-alpine:v1.25.0\n    networks:\n      - web\n# TLS, SPIFFE, and policy integration are configured via mounts/ENVs in real setup\nnetworks:\n  web:\n    driver: overlay\n```\n\n## Follow-up Questions\n- How would you measure policy evaluation latency and ensure webhook refresh doesn’t become a bottleneck?\n- How would you simulate a DC failure and verify rollback guarantees in production-like conditions?","diagram":"flowchart TD\n  Client([Client]) --> Edge[Nginx Edge Proxy]\n  Edge --> App[App Service]\n  App --> Env[Envoy Sidecar]\n  Env --> Vault[Vault SPIFFE SVIDs]\n  Env --> Polis[OPA Policies (Consul KV)]\n  Client -. TLS .-> Edge","difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Plaid","Square","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-18T18:41:16.663Z","createdAt":"2026-01-18T18:41:16.663Z"},{"id":"q-4237","question":"Three-node docker-dca Swarm across two data centers; deploy a TLS-enabled API behind an Nginx edge proxy. Enforce a Cosign-based supply-chain policy: only attested images for API and sidecars; add a gate sidecar that verifies attestations at startup with Vault keys. Attach to a single overlay network. Implement a 10% canary rollout with health checks and automatic rollback. Provide exact commands and docker-compose skeleton?","answer":"Use Cosign attestations for the API and all sidecars; store keys in Vault and provide a gate sidecar that runs cosign verify on startup to reject non-attested images. Route through TLS-enabled Nginx e","explanation":"## Why This Is Asked\nTests real-world supply-chain security in container deployments, using Cosign attestations, Vault-managed keys, and a gate sidecar to enforce policy before service start. Combines Swarm networking, TLS, and canary rollback under load.\n\n## Key Concepts\n- Image attestation with Cosign\n- Vault for key distribution and rotation\n- Gate sidecar for startup verification\n- Nginx TLS edge proxy integration\n- Canary rollout with health checks and rollback\n- Overlay network in Swarm\n\n## Code Example\n```javascript\n// gate sidecar startup check (pseudo)\nasync function verify(image){/* cosign verify against Vault key */}\nverify(process.env.IMAGE);\n```\n\n## Follow-up Questions\n- How would you test attestation failure scenarios in CI/CD?\n- How would you rotate Cosign keys without downtime?","diagram":null,"difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Netflix","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T09:51:29.455Z","createdAt":"2026-01-19T09:51:29.455Z"},{"id":"q-4268","question":"In a two-node docker-dca cluster, deploy a TLS-enabled API behind an Nginx edge proxy. Use Redis as a feature flag to roll a 20% canary to a v2 endpoint, controlled by flag feature.canary.v2. Implement latency and error-rate thresholds (latency <300ms, error rate <5%) with automatic rollback. Provide exact commands and a docker-compose.yml skeleton to implement this setup?","answer":"Proposed approach: 2-node docker-dca Swarm with an Nginx TLS edge proxy and TLS certs in Secrets. Deploy Redis as the feature-flag store; canary routing sends 20% to /api/v2 when feature.canary.v2=1. ","explanation":"## Why This Is Asked\nTests practical canary rollout using a lightweight feature flag store, edge routing, and rollback logic. It also covers TLS handling and essential orchestration setup.\n\n## Key Concepts\n- Canary routing via feature flags in Redis\n- TLS termination at Nginx edge proxy\n- Lightweight health checks and rollback policy\n- Docker Compose/Swarm bootstrap and service interdependence\n\n## Code Example\n```yaml\n# docker-compose.yml (skeleton)\nversion: '3.8'\nservices:\n  edge:\n    image: nginx:alpine\n    ports:\n      - \"443:443\"\n    secrets:\n      - tls-cert\n    configs:\n      - source: nginx.conf\n        target: /etc/nginx/nginx.conf\n  api:\n    image: myorg/api:latest\n  redis:\n    image: redis:6-alpine\nsecrets:\n  tls-cert:\n    file: ./certs/server.crt\n  tls-key:\n    file: ./certs/server.key\nbr:\n  version: '3.8'\n```\n\n## Follow-up Questions\n- How would you extend this to 3+ nodes across data centers?\n- What metrics and dashboards would you build to verify rollout health?","diagram":"flowchart TD\n  A(Client) --> B[Edge TLS Nginx]\n  B --> C[API v1]\n  B --> D[API v2 (canary)]\n  D --> E[Redis canary flag]\n  E --> F[Health/Latency Monitor]\n  F --> G{Threshold breached?}\n  G -->|Yes| H[Rollback & flip flag]\n  G -->|No| I[Continue canary]","difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T10:58:50.382Z","createdAt":"2026-01-19T10:58:50.382Z"},{"id":"q-4345","question":"In a three-node docker-dca cluster across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy. Enforce image provenance with Docker Content Trust (Notary) by signing base images and validating signatures in a dedicated 'sigver' sidecar before the request reaches the app. Use Vault to issue SPIFFE SVIDs; enforce an OPA policy loaded via a Docker Config to gate access. Implement a 20% canary rollout with latency and error-rate thresholds and automatic rollback. Provide exact commands and a docker-compose.yml skeleton?","answer":"I would sign images with Docker Content Trust, add a lightweight 'sigver' sidecar that validates signatures against Notary on container startup and before traffic; issue SVIDs with Vault for mTLS; fet","explanation":"## Why This Is Asked\nTests ability to design end-to-end trust from image provenance to runtime policy enforcement, plus canary control and rollback.\n\n## Key Concepts\n- Docker Content Trust/Notary for image provenance\n- SPIFFE/SVIDs via Vault for mTLS\n- OPA policies loaded via Docker Config\n- Sigver sidecar for per-request validation\n- Canary rollout with latency/error thresholds and rollback\n\n## Code Example\n```javascript\n// Pseudo example: verify image signature before starting app\nasync function verifySignature(image) {\n  const sig = await notary.verify(image);\n  if (!sig || !sig.valid) throw new Error('invalid image');\n  return true;\n}\n```\n\n## Follow-up Questions\n- How would you handle signature revocation and key rotation in this setup?\n- How would you scale the sigver and policy sidecars across nodes?\n","diagram":"flowchart TD\n  Client(Client) --> Edge[Nginx Edge Proxy]\n  Edge --> SigVer[Signature Verifier Sidecar]\n  SigVer --> App[App Container]\n  App --> Policy[OPA Policy Enforcer]\n  Policy --> Vault[Vault SPIFFE SVIDs]\n","difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","LinkedIn","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T14:53:51.311Z","createdAt":"2026-01-19T14:53:51.312Z"},{"id":"q-4384","question":"In a three-node docker-dca cluster across two data centers, implement a per-tenant dynamic rate limiter as an Envoy-sidecar attached to the API container. The limiter reads quotas from Vault KV, caches in Redis, and uses a token-bucket or sliding-window approach. Edge TLS is terminated by Nginx; tenants are identified by a JWT in Authorization header validated by Vault OIDC. Provide exact commands and a docker-compose.yml skeleton?","answer":"Use Envoy as a per-tenant rate limiter sidecar tied to the API service. Validate JWT via Vault OIDC, fetch quotas from Vault KV on cache miss, store per-tenant state in Redis with short TTL, and enfor","explanation":"## Why This Is Asked\nTests ability to design per-tenant traffic control with dynamic quotas, secrets management, and a tight edge-data plane boundary. Requires knowledge of Envoy filters, Vault, Redis, and container wiring.\n\n## Key Concepts\n- Per-tenant rate limiting with Envoy sidecar\n- Vault KV storage and OIDC for JWT validation\n- Redis caching and TTL for quota state\n- Edge TLS termination with Nginx and secure service-to-service routing\n\n## Code Example\n```yaml\n# docker-compose.yml sketch\nversion: '3.8'\nservices:\n  edge-nginx:\n    image: nginx:alpine\n    ports:\n      - \"443:443\"\n    volumes:\n      - ./nginx/conf.d:/etc/nginx/conf.d\n  api:\n    image: myorg/api:latest\n  envoy-svc:\n    image: envoyproxy/envoy:v1.26.0\n    depends_on:\n      - api\n  vault:\n    image: vault:1.14\n  redis:\n    image: redis:7\n```\n\n## Follow-up Questions\n- How would you rotate JWT signing keys without downtime?\n- How would you scale quotas across multiple regions?\n","diagram":"flowchart TD\n  A[Client Request] --> B[Nginx Edge TLS]\n  B --> C[Envoy RateLimiter Sidecar]\n  C --> D[API Service]\n  D --> E[(Redis Cache)]\n  E --> F[Vault KV: quotas]","difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","OpenAI","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T16:49:30.908Z","createdAt":"2026-01-19T16:49:30.908Z"},{"id":"q-4491","question":"In a two-node docker-dca cluster, deploy a TLS-enabled API behind an Nginx edge proxy. Implement 10% canary routing to a v2 image using a Lua-based split controlled by a Docker Config flag. Enforce image provenance with cosign; reject unsigned images at the edge. Provide exact commands and a docker-compose.yml skeleton?","answer":"Cosign sign both images: cosign sign myapi:v1; cosign sign myapi:v2. Deploy nginx edge with a Lua script to route 10% of requests to v2 via a canary_percent flag from Docker Config. Verify signatures ","explanation":"## Why This Is Asked\nPractical edge-routing scenario with basic security.\n\n## Key Concepts\n- TLS termination at edge\n- Lua-based traffic split controlled by Docker Config\n- Image provenance with cosign\n- Canary and rollout basics\n- Docker Secrets/Configs usage\n\n## Code Example\n```lua\n-- nginx Lua for canary routing (conceptual)\nmath.randomseed(ngx.now())\nlocal rate = tonumber(ngx.shared.canary_ratio:get('v2') or '0.1')\nif math.random() < rate then\n  ngx.exec('@v2')\nelse\n  ngx.exec('@v1')\nend\n```\n\n## Follow-up Questions\n- How would you extend to multiple regions with per-region canaries?\n- How would you instrument and alert on canary success/failure?","diagram":"flowchart TD\n  A[Edge proxy] --> B[TLS terminated]\n  B --> C[Canary check]\n  C --> D[Back-end v1]\n  C --> E[Back-end v2]","difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Scale Ai","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-19T20:53:55.044Z","createdAt":"2026-01-19T20:53:55.044Z"},{"id":"q-4507","question":"In a two-node docker-dca Swarm, deploy a TLS-enabled API behind an Nginx edge proxy and enforce mutual TLS (mTLS) with client certificates issued by a private CA. Store server and CA certs as Docker secrets, configure Nginx to require and validate client certs, and ensure the API receives the client identity via a header. Provide exact docker-compose.yml skeleton and the certificate generation steps, plus test commands?","answer":"Generate a private CA, issue server and client certificates, and store them as Docker secrets. Configure Nginx with `ssl_verify_client on; ssl_client_certificate /certs/ca.pem; proxy_set_header X-Client-Subject $ssl_client_s_dn;` to enforce mutual TLS and propagate client identity to the backend API.","explanation":"## Why This Is Asked\nTests practical mTLS implementation, Docker secrets management, and edge proxy configuration in a Swarm environment. Evaluates operational skills for secure service-to-service communication.\n\n## Key Concepts\n- Docker secrets for TLS certificate management\n- Nginx mutual TLS verification and client certificate validation\n- Client identity propagation to backend services via HTTP headers\n\n## Code Example\n```yaml\n# skeleton docker-compose.yml\nversion: '3.9'\nservices:\n  edge:\n    image: nginx:alpine\n    ports:\n      - '443:443'\n    secrets:\n      - source: nginx_server_cert\n        target: server.crt\n      - source: nginx_server_key\n        target: server.key\n      - source: nginx_ca_cert\n        target: ca.pem\n    configs:\n      - source: nginx_config\n        target: /etc/nginx/nginx.conf\n  api:\n    image: your-api:latest\n    deploy:\n      replicas: 2\nsecrets:\n  nginx_server_cert:\n    external: true\n  nginx_server_key:\n    external: true\n  nginx_ca_cert:\n    external: true\nconfigs:\n  nginx_config:\n    external: true\n```\n\n## Certificate Generation\n```bash\n# Create private CA\nopenssl genrsa -out ca.key 4096\nopenssl req -new -x509 -days 365 -key ca.key -out ca.pem\n\n# Generate server certificate\nopenssl genrsa -out server.key 2048\nopenssl req -new -key server.key -out server.csr\nopenssl x509 -req -in server.csr -CA ca.pem -CAkey ca.key -CAcreateserial -out server.crt -days 365\n\n# Generate client certificate\nopenssl genrsa -out client.key 2048\nopenssl req -new -key client.key -out client.csr\nopenssl x509 -req -in client.csr -CA ca.pem -CAkey ca.key -CAcreateserial -out client.crt -days 365\n\n# Create Docker secrets\ndocker secret create nginx_server_cert server.crt\ndocker secret create nginx_server_key server.key\ndocker secret create nginx_ca_cert ca.pem\n```\n\n## Testing Commands\n```bash\n# Test with client certificate\ncurl --cert client.crt --key client.key --cacert ca.pem https://your-service:443/api\n\n# Test without client certificate (should fail)\ncurl --cacert ca.pem https://your-service:443/api\n\n# Verify client identity header\ncurl --cert client.crt --key client.key --cacert ca.pem https://your-service:443/api -v | grep X-Client-Subject\n```","diagram":null,"difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T05:27:44.837Z","createdAt":"2026-01-19T21:48:03.538Z"},{"id":"q-4640","question":"Across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy in a docker-dca cluster. Enforce mTLS between services with SPIFFE SVIDs issued by Vault, and attach a sidecar that validates the SVIDs and enforces an OPA policy loaded via Docker Config for per-tenant feature gates. Implement a canary rollout with traffic shifting and automatic rollback. Include exact bootstrap commands and a minimal docker-compose skeleton?","answer":"To solve this, terminate TLS at the edge, enable mTLS inside the cluster via Vault-issued SPIFFE SVIDs, and run a cert-verifying sidecar next to each service. The OPA policy, stored in a Docker Config","explanation":"## Why This Is Asked\nTests ability to design multi-DC, mTLS, SPIFFE, Vault, OPA, canary strategy in docker-dca, with real-world constraints and measurable rollouts.\n\n## Key Concepts\n- mTLS inside cluster, SPIFFE SVIDs, Vault integration\n- Sidecar pattern for security and policy enforcement\n- Docker Config-based policies and per-tenant gates\n- Canary rollouts and automatic rollback using traffic shaping\n- Observability thresholds and rollback criteria\n\n## Code Example\n```bash\n# bootstrap sketch\ndocker swarm init\nvault server -config=config.hcl\nvault write ... # issue SVIDs\n```\n```\n\n## Follow-up Questions\n- How to test canary path under latency spikes?\n- How to rotate SVIDs without downtime?\n```\n","diagram":null,"difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T05:57:01.519Z","createdAt":"2026-01-20T05:57:01.519Z"},{"id":"q-4660","question":"In a two-data-center docker-dca cluster, design a cross-tenant image attestation workflow that runs before deployment: an attest sidecar validates base image digests against an external Notary v2 attestation service, with tenant policies stored in an OPA Docker Config and enforced by the edge proxy. Implement a 10% canary rollout with latency budgets and automatic rollback. Provide exact commands and a docker-compose.yml skeleton?","answer":"Describe an image-attestation workflow for docker-dca: an attest sidecar calls an external Notary v2 attestor to verify the image digest against a tenant-scoped policy stored in an OPA Docker Config; ","explanation":"## Why This Is Asked\n\nTests ability to design a pre-deploy attestation pipeline using Notary v2, OPA, and canary strategies in a docker-dca multi-DC setup.\n\n## Key Concepts\n\n- Image attestation with Notary v2\n- Attestation sidecar pattern\n- OPA policies via Docker Config\n- Edge-proxy gating\n- Canary rollout with latency budgets\n\n## Code Example\n\n```yaml\nversion: \"3.9\"\nservices:\n  edge:\n    image: nginx:stable\n  attest:\n    image: attestor/notary-v2:latest\n  app:\n    image: registry.example/tenant-app:latest\n    depends_on:\n      - attest\n    environment:\n      - TENANT=tenant-a\n```\n\n## Follow-up Questions\n\n- How would you rotate attestation keys and revoke compromised policies?\n- How would you measure success during canary and trigger rollback?","diagram":null,"difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Oracle","Plaid","Slack"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-20T07:05:32.234Z","createdAt":"2026-01-20T07:05:32.234Z"},{"id":"q-4996","question":"In a two-node docker-dca cluster, deploy a TLS-enabled API behind a Traefik edge proxy. Use a lightweight in-cluster CA (OpenSSL) to issue and rotate certificates for both the API and the proxy; store certs in Docker Secrets and configure mTLS between Traefik and the API. Add a tiny feature flag service via Docker Config that toggles a field in the API response per-tenant. Implement a 10% canary routing to a forked backend; provide a docker-compose skeleton and exact bootstrap steps?","answer":"Bootstrap a lightweight OpenSSL CA, issue certificates for Traefik and the API, and load them as Docker Secrets. Configure Traefik to enforce mTLS with clientAuth and TLS settings, route via TLS to the API, add a canary backend with weighted routing, and implement feature flags using Docker Config for per-tenant toggles.","explanation":"## Why This Is Asked\n\nTests comprehensive TLS setup, mTLS implementation without external tools like Vault, feature flag management with Docker Config, and canary deployment strategies in a beginner-friendly Docker Swarm scenario.\n\n## Key Concepts\n\n- **TLS mutual authentication** with a lightweight in-cluster CA using OpenSSL\n- **Docker Secrets** for secure certificate storage and management\n- **Docker Config** for per-tenant feature flag configuration\n- **Traefik edge proxy** dynamic routing and weighted canary deployments\n- **Certificate rotation** strategies and automated renewal processes\n- **Simple bootstrap steps** with a minimal docker-compose skeleton\n\n## Code Example\n\n```bash\n# Bootstrap OpenSSL CA\n#!/bin/bash\n# Create CA\nopenssl req -new -x509 -days 365 -nodes -out ca.crt -keyout ca.key -subj \"/CN=My-CA\"\n\n# Create Traefik certs\nopenssl req -new -nodes -out traefik.csr -keyout traefik.key -subj \"/CN=traefik\"\nopenssl x509 -req -in traefik.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out traefik.crt -days 365\n\n# Create API certs\nopenssl req -new -nodes -out api.csr -keyout api.key -subj \"/CN=api\"\nopenssl x509 -req -in api.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out api.crt -days 365\n\n# Create Docker Secrets\ndocker secret create ca.crt ca.crt\ndocker secret create traefik.key traefik.key\ndocker secret create traefik.crt traefik.crt\ndocker secret create api.key api.key\ndocker secret create api.crt api.crt\n\n# Create feature flag config\necho '{\"tenant_a\": {\"premium_features\": true}, \"tenant_b\": {\"premium_features\": false}}' > feature-flags.json\ndocker config create feature-flags feature-flags.json\n```\n\n```yaml\n# docker-compose.yml\nversion: '3.8'\n\nservices:\n  traefik:\n    image: traefik:v2.10\n    command:\n      - --api.insecure=true\n      - --providers.docker=true\n      - --entrypoints.websecure.address=:443\n      - --entrypoints.websecure.http.tls=true\n      - --entrypoints.websecure.http.tls.options=default\n      - --entrypoints.websecure.http.tls.certResolver=myresolver\n      - --certificatesresolvers.myresolver.acme.tlschallenge=true\n      - --serversTransport.insecureSkipVerify=false\n    ports:\n      - \"443:443\"\n      - \"8080:8080\"\n    secrets:\n      - ca.crt\n      - traefik.key\n      - traefik.crt\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n    deploy:\n      replicas: 1\n\n  api:\n    image: my-api:latest\n    environment:\n      - TLS_CERT_PATH=/run/secrets/api.crt\n      - TLS_KEY_PATH=/run/secrets/api.key\n      - CA_CERT_PATH=/run/secrets/ca.crt\n    secrets:\n      - api.crt\n      - api.key\n      - ca.crt\n    configs:\n      - feature-flags\n    deploy:\n      replicas: 2\n      labels:\n        - \"traefik.enable=true\"\n        - \"traefik.http.routers.api.rule=Host(`api.example.com`)\"\n        - \"traefik.http.routers.api.entrypoints=websecure\"\n        - \"traefik.http.routers.api.tls=true\"\n        - \"traefik.http.routers.api.tls.options=default\"\n        - \"traefik.http.services.api.loadbalancer.server.port=8443\"\n\n  api-canary:\n    image: my-api:canary\n    environment:\n      - TLS_CERT_PATH=/run/secrets/api.crt\n      - TLS_KEY_PATH=/run/secrets/api.key\n      - CA_CERT_PATH=/run/secrets/ca.crt\n    secrets:\n      - api.crt\n      - api.key\n      - ca.crt\n    configs:\n      - feature-flags\n    deploy:\n      replicas: 1\n      labels:\n        - \"traefik.enable=true\"\n        - \"traefik.http.routers.api-canary.rule=Host(`api.example.com`)\"\n        - \"traefik.http.routers.api-canary.entrypoints=websecure\"\n        - \"traefik.http.routers.api-canary.tls=true\"\n        - \"traefik.http.services.api-canary.loadbalancer.server.port=8443\"\n        - \"traefik.http.services.api.loadbalancer.mirrors.api-canary.weight=10\"\n\nsecrets:\n  ca.crt:\n    external: true\n  traefik.key:\n    external: true\n  traefik.crt:\n    external: true\n  api.key:\n    external: true\n  api.crt:\n    external: true\n\nconfigs:\n  feature-flags:\n    external: true\n```\n\n## Implementation Steps\n\n1. **Initialize CA infrastructure** using OpenSSL commands\n2. **Generate and sign certificates** for both Traefik and API services\n3. **Create Docker Secrets** for secure certificate distribution\n4. **Configure Traefik** with TLS and mTLS settings\n5. **Set up canary deployment** with 10% weighted routing\n6. **Implement feature flags** using Docker Config for tenant-specific toggles\n7. **Deploy stack** and verify TLS handshake and mTLS enforcement","diagram":null,"difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Coinbase","Google","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-21T04:32:31.359Z","createdAt":"2026-01-20T22:50:46.791Z"},{"id":"q-5175","question":"In a two-node docker-dca cluster, deploy a TLS-enabled API behind an Nginx edge proxy. Extend with OpenTelemetry tracing: instrument the API, add an otel-collector sidecar, and export traces to Jaeger; ensure trace context propagates through Nginx to the API; provide a docker-compose.yml skeleton and bootstrap commands; include a quick validation by hitting an endpoint and checking Jaeger UI?","answer":"Instrument the API with OpenTelemetry (Node/Python). Export traces via OTLP to an otel-collector sidecar, which forwards to Jaeger. Propagate trace-context through Nginx by enabling W3C traceparent he","explanation":"## Why This Is Asked\nTests ability to add observability with OpenTelemetry in a docker-dca context, ensuring end-to-end trace propagation from edge to service and basic validation.\n\n## Key Concepts\n- OpenTelemetry instrumentation and OTLP exporter\n- otel-collector as a sidecar and Jaeger backend\n- Trace context propagation (traceparent/tracestate)\n- docker-compose scaffolding for api, nginx, collector\n- Basic end-to-end validation via Jaeger UI\n\n## Code Example\n```javascript\n// Node.js OpenTelemetry setup (simplified)\nconst { NodeTracerProvider } = require('@opentelemetry/sdk-trace-node');\nconst { OTLPTraceExporter } = require('@opentelemetry/exporter-otlp-http');\nconst { SimpleSpanProcessor } = require('@opentelemetry/tracing');\n\nconst provider = new NodeTracerProvider();\nprovider.addSpanProcessor(new SimpleSpanProcessor(\n  new OTLPTraceExporter({ url: 'http://otel-collector:4318/v1/traces' })\n));\nprovider.register();\n```\n\n## Follow-up Questions\n- How would you adjust sampling and metric correlation to avoid perf impact on a busy API?\n- How would you test trace failures or collector downtime without losing observability?","diagram":null,"difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T09:45:25.513Z","createdAt":"2026-01-21T09:45:25.513Z"},{"id":"q-5338","question":"Two-node docker-dca cluster across two data centers. Deploy a TLS-enabled API behind an Nginx edge proxy. Implement distributed tracing with OpenTelemetry: run a Jaeger all-in-one collector, export traces via OTLP, and instrument the API with OpenTelemetry auto-instrumentation; sample traces at 20% and ensure trace context propagates through the edge to downstream service. Provide exact bootstrap commands and a minimal docker-compose.yml skeleton showing API, edge, and Jaeger integration?","answer":"Configure OpenTelemetry in edge and API with OTLP export to Jaeger, sampling 0.2, and propagate trace context. Use docker secrets for TLS on the edge; instrument API with auto-instrumentation. Bootstr","explanation":"## Why This Is Asked\nTests ability to integrate observability into a containerized edge API stack, ensuring TLS, cross-host tracing, and trace propagation.\n\n## Key Concepts\n- OpenTelemetry auto-instrumentation\n- OTLP export to Jaeger\n- Trace sampling and propagation\n- Docker Secrets for TLS\n- Nginx edge proxy integration\n\n## Code Example\n```bash\n# bootstrap example\ndocker secret create edge_cert edge.pem\ndocker secret create edge_key key.pem\n```\n\n## Follow-up Questions\n- How would you adjust sampling in prod vs test?\n- How would you verify traces across components with limited telemetry data?","diagram":null,"difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Google","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T17:33:21.380Z","createdAt":"2026-01-21T17:33:21.381Z"},{"id":"q-5345","question":"In a two-node docker-dca Swarm within one data center, deploy a TLS-enabled API behind an Nginx edge proxy. Use Cosign for image provenance (sign base images and verify with a 'sigver' sidecar before the app), use Vault to issue SPIFFE SVIDs, enforce an OPA policy via Docker Config to gate per-tenant features, and implement a 10% canary with traffic shifting and automatic rollback. Provide exact bootstrap commands and a docker-compose.yml skeleton?","answer":"Sign all base images with cosign and verify in a dedicated 'sigver' sidecar before the app runs. Use Vault to issue SPIFFE SVIDs for services, and enforce an OPA policy loaded via Docker Config to gat","explanation":"## Why This Is Asked\n\nTests a practical, beginner-friendly workflow that introduces image provenance, SPIFFE, and policy enforcement in a real Swarm setup. It also covers canary deployment with rollback, a common production pattern.\n\n## Key Concepts\n\n- Cosign/Sigstore image provenance\n- Sigver sidecar validation\n- SPIFFE SVIDs via Vault\n- OPA policies via Docker Config\n- Canary deployments and rollback in Swarm\n\n## Code Example\n\n```javascript\n// Sign and verify with cosign (shown for reference)\ncosign sign myimage:tag\ncosign verify myimage:tag\n```\n\n## Follow-up Questions\n\n- How would you rotate SPIFFE SVIDs without interrupting traffic?\n- What metrics would you monitor to decide on rollback timing?","diagram":null,"difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Databricks","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T19:00:15.600Z","createdAt":"2026-01-21T19:00:15.600Z"},{"id":"q-5383","question":"In a two-datacenter docker-dca deployment, implement per-tenant traffic governance: extract tenant from SPIFFE SVIDs via a sidecar, enforce quotas with Redis-based token buckets at the edge proxy, and apply a 10% canary rollout. Use a single overlay network, TLS, and a lightweight log shipper. Provide exact bootstrap commands and a docker-compose.yml skeleton?","answer":"Edge proxy enforces per-tenant rate limits by decoding the SPIFFE SVID in a sidecar, extracting tenant_id, and querying a Redis per-tenant token bucket. A separate SVID-validate sidecar authenticates ","explanation":"## Why This Is Asked\nTests ability to introduce tenant-aware QoS at the edge, integrating SPIFFE SVIDs, per-tenant state in Redis, and canary rollout in a multi-dc setup.\n\n## Key Concepts\n- SPIFFE SVID parsing at edge\n- Redis per-tenant token buckets\n- Sidecar validation posture and metadata propagation\n- Canary rollout with Argo Rollouts and observability\n\n## Code Example\n```yaml\n# docker-compose skeleton snippet\nversion: '3.8'\nservices:\n  edge:\n    image: nginx:edge\n    networks:\n      - app\n    depends_on:\n      - app\n  sidecar-svid:\n    image: svid-validator\n    networks:\n      - app\n  redis:\n    image: redis:7\n    networks:\n      - app\nnetworks:\n  app:\n    driver: overlay\n```\n\n## Follow-up Questions\n- How would you test per-tenant quotas under bursty traffic?\n- How to ensure fairness across data centers during failover?","diagram":"flowchart TD\nA[Edge Nginx] --> B[SVID Validation Sidecar]\nB --> C[Redis Token Buckets]\nA --> D[Backend API]\nC --> E[Allow if bucket > 0]\nE --> D","difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","NVIDIA","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-21T19:56:07.400Z","createdAt":"2026-01-21T19:56:07.400Z"},{"id":"q-5441","question":"In a three-node docker-dca cluster across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy. Use Vault to issue SPIFFE SVIDs; attach a Verifier sidecar to validate SVIDs and an OPA policy sidecar to gate per-tenant features. Add a Rate-Limiter sidecar backed by Redis with a dynamic token-bucket that can be reconfigured without redeploy. Implement a 15% canary with SLA-based latency targets and automatic rollback on breach. Provide exact bootstrap commands and a docker-compose skeleton?","answer":"Deploy a three-node Docker Swarm cluster across two data centers with TLS-terminated API traffic behind an Nginx edge proxy. Utilize HashiCorp Vault to issue SPIFFE SVIDs for workload identity, and attach a Verifier sidecar to validate these identities. Implement an OPA policy sidecar to enforce per-tenant feature gates and access controls. Add a Rate-Limiter sidecar backed by Redis with dynamic token-bucket configuration that can be updated without redeployment. Execute a 15% canary rollout with SLA-based latency monitoring and automatic rollback on threshold breach.","explanation":"Why This Is Asked\n\nTests ability to orchestrate multiple runtime security controls: SPIFFE-based identity management, policy enforcement via OPA, adaptive rate limiting, and observability during cross-site deployments.\n\nKey Concepts\n\n- SPIFFE/SVID identity issuance and validation using Vault\n- Sidecar architecture for verification and policy enforcement\n- Per-tenant feature gating through OPA policies\n- Dynamic rate limiting with Redis-backed token buckets\n- Canary deployment strategies with SLA-driven rollback mechanisms\n- Multi-site Docker Swarm cluster management\n\nCode Example\n\n```yaml\n# docker-compose.yml\nversion: '3.8'\n\nservices:\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"443:443\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n      - ./certs:/etc/nginx/certs\n    depends_on:\n      - api\n\n  api:\n    image: my-api:latest\n    environment:\n      - VAULT_ADDR=https://vault.example.com\n      - SPIFFE_WORKLOAD_API=unix:///spiffe-agent/socket\n    volumes:\n      - /var/run/spiffe/agent:/spiffe-agent\n    depends_on:\n      - redis\n\n  verifier:\n    image: spiffe-verifier:latest\n    volumes:\n      - /var/run/spiffe/agent:/spiffe-agent\n\n  opa:\n    image: openpolicyagent/opa:latest\n    command: run --server --addr=localhost:8181 /policy\n    volumes:\n      - ./policy:/policy\n\n  rate-limiter:\n    image: rate-limiter:latest\n    environment:\n      - REDIS_URL=redis://redis:6379\n    depends_on:\n      - redis\n\n  redis:\n    image: redis:alpine\n    ports:\n      - \"6379:6379\"\n\n  vault:\n    image: vault:latest\n    environment:\n      - VAULT_ADDR=https://vault.example.com\n    ports:\n      - \"8200:8200\"\n```\n\nBootstrap Commands:\n\n```bash\n# Initialize Docker Swarm cluster\ndocker swarm init --advertise-addr <MANAGER_IP>\n\n# Join worker nodes\ndocker swarm join --token <TOKEN> <MANAGER_IP>:2377\n\n# Deploy stack\ndocker stack deploy -c docker-compose.yml secure-api\n\n# Configure Vault for SPIFFE\nvault secrets enable spiffe\nvault write spiffe/config trust_domain \"example.com\"\n\n# Initialize canary deployment\ndocker service update --replicas 1 --update-parallelism 1 --update-delay 10s secure-api_api\n```","diagram":"flowchart TD\n  Edge[Nginx Edge Proxy] --> App[TLS-Enabled API]\n  Verifier[Verifier Sidecar] --> App\n  Policy[OPA Policy Sidecar] --> App\n  RateLimiter[Rate-Limiter Sidecar (Redis)] --> App\n  OTEL[OpenTelemetry Collector] --> Telemetry[Telemetry Backend]","difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Google","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T06:03:25.754Z","createdAt":"2026-01-21T22:39:24.824Z"},{"id":"q-5574","question":"In a three-node docker-dca cluster across two data centers, deploy a TLS-enabled API behind an Envoy edge proxy. Use Vault to issue SPIFFE SVIDs; attach a Verifier sidecar to validate SVIDs and a separate policy engine sidecar that evaluates tenant-specific OPA policies stored in Vault's KV with versioned revisions. Implement per-tenant feature gates with dynamic reloading without redeploy, and a 10% canary rollout of a new policy that must meet SLA latency targets; automatic rollback on breach. Provide exact bootstrap commands and a docker-compose.yml skeleton?","answer":"Explain an end-to-end flow: Envoy edge proxy terminates TLS with SPIFFE SVIDs issued by Vault; Verifier sidecar validates SVIDs; policy engine sidecar pulls versioned OPA policies from Vault KV and ho","explanation":"## Why This Is Asked\n\nTests ability to compose a multi-tenant, zero-trust API path using a real edge proxy (Envoy), SPIFFE-based identities, policy-driven gatekeeping, and runtime policy hot-reloading. It also probes disaster readiness with canaries and automatic rollback.\n\n## Key Concepts\n- Envoy TLS with SPIFFE/SVIDs from Vault\n- Verifier sidecar for mTLS identity validation\n- OPA policies versioning in Vault KV with hot reload\n- Per-tenant feature flags and runtime config via Redis/Canary\n- Cross-DC canary strategy and SLA-driven rollback\n\n## Code Example\n```bash\n# bootstrap (illustrative)\nvault server -config=config.hcl &\nopenssl req -new -x509 -days 365 -nodes -out tls.crt -keyout tls.key\n```\n\n## Follow-up Questions\n- How would you handle policy revocation in-flight?\n- How do you measure canary success across tenants?","diagram":"flowchart TD\nA(Client) --> B[Envoy Edge Proxy TLS termination]\nB --> C[Verifier Sidecar: SPIFFE SVID validation]\nC --> D[Policy Engine Sidecar: OPA policies from Vault KV (versioned)]\nD --> E[Backend API Service]\nE --> F[Redis: tenant rate limits]\n","difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Netflix","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T07:03:17.953Z","createdAt":"2026-01-22T07:03:17.953Z"},{"id":"q-5608","question":"In a three-node docker-dca cluster across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy. Enforce per-tenant data residency by routing tenant traffic to tenant-scoped overlay networks and gating access with an external policy service invoked by a sidecar. Use Vault to issue SPIFFE SVIDs and rotate certs with zero downtime, verify SVIDs at the edge with a verifier sidecar, and ship tenant-auditable events to a central SIEM via a lightweight audit sidecar. Provide exact bootstrap commands and a docker-compose.yml skeleton?","answer":"Bootstrap Vault for SPIFFE SVID issuance, configure Nginx to terminate TLS using SVIDs, attach a verifier sidecar that validates SVIDs, and a policy sidecar that queries an external OPA-like service p","explanation":"## Why This Is Asked\nTests complex multi-tenant isolation, policy-driven gating, and certificate rotation in docker-dca with an edge proxy.\n\n## Key Concepts\n- SPIFFE/SVID issuance via Vault\n- TLS termination and edge-side verification\n- Per-tenant overlay networking for data residency\n- External policy service (OPA) integration via sidecars\n- Lightweight audit sidecar to SIEM with zero-downtime rotation\n\n## Code Example\n```bash\n# Pseudo commands for illustration (do not run as-is)\nvault secrets enable pki\nvault write spiffe/svid create -t tenantA\n# Configure nginx to fetch certs from Vault and terminate TLS\n# Start verifier and policy sidecars with tenant-scoped config\n```\n\n## Follow-up Questions\n- How would you test tenant residency routing end-to-end?\n- How do you handle SVID revocation without impacting other tenants?\n- What metrics indicate successful zero-downtime rotation?\n","diagram":null,"difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Goldman Sachs","IBM"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T08:05:25.797Z","createdAt":"2026-01-22T08:05:25.797Z"},{"id":"q-5645","question":"In a two-data-center docker-dca cluster, deploy a TLS-enabled API behind an Nginx edge proxy. Enforce zero-trust network segmentation per-tenant using a Cilium-based policy plane with dynamic tenant-scoped policies, while Vault issues SPIFFE SVIDs for mTLS. Implement a 12% canary with traffic shifting and automatic rollback. Provide exact bootstrap commands and a docker-compose.yml skeleton?","answer":"Deploy Cilium+Hubble in docker-dca to enforce tenant-scoped NetworkPolicy with per-tenant labels and eBPF enforcement. Use Vault to mint SPIFFE SVIDs for all services and a sidecar to validate SVIDs a","explanation":"## Why This Is Asked\nTests zero-trust, per-tenant isolation, and SPIFFE/Vault integration in a multi-dc docker-dca setup.\n\n## Key Concepts\n- Cilium/Hubble, eBPF enforcement; tenant-scoped policies\n- Vault SPIFFE SVID issuance and rotation\n- Canary rollout with traffic weights and automatic rollback\n- Observability hooks (tracing) and policy enforcement via OPA\n\n## Code Example\n```yaml\n# docker-compose skeleton excerpt\nversion: '3.9'\nservices:\n  edge:\n    image: nginx:stable\n    ports: ['443:443']\n  api-canary:\n    image: your-api:canary\n    labels:\n      - tenant=tenantA\n```\n\n## Follow-up Questions\n- How would you test per-tenant policy changes? \n- How to extend to additional data centers?","diagram":"flowchart TD\n  edge[Nginx Edge Proxy] --> canary[Canary API] \n  canary --> pol[Cilium Tenant Policies] \n  pol --> svid[SPIFFE SVIDs (Vault)] \n  svid --> trace[OpenTelemetry]\n  trace --> audit[Audit/Telemetry]","difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Databricks"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T09:49:59.532Z","createdAt":"2026-01-22T09:49:59.532Z"},{"id":"q-5738","question":"In a two-DC docker-dca setup, deploy a TLS-enabled API behind an Nginx edge proxy with a Verifier sidecar that validates Vault-issued SPIFFE SVIDs. Add end-to-end observability by wiring OpenTelemetry tracing from the API and edge proxy to a Jaeger collector within the same docker-compose. Ensure trace context propagates across callers and across DCs. Provide exact bootstrap commands and a docker-compose skeleton?","answer":"Instrument the API and edge proxy with OpenTelemetry (OTLP exporter to Jaeger), initialize a tracer in each service, and propagate traceparent headers through Nginx and the Verifier sidecar. Run Jaege","explanation":"## Why This Is Asked\nTests practical tracing integration in a docker-dca flow, focusing on OpenTelemetry setup, header propagation, and cross-DC observability.\n\n## Key Concepts\n- OpenTelemetry instrumentation and OTLP exports\n- Trace context propagation (traceparent/b3)\n- Edge proxy role and sidecar context relay\n- Jaeger as collector for end-to-end traces\n\n## Code Example\n```javascript\n// Edge/Proxy pseudocode: forward trace headers\nproxy_set_header traceparent $traceparent;\n```\n```go\n// API: init tracer with OTLP exporter\nexp, _ := otlptracehttp.New(context.Background(), \"http://jaeger:4317/v1/traces\")\np := sdktrace.NewTracerProvider(sdktrace.WithBatcher(exp))\notel.SetTracerProvider(p)\n```\n\n## Follow-up Questions\n- How would you test trace propagation if TLS terminates at the edge?\n- How would sampling impact low-traffic environments?","diagram":"flowchart TD\nA[Client] --> B[Nginx Edge Proxy]\nB --> C[Verifier Sidecar]\nC --> D[API Service]\nD --> E[Jaeger Collector]","difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Snowflake","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T14:48:56.228Z","createdAt":"2026-01-22T14:48:56.228Z"},{"id":"q-5829","question":"In a three-node docker-dca cluster across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy. Replace the policy sidecar with a WASM-based policy engine (Wasmtime) that hot-loads per-tenant policy modules from a live policy broker and reloads on changes; Vault issues SPIFFE SVIDs and a verifier sidecar authenticates SVIDs; gate features with per-tenant flags; implement a 15% canary with SLA latency targets and automatic rollback. Provide exact bootstrap commands and a docker-compose skeleton?","answer":"Use Nginx as the edge proxy with mTLS; Vault issues SPIFFE SVIDs and a verifier sidecar authenticates requests; swap OPA for a WASM-based policy engine (Wasmtime) that hot-loads per-tenant policy modu","explanation":"## Why This Is Asked\nTests ability to integrate a WASM-based policy engine, live policy loading, and canary rollback in a docker-dca stack.\n\n## Key Concepts\n- WASM policy engine (Wasmtime)\n- Live policy broker and hot-reload\n- Vault SPIFFE SVIDs and verifier sidecar\n- Nginx edge TLS termination and per-tenant gating\n- 15% canary with SLA targets\n\n## Code Example\n```javascript\n// Pseudo policy loader\nasync function loadPolicy(tenantId){ const bytes = await fetchFromBroker(tenantId); /* compile to wasm */ }\n```\n\n## Follow-up Questions\n- How to ensure cold-start latency stays within SLA?\n- How to rollback a policy module without disrupting traffic?","diagram":"flowchart TD\n  Edge[Nginx Edge Proxy] --> API[TLS-enabled API]\n  API --> Verifier[Verifier Sidecar]\n  Verifier --> Policy[WASM Policy Engine]\n  Policy --> Broker[Policy Broker]\n  Broker --> Canary[Canary Router]","difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Netflix","OpenAI"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T18:45:31.442Z","createdAt":"2026-01-22T18:45:31.444Z"},{"id":"q-5857","question":"In a two-data-center docker-dca cluster, deploy a TLS-enabled API behind an Nginx edge proxy. Add per-tenant data residency by routing tenant data to dedicated overlay volumes and enforce locality with a sidecar policy engine that consults a central policy store. Use Vault SPIFFE SVIDs, and implement a 15% canary with rollbacks. Provide exact bootstrap commands and a docker-compose.yml skeleton?","answer":"Use Vault to mint SPIFFE SVIDs for all services and terminate TLS at Nginx with mTLS. Enforce per-tenant data residency by binding tenant data to overlay volumes and a sidecar loc-enforcer that valida","explanation":"## Why This Is Asked\nTests ability to design tenant-aware data residency and policy enforcement in docker-dca, including edge TLS termination, overlay data paths, and canary safety net.\n\n## Key Concepts\n- docker-dca multi-DC deployment\n- SPIFFE/SVID rotation with Vault\n- data residency via overlay networks/volumes\n- per-tenant policy with OPA\n- canary rollout and rollback criteria\n\n## Code Example\n```javascript\n// Pseudo-code illustrating policy check\nconst tenant = req.headers['x-tenant'];\nif (allowedTenants[tenant]?.includes(req.path)) {\n  next();\n} else {\n  res.status(403).send('forbidden');\n}\n```\n\n## Follow-up Questions\n- How would you monitor cross-tenant data access to ensure residency is respected?\n- How would you test the canary rollback under sudden quota breach?","diagram":null,"difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Cloudflare","Hugging Face"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T19:37:06.880Z","createdAt":"2026-01-22T19:37:06.881Z"},{"id":"q-5895","question":"In a two-data-center docker-dca deployment, design a real-time policy-driven API access control where an Envoy sidecar validates every request against a remote policy service over gRPC, rather than in-process checks. Use Vault to issue SPIFFE SVIDs for mTLS, deploy a Redis-backed per-tenant rate limiter shared across DCs, and ensure policy updates propagate quickly with eventual consistency. Provide exact bootstrap commands and a docker-compose.yml skeleton?","answer":"Envoy sidecar routes each API call through a remote policy service over gRPC, authenticated with Vault SPIFFE SVIDs for mTLS. Cache decisions with 30s TTL and invalidate on policy reload via Redis pub","explanation":"## Why This Is Asked\n\nTests ability to design distributed policy decisions outside the application boundary, using SPIFFE/mTLS, cross-DC state, and dynamic quotas.\n\n## Key Concepts\n\n- Remote policy decisions via gRPC from Envoy sidecar\n- SPIFFE SVIDs and mTLS with Vault\n- Envoy as a policy enforcer in the data path\n- Redis pub/sub for policy-cache invalidation across DCs\n- Per-tenant rate limiting and tenancy isolation\n\n## Code Example\n\n```yaml\n# Envoy bootstrap excerpt (high level, not complete)\nhttp_filters:\n  - name: envoy.filters.http_ext_proc\n    typed_config:\n      \"${ext_proc_config}\"\n```\n\n## Follow-up Questions\n\n- How would you handle policy cache staleness?\n- How would you observe latency and reliability of the remote policy check?","diagram":null,"difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Scale Ai","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-22T21:08:55.817Z","createdAt":"2026-01-22T21:08:55.817Z"},{"id":"q-6033","question":"In a two-data-center docker-dca deployment, wire a TLS-enabled API behind an Nginx edge proxy to stream tenant events into a central Kafka cluster. Use Vault-issued SPIFFE SVIDs for mTLS, enforce per-tenant rate limits with an OPA policy loaded via Docker Config, and add a 15% canary that publishes to a new tenant-audit topic with an audit-proxy validating events. Provide exact bootstrap commands and a docker-compose.yml skeleton?","answer":"Mint SVIDs via Vault for all services; edge Nginx terminates TLS; ship an audit-proxy sidecar that forwards 15% of tenant events to the tenant-audit Kafka topic; enforce per-tenant rate limits via an ","explanation":"## Why This Is Asked\nTests ability to design secure, observable cross-datacenter data paths with mTLS and policy gates, plus a safe canary for event schemas.\n\n## Key Concepts\n- SPIFFE/SVID with Vault for mTLS\n- Nginx edge TLS termination\n- Docker Config for OPA policy\n- Per-tenant rate limiting\n- Canary routing with Envoy and Kafka topics\n- Audit proxy pattern for compliant event streams\n\n## Code Example\n```javascript\n// bootstrap sketch for docker-compose skeleton\nconst compose = {\n  version: '3.8',\n  services: {\n    edge: { image: 'nginx:latest' },\n    api: { image: 'repo/api:latest' },\n    audit: { image: 'repo/audit-proxy:latest' },\n    kafka: { image: 'confluentinc/cp-kafka:7.x' }\n  }\n}\n```\n\n## Follow-up Questions\n- How would you validate the canary without impacting the main path?\n- How would you rotate SVIDs without restart downtime?","diagram":"flowchart TD\n  EdgeProxy[Edge Proxy] --> SVIDValidator[SPIFFE SVID Validation]\n  SVIDValidator --> PolicyEngine[Policy Engine (OPA)]\n  PolicyEngine --> CanaryRouter[Canary Router]\n  CanaryRouter --> MainKafka[Main Kafka Topic]\n  CanaryRouter --> AuditKafka[Tenant-Audit Topic]","difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T05:50:14.508Z","createdAt":"2026-01-23T05:50:14.508Z"},{"id":"q-6087","question":"Across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy in docker-dca. Use TLS termination at the edge with certificates stored as Docker secrets, and implement a simple per-tenant rate limiter in Nginx using a header X-Org-Id. Provide exact bootstrap commands and a minimal docker-compose.yml skeleton?","answer":"Configure Nginx at the edge for TLS termination using Docker secrets (edge.crt, edge.key), proxying to the API, and apply a per-tenant rate limit based on X-Org-Id. Include a /health probe and a reque","explanation":"## Why This Is Asked\nTests practical skills in configuring TLS, Docker secrets, and edge control flow without external IAM.\n\n## Key Concepts\n- Docker secrets\n- Nginx TLS termination\n- docker-compose composition\n- Basic per-tenant rate limiting\n\n## Code Example\n```javascript\n# Minimal nginx.conf (conceptual)\nserver {\n  listen 443 ssl; ssl_certificate /run/secrets/edge.crt;\n  ssl_certificate_key /run/secrets/edge.key;\n  location / { proxy_pass http://api:8080; }\n}\n```\n\n## Follow-up Questions\n- How would you test cross-DC failover of the edge proxy?\n- What are the security implications of storing certs as secrets vs files? ","diagram":null,"difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Hashicorp"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T07:59:41.885Z","createdAt":"2026-01-23T07:59:41.885Z"},{"id":"q-6133","question":"In a three-node docker-dca cluster across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy. Implement a lightweight per-key rate limiter as a sidecar to enforce 60 requests/min per API key and a 15% canary to route a subset of keys to a new version; provide exact bootstrap commands and a docker-compose skeleton?","answer":"Prototype a per-key rate limiter as a Docker sidecar in Python. The sidecar reads the X-API-Key header, tracks windowed counts in a tiny JSON file mounted as a volume, and returns 429 when the count e","explanation":"## Why This Is Asked\n\nTests practical setup: TLS Terminations via Nginx, a lightweight sidecar pattern, and a beginner-friendly canary workflow without external policy engines.\n\n## Key Concepts\n\n- Docker Compose wiring for multi-node, multi-DC setups\n- TLS termination at Nginx and service-to-edge security\n- Sidecar pattern to implement per-key rate limiting\n- Canary rollout basics with URL/param-based routing\n\n## Code Example\n\n```python\n# rate_limiter.py\nimport time, json\nSTATE_PATH = \"/shared/state.json\"\n\ndef load():\n    try:\n        with open(STATE_PATH) as f: return json.load(f)\n    except Exception:\n        return {}\n\ndef save(state):\n    with open(STATE_PATH, \"w\") as f: json.dump(state, f)\n\n\ndef allow(key, window=60, limit=60, state=None):\n    if state is None: state = load()\n    now = int(time.time())\n    bucket = state.get(key, [])\n    bucket = [t for t in bucket if now - t < window]\n    if len(bucket) >= limit: return False\n    bucket.append(now)\n    state[key] = bucket\n    save(state)\n    return True\n```\n\n## Follow-up Questions\n\n- How would clock drift affect accuracy and how to mitigate it?\n- How would you scale the limiter across many nodes?\n- What tests would you add to validate 15% canary behavior?","diagram":"flowchart TD\n  A[Client] --> B[Nginx Edge Proxy]\n  B --> C[Service A]\n  B --> D[RateLimiter Sidecar]\n  D --> C\n  C --> E[Canary Router]","difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Instacart","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T10:10:02.141Z","createdAt":"2026-01-23T10:10:02.141Z"},{"id":"q-6152","question":"In a three-node docker-dca cluster spanning two data centers, implement end-to-end tracing of requests from edge proxy to service, ensuring mTLS with SPIFFE SVIDs and per-tenant identity surfaced in traces. Use OpenTelemetry with a central collector and a tenant-aware sampler; propagate tenant_id via trace attributes. Describe deployment steps, instrumentation points, and how to verify trace completeness during a canary rollout?","answer":"Instrument requests with OpenTelemetry: edge proxy, service, and sidecar emit OTLP traces to a central Collector. Use W3C TraceContext; propagate tenant_id via trace attributes. Enforce mTLS with SPIF","explanation":"## Why This Is Asked\nTests ability to design observability in a docker-dca multi-node setup, focusing on secure traces across edge and services, tenant-awareness, and canary rollout.\n\n## Key Concepts\n- OpenTelemetry instrumentation across proxies and services\n- SPIFFE SVIDs and mTLS across gRPC/HTTP\n- Tenant-aware tracing and sampling\n- Central collector and trace storage (Jaeger/Tempo)\n\n## Code Example\n```yaml\nreceivers:\n  otlp:\n    protocols:\n      grpc: {}\n      http: {}\nexporters:\n  logging: {}\n  otlp:\n    endpoint: \"tempo-collector:4317\"\nservice:\n  pipelines:\n    traces:\n      receivers: [otlp]\n      exporters: [logging, otlp]\n```\n\n## Follow-up Questions\n- How would you adapt tracing for high-cardinality tenants?\n- What changes if a tenant disables tracing mid-rollout?","diagram":null,"difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Stripe","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T11:03:30.313Z","createdAt":"2026-01-23T11:03:30.313Z"},{"id":"q-6293","question":"In a three-node docker-dca cluster spanning two data centers, deploy a TLS-enabled API behind an Nginx edge proxy. Implement a per-tenant dynamic regional failover router sidecar that reads tenant region preferences from a central config store (Consul KV) and updates upstream mappings on-the-fly without redeploy. Roll out a 25% canary to a new regional backend, enforce SLA-based health checks, and auto rollback if latency exceeds target thresholds. Provide exact bootstrap commands and a docker-compose.yml skeleton?","answer":"Build a Go sidecar that subscribes to Consul KV at /tenants/*/region and updates Nginx upstreams via the Admin API, enabling per-tenant dynamic regional failover with a 25% canary to region B. TLS at ","explanation":"## Why This Is Asked\nTests ability to implement real-time, per-tenant routing without redeploys, integrating a central config store and edge proxy. Emphasizes observability, deterministic canaries, and safe rollback.\n\n## Key Concepts\n- Dynamic per-tenant routing via Consul KV\n- Edge TLS with mTLS to services\n- Real-time upstream updates without redeploy\n- Canary rollout and SLA-driven rollback\n\n## Code Example\n```go\n// Pseudo: watch Consul KV and call Nginx upstream API\n// - fetch /tenants/*/region\n// - if region changes, push to nginx/upstream via REST\n```\n\n## Follow-up Questions\n- How would you test latency-based rollback in CI/CD?\n- How would you handle Consul KV outages and ensure idempotent updates?","diagram":null,"difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","LinkedIn","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T18:00:27.304Z","createdAt":"2026-01-23T18:00:27.304Z"},{"id":"q-6345","question":"Design a two-datacenter docker-dca deployment with Envoy as the edge proxy and TLS/mTLS via Vault-issued SPIFFE IDs. Add a per-tenant feature-flag sidecar that queries a central service, caches decisions, and injects a tenant header. Canary 15% with traffic-split and auto-rollback on latency/errors. Provide exact bootstrap commands and a docker-compose skeleton?","answer":"TLS/mTLS via Vault SPIFFE IDs; Envoy edge proxy; a per-tenant feature-flag sidecar that queries a central service, caches decisions, and injects a Tenant header for downstream routing; a 15% canary us","explanation":"## Why This Is Asked\nNew angle: per-tenant feature flags orchestrated by a sidecar pattern in docker-dca, with cross-datacenter routing and observability.\n\n## Key Concepts\n- Envoy edge proxy with TLS/mTLS\n- Vault-issued SPIFFE IDs and cert rotation\n- Per-tenant feature-flag sidecar\n- Weight-based canary with automatic rollback\n- docker-dca multi-datacenter routing and monitoring\n\n## Code Example\n```yaml\n# Envoy route weights (simplified)\nstatic_resources:\n  listeners:\n  - name: listener_0\n    address: { socket_address: { address: 0.0.0.0, port_value: 8443 } }\n    filter_chains:\n    - filters: []\n  routing:\n    routes:\n    - match: { prefix: \"/api\" }\n      route:\n        weighted_clusters:\n          clusters:\n          - name: main\n            weight: 85\n          - name: canary\n            weight: 15\n```\n```bash\n# Bootstrap (example)\ndocker swarm init\ndocker network create -d overlay appnet\ndocker stack deploy -c docker-compose.yml app\n```","diagram":null,"difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-23T19:55:10.443Z","createdAt":"2026-01-23T19:55:10.443Z"},{"id":"q-6391","question":"In a two-node docker-dca cluster across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy. Use Vault to issue SPIFFE SVIDs and a verifier sidecar to validate them; implement a tiny in-memory per-tenant allowlist loaded via a Docker Config and enforce access. Add a 10% canary routing to a feature-flagged endpoint for tenants in a separate canary Config. Provide exact bootstrap commands and a docker-compose skeleton?","answer":"Configure an edge TLS setup with Nginx, mTLS, and SPIFFE SVIDs issued by Vault. Implement a verifier sidecar in Go that reads the client certificate's SPIFFE ID and checks an in-memory per-tenant allowlist loaded via Docker Config. Set up 10% canary routing to feature-flagged endpoints using a separate canary Config.","explanation":"## Why This Is Asked\nTests practical TLS setup, SPIFFE SVID handling, and lightweight per-tenant policy via a sidecar. Also validates a simple canary routing path without heavy policy engines.\n\n## Key Concepts\n- TLS mutual authentication and edge termination\n- SPIFFE SVIDs issued by Vault\n- Verifier sidecar pattern for certificate-based authorization\n- Docker Config as a lightweight configuration source\n- Lightweight canary routing by tenant\n\n## Code Example\n```go\n// verifier.go (conceptual)\npackage main\nimport (\n  \"crypto/tls\"\n  \"crypto/x509\"\n  \"fmt\"\n  \"net/http\"\n)\nfunc main() {\n  // Load all","diagram":"flowchart TD\n  Edge[Edge Proxy] --> Verifier[Verifier Sidecar]\n  Verifier --> Api[API Service]\n  Vault[(Vault SPIFFE SVIDs)] --> Edge\n  Config[(Docker Config: tenant allowlist)] --> Verifier\n  Edge -->|canary| ApiV2[API v2]","difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","MongoDB"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T05:32:28.551Z","createdAt":"2026-01-23T21:57:15.821Z"},{"id":"q-6504","question":"In a four-node docker-dca cluster spread across three data centers, deploy a TLS-enabled API behind an edge proxy. Implement cross-DC SPIFFE SVID issuance with Vault, a per-tenant WASM-based policy engine sidecar that hot-loads modules from a broker, and a verifier edge sidecar that validates SVIDs. Use blue/green deployment with 25% canary and latency/error-rate thresholds for automatic rollback. Provide exact bootstrap commands and a docker-compose skeleton?","answer":"Describe the end-to-end setup: Vault-issued SPIFFE SVIDs for all tenants, edge verifier sidecar validating SVIDs, WASM policy engine hot-loading tenant modules from a broker, blue/green with 25% canar","explanation":"## Why This Is Asked\n\nTests depth of cross-DC security, dynamic policy loading, and automated risk controls in a real-world Docker DC architecture.\n\n## Key Concepts\n\n- SPIFFE/SVIDs with Vault\n- WASM-based per-tenant policy engine\n- Edge verifier sidecar for SVID validation\n- Canary deployments and automatic rollback\n- Cross-DC routing and observability\n\n## Code Example\n\n```javascript\n// Pseudo: load tenant policy from broker and validate SVID before calling app\nasync function handleRequest(req){\n  const svid = await verifySVID(req.headers['spiffe']);\n  const(policy) = await loadPolicyFromBroker(req.tenant);\n  if(!policy.allows(req)) throw new Error('Access denied');\n  return proxyToApp(req);\n}\n```\n\n## Follow-up Questions\n\n- How would you debug a scenario where Vault rotation invalidates existing SVIDs mid-rollout?\n- What metrics and traces would you collect to enforce the 25% canary SLA thresholds across DCs?","diagram":"flowchart TD\n  A[Edge Proxy] --> B[Verifier Sidecar]\n  B --> C[App Service]\n  D[Policy Broker] --> E[WASM Policy Engine Sidecar]\n  F[Vault] --> A\n  subgraph DataCenters\n    DC1[DC1] --- DC2[DC2] --- DC3[DC3]\n  end","difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T05:47:21.180Z","createdAt":"2026-01-24T05:47:21.180Z"},{"id":"q-6612","question":"In a three-node docker-dca cluster across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy. Introduce a quota-enforcement sidecar that reads per-tenant quotas from a YAML file mounted via Docker Config and enforces them on incoming requests before they reach the app. Route 20% of tenants to a new path version for canary; provide exact bootstrap commands and a docker-compose.yml skeleton?","answer":"Build a lightweight quota sidecar (Go/JS) that loads per-tenant quotas from YAML via Docker Config, watches for changes, and enforces per-tenant token bucket. Read tenant from header X-Tenant, return ","explanation":"## Why This Is Asked\nThis tests practical reasoning on per-tenant quotas, config hot-reload via Docker Config, and lightweight canary routing in a docker-dca cluster.\n\n## Key Concepts\n- Docker Config for runtime config delivery\n- Lightweight sidecar pattern for per-tenant quotas\n- Token-bucket rate limiting per tenant\n- Canary routing and rollback\n\n## Code Example\n```javascript\n// Simplified quota sidecar sketch (pseudo)\nconst quotas = loadYamlFromConfig('quotas.yaml');\nlet buckets = {};\n\nfunction handle(req, res, next){\n  const tenant = req.headers['x-tenant'];\n  if(!tenant) return res.status(400).send('Tenant required');\n  const q = quotas[tenant] || quotas.default;\n  const now = Date.now();\n  let b = buckets[tenant] || { tokens: q.limit, last: now };\n  const elapsed = Math.max(0, now - b.last);\n  b.tokens = Math.min(q.limit, b.tokens + (elapsed * (q.refillRate || 1)));\n  b.last = now;\n  if(b.tokens < 1){ return res.status(429).send('Quota exceeded'); }\n  b.tokens -= 1;\n  buckets[tenant] = b;\n  next();\n}\n```\n\n## Follow-up Questions\n- How would you test quota correctness across tenants?\n- How would you handle config validation errors and rollback?\n","diagram":null,"difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","IBM","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T10:04:03.669Z","createdAt":"2026-01-24T10:04:03.669Z"},{"id":"q-6673","question":"In a three-node docker-dca cluster across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy. Implement end-to-end distributed tracing with OpenTelemetry: run an OpenTelemetry collector as a sidecar on each node, instrument services to emit traces, export via OTLP to a central Jaeger backend over TLS, and apply sampling (baseline 10%, canary 25% for a new version). Provide exact bootstrap commands and a docker-compose.yml skeleton?","answer":"Instrument a three-node docker-dca cluster with TLS behind Nginx and end-to-end tracing via OpenTelemetry. Run an OpenTelemetry Collector as a sidecar on each node, emit traces from the API, export to","explanation":"## Why This Is Asked\nTests knowledge of distributed tracing integration in a multi‑DC Docker context, including sidecar patterns, TLS OTLP transport, sampling strategies, and a practical docker-compose setup.\n\n## Key Concepts\n- OpenTelemetry collector as a per-node sidecar\n- OTLP over TLS to Jaeger\n- W3C trace context propagation\n- Canary sampling and observability buy-in\n\n## Code Example\n```yaml\n# docker-compose skeleton\nversion: '3.8'\nservices:\n  nginx-edge:\n    image: nginx:latest\n  api:\n    image: myorg/api:latest\n  otel:\n    image: otel/opentelemetry-collector:latest\n  jaeger:\n    image: jaegertracing/all-in-one:1.33\n```\n\n## Follow-up Questions\n- How would you validate tracing data quality under network hiccups?\n- How would you handle TLS certificate rotation with OTLP endpoints?","diagram":"flowchart TD\n  Client(Client) --> Edge[Nginx Edge Proxy]\n  Edge --> DC1[API Node DC1]\n  Edge --> DC2[API Node DC2]\n  DC1 --> OTEL[OTel Collector]\n  DC2 --> OTEL\n  OTEL --> Jaeger[Jaeger Backend]","difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Discord","Robinhood"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T13:12:10.382Z","createdAt":"2026-01-24T13:12:10.382Z"},{"id":"q-6746","question":"In a three-node docker-dca cluster across two data centers, implement tenant-scoped egress controls for outbound calls from services to external APIs using a Cilium-based policy plane. Source per-tenant rules from a central policy store, push updates via Docker Config, enforce TLS mutual auth at edges, and verify policy changes take effect within 5 seconds with an automated test. Provide bootstrap commands and a docker-compose skeleton?","answer":"Use Cilium with tenant-scoped egress policies backed by a central policy store (OPA). Distribute rules via Docker Config and a small policy-consumer sidecar in each service to enforce outbound access ","explanation":"## Why This Is Asked\nTests the ability to design dynamic, tenant-aware egress controls across data centers, including policy propagation latency, cross-cluster coordination, and safe rollback.\n\n## Key Concepts\n- Cilium eBPF policy\n- OPA-based policy store\n- Docker Config propagation\n- Edge mTLS\n- Canary testing and rollback\n\n## Code Example\n```yaml\n# CiliumNetworkPolicy sample (tenant A)\napiVersion: \"cilium.io/v2\"\nkind: CiliumNetworkPolicy\nmetadata:\n  name: tenant-a-egress\nspec:\n  egress:\n  - toEndpoints:\n    - matchLabels:\n        tenant: a\n  - toPorts:\n    - ports: [{port: \"443\", protocol: \"TCP\"}]\n```\n\n## Follow-up Questions\n- How would you measure end-to-end policy-apply latency?\n- How would you handle policy-store outages and ensure safe rollback?","diagram":"flowchart TD\n  A[Policy Arrival] --> B[Push to Cilium]\n  B --> C[Edge Enforce]\n  C --> D[Test]\n  D --> E{Pass?}\n  E -->|Yes| F[Rollout]\n  E -->|No| G[Rollback]","difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","Instacart","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-24T15:37:59.281Z","createdAt":"2026-01-24T15:37:59.281Z"},{"id":"q-6895","question":"In a three-node docker-dca cluster across two data centers, implement tenant-aware GPU acceleration sharing using NVIDIA runtime. Enforce per-tenant GPU quotas with a device-plugin-based quota allocator and a sidecar that negotiates GPU tokens from a central policy service, while the edge API remains TLS-enabled behind Nginx. Provide exact bootstrap commands and a docker-compose.yml skeleton?","answer":"Leverage the NVIDIA runtime with a per-tenant quota allocator sidecar that retrieves GPU tokens from a Vault-backed central policy service and configures CUDA_VISIBLE_DEVICES accordingly. A device plugin enforces quota compliance through cgroups isolation, while Nginx provides TLS termination for the edge API across the multi-datacenter deployment.","explanation":"## Why This Is Asked\n\nAssesses the ability to design GPU isolation in a multi-datacenter Docker Datacenter Appliance, with per-tenant quotas, device plugins, central policy, and edge TLS; tests security, performance, and operations knowledge.\n\n## Key Concepts\n\n- NVIDIA GPU runtime for containers\n- Per-tenant quota management with central policy\n- Device plugin enforcement and cgroups isolation\n- Edge TLS/mTLS with Nginx or edge router\n- Cross-datacenter deployment and DR considerations\n\n## Code Example\n\n```javascript\n// Pseudo-implementation outline for quota negotiation and device assignment\n```","diagram":"flowchart TD\n  Edge[Edge Proxy] --> API[API Service]\n  API --> Policy[Quota Policy Service]\n  Policy --> Sidecar[Quota Allocator Sidecar]\n  Sidecar --> GPU[Device Allocation]\n  API --> Audit[Audit Trail]","difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","NVIDIA","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T06:11:10.970Z","createdAt":"2026-01-24T21:48:34.614Z"},{"id":"q-6936","question":"In a two-data-center docker-dca cluster, deploy a TLS-enabled API behind an Nginx edge proxy. Add a dynamic routing sidecar that watches a central config store (Vault KV) for per-tenant routes and reloads Nginx with zero downtime when rules change. Implement a 10% canary for a new tenant route with health checks and automatic rollback. Provide bootstrap commands and a docker-compose.yml skeleton?","answer":"Deploy across two data centers with TLS termination at the Nginx edge proxy. A config-watcher sidecar monitors Vault KV for per-tenant route configurations, dynamically rewrites the Nginx configuration, and performs graceful reloads (nginx -s reload) to maintain zero downtime during routing updates.","explanation":"## Why This Is Asked\nTests practical edge routing capabilities with dynamic configuration management, zero-downtime reloads, and canary deployment controls in a realistic multi-data center environment.\n\n## Key Concepts\n- Dynamic in-cluster configuration watching and hot-reload of proxy services\n- Canary rollout strategies with metrics-driven rollback mechanisms\n- Vault KV-backed per-tenant routing configuration\n- Multi-data center deployment with TLS security\n\n## Code Example\n```bash\n# Sample reload trigger (simplified)\n# Watcher detects configuration changes, regenerates /etc/nginx/conf.d/tenant-routes.conf\n# and triggers nginx -s reload for zero-downtime updates\n```","diagram":"flowchart TD\n  Client(Client) --> Edge[Nginx Edge Proxy]\n  Edge --> App[API Service]\n  Watcher[Config Watcher Sidecar] --> Edge\n  Vault[Vault KV Store] --> Watcher\n  Canary[Canary Route] --> Edge","difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Hugging Face","Lyft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T05:50:18.852Z","createdAt":"2026-01-24T23:36:07.193Z"},{"id":"q-7051","question":"In a four-node docker-dca cluster across two data centers, deploy an edge-proxied TLS API with per-tenant audit logging. Introduce an audit sidecar that cryptographically signs tenant events with Vault-issued keys, streams to a central SIEM over TLS, and uses a lightweight WORM storage on the audit broker. Outline bootstrap commands and a docker-compose skeleton?","answer":"Sign events in a dedicated audit sidecar using Vault Transit per-tenant keys, rotate automatically, forward to a TLS-enabled SIEM via a lightweight broker, and store immutable digests in a WORM bucket","explanation":"## Why This Is Asked\nExplores secure, immutable audit trails in a multi-tenant, DC-distributed stack. Tests integration of Vault, TLS, and a SIEM while maintaining tenant isolation and zero-downtime upgrades.\n\n## Key Concepts\n- Per-tenant audit signing using Vault Transit\n- TLS end-to-end for audit channels\n- Immutable storage and signature verification in SIEM\n\n## Code Example\n```bash\n# bootstrap snippet (illustrative)\ndocker network create audit-net\n# compose skeleton to wire audit sidecar, broker, and SIEM\n```\n\n## Follow-up Questions\n- How would you test key rotation without breaking ongoing audits?\n- How would onboarding and revocation of tenants affect audit integrity?","diagram":null,"difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Microsoft","Salesforce","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T07:29:43.582Z","createdAt":"2026-01-25T07:29:43.582Z"},{"id":"q-7133","question":"In a three-node docker-dca cluster across two data centers, deploy a TLS-enabled event API behind an Envoy edge proxy. Implement per-tenant namespaces via tenant-scoped overlays, gating mutations with an OPA sidecar. Use Vault for SPIFFE SVIDs with zero-downtime rotation; ensure exactly-once delivery with per-tenant Kafka topics and idempotent processors, and ship audit events to a central SIEM. Provide exact bootstrap commands and a docker-compose.yml skeleton?","answer":"Describe a plan that uses Envoy mTLS to a TLS API, Vault-issued SPIFFE SVID rotations with zero downtime, per-tenant Kafka topics on overlay networks with ACLs, and an OPA policy sidecar gating mutati","explanation":"## Why This Is Asked\nTests advanced multi-DC deployment, strict tenant isolation, and production-grade data integrity. Evaluates TLS, SPIFFE, Vault rotation, policy enforcement, event sourcing, and auditing in one scenario.\n\n## Key Concepts\n- TLS termination and mutual authentication with Envoy\n- SPIFFE/SVIDs and Vault-managed rotation\n- Tenant-scoped overlay networks and per-tenant Kafka topics\n- OPA policy enforcement in sidecar\n- Exactly-once semantics with Kafka transactions\n- SIEM auditing of events\n\n## Code Example\n```yaml\n# docker-compose skeleton (high level)\nversion: '3.9'\nservices:\n  envoy:\n    image: envoyproxy/envoy:v1.28.0\n    ports:\n      - \"443:8443\"\n    # TLS, mTLS, and routing config omitted for brevity\n  api-service:\n    image: myorg/event-api:latest\n    depends_on:\n      - envoy\n  kafka:\n    image: confluentinc/cp-kafka:7.3.1\n    environment:\n      KAFKA_NUM_PARTITIONS: 3\n      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3\n```\n\n## Follow-up Questions\n- How would you test tenant isolation under data-center partition? \n- How would you roll out a tenant-specific upgrade with zero-downtime and data fidelity?","diagram":"flowchart TD\n  Tenant[Tenant] --> Edge[Envoy Edge Proxy]\n  Edge --> Api[API Service]\n  Api --> Kafka[Per-tenant Kafka Topic]\n  Api --> Verifier[Verifier Sidecar]\n  Verifier --> Processor[Event Processor]\n  Processor --> SIEM[SIEM Audit]\n","difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","NVIDIA","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-25T10:38:11.172Z","createdAt":"2026-01-25T10:38:11.172Z"},{"id":"q-7445","question":"Across two data centers, deploy a TLS-enabled API behind Nginx. Implement a tenant-aware service mesh created on demand by a policy controller, isolating each tenant on ephemeral overlay networks and routing to their backends. Use Vault for per-tenant short-lived certs with zero-downtime rotation; edge verifier validates; and a dedicated SIEM pipeline for tenant events. Provide exact bootstrap commands and a docker-compose skeleton?","answer":"Deploy a policy-driven tenant service mesh created on demand. For each tenant, provision an ephemeral overlay network with sidecar proxies, route through Nginx with TLS termination, and issue per-tenant TLS certificates using Vault with TTL rotation and zero-downtime certificate rekeying.","explanation":"## Why This Is Asked\nTests ability to design on-demand, tenant-scoped service meshes with dynamic networking, secure edge routing, and automated certificate management.\n\n## Key Concepts\n- On-demand tenant service meshes and ephemeral overlay networks\n- Policy-driven orchestration for per-tenant isolation\n- Vault-based TLS certificates with TTL rotation and zero-downtime rekeying\n- Edge certificate validation and centralized SIEM pipeline for tenant events\n- Canary rollout and rollback strategies\n\n## Code Example\n```javascript\n// illustrative bootstrap\nconsole.log('deploy tenant mesh bootstrap');\n```\n\n## Follow-up Questions\n- How would you migrate a single tenant to another data center with zero downtime?\n- What metrics would you monitor to ensure mesh health and performance?\n- How do you handle certificate revocation for terminated tenants?","diagram":null,"difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","IBM","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:52:27.290Z","createdAt":"2026-01-25T23:43:53.829Z"},{"id":"q-7463","question":"Design and implement a three-node docker-dca cluster across two data centers hosting a TLS-enabled API behind an Nginx edge proxy. Add a runtime security loop: a Falco-based detector watches the container host; on detecting a predefined suspicious syscall pattern, it triggers a quarantine action that updates edge proxy ACLs, revokes the API's SPIFFE SVID via Vault, rotates TLS certs, and performs a zero-downtime rolling update to substitute the compromised container. Provide exact bootstrap commands and a docker-compose.yml skeleton?","answer":"Implement a Falco-driven automated quarantine system: upon detecting a predefined suspicious syscall pattern, isolate the compromised container to a tenant-scoped overlay network, propagate edge proxy ACL updates through a configuration sidecar, revoke the API's SPIFFE SVID via Vault integration, rotate TLS certificates, and execute a zero-downtime rolling update to replace the compromised container.","explanation":"## Why This Is Asked\n\nThis question evaluates the integration of runtime security, automated policy enforcement, and zero-downtime deployment strategies in a distributed Docker Swarm environment spanning multiple data centers.\n\n## Key Concepts\n\n- Falco-based anomaly detection and automated container quarantine\n- Vault-managed SPIFFE SVID lifecycle and certificate rotation\n- Dynamic edge proxy ACL updates without service interruption\n- Canary deployments with automated rollback capabilities\n\n## Code Example\n\n```javascript\n// Bootstrap example (pseudocode)\n```\n\n## Follow-up Questions\n\n- How would you handle split-brain scenarios in a multi-data center deployment?\n- What monitoring and observability metrics would you implement for the quarantine pipeline?\n- How would you ensure compliance with regulatory requirements during automated security responses?","diagram":"flowchart TD\n  A[Ingress] --> B[Nginx Edge Proxy]\n  B --> C[App Service]\n  subgraph Security\n   D[Falco Detector]\n   E[Quarantine & ACL Update]\n   F[Vault SVID Revoke]\n   G[Rolling Update]\n  end\n  C --> D\n  D --> E\n  E --> F\n  F --> G\n  G --> B","difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Cloudflare","NVIDIA"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T05:39:00.675Z","createdAt":"2026-01-26T02:33:35.722Z"},{"id":"q-7541","question":"In a two-node docker-dca cluster spread across two data centers, deploy a TLS enabled API behind an Nginx edge proxy. Add a lightweight tenant aware rate limiter as a sidecar that reads quotas from a Docker Config and enforces per-tenant limits at the edge. Implement a 10% canary rollout for changing the quota window and verify with latency tests. Provide exact bootstrap commands and a docker-compose.yml skeleton?","answer":"Plan: deploy a TLS API behind Nginx, with a lightweight Go sidecar enforcing per-tenant quotas loaded from a Docker Config. The sidecar inspects X-Tenant, maintains in-memory counters with TTL, and re","explanation":"## Why This Is Asked\nTests ability to design a small edge-guarding workflow, basic service mesh concepts, and practical canary rollout in docker-dca.\n\n## Key Concepts\n- TLS termination at edge\n- Sidecar pattern for per-tenant policy\n- Docker Config as runtime data store\n- Canary rollout with simple traffic split\n\n## Code Example\n```go\n// Pseudo Go snippet: load quotas, track per-tenant usage, enforce TTL\n```\n\n## Follow-up Questions\n- How would you extend this to use JWTs for tenant identity?\n- What are the failure modes if the sidecar crashes or quotas data is stale?","diagram":null,"difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Discord","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T07:03:31.440Z","createdAt":"2026-01-26T07:03:31.440Z"},{"id":"q-7569","question":"Across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy in a docker-dca cluster. Add a canary release controlled by SLI/SLO: shift 25% of traffic to a new version only if 99th percentile latency < 200ms and error rate < 1% for 5 consecutive minutes, using Prometheus for metrics, Alertmanager, and a lightweight traffic-shift sidecar. Provide exact bootstrap commands and a docker-compose.yml skeleton?","answer":"Prometheus-driven canary. 25% traffic to v2 behind Nginx; SLI: p99 latency <200ms and error rate <1% for 5m. A traffic-sidecar polls metrics, updates Nginx weights via API, and triggers automatic roll","explanation":"## Why This Is Asked\nThis tests practical ability to implement data-driven, automated canary rollouts in a real multi-dc docker-dca setup, not just theory.\n\n## Key Concepts\n- Canary deployments\n- SLI/SLO definitions\n- Prometheus/Alertmanager integration\n- Nginx dynamic weight routing\n- mTLS with SPIFFE (Vault-backed)\n- Docker Compose bootstrap\n\n## Code Example\n```javascript\n// pseudo: sidecar update weights API\n```\n\n## Follow-up Questions\n- How would you define and collect SLI metrics in a multi-dc environment?\n- What failure modes require rollback and how would you test them?","diagram":null,"difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","MongoDB","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T07:55:11.006Z","createdAt":"2026-01-26T07:55:11.006Z"},{"id":"q-7662","question":"In a three-node docker-dca cluster across two data centers, deploy a TLS-enabled API behind an edge proxy. Implement image provenance with Sigstore cosign attestations verified by a dedicated edge verifier and per-node sidecar; route tenants to tenant-scoped service groups on a shared overlay and gate access with tenant-specific OPA policies loaded from a Docker Config. Provide exact bootstrap commands and a docker-compose.yml skeleton?","answer":"Use Sigstore cosign to sign images; deploy a verifier sidecar on each node and an edge-verifier at the gateway. Enable mTLS via SPIFFE/SVIDs and rotate keys with Vault or a similar CA. Isolate tenants","explanation":"## Why This Is Asked\n\nTests deep understanding of runtime image provenance, edge verification, and tenant isolation in a multi‑DC docker-dca setup. Evaluates how to integrate Sigstore attestations, per‑tenant routing, and policy enforcement with operational bootstrap reproducibility.\n\n## Key Concepts\n\n- Sigstore cosign attestations for image provenance\n- Edge and node‑level verifiers with mTLS\n- Tenant-scoped routing on a shared overlay\n- OPA policies loaded via Docker Config\n- Bootstrap reproducibility with docker-compose\n\n## Code Example\n\n```bash\n# sign and verify images with cosign\ncosign sign --key cosign.key repo/app:tag\ncosign verify --certificate-identity-regexp 'edge-registry.*' repo/app:tag\n```\n\n## Follow-up Questions\n\n- How would you handle cross‑DC key rotation without downtime?\n- How would you test provenance enforcement under a canary rollout?","diagram":"flowchart TD\n  EdgeProxy --> VerifierEdge[Edge Verifier]\n  VerifierEdge --> Router[Tenant Router]\n  Router --> ServiceGroup1[Tenant-Scoped Service Group 1]\n  Router --> ServiceGroup2[Tenant-Scoped Service Group 2]\n  ServiceGroup1 --> Audit[Audit Logs to SIEM]\n  ServiceGroup2 --> Audit","difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Plaid","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T11:47:53.461Z","createdAt":"2026-01-26T11:47:53.461Z"},{"id":"q-7699","question":"In a two-datacenter docker-dca cluster, deploy a TLS-enabled API behind an Nginx edge proxy. Implement per-tenant resource isolation with a gatekeeper container that uses CGroups to cap CPU/memory for each tenant's app container, with quotas stored in Redis and enforced by a lightweight sidecar. Add automatic throttling and a feedback loop to throttle or drop requests when quotas are exceeded, and ensure latency SLOs during a simulated burst. Provide exact bootstrap commands and a docker-compose.yml skeleton?","answer":"Outline a two-DC deployment with a TLS API behind Nginx. Implement a per-tenant gatekeeper container that reads quotas from Redis and enforces CPU/memory limits via CGroups, throttling requests when a","explanation":"## Why This Is Asked\n\nTests ability to design per-tenant resource governance in a multi-DC docker-dca stack, balancing isolation and performance under bursty load.\n\n## Key Concepts\n\n- CGroups per-tenant, Redis as quota store, edge proxy TLS, sidecar enforcement, backpressure, cross-DC consistency.\n\n## Code Example\n\n```go\n// Pseudo-code: gatekeeper reads quota and applies cgroup limits\n```\n\n## Follow-up Questions\n\n- How would you test quota violations? How would you handle Redis unavailability without starving tenants?","diagram":null,"difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Google","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-26T13:38:51.767Z","createdAt":"2026-01-26T13:38:51.767Z"},{"id":"q-7913","question":"In a two-node docker-dca cluster, deploy a TLS-protected API behind an Nginx edge proxy. Add OpenTelemetry tracing with a collector and Jaeger integration, plus a simple header-based canary rollout (X-Canary) that shifts 25% of traffic to a new image variant. Collect latency and error metrics with Prometheus and provide a minimal docker-compose.yml skeleton and bootstrap commands?","answer":"Deploy two API variants (v1 and v2) in a Docker DCA cluster. Configure Nginx as an edge proxy with TLS termination, implementing a 25% canary traffic split based on the X-Canary header. Instrument both services with OpenTelemetry, exporting traces to Jaeger via the OTLP collector. Configure Prometheus to collect latency and error metrics, with a complete docker-compose.yml configuration and bootstrap commands.","explanation":"## Why This Is Asked\n\nTests practical skills in observability, canary deployments, and Docker DCA cluster management.\n\n## Key Concepts\n\n- OpenTelemetry instrumentation and Jaeger trace visualization\n- OTLP (OpenTelemetry Protocol) exporter configuration\n- Nginx traffic splitting and header-based canary routing\n- Prometheus metrics collection and TLS-secured API exposure\n- Docker DCA cluster orchestration with docker-compose\n\n## Code Example\n\n```yaml\nversion: '3.8'\nservices:\n  api-v1:\n    image: myrepo/api:v1\n    environment:\n      - OTEL_SERVICE_NAME=api-v1\n      - OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317\n  \n  api-v2:\n    image: myrepo/api:v2\n    environment:\n      - OTEL_SERVICE_NAME=api-v2\n      - OTEL_EXPORTER_OTLP_ENDPOINT=otel-collector:4317\n  \n  nginx:\n    image: nginx:latest\n    ports:\n      - '443:443'\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n      - ./certs:/etc/nginx/certs:ro\n  \n  otel-collector:\n    image: otel/opentelemetry-collector:latest\n    volumes:\n      - ./otel-config.yaml:/etc/otel/config.yaml:ro\n  \n  jaeger:\n    image: jaegertracing/all-in-one:1.42\n    ports:\n      - '16686:16686'\n  \n  prometheus:\n    image: prom/prometheus:latest\n    ports:\n      - '9090:9090'\n    volumes:\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro\n```\n\n## Bootstrap Commands\n\n```bash\n# Deploy the cluster\ndocker-compose up -d\n\n# Verify services\ndocker-compose ps\n\n# Test canary routing\ncurl -k -H \"X-Canary: true\" https://localhost/health\ncurl -k https://localhost/health\n```","diagram":null,"difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Stripe"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T04:55:35.878Z","createdAt":"2026-01-26T22:45:55.153Z"},{"id":"q-7996","question":"In a two-data-center docker-dca Swarm, deploy a TLS-enabled gRPC API behind an Envoy edge proxy. Add a sidecar that translates gRPC to REST for downstream services, implement per-tenant rate limiting with Redis, and enable a 15% canary rollout of a new version with traffic-shifting and automatic rollback. Provide exact bootstrap commands and a docker-compose.yml skeleton?","answer":"Deploy Envoy as the TLS edge proxy in each data center and a small gRPC-to-REST bridge sidecar for downstream services. Use Vault-issued mTLS certs, Redis for per-tenant rate limits (tenant-id header)","explanation":"## Why This Is Asked\nTests cross-DC multi-service patterns, gRPC, Envoy, sidecar, rate limiting, and canary with rollback. Includes observability, TLS, and cross-data-center routing.\n\n## Key Concepts\n- Envoy edge proxy, TLS and mTLS with Vault\n- gRPC-to-REST bridge sidecar pattern\n- Per-tenant rate limiting with Redis\n- Canary rollout with weighted Envoy routes\n- Prometheus + Alertmanager for automatic rollback\n- Cross-DC service discovery and resilience\n\n## Code Example\n```yaml\n# Envoy bootstrap snippet (pseudo)\nstatic_resources:\n  listeners:\n  - address: {SocketAddress: {address: 0.0.0.0, port_value: 8443}}\n    filter_chains:\n    - filters: [{ name: envoy.filters.network.http_connection_manager, config: { /* TLS, routing, etc. */ }}]\n```\n\n## Follow-up Questions\n- How would you test the rollback in staging?\n- What failure modes should trigger a rollback and how would you validate tenant isolation during the canary?","diagram":"flowchart TD\n  Edge[Envoy Edge Proxy] --> Bridge[Grpc-REST Bridge]\n  Bridge --> Backend[Downstream REST Service]\n  Edge --> Redis[Redis per-tenant rate limits]\n  Metrics[Prometheus] --> Alerting[Alertmanager]\n","difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T04:38:05.722Z","createdAt":"2026-01-27T04:38:05.722Z"},{"id":"q-8238","question":"Across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy in a docker-dca cluster. Add OpenTelemetry tracing: instrument the API (language of choice) and run an OpenTelemetry Collector that exports traces via OTLP to Jaeger. Provide a docker-compose.yml skeleton with api, nginx, otel-collector, and jaeger; include basic 50% sampling?","answer":"Instrument the API with OpenTelemetry (Flask/Express as example), add an OTLP exporter to a local OpenTelemetry Collector, and forward traces to Jaeger. Use a 50% sampler, a minimal collector.yaml, an","explanation":"## Why This Is Asked\nTests ability to add observability to a Docker-based multi‑DC setup: tracing, TLS, and simple integration between app, edge, and backend collector. It’s beginner-friendly yet practical for real-world deployments.\n\n## Key Concepts\n- OpenTelemetry instrumentation and OTLP exporter\n- OpenTelemetry Collector and Jaeger backend\n- TLS edge proxy (Nginx) and service TLS wiring\n- Docker-Compose service wiring and basic config validation\n\n## Code Example\n```javascript\n// docker-compose.yml (skeleton)\nversion: '3.8'\nservices:\n  api:\n    image: your-api-image\n    ports:\n      - \"8080:8080\"\n  nginx:\n    image: nginx:stable\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n  otel-collector:\n    image: otel/opentelemetry-collector-contrib:latest\n    command: [\"--config=/etc/otel-collector-config.yaml\"]\n    volumes:\n      - ./otel-config.yaml:/etc/otel-collector-config.yaml:ro\n  jaeger:\n    image: jaegertracing/all-in-one\n    ports:\n      - \"16686:16686\"\n      - \"4317:4317\"\n```\n\n```python\n# Example: instrument Flask app (conceptual)\nfrom flask import Flask\nfrom opentelemetry.instrumentation.flask import FlaskInstrumentor\nfrom opentelemetry.exporter.otlp.proto.http import OTLPSpanExporter\nfrom opentelemetry.sdk.resources import SERVICE_NAME, Resource\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\n\napp = Flask(__name__)\nFlaskInstrumentor().instrument_app(app)\ntrace_provider = TracerProvider(resource=Resource.create({SERVICE_NAME: 'api-service'}))\ntrace_provider.add_span_processor(BatchSpanProcessor(OTLPSpanExporter(endpoint='http://otel-collector:4317/v1/traces')))\napp.run()\n```\n\n## Follow-up Questions\n- How would you adjust sampling or add baggage propagation across DCs?\n- How would you validate traces when altering infrastructure (edge restarts, collector restarts)?","diagram":null,"difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","Tesla","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T16:14:27.257Z","createdAt":"2026-01-27T16:14:27.257Z"},{"id":"q-8260","question":"Across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy in a docker-dca cluster. Implement a zero-downtime cross-DC Postgres migration: add a nullable new_col, perform a rolling backfill, and route tenants through a feature-gated read/write path via a lightweight sidecar that rewrites queries per-tenant. Canary 5%; monitor latency and error rate; auto-rollback if SLOs fail for 10 minutes. Include bootstrap commands and a docker-compose skeleton?","answer":"Implement a zero-downtime cross-DC Postgres migration: add a nullable new_col, perform a rolling backfill, and route tenants through a feature-gated read/write path via a lightweight sidecar that rewr","explanation":"## Why This Is Asked\nTests ability to design zero-downtime migrations in a multi-DC docker-dca setup, including per-tenant gating and rollback.\n\n## Key Concepts\n- Zero-downtime DB migrations; backward-compatibility; rolling backfill\n- Per-tenant feature flags; query rewriting sidecar\n- Canary deployments with strict SLOs; automatic rollback\n- TLS, Nginx edge proxy, docker-dca clustering\n\n## Code Example\n```yaml\n# docker-compose skeleton (high level, see repo for exact versions)\nversion: '3.8'\nservices:\n  api:\n    image: myapi:latest\n    environment:\n      - TENANT_GATE=on\n  nginx:\n    image: nginx:stable\n    ports:\n      - 443:443\n```\n\n## Follow-up Questions\n- How would you ensure data consistency during backfill across DCs?\n- How would you model per-tenant flags in etcd/Consul/Vault for the sidecar?","diagram":null,"difficulty":"intermediate","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Uber"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T17:07:05.006Z","createdAt":"2026-01-27T17:07:05.006Z"},{"id":"q-8384","question":"Two-DC docker-dca setup: TLS-enabled API behind an Nginx edge proxy. Implement a beginner-friendly mTLS using a self-signed CA (no Vault) and a simple 10% canary that routes to a new version via a basic Nginx rule. Provide bootstrap commands and a minimal docker-compose skeleton?","answer":"Generate a private CA, sign certs for edge and API services, configure Nginx as TLS terminator with two upstreams (api_v1, api_v2). Use nginx split_clients to route ~10% to api_v2. Include a simple he","explanation":"## Why This Is Asked\n\nTests basic TLS, container networking, and a simple canary without external vaults.\n\n## Key Concepts\n\n- TLS termination in Nginx, mTLS with a local CA\n- Basic canary routing in Nginx (split_clients)\n- Lightweight multi-service docker-compose skeleton across DCs\n\n## Code Example\n\n```yaml\n# docker-compose.yaml\nversion: '3.8'\nservices:\n  nginx:\n    image: nginx:stable\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n      - ./certs:/etc/nginx/certs:ro\n    ports:\n      - '8443:443'\n  api_v1:\n    image: yourapi:1\n  api_v2:\n    image: yourapi:2\n```\n\n```nginx\n# nginx.conf (TLS terminator + canary)\nserver {\n  listen 443 ssl;\n  ssl_certificate /etc/nginx/certs/server.crt;\n  ssl_certificate_key /etc/nginx/certs/server.key;\n  location / {\n    split_clients $request_uri $path_in_canary {\n      10%  v2;\n      default: v1;\n    }\n    if ($path_in_canary = v2) {\n      proxy_pass http://api_v2;\n    }\n    proxy_pass http://api_v1;\n  }\n}\n```\n\n## Follow-up Questions\n\n- How would you monitor the canary latency and set a rollback trigger?\n- How would you securely distribute the initial CA certificates to DCs?\n","diagram":null,"difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Google","IBM","LinkedIn"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-27T22:31:41.622Z","createdAt":"2026-01-27T22:31:41.622Z"},{"id":"q-8414","question":"In a three-node docker-dca cluster across two data centers, deploy a TLS-enabled API behind an Nginx edge proxy. Implement a policy-driven data plane: a sidecar validates tenant-scoped egress against a central policy store, with a policy controller that propagates updates across DCs and supports atomic canary rollouts. Add envelope encryption for per-tenant secrets and key rotation with zero downtime. Provide exact bootstrap commands and a docker-compose.yml skeleton?","answer":"Deploy a TLS-enabled API behind an Nginx edge proxy using Vault-issued SPIFFE SVIDs for mutual authentication. Implement a policy-driven data plane where a sidecar validates tenant-scoped egress against a central policy store, with a policy controller that propagates updates across data centers and supports atomic canary rollouts. Include envelope encryption for per-tenant secrets and zero-downtime key rotation.","explanation":"## Why This Is Asked\nThis evaluates expertise in designing distributed systems with policy-driven data planes across multiple data centers, requiring sophisticated approaches to live policy updates, drift detection, and secure per-tenant secret management with zero-downtime operations.\n\n## Key Concepts\n- Policy-driven egress enforcement through centralized policy stores\n- Cross-datacenter policy propagation with drift detection mechanisms\n- Envelope encryption for per-tenant secrets using Vault KMS\n- Zero-downtime key rotation strategies with atomic canary rollouts\n- Edge TLS termination with SPIFFE SVIDs and policy-enforcing sidecar architecture\n\n## Code Example\n```yaml\n# docker-compose skeleton\nversion: \"3.8\"\nservices:\n  edge-nginx:\n    image: nginx:alpine\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf\n      - ./certs:/etc/nginx/certs\n    ports:\n      - \"443:443\"\n    depends_on:\n      - api-service\n```\n\n```bash\n# Bootstrap commands\n# Initialize Vault with SPIFFE support\nvault operator init\nvault login <root-token>\nvault secrets enable transit\nvault write transit/keys/tenant-key type=rsa-4096\n\n# Generate SPIFFE SVIDs for services\nvault write pki_int/issue/api-role common_name=api.example.com ttl=24h\n```","diagram":null,"difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Databricks","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-28T04:52:33.863Z","createdAt":"2026-01-27T23:37:51.193Z"},{"id":"q-856","question":"You're running a Docker Swarm with services frontend, api, and worker. A feature-flag config is provided via Docker Config mounted at /etc/flags.json in all containers. You must rotate this config weekly with zero downtime. Describe the exact sequence of commands to create a new config version, rotate the services to use it, and implement a graceful reload inside apps so the new flags are picked up without losing requests. Include any Swarm update options you would tune?","answer":"Create a new config file (e.g., /tmp/flags.v2.json) and docker config create flags.v2 /tmp/flags.v2.json. Then update each service with rolling update options: docker service update --config-rm flags.v1 --config-add flags.v2 --update-parallelism 1 --update-delay 10s --update-order start-first frontend api worker. The apps should handle SIGHUP to reload /etc/flags.json without restart.","explanation":"## Why This Is Asked\nTo assess practical config rotation in Swarm and application reload behavior.\n\n## Key Concepts\n- Docker Config rotation with versioned configs\n- Rolling updates with update-parallelism and update-delay for zero downtime\n- Graceful reload semantics (SIGHUP or /reload endpoint)\n- Update order start-first to ensure new containers start before old ones stop\n\n## Code Example\n```javascript\n// Node.js reload handler\nconst fs = require('fs');\nlet flags = JSON.parse(fs.readFileSync('/etc/flags.json','utf8'));\nprocess.on('SIGHUP', () => {\n  flags = JSON.parse(fs.readFileSync('/etc/flags.json','utf8'));\n  // apply flags to runtime features\n});\n```\n\n## Follow-up Questions\n- How would you test the zero-downtime rollout in a CI pipeline?\n- What failure scenarios would prompt you to revert the config rotation?\n- How would you handle config validation before rotation?","diagram":null,"difficulty":"beginner","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hashicorp","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":["docker swarm","feature flag config","zero downtime","rolling update","config rotation","graceful reload","sighup signal","update parallelism","start-first order","versioned configs","service update","config mount"],"voiceSuitable":true,"isNew":false,"lastUpdated":"2026-01-22T05:03:30.080Z","createdAt":"2026-01-12T13:38:40.484Z"},{"id":"q-862","question":"In a Docker Swarm with a stateful web app that uses Postgres, you must roll out version 3.2 with a DB schema migration and zero downtime. Propose a concrete upgrade plan that uses a start-first, one-task-at-a-time update, a separate migration container, and post-migration validation. Include exact Swarm commands and a minimal docker-compose snippet showing update_config?","answer":"Plan: run DB migration first in a dedicated one-off container, then perform a start-first, one-task-at-a-time web upgrade. Commands: docker run --rm --network swarm_net migrate:3.2; docker service upd","explanation":"## Why This Is Asked\nAssesses ability to coordinate schema migrations with zero downtime, manage Swarm update strategies, and handle rollback safely.\n\n## Key Concepts\n- Start-first updates and parallelism control\n- Separate migration container orchestration\n- Post-migration validation and rollback\n- Health checks and observability during upgrade\n\n## Code Example\n```yaml\nversion: '3.8'\nservices:\n  web:\n    image: myapp:3.2\n    deploy:\n      update_config:\n        parallelism: 1\n        delay: 15s\n        order: start-first\n```\n\n## Follow-up Questions\n- How would you handle long-running migrations?\n- How ensure data consistency with read replicas during upgrade?","diagram":null,"difficulty":"advanced","tags":["docker-dca"],"channel":"docker-dca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["IBM","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:43:29.032Z","createdAt":"2026-01-12T13:43:29.032Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Citadel","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hashicorp","Hugging Face","IBM","Instacart","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","OpenAI","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Slack","Snap","Snowflake","Square","Stripe","Tesla","Twitter","Two Sigma","Uber","Zoom"],"stats":{"total":92,"beginner":29,"intermediate":29,"advanced":34,"newThisWeek":39}}