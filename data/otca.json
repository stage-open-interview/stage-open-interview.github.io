{"questions":[{"id":"q-1070","question":"You are building a latency-sensitive telemetry ingestion pipeline for autonomous fleets. Design an end-to-end data path from edge sensors to a real-time analytics sink. Include data schemas, serialization (protobuf vs Avro), transport (Kafka vs Kinesis), fault tolerance (idempotency, replay), schema evolution, and testing strategy. Explain trade-offs for throughput, latency, and reliability?","answer":"Propose a streaming telemetry pipeline: edge devices protobuf-encode data and publish to a transactional Kafka topic; use a Schema Registry for evolution; achieve exactly-once with Kafka transactions ","explanation":"## Why This Is Asked\nThis question probes end-to-end streaming design, practical trade-offs in serialization, transport, exactly-once semantics, schema evolution, and testing under production constraints.\n\n## Key Concepts\n- Protobuf schemas and Schema Registry\n- Kafka transactions and idempotent producers\n- Schema evolution compatibility\n- Throughput, latency, partitioning, backpressure\n- Replay and recovery strategies\n- Monitoring and observability\n\n## Code Example\n```javascript\n// Producer initialization with transactions\nconst producer = new Kafka.Producer({ transactionalId: 'telemetry-producer' });\nawait producer.initTransactions();\nawait producer.beginTransaction();\ntry {\n  await producer.send({ topic: 'telemetry', messages: [{ key, value }] });\n  await producer.commitTransaction();\n} catch (e) {\n  await producer.abortTransaction();\n}\n```\n\n## Follow-up Questions\n- How would you test resilience to broker outages and partition rebalancing?\n- How do you ensure backward/forward schema compatibility in the registry?","diagram":null,"difficulty":"advanced","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","NVIDIA","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T21:31:06.917Z","createdAt":"2026-01-12T21:31:06.917Z"},{"id":"q-1090","question":"Design a real-time notification system for 1M+ concurrent users with end-to-end latency under 200ms. Describe architecture, data model, delivery guarantees, back-pressure, disaster recovery, and how you measure and enforce SLOs?","answer":"Use a partitioned log (Kafka) by chat-room/region; producers publish idempotent messages; consumers deduplicate and write to a durable store (Cassandra). Fan-out via WebSocket gateways; store delivery","explanation":"## Why This Is Asked\n\nTests ability to design scalable real-time data pipelines with strict latency, global distribution, and reliable delivery guarantees.\n\n## Key Concepts\n\n- Distributed logs\n- Exactly/at-least-once processing\n- Deduplication and idempotency\n- Back-pressure and circuit breaking\n- Multi-region replication and SLIs\n\n## Code Example\n\n```javascript\n// Pseudo data model for message\nclass Message { constructor(id, chatId, payload, ts) { /* ... */ } }\n```\n\n## Follow-up Questions\n\n- How would you handle message replay after a consumer failure?\n- How would you ensure privacy/compliance across regions?","diagram":"flowchart TD\n  A[Producer] --> B[Log(Kafka)]\n  B --> C[Consumers]\n  C --> D[WebSocket Gateway]\n  D --> E[Clients]","difficulty":"advanced","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Cloudflare","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T22:22:14.986Z","createdAt":"2026-01-12T22:22:14.986Z"},{"id":"q-1115","question":"In a global OTCA stack for a large platform, design a fault-tolerant telemetry pipeline that ingests 10k events/sec, preserves dashboards with sub-30s freshness, and scales regionally. Describe data model, streaming, storage, and tracing choices (e.g., Kafka, OpenTelemetry, ClickHouse/BigQuery), backpressure handling, and testing strategy?","answer":"In a global OTCA stack, route regional telemetry to a Kafka pipeline with per-region partitions, ensuring at-least-once delivery and idempotent upserts. Store traces in a distributed trace backend (Op","explanation":"## Why This Is Asked\n\nAssesses ability to design scalable, fault-tolerant telemetry systems across regions, balancing latency, cost, and correctness. Requires concrete choices, trade-offs, and testing strategies.\n\n## Key Concepts\n\n- Telemetry pipelines and data models\n- Region-aware streaming and backpressure\n- Storage and query optimization for dashboards\n\n## Code Example\n\n```yaml\npipeline:\n  sources:\n  - name: otca_events\n    type: kafka\n    topics: otca_events\n    partitions: 24\n  sinks:\n  - name: metrics\n    type: clickhouse\n    database: otca_metrics\n    table: metrics_rollup\n```\n\n## Follow-up Questions\n\n- How would you test regional failure scenarios and roll back safely?\n- What cost-control strategies would you deploy without sacrificing observability?","diagram":null,"difficulty":"advanced","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Meta","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T23:26:12.996Z","createdAt":"2026-01-12T23:26:12.996Z"},{"id":"q-1171","question":"Design a multi-tenant OTCA telemetry pipeline with per-tenant quotas and privacy masking. Ingest 20k events/sec regionally via Kafka, route by tenant_id, and compute deduplicated aggregates in Flink. Hot metrics in Redis, long-term data in ClickHouse/BigQuery, traces via OpenTelemetry. Include idempotent writes, backpressure handling, and robust testing?","answer":"Design a multi-tenant OTCA telemetry pipeline with per-tenant quotas and privacy masking. Ingest 20k events/sec regionally via Kafka, route by tenant_id, and compute deduplicated aggregates in Flink. ","explanation":"## Why This Is Asked\nTests ability to design a scalable, privacy-conscious telemetry stack for a multi-tenant platform, with clear trade-offs between latency, storage, and governance.\n\n## Key Concepts\n- Multi-tenant isolation and privacy masking\n- Backpressure and deduplication in stream processing\n- Hot/cold storage architecture and cost control\n- Distributed tracing with OpenTelemetry\n- Quotas, testing, and privacy compliance\n\n## Code Example\n```javascript\n// Example: event envelope\n{ tenant_id: 't1', region: 'us-east', ts: 1630000000, type: 'metric', payload: { ... } }\n```\n\n## Follow-up Questions\n- How would you enforce per-tenant quotas in the streaming path?\n- What testing strategies validate privacy masking and quota enforcement?","diagram":"flowchart TD\n  A[Ingress] --> B[Partition by tenant_id]\n  B --> C[Flink processing]\n  C --> D[Hot storage: Redis]\n  C --> E[Cold storage: ClickHouse/BigQuery]\n  D --> F[Dashboards]\n  E --> G[Long-term analytics]","difficulty":"intermediate","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Coinbase","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T03:36:48.855Z","createdAt":"2026-01-13T03:36:48.855Z"},{"id":"q-1219","question":"In a multi-tenant OTCA pipeline for a global fintech platform, how would you implement tenant-scoped telemetry collection that isolates data, preserves sub-second dashboards, and enforces per-tenant quotas? Describe data model, per-tenant exporters, sampling, storage with row-level security, and testing strategy?","answer":"Implement a per-tenant telemetry schema with tenant_id, service, trace_id, span_id, metrics, and tags; route OTLP exports to tenant-scoped destinations; apply adaptive sampling based on per-tenant quo","explanation":"## Why This Is Asked\n\nAssess ability to design a scalable, multi-tenant OTCA telemetry pipeline with strict data isolation, low latency dashboards, and enforceable per-tenant quotas in a fintech context.\n\n## Key Concepts\n\n- Multi-tenant isolation and access controls\n- Telemetry data modeling with tenant scope\n- Per-tenant quotas and adaptive sampling\n- Per-tenant exporters and OTLP enrichment\n- Storage with Row-Level Security and tenant-aware access\n- Testing: synthetic tenants, load/chaos testing, privacy checks\n\n## Code Example\n\n```javascript\n// Enrich spans with tenant context before export (pseudo)\nfunction enrichWithTenant(span, tenantId) {\n  span.attributes = span.attributes || {};\n  span.attributes['tenant.id'] = tenantId;\n  return span;\n}\n```\n\n## Follow-up Questions\n\n- How would you implement per-tenant quotas and burst control?\n- How would you test for cross-tenant data leakage in queries?\n- How would you validate latency and freshness for tenant-specific dashboards?\n","diagram":null,"difficulty":"intermediate","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T05:31:34.966Z","createdAt":"2026-01-13T05:31:34.966Z"},{"id":"q-1237","question":"In a multi-tenant OTCA telemetry stack for a global SaaS, migrate from a fixed event schema to an evolving, backward/forward-compatible schema while preserving per-tenant data residency. The system must sustain up to 60k events/sec with regional bursts. Describe data model, schema versioning, streaming backbone, storage strategy (raw + materialized), tracing, backpressure handling, tenant-level sampling, and testing plan?","answer":"Adopt a versioned schema with a central registry; keep tenant data isolated via keyed partitions or per-tenant topics. Use Kafka with idempotent producers and backpressure driven by consumer lag. Raw ","explanation":"## Why This Is Asked\n\nTests ability to design evolving schemas, tenant isolation, and production-grade telemetry under bursty load.\n\n## Key Concepts\n\n- Versioned schema with registry and compatibility rules\n- Tenant isolation via keys/topics and residency constraints\n- Streaming backbone, backpressure, and idempotent producers\n- Storage strategy: raw (Parquet on S3) + materialized views (ClickHouse)\n- Observability: OpenTelemetry tracing and sampling\n- Reliability: DLQ, replay, canary tests\n\n## Code Example\n\n```javascript\n// Pseudo: resolve schema version and emit event with tenant key\nconst schema = registry.resolve(tenantId, eventType, version);\nconst payload = schema.serialize(event);\nproducer.send({ topic: topicForTenant(tenantId), value: payload });\n```\n\n## Follow-up Questions\n\n- How would you handle cross-version compatibility across consumers?\n- How do you validate performance during a simulated regional burst?","diagram":"flowchart TD\n  Tenant[Tenant] --> Topic[Telemetry Topic]\n  Topic --> Storage[Raw Parquet on S3 / Materialized in ClickHouse]\n  Storage --> Traces[OpenTelemetry]\n  Storage --> DLQ[Dead-Letter Queue]","difficulty":"intermediate","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","MongoDB","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T06:33:54.906Z","createdAt":"2026-01-13T06:33:54.906Z"},{"id":"q-1286","question":"In a global OTCA stack, design a fault-tolerant telemetry pipeline that ingests 20k events/sec per region from web/mobile clients, enforces per-tenant data residency, and uses adaptive sampling for long-tail tenants. Specify data model, streaming, storage, tracing, backpressure, and a test plan that validates burst behavior, schema evolution, and failover?","answer":"Design a global OTCA telemetry pipeline with per-tenant data residency, handling 20k events/sec per region, and dynamic sampling. Propose data model, streaming (e.g., Kafka + schema registry). Storage","explanation":"## Why This Is Asked\nThis question probes end-to-end telemetry design with residency, scalability, and reliability constraints, plus testing depth.\n\n## Key Concepts\n- Data residency and tenant isolation\n- Adaptive sampling and backpressure\n- Streaming, storage tiering, and tracing\n- Schema evolution and failover testing\n\n## Code Example\n```javascript\nfunction shouldSample(tenant, rate, burst) {\n  const key = tenant + ':' + burst;\n  const base = getTenantBaseRate(key);\n  return Math.random() < Math.min(1, rate / (base || 1));\n}\n```\n\n## Follow-up Questions\n- How would you validate schema evolution across tenants?\n- What monitoring signals indicate backpressure under burst load?","diagram":"flowchart TD\n  Edge[Clients] --> RegionalCollector[Regional Collector]\n  RegionalCollector --> Ingest[Kafka/Stream]\n  Ingest --> Store[Hot/Cold Storage]\n  Store --> Analyze[Analytics/Alerts]","difficulty":"advanced","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T08:32:26.722Z","createdAt":"2026-01-13T08:32:26.722Z"},{"id":"q-1307","question":"In a global OTCA telemetry stack with three regions, enforce tenant residency by region while enabling real-time global dashboards with sub-500ms latency. Provide the end-to-end ingestion, storage, and aggregation plan, including data model, streaming/backplane choices, per-tenant partitioning, cross-region replication policy, and a validation strategy for residency, schema evolution, and burst traffic?","answer":"Design a global OTCA telemetry stack across three regions with strict tenant residency. Route events to region-local Kafka topics and store raw data in regional storage; replicate non-sovereign tenant","explanation":"## Why This Is Asked\nTests residency enforcement, cross-region data routing, and real-time analytics under burst traffic.\n\n## Key Concepts\n- Region-local ingestion and storage\n- Tenant residency policies and data governance\n- Cross-region replication controls\n- Per-tenant partitioning and schema evolution\n- Streaming (Kafka) and backpressure strategies\n- Observability with OpenTelemetry\n\n## Code Example\n```javascript\n// Example event schema (versioned)\nconst event = {\n  tenant_id: 't123',\n  region: 'us-west',\n  ts: '2026-01-13T12:34:56Z',\n  event_type: 'click',\n  payload: { x: 42, y: 17 },\n  schema_version: 3\n}\n```\n\n## Follow-up Questions\n- How would you detect schema drift across regions and migrate without downtime?\n- What auditing would you add to prove residency commitments during failover?","diagram":null,"difficulty":"intermediate","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T10:34:28.056Z","createdAt":"2026-01-13T10:34:28.057Z"},{"id":"q-1495","question":"In a two-region OTCA telemetry pipeline for a microservices platform, design a privacy-preserving, adaptive sampling plan for distributed traces that enforces per-tenant data residency, minimizes data egress, and sustains sub-400ms end-to-end latency for dashboards. Detail the trace data model, propagation scheme, sampling algorithm, backpressure handling, and a validation plan?","answer":"Implement regional collectors bound to tenants; store traces regionally with encryption; use OTLP over gRPC with traceparent baggage carrying tenant id. Adaptive per-tenant budgets drive sampling; bas","explanation":"## Why This Is Asked\n\nTests ability to design privacy-aware telemetry with strict residency, while maintaining real-time observability.\n\n## Key Concepts\n\n- Per-tenant data residency and regional storage\n- Adaptive sampling with per-tenant budgets and priorities\n- Trace propagation (traceparent/baggage) and tenant tagging\n- Backpressure and per-tenant queuing to avoid jams\n- Validation: residency audits, latency targets, schema evolution, burst testing\n\n## Code Example\n\n```python\n# Simple adaptive sampling decision (conceptual)\nimport random\n\ndef should_sample(tenant, traffic, error_rate, config):\n    budget = config.get('budgets', {}).get(tenant, 0.02)  # 2% default\n    latency_factor = config.get('latency', 0.4)\n    scale = max(0.0, min(1.0, budget * (1.0 - error_rate) * (latency_factor / 0.4)))\n    return random.random() < scale\n```\n\n## Follow-up Questions\n\n- How would you test residency during tenant migrations across regions?\n- How would you backfill missed traces after an outage without leaking tenant data?","diagram":"flowchart TD\n  A[Tenant ID] --> B[Regional Collector]\n  B --> C[Regional Storage (Encrypted)]\n  A --> D[Propagation: traceparent/baggage]\n  B --> E[Sampling Engine (Per-tenant Budget)]\n  E --> F[Dashboard Analytics]\n  F --> G[Cross-region Correlation (privacy-preserving IDs)]","difficulty":"intermediate","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","NVIDIA","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:30:52.434Z","createdAt":"2026-01-13T19:30:52.434Z"},{"id":"q-1598","question":"Design a lightweight client-side OTCA telemetry exporter for a mobile app used across regions. The exporter must batch events (5 seconds or 1000 events), attach fields: tenant_id, device_id, app_version, event_type, and timestamp; implement offline queuing with local storage, retry with exponential backoff and jitter, and ensure eventual delivery when connectivity returns. Describe data model, batching, retry, and testing plan, plus how you measure correctness and dashboards?","answer":"Implement a lightweight client-side OTCA telemetry exporter with configurable batching (5-second window or 1000 events maximum). Each event includes required fields: tenant_id, device_id, app_version, event_type, and timestamp. Use local storage for offline queuing with serialized JSON persistence. Implement retry logic using exponential backoff with full jitter to prevent thundering herd problems. Ensure eventual delivery through automatic reconnection detection and queue processing. The exporter maintains in-memory buffers during normal operation and persists to local storage when offline, with automatic recovery on app restart.","explanation":"## Why This Is Asked\n\nAssess client-side OTCA export reliability and practical constraints in mobile contexts.\n\n## Key Concepts\n\n- Batched export, offline queue, idempotent delivery\n- Local storage strategy and data schema\n- Basic observability: latency, success rate, retries\n\n## Code Example\n\n```javascript\n// Simple batching skeleton (pseudo)\nclass Exporter {\n  constructor(batchMs=5000, maxBatch=1000) {...}\n  addEvent(e){...}\n  flush(){...}\n}\n```\n\n## Follow-up Questions\n\n- How would you test bursty network loss and ensure no data loss?\n- How would you evolve the schema without breaking dashboards?","diagram":"flowchart TD\n  A[Mobile App] --> B[Batcher & Serializer]\n  B --> C[Local Offline Cache]\n  C --> D[Network conditions]\n  D --> E[Regional Collector]\n  E --> F[Central Telemetry Store]","difficulty":"beginner","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:33:45.033Z","createdAt":"2026-01-14T02:27:47.039Z"},{"id":"q-1731","question":"In a global OTCA telemetry stack spanning five regions, tenants' raw events must remain within their origin region; only anonymized aggregates cross regions for global dashboards with sub-200ms latency. Design the end-to-end ingestion, streaming, storage, and access controls. Specify data models, de-identification/privacy controls, cross-region aggregation, backpressure, and a testing plan to validate residency, privacy, and latency under burst traffic?","answer":"Propose regional residency by design: ingest locally, emit only anonymized aggregates across regions. Use per-tenant, region-scoped streams; apply de-identification/tokenization at ingress; keep raw p","explanation":"## Why This Is Asked\n\nTests ability to design cross-region privacy-preserving telemetry with strict data residency and low-latency dashboards, plus concrete trade-offs between streaming platforms and governance.\n\n## Key Concepts\n\n- Data residency and privacy controls\n- Region-scoped streams and per-tenant schemas\n- Cross-region aggregation and governance\n- Backpressure and failover strategies\n\n## Code Example\n\n```javascript\n{\n  tenantId: string,\n  region: string,\n  eventType: string,\n  payload: object\n}\n```\n\n## Follow-up Questions\n\n- How would you validate residency and privacy under sudden burst traffic?\n- What metrics would you monitor to ensure sub-200ms dashboards remain stable?","diagram":"flowchart TD\n  A[Tenant Ingests Local Region] --> B[Local Streams]\n  B --> C[Local Aggregates]\n  C --> D[Anonymized Ship]\n  D --> E[Global Dashboards]","difficulty":"intermediate","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Microsoft","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T08:49:50.295Z","createdAt":"2026-01-14T08:49:50.295Z"},{"id":"q-1847","question":"For a global OTCA telemetry stack supporting a multi-tenant ML inference platform, design a region-aware telemetry pipeline that records: tenant_id, model_id, input_hash, latency_ms, outcome, and drift_score. Propose a streaming backbone, OLAP store, and a per-tenant residency policy with SLA-based QoS, plus backpressure, schema evolution, and testing plan?","answer":"Leverage a per-tenant, region-scoped ingest with a single router, using Pulsar for tenant namespaces and region-local ClickHouse for analytics. Ingest fields: tenant_id, model_id, input_hash, latency_","explanation":"## Why This Is Asked\nDesigning an OTCA pipeline with per-tenant residency and SLA-aware routing tests tenant isolation, cross-region analytics, and resilience.\n\n## Key Concepts\n- Multi-tenant namespaces in streaming\n- Region-local analytics with cross-region dashboards\n- CDC-based schema evolution and drift monitoring\n\n## Code Example\n```python\ndef route(event):\n    tenant = event['tenant_id']\n    topic = f'telemetry.{tenant}'\n    publish(topic, event)\n```\n\n## Follow-up Questions\n- How would you verify residency and backpressure under burst traffic?\n- How would you handle schema evolution without breaking dashboards?","diagram":"flowchart TD\n A[Ingress Router] --> B[Per-tenant Ingest]\n B --> C[Region Local Processing]\n C --> D[OLAP Store: ClickHouse]\n D --> E[Global Dashboards]","difficulty":"advanced","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Hugging Face","Robinhood","Two Sigma"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T14:34:10.648Z","createdAt":"2026-01-14T14:34:10.648Z"},{"id":"q-1900","question":"You're building a beginner OTCA telemetry pipeline for a mobile app used in two regions. Design a minimal streaming path that logs only essential fields (tenant_id, event_type, timestamp, latency_ms) and enforces per-tenant data access and privacy (redaction/anonymization). Describe the data model, streaming backbone, storage, and a practical test plan to validate privacy, QoS, and dashboard freshness (<=60s)?","answer":"Use Kafka with a per-tenant topic, schema {tenant_id, event_type, ts, latency_ms}. Redact device_id/location at edge; tokenize PII before publish. Store in a columnar store with tenant-scoped views (e","explanation":"## Why This Is Asked\nTests data minimization, tenant isolation, streaming choices, and privacy testing in a beginner-friendly OTCA setup.\n\n## Key Concepts\n- Data minimization\n- Tenant isolation\n- Streaming backbones\n- Privacy auditing\n\n## Code Example\n```json\n{\n  \"tenant_id\": \"tenant-123\",\n  \"event_type\": \"click\",\n  \"ts\": 1700000000000,\n  \"latency_ms\": 45\n}\n```\n\n## Follow-up Questions\n- How would you extend to support new event types without breaking dashboards?\n- How would you validate retention and privacy in production?","diagram":"flowchart TD\n  A[Mobile App] --> B[Edge Redaction]\n  B --> C[Per-tenant Kafka Topic]\n  C --> D[Storage with tenant ACLs]\n  D --> E[Dashboards]","difficulty":"beginner","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Microsoft"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T16:44:59.633Z","createdAt":"2026-01-14T16:44:59.633Z"},{"id":"q-1924","question":"Design a beginner OTCA telemetry path for a mobile app where events include tenant_id, event_type, timestamp, latency_ms. Implement a simple on-device dedup using event_id, normalize event_type into a canonical event family, and publish to a single Kafka topic with per-tenant routing to regional storage (Parquet on S3) to meet residency. Describe data model, streaming path, storage layout, and a minimal test plan validating dedup, schema evolution, and cross-region consistency?","answer":"Demonstrate a pipeline with on-device dedup (event_id), a canonical event_family derived from event_type, a Kafka sink (otca.events) partitioned by region with key tenant_id, and region-based Parquet ","explanation":"## Why This Is Asked\nTests ability to design a practical, beginner OTCA path with dedup, canonicalization, and residency. It highlights end-to-end thinking from device to durable store.\n\n## Key Concepts\n- De-duplication, canonical event families, idempotent writes\n- Streaming paths: local buffer → Kafka → regional sinks\n- Schema evolution and per-tenant data residency\n\n## Code Example\n```javascript\n// Example: simple canonical mapping (pseudo)\nconst mapEvent = (type)=> type.startsWith('click')? 'user_action':'system_event'\n```\n\n## Follow-up Questions\n- How would you test dedup at scale? \n- How would you handle schema changes without downtime?","diagram":"flowchart TD\n  Device[Mobile Device] --> Ingest[Ingest Layer]\n  Ingest --> Kafka[Kafka otca.events]\n  Kafka --> RegionSink[Regional Parquet store on S3]\n  RegionSink --> Analytics[BI Dashboards]","difficulty":"beginner","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Amazon","Plaid"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T17:37:24.993Z","createdAt":"2026-01-14T17:37:24.993Z"},{"id":"q-1999","question":"Design a global OTCA telemetry pipeline for real-time feature experimentation across MongoDB, Netflix, and Nvidia workloads. Each event includes device_id, experiment_id, feature_id, timestamp, latency_ms, and consent_flag. Requirements: per-tenant QoS with adaptive sampling, privacy masking for device_id, versioned schema with backward compatibility, cross-region attribution, and sub-200ms dashboard freshness. Outline data model, streaming backbone, storage layout, backpressure handling, and testing strategy?","answer":"Adopt a layered pipeline: versioned schema in a registry; device_id masked via HMAC per region; per-tenant QoS with adaptive sampling based on latency and traffic; ingest via Kafka topics partitioned ","explanation":"## Why This Is Asked\nThis question probes practical OTCA telemetry design for multi-region, multi-tenant experimentation with privacy, performance, and attribution.\n\n## Key Concepts\n- Versioned schema and registry for backward compatibility\n- PII masking via region-scoped HMAC\n- Adaptive sampling and per-tenant QoS\n- Cross-region attribution and lineage in storage\n- End-to-end tracing with OpenTelemetry\n\n## Code Example\n```javascript\nconst crypto = require('crypto');\nfunction maskDeviceId(deviceId, secret) {\n  return crypto.createHmac('sha256', secret).update(deviceId).digest('hex');\n}\n```\n\n## Follow-up Questions\n- How would you test cross-region attribution accuracy under partial network failure?\n- How would you validate schema evolution without breaking existing dashboards?","diagram":null,"difficulty":"intermediate","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["MongoDB","NVIDIA","Netflix"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T20:34:13.305Z","createdAt":"2026-01-14T20:34:13.305Z"},{"id":"q-2044","question":"Design a beginner OTCA telemetry path for a mobile app that must enforce per-tenant residency, basic privacy redaction, and monthly cost quotas while handling up to 5k events/sec. Provide a concrete data model (tenant_id, event_type, timestamp, latency_ms, event_id), a streaming topology (on-device dedup, region-specific Kafka, region-local Parquet storage with cross-region replication), and a test plan to verify residency, privacy, dedup, quota enforcement, and dashboard freshness?","answer":"Data model: tenant_id, event_type, timestamp, latency_ms, event_id. Ingest uses per-tenant Kafka topics with ACLs; region-local Parquet storage with cross-region replication. On-device deduplication by event_id prevents duplicates before transmission. Privacy redaction occurs at ingestion using field-level filtering. Per-tenant quotas are enforced through rate limiting and monthly cost tracking. Dashboard freshness is maintained through real-time streaming and materialized views.","explanation":"## Why This Is Asked\n\nTests ability to design a practical OTCA path at beginner scale with privacy, residency, and cost constraints, including end-to-end flow, data model, and testability.\n\n## Key Concepts\n\n- Data residency and tenant isolation\n- Privacy redaction at ingestion\n- Per-tenant quota enforcement\n- Streaming topology choices (Kafka)\n- Cross-region replication and storage layout\n- Test strategy for residency, privacy, deduplication, quotas, dashboard freshness\n\n## Code Example\n\n```javascript\nfunction redact(obj, fields) {\n  const copy = { ...obj };\n  fields.forEach(f => delete copy[f]);\n  return copy;\n}\n```","diagram":"flowchart TD\n  Client[Mobile App] --> Ingest[Ingest Layer]\n  Ingest --> Kafka[Region Kafka]\n  Kafka --> Store[Region Parquet Storage]\n  Store --> Dashboard[Analytics Dashboard]","difficulty":"beginner","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["NVIDIA","Oracle","Scale Ai"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T04:48:10.224Z","createdAt":"2026-01-14T21:48:47.026Z"},{"id":"q-2157","question":"Design a federated OTCA telemetry pipeline for a multi-tenant platform deployed on AWS, GCP, and Azure, where each event includes tenant_id, service_id, event_type, timestamp, latency_ms, and model_version. Propose a central schema registry with per-tenant Protobuf contracts and strict forward/backward compatibility, a streaming backbone with tenant-scoped topics (e.g., Pulsar), cross-cloud replication, and a compliant data deletion/retention policy. Outline data contracts, backfill strategy, testing, and observability?","answer":"Leverage a central Protobuf schema registry with per-tenant contracts and strict forward/backward compatibility. Ingest events with tenant_id, service_id, event_type, timestamp, latency_ms, model_vers","explanation":"## Why This Is Asked\n\nTests ability to design cross-cloud, multi-tenant OTCA pipelines with governance and schema contracts.\n\n## Key Concepts\n\n- Federated schema registry with per-tenant Protobuf contracts\n- Forward/backward compatibility and versioning\n- Tenant isolation via topic scoping and ACLs\n- Cross-cloud replication and data deletion/retention governance\n- Observability and testing strategies\n\n## Code Example\n\n```protobuf\nsyntax = \"proto3\";\npackage otca;\nmessage TelemetryEvent {\n  string tenant_id = 1;\n  string service_id = 2;\n  string event_type = 3;\n  int64 timestamp = 4;\n  int32 latency_ms = 5;\n  string model_version = 6;\n}\n```\n\n## Follow-up Questions\n\n- How would you test per-tenant deletion across all stores?\n- What are trade-offs of Pulsar vs Kafka in a multi-cloud OTCA setup?","diagram":"flowchart TD\n  A[Ingress] --> B[Schema Registry]\n  B --> C[Pulsar Tenant Topics]\n  C --> D[Cross-Cloud Replication]\n  D --> E[ClickHouse Partitions]\n  E --> F[Governance & Deletion]","difficulty":"advanced","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Plaid","Salesforce"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-15T05:37:24.244Z","createdAt":"2026-01-15T05:37:24.244Z"},{"id":"q-838","question":"You’re building a minimal product API in Express. Implement routes GET /products and GET /products/:id that returns 404 when not found. Include input validation, safe DB access via parameterized queries, and robust error handling. Explain your approach and provide a concise code sample?","answer":"Define two routes in Express: GET /products returns a list, GET /products/:id returns a single product or 404. Validate id as a positive integer; use parameterized queries to fetch data; sanitize inpu","explanation":"## Why This Is Asked\nInterview context explanation.\n\n## Key Concepts\n- Express routing\n- Input validation and sanitization\n- Parameterized queries to prevent SQL injection\n- Error handling middleware\n- Testing coverage (200/400/404)\n- Response shaping and caching\n\n## Code Example\n```javascript\nconst express = require('express');\nconst app = express();\nconst sqlite3 = require('sqlite3').verbose();\nconst db = new sqlite3.Database(':memory:');\napp.get('/products', (req, res) => {\n  db.all('SELECT id, name, price FROM products', [], (err, rows) => {\n    if (err) return res.status(500).json({error:'db'});\n    res.json(rows);\n  });\n});\napp.get('/products/:id', (req, res) => {\n  const id = parseInt(req.params.id, 10);\n  if (Number.isNaN(id) || id <= 0) return res.status(400).json({error:'invalid_id'});\n  db.get('SELECT id, name, price FROM products WHERE id = ?', [id], (err, row) => {\n    if (err) return res.status(500).json({error:'db'});\n    if (!row) return res.status(404).json({error:'not_found'});\n    res.json(row);\n  });\n});\n```\n\n## Follow-up Questions\n- How would you add pagination to /products?\n- How do you test error paths (400/404/500) effectively?","diagram":"flowchart TD\n  A[Client] --> B[Request /products]\n  B --> C{Find in DB}\n  C -- Found --> D[Return 200]\n  C -- Not Found --> E[Return 404]\n  D --> F[End]\n  E --> F","difficulty":"beginner","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Discord","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:23:19.628Z","createdAt":"2026-01-12T13:23:19.628Z"},{"id":"q-908","question":"You have a global edge API gateway handling peak bursts. How would you implement a distributed token-bucket rate limiter per API key across regions using Redis? Outline the Lua script for atomic refill and consume (capacity 1000, refill 50 tokens/sec), how you expose headers, handle clock skew, and fallback when Redis is unavailable?","answer":"Design a distributed token-bucket rate limiter for a global edge API gateway across regions using Redis. Outline the Lua script for atomic refill and consume (capacity 1000, refill 50 tokens/sec), how","explanation":"## Why This Is Asked\nTests real distributed rate limiting in a high-traffic, multi-region setup and probes practical trade-offs for latency, consistency, and failure handling.\n\n## Key Concepts\n- Distributed state via Redis\n- Token bucket algorithm\n- Atomic Lua scripting for refill/consume\n- Client-facing headers for rate limits\n- Fault tolerance and Redis fallback strategies\n\n## Code Example\n```lua\n-- Redis Lua script for token bucket\nlocal key = KEYS[1]\nlocal capacity = tonumber(ARGV[1])\nlocal refill = tonumber(ARGV[2])\nlocal now = tonumber(ARGV[3])\n\nlocal tokens = tonumber(redis.call('GET', key) or capacity)\nlocal last = tonumber(redis.call('GET', key .. ':t') or now)\nlocal delta = math.max(0, now - last)\nlocal new_tokens = math.min(capacity, tokens + delta * refill)\n\nif new_tokens < 1 then\n  return {0, new_tokens}\nelse\n  new_tokens = new_tokens - 1\n  redis.call('SET', key, new_tokens)\n  redis.call('SET', key .. ':t', now)\n  return {1, new_tokens}\nend\n```\n\n## Follow-up Questions\n- How would you test correctness under clock skew?\n- How would you monitor and alert for rate-limiter misbehaviors?","diagram":null,"difficulty":"intermediate","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Cloudflare","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T14:48:07.232Z","createdAt":"2026-01-12T14:48:07.232Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Amazon","Anthropic","Apple","Bloomberg","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Hugging Face","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Netflix","Oracle","PayPal","Plaid","Robinhood","Salesforce","Scale Ai","Snap","Snowflake","Square","Tesla","Twitter","Two Sigma","Zoom"],"stats":{"total":19,"beginner":5,"intermediate":8,"advanced":6,"newThisWeek":19}}