{"questions":[{"id":"q-1070","question":"You are building a latency-sensitive telemetry ingestion pipeline for autonomous fleets. Design an end-to-end data path from edge sensors to a real-time analytics sink. Include data schemas, serialization (protobuf vs Avro), transport (Kafka vs Kinesis), fault tolerance (idempotency, replay), schema evolution, and testing strategy. Explain trade-offs for throughput, latency, and reliability?","answer":"Propose a streaming telemetry pipeline: edge devices protobuf-encode data and publish to a transactional Kafka topic; use a Schema Registry for evolution; achieve exactly-once with Kafka transactions ","explanation":"## Why This Is Asked\nThis question probes end-to-end streaming design, practical trade-offs in serialization, transport, exactly-once semantics, schema evolution, and testing under production constraints.\n\n## Key Concepts\n- Protobuf schemas and Schema Registry\n- Kafka transactions and idempotent producers\n- Schema evolution compatibility\n- Throughput, latency, partitioning, backpressure\n- Replay and recovery strategies\n- Monitoring and observability\n\n## Code Example\n```javascript\n// Producer initialization with transactions\nconst producer = new Kafka.Producer({ transactionalId: 'telemetry-producer' });\nawait producer.initTransactions();\nawait producer.beginTransaction();\ntry {\n  await producer.send({ topic: 'telemetry', messages: [{ key, value }] });\n  await producer.commitTransaction();\n} catch (e) {\n  await producer.abortTransaction();\n}\n```\n\n## Follow-up Questions\n- How would you test resilience to broker outages and partition rebalancing?\n- How do you ensure backward/forward schema compatibility in the registry?","diagram":null,"difficulty":"advanced","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","NVIDIA","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T21:31:06.917Z","createdAt":"2026-01-12T21:31:06.917Z"},{"id":"q-1090","question":"Design a real-time notification system for 1M+ concurrent users with end-to-end latency under 200ms. Describe architecture, data model, delivery guarantees, back-pressure, disaster recovery, and how you measure and enforce SLOs?","answer":"Use a partitioned log (Kafka) by chat-room/region; producers publish idempotent messages; consumers deduplicate and write to a durable store (Cassandra). Fan-out via WebSocket gateways; store delivery","explanation":"## Why This Is Asked\n\nTests ability to design scalable real-time data pipelines with strict latency, global distribution, and reliable delivery guarantees.\n\n## Key Concepts\n\n- Distributed logs\n- Exactly/at-least-once processing\n- Deduplication and idempotency\n- Back-pressure and circuit breaking\n- Multi-region replication and SLIs\n\n## Code Example\n\n```javascript\n// Pseudo data model for message\nclass Message { constructor(id, chatId, payload, ts) { /* ... */ } }\n```\n\n## Follow-up Questions\n\n- How would you handle message replay after a consumer failure?\n- How would you ensure privacy/compliance across regions?","diagram":"flowchart TD\n  A[Producer] --> B[Log(Kafka)]\n  B --> C[Consumers]\n  C --> D[WebSocket Gateway]\n  D --> E[Clients]","difficulty":"advanced","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Cloudflare","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T22:22:14.986Z","createdAt":"2026-01-12T22:22:14.986Z"},{"id":"q-1115","question":"In a global OTCA stack for a large platform, design a fault-tolerant telemetry pipeline that ingests 10k events/sec, preserves dashboards with sub-30s freshness, and scales regionally. Describe data model, streaming, storage, and tracing choices (e.g., Kafka, OpenTelemetry, ClickHouse/BigQuery), backpressure handling, and testing strategy?","answer":"In a global OTCA stack, route regional telemetry to a Kafka pipeline with per-region partitions, ensuring at-least-once delivery and idempotent upserts. Store traces in a distributed trace backend (Op","explanation":"## Why This Is Asked\n\nAssesses ability to design scalable, fault-tolerant telemetry systems across regions, balancing latency, cost, and correctness. Requires concrete choices, trade-offs, and testing strategies.\n\n## Key Concepts\n\n- Telemetry pipelines and data models\n- Region-aware streaming and backpressure\n- Storage and query optimization for dashboards\n\n## Code Example\n\n```yaml\npipeline:\n  sources:\n  - name: otca_events\n    type: kafka\n    topics: otca_events\n    partitions: 24\n  sinks:\n  - name: metrics\n    type: clickhouse\n    database: otca_metrics\n    table: metrics_rollup\n```\n\n## Follow-up Questions\n\n- How would you test regional failure scenarios and roll back safely?\n- What cost-control strategies would you deploy without sacrificing observability?","diagram":null,"difficulty":"advanced","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Meta","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T23:26:12.996Z","createdAt":"2026-01-12T23:26:12.996Z"},{"id":"q-1171","question":"Design a multi-tenant OTCA telemetry pipeline with per-tenant quotas and privacy masking. Ingest 20k events/sec regionally via Kafka, route by tenant_id, and compute deduplicated aggregates in Flink. Hot metrics in Redis, long-term data in ClickHouse/BigQuery, traces via OpenTelemetry. Include idempotent writes, backpressure handling, and robust testing?","answer":"Design a multi-tenant OTCA telemetry pipeline with per-tenant quotas and privacy masking. Ingest 20k events/sec regionally via Kafka, route by tenant_id, and compute deduplicated aggregates in Flink. ","explanation":"## Why This Is Asked\nTests ability to design a scalable, privacy-conscious telemetry stack for a multi-tenant platform, with clear trade-offs between latency, storage, and governance.\n\n## Key Concepts\n- Multi-tenant isolation and privacy masking\n- Backpressure and deduplication in stream processing\n- Hot/cold storage architecture and cost control\n- Distributed tracing with OpenTelemetry\n- Quotas, testing, and privacy compliance\n\n## Code Example\n```javascript\n// Example: event envelope\n{ tenant_id: 't1', region: 'us-east', ts: 1630000000, type: 'metric', payload: { ... } }\n```\n\n## Follow-up Questions\n- How would you enforce per-tenant quotas in the streaming path?\n- What testing strategies validate privacy masking and quota enforcement?","diagram":"flowchart TD\n  A[Ingress] --> B[Partition by tenant_id]\n  B --> C[Flink processing]\n  C --> D[Hot storage: Redis]\n  C --> E[Cold storage: ClickHouse/BigQuery]\n  D --> F[Dashboards]\n  E --> G[Long-term analytics]","difficulty":"intermediate","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Coinbase","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T03:36:48.855Z","createdAt":"2026-01-13T03:36:48.855Z"},{"id":"q-1219","question":"In a multi-tenant OTCA pipeline for a global fintech platform, how would you implement tenant-scoped telemetry collection that isolates data, preserves sub-second dashboards, and enforces per-tenant quotas? Describe data model, per-tenant exporters, sampling, storage with row-level security, and testing strategy?","answer":"Implement a per-tenant telemetry schema with tenant_id, service, trace_id, span_id, metrics, and tags; route OTLP exports to tenant-scoped destinations; apply adaptive sampling based on per-tenant quo","explanation":"## Why This Is Asked\n\nAssess ability to design a scalable, multi-tenant OTCA telemetry pipeline with strict data isolation, low latency dashboards, and enforceable per-tenant quotas in a fintech context.\n\n## Key Concepts\n\n- Multi-tenant isolation and access controls\n- Telemetry data modeling with tenant scope\n- Per-tenant quotas and adaptive sampling\n- Per-tenant exporters and OTLP enrichment\n- Storage with Row-Level Security and tenant-aware access\n- Testing: synthetic tenants, load/chaos testing, privacy checks\n\n## Code Example\n\n```javascript\n// Enrich spans with tenant context before export (pseudo)\nfunction enrichWithTenant(span, tenantId) {\n  span.attributes = span.attributes || {};\n  span.attributes['tenant.id'] = tenantId;\n  return span;\n}\n```\n\n## Follow-up Questions\n\n- How would you implement per-tenant quotas and burst control?\n- How would you test for cross-tenant data leakage in queries?\n- How would you validate latency and freshness for tenant-specific dashboards?\n","diagram":null,"difficulty":"intermediate","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T05:31:34.966Z","createdAt":"2026-01-13T05:31:34.966Z"},{"id":"q-1237","question":"In a multi-tenant OTCA telemetry stack for a global SaaS, migrate from a fixed event schema to an evolving, backward/forward-compatible schema while preserving per-tenant data residency. The system must sustain up to 60k events/sec with regional bursts. Describe data model, schema versioning, streaming backbone, storage strategy (raw + materialized), tracing, backpressure handling, tenant-level sampling, and testing plan?","answer":"Adopt a versioned schema with a central registry; keep tenant data isolated via keyed partitions or per-tenant topics. Use Kafka with idempotent producers and backpressure driven by consumer lag. Raw ","explanation":"## Why This Is Asked\n\nTests ability to design evolving schemas, tenant isolation, and production-grade telemetry under bursty load.\n\n## Key Concepts\n\n- Versioned schema with registry and compatibility rules\n- Tenant isolation via keys/topics and residency constraints\n- Streaming backbone, backpressure, and idempotent producers\n- Storage strategy: raw (Parquet on S3) + materialized views (ClickHouse)\n- Observability: OpenTelemetry tracing and sampling\n- Reliability: DLQ, replay, canary tests\n\n## Code Example\n\n```javascript\n// Pseudo: resolve schema version and emit event with tenant key\nconst schema = registry.resolve(tenantId, eventType, version);\nconst payload = schema.serialize(event);\nproducer.send({ topic: topicForTenant(tenantId), value: payload });\n```\n\n## Follow-up Questions\n\n- How would you handle cross-version compatibility across consumers?\n- How do you validate performance during a simulated regional burst?","diagram":"flowchart TD\n  Tenant[Tenant] --> Topic[Telemetry Topic]\n  Topic --> Storage[Raw Parquet on S3 / Materialized in ClickHouse]\n  Storage --> Traces[OpenTelemetry]\n  Storage --> DLQ[Dead-Letter Queue]","difficulty":"intermediate","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","MongoDB","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T06:33:54.906Z","createdAt":"2026-01-13T06:33:54.906Z"},{"id":"q-1286","question":"In a global OTCA stack, design a fault-tolerant telemetry pipeline that ingests 20k events/sec per region from web/mobile clients, enforces per-tenant data residency, and uses adaptive sampling for long-tail tenants. Specify data model, streaming, storage, tracing, backpressure, and a test plan that validates burst behavior, schema evolution, and failover?","answer":"Design a global OTCA telemetry pipeline with per-tenant data residency, handling 20k events/sec per region, and dynamic sampling. Propose data model, streaming (e.g., Kafka + schema registry). Storage","explanation":"## Why This Is Asked\nThis question probes end-to-end telemetry design with residency, scalability, and reliability constraints, plus testing depth.\n\n## Key Concepts\n- Data residency and tenant isolation\n- Adaptive sampling and backpressure\n- Streaming, storage tiering, and tracing\n- Schema evolution and failover testing\n\n## Code Example\n```javascript\nfunction shouldSample(tenant, rate, burst) {\n  const key = tenant + ':' + burst;\n  const base = getTenantBaseRate(key);\n  return Math.random() < Math.min(1, rate / (base || 1));\n}\n```\n\n## Follow-up Questions\n- How would you validate schema evolution across tenants?\n- What monitoring signals indicate backpressure under burst load?","diagram":"flowchart TD\n  Edge[Clients] --> RegionalCollector[Regional Collector]\n  RegionalCollector --> Ingest[Kafka/Stream]\n  Ingest --> Store[Hot/Cold Storage]\n  Store --> Analyze[Analytics/Alerts]","difficulty":"advanced","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T08:32:26.722Z","createdAt":"2026-01-13T08:32:26.722Z"},{"id":"q-1307","question":"In a global OTCA telemetry stack with three regions, enforce tenant residency by region while enabling real-time global dashboards with sub-500ms latency. Provide the end-to-end ingestion, storage, and aggregation plan, including data model, streaming/backplane choices, per-tenant partitioning, cross-region replication policy, and a validation strategy for residency, schema evolution, and burst traffic?","answer":"Design a global OTCA telemetry stack across three regions with strict tenant residency. Route events to region-local Kafka topics and store raw data in regional storage; replicate non-sovereign tenant","explanation":"## Why This Is Asked\nTests residency enforcement, cross-region data routing, and real-time analytics under burst traffic.\n\n## Key Concepts\n- Region-local ingestion and storage\n- Tenant residency policies and data governance\n- Cross-region replication controls\n- Per-tenant partitioning and schema evolution\n- Streaming (Kafka) and backpressure strategies\n- Observability with OpenTelemetry\n\n## Code Example\n```javascript\n// Example event schema (versioned)\nconst event = {\n  tenant_id: 't123',\n  region: 'us-west',\n  ts: '2026-01-13T12:34:56Z',\n  event_type: 'click',\n  payload: { x: 42, y: 17 },\n  schema_version: 3\n}\n```\n\n## Follow-up Questions\n- How would you detect schema drift across regions and migrate without downtime?\n- What auditing would you add to prove residency commitments during failover?","diagram":null,"difficulty":"intermediate","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Cloudflare","Oracle","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T10:34:28.056Z","createdAt":"2026-01-13T10:34:28.057Z"},{"id":"q-1495","question":"In a two-region OTCA telemetry pipeline for a microservices platform, design a privacy-preserving, adaptive sampling plan for distributed traces that enforces per-tenant data residency, minimizes data egress, and sustains sub-400ms end-to-end latency for dashboards. Detail the trace data model, propagation scheme, sampling algorithm, backpressure handling, and a validation plan?","answer":"Implement regional collectors bound to tenants; store traces regionally with encryption; use OTLP over gRPC with traceparent baggage carrying tenant id. Adaptive per-tenant budgets drive sampling; bas","explanation":"## Why This Is Asked\n\nTests ability to design privacy-aware telemetry with strict residency, while maintaining real-time observability.\n\n## Key Concepts\n\n- Per-tenant data residency and regional storage\n- Adaptive sampling with per-tenant budgets and priorities\n- Trace propagation (traceparent/baggage) and tenant tagging\n- Backpressure and per-tenant queuing to avoid jams\n- Validation: residency audits, latency targets, schema evolution, burst testing\n\n## Code Example\n\n```python\n# Simple adaptive sampling decision (conceptual)\nimport random\n\ndef should_sample(tenant, traffic, error_rate, config):\n    budget = config.get('budgets', {}).get(tenant, 0.02)  # 2% default\n    latency_factor = config.get('latency', 0.4)\n    scale = max(0.0, min(1.0, budget * (1.0 - error_rate) * (latency_factor / 0.4)))\n    return random.random() < scale\n```\n\n## Follow-up Questions\n\n- How would you test residency during tenant migrations across regions?\n- How would you backfill missed traces after an outage without leaking tenant data?","diagram":"flowchart TD\n  A[Tenant ID] --> B[Regional Collector]\n  B --> C[Regional Storage (Encrypted)]\n  A --> D[Propagation: traceparent/baggage]\n  B --> E[Sampling Engine (Per-tenant Budget)]\n  E --> F[Dashboard Analytics]\n  F --> G[Cross-region Correlation (privacy-preserving IDs)]","difficulty":"intermediate","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["LinkedIn","NVIDIA","Snap"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-13T19:30:52.434Z","createdAt":"2026-01-13T19:30:52.434Z"},{"id":"q-1598","question":"Design a lightweight client-side OTCA telemetry exporter for a mobile app used across regions. The exporter must batch events (5 seconds or 1000 events), attach fields: tenant_id, device_id, app_version, event_type, and timestamp; implement offline queuing with local storage, retry with exponential backoff and jitter, and ensure eventual delivery when connectivity returns. Describe data model, batching, retry, and testing plan, plus how you measure correctness and dashboards?","answer":"Implement a lightweight client-side OTCA telemetry exporter with configurable batching (5-second window or 1000 events maximum). Each event includes required fields: tenant_id, device_id, app_version, event_type, and timestamp. Use local storage for offline queuing with serialized JSON persistence. Implement retry logic using exponential backoff with full jitter to prevent thundering herd problems. Ensure eventual delivery through automatic reconnection detection and queue processing. The exporter maintains in-memory buffers during normal operation and persists to local storage when offline, with automatic recovery on app restart.","explanation":"## Why This Is Asked\n\nAssess client-side OTCA export reliability and practical constraints in mobile contexts.\n\n## Key Concepts\n\n- Batched export, offline queue, idempotent delivery\n- Local storage strategy and data schema\n- Basic observability: latency, success rate, retries\n\n## Code Example\n\n```javascript\n// Simple batching skeleton (pseudo)\nclass Exporter {\n  constructor(batchMs=5000, maxBatch=1000) {...}\n  addEvent(e){...}\n  flush(){...}\n}\n```\n\n## Follow-up Questions\n\n- How would you test bursty network loss and ensure no data loss?\n- How would you evolve the schema without breaking dashboards?","diagram":"flowchart TD\n  A[Mobile App] --> B[Batcher & Serializer]\n  B --> C[Local Offline Cache]\n  C --> D[Network conditions]\n  D --> E[Regional Collector]\n  E --> F[Central Telemetry Store]","difficulty":"beginner","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Anthropic","Tesla","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T05:33:45.033Z","createdAt":"2026-01-14T02:27:47.039Z"},{"id":"q-1731","question":"In a global OTCA telemetry stack spanning five regions, tenants' raw events must remain within their origin region; only anonymized aggregates cross regions for global dashboards with sub-200ms latency. Design the end-to-end ingestion, streaming, storage, and access controls. Specify data models, de-identification/privacy controls, cross-region aggregation, backpressure, and a testing plan to validate residency, privacy, and latency under burst traffic?","answer":"Propose regional residency by design: ingest locally, emit only anonymized aggregates across regions. Use per-tenant, region-scoped streams; apply de-identification/tokenization at ingress; keep raw p","explanation":"## Why This Is Asked\n\nTests ability to design cross-region privacy-preserving telemetry with strict data residency and low-latency dashboards, plus concrete trade-offs between streaming platforms and governance.\n\n## Key Concepts\n\n- Data residency and privacy controls\n- Region-scoped streams and per-tenant schemas\n- Cross-region aggregation and governance\n- Backpressure and failover strategies\n\n## Code Example\n\n```javascript\n{\n  tenantId: string,\n  region: string,\n  eventType: string,\n  payload: object\n}\n```\n\n## Follow-up Questions\n\n- How would you validate residency and privacy under sudden burst traffic?\n- What metrics would you monitor to ensure sub-200ms dashboards remain stable?","diagram":"flowchart TD\n  A[Tenant Ingests Local Region] --> B[Local Streams]\n  B --> C[Local Aggregates]\n  C --> D[Anonymized Ship]\n  D --> E[Global Dashboards]","difficulty":"intermediate","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Microsoft","Oracle"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-14T08:49:50.295Z","createdAt":"2026-01-14T08:49:50.295Z"},{"id":"q-838","question":"Youâ€™re building a minimal product API in Express. Implement routes GET /products and GET /products/:id that returns 404 when not found. Include input validation, safe DB access via parameterized queries, and robust error handling. Explain your approach and provide a concise code sample?","answer":"Define two routes in Express: GET /products returns a list, GET /products/:id returns a single product or 404. Validate id as a positive integer; use parameterized queries to fetch data; sanitize inpu","explanation":"## Why This Is Asked\nInterview context explanation.\n\n## Key Concepts\n- Express routing\n- Input validation and sanitization\n- Parameterized queries to prevent SQL injection\n- Error handling middleware\n- Testing coverage (200/400/404)\n- Response shaping and caching\n\n## Code Example\n```javascript\nconst express = require('express');\nconst app = express();\nconst sqlite3 = require('sqlite3').verbose();\nconst db = new sqlite3.Database(':memory:');\napp.get('/products', (req, res) => {\n  db.all('SELECT id, name, price FROM products', [], (err, rows) => {\n    if (err) return res.status(500).json({error:'db'});\n    res.json(rows);\n  });\n});\napp.get('/products/:id', (req, res) => {\n  const id = parseInt(req.params.id, 10);\n  if (Number.isNaN(id) || id <= 0) return res.status(400).json({error:'invalid_id'});\n  db.get('SELECT id, name, price FROM products WHERE id = ?', [id], (err, row) => {\n    if (err) return res.status(500).json({error:'db'});\n    if (!row) return res.status(404).json({error:'not_found'});\n    res.json(row);\n  });\n});\n```\n\n## Follow-up Questions\n- How would you add pagination to /products?\n- How do you test error paths (400/404/500) effectively?","diagram":"flowchart TD\n  A[Client] --> B[Request /products]\n  B --> C{Find in DB}\n  C -- Found --> D[Return 200]\n  C -- Not Found --> E[Return 404]\n  D --> F[End]\n  E --> F","difficulty":"beginner","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Discord","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T13:23:19.628Z","createdAt":"2026-01-12T13:23:19.628Z"},{"id":"q-908","question":"You have a global edge API gateway handling peak bursts. How would you implement a distributed token-bucket rate limiter per API key across regions using Redis? Outline the Lua script for atomic refill and consume (capacity 1000, refill 50 tokens/sec), how you expose headers, handle clock skew, and fallback when Redis is unavailable?","answer":"Design a distributed token-bucket rate limiter for a global edge API gateway across regions using Redis. Outline the Lua script for atomic refill and consume (capacity 1000, refill 50 tokens/sec), how","explanation":"## Why This Is Asked\nTests real distributed rate limiting in a high-traffic, multi-region setup and probes practical trade-offs for latency, consistency, and failure handling.\n\n## Key Concepts\n- Distributed state via Redis\n- Token bucket algorithm\n- Atomic Lua scripting for refill/consume\n- Client-facing headers for rate limits\n- Fault tolerance and Redis fallback strategies\n\n## Code Example\n```lua\n-- Redis Lua script for token bucket\nlocal key = KEYS[1]\nlocal capacity = tonumber(ARGV[1])\nlocal refill = tonumber(ARGV[2])\nlocal now = tonumber(ARGV[3])\n\nlocal tokens = tonumber(redis.call('GET', key) or capacity)\nlocal last = tonumber(redis.call('GET', key .. ':t') or now)\nlocal delta = math.max(0, now - last)\nlocal new_tokens = math.min(capacity, tokens + delta * refill)\n\nif new_tokens < 1 then\n  return {0, new_tokens}\nelse\n  new_tokens = new_tokens - 1\n  redis.call('SET', key, new_tokens)\n  redis.call('SET', key .. ':t', now)\n  return {1, new_tokens}\nend\n```\n\n## Follow-up Questions\n- How would you test correctness under clock skew?\n- How would you monitor and alert for rate-limiter misbehaviors?","diagram":null,"difficulty":"intermediate","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Cloudflare","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":true,"lastUpdated":"2026-01-12T14:48:07.232Z","createdAt":"2026-01-12T14:48:07.232Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Anthropic","Apple","Bloomberg","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","LinkedIn","Lyft","Meta","Microsoft","MongoDB","NVIDIA","Oracle","PayPal","Snap","Snowflake","Square","Tesla","Twitter","Zoom"],"stats":{"total":13,"beginner":2,"intermediate":7,"advanced":4,"newThisWeek":13}}