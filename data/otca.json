{"questions":[{"id":"q-1070","question":"You are building a latency-sensitive telemetry ingestion pipeline for autonomous fleets. Design an end-to-end data path from edge sensors to a real-time analytics sink. Include data schemas, serialization (protobuf vs Avro), transport (Kafka vs Kinesis), fault tolerance (idempotency, replay), schema evolution, and testing strategy. Explain trade-offs for throughput, latency, and reliability?","answer":"Propose a streaming telemetry pipeline: edge devices protobuf-encode data and publish to a transactional Kafka topic; use a Schema Registry for evolution; achieve exactly-once with Kafka transactions ","explanation":"## Why This Is Asked\nThis question probes end-to-end streaming design, practical trade-offs in serialization, transport, exactly-once semantics, schema evolution, and testing under production constraints.\n\n## Key Concepts\n- Protobuf schemas and Schema Registry\n- Kafka transactions and idempotent producers\n- Schema evolution compatibility\n- Throughput, latency, partitioning, backpressure\n- Replay and recovery strategies\n- Monitoring and observability\n\n## Code Example\n```javascript\n// Producer initialization with transactions\nconst producer = new Kafka.Producer({ transactionalId: 'telemetry-producer' });\nawait producer.initTransactions();\nawait producer.beginTransaction();\ntry {\n  await producer.send({ topic: 'telemetry', messages: [{ key, value }] });\n  await producer.commitTransaction();\n} catch (e) {\n  await producer.abortTransaction();\n}\n```\n\n## Follow-up Questions\n- How would you test resilience to broker outages and partition rebalancing?\n- How do you ensure backward/forward schema compatibility in the registry?","diagram":null,"difficulty":"advanced","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Lyft","NVIDIA","Tesla"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T21:31:06.917Z","createdAt":"2026-01-12T21:31:06.917Z"},{"id":"q-1090","question":"Design a real-time notification system for 1M+ concurrent users with end-to-end latency under 200ms. Describe architecture, data model, delivery guarantees, back-pressure, disaster recovery, and how you measure and enforce SLOs?","answer":"Use a partitioned log (Kafka) by chat-room/region; producers publish idempotent messages; consumers deduplicate and write to a durable store (Cassandra). Fan-out via WebSocket gateways; store delivery","explanation":"## Why This Is Asked\n\nTests ability to design scalable real-time data pipelines with strict latency, global distribution, and reliable delivery guarantees.\n\n## Key Concepts\n\n- Distributed logs\n- Exactly/at-least-once processing\n- Deduplication and idempotency\n- Back-pressure and circuit breaking\n- Multi-region replication and SLIs\n\n## Code Example\n\n```javascript\n// Pseudo data model for message\nclass Message { constructor(id, chatId, payload, ts) { /* ... */ } }\n```\n\n## Follow-up Questions\n\n- How would you handle message replay after a consumer failure?\n- How would you ensure privacy/compliance across regions?","diagram":"flowchart TD\n  A[Producer] --> B[Log(Kafka)]\n  B --> C[Consumers]\n  C --> D[WebSocket Gateway]\n  D --> E[Clients]","difficulty":"advanced","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Apple","Cloudflare","Discord"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T22:22:14.986Z","createdAt":"2026-01-12T22:22:14.986Z"},{"id":"q-1115","question":"In a global OTCA stack for a large platform, design a fault-tolerant telemetry pipeline that ingests 10k events/sec, preserves dashboards with sub-30s freshness, and scales regionally. Describe data model, streaming, storage, and tracing choices (e.g., Kafka, OpenTelemetry, ClickHouse/BigQuery), backpressure handling, and testing strategy?","answer":"In a global OTCA stack, route regional telemetry to a Kafka pipeline with per-region partitions, ensuring at-least-once delivery and idempotent upserts. Store traces in a distributed trace backend (Op","explanation":"## Why This Is Asked\n\nAssesses ability to design scalable, fault-tolerant telemetry systems across regions, balancing latency, cost, and correctness. Requires concrete choices, trade-offs, and testing strategies.\n\n## Key Concepts\n\n- Telemetry pipelines and data models\n- Region-aware streaming and backpressure\n- Storage and query optimization for dashboards\n\n## Code Example\n\n```yaml\npipeline:\n  sources:\n  - name: otca_events\n    type: kafka\n    topics: otca_events\n    partitions: 24\n  sinks:\n  - name: metrics\n    type: clickhouse\n    database: otca_metrics\n    table: metrics_rollup\n```\n\n## Follow-up Questions\n\n- How would you test regional failure scenarios and roll back safely?\n- What cost-control strategies would you deploy without sacrificing observability?","diagram":null,"difficulty":"advanced","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["DoorDash","Meta","Twitter"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T23:26:12.996Z","createdAt":"2026-01-12T23:26:12.996Z"},{"id":"q-1171","question":"Design a multi-tenant OTCA telemetry pipeline with per-tenant quotas and privacy masking. Ingest 20k events/sec regionally via Kafka, route by tenant_id, and compute deduplicated aggregates in Flink. Hot metrics in Redis, long-term data in ClickHouse/BigQuery, traces via OpenTelemetry. Include idempotent writes, backpressure handling, and robust testing?","answer":"Design a multi-tenant OTCA telemetry pipeline with per-tenant quotas and privacy masking. Ingest 20k events/sec regionally via Kafka, route by tenant_id, and compute deduplicated aggregates in Flink. ","explanation":"## Why This Is Asked\nTests ability to design a scalable, privacy-conscious telemetry stack for a multi-tenant platform, with clear trade-offs between latency, storage, and governance.\n\n## Key Concepts\n- Multi-tenant isolation and privacy masking\n- Backpressure and deduplication in stream processing\n- Hot/cold storage architecture and cost control\n- Distributed tracing with OpenTelemetry\n- Quotas, testing, and privacy compliance\n\n## Code Example\n```javascript\n// Example: event envelope\n{ tenant_id: 't1', region: 'us-east', ts: 1630000000, type: 'metric', payload: { ... } }\n```\n\n## Follow-up Questions\n- How would you enforce per-tenant quotas in the streaming path?\n- What testing strategies validate privacy masking and quota enforcement?","diagram":"flowchart TD\n  A[Ingress] --> B[Partition by tenant_id]\n  B --> C[Flink processing]\n  C --> D[Hot storage: Redis]\n  C --> E[Cold storage: ClickHouse/BigQuery]\n  D --> F[Dashboards]\n  E --> G[Long-term analytics]","difficulty":"intermediate","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Adobe","Coinbase","Google"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T03:36:48.855Z","createdAt":"2026-01-13T03:36:48.855Z"},{"id":"q-1219","question":"In a multi-tenant OTCA pipeline for a global fintech platform, how would you implement tenant-scoped telemetry collection that isolates data, preserves sub-second dashboards, and enforces per-tenant quotas? Describe data model, per-tenant exporters, sampling, storage with row-level security, and testing strategy?","answer":"Implement a per-tenant telemetry schema with tenant_id, service, trace_id, span_id, metrics, and tags; route OTLP exports to tenant-scoped destinations; apply adaptive sampling based on per-tenant quo","explanation":"## Why This Is Asked\n\nAssess ability to design a scalable, multi-tenant OTCA telemetry pipeline with strict data isolation, low latency dashboards, and enforceable per-tenant quotas in a fintech context.\n\n## Key Concepts\n\n- Multi-tenant isolation and access controls\n- Telemetry data modeling with tenant scope\n- Per-tenant quotas and adaptive sampling\n- Per-tenant exporters and OTLP enrichment\n- Storage with Row-Level Security and tenant-aware access\n- Testing: synthetic tenants, load/chaos testing, privacy checks\n\n## Code Example\n\n```javascript\n// Enrich spans with tenant context before export (pseudo)\nfunction enrichWithTenant(span, tenantId) {\n  span.attributes = span.attributes || {};\n  span.attributes['tenant.id'] = tenantId;\n  return span;\n}\n```\n\n## Follow-up Questions\n\n- How would you implement per-tenant quotas and burst control?\n- How would you test for cross-tenant data leakage in queries?\n- How would you validate latency and freshness for tenant-specific dashboards?\n","diagram":null,"difficulty":"intermediate","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Goldman Sachs","PayPal"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T05:31:34.966Z","createdAt":"2026-01-13T05:31:34.966Z"},{"id":"q-1237","question":"In a multi-tenant OTCA telemetry stack for a global SaaS, migrate from a fixed event schema to an evolving, backward/forward-compatible schema while preserving per-tenant data residency. The system must sustain up to 60k events/sec with regional bursts. Describe data model, schema versioning, streaming backbone, storage strategy (raw + materialized), tracing, backpressure handling, tenant-level sampling, and testing plan?","answer":"Adopt a versioned schema with a central registry; keep tenant data isolated via keyed partitions or per-tenant topics. Use Kafka with idempotent producers and backpressure driven by consumer lag. Raw ","explanation":"## Why This Is Asked\n\nTests ability to design evolving schemas, tenant isolation, and production-grade telemetry under bursty load.\n\n## Key Concepts\n\n- Versioned schema with registry and compatibility rules\n- Tenant isolation via keys/topics and residency constraints\n- Streaming backbone, backpressure, and idempotent producers\n- Storage strategy: raw (Parquet on S3) + materialized views (ClickHouse)\n- Observability: OpenTelemetry tracing and sampling\n- Reliability: DLQ, replay, canary tests\n\n## Code Example\n\n```javascript\n// Pseudo: resolve schema version and emit event with tenant key\nconst schema = registry.resolve(tenantId, eventType, version);\nconst payload = schema.serialize(event);\nproducer.send({ topic: topicForTenant(tenantId), value: payload });\n```\n\n## Follow-up Questions\n\n- How would you handle cross-version compatibility across consumers?\n- How do you validate performance during a simulated regional burst?","diagram":"flowchart TD\n  Tenant[Tenant] --> Topic[Telemetry Topic]\n  Topic --> Storage[Raw Parquet on S3 / Materialized in ClickHouse]\n  Storage --> Traces[OpenTelemetry]\n  Storage --> DLQ[Dead-Letter Queue]","difficulty":"intermediate","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Databricks","MongoDB","Zoom"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T06:33:54.906Z","createdAt":"2026-01-13T06:33:54.906Z"},{"id":"q-1286","question":"In a global OTCA stack, design a fault-tolerant telemetry pipeline that ingests 20k events/sec per region from web/mobile clients, enforces per-tenant data residency, and uses adaptive sampling for long-tail tenants. Specify data model, streaming, storage, tracing, backpressure, and a test plan that validates burst behavior, schema evolution, and failover?","answer":"Design a global OTCA telemetry pipeline with per-tenant data residency, handling 20k events/sec per region, and dynamic sampling. Propose data model, streaming (e.g., Kafka + schema registry). Storage","explanation":"## Why This Is Asked\nThis question probes end-to-end telemetry design with residency, scalability, and reliability constraints, plus testing depth.\n\n## Key Concepts\n- Data residency and tenant isolation\n- Adaptive sampling and backpressure\n- Streaming, storage tiering, and tracing\n- Schema evolution and failover testing\n\n## Code Example\n```javascript\nfunction shouldSample(tenant, rate, burst) {\n  const key = tenant + ':' + burst;\n  const base = getTenantBaseRate(key);\n  return Math.random() < Math.min(1, rate / (base || 1));\n}\n```\n\n## Follow-up Questions\n- How would you validate schema evolution across tenants?\n- What monitoring signals indicate backpressure under burst load?","diagram":"flowchart TD\n  Edge[Clients] --> RegionalCollector[Regional Collector]\n  RegionalCollector --> Ingest[Kafka/Stream]\n  Ingest --> Store[Hot/Cold Storage]\n  Store --> Analyze[Analytics/Alerts]","difficulty":"advanced","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Meta","Snowflake"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-13T08:32:26.722Z","createdAt":"2026-01-13T08:32:26.722Z"},{"id":"q-838","question":"Youâ€™re building a minimal product API in Express. Implement routes GET /products and GET /products/:id that returns 404 when not found. Include input validation, safe DB access via parameterized queries, and robust error handling. Explain your approach and provide a concise code sample?","answer":"Define two routes in Express: GET /products returns a list, GET /products/:id returns a single product or 404. Validate id as a positive integer; use parameterized queries to fetch data; sanitize inpu","explanation":"## Why This Is Asked\nInterview context explanation.\n\n## Key Concepts\n- Express routing\n- Input validation and sanitization\n- Parameterized queries to prevent SQL injection\n- Error handling middleware\n- Testing coverage (200/400/404)\n- Response shaping and caching\n\n## Code Example\n```javascript\nconst express = require('express');\nconst app = express();\nconst sqlite3 = require('sqlite3').verbose();\nconst db = new sqlite3.Database(':memory:');\napp.get('/products', (req, res) => {\n  db.all('SELECT id, name, price FROM products', [], (err, rows) => {\n    if (err) return res.status(500).json({error:'db'});\n    res.json(rows);\n  });\n});\napp.get('/products/:id', (req, res) => {\n  const id = parseInt(req.params.id, 10);\n  if (Number.isNaN(id) || id <= 0) return res.status(400).json({error:'invalid_id'});\n  db.get('SELECT id, name, price FROM products WHERE id = ?', [id], (err, row) => {\n    if (err) return res.status(500).json({error:'db'});\n    if (!row) return res.status(404).json({error:'not_found'});\n    res.json(row);\n  });\n});\n```\n\n## Follow-up Questions\n- How would you add pagination to /products?\n- How do you test error paths (400/404/500) effectively?","diagram":"flowchart TD\n  A[Client] --> B[Request /products]\n  B --> C{Find in DB}\n  C -- Found --> D[Return 200]\n  C -- Not Found --> E[Return 404]\n  D --> F[End]\n  E --> F","difficulty":"beginner","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Airbnb","Discord","Meta"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T13:23:19.628Z","createdAt":"2026-01-12T13:23:19.628Z"},{"id":"q-908","question":"You have a global edge API gateway handling peak bursts. How would you implement a distributed token-bucket rate limiter per API key across regions using Redis? Outline the Lua script for atomic refill and consume (capacity 1000, refill 50 tokens/sec), how you expose headers, handle clock skew, and fallback when Redis is unavailable?","answer":"Design a distributed token-bucket rate limiter for a global edge API gateway across regions using Redis. Outline the Lua script for atomic refill and consume (capacity 1000, refill 50 tokens/sec), how","explanation":"## Why This Is Asked\nTests real distributed rate limiting in a high-traffic, multi-region setup and probes practical trade-offs for latency, consistency, and failure handling.\n\n## Key Concepts\n- Distributed state via Redis\n- Token bucket algorithm\n- Atomic Lua scripting for refill/consume\n- Client-facing headers for rate limits\n- Fault tolerance and Redis fallback strategies\n\n## Code Example\n```lua\n-- Redis Lua script for token bucket\nlocal key = KEYS[1]\nlocal capacity = tonumber(ARGV[1])\nlocal refill = tonumber(ARGV[2])\nlocal now = tonumber(ARGV[3])\n\nlocal tokens = tonumber(redis.call('GET', key) or capacity)\nlocal last = tonumber(redis.call('GET', key .. ':t') or now)\nlocal delta = math.max(0, now - last)\nlocal new_tokens = math.min(capacity, tokens + delta * refill)\n\nif new_tokens < 1 then\n  return {0, new_tokens}\nelse\n  new_tokens = new_tokens - 1\n  redis.call('SET', key, new_tokens)\n  redis.call('SET', key .. ':t', now)\n  return {1, new_tokens}\nend\n```\n\n## Follow-up Questions\n- How would you test correctness under clock skew?\n- How would you monitor and alert for rate-limiter misbehaviors?","diagram":null,"difficulty":"intermediate","tags":["otca"],"channel":"otca","subChannel":"general","sourceUrl":null,"videos":{"shortVideo":null,"longVideo":null},"companies":["Bloomberg","Cloudflare","Square"],"eli5":null,"relevanceScore":null,"voiceKeywords":null,"voiceSuitable":false,"isNew":false,"lastUpdated":"2026-01-12T14:48:07.232Z","createdAt":"2026-01-12T14:48:07.232Z"}],"subChannels":["general"],"companies":["Adobe","Airbnb","Apple","Bloomberg","Cloudflare","Coinbase","Databricks","Discord","DoorDash","Goldman Sachs","Google","Lyft","Meta","MongoDB","NVIDIA","PayPal","Snowflake","Square","Tesla","Twitter","Zoom"],"stats":{"total":9,"beginner":1,"intermediate":4,"advanced":4,"newThisWeek":9}}